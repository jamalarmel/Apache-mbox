From dev-return-8657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 09:02:01 2014
Return-Path: <dev-return-8657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06AAB11213
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 09:02:01 +0000 (UTC)
Received: (qmail 72722 invoked by uid 500); 1 Aug 2014 09:02:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72663 invoked by uid 500); 1 Aug 2014 09:02:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72647 invoked by uid 99); 1 Aug 2014 09:01:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 09:01:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi@asiainfo.com designates 202.85.210.242 as permitted sender)
Received: from [202.85.210.242] (HELO asiainfo-linkage.com) (202.85.210.242)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 01 Aug 2014 09:01:56 +0000
Received: from [10.1.51.163]([10.1.51.163]) by asiainfo.com(AIMCPRO 1.0.0.0)
	with SMTP id jm16353db6926; Fri, 01 Aug 2014 17:00:29 +0800
From: =?GB2312?B?zO/S4w==?= <tianyi@asiainfo.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_AB1A1938-C1E1-44AD-86B0-2FD5941744D9"
Subject: How to run specific sparkSQL test with maven
Message-Id: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com>
Date: Fri, 1 Aug 2014 17:00:19 +0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
X-Mailer: Apple Mail (2.1878.6)
X-AIMC-AUTH: tianyi
X-AIMC-MAILFROM: tianyi@asiainfo.com
X-AIMC-Msg-ID: NI1hIN7B
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_AB1A1938-C1E1-44AD-86B0-2FD5941744D9
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=GB2312

Hi everyone!

Could any one tell me how to run specific sparkSQL test with maven?

For example:

I want to test HiveCompatibilitySuite.

I ran =A1=B0mvm test -Dtest=3DHiveCompatibilitySuite=A1=B1

It did not work.=20

BTW, is there any information about how to build a test environment of =
sparkSQL?

I got this error when i ran the test.

It seems that the HiveCompatibilitySuite need a hadoop and hive =
environment, am I right?
=20
"Relative path in absolute URI: =
file:$%7Bsystem:test.tmp.dir%7D/tmp_showcrt1=A1=B1=20






--Apple-Mail=_AB1A1938-C1E1-44AD-86B0-2FD5941744D9--

From dev-return-8658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 13:51:16 2014
Return-Path: <dev-return-8658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A8BE119B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 13:51:16 +0000 (UTC)
Received: (qmail 5799 invoked by uid 500); 1 Aug 2014 13:51:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5735 invoked by uid 500); 1 Aug 2014 13:51:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5715 invoked by uid 99); 1 Aug 2014 13:51:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 13:51:13 +0000
X-ASF-Spam-Status: No, hits=3.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of witgo@qq.com does not designate 103.7.29.139 as permitted sender)
Received: from [103.7.29.139] (HELO smtpbg64.qq.com) (103.7.29.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 13:51:09 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1406901043; bh=k2G1zU2V51NU5s3M16+SydG++fRERQT70HXyS3G5ZIQ=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=b1KAwp+5awRK/IpBlAMk2mGCFtW3bp7bm1fzq9cQZpmztDpM7hTjsrX4DNsJacqZ2
	 mSEuX6vyoogF0J04wEEvkipcVa8vCp7t3PScTVMOcdSDQv00Pe7HV4yNG5+Bi4rCqn
	 zCRk+e4o0UpnD8cb1ZuE4zyG7MM9rv7TEcMttcpU=
X-QQ-FEAT: QrzP523gfI+uCjJjYiGeQxNwHyTNPOnJqXHlWLccnNRT3p5WVE46W8zq2doyC
	4gEMVx8YFsZpHe051+vmtUjE2RNc9j1ID8yD+O/c3ANYFO3Nw8d0ULVaaPi+La0JoSJia5X
	sCAG84gkWtHlWIiANqTraqkrkpb6zgWcUl1xsiac6JZy3GtYONPLnIRRiG9T
X-QQ-SSF: 0000000A0000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 60.10.154.184
In-Reply-To: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com>
References: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1406901042t3713
From: "=?utf-8?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?utf-8?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re:How to run specific sparkSQL test with maven
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53DB9B32_08D80450_65C7C3B1"
Content-Transfer-Encoding: 8Bit
Date: Fri, 1 Aug 2014 21:50:42 +0800
X-Priority: 3
Message-ID: <tencent_29D16B3809F43463084CD4CB@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 1361782347
X-QQ-SENDSIZE: 520
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53DB9B32_08D80450_65C7C3B1
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64

WW91IGNhbiB0cnkgdGhlc2UgY29tbWFuZHPigI0NCi4vc2J0L3NidCBhc3NlbWJseeKAjS4v
c2J0L3NidCAidGVzdC1vbmx5ICouSGl2ZUNvbXBhdGliaWxpdHlTdWl0ZSIgLVBoaXZl4oCN
DQoNCuKAjQ0KDQoNCg0KDQoNCi0tLS0tLS0tLS0tLS0tLS0tLSBPcmlnaW5hbCAtLS0tLS0t
LS0tLS0tLS0tLS0NCkZyb206ICAi55Sw5q+FIjs8dGlhbnlpQGFzaWFpbmZvLmNvbT47DQpE
YXRlOiAgRnJpLCBBdWcgMSwgMjAxNCAwNTowMCBQTQ0KVG86ICAiZGV2IjxkZXZAc3Bhcmsu
YXBhY2hlLm9yZz47IA0KDQpTdWJqZWN0OiAgSG93IHRvIHJ1biBzcGVjaWZpYyBzcGFya1NR
TCB0ZXN0IHdpdGggbWF2ZW4NCg0KDQoNCkhpIGV2ZXJ5b25lIQ0KDQpDb3VsZCBhbnkgb25l
IHRlbGwgbWUgaG93IHRvIHJ1biBzcGVjaWZpYyBzcGFya1NRTCB0ZXN0IHdpdGggbWF2ZW4/
DQoNCkZvciBleGFtcGxlOg0KDQpJIHdhbnQgdG8gdGVzdCBIaXZlQ29tcGF0aWJpbGl0eVN1
aXRlLg0KDQpJIHJhbiDigJxtdm0gdGVzdCAtRHRlc3Q9SGl2ZUNvbXBhdGliaWxpdHlTdWl0
ZeKAnQ0KDQpJdCBkaWQgbm90IHdvcmsuIA0KDQpCVFcsIGlzIHRoZXJlIGFueSBpbmZvcm1h
dGlvbiBhYm91dCBob3cgdG8gYnVpbGQgYSB0ZXN0IGVudmlyb25tZW50IG9mIHNwYXJrU1FM
Pw0KDQpJIGdvdCB0aGlzIGVycm9yIHdoZW4gaSByYW4gdGhlIHRlc3QuDQoNCkl0IHNlZW1z
IHRoYXQgdGhlIEhpdmVDb21wYXRpYmlsaXR5U3VpdGUgbmVlZCBhIGhhZG9vcCBhbmQgaGl2
ZSBlbnZpcm9ubWVudCwgYW0gSSByaWdodD8NCiANCiJSZWxhdGl2ZSBwYXRoIGluIGFic29s
dXRlIFVSSTogZmlsZTokJTdCc3lzdGVtOnRlc3QudG1wLmRpciU3RC90bXBfc2hvd2NydDHi
gJ0=

------=_NextPart_53DB9B32_08D80450_65C7C3B1--


From dev-return-8659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 14:24:07 2014
Return-Path: <dev-return-8659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CCB9911A7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 14:24:07 +0000 (UTC)
Received: (qmail 78767 invoked by uid 500); 1 Aug 2014 14:24:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78704 invoked by uid 500); 1 Aug 2014 14:24:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78691 invoked by uid 99); 1 Aug 2014 14:24:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 14:24:06 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of freeman.jeremy@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 14:24:05 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <freeman.jeremy@gmail.com>)
	id 1XDDkC-0003I7-G0
	for dev@spark.incubator.apache.org; Fri, 01 Aug 2014 07:23:40 -0700
Date: Fri, 1 Aug 2014 07:23:40 -0700 (PDT)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1406903020339-7626.post@n3.nabble.com>
In-Reply-To: <tencent_29D16B3809F43463084CD4CB@qq.com>
References: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com> <tencent_29D16B3809F43463084CD4CB@qq.com>
Subject: Re: Re:How to run specific sparkSQL test with maven
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

With maven you can run a particular test suite like this:

mvn -DwildcardSuites=org.apache.spark.sql.SQLQuerySuite test

see the note here (under "Spark Tests in Maven"):

http://spark.apache.org/docs/latest/building-with-maven.html



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-run-specific-sparkSQL-test-with-maven-tp7624p7626.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 16:49:00 2014
Return-Path: <dev-return-8660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E65811125
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 16:49:00 +0000 (UTC)
Received: (qmail 73259 invoked by uid 500); 1 Aug 2014 16:48:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73202 invoked by uid 500); 1 Aug 2014 16:48:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73177 invoked by uid 99); 1 Aug 2014 16:48:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 16:48:58 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.52 as permitted sender)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 16:48:54 +0000
Received: by mail-la0-f52.google.com with SMTP id e16so3383593lan.39
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 09:48:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=khdmDtIkjdeb5Po6FhWFYr72EwXP4tzMdQn1zfM8ZDM=;
        b=PR1j8whFJ/cL+NpVTqIi6/7S/l211kI11vPvbCNnOOqUysXb9Zq79SlmLkGKswClB2
         ORN+BytMuLJJrTI93GXr/93h07A/M+bMvKEar9bTek60+0NWRc24OxU9l3O4RLQG7Mvp
         EXx+1WCZ2H0AnHwrwqkRhp7YQpAqBm1rHXcMLiCjRQHKMpyruU1ihGPMNpHuBVVHLvl7
         bcsRVtqmnqRZN+pM6MvBYY9dfWwnZVSb3V9tZTCBU7pBhIb7oIf8j/yOeOxA6jsNI8IM
         HjPO0lvuFoyMFKM3RKRBIp5GHYVKSsMKQz7Gsim2UvonfwYyHkb/vcfPqpKpj44cZeRp
         gbww==
X-Received: by 10.152.179.137 with SMTP id dg9mr7571544lac.11.1406911711673;
 Fri, 01 Aug 2014 09:48:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Fri, 1 Aug 2014 09:48:11 -0700 (PDT)
In-Reply-To: <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
 <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com> <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Fri, 1 Aug 2014 18:48:11 +0200
Message-ID: <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11349906ce048e04ff942892
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11349906ce048e04ff942892
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Heya,
Dunno if these ideas are still in the air or felt in the warp ^^.
However there is a paper on avocado
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/p=
roject8_report.pdf>
that
mentions a way of working with their data (sequence's reads) in a windowed
manner without neither time nor timestamp field's value, but a kind-of
internal index as range delimiter -- thus defining their own exotic
continuum and break function.

greetz,

 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Thu, Jul 17, 2014 at 1:11 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Indeed, these two cases are tightly coupled (the first one is a special
> case of the second).
>
> Actually, these "outliers" could be handled by a dedicated function what =
I
> named outliersManager -- I was not so much inspired ^^, but we could name
> these outliers, "outlaws" and thus the function would be "sheriff".
> The purpose of this "sheriff" function would be to create yet another
> distributed collection (RDD, CRDD, ...?) with only the --outliers-- outla=
ws
> in it.
>
> Because these problems have a nature which will be as different as the us=
e
> case will be, it's hard to find a generic way to tackle them. So, you
> know... that's why... I put temporarily them in jail and wait for the jud=
ge
> to show them the right path! (.... okay it's late in Belgium -- 1AM).
>
> All in all, it's more or less what we would do in DStream as well actuall=
y.
> Let me expand a bit this reasoning, let's assume that some data points ca=
n
> come along with the time, but aren't in sync with it -- f.i., a device th=
at
> wakes up and send all it's data at once.
> The DStream will package them into RDDs mixed-up with true current data
> points, however, the logic of the job will have to use a 'Y' road :
> * to integrate them into a database at the right place
> * to simply drop them out because they're won't be part of a shown chart
> * etc
>
> In this case, the 'Y' road would be of the contract ;-), and so left at
> the appreciation of the dev.
>
> Another way, to do it would be to ignore but log them, but that would be
> very crappy, non professional and useful (and of course I'm just kidding)=
.
>
> my0.002=C2=A2
>
>
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <
> tathagata.das1565@gmail.com> wrote:
>
>> I think it makes sense, though without a concrete implementation its har=
d
>> to be sure. Applying sorting on the RDD according to the RDDs makes sens=
e,
>> but I can think of two kinds of fundamental problems.
>>
>> 1. How do you deal with ordering across RDD boundaries. Say two
>> consecutive
>> RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2,
>> 3,
>> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function throu=
gh
>> all these records in the timestamp order. I am curious to find how this
>> problem can be solved without sacrificing efficiency (e.g. I can imagine
>> doing multiple pass magic)
>>
>> 2. An even more fundamental question is how do you ensure ordering with
>> delayed records. If you want to process in order of application time, an=
d
>> records are delayed how do you deal with them.
>>
>> Any ideas? ;)
>>
>> TD
>>
>>
>>
>> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
>> wrote:
>>
>> > Heya TD,
>> >
>> > Thanks for the detailed answer! Much appreciated.
>> >
>> > Regarding order among elements within an RDD, you're definitively righ=
t,
>> > it'd kill the //ism and would require synchronization which is
>> completely
>> > avoided in distributed env.
>> >
>> > That's why, I won't push this constraint to the RDDs themselves
>> actually,
>> > only the Space is something that *defines* ordered elements, and thus
>> there
>> > are two functions that will break the RDDs based on a given (extensibl=
e,
>> > plugable) heuristic f.i.
>> > Since the Space is rather decoupled from the data, thus the source and
>> the
>> > partitions, it's the responsibility of the CRRD implementation to
>> dictate
>> > how (if necessary) the elements should be sorted in the RDDs... which
>> will
>> > require some shuffles :-s -- Or the couple (source, space) is somethin=
g
>> > intrinsically ordered (like it is for DStream).
>> >
>> > To be more concrete an RDD would be composed of un-ordered iterator of
>> > millions of events for which all timestamps land into the same time
>> > interval.
>> >
>> > WDYT, would that makes sense?
>> >
>> > thanks again for the answer!
>> >
>> > greetz
>> >
>> >  a=E2=84=95dy =E2=84=99etrella
>> > about.me/noootsab
>> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>> >
>> > <http://about.me/noootsab>
>> >
>> >
>> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
>> > tathagata.das1565@gmail.com
>> > > wrote:
>> >
>> > > Very interesting ideas Andy!
>> > >
>> > > Conceptually i think it makes sense. In fact, it is true that dealin=
g
>> > with
>> > > time series data, windowing over application time, windowing over
>> number
>> > of
>> > > events, are things that DStream does not natively support. The real
>> > > challenge is actually mapping the conceptual windows with the
>> underlying
>> > > RDD model. On aspect you correctly observed in the ordering of event=
s
>> > > within the RDDs of the DStream. Another fundamental aspect is the fa=
ct
>> > that
>> > > RDDs as parallel collections, with no well-defined ordering in the
>> > records
>> > > in the RDDs. If you want to process the records in an RDD as a order=
ed
>> > > stream of events, you kind of have to process the stream sequentiall=
y,
>> > > which means you have to process each RDD partition one-by-one, and
>> > > therefore lose the parallelism. So implementing all these
>> functionality
>> > may
>> > > mean adding functionality at the cost of performance. Whether that i=
s
>> > okay
>> > > for Spark Streaming to have these OR this tradeoff is not-intuitive
>> for
>> > > end-users and therefore should not come out-of-the-box with Spark
>> > Streaming
>> > > -- that is a definitely a question worth debating upon.
>> > >
>> > > That said, for some limited usecases, like windowing over N events,
>> can
>> > be
>> > > implemented using custom RDDs like SlidingRDD
>> > > <
>> > >
>> >
>> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apa=
che/spark/mllib/rdd/SlidingRDD.scala
>> > > >
>> > > without
>> > > losing parallelism. For things like app time based windows, and
>> > > random-application-event based windows, its much harder.
>> > >
>> > > Interesting ideas nonetheless. I am curious to see how far we can pu=
sh
>> > > using the RDD model underneath, without losing parallelism and
>> > performance.
>> > >
>> > > TD
>> > >
>> > >
>> > >
>> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
>> andy.petrella@gmail.com
>> > >
>> > > wrote:
>> > >
>> > > > Dear Sparkers,
>> > > >
>> > > > *[sorry for the lengthy email... =3D> head to the gist
>> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
>> > > preview
>> > > > :-p**]*
>> > > >
>> > > > I would like to share some thinking I had due to a use case I face=
d.
>> > > > Basically, as the subject announced it, it's a generalization of t=
he
>> > > > DStream currently available in the streaming project.
>> > > > First of all, I'd like to say that it's only a result of some
>> personal
>> > > > thinking, alone in the dark with a use case, the spark code, a
>> sheet of
>> > > > paper and a poor pen.
>> > > >
>> > > >
>> > > > DStream is a very great concept to deal with micro-batching use
>> cases,
>> > > and
>> > > > it does it very well too!
>> > > > Also, it hardly relies on the elapsing time to create its internal
>> > > > micro-batches.
>> > > > However, there are similar use cases where we need micro-batches
>> where
>> > > this
>> > > > constraint on the time doesn't hold, here are two of them:
>> > > > * a micro-batch has to be created every *n* events received
>> > > > * a micro-batch has to be generate based on the values of the item=
s
>> > > pushed
>> > > > by the source (which might even not be a stream!).
>> > > >
>> > > > An example of use case (mine ^^) would be
>> > > > * the creation of timeseries from a cold source containing
>> timestamped
>> > > > events (like S3).
>> > > > * one these timeseries have cells being the mean (sum, count, ...)
>> of
>> > one
>> > > > of the fields of the event
>> > > > * the mean has to be computed over a window depending on a field
>> > > > *timestamp*.
>> > > >
>> > > > * a timeserie is created for each type of event (the number of
>> types is
>> > > > high)
>> > > > So, in this case, it'd be interesting to have an RDD for each cell=
,
>> > which
>> > > > will generate all cells for all neede timeseries.
>> > > > It's more or less what DStream does, but here it won't help due wh=
at
>> > was
>> > > > stated above.
>> > > >
>> > > > That's how I came to a raw sketch of what could be named
>> ContinuousRDD
>> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
>> > > simplicity
>> > > > I've stuck with the definition of a DStream to think about it. Oka=
y,
>> > > let's
>> > > > go ^^.
>> > > >
>> > > >
>> > > > Looking at the DStream contract, here is something that could be
>> > drafted
>> > > > around CRDD.
>> > > > A *CRDD* would be a generalized concept that relies on:
>> > > > * a reference space/continuum (to which data can be bound)
>> > > > * a binning function that can breaks the continuum into splits.
>> > > > Since *Space* is a continuum we could define it as:
>> > > > * a *SpacePoint* (the origin)
>> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
>> > > > * a Ordering[SpacePoint]
>> > > >
>> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
>> > using
>> > > > timer and clock to do their work, in the case of a CRDD we'll have
>> to
>> > > > define also a point generator, as a more generic but also adaptabl=
e
>> > > > concept.
>> > > >
>> > > >
>> > > > So far (so good?), these definition should work quite fine for
>> > *ordered*
>> > > > space
>> > > > for which:
>> > > > * points are coming/fetched in order
>> > > > * the space is fully filled (no gaps)
>> > > > For these cases, the JobGenerator (f.i.) could be defined with two
>> > extra
>> > > > functions:
>> > > > * one is responsible to chop the batches even if the upper bound o=
f
>> the
>> > > > batch hasn't been seen yet
>> > > > * the other is responsible to handle outliers (and could wrap them
>> into
>> > > yet
>> > > > another CRDD ?)
>> > > >
>> > > >
>> > > > I created a gist here wrapping up the types and thus the skeleton =
of
>> > this
>> > > > idea, you can find it here:
>> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
>> > > >
>> > > > WDYT?
>> > > > *The answer can be: you're a fool!*
>> > > > Actually, I already I am, but also I like to know why.... so some
>> > > > explanations will help me :-D.
>> > > >
>> > > > Thanks to read 'till this point.
>> > > >
>> > > > Greetz,
>> > > >
>> > > >
>> > > >
>> > > >  a=E2=84=95dy =E2=84=99etrella
>> > > > about.me/noootsab
>> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>> > > >
>> > > > <http://about.me/noootsab>
>> > > >
>> > >
>> >
>>
>
>

--001a11349906ce048e04ff942892--

From dev-return-8661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 16:49:00 2014
Return-Path: <dev-return-8661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C2A611126
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 16:49:00 +0000 (UTC)
Received: (qmail 74251 invoked by uid 500); 1 Aug 2014 16:48:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74189 invoked by uid 500); 1 Aug 2014 16:48:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73652 invoked by uid 99); 1 Aug 2014 16:48:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 16:48:59 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 16:48:56 +0000
Received: by mail-la0-f48.google.com with SMTP id gl10so3387664lab.7
        for <dev@spark.incubator.apache.org>; Fri, 01 Aug 2014 09:48:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=khdmDtIkjdeb5Po6FhWFYr72EwXP4tzMdQn1zfM8ZDM=;
        b=PR1j8whFJ/cL+NpVTqIi6/7S/l211kI11vPvbCNnOOqUysXb9Zq79SlmLkGKswClB2
         ORN+BytMuLJJrTI93GXr/93h07A/M+bMvKEar9bTek60+0NWRc24OxU9l3O4RLQG7Mvp
         EXx+1WCZ2H0AnHwrwqkRhp7YQpAqBm1rHXcMLiCjRQHKMpyruU1ihGPMNpHuBVVHLvl7
         bcsRVtqmnqRZN+pM6MvBYY9dfWwnZVSb3V9tZTCBU7pBhIb7oIf8j/yOeOxA6jsNI8IM
         HjPO0lvuFoyMFKM3RKRBIp5GHYVKSsMKQz7Gsim2UvonfwYyHkb/vcfPqpKpj44cZeRp
         gbww==
X-Received: by 10.152.179.137 with SMTP id dg9mr7571544lac.11.1406911711673;
 Fri, 01 Aug 2014 09:48:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Fri, 1 Aug 2014 09:48:11 -0700 (PDT)
In-Reply-To: <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
 <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com> <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Fri, 1 Aug 2014 18:48:11 +0200
Message-ID: <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11349906ce048e04ff942892
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11349906ce048e04ff942892
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Heya,
Dunno if these ideas are still in the air or felt in the warp ^^.
However there is a paper on avocado
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/p=
roject8_report.pdf>
that
mentions a way of working with their data (sequence's reads) in a windowed
manner without neither time nor timestamp field's value, but a kind-of
internal index as range delimiter -- thus defining their own exotic
continuum and break function.

greetz,

 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Thu, Jul 17, 2014 at 1:11 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Indeed, these two cases are tightly coupled (the first one is a special
> case of the second).
>
> Actually, these "outliers" could be handled by a dedicated function what =
I
> named outliersManager -- I was not so much inspired ^^, but we could name
> these outliers, "outlaws" and thus the function would be "sheriff".
> The purpose of this "sheriff" function would be to create yet another
> distributed collection (RDD, CRDD, ...?) with only the --outliers-- outla=
ws
> in it.
>
> Because these problems have a nature which will be as different as the us=
e
> case will be, it's hard to find a generic way to tackle them. So, you
> know... that's why... I put temporarily them in jail and wait for the jud=
ge
> to show them the right path! (.... okay it's late in Belgium -- 1AM).
>
> All in all, it's more or less what we would do in DStream as well actuall=
y.
> Let me expand a bit this reasoning, let's assume that some data points ca=
n
> come along with the time, but aren't in sync with it -- f.i., a device th=
at
> wakes up and send all it's data at once.
> The DStream will package them into RDDs mixed-up with true current data
> points, however, the logic of the job will have to use a 'Y' road :
> * to integrate them into a database at the right place
> * to simply drop them out because they're won't be part of a shown chart
> * etc
>
> In this case, the 'Y' road would be of the contract ;-), and so left at
> the appreciation of the dev.
>
> Another way, to do it would be to ignore but log them, but that would be
> very crappy, non professional and useful (and of course I'm just kidding)=
.
>
> my0.002=C2=A2
>
>
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <
> tathagata.das1565@gmail.com> wrote:
>
>> I think it makes sense, though without a concrete implementation its har=
d
>> to be sure. Applying sorting on the RDD according to the RDDs makes sens=
e,
>> but I can think of two kinds of fundamental problems.
>>
>> 1. How do you deal with ordering across RDD boundaries. Say two
>> consecutive
>> RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2,
>> 3,
>> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function throu=
gh
>> all these records in the timestamp order. I am curious to find how this
>> problem can be solved without sacrificing efficiency (e.g. I can imagine
>> doing multiple pass magic)
>>
>> 2. An even more fundamental question is how do you ensure ordering with
>> delayed records. If you want to process in order of application time, an=
d
>> records are delayed how do you deal with them.
>>
>> Any ideas? ;)
>>
>> TD
>>
>>
>>
>> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
>> wrote:
>>
>> > Heya TD,
>> >
>> > Thanks for the detailed answer! Much appreciated.
>> >
>> > Regarding order among elements within an RDD, you're definitively righ=
t,
>> > it'd kill the //ism and would require synchronization which is
>> completely
>> > avoided in distributed env.
>> >
>> > That's why, I won't push this constraint to the RDDs themselves
>> actually,
>> > only the Space is something that *defines* ordered elements, and thus
>> there
>> > are two functions that will break the RDDs based on a given (extensibl=
e,
>> > plugable) heuristic f.i.
>> > Since the Space is rather decoupled from the data, thus the source and
>> the
>> > partitions, it's the responsibility of the CRRD implementation to
>> dictate
>> > how (if necessary) the elements should be sorted in the RDDs... which
>> will
>> > require some shuffles :-s -- Or the couple (source, space) is somethin=
g
>> > intrinsically ordered (like it is for DStream).
>> >
>> > To be more concrete an RDD would be composed of un-ordered iterator of
>> > millions of events for which all timestamps land into the same time
>> > interval.
>> >
>> > WDYT, would that makes sense?
>> >
>> > thanks again for the answer!
>> >
>> > greetz
>> >
>> >  a=E2=84=95dy =E2=84=99etrella
>> > about.me/noootsab
>> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>> >
>> > <http://about.me/noootsab>
>> >
>> >
>> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
>> > tathagata.das1565@gmail.com
>> > > wrote:
>> >
>> > > Very interesting ideas Andy!
>> > >
>> > > Conceptually i think it makes sense. In fact, it is true that dealin=
g
>> > with
>> > > time series data, windowing over application time, windowing over
>> number
>> > of
>> > > events, are things that DStream does not natively support. The real
>> > > challenge is actually mapping the conceptual windows with the
>> underlying
>> > > RDD model. On aspect you correctly observed in the ordering of event=
s
>> > > within the RDDs of the DStream. Another fundamental aspect is the fa=
ct
>> > that
>> > > RDDs as parallel collections, with no well-defined ordering in the
>> > records
>> > > in the RDDs. If you want to process the records in an RDD as a order=
ed
>> > > stream of events, you kind of have to process the stream sequentiall=
y,
>> > > which means you have to process each RDD partition one-by-one, and
>> > > therefore lose the parallelism. So implementing all these
>> functionality
>> > may
>> > > mean adding functionality at the cost of performance. Whether that i=
s
>> > okay
>> > > for Spark Streaming to have these OR this tradeoff is not-intuitive
>> for
>> > > end-users and therefore should not come out-of-the-box with Spark
>> > Streaming
>> > > -- that is a definitely a question worth debating upon.
>> > >
>> > > That said, for some limited usecases, like windowing over N events,
>> can
>> > be
>> > > implemented using custom RDDs like SlidingRDD
>> > > <
>> > >
>> >
>> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apa=
che/spark/mllib/rdd/SlidingRDD.scala
>> > > >
>> > > without
>> > > losing parallelism. For things like app time based windows, and
>> > > random-application-event based windows, its much harder.
>> > >
>> > > Interesting ideas nonetheless. I am curious to see how far we can pu=
sh
>> > > using the RDD model underneath, without losing parallelism and
>> > performance.
>> > >
>> > > TD
>> > >
>> > >
>> > >
>> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
>> andy.petrella@gmail.com
>> > >
>> > > wrote:
>> > >
>> > > > Dear Sparkers,
>> > > >
>> > > > *[sorry for the lengthy email... =3D> head to the gist
>> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
>> > > preview
>> > > > :-p**]*
>> > > >
>> > > > I would like to share some thinking I had due to a use case I face=
d.
>> > > > Basically, as the subject announced it, it's a generalization of t=
he
>> > > > DStream currently available in the streaming project.
>> > > > First of all, I'd like to say that it's only a result of some
>> personal
>> > > > thinking, alone in the dark with a use case, the spark code, a
>> sheet of
>> > > > paper and a poor pen.
>> > > >
>> > > >
>> > > > DStream is a very great concept to deal with micro-batching use
>> cases,
>> > > and
>> > > > it does it very well too!
>> > > > Also, it hardly relies on the elapsing time to create its internal
>> > > > micro-batches.
>> > > > However, there are similar use cases where we need micro-batches
>> where
>> > > this
>> > > > constraint on the time doesn't hold, here are two of them:
>> > > > * a micro-batch has to be created every *n* events received
>> > > > * a micro-batch has to be generate based on the values of the item=
s
>> > > pushed
>> > > > by the source (which might even not be a stream!).
>> > > >
>> > > > An example of use case (mine ^^) would be
>> > > > * the creation of timeseries from a cold source containing
>> timestamped
>> > > > events (like S3).
>> > > > * one these timeseries have cells being the mean (sum, count, ...)
>> of
>> > one
>> > > > of the fields of the event
>> > > > * the mean has to be computed over a window depending on a field
>> > > > *timestamp*.
>> > > >
>> > > > * a timeserie is created for each type of event (the number of
>> types is
>> > > > high)
>> > > > So, in this case, it'd be interesting to have an RDD for each cell=
,
>> > which
>> > > > will generate all cells for all neede timeseries.
>> > > > It's more or less what DStream does, but here it won't help due wh=
at
>> > was
>> > > > stated above.
>> > > >
>> > > > That's how I came to a raw sketch of what could be named
>> ContinuousRDD
>> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
>> > > simplicity
>> > > > I've stuck with the definition of a DStream to think about it. Oka=
y,
>> > > let's
>> > > > go ^^.
>> > > >
>> > > >
>> > > > Looking at the DStream contract, here is something that could be
>> > drafted
>> > > > around CRDD.
>> > > > A *CRDD* would be a generalized concept that relies on:
>> > > > * a reference space/continuum (to which data can be bound)
>> > > > * a binning function that can breaks the continuum into splits.
>> > > > Since *Space* is a continuum we could define it as:
>> > > > * a *SpacePoint* (the origin)
>> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
>> > > > * a Ordering[SpacePoint]
>> > > >
>> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
>> > using
>> > > > timer and clock to do their work, in the case of a CRDD we'll have
>> to
>> > > > define also a point generator, as a more generic but also adaptabl=
e
>> > > > concept.
>> > > >
>> > > >
>> > > > So far (so good?), these definition should work quite fine for
>> > *ordered*
>> > > > space
>> > > > for which:
>> > > > * points are coming/fetched in order
>> > > > * the space is fully filled (no gaps)
>> > > > For these cases, the JobGenerator (f.i.) could be defined with two
>> > extra
>> > > > functions:
>> > > > * one is responsible to chop the batches even if the upper bound o=
f
>> the
>> > > > batch hasn't been seen yet
>> > > > * the other is responsible to handle outliers (and could wrap them
>> into
>> > > yet
>> > > > another CRDD ?)
>> > > >
>> > > >
>> > > > I created a gist here wrapping up the types and thus the skeleton =
of
>> > this
>> > > > idea, you can find it here:
>> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
>> > > >
>> > > > WDYT?
>> > > > *The answer can be: you're a fool!*
>> > > > Actually, I already I am, but also I like to know why.... so some
>> > > > explanations will help me :-D.
>> > > >
>> > > > Thanks to read 'till this point.
>> > > >
>> > > > Greetz,
>> > > >
>> > > >
>> > > >
>> > > >  a=E2=84=95dy =E2=84=99etrella
>> > > > about.me/noootsab
>> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>> > > >
>> > > > <http://about.me/noootsab>
>> > > >
>> > >
>> >
>>
>
>

--001a11349906ce048e04ff942892--

From dev-return-8662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 17:37:46 2014
Return-Path: <dev-return-8662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7348F11428
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 17:37:46 +0000 (UTC)
Received: (qmail 19949 invoked by uid 500); 1 Aug 2014 17:37:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19893 invoked by uid 500); 1 Aug 2014 17:37:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19882 invoked by uid 99); 1 Aug 2014 17:37:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 17:37:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 17:37:41 +0000
Received: by mail-qa0-f44.google.com with SMTP id f12so4354053qad.3
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 10:37:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=WHlnxGdarzfiOufbgZtE2o2GtVaLWAeClUewjO/7dI4=;
        b=NhCOwSI6bm/RxLUlPL6lZtyHgKBw7wkP1cq60srN85Kuq9qWT+ubrr/T9Z3EVnHkmM
         aQaBAiBHHllcaP1640KQZlxqsj2MZUAL+F6yodZmYadmrSUPrG6STH67CSXE4L7pFIkV
         wL1y0LrF9wrlSUefyxY5aY0HOEgzvkeVzrYaQ7BS241715Q/wUyL+kNztWphSgFY50F+
         45Ne2+5MqFxFZH2Cy8XtCbhhosnpcQFCPDHkO0jqfo3TplNx4lGgr9YjHB0erwDJEuol
         nUUrgk1LHooZesj5kmg2CTEtpFV090RPqsW+QntXDKMal1CJDX2FEUB83qL8jrBgwPo4
         FFMw==
X-Gm-Message-State: ALoCoQmOAI92VNPmDbqqjH8cZTPnaSrpwLBlkzVWo/tRl3lcrqVQ6U7btU0afcVzEXVI5hQjxXIo
X-Received: by 10.140.86.147 with SMTP id p19mr10969707qgd.66.1406914640104;
 Fri, 01 Aug 2014 10:37:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.16.3 with HTTP; Fri, 1 Aug 2014 10:36:59 -0700 (PDT)
In-Reply-To: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com>
References: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 1 Aug 2014 10:36:59 -0700
Message-ID: <CAAswR-7r=rZ8LhDLvhELXNyFGqDN90b8-YiCuw7dDBb_UR-HVw@mail.gmail.com>
Subject: Re: How to run specific sparkSQL test with maven
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c127505a6efe04ff94d727
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c127505a6efe04ff94d727
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

>
> It seems that the HiveCompatibilitySuite need a hadoop and hive
> environment, am I right?
>
> "Relative path in absolute URI:
> file:$%7Bsystem:test.tmp.dir%7D/tmp_showcrt1=E2=80=9D
>

You should only need Hadoop and Hive if you are creating new tests that we
need to compute the answers for.  Existing tests are run with cached
answers.  There are details about the configuration here:
https://github.com/apache/spark/tree/master/sql

--001a11c127505a6efe04ff94d727--

From dev-return-8663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 18:07:57 2014
Return-Path: <dev-return-8663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9770115C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 18:07:56 +0000 (UTC)
Received: (qmail 31600 invoked by uid 500); 1 Aug 2014 18:07:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31540 invoked by uid 500); 1 Aug 2014 18:07:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31523 invoked by uid 99); 1 Aug 2014 18:07:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 18:07:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rajiv.abraham@gmail.com designates 209.85.217.171 as permitted sender)
Received: from [209.85.217.171] (HELO mail-lb0-f171.google.com) (209.85.217.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 18:07:50 +0000
Received: by mail-lb0-f171.google.com with SMTP id l4so3467977lbv.16
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 11:07:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=ohfIa5hQ74XZufcVLWUIg4PiTQcj19tDwzcF6BEoFSI=;
        b=z944IM9/MCNtS80SD2SGSqXTBXjRVnwb/rC1np/ROlxICgYPsXdTzxDMDzBG0ytv5V
         250kTdFx1qw4PPcVUiG+/iQ1Cim1CNTBfwluiOIJd3gYTLYzMzzkZvoGBUo7PCVAsxJj
         Mp4I/Tlv0eZpmSpm49wtmGrlWVmvpZPhMeZXPlziFAp5yNFoO8lfLFWLp0mmCzYHxyPy
         I/uSFyX5SLf7OMZ/DCyFMDPvEq3FVaIpJ6KEPrpCP7QCPjCyX8xgPlTtIYY9LkzeH5ca
         ial4alLlEs7ewQ7SktZqAaKa9Nq5H9nlH/URT+WkxkgvFv89QsKq1u43rWzGNSoP/Hfn
         pG5Q==
X-Received: by 10.112.167.67 with SMTP id zm3mr7613166lbb.27.1406916448667;
 Fri, 01 Aug 2014 11:07:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.254.98 with HTTP; Fri, 1 Aug 2014 11:06:58 -0700 (PDT)
From: Rajiv Abraham <rajiv.abraham@gmail.com>
Date: Fri, 1 Aug 2014 14:06:58 -0400
Message-ID: <CADnDY-X8BpPnX0D7cV_zM9ewiuFNw6eZBuEhrxfpsxketew=fw@mail.gmail.com>
Subject: Interested in contributing to GraphX in Python
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c38b8c26ce1804ff9543c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38b8c26ce1804ff9543c3
Content-Type: text/plain; charset=UTF-8

Hi,
I just saw Ankur's GraphX presentation and it looks very exciting! I would
like to contribute to a Python version of GraphX. I checked out JIRA and
Github but I did not find much info.

- Are there limitations currently to port GraphX in Python? (e.g. Maybe the
Python Spark RDD API is incomplete or not refactored for GraphX as compared
to the Scala version)
- If I had to start, could  I take inspiration from the Scala version and
try to emulate it in Python?
- Otherwise any suggestions of  starter tasks regarding GraphX in Python
would be appreciated



-- 
Take care,
Rajiv

--001a11c38b8c26ce1804ff9543c3--

From dev-return-8664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 18:49:20 2014
Return-Path: <dev-return-8664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDC4F117E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 18:49:19 +0000 (UTC)
Received: (qmail 42296 invoked by uid 500); 1 Aug 2014 18:49:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42219 invoked by uid 500); 1 Aug 2014 18:49:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 79681 invoked by uid 99); 1 Aug 2014 08:14:32 -0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhonghui.jin@intel.com designates 134.134.136.24 as permitted sender)
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.01,778,1400050800"; 
   d="scan'208,217";a="552254284"
From: "Jin, Zhonghui" <zhonghui.jin@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: "tgraves@apache.org" <tgraves@apache.org>, "Jin, Zhonghui"
	<zhonghui.jin@intel.com>
Subject: My Spark application had huge performance refression after Spark
 git commit: 0441515f221146756800dc583b225bdec8a6c075
Thread-Topic: My Spark application had huge performance refression after
 Spark git commit: 0441515f221146756800dc583b225bdec8a6c075
Thread-Index: Ac+tYBBvUyUfyDbGTRKsMCnoh5GU+A==
Date: Fri, 1 Aug 2014 08:14:02 +0000
Message-ID: <3563F066C74A764489EC361153362A8E010A2406@SHSMSX103.ccr.corp.intel.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_3563F066C74A764489EC361153362A8E010A2406SHSMSX103ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_3563F066C74A764489EC361153362A8E010A2406SHSMSX103ccrcor_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable


I found huge performance regression ( 1/20 of original) of my application a=
fter Spark git commit: 0441515f221146756800dc583b225bdec8a6c075.

Apply the following patch, will fix my issue:

diff --git a/core/src/main/scala/org/apache/spark/executor/Executor.scala b=
/core/src/main/scala/org/apache/spark/executor/Executor.scala
index 214a8c8..ebec21d 100644
--- a/core/src/main/scala/org/apache/spark/executor/Executor.scala
+++ b/core/src/main/scala/org/apache/spark/executor/Executor.scala
@@ -145,7 +145,7 @@ private[spark] class Executor(
       }
     }
-    override def run() {
+    override def run() : Unit =3D SparkHadoopUtil.get.runAsSparkUser { () =
=3D>
       val startTime =3D System.currentTimeMillis()
       SparkEnv.set(env)
       Thread.currentThread.setContextClassLoader(replClassLoader)

In the runAsSparkUser will call the 'UserGroupInformation.doAs()' to execut=
e the task and my application running OK;
if not through it, the performance was very poor. Application hotspot was J=
NIHandleBlock::alloc_handle (JVM code, very high CPI (cycles per instructio=
n, < 1 is OK) > 10)

My application passed large array data (>80K length) to native C code throu=
gh JNI.

Why the "UserGroupInformation.doAs()" great impacted the performance under =
this situation?


Thanks,
Zhonghui


--_000_3563F066C74A764489EC361153362A8E010A2406SHSMSX103ccrcor_--

From dev-return-8665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 21:46:13 2014
Return-Path: <dev-return-8665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F2F0511F75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 21:46:13 +0000 (UTC)
Received: (qmail 62260 invoked by uid 500); 1 Aug 2014 21:46:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62199 invoked by uid 500); 1 Aug 2014 21:46:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62188 invoked by uid 99); 1 Aug 2014 21:46:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 21:46:12 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 21:46:07 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so7652563vcb.23
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 14:45:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=AqUb2GYj+FnWwF9F2F+KKBiUnzZw0MskJM1DnlYpYpw=;
        b=DFSDsFAdpgvsbDanNv0bpenXGkwhB8C2BQdYDkH/nK9Dxzb78hIdrCcuicnk0xW9O+
         s5yT5mjEOL0jJbXxXBoUWLzJFemaRubsktn1O0+qhhc7uKBGhlXWQlcOOytFE6LPhcEi
         TaDd74wi+NMO+3hFg0rNwwDXLD/5SrNe/a08vhFwQsdKG7c+LoNIpyJPbYJ/6Hu+T5yt
         A9Z3+wcBax01HiH+BmSEDroy3ceMOEOX+COH7IIK3wywjPhn9v9IhUxsptXopRfaw/A5
         ML/4HYmBtFq0L919tm4agmC4kgptZDcDkVCMPJ/LC2Zr3YeJclY0nuW4wZa+t5TVOJLt
         NVnA==
X-Gm-Message-State: ALoCoQk9C0ukQHXXfxzhvTMoDx5PwcKsNaEWH3dMTIeqXK3NYUukxlYYnqmppkLvC5JVbSqnggvu
X-Received: by 10.221.25.210 with SMTP id rj18mr10025773vcb.5.1406929546647;
        Fri, 01 Aug 2014 14:45:46 -0700 (PDT)
Received: from mail-vc0-f170.google.com (mail-vc0-f170.google.com [209.85.220.170])
        by mx.google.com with ESMTPSA id td9sm7657937veb.8.2014.08.01.14.45.45
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 01 Aug 2014 14:45:45 -0700 (PDT)
Received: by mail-vc0-f170.google.com with SMTP id lf12so7709200vcb.29
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 14:45:44 -0700 (PDT)
X-Received: by 10.220.59.65 with SMTP id k1mr9893739vch.22.1406929544837; Fri,
 01 Aug 2014 14:45:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Fri, 1 Aug 2014 14:45:24 -0700 (PDT)
In-Reply-To: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
References: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Fri, 1 Aug 2014 17:45:24 -0400
Message-ID: <CA+-p3AEOuzkC2+0mFG2Y6nVaVn=WfyCOVXK-V9t=qesMAY8Siw@mail.gmail.com>
Subject: Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c29502be816e04ff984f74
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29502be816e04ff984f74
Content-Type: text/plain; charset=UTF-8

After several days of debugging, we think the issue is that we have
conflicting versions of Guava.  Our application was running with Guava 14
and the Spark services (Master, Workers, Executors) had Guava 16.  We had
custom Kryo serializers for Guava's ImmutableLists, and commenting out
those register calls did the trick.

Have people had issues with Guava version mismatches in the past?

I've found @srowen's Guava 14 -> 11 downgrade PR here
https://github.com/apache/spark/pull/1610 and some extended discussion on
https://issues.apache.org/jira/browse/SPARK-2420 for Hive compatibility


On Thu, Jul 31, 2014 at 10:47 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi everyone,
>
> I'm seeing the below exception coming out of Spark 1.0.1 when I call it
> from my application.  I can't share the source to that application, but the
> quick gist is that it uses Spark's Java APIs to read from Avro files in
> HDFS, do processing, and write back to Avro files.  It does this by
> receiving a REST call, then spinning up a new JVM as the driver application
> that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and also
> speculation.  The cluster is running in standalone mode on a 6 node cluster
> in AWS (not using Spark's EC2 scripts though).
>
> The below stacktraces are reliably reproduceable on every run of the job.
>  The issue seems to be that on deserialization of a task result on the
> driver, Kryo spits up while reading the ClassManifest.
>
> I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
> backcompat issues) but had the same error.
>
> Any ideas on what can be done here?
>
> Thanks!
> Andrew
>
>
>
> In the driver (Kryo exception while deserializing a DirectTaskResult):
>
> INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
> thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
> getting task result
> INFO   | jvm 1    | 2014/07/30 20:52:52 |
> com.esotericsoftware.kryo.KryoException: Buffer underflow.
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.esotericsoftware.kryo.io.Input.require(Input.java:156)
> ~[kryo-2.21.jar:na]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
> ~[kryo-2.21.jar:na]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
> ~[kryo-2.21.jar:na]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624) ~[kryo-2.21.jar:na]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
> ~[chill_2.10-0.3.6.jar:0.3.6]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
> ~[chill_2.10-0.3.6.jar:0.3.6]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
> ~[kryo-2.21.jar:na]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
> [spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> [spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> [spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
> [spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
> [spark-core_2.10-1.0.1.jar:1.0.1]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> [na:1.7.0_65]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> [na:1.7.0_65]
> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
>
>
> In the DAGScheduler (job gets aborted):
>
> org.apache.spark.SparkException: Job aborted due to stage failure:
> Exception while getting task result:
> com.esotericsoftware.kryo.KryoException: Buffer underflow.
>     at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>     at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>     at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>     at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>     at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>     at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>     at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>     at scala.Option.foreach(Option.scala:236)
>     at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>     at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>     at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>     at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>     at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>     at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> In an Executor (running tasks get killed):
>
> 14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading broadcast
> variable 0
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 153
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 147
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 141
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 135
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 150
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 144
> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
> 138
> 14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733) called
> with curMem=0, maxMem=30870601728
> 14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
> values to memory (estimated size 236.1 KB, free 28.8 GB)
> 14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast variable
> 0 took 0.91790748 s
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> locally
> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
> org.apache.spark.TaskKilledException
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
> org.apache.spark.TaskKilledException
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
> org.apache.spark.TaskKilledException
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
> org.apache.spark.TaskKilledException
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
> org.apache.spark.TaskKilledException
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
>

--001a11c29502be816e04ff984f74--

From dev-return-8666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 22:28:22 2014
Return-Path: <dev-return-8666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7E9D111B5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 22:28:22 +0000 (UTC)
Received: (qmail 13818 invoked by uid 500); 1 Aug 2014 22:28:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13693 invoked by uid 500); 1 Aug 2014 22:28:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12628 invoked by uid 99); 1 Aug 2014 22:28:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:28:20 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.47 as permitted sender)
Received: from [209.85.219.47] (HELO mail-oa0-f47.google.com) (209.85.219.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:28:16 +0000
Received: by mail-oa0-f47.google.com with SMTP id g18so3490272oah.20
        for <multiple recipients>; Fri, 01 Aug 2014 15:27:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=oK07CqTjrTiQ8rY0oheGylS67Qb6KH8hipKIliZMEp0=;
        b=0pZyRv5rPWVBYS5aSywHt4COHO/y1L7jmB/u2j811OsDIc/pAp9IQZJ4T4l1p4WReG
         ZCPUj7+qSWyP/zuT8fsc03M/fcF3EBQAaRtshANW7XF2f1ey4+wtXi4GzE/8trf+4jSu
         6QG8zyaM97pgMeLeiMknxzvX5UZrmJEq2wSVeoZjTDWW9Rk8JlsW85f6QLBpqBmuRLXt
         /O8yp/G2vFK1bbObBsIpn3T+fu5ogc5Zj1s0FvT4Ng9nUvOaeIiQkwZukripo4ZYvzBB
         zOwYwQmTsRcgGlefiXKyNSoRn/uh2FK8dd4MXIABrYbtzfyt0KBlOr8IEqxpLReApSsz
         Oeaw==
MIME-Version: 1.0
X-Received: by 10.60.46.167 with SMTP id w7mr12830102oem.50.1406932076007;
 Fri, 01 Aug 2014 15:27:56 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 1 Aug 2014 15:27:55 -0700 (PDT)
In-Reply-To: <CAJLcJd_3M0CSPmLP8fDpdPFxOv=3NFqM7wiutgRj+J5n12LLxw@mail.gmail.com>
References: <m2lhr85w52.fsf@gmail.com>
	<1406916953716-11189.post@n3.nabble.com>
	<CAJLcJd-LDEMkOAQn7h+aqSiKVwLuvsYVrvKZQ9HkMEV0G2ahaw@mail.gmail.com>
	<CAJLcJd_3M0CSPmLP8fDpdPFxOv=3NFqM7wiutgRj+J5n12LLxw@mail.gmail.com>
Date: Fri, 1 Aug 2014 15:27:55 -0700
Message-ID: <CABPQxsvOv1DztKbo_=DPbkN0a=86NAbiw2jbgrASG_jp7z97=w@mail.gmail.com>
Subject: Re: Compiling Spark master (284771ef) with sbt/sbt assembly fails on EC2
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e015371c49d177704ff98e6bb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015371c49d177704ff98e6bb
Content-Type: text/plain; charset=ISO-8859-1

This is a Scala bug - I filed something upstream, hopefully they can fix it
soon and/or we can provide a work around:

https://issues.scala-lang.org/browse/SI-8772

- Patrick


On Fri, Aug 1, 2014 at 3:15 PM, Holden Karau <holden@pigscanfly.ca> wrote:

> Currently scala 2.10.2 can't be pulled in from maven central it seems,
> however if you have it in your ivy cache it should work.
>
>
> On Fri, Aug 1, 2014 at 3:15 PM, Holden Karau <holden@pigscanfly.ca> wrote:
>
>> Me 3
>>
>>
>> On Fri, Aug 1, 2014 at 11:15 AM, nit <nitinpanj@gmail.com> wrote:
>>
>>> I also ran into same issue. What is the solution?
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://apache-spark-user-list.1001560.n3.nabble.com/Compiling-Spark-master-284771ef-with-sbt-sbt-assembly-fails-on-EC2-tp11155p11189.html
>>> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>>>
>>
>>
>>
>> --
>> Cell : 425-233-8271
>>
>
>
>
> --
> Cell : 425-233-8271
>

--089e015371c49d177704ff98e6bb--

From dev-return-8667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 22:30:42 2014
Return-Path: <dev-return-8667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02327111D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 22:30:42 +0000 (UTC)
Received: (qmail 29042 invoked by uid 500); 1 Aug 2014 22:30:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28990 invoked by uid 500); 1 Aug 2014 22:30:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28962 invoked by uid 99); 1 Aug 2014 22:30:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:30:40 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:30:39 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so2106449wib.9
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 15:30:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=04F7mRyWXlIf/3UAET81BOBtFPj/o/DHcVSq+bUsTHM=;
        b=E5DHYuzgAqvet/tIFNLmFXY2BqPlmHsJPHqxrOg5S6gqZmcLjbrRvzoRnwpi28oY/e
         SgfeW3kPAHx1mruIxPWdIxCuJDRZ3Snmme+T2BUvrf6dTKjJ5An214/rLEmCxpjaDiJt
         NHxaFLjy5A0pMfwc15DBtO1MavdhBfkH6AduiGPSKPsvArDStsPHCMA+4Nir49hxhl7O
         Rxrapq8O1Ob0plwa8f73fzN9tPkiy70hxbYKrKcWCfFgmUUymoDZ+Fxj86cvz7ZS/ba2
         wcBeg8M4LHyr12qQhyjG7RyRDOcPu4K5r31Fj3dB3L3dXwppa73hiHHWU5LzS9V/9n0v
         ziJg==
X-Gm-Message-State: ALoCoQkBHjqPSfK+ukDlXMF+EUvIeJCtEaV1hWLqKRPdvDhKyjnAN2je4yYGmFVA5hw6p99pUzza
MIME-Version: 1.0
X-Received: by 10.194.77.233 with SMTP id v9mr367579wjw.129.1406932213853;
 Fri, 01 Aug 2014 15:30:13 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.217.48.72 with HTTP; Fri, 1 Aug 2014 15:30:13 -0700 (PDT)
In-Reply-To: <CABPQxsvOv1DztKbo_=DPbkN0a=86NAbiw2jbgrASG_jp7z97=w@mail.gmail.com>
References: <m2lhr85w52.fsf@gmail.com>
	<1406916953716-11189.post@n3.nabble.com>
	<CAJLcJd-LDEMkOAQn7h+aqSiKVwLuvsYVrvKZQ9HkMEV0G2ahaw@mail.gmail.com>
	<CAJLcJd_3M0CSPmLP8fDpdPFxOv=3NFqM7wiutgRj+J5n12LLxw@mail.gmail.com>
	<CABPQxsvOv1DztKbo_=DPbkN0a=86NAbiw2jbgrASG_jp7z97=w@mail.gmail.com>
Date: Fri, 1 Aug 2014 15:30:13 -0700
Message-ID: <CAKx7Bf-wb=aDnf+=-idDKjNA-9XuoRwz=vrBx8nzR5xpp9R-MA@mail.gmail.com>
Subject: Re: Compiling Spark master (284771ef) with sbt/sbt assembly fails on EC2
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfced9ad48e6804ff98ee91
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfced9ad48e6804ff98ee91
Content-Type: text/plain; charset=UTF-8

Thanks Patrick -- It does look like some maven misconfiguration as

wget
http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.2/scala-library-2.10.2.pom

works for me.

Shivaram



On Fri, Aug 1, 2014 at 3:27 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> This is a Scala bug - I filed something upstream, hopefully they can fix
> it soon and/or we can provide a work around:
>
> https://issues.scala-lang.org/browse/SI-8772
>
> - Patrick
>
>
> On Fri, Aug 1, 2014 at 3:15 PM, Holden Karau <holden@pigscanfly.ca> wrote:
>
>> Currently scala 2.10.2 can't be pulled in from maven central it seems,
>> however if you have it in your ivy cache it should work.
>>
>>
>> On Fri, Aug 1, 2014 at 3:15 PM, Holden Karau <holden@pigscanfly.ca>
>> wrote:
>>
>>> Me 3
>>>
>>>
>>> On Fri, Aug 1, 2014 at 11:15 AM, nit <nitinpanj@gmail.com> wrote:
>>>
>>>> I also ran into same issue. What is the solution?
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://apache-spark-user-list.1001560.n3.nabble.com/Compiling-Spark-master-284771ef-with-sbt-sbt-assembly-fails-on-EC2-tp11155p11189.html
>>>> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>>>>
>>>
>>>
>>>
>>> --
>>> Cell : 425-233-8271
>>>
>>
>>
>>
>> --
>> Cell : 425-233-8271
>>
>
>

--047d7bfced9ad48e6804ff98ee91--

From dev-return-8668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 22:43:23 2014
Return-Path: <dev-return-8668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38D351125E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 22:43:23 +0000 (UTC)
Received: (qmail 58519 invoked by uid 500); 1 Aug 2014 22:43:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58457 invoked by uid 500); 1 Aug 2014 22:43:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58445 invoked by uid 99); 1 Aug 2014 22:43:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:43:22 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:43:18 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so4906129wgg.0
        for <dev@spark.incubator.apache.org>; Fri, 01 Aug 2014 15:42:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=0wU0MsUCh+QysUDEFhp0lWofE24SivqRKgZWurvL3Kg=;
        b=TYXXcvbiFS8UqnJz2ZKV+73yFFRpYNlB5i1YFY1QCY7eRf08BslxPCw1FzMKBp+W0P
         hJzvnFZdkbvHuwUjXOohekiKrADkWY1eLtH+9S6O/2ilqSmqA7eBM3U+/26YDYgiQC3Z
         n2TpkLYXqnm9X9VTWquytwc+Qkw6IJetdnSrEcCln3ry3/3VNXKDFqReFTSLdVsGAMpS
         dtIV7mI5/4Ow+lhEZaVn17k1mBP7FDk3xeOJOptXKRMYtVIRAcCyjQZA4jxcFkWU+jNP
         iHFjJ2xCQW7ICU2AMpR4EvkOFD5lZnifVws1GNhG8Zn7jxvwRUlvtIuEdH2FnA6qPw5j
         ikmA==
X-Received: by 10.194.158.226 with SMTP id wx2mr12536532wjb.107.1406932977060;
 Fri, 01 Aug 2014 15:42:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.153.10 with HTTP; Fri, 1 Aug 2014 15:42:36 -0700 (PDT)
In-Reply-To: <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
 <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
 <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com> <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Fri, 1 Aug 2014 15:42:36 -0700
Message-ID: <CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01184cc652121504ff991c95
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184cc652121504ff991c95
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Interesting, clickstream data would have its own window concept based on
session of User , I can imagine windows would change across streams but
wouldnt they large be domain specific in Nature?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Fri, Aug 1, 2014 at 9:48 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Heya,
> Dunno if these ideas are still in the air or felt in the warp ^^.
> However there is a paper on avocado
> <
> http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/=
project8_report.pdf
> >
> that
> mentions a way of working with their data (sequence's reads) in a windowe=
d
> manner without neither time nor timestamp field's value, but a kind-of
> internal index as range delimiter -- thus defining their own exotic
> continuum and break function.
>
> greetz,
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Thu, Jul 17, 2014 at 1:11 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Indeed, these two cases are tightly coupled (the first one is a special
> > case of the second).
> >
> > Actually, these "outliers" could be handled by a dedicated function wha=
t
> I
> > named outliersManager -- I was not so much inspired ^^, but we could na=
me
> > these outliers, "outlaws" and thus the function would be "sheriff".
> > The purpose of this "sheriff" function would be to create yet another
> > distributed collection (RDD, CRDD, ...?) with only the --outliers--
> outlaws
> > in it.
> >
> > Because these problems have a nature which will be as different as the
> use
> > case will be, it's hard to find a generic way to tackle them. So, you
> > know... that's why... I put temporarily them in jail and wait for the
> judge
> > to show them the right path! (.... okay it's late in Belgium -- 1AM).
> >
> > All in all, it's more or less what we would do in DStream as well
> actually.
> > Let me expand a bit this reasoning, let's assume that some data points
> can
> > come along with the time, but aren't in sync with it -- f.i., a device
> that
> > wakes up and send all it's data at once.
> > The DStream will package them into RDDs mixed-up with true current data
> > points, however, the logic of the job will have to use a 'Y' road :
> > * to integrate them into a database at the right place
> > * to simply drop them out because they're won't be part of a shown char=
t
> > * etc
> >
> > In this case, the 'Y' road would be of the contract ;-), and so left at
> > the appreciation of the dev.
> >
> > Another way, to do it would be to ignore but log them, but that would b=
e
> > very crappy, non professional and useful (and of course I'm just
> kidding).
> >
> > my0.002=C2=A2
> >
> >
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
> >
> > On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <
> > tathagata.das1565@gmail.com> wrote:
> >
> >> I think it makes sense, though without a concrete implementation its
> hard
> >> to be sure. Applying sorting on the RDD according to the RDDs makes
> sense,
> >> but I can think of two kinds of fundamental problems.
> >>
> >> 1. How do you deal with ordering across RDD boundaries. Say two
> >> consecutive
> >> RDDs in the DStream has the following record timestamps    RDD1: [ 1, =
2,
> >> 3,
> >> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function
> through
> >> all these records in the timestamp order. I am curious to find how thi=
s
> >> problem can be solved without sacrificing efficiency (e.g. I can imagi=
ne
> >> doing multiple pass magic)
> >>
> >> 2. An even more fundamental question is how do you ensure ordering wit=
h
> >> delayed records. If you want to process in order of application time,
> and
> >> records are delayed how do you deal with them.
> >>
> >> Any ideas? ;)
> >>
> >> TD
> >>
> >>
> >>
> >> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.co=
m
> >
> >> wrote:
> >>
> >> > Heya TD,
> >> >
> >> > Thanks for the detailed answer! Much appreciated.
> >> >
> >> > Regarding order among elements within an RDD, you're definitively
> right,
> >> > it'd kill the //ism and would require synchronization which is
> >> completely
> >> > avoided in distributed env.
> >> >
> >> > That's why, I won't push this constraint to the RDDs themselves
> >> actually,
> >> > only the Space is something that *defines* ordered elements, and thu=
s
> >> there
> >> > are two functions that will break the RDDs based on a given
> (extensible,
> >> > plugable) heuristic f.i.
> >> > Since the Space is rather decoupled from the data, thus the source a=
nd
> >> the
> >> > partitions, it's the responsibility of the CRRD implementation to
> >> dictate
> >> > how (if necessary) the elements should be sorted in the RDDs... whic=
h
> >> will
> >> > require some shuffles :-s -- Or the couple (source, space) is
> something
> >> > intrinsically ordered (like it is for DStream).
> >> >
> >> > To be more concrete an RDD would be composed of un-ordered iterator =
of
> >> > millions of events for which all timestamps land into the same time
> >> > interval.
> >> >
> >> > WDYT, would that makes sense?
> >> >
> >> > thanks again for the answer!
> >> >
> >> > greetz
> >> >
> >> >  a=E2=84=95dy =E2=84=99etrella
> >> > about.me/noootsab
> >> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >> >
> >> > <http://about.me/noootsab>
> >> >
> >> >
> >> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> >> > tathagata.das1565@gmail.com
> >> > > wrote:
> >> >
> >> > > Very interesting ideas Andy!
> >> > >
> >> > > Conceptually i think it makes sense. In fact, it is true that
> dealing
> >> > with
> >> > > time series data, windowing over application time, windowing over
> >> number
> >> > of
> >> > > events, are things that DStream does not natively support. The rea=
l
> >> > > challenge is actually mapping the conceptual windows with the
> >> underlying
> >> > > RDD model. On aspect you correctly observed in the ordering of
> events
> >> > > within the RDDs of the DStream. Another fundamental aspect is the
> fact
> >> > that
> >> > > RDDs as parallel collections, with no well-defined ordering in the
> >> > records
> >> > > in the RDDs. If you want to process the records in an RDD as a
> ordered
> >> > > stream of events, you kind of have to process the stream
> sequentially,
> >> > > which means you have to process each RDD partition one-by-one, and
> >> > > therefore lose the parallelism. So implementing all these
> >> functionality
> >> > may
> >> > > mean adding functionality at the cost of performance. Whether that
> is
> >> > okay
> >> > > for Spark Streaming to have these OR this tradeoff is not-intuitiv=
e
> >> for
> >> > > end-users and therefore should not come out-of-the-box with Spark
> >> > Streaming
> >> > > -- that is a definitely a question worth debating upon.
> >> > >
> >> > > That said, for some limited usecases, like windowing over N events=
,
> >> can
> >> > be
> >> > > implemented using custom RDDs like SlidingRDD
> >> > > <
> >> > >
> >> >
> >>
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> >> > > >
> >> > > without
> >> > > losing parallelism. For things like app time based windows, and
> >> > > random-application-event based windows, its much harder.
> >> > >
> >> > > Interesting ideas nonetheless. I am curious to see how far we can
> push
> >> > > using the RDD model underneath, without losing parallelism and
> >> > performance.
> >> > >
> >> > > TD
> >> > >
> >> > >
> >> > >
> >> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
> >> andy.petrella@gmail.com
> >> > >
> >> > > wrote:
> >> > >
> >> > > > Dear Sparkers,
> >> > > >
> >> > > > *[sorry for the lengthy email... =3D> head to the gist
> >> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for =
a
> >> > > preview
> >> > > > :-p**]*
> >> > > >
> >> > > > I would like to share some thinking I had due to a use case I
> faced.
> >> > > > Basically, as the subject announced it, it's a generalization of
> the
> >> > > > DStream currently available in the streaming project.
> >> > > > First of all, I'd like to say that it's only a result of some
> >> personal
> >> > > > thinking, alone in the dark with a use case, the spark code, a
> >> sheet of
> >> > > > paper and a poor pen.
> >> > > >
> >> > > >
> >> > > > DStream is a very great concept to deal with micro-batching use
> >> cases,
> >> > > and
> >> > > > it does it very well too!
> >> > > > Also, it hardly relies on the elapsing time to create its intern=
al
> >> > > > micro-batches.
> >> > > > However, there are similar use cases where we need micro-batches
> >> where
> >> > > this
> >> > > > constraint on the time doesn't hold, here are two of them:
> >> > > > * a micro-batch has to be created every *n* events received
> >> > > > * a micro-batch has to be generate based on the values of the
> items
> >> > > pushed
> >> > > > by the source (which might even not be a stream!).
> >> > > >
> >> > > > An example of use case (mine ^^) would be
> >> > > > * the creation of timeseries from a cold source containing
> >> timestamped
> >> > > > events (like S3).
> >> > > > * one these timeseries have cells being the mean (sum, count, ..=
.)
> >> of
> >> > one
> >> > > > of the fields of the event
> >> > > > * the mean has to be computed over a window depending on a field
> >> > > > *timestamp*.
> >> > > >
> >> > > > * a timeserie is created for each type of event (the number of
> >> types is
> >> > > > high)
> >> > > > So, in this case, it'd be interesting to have an RDD for each
> cell,
> >> > which
> >> > > > will generate all cells for all neede timeseries.
> >> > > > It's more or less what DStream does, but here it won't help due
> what
> >> > was
> >> > > > stated above.
> >> > > >
> >> > > > That's how I came to a raw sketch of what could be named
> >> ContinuousRDD
> >> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> >> > > simplicity
> >> > > > I've stuck with the definition of a DStream to think about it.
> Okay,
> >> > > let's
> >> > > > go ^^.
> >> > > >
> >> > > >
> >> > > > Looking at the DStream contract, here is something that could be
> >> > drafted
> >> > > > around CRDD.
> >> > > > A *CRDD* would be a generalized concept that relies on:
> >> > > > * a reference space/continuum (to which data can be bound)
> >> > > > * a binning function that can breaks the continuum into splits.
> >> > > > Since *Space* is a continuum we could define it as:
> >> > > > * a *SpacePoint* (the origin)
> >> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
> >> > > > * a Ordering[SpacePoint]
> >> > > >
> >> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which a=
re
> >> > using
> >> > > > timer and clock to do their work, in the case of a CRDD we'll ha=
ve
> >> to
> >> > > > define also a point generator, as a more generic but also
> adaptable
> >> > > > concept.
> >> > > >
> >> > > >
> >> > > > So far (so good?), these definition should work quite fine for
> >> > *ordered*
> >> > > > space
> >> > > > for which:
> >> > > > * points are coming/fetched in order
> >> > > > * the space is fully filled (no gaps)
> >> > > > For these cases, the JobGenerator (f.i.) could be defined with t=
wo
> >> > extra
> >> > > > functions:
> >> > > > * one is responsible to chop the batches even if the upper bound
> of
> >> the
> >> > > > batch hasn't been seen yet
> >> > > > * the other is responsible to handle outliers (and could wrap th=
em
> >> into
> >> > > yet
> >> > > > another CRDD ?)
> >> > > >
> >> > > >
> >> > > > I created a gist here wrapping up the types and thus the skeleto=
n
> of
> >> > this
> >> > > > idea, you can find it here:
> >> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> >> > > >
> >> > > > WDYT?
> >> > > > *The answer can be: you're a fool!*
> >> > > > Actually, I already I am, but also I like to know why.... so som=
e
> >> > > > explanations will help me :-D.
> >> > > >
> >> > > > Thanks to read 'till this point.
> >> > > >
> >> > > > Greetz,
> >> > > >
> >> > > >
> >> > > >
> >> > > >  a=E2=84=95dy =E2=84=99etrella
> >> > > > about.me/noootsab
> >> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >> > > >
> >> > > > <http://about.me/noootsab>
> >> > > >
> >> > >
> >> >
> >>
> >
> >
>

--089e01184cc652121504ff991c95--

From dev-return-8669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 22:43:25 2014
Return-Path: <dev-return-8669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A73611125F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 22:43:25 +0000 (UTC)
Received: (qmail 59541 invoked by uid 500); 1 Aug 2014 22:43:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59485 invoked by uid 500); 1 Aug 2014 22:43:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59473 invoked by uid 99); 1 Aug 2014 22:43:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:43:24 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 22:43:21 +0000
Received: by mail-we0-f179.google.com with SMTP id u57so5056182wes.10
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 15:42:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=0wU0MsUCh+QysUDEFhp0lWofE24SivqRKgZWurvL3Kg=;
        b=TYXXcvbiFS8UqnJz2ZKV+73yFFRpYNlB5i1YFY1QCY7eRf08BslxPCw1FzMKBp+W0P
         hJzvnFZdkbvHuwUjXOohekiKrADkWY1eLtH+9S6O/2ilqSmqA7eBM3U+/26YDYgiQC3Z
         n2TpkLYXqnm9X9VTWquytwc+Qkw6IJetdnSrEcCln3ry3/3VNXKDFqReFTSLdVsGAMpS
         dtIV7mI5/4Ow+lhEZaVn17k1mBP7FDk3xeOJOptXKRMYtVIRAcCyjQZA4jxcFkWU+jNP
         iHFjJ2xCQW7ICU2AMpR4EvkOFD5lZnifVws1GNhG8Zn7jxvwRUlvtIuEdH2FnA6qPw5j
         ikmA==
X-Received: by 10.194.158.226 with SMTP id wx2mr12536532wjb.107.1406932977060;
 Fri, 01 Aug 2014 15:42:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.153.10 with HTTP; Fri, 1 Aug 2014 15:42:36 -0700 (PDT)
In-Reply-To: <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
 <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
 <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com> <CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Fri, 1 Aug 2014 15:42:36 -0700
Message-ID: <CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01184cc652121504ff991c95
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184cc652121504ff991c95
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Interesting, clickstream data would have its own window concept based on
session of User , I can imagine windows would change across streams but
wouldnt they large be domain specific in Nature?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Fri, Aug 1, 2014 at 9:48 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Heya,
> Dunno if these ideas are still in the air or felt in the warp ^^.
> However there is a paper on avocado
> <
> http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/=
project8_report.pdf
> >
> that
> mentions a way of working with their data (sequence's reads) in a windowe=
d
> manner without neither time nor timestamp field's value, but a kind-of
> internal index as range delimiter -- thus defining their own exotic
> continuum and break function.
>
> greetz,
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Thu, Jul 17, 2014 at 1:11 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Indeed, these two cases are tightly coupled (the first one is a special
> > case of the second).
> >
> > Actually, these "outliers" could be handled by a dedicated function wha=
t
> I
> > named outliersManager -- I was not so much inspired ^^, but we could na=
me
> > these outliers, "outlaws" and thus the function would be "sheriff".
> > The purpose of this "sheriff" function would be to create yet another
> > distributed collection (RDD, CRDD, ...?) with only the --outliers--
> outlaws
> > in it.
> >
> > Because these problems have a nature which will be as different as the
> use
> > case will be, it's hard to find a generic way to tackle them. So, you
> > know... that's why... I put temporarily them in jail and wait for the
> judge
> > to show them the right path! (.... okay it's late in Belgium -- 1AM).
> >
> > All in all, it's more or less what we would do in DStream as well
> actually.
> > Let me expand a bit this reasoning, let's assume that some data points
> can
> > come along with the time, but aren't in sync with it -- f.i., a device
> that
> > wakes up and send all it's data at once.
> > The DStream will package them into RDDs mixed-up with true current data
> > points, however, the logic of the job will have to use a 'Y' road :
> > * to integrate them into a database at the right place
> > * to simply drop them out because they're won't be part of a shown char=
t
> > * etc
> >
> > In this case, the 'Y' road would be of the contract ;-), and so left at
> > the appreciation of the dev.
> >
> > Another way, to do it would be to ignore but log them, but that would b=
e
> > very crappy, non professional and useful (and of course I'm just
> kidding).
> >
> > my0.002=C2=A2
> >
> >
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
> >
> > On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <
> > tathagata.das1565@gmail.com> wrote:
> >
> >> I think it makes sense, though without a concrete implementation its
> hard
> >> to be sure. Applying sorting on the RDD according to the RDDs makes
> sense,
> >> but I can think of two kinds of fundamental problems.
> >>
> >> 1. How do you deal with ordering across RDD boundaries. Say two
> >> consecutive
> >> RDDs in the DStream has the following record timestamps    RDD1: [ 1, =
2,
> >> 3,
> >> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function
> through
> >> all these records in the timestamp order. I am curious to find how thi=
s
> >> problem can be solved without sacrificing efficiency (e.g. I can imagi=
ne
> >> doing multiple pass magic)
> >>
> >> 2. An even more fundamental question is how do you ensure ordering wit=
h
> >> delayed records. If you want to process in order of application time,
> and
> >> records are delayed how do you deal with them.
> >>
> >> Any ideas? ;)
> >>
> >> TD
> >>
> >>
> >>
> >> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.co=
m
> >
> >> wrote:
> >>
> >> > Heya TD,
> >> >
> >> > Thanks for the detailed answer! Much appreciated.
> >> >
> >> > Regarding order among elements within an RDD, you're definitively
> right,
> >> > it'd kill the //ism and would require synchronization which is
> >> completely
> >> > avoided in distributed env.
> >> >
> >> > That's why, I won't push this constraint to the RDDs themselves
> >> actually,
> >> > only the Space is something that *defines* ordered elements, and thu=
s
> >> there
> >> > are two functions that will break the RDDs based on a given
> (extensible,
> >> > plugable) heuristic f.i.
> >> > Since the Space is rather decoupled from the data, thus the source a=
nd
> >> the
> >> > partitions, it's the responsibility of the CRRD implementation to
> >> dictate
> >> > how (if necessary) the elements should be sorted in the RDDs... whic=
h
> >> will
> >> > require some shuffles :-s -- Or the couple (source, space) is
> something
> >> > intrinsically ordered (like it is for DStream).
> >> >
> >> > To be more concrete an RDD would be composed of un-ordered iterator =
of
> >> > millions of events for which all timestamps land into the same time
> >> > interval.
> >> >
> >> > WDYT, would that makes sense?
> >> >
> >> > thanks again for the answer!
> >> >
> >> > greetz
> >> >
> >> >  a=E2=84=95dy =E2=84=99etrella
> >> > about.me/noootsab
> >> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >> >
> >> > <http://about.me/noootsab>
> >> >
> >> >
> >> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> >> > tathagata.das1565@gmail.com
> >> > > wrote:
> >> >
> >> > > Very interesting ideas Andy!
> >> > >
> >> > > Conceptually i think it makes sense. In fact, it is true that
> dealing
> >> > with
> >> > > time series data, windowing over application time, windowing over
> >> number
> >> > of
> >> > > events, are things that DStream does not natively support. The rea=
l
> >> > > challenge is actually mapping the conceptual windows with the
> >> underlying
> >> > > RDD model. On aspect you correctly observed in the ordering of
> events
> >> > > within the RDDs of the DStream. Another fundamental aspect is the
> fact
> >> > that
> >> > > RDDs as parallel collections, with no well-defined ordering in the
> >> > records
> >> > > in the RDDs. If you want to process the records in an RDD as a
> ordered
> >> > > stream of events, you kind of have to process the stream
> sequentially,
> >> > > which means you have to process each RDD partition one-by-one, and
> >> > > therefore lose the parallelism. So implementing all these
> >> functionality
> >> > may
> >> > > mean adding functionality at the cost of performance. Whether that
> is
> >> > okay
> >> > > for Spark Streaming to have these OR this tradeoff is not-intuitiv=
e
> >> for
> >> > > end-users and therefore should not come out-of-the-box with Spark
> >> > Streaming
> >> > > -- that is a definitely a question worth debating upon.
> >> > >
> >> > > That said, for some limited usecases, like windowing over N events=
,
> >> can
> >> > be
> >> > > implemented using custom RDDs like SlidingRDD
> >> > > <
> >> > >
> >> >
> >>
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> >> > > >
> >> > > without
> >> > > losing parallelism. For things like app time based windows, and
> >> > > random-application-event based windows, its much harder.
> >> > >
> >> > > Interesting ideas nonetheless. I am curious to see how far we can
> push
> >> > > using the RDD model underneath, without losing parallelism and
> >> > performance.
> >> > >
> >> > > TD
> >> > >
> >> > >
> >> > >
> >> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
> >> andy.petrella@gmail.com
> >> > >
> >> > > wrote:
> >> > >
> >> > > > Dear Sparkers,
> >> > > >
> >> > > > *[sorry for the lengthy email... =3D> head to the gist
> >> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for =
a
> >> > > preview
> >> > > > :-p**]*
> >> > > >
> >> > > > I would like to share some thinking I had due to a use case I
> faced.
> >> > > > Basically, as the subject announced it, it's a generalization of
> the
> >> > > > DStream currently available in the streaming project.
> >> > > > First of all, I'd like to say that it's only a result of some
> >> personal
> >> > > > thinking, alone in the dark with a use case, the spark code, a
> >> sheet of
> >> > > > paper and a poor pen.
> >> > > >
> >> > > >
> >> > > > DStream is a very great concept to deal with micro-batching use
> >> cases,
> >> > > and
> >> > > > it does it very well too!
> >> > > > Also, it hardly relies on the elapsing time to create its intern=
al
> >> > > > micro-batches.
> >> > > > However, there are similar use cases where we need micro-batches
> >> where
> >> > > this
> >> > > > constraint on the time doesn't hold, here are two of them:
> >> > > > * a micro-batch has to be created every *n* events received
> >> > > > * a micro-batch has to be generate based on the values of the
> items
> >> > > pushed
> >> > > > by the source (which might even not be a stream!).
> >> > > >
> >> > > > An example of use case (mine ^^) would be
> >> > > > * the creation of timeseries from a cold source containing
> >> timestamped
> >> > > > events (like S3).
> >> > > > * one these timeseries have cells being the mean (sum, count, ..=
.)
> >> of
> >> > one
> >> > > > of the fields of the event
> >> > > > * the mean has to be computed over a window depending on a field
> >> > > > *timestamp*.
> >> > > >
> >> > > > * a timeserie is created for each type of event (the number of
> >> types is
> >> > > > high)
> >> > > > So, in this case, it'd be interesting to have an RDD for each
> cell,
> >> > which
> >> > > > will generate all cells for all neede timeseries.
> >> > > > It's more or less what DStream does, but here it won't help due
> what
> >> > was
> >> > > > stated above.
> >> > > >
> >> > > > That's how I came to a raw sketch of what could be named
> >> ContinuousRDD
> >> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> >> > > simplicity
> >> > > > I've stuck with the definition of a DStream to think about it.
> Okay,
> >> > > let's
> >> > > > go ^^.
> >> > > >
> >> > > >
> >> > > > Looking at the DStream contract, here is something that could be
> >> > drafted
> >> > > > around CRDD.
> >> > > > A *CRDD* would be a generalized concept that relies on:
> >> > > > * a reference space/continuum (to which data can be bound)
> >> > > > * a binning function that can breaks the continuum into splits.
> >> > > > Since *Space* is a continuum we could define it as:
> >> > > > * a *SpacePoint* (the origin)
> >> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
> >> > > > * a Ordering[SpacePoint]
> >> > > >
> >> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which a=
re
> >> > using
> >> > > > timer and clock to do their work, in the case of a CRDD we'll ha=
ve
> >> to
> >> > > > define also a point generator, as a more generic but also
> adaptable
> >> > > > concept.
> >> > > >
> >> > > >
> >> > > > So far (so good?), these definition should work quite fine for
> >> > *ordered*
> >> > > > space
> >> > > > for which:
> >> > > > * points are coming/fetched in order
> >> > > > * the space is fully filled (no gaps)
> >> > > > For these cases, the JobGenerator (f.i.) could be defined with t=
wo
> >> > extra
> >> > > > functions:
> >> > > > * one is responsible to chop the batches even if the upper bound
> of
> >> the
> >> > > > batch hasn't been seen yet
> >> > > > * the other is responsible to handle outliers (and could wrap th=
em
> >> into
> >> > > yet
> >> > > > another CRDD ?)
> >> > > >
> >> > > >
> >> > > > I created a gist here wrapping up the types and thus the skeleto=
n
> of
> >> > this
> >> > > > idea, you can find it here:
> >> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> >> > > >
> >> > > > WDYT?
> >> > > > *The answer can be: you're a fool!*
> >> > > > Actually, I already I am, but also I like to know why.... so som=
e
> >> > > > explanations will help me :-D.
> >> > > >
> >> > > > Thanks to read 'till this point.
> >> > > >
> >> > > > Greetz,
> >> > > >
> >> > > >
> >> > > >
> >> > > >  a=E2=84=95dy =E2=84=99etrella
> >> > > > about.me/noootsab
> >> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >> > > >
> >> > > > <http://about.me/noootsab>
> >> > > >
> >> > >
> >> >
> >>
> >
> >
>

--089e01184cc652121504ff991c95--

From dev-return-8670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 23:15:55 2014
Return-Path: <dev-return-8670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28F18113C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 23:15:55 +0000 (UTC)
Received: (qmail 41132 invoked by uid 500); 1 Aug 2014 23:15:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41071 invoked by uid 500); 1 Aug 2014 23:15:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41012 invoked by uid 99); 1 Aug 2014 23:15:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:53 +0000
Received: by mail-wi0-f182.google.com with SMTP id d1so2112134wiv.9
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 16:15:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=ZfXKd0/SBs60ndntlK5zu1JAF1/lmtEZxL2+eNWjrlk=;
        b=0V6G54/MiLp1bv5Y9/RKxpbGgbz/vTiM9nvDDExbSJnaUJkPOICV6EEXDBpB4swQC4
         gt6lgEXeYXBktsmz3KBga1m+/0p8CHNmlsXyUOxDIIPeScbcEZnjOZR5+c0W7c4DsczR
         ToimSXKAsAT62T3ffiYCQ6wUAqnNIIZOTO2KIUVEr3tQo7ZoEPVCiNwenxgoGwbINF/0
         7XzEsuFU1UV9Gv9FlzWLrlFFZnWEtIV+rFsRf1wp5l0llqJmvlgE3pFVVj3Qj3vRNYVd
         70DY+MkudS/9MoCKkQ5fUoAV9TcGa5JjCfbosojn8xVytBGZ86MLHIMVrgQjohWDHf5n
         y7Ww==
MIME-Version: 1.0
X-Received: by 10.180.87.199 with SMTP id ba7mr10993616wib.49.1406934928894;
 Fri, 01 Aug 2014 16:15:28 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.6.5 with HTTP; Fri, 1 Aug 2014 16:15:28 -0700 (PDT)
In-Reply-To: <CA+-p3AEOuzkC2+0mFG2Y6nVaVn=WfyCOVXK-V9t=qesMAY8Siw@mail.gmail.com>
References: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
	<CA+-p3AEOuzkC2+0mFG2Y6nVaVn=WfyCOVXK-V9t=qesMAY8Siw@mail.gmail.com>
Date: Fri, 1 Aug 2014 16:15:28 -0700
X-Google-Sender-Auth: hFc9DFNBg9jPjUQ9Jv0Ubz4XFMM
Message-ID: <CA+qbEUPv4d0dh2+GOgrOf8ZGjnzwgQRTgFvvwXwaCuX7DG3C1g@mail.gmail.com>
Subject: Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, Aug 1, 2014 at 2:45 PM, Andrew Ash <andrew@andrewash.com> wrote:
> After several days of debugging, we think the issue is that we have
> conflicting versions of Guava.  Our application was running with Guava 14
> and the Spark services (Master, Workers, Executors) had Guava 16.  We had
> custom Kryo serializers for Guava's ImmutableLists, and commenting out
> those register calls did the trick.
>
> Have people had issues with Guava version mismatches in the past?

There's some discussion about dealing with Guava version issues in
Spark in SPARK-2420.

best,
Colin


>
> I've found @srowen's Guava 14 -> 11 downgrade PR here
> https://github.com/apache/spark/pull/1610 and some extended discussion on
> https://issues.apache.org/jira/browse/SPARK-2420 for Hive compatibility
>
>
> On Thu, Jul 31, 2014 at 10:47 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> Hi everyone,
>>
>> I'm seeing the below exception coming out of Spark 1.0.1 when I call it
>> from my application.  I can't share the source to that application, but the
>> quick gist is that it uses Spark's Java APIs to read from Avro files in
>> HDFS, do processing, and write back to Avro files.  It does this by
>> receiving a REST call, then spinning up a new JVM as the driver application
>> that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and also
>> speculation.  The cluster is running in standalone mode on a 6 node cluster
>> in AWS (not using Spark's EC2 scripts though).
>>
>> The below stacktraces are reliably reproduceable on every run of the job.
>>  The issue seems to be that on deserialization of a task result on the
>> driver, Kryo spits up while reading the ClassManifest.
>>
>> I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
>> backcompat issues) but had the same error.
>>
>> Any ideas on what can be done here?
>>
>> Thanks!
>> Andrew
>>
>>
>>
>> In the driver (Kryo exception while deserializing a DirectTaskResult):
>>
>> INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
>> thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
>> getting task result
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |
>> com.esotericsoftware.kryo.KryoException: Buffer underflow.
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.esotericsoftware.kryo.io.Input.require(Input.java:156)
>> ~[kryo-2.21.jar:na]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
>> ~[kryo-2.21.jar:na]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
>> ~[kryo-2.21.jar:na]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624) ~[kryo-2.21.jar:na]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
>> ~[chill_2.10-0.3.6.jar:0.3.6]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
>> ~[chill_2.10-0.3.6.jar:0.3.6]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
>> ~[kryo-2.21.jar:na]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
>> ~[spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
>> ~[spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
>> ~[spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
>> ~[spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
>> [spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
>> [spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
>> [spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
>> [spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
>> [spark-core_2.10-1.0.1.jar:1.0.1]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> [na:1.7.0_65]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> [na:1.7.0_65]
>> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
>> java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
>>
>>
>> In the DAGScheduler (job gets aborted):
>>
>> org.apache.spark.SparkException: Job aborted due to stage failure:
>> Exception while getting task result:
>> com.esotericsoftware.kryo.KryoException: Buffer underflow.
>>     at org.apache.spark.scheduler.DAGScheduler.org
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>     at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>     at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>     at
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>     at
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>     at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>     at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>     at scala.Option.foreach(Option.scala:236)
>>     at
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>     at
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>     at
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>     at
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>     at
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>     at
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>
>>
>> In an Executor (running tasks get killed):
>>
>> 14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading broadcast
>> variable 0
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 153
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 147
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 141
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 135
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 150
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 144
>> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
>> 138
>> 14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733) called
>> with curMem=0, maxMem=30870601728
>> 14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
>> values to memory (estimated size 236.1 KB, free 28.8 GB)
>> 14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast variable
>> 0 took 0.91790748 s
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
>> locally
>> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
>> org.apache.spark.TaskKilledException
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         at java.lang.Thread.run(Thread.java:745)
>> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
>> org.apache.spark.TaskKilledException
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         at java.lang.Thread.run(Thread.java:745)
>> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
>> org.apache.spark.TaskKilledException
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         at java.lang.Thread.run(Thread.java:745)
>> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
>> org.apache.spark.TaskKilledException
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         at java.lang.Thread.run(Thread.java:745)
>> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
>> org.apache.spark.TaskKilledException
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         at java.lang.Thread.run(Thread.java:745)
>>

From dev-return-8671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 23:15:59 2014
Return-Path: <dev-return-8671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6CAB7113C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 23:15:59 +0000 (UTC)
Received: (qmail 42170 invoked by uid 500); 1 Aug 2014 23:15:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42111 invoked by uid 500); 1 Aug 2014 23:15:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42088 invoked by uid 99); 1 Aug 2014 23:15:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.52 as permitted sender)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:52 +0000
Received: by mail-la0-f52.google.com with SMTP id e16so3677034lan.25
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 16:15:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5+syknNSY9JQGiiJHAdZyW+FXcByO/F6XZeOReU03d8=;
        b=L2xAHBTUyrY0U0IxPPGC6qu3NPN3FPeC+ERiaTwo1kD5bttI27l1lxxo8jnbOUlGdr
         W5q3la3y6lUMGJFlxwKuDxpa3bot8QuknLi9OUs3FENXVoSt061OtQvg0sTTjmHRo+Pi
         wKkRJz485ho7ln65w27caNtYMd/fhC+DwGd9XFgfCopL6lXfJlqWZgqeItYrFzCnSkfl
         snwfWsiyVdjPxSlw8jcf4QsLixzmpUrs1UMWng7XRs/Ikqe3LF+54ex539f9rNqygWfm
         k7CxbdkowvbiqNyGweIy+CniguwTDTdBy84WLZJj4aN4YY5fpvMec3rTJoBadDWdP93n
         qPSw==
MIME-Version: 1.0
X-Received: by 10.152.245.171 with SMTP id xp11mr9502345lac.61.1406934931479;
 Fri, 01 Aug 2014 16:15:31 -0700 (PDT)
Received: by 10.112.122.75 with HTTP; Fri, 1 Aug 2014 16:15:31 -0700 (PDT)
Received: by 10.112.122.75 with HTTP; Fri, 1 Aug 2014 16:15:31 -0700 (PDT)
In-Reply-To: <CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
	<CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
	<CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
	<CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
	<CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
	<CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
	<CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
Date: Sat, 2 Aug 2014 01:15:31 +0200
Message-ID: <CAKn3j0uM-m9oXXb5qF-FZHNPJAx=0uWcPxC=-9cgZs0oSpnO8w@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
From: andy petrella <andy.petrella@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11345e12d0222d04ff9990f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11345e12d0222d04ff9990f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Actually for click stream, the users space wouldn't be a continuum, unless
the order of users is important or the fact that they are coming in a kind
of order can be used by the algo.
The purpose of the break or binning function is to package things in a
cluster for which we know the properties, but we don't know in advance
which or how many elements it will contain.
However,  this would need to extend the notion of continuum I thought of,
to, indeed,  include categorical space and thus allowing groupBy mapping to
RDDs.
And actually,  there would be a way to fallback to a continuum if the
breaks function would be dictated by a trained model that can cluster the
users,  and they were previously and accordingly shuffled to form a
sequence where they come in batch.
Just thinking (and hardly trying to use a tablet to write it, man... How
unfriendly is this keyboard and small screen =E2=98=BA)
Cheers
Andy

--001a11345e12d0222d04ff9990f8--

From dev-return-8672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 23:16:00 2014
Return-Path: <dev-return-8672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DECF113CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 23:16:00 +0000 (UTC)
Received: (qmail 42319 invoked by uid 500); 1 Aug 2014 23:15:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42114 invoked by uid 500); 1 Aug 2014 23:15:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42099 invoked by uid 99); 1 Aug 2014 23:15:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.51 as permitted sender)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:15:52 +0000
Received: by mail-la0-f51.google.com with SMTP id pn19so3650094lab.24
        for <dev@spark.incubator.apache.org>; Fri, 01 Aug 2014 16:15:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5+syknNSY9JQGiiJHAdZyW+FXcByO/F6XZeOReU03d8=;
        b=L2xAHBTUyrY0U0IxPPGC6qu3NPN3FPeC+ERiaTwo1kD5bttI27l1lxxo8jnbOUlGdr
         W5q3la3y6lUMGJFlxwKuDxpa3bot8QuknLi9OUs3FENXVoSt061OtQvg0sTTjmHRo+Pi
         wKkRJz485ho7ln65w27caNtYMd/fhC+DwGd9XFgfCopL6lXfJlqWZgqeItYrFzCnSkfl
         snwfWsiyVdjPxSlw8jcf4QsLixzmpUrs1UMWng7XRs/Ikqe3LF+54ex539f9rNqygWfm
         k7CxbdkowvbiqNyGweIy+CniguwTDTdBy84WLZJj4aN4YY5fpvMec3rTJoBadDWdP93n
         qPSw==
MIME-Version: 1.0
X-Received: by 10.152.245.171 with SMTP id xp11mr9502345lac.61.1406934931479;
 Fri, 01 Aug 2014 16:15:31 -0700 (PDT)
Received: by 10.112.122.75 with HTTP; Fri, 1 Aug 2014 16:15:31 -0700 (PDT)
Received: by 10.112.122.75 with HTTP; Fri, 1 Aug 2014 16:15:31 -0700 (PDT)
In-Reply-To: <CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
	<CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
	<CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
	<CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
	<CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
	<CAKn3j0vmVF=ZfE_V4DnHKFeDyZzrGtxpW84fisn1HjbMpPaBZg@mail.gmail.com>
	<CAAqHKj5Kdtfgo7Vpv8f1KrwOu9nA3qikBouMJ23ZbRi3PfAeoQ@mail.gmail.com>
Date: Sat, 2 Aug 2014 01:15:31 +0200
Message-ID: <CAKn3j0uM-m9oXXb5qF-FZHNPJAx=0uWcPxC=-9cgZs0oSpnO8w@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
From: andy petrella <andy.petrella@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11345e12d0222d04ff9990f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11345e12d0222d04ff9990f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Actually for click stream, the users space wouldn't be a continuum, unless
the order of users is important or the fact that they are coming in a kind
of order can be used by the algo.
The purpose of the break or binning function is to package things in a
cluster for which we know the properties, but we don't know in advance
which or how many elements it will contain.
However,  this would need to extend the notion of continuum I thought of,
to, indeed,  include categorical space and thus allowing groupBy mapping to
RDDs.
And actually,  there would be a way to fallback to a continuum if the
breaks function would be dictated by a trained model that can cluster the
users,  and they were previously and accordingly shuffled to form a
sequence where they come in batch.
Just thinking (and hardly trying to use a tablet to write it, man... How
unfriendly is this keyboard and small screen =E2=98=BA)
Cheers
Andy

--001a11345e12d0222d04ff9990f8--

From dev-return-8673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  1 23:48:31 2014
Return-Path: <dev-return-8673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 816F111513
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  1 Aug 2014 23:48:31 +0000 (UTC)
Received: (qmail 15821 invoked by uid 500); 1 Aug 2014 23:48:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15779 invoked by uid 500); 1 Aug 2014 23:48:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15767 invoked by uid 99); 1 Aug 2014 23:48:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:48:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 01 Aug 2014 23:48:25 +0000
Received: by mail-qa0-f52.google.com with SMTP id j15so4737450qaq.11
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 16:48:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=UkvifdhtVxn15O11fUlDMBH3pWaOtBhoWJf0LFx4fk8=;
        b=CNjW4p0QQKFQ9SVu/QZ5vSI5fuuNu2bmv9pB4/685T4xeJEiDOnRx0b57lyFjlD+mD
         4pECvKNNl7eT1UK5SArc3aDZwP2mRJKmI9z77O4oopPhMRomyQAgh2DK8eyO6Qtq1zQo
         dSczI9cCPFf9bSf7nPddDSh79FX/5LCDvtDAcNr/D45pRZt7qyZj0IB8kkaSxOFeKdfm
         f/tfj7Px145sRqlrxTnG2t3Ny6h9dtg3Z46d0eBsTi/t9oS7C+Mfo8uJ3HzRIlDDhEG1
         D/SC3qRl8W/SX9xluF/RdGVwNmvUrFV36anCgA1He7EfJoP5/429V5FzYf0cIQpy6Onn
         SsQQ==
X-Gm-Message-State: ALoCoQmntioONmPvFgVILMon4ZmPYtZz/WrHNOsnT89ZiakYZgfr7LCyI/aSvlfn8pFG7v5I+JJR
MIME-Version: 1.0
X-Received: by 10.140.20.17 with SMTP id 17mr13236527qgi.85.1406936884684;
 Fri, 01 Aug 2014 16:48:04 -0700 (PDT)
Received: by 10.96.224.104 with HTTP; Fri, 1 Aug 2014 16:48:04 -0700 (PDT)
Date: Fri, 1 Aug 2014 16:48:04 -0700
Message-ID: <CAAOnQ7vsv+=6-TA7dL_=_JK3OZk=JzLFyMBhnrQMOxLMGq0MFw@mail.gmail.com>
Subject: SparkContext.hadoopConfiguration vs. SparkHadoopUtil.newConfiguration()
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

While working on some seemingly unrelated code, I ran into this issue
where "spark.hadoop.*" configs were not making it to the Configuration
objects in some parts of the code. I was trying to do that to avoid
having to do dirty ticks with the classpath while running tests, but
that's a little besides the point.

Since I don't know the history of that code in SparkContext, does
anybody see any issue with moving it up a layer so that all code that
uses SparkHadoopUtil.newConfiguration() does the same thing?

This would also include some code (e.g. in the yarn module) that does
"new Configuration()" directly instead of going through the wrapper.


-- 
Marcelo

From dev-return-8674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 03:13:57 2014
Return-Path: <dev-return-8674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F2BB311A28
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 03:13:56 +0000 (UTC)
Received: (qmail 14586 invoked by uid 500); 2 Aug 2014 03:13:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14524 invoked by uid 500); 2 Aug 2014 03:13:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14509 invoked by uid 99); 2 Aug 2014 03:13:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 03:13:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 03:13:52 +0000
Received: by mail-oi0-f46.google.com with SMTP id i138so3243918oig.19
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 20:13:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=yX5cF1qtIMoFNpCIngPcsnBdtJVy/3LsDCmOhNE3st0=;
        b=jZsbQC5bz7FxdaGpYZKhIcf5RpuL0nYpHf7CYLQxAMYUurs9j8Vkjr+zu04QmNifnz
         NMpouPEjT0XKHYomEcZOc8X3ZOB7VxNoJNxTnyfUgzPrcgFIThitDYc7NOx5Wl67mBrh
         lvevG4tO5PnlHPchDbH0FtDJesnFOgkVSOvVjlCGXx+AbD/vjL1Axq59U4ZY13JR91Wt
         eCWng4CQNIO4Ow0S7bn49qqcDV9Z4mIDWZrtn9ojlamUfz68+nqjNrhWyXQhgSaXmWEY
         g0SKKXsDaeGFuCRZTCEajUMuGJe0i8K+58I03yirNkL0po3VAWqTKo1ne+e1SZn/Qj2h
         pjpQ==
MIME-Version: 1.0
X-Received: by 10.182.18.101 with SMTP id v5mr14139039obd.64.1406949207453;
 Fri, 01 Aug 2014 20:13:27 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 1 Aug 2014 20:13:27 -0700 (PDT)
In-Reply-To: <CA+qbEUPv4d0dh2+GOgrOf8ZGjnzwgQRTgFvvwXwaCuX7DG3C1g@mail.gmail.com>
References: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
	<CA+-p3AEOuzkC2+0mFG2Y6nVaVn=WfyCOVXK-V9t=qesMAY8Siw@mail.gmail.com>
	<CA+qbEUPv4d0dh2+GOgrOf8ZGjnzwgQRTgFvvwXwaCuX7DG3C1g@mail.gmail.com>
Date: Fri, 1 Aug 2014 20:13:27 -0700
Message-ID: <CABPQxsvx0_=UV4suMqzLT2qf0+cJSW1WVpAtYtTssELcvgKZeQ@mail.gmail.com>
Subject: Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33060ba358404ff9ce3da
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33060ba358404ff9ce3da
Content-Type: text/plain; charset=ISO-8859-1

Andrew - I think Spark is using Guava 14... are you using Guava 16 in your
user app (i.e. you inverted the versions in your earlier e-mail)?

- Patrick


On Fri, Aug 1, 2014 at 4:15 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:

> On Fri, Aug 1, 2014 at 2:45 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > After several days of debugging, we think the issue is that we have
> > conflicting versions of Guava.  Our application was running with Guava 14
> > and the Spark services (Master, Workers, Executors) had Guava 16.  We had
> > custom Kryo serializers for Guava's ImmutableLists, and commenting out
> > those register calls did the trick.
> >
> > Have people had issues with Guava version mismatches in the past?
>
> There's some discussion about dealing with Guava version issues in
> Spark in SPARK-2420.
>
> best,
> Colin
>
>
> >
> > I've found @srowen's Guava 14 -> 11 downgrade PR here
> > https://github.com/apache/spark/pull/1610 and some extended discussion
> on
> > https://issues.apache.org/jira/browse/SPARK-2420 for Hive compatibility
> >
> >
> > On Thu, Jul 31, 2014 at 10:47 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> >> Hi everyone,
> >>
> >> I'm seeing the below exception coming out of Spark 1.0.1 when I call it
> >> from my application.  I can't share the source to that application, but
> the
> >> quick gist is that it uses Spark's Java APIs to read from Avro files in
> >> HDFS, do processing, and write back to Avro files.  It does this by
> >> receiving a REST call, then spinning up a new JVM as the driver
> application
> >> that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and
> also
> >> speculation.  The cluster is running in standalone mode on a 6 node
> cluster
> >> in AWS (not using Spark's EC2 scripts though).
> >>
> >> The below stacktraces are reliably reproduceable on every run of the
> job.
> >>  The issue seems to be that on deserialization of a task result on the
> >> driver, Kryo spits up while reading the ClassManifest.
> >>
> >> I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
> >> backcompat issues) but had the same error.
> >>
> >> Any ideas on what can be done here?
> >>
> >> Thanks!
> >> Andrew
> >>
> >>
> >>
> >> In the driver (Kryo exception while deserializing a DirectTaskResult):
> >>
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
> >> thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
> >> getting task result
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |
> >> com.esotericsoftware.kryo.KryoException: Buffer underflow.
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> com.esotericsoftware.kryo.io.Input.require(Input.java:156)
> >> ~[kryo-2.21.jar:na]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
> >> ~[kryo-2.21.jar:na]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
> >> ~[kryo-2.21.jar:na]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624)
> ~[kryo-2.21.jar:na]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
> >> ~[chill_2.10-0.3.6.jar:0.3.6]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
> >> ~[chill_2.10-0.3.6.jar:0.3.6]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
> >> ~[kryo-2.21.jar:na]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
> >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
> >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
> >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
> >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
> >> [spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> >> [spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> >> [spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
> >> [spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
> >> [spark-core_2.10-1.0.1.jar:1.0.1]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >> [na:1.7.0_65]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >> [na:1.7.0_65]
> >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> >> java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
> >>
> >>
> >> In the DAGScheduler (job gets aborted):
> >>
> >> org.apache.spark.SparkException: Job aborted due to stage failure:
> >> Exception while getting task result:
> >> com.esotericsoftware.kryo.KryoException: Buffer underflow.
> >>     at org.apache.spark.scheduler.DAGScheduler.org
> >>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
> >>     at
> >>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >>     at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >>     at scala.Option.foreach(Option.scala:236)
> >>     at
> >>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
> >>     at
> >>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
> >>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >>     at
> >>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >>     at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >>     at
> >>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >>     at
> >> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >>     at
> >>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >>
> >>
> >> In an Executor (running tasks get killed):
> >>
> >> 14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading
> broadcast
> >> variable 0
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 153
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 147
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 141
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 135
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 150
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 144
> >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> task
> >> 138
> >> 14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733)
> called
> >> with curMem=0, maxMem=30870601728
> >> 14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
> >> values to memory (estimated size 236.1 KB, free 28.8 GB)
> >> 14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast
> variable
> >> 0 took 0.91790748 s
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> >> locally
> >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
> >> org.apache.spark.TaskKilledException
> >>         at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>         at java.lang.Thread.run(Thread.java:745)
> >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
> >> org.apache.spark.TaskKilledException
> >>         at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>         at java.lang.Thread.run(Thread.java:745)
> >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
> >> org.apache.spark.TaskKilledException
> >>         at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>         at java.lang.Thread.run(Thread.java:745)
> >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
> >> org.apache.spark.TaskKilledException
> >>         at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>         at java.lang.Thread.run(Thread.java:745)
> >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
> >> org.apache.spark.TaskKilledException
> >>         at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>         at
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>         at java.lang.Thread.run(Thread.java:745)
> >>
>

--001a11c33060ba358404ff9ce3da--

From dev-return-8675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 05:07:22 2014
Return-Path: <dev-return-8675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1DE9A11C1B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 05:07:22 +0000 (UTC)
Received: (qmail 4926 invoked by uid 500); 2 Aug 2014 05:07:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4866 invoked by uid 500); 2 Aug 2014 05:07:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4837 invoked by uid 99); 2 Aug 2014 05:07:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 05:07:20 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 05:07:18 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so8168042vcb.19
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 22:06:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=kZ+/c+v7u+fn8YrTjXOmQ1ZwwLrZ2pzo/RnVI5Bv6QU=;
        b=Btb+9nmXWN2E4r8InNIFemIQbE1NYdeXpQcI31uLApwSWF/xUhmW0nYFo03Al1MGay
         8FJgePXa1QvjHriu6CJe9DpfZrdp/smUEpYjF4puQTry4Kg69DI7HxwrC2DwBvapQoZU
         5lXi//y1t6ICESkZrppABpkrmf1ZPEIC+bMiGVlLoQBJgWrUiTfNlVC43ygPgxwMTx4d
         QY6wQKnCOgirASVvgY6k0EtmYA8x4ZYCj7f5+Sn0HG/Rfkn1rXYd8oGyvqZUdOfGeUCl
         rkWtNdYb2o4N33pSG+Mt3PlWTdyucLTTlP+AkVRpVBgMGMkvLs/J7Xxoqh3bf/Zpn0+5
         vZhg==
X-Gm-Message-State: ALoCoQlLt/+oTkIOFqJRBD+vpN8ki7bYXf1BEOF5pYcxtafXAGsujZ+NY+wkHt8JH2FMVb7IG3ck
X-Received: by 10.220.192.129 with SMTP id dq1mr7987698vcb.57.1406956012812;
        Fri, 01 Aug 2014 22:06:52 -0700 (PDT)
Received: from mail-vc0-f178.google.com (mail-vc0-f178.google.com [209.85.220.178])
        by mx.google.com with ESMTPSA id dr7sm26950071vdc.26.2014.08.01.22.06.51
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 01 Aug 2014 22:06:51 -0700 (PDT)
Received: by mail-vc0-f178.google.com with SMTP id la4so8049175vcb.23
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 22:06:51 -0700 (PDT)
X-Received: by 10.221.66.67 with SMTP id xp3mr11292120vcb.44.1406956011269;
 Fri, 01 Aug 2014 22:06:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Fri, 1 Aug 2014 22:06:31 -0700 (PDT)
In-Reply-To: <CABPQxsvx0_=UV4suMqzLT2qf0+cJSW1WVpAtYtTssELcvgKZeQ@mail.gmail.com>
References: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
 <CA+-p3AEOuzkC2+0mFG2Y6nVaVn=WfyCOVXK-V9t=qesMAY8Siw@mail.gmail.com>
 <CA+qbEUPv4d0dh2+GOgrOf8ZGjnzwgQRTgFvvwXwaCuX7DG3C1g@mail.gmail.com> <CABPQxsvx0_=UV4suMqzLT2qf0+cJSW1WVpAtYtTssELcvgKZeQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sat, 2 Aug 2014 01:06:31 -0400
Message-ID: <CA+-p3AEA_7-rfanSCTRgi2rRCT_UkbhM2U0w-Yu-p7Y58the+A@mail.gmail.com>
Subject: Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113651d844378f04ff9e79e4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113651d844378f04ff9e79e4
Content-Type: text/plain; charset=UTF-8

The original version numbers I reported were indeed what we had, so let me
clarify the situation.

Our application had Guava 14 because that's what Spark depends on.  But we
had added an in-house library to the Hadoop cluster and also the Spark
cluster to add a new FileSystem (think hdfs://, s3n://, etc) that was using
Guava 16.  So the Guava 16 from our additional FileSystem overrode the
Guava 11 jar from the CDH4.4.0 lib directory and the Guava 14 class files
that are bundled in the Spark assembly jar.  That mismatch between 16 on
the cluster and 14 on the driver caused us problems with ImmutableLists,
which must have changed in a way between 14 and 16 that aren't binary
compatible in Kryo serialization.

At least that's our current understanding of the bug we experienced.


On Fri, Aug 1, 2014 at 11:13 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Andrew - I think Spark is using Guava 14... are you using Guava 16 in your
> user app (i.e. you inverted the versions in your earlier e-mail)?
>
> - Patrick
>
>
> On Fri, Aug 1, 2014 at 4:15 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
>
> > On Fri, Aug 1, 2014 at 2:45 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > > After several days of debugging, we think the issue is that we have
> > > conflicting versions of Guava.  Our application was running with Guava
> 14
> > > and the Spark services (Master, Workers, Executors) had Guava 16.  We
> had
> > > custom Kryo serializers for Guava's ImmutableLists, and commenting out
> > > those register calls did the trick.
> > >
> > > Have people had issues with Guava version mismatches in the past?
> >
> > There's some discussion about dealing with Guava version issues in
> > Spark in SPARK-2420.
> >
> > best,
> > Colin
> >
> >
> > >
> > > I've found @srowen's Guava 14 -> 11 downgrade PR here
> > > https://github.com/apache/spark/pull/1610 and some extended discussion
> > on
> > > https://issues.apache.org/jira/browse/SPARK-2420 for Hive
> compatibility
> > >
> > >
> > > On Thu, Jul 31, 2014 at 10:47 AM, Andrew Ash <andrew@andrewash.com>
> > wrote:
> > >
> > >> Hi everyone,
> > >>
> > >> I'm seeing the below exception coming out of Spark 1.0.1 when I call
> it
> > >> from my application.  I can't share the source to that application,
> but
> > the
> > >> quick gist is that it uses Spark's Java APIs to read from Avro files
> in
> > >> HDFS, do processing, and write back to Avro files.  It does this by
> > >> receiving a REST call, then spinning up a new JVM as the driver
> > application
> > >> that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and
> > also
> > >> speculation.  The cluster is running in standalone mode on a 6 node
> > cluster
> > >> in AWS (not using Spark's EC2 scripts though).
> > >>
> > >> The below stacktraces are reliably reproduceable on every run of the
> > job.
> > >>  The issue seems to be that on deserialization of a task result on the
> > >> driver, Kryo spits up while reading the ClassManifest.
> > >>
> > >> I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
> > >> backcompat issues) but had the same error.
> > >>
> > >> Any ideas on what can be done here?
> > >>
> > >> Thanks!
> > >> Andrew
> > >>
> > >>
> > >>
> > >> In the driver (Kryo exception while deserializing a DirectTaskResult):
> > >>
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result
> resolver
> > >> thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
> > >> getting task result
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |
> > >> com.esotericsoftware.kryo.KryoException: Buffer underflow.
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> com.esotericsoftware.kryo.io.Input.require(Input.java:156)
> > >> ~[kryo-2.21.jar:na]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
> > >> ~[kryo-2.21.jar:na]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
> > >> ~[kryo-2.21.jar:na]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624)
> > ~[kryo-2.21.jar:na]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
> > >> ~[chill_2.10-0.3.6.jar:0.3.6]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
> > >> ~[chill_2.10-0.3.6.jar:0.3.6]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
> > >> ~[kryo-2.21.jar:na]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
> > >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
> > >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
> > >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
> > >> ~[spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
> > >> [spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> > >> [spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
> > >> [spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
> > >> [spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
> > >> [spark-core_2.10-1.0.1.jar:1.0.1]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >> [na:1.7.0_65]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >> [na:1.7.0_65]
> > >> INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
> > >> java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
> > >>
> > >>
> > >> In the DAGScheduler (job gets aborted):
> > >>
> > >> org.apache.spark.SparkException: Job aborted due to stage failure:
> > >> Exception while getting task result:
> > >> com.esotericsoftware.kryo.KryoException: Buffer underflow.
> > >>     at org.apache.spark.scheduler.DAGScheduler.org
> > >>
> >
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
> > >>     at
> > >>
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > >>     at
> > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> > >>     at scala.Option.foreach(Option.scala:236)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
> > >>     at
> > >>
> >
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
> > >>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > >>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > >>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> > >>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> > >>     at
> > >>
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> > >>     at
> > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> > >>     at
> > >>
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> > >>     at
> > >>
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> > >>     at
> > >>
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> > >>
> > >>
> > >> In an Executor (running tasks get killed):
> > >>
> > >> 14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading
> > broadcast
> > >> variable 0
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 153
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 147
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 141
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 135
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 150
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 144
> > >> 14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill
> > task
> > >> 138
> > >> 14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733)
> > called
> > >> with curMem=0, maxMem=30870601728
> > >> 14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored
> as
> > >> values to memory (estimated size 236.1 KB, free 28.8 GB)
> > >> 14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast
> > variable
> > >> 0 took 0.91790748 s
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0
> > >> locally
> > >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
> > >> org.apache.spark.TaskKilledException
> > >>         at
> > >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >>         at java.lang.Thread.run(Thread.java:745)
> > >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
> > >> org.apache.spark.TaskKilledException
> > >>         at
> > >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >>         at java.lang.Thread.run(Thread.java:745)
> > >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
> > >> org.apache.spark.TaskKilledException
> > >>         at
> > >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >>         at java.lang.Thread.run(Thread.java:745)
> > >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
> > >> org.apache.spark.TaskKilledException
> > >>         at
> > >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >>         at java.lang.Thread.run(Thread.java:745)
> > >> 14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
> > >> org.apache.spark.TaskKilledException
> > >>         at
> > >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >>         at
> > >>
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >>         at java.lang.Thread.run(Thread.java:745)
> > >>
> >
>

--001a113651d844378f04ff9e79e4--

From dev-return-8676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 05:19:52 2014
Return-Path: <dev-return-8676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D7AAB11C67
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 05:19:52 +0000 (UTC)
Received: (qmail 16016 invoked by uid 500); 2 Aug 2014 05:19:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15959 invoked by uid 500); 2 Aug 2014 05:19:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15947 invoked by uid 99); 2 Aug 2014 05:19:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 05:19:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 05:19:47 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so6575887qgd.41
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 22:19:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=eDrK+UdfxIv69tMjBDw3kztW1Umg7DcUqCpPqRCbEtk=;
        b=gO87HGgZ3ZJ3u+e1p4tyed+FIY4nxzVNnhkunJeULyTDuU9OZhK51mAaUGKc0CBfGr
         /MhGJkfAhf6VmVqwGoPtgbE+c3t4Y5Q54PuEoccRtu21wAl8xZPQtjKXozl3Bxku3hpw
         SLyI3oPHSMFO0jcG6OOjgBtzIKiDIE3WffWBOqSmg1LsZz2QKiVKtcqUrfau6E4JL31H
         lmTkploIMOpBynUvc3igPq0eNWSrCabRgiSAEcRNjB3H95WVWX59GKucOXeqY97KH4xF
         242hqFt2l1Q6M7TWFmQ862fQH7m+YTxUriLIsZztge1mNgXN8Jt8TL+puIFwfVsaa0dT
         cEjg==
X-Received: by 10.224.47.67 with SMTP id m3mr13360968qaf.28.1406956766726;
 Fri, 01 Aug 2014 22:19:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Fri, 1 Aug 2014 22:19:06 -0700 (PDT)
In-Reply-To: <CAAswR-7r=rZ8LhDLvhELXNyFGqDN90b8-YiCuw7dDBb_UR-HVw@mail.gmail.com>
References: <7C296C02-D333-4C8A-AF00-09953C4D718F@asiainfo.com> <CAAswR-7r=rZ8LhDLvhELXNyFGqDN90b8-YiCuw7dDBb_UR-HVw@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Sat, 2 Aug 2014 13:19:06 +0800
Message-ID: <CAA_qdLqxyk-EzaMWsXVzxfNMbucyuKxQSJoSu6F-Nj0xf7t-hw@mail.gmail.com>
Subject: Re: How to run specific sparkSQL test with maven
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134ab004b8fd004ff9ea60e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134ab004b8fd004ff9ea60e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It=E2=80=99s also useful to set hive.exec.mode.local.auto to true to accele=
rate the
test.
=E2=80=8B


On Sat, Aug 2, 2014 at 1:36 AM, Michael Armbrust <michael@databricks.com>
wrote:

> >
> > It seems that the HiveCompatibilitySuite need a hadoop and hive
> > environment, am I right?
> >
> > "Relative path in absolute URI:
> > file:$%7Bsystem:test.tmp.dir%7D/tmp_showcrt1=E2=80=9D
> >
>
> You should only need Hadoop and Hive if you are creating new tests that w=
e
> need to compute the answers for.  Existing tests are run with cached
> answers.  There are details about the configuration here:
> https://github.com/apache/spark/tree/master/sql
>

--001a1134ab004b8fd004ff9ea60e--

From dev-return-8677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 06:07:15 2014
Return-Path: <dev-return-8677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47E1F11D2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 06:07:15 +0000 (UTC)
Received: (qmail 51059 invoked by uid 500); 2 Aug 2014 06:07:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50996 invoked by uid 500); 2 Aug 2014 06:07:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50984 invoked by uid 99); 2 Aug 2014 06:07:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 06:07:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 06:07:11 +0000
Received: by mail-ob0-f177.google.com with SMTP id wp18so3122270obc.22
        for <dev@spark.apache.org>; Fri, 01 Aug 2014 23:06:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=SwpiYeigHIBjm8u1NfP9R5AHcdLZV0BumfKWw1f2pG0=;
        b=o7XnB+uKPsGi6ORplK5IOLl7wN4Q2yQiEr9D5Pe2Fs+cVEfBu4XxHaTW+7nwA6iXg+
         OGltFy1bbZZNtldnBHMaN+bGO/4s3CaLA1TyEwnjzvsiMyIaESWdDGebCt+U95N60nvi
         Al5+ZBaWxDa5VxFMrrwAvH9qS3CKJVdn7v0T1l0Rz8tNg+qAvBc/G/wr9WDvdFNUrWbb
         y9ppK31v6oIJ3mGMuE6LMhDt5RcB61Ym0TgkdqoSEgz//VlW2NDCOoAIrwek0ZEXukBP
         9DiJUDrQY2Z2ubWbXt+YuGRQX/Cbv+yR66d9gFpc1vb65c6IQPhK/5tGHn2z83sHzGAc
         Mh1Q==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr14513978obb.34.1406959606470;
 Fri, 01 Aug 2014 23:06:46 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 1 Aug 2014 23:06:46 -0700 (PDT)
Date: Fri, 1 Aug 2014 23:06:46 -0700
Message-ID: <CABPQxst=VuaFNRuzmjTRZC4VLgbUxQh7Lfm8vk9ZvHv2HWzoWA@mail.gmail.com>
Subject: ASF JIRA is down for maintenance
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cf608e9de804ff9f4fe4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cf608e9de804ff9f4fe4
Content-Type: text/plain; charset=ISO-8859-1

Please don't let this prevent you from merging patches, just keep a list
and we can update the JIRA later.

- Patrick

--089e0149cf608e9de804ff9f4fe4--

From dev-return-8678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 08:52:38 2014
Return-Path: <dev-return-8678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B42AF11EA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 08:52:38 +0000 (UTC)
Received: (qmail 41511 invoked by uid 500); 2 Aug 2014 08:52:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41449 invoked by uid 500); 2 Aug 2014 08:52:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41436 invoked by uid 99); 2 Aug 2014 08:52:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 08:52:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 08:52:32 +0000
Received: by mail-oi0-f47.google.com with SMTP id x69so3378984oia.34
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 01:52:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=xghe28/RkB20ArXsYa3xGvSyjKxetaMXKNNtzWM/dok=;
        b=wFLUV399x4QmbHSZiAUd5JABpkERupGRTTkJUUYcPpVYlEelmLYKyuATNCKDUvIuaD
         6WhoBwFRyfDZfmEOT51Q4V0PtwJGv1qpXRw8+Rtsmg1lPD7hSgABFd5EqyFrYB2FyDKs
         onJM9xyppbt75h6MFvukenfWuO8A4LH1DLm7sOp9s8iD1A4uMqUMz6g088Gzkrb/wq8P
         xzWQaI8WxwszDxnynC0UdL+uPDOYL6/mR8TxYXqtrcud+sttaGiB8SA1EXHmwARYWpl1
         UCeME78x9EtH1zhNeS/kuHQPuQ543hRdwV1BnRg7uTlXP1cJ+BPUoohx/WqSj/sX+AwK
         O5ag==
MIME-Version: 1.0
X-Received: by 10.60.46.167 with SMTP id w7mr15772342oem.50.1406969531706;
 Sat, 02 Aug 2014 01:52:11 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sat, 2 Aug 2014 01:52:11 -0700 (PDT)
Date: Sat, 2 Aug 2014 01:52:11 -0700
Message-ID: <CABPQxsvR9+Kas3Q4-Rs9Ee81=i0A_1HWdPCnCHiLts_DwF8FKQ@mail.gmail.com>
Subject: branch-1.1 of Spark has been cut
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e015371c425b23004ffa19fde
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015371c425b23004ffa19fde
Content-Type: text/plain; charset=ISO-8859-1

Hey All,

I'm happy to announce branch-1.1 of Spark [1] - this branch will eventually
become the 1.1 release. Committers: new patches will need to be explicitly
back-ported into this branch in order to appear in the 1.1 release.

Thanks so much to all the committers and contributors who were extremely
active on review in the last week!

We'll now enter the standard triage process:
-> In the next 48 hours, smaller features that are very late in review
(e.g. loose ends) are okay to go in.
-> On Monday we'll package a community build and start community testing.
At that point we are solidly "bug fixes only" for most Spark components.
-> When deemed appropriate, we'll cut a release candidate for official
voting and start the process. As voting goes on, we'll eventually escalate
to "regressions only" - i.e. we hold the release only for fixes that
regress from earlier version.

I'll also try to document the release/triage process on the wiki in a bit
more detail. For those interested, here is a good example - the guidelines
for the Linux kernel release process:
https://www.kernel.org/doc/Documentation/development-process/2.Process

Thanks everyone, looking forward to a great 1.1 release!

[1]
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog;h=refs/heads/branch-1.1

- Patrick

--089e015371c425b23004ffa19fde--

From dev-return-8679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 14:25:53 2014
Return-Path: <dev-return-8679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 65BF311382
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 14:25:53 +0000 (UTC)
Received: (qmail 87307 invoked by uid 500); 2 Aug 2014 14:25:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87242 invoked by uid 500); 2 Aug 2014 14:25:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87229 invoked by uid 99); 2 Aug 2014 14:25:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 14:25:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 14:25:49 +0000
Received: by mail-we0-f179.google.com with SMTP id u57so5674258wes.10
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 07:25:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=v45xMkq/EG/vCDUAlgmvSga+dswfstT3KmkaCW6u7Y0=;
        b=c3PZUH51xMrPp2xmEd/fZoqLeTcpqm/1OWLZ9yVBmW5PkLjgHqj+0FkQVFuPjbU6vx
         Ro2ehj1PcZwtgoegXGDCEGcetorIPUtiVQ3qxjLp10tVHfdqq7aEC5UT+EOGXRj5qaH5
         13BbQdf9ERL066xAftTpZKasZe8PIuoVh7sI+1/mdGSFCtUiRrKF3QROJ1hRb2fa0/0G
         qbu8ubfqxesdz/Xo+m9JxBI1kKkpQPiPXDgDI4SQSy42EAuAPYyNiDFpOpLCiPGo1g8i
         Blc8igyEBunoYdzX07F70zhUd+v+1YoKi19t0jvdPa7BfaU8N0T+6E1EEMXZ6tqxzDd1
         iw5w==
X-Received: by 10.194.57.132 with SMTP id i4mr17494635wjq.6.1406989525388;
 Sat, 02 Aug 2014 07:25:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sat, 2 Aug 2014 07:24:45 -0700 (PDT)
In-Reply-To: <CABPQxst=VuaFNRuzmjTRZC4VLgbUxQh7Lfm8vk9ZvHv2HWzoWA@mail.gmail.com>
References: <CABPQxst=VuaFNRuzmjTRZC4VLgbUxQh7Lfm8vk9ZvHv2HWzoWA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 2 Aug 2014 10:24:45 -0400
Message-ID: <CAOhmDzeWo861OOD=UoJq2TF6dpS-hRME+A7vU_KCPo64DiuowA@mail.gmail.com>
Subject: Re: ASF JIRA is down for maintenance
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba9782edd12bc04ffa64693
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9782edd12bc04ffa64693
Content-Type: text/plain; charset=UTF-8

Seems to be back up now.


On Sat, Aug 2, 2014 at 2:06 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please don't let this prevent you from merging patches, just keep a list
> and we can update the JIRA later.
>
> - Patrick
>

--047d7ba9782edd12bc04ffa64693--

From dev-return-8680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 17:10:12 2014
Return-Path: <dev-return-8680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D2E09115E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 17:10:12 +0000 (UTC)
Received: (qmail 54076 invoked by uid 500); 2 Aug 2014 17:10:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53976 invoked by uid 500); 2 Aug 2014 17:10:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52948 invoked by uid 99); 2 Aug 2014 17:10:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:10:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dibyendu.bhattachary@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:10:07 +0000
Received: by mail-oa0-f43.google.com with SMTP id i7so3993465oag.16
        for <multiple recipients>; Sat, 02 Aug 2014 10:09:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=OJIErksgPdA7mzmD4t8TL6LHw8qUMNlGl9gcqzV0Df8=;
        b=H7/CrZBJ8f4mEQvu3/nHa2YWzJ8b/1u1xiXh18jehYWSd49YZpLF0J676ZK2kDWTeH
         keWaj0oAdP0hZuVzKcXlTW7zRr4OdiiliBMBtSVNbkKmmqFaHS020AqZnTZy1XZfiiP0
         exhDI9arEzFfxSkiceCkxdJt1x472fgvyflgkS4M9r0/n/h5C8d5a+Ebul3sBa2xZ9MY
         pBRe7o5FMhqmnLSfuQsW6UwiUHJnqJkb8tfFfWMJXoxdEbufkTQAjovDxzFyEcoFUZyV
         LSTYtY3ScOCmdBF6MJuSggzGLvVrJY4S/gZNspAFM44ZdRAebkgnqXaEiwW2sx89oYDS
         grwg==
MIME-Version: 1.0
X-Received: by 10.182.158.68 with SMTP id ws4mr18999083obb.86.1406999382615;
 Sat, 02 Aug 2014 10:09:42 -0700 (PDT)
Received: by 10.76.76.169 with HTTP; Sat, 2 Aug 2014 10:09:42 -0700 (PDT)
Date: Sat, 2 Aug 2014 22:39:42 +0530
Message-ID: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
Subject: Low Level Kafka Consumer for Spark
From: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013cc08666699604ffa89282
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cc08666699604ffa89282
Content-Type: text/plain; charset=UTF-8

Hi,

I have implemented a Low Level Kafka Consumer for Spark Streaming using
Kafka Simple Consumer API. This API will give better control over the Kafka
offset management and recovery from failures. As the present Spark
KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
control over the offset management which is not possible in Kafka HighLevel
consumer.

This Project is available in below Repo :

https://github.com/dibbhatt/kafka-spark-consumer


I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver.
The KafkaReceiver uses low level Kafka Consumer API (implemented in
consumer.kafka packages) to fetch messages from Kafka and 'store' it in
Spark.

The logic will detect number of partitions for a topic and spawn that many
threads (Individual instances of Consumers). Kafka Consumer uses Zookeeper
for storing the latest offset for individual partitions, which will help to
recover in case of failure. The Kafka Consumer logic is tolerant to ZK
Failures, Kafka Leader of Partition changes, Kafka broker failures,
 recovery from offset errors and other fail-over aspects.

The consumer.kafka.client.Consumer is the sample Consumer which uses this
Kafka Receivers to generate DStreams from Kafka and apply a Output
operation for every messages of the RDD.

We are planning to use this Kafka Spark Consumer to perform Near Real Time
Indexing of Kafka Messages to target Search Cluster and also Near Real Time
Aggregation using target NoSQL storage.

Kindly let me know your view. Also if this looks good, can I contribute to
Spark Streaming project.

Regards,
Dibyendu

--089e013cc08666699604ffa89282--

From dev-return-8681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 17:13:42 2014
Return-Path: <dev-return-8681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6CB89115F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 17:13:42 +0000 (UTC)
Received: (qmail 64242 invoked by uid 500); 2 Aug 2014 17:13:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64183 invoked by uid 500); 2 Aug 2014 17:13:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64171 invoked by uid 99); 2 Aug 2014 17:13:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:13:40 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:13:37 +0000
Received: by mail-qg0-f42.google.com with SMTP id j5so7332129qga.1
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 10:13:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=anmyPVNv+OmjlkJnc4xMzr1Qfa6UROi3SNpZpmVXR4c=;
        b=Tq3COn/RDxXJPHicOkRElVff4HSutz+4CpEQe5cvefgojBCT63Yx9HV3el7GRfAYeg
         1TB2kY86Wmklu0js5M9effsoQ4o9oeCHFkfuxsmNIWfm7Tv8oZcBaNXNPAOh7LS161BG
         jY04SqPUlZ7A6BpM6zlGKuHgGzUP7VVqDGnZ3dz1RlU3Zxxw8n4uGKBOfNLtODmhEJ4f
         6MBEnUlPjWz8OtzKkLipQ4+odVoXblSuHgs40Hju+fTZkkKuhfwQUOMky5oV/mKT8AK4
         PZ+nO0KfE1hHRdGO6EoMq0RYoij4XMProlcLm8qCWEPE8HqFkp03VnNgm+Vsfdx/FgzZ
         Zo2w==
MIME-Version: 1.0
X-Received: by 10.140.93.161 with SMTP id d30mr19563835qge.53.1406999592815;
 Sat, 02 Aug 2014 10:13:12 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 2 Aug 2014 10:13:12 -0700 (PDT)
Date: Sat, 2 Aug 2014 10:13:12 -0700
Message-ID: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
Subject: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b9776edd19704ffa89ecc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b9776edd19704ffa89ecc
Content-Type: text/plain; charset=UTF-8

Hi,

I have deployed spark stable 1.0.1 on the cluster but I have new code that
I added in mllib-1.1.0-SNAPSHOT.

I am trying to access the new code using spark-submit as follows:

spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
--executor-memory 16g --total-executor-cores 16 --jars
spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
--qpProblem 2 inputPath outputPath

I can see the jars are getting added to httpServer as expected:

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with
timestamp 1406998204236

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
1406998204237

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with timestamp
1406998204238

But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I think
it's picking up the mllib from cluster which is at 1.0.1...

Please help. I will ask for a PR tomorrow but internally we want to
generate results from the new code.

Thanks.

Deb

--001a113b9776edd19704ffa89ecc--

From dev-return-8682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 17:47:06 2014
Return-Path: <dev-return-8682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1C9D11645
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 17:47:06 +0000 (UTC)
Received: (qmail 97106 invoked by uid 500); 2 Aug 2014 17:47:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97025 invoked by uid 500); 2 Aug 2014 17:47:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97008 invoked by uid 99); 2 Aug 2014 17:47:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:47:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:47:02 +0000
Received: by mail-ie0-f175.google.com with SMTP id x19so7688727ier.20
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 10:46:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=I+O6eg5x1UhzbcTOqcsOPm61+PcIe4i3EUIo4oMj8O8=;
        b=ATHmKfQlraCqxhsyDWO+L2sS+pfbE8bu+H4WZDf6in+V7hfbp+OIhiqpeUR8mR79b4
         T9np1smOX/Zwi2RneD1AYdSCNmkgBX7/yO7p27MmwZ5pMfxIp+nJM9a/pnXciCrE5Ib1
         +1e5jxwe+kbLRDlcbDTwFXHuaKb22uuXGHEEajX4fPJ+Szal7mtFxIJTD3hEWi5wgUnm
         Wf6UMLet1nRcRtWj/RUJbjH7RtlVhqhhSYon5SCUzvxsDhiO+rNGYnDm3AMi72DA/kiw
         03aUs16cyN3DJcsZPPMz/D8o1WVhOn9+cH9TikZYpwa+3KIjWirNGxbSEWlaV5M9ye1E
         u/Kg==
MIME-Version: 1.0
X-Received: by 10.50.80.76 with SMTP id p12mr20264558igx.34.1407001597787;
 Sat, 02 Aug 2014 10:46:37 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Sat, 2 Aug 2014 10:46:37 -0700 (PDT)
In-Reply-To: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
Date: Sat, 2 Aug 2014 10:46:37 -0700
Message-ID: <CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You can try enabling "spark.files.userClassPathFirst". But I'm not
sure whether it could solve your problem. -Xiangrui

On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I have deployed spark stable 1.0.1 on the cluster but I have new code that
> I added in mllib-1.1.0-SNAPSHOT.
>
> I am trying to access the new code using spark-submit as follows:
>
> spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
> --executor-memory 16g --total-executor-cores 16 --jars
> spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
> --qpProblem 2 inputPath outputPath
>
> I can see the jars are getting added to httpServer as expected:
>
> 14/08/02 12:50:04 INFO SparkContext: Added JAR
> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with
> timestamp 1406998204236
>
> 14/08/02 12:50:04 INFO SparkContext: Added JAR
> file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
> 1406998204237
>
> 14/08/02 12:50:04 INFO SparkContext: Added JAR
> file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with timestamp
> 1406998204238
>
> But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I think
> it's picking up the mllib from cluster which is at 1.0.1...
>
> Please help. I will ask for a PR tomorrow but internally we want to
> generate results from the new code.
>
> Thanks.
>
> Deb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 17:54:31 2014
Return-Path: <dev-return-8683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 914C511659
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 17:54:31 +0000 (UTC)
Received: (qmail 9871 invoked by uid 500); 2 Aug 2014 17:54:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9811 invoked by uid 500); 2 Aug 2014 17:54:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9799 invoked by uid 99); 2 Aug 2014 17:54:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:54:30 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.53 as permitted sender)
Received: from [209.85.216.53] (HELO mail-qa0-f53.google.com) (209.85.216.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 17:54:27 +0000
Received: by mail-qa0-f53.google.com with SMTP id v10so5243881qac.26
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 10:54:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=NtrTZMpp56nko3XiHFcolwsf4h4KyXeU/oBq6Ds8eNI=;
        b=krvgGKrKTdvuKhMTMzredKloiuAe31eemU1J0dPcOsKvbczY7HXQ5qbXyjhpKd7dT+
         DHfBqdBW1+ruoXNeEVRBb3KiTj/zMGl3BngL2itbyqjWc0puX+qdXvsMVm3EIH3DqdQ0
         t2T5G4tjc/1248eDv+Fq96nBTP1okI1G2/EazzQY811PEaK3jtp82BFP+doeNnOLmuf5
         1w6ewkEmpA/gvcx3vBnJU9e7heo8EarrSWMrJy2fgx3ZkaDAxt+SDI3IcSdoGqDjt7Yo
         PgC9yb6wxKN2uCpngbmW7PtYv35IjgEDyyrwY9qm3daAdDatJHAhCevvC83bVvL6wZ3N
         Wv2A==
MIME-Version: 1.0
X-Received: by 10.224.156.194 with SMTP id y2mr20378583qaw.15.1407002042760;
 Sat, 02 Aug 2014 10:54:02 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 2 Aug 2014 10:54:02 -0700 (PDT)
In-Reply-To: <CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
Date: Sat, 2 Aug 2014 10:54:02 -0700
Message-ID: <CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01536e54f5070b04ffa930c6
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01536e54f5070b04ffa930c6
Content-Type: text/plain; charset=UTF-8

Let me try it...

Will this be fixed if I generate a assembly file with mllib-1.1.0 SNAPSHOT
jar and other dependencies with the rest of the application code ?



On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> You can try enabling "spark.files.userClassPathFirst". But I'm not
> sure whether it could solve your problem. -Xiangrui
>
> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi,
> >
> > I have deployed spark stable 1.0.1 on the cluster but I have new code
> that
> > I added in mllib-1.1.0-SNAPSHOT.
> >
> > I am trying to access the new code using spark-submit as follows:
> >
> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
> > --executor-memory 16g --total-executor-cores 16 --jars
> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
> > --qpProblem 2 inputPath outputPath
> >
> > I can see the jars are getting added to httpServer as expected:
> >
> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with
> > timestamp 1406998204236
> >
> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
> > 1406998204237
> >
> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
> timestamp
> > 1406998204238
> >
> > But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I
> think
> > it's picking up the mllib from cluster which is at 1.0.1...
> >
> > Please help. I will ask for a PR tomorrow but internally we want to
> > generate results from the new code.
> >
> > Thanks.
> >
> > Deb
>

--089e01536e54f5070b04ffa930c6--

From dev-return-8684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  2 18:13:24 2014
Return-Path: <dev-return-8684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8AAF1116A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Aug 2014 18:13:24 +0000 (UTC)
Received: (qmail 26276 invoked by uid 500); 2 Aug 2014 18:13:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26226 invoked by uid 500); 2 Aug 2014 18:13:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26199 invoked by uid 99); 2 Aug 2014 18:13:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 18:13:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Aug 2014 18:13:18 +0000
Received: by mail-ig0-f171.google.com with SMTP id l13so3238994iga.10
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 11:12:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=jcPcSTMcxDD6abalDlpOXoaJfL5JirmRVkLzv6LCkMc=;
        b=MJ3uKBnS5CDQd6gP8npH6apUnyqWdPv/Kbqc0Ztcvpe/0TxLTAM98I1t2fXpMHw+g3
         zjtTO3u+VsZXLQazAkkmXopBsD3EAT1P7Tr+TanAauv/8RxCXgN7rXfw4vB8VqPnQp+7
         kW3Lms8RniSnT8G+Ueq5EHbZUHjwx902zed8+s8Eu2n57fe6ZXtt5T5Vjtg5C4qH6j3Q
         Bc4q99MPRta1CsqGQHKeAM3OCser+DIACRHOkY/bjXh0E4NQroDZIJfvMG6NnTKkLeRm
         Fm/n7BCUUY7J2wkU2y4BNsTH/s8mekViHMC7EXeg0MFdjyfn/dw4jTysHNFdYTwcfs48
         1fVg==
MIME-Version: 1.0
X-Received: by 10.50.80.76 with SMTP id p12mr20439003igx.34.1407003173777;
 Sat, 02 Aug 2014 11:12:53 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Sat, 2 Aug 2014 11:12:53 -0700 (PDT)
In-Reply-To: <CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
Date: Sat, 2 Aug 2014 11:12:53 -0700
Message-ID: <CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, that should work. spark-mllib-1.1.0 should be compatible with
spark-core-1.0.1.

On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Let me try it...
>
> Will this be fixed if I generate a assembly file with mllib-1.1.0 SNAPSHOT
> jar and other dependencies with the rest of the application code ?
>
>
>
> On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> You can try enabling "spark.files.userClassPathFirst". But I'm not
>> sure whether it could solve your problem. -Xiangrui
>>
>> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi,
>> >
>> > I have deployed spark stable 1.0.1 on the cluster but I have new code
>> > that
>> > I added in mllib-1.1.0-SNAPSHOT.
>> >
>> > I am trying to access the new code using spark-submit as follows:
>> >
>> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
>> > --executor-memory 16g --total-executor-cores 16 --jars
>> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
>> > --qpProblem 2 inputPath outputPath
>> >
>> > I can see the jars are getting added to httpServer as expected:
>> >
>> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with
>> > timestamp 1406998204236
>> >
>> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
>> > 1406998204237
>> >
>> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
>> > timestamp
>> > 1406998204238
>> >
>> > But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I
>> > think
>> > it's picking up the mllib from cluster which is at 1.0.1...
>> >
>> > Please help. I will ask for a PR tomorrow but internally we want to
>> > generate results from the new code.
>> >
>> > Thanks.
>> >
>> > Deb
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 01:05:10 2014
Return-Path: <dev-return-8685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3839111BD3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 01:05:10 +0000 (UTC)
Received: (qmail 46622 invoked by uid 500); 3 Aug 2014 01:05:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46567 invoked by uid 500); 3 Aug 2014 01:05:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46549 invoked by uid 99); 3 Aug 2014 01:05:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 01:05:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 01:05:03 +0000
Received: by mail-oa0-f42.google.com with SMTP id n16so4088044oag.29
        for <dev@spark.apache.org>; Sat, 02 Aug 2014 18:04:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=GMGsvjl71T7srRGSSseknXB0NfqWx8pJSDkly+8kNHY=;
        b=uPn4UHlQ8txCSHl4SUqowqk5uJ61OUvr/EGY9dK1upQsTu2sYpK9fPSsTKqfAEqIbz
         dVN7Ec+e54hiD77YxagFzluDWhlxECYmji93YZCkXDUhzJu2X2Uwg/4B7zTfe4Wj6KLh
         mU6cWm2/bCOY02EvRoj3aHjmplY++mR3tpducgApuKGc7KiODjkZbB2cC6n/RrcboToU
         rSM2uSp3f7dHWCq54+ssm1QlOkjuaVKL04eDzllE0WZRMRTrL1rBaupKavRJ4AEysQyK
         Oz9Vz8l96Va50CghJLq8v5hvSUlJjOvMLC615aMDPcDxAKCvO/Q4QV5uqTLx5J8fqZ8e
         mGkw==
MIME-Version: 1.0
X-Received: by 10.60.33.35 with SMTP id o3mr21386181oei.7.1407027883124; Sat,
 02 Aug 2014 18:04:43 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.202.226.147 with HTTP; Sat, 2 Aug 2014 18:04:43 -0700 (PDT)
Date: Sat, 2 Aug 2014 18:04:43 -0700
X-Google-Sender-Auth: 3ZfGN7GQhhoYx2C93YZT6OXuQpg
Message-ID: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
Subject: Scala 2.11 external dependencies
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011842a629aea204ffaf35b7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011842a629aea204ffaf35b7
Content-Type: text/plain; charset=UTF-8

We are currently blocked on non availability of the following external
dependencies for porting Spark to Scala 2.11 [SPARK-1812 Jira]:

- akka-*_2.11 (2.3.4-shaded-protobuf from org.spark-project). The shaded
protobuf needs to be 2.5.0, and the shading is needed because Hadoop1
specifically needs protobuf 2.4. Issues arising because of this
incompatibility is already explained in SPARK-1812 Jira.

- chill_2.11 (0.4 from com.twitter) for core
- algebird_2.11 (0.7 from com.twitter) for examples
- kafka_2.11 (0.8 from org.apache) for external/kafka and examples
- akka-zeromq_2.11 (2.3.4 from com.typesafe, but probably not needed if a
shaded-protobuf version is released from org.spark-project)

First,
Who do I pester to get org.spark-project artifacts published for the akka
shaded-protobuf version?

Second,
In the past what has been the convention to request/pester external
projects to re-release artifacts in a new Scala version?

Thanks!

--089e011842a629aea204ffaf35b7--

From dev-return-8686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 09:04:03 2014
Return-Path: <dev-return-8686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50D2111197
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 09:04:03 +0000 (UTC)
Received: (qmail 67105 invoked by uid 500); 3 Aug 2014 09:04:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67041 invoked by uid 500); 3 Aug 2014 09:04:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67029 invoked by uid 99); 3 Aug 2014 09:04:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:04:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kitaev@126.com designates 220.181.15.35 as permitted sender)
Received: from [220.181.15.35] (HELO m15-35.126.com) (220.181.15.35)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:04:00 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=126.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=v9ha/
	PUNVJoaUHBIYhjiGxaBNsJ/jhKSHG7YReNzcNM=; b=C1MCQxOiFr+DdcL5sdFSR
	BMXW6uvqFxYaMHmwsYq7TtmcRUQMz0eAErj3RMV2tghsoBD4eFNy9Zdg5NYq324B
	lJb6b/SQN2fOUYOhHG6zc2WN6LGxQ5tl/WXZXGc7awSqIKK5rqZhdw/h3iCd7aoF
	GcoHdMWp6Zu6CmUH2/4UKI=
Received: from kitaev$126.com ( [116.230.9.45] ) by ajax-webmail-wmsvr35
 (Coremail) ; Sun, 3 Aug 2014 17:03:31 +0800 (CST)
X-Originating-IP: [116.230.9.45]
Date: Sun, 3 Aug 2014 17:03:31 +0800 (CST)
From: jun <kitaev@126.com>
To: dev-spark <dev@spark.apache.org>
Subject: Intellij IDEA can not recognize the MLlib package
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20140422(26738.6344) Copyright (c) 2002-2014 www.mailtech.cn 126com
X-CM-CTRLDATA: mkCSa2Zvb3Rlcl9odG09MzI2NDo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_62015_1197251378.1407056611431"
MIME-Version: 1.0
Message-ID: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
X-CM-TRANSID:I8qowEB5T0Dk+t1TDeMNAA--.3055W
X-CM-SenderInfo: 5nlwtvby6rjloofrz/1tbiZh3cxEm+YcvkQQAAsb
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_62015_1197251378.1407056611431
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: base64

SGksCgoKSSBoYXZlIHN0YXJ0ZWQgbXkgc3BhcmsgZXhwbG9yYXRpb24gaW4gaW50ZWxsaWogSURF
QSBsb2NhbCBtb2RlbCBhbmQgd2FudCB0byBmb2N1cyBvbiBNTGxpYiBwYXJ0LgpidXQgd2hlbiBJ
IHB1dCBzb21lIGV4YW1wbGUgY29kZXMgaW4gSURFQSwgSXQgY2FuIG5vdCByZWNvZ25pemUgbWxs
aWIgcGFja2FnZSwganVzdCBsb29zIGxpa2UgdGhhdDoKCgo+Cj4gaW1wb3J0IG9yZy5hcGFjaGUu
c3BhcmsuU3BhcmtDb250ZXh0Cj5pbXBvcnQgb3JnLmFwYWNoZS5zcGFyay5tbGxpYi5yZWNvbW1l
bmRhdGlvbi5BTFMKPgoKCkkgaGF2YSBjb25maWd1cmVkIHRoZSBicmVlemUgaW4gYnVpbGQuc2J0
IGZpbGUgYW5kIGFsc28gaW5zdGFsbCB0aGUgbWluZ3cgZ2NjICYgZ2ZvcnRyYW4gbGliLiBIZXJl
IGlzIG15IGJ1aWxkLnNidDoKCgo+Pj4+IGJ1aWxkLnNidCA8PDw8Cm5hbWUgOj0gIlNwYXJrTUxs
aWJMb2NhbCIgdmVyc2lvbiA6PSAiMS4wIiByZXNvbHZlcnMgKz0gIk9veWFsYSBCaW50cmF5IiBh
dCAiaHR0cDovL2RsLmJpbnRyYXkuY29tL29veWFsYS9tYXZlbiIgcmVzb2x2ZXJzICs9ICJBa2th
IFJlcG9zaXRvcnkiIGF0ICJodHRwOi8vcmVwby5ha2thLmlvL3JlbGVhc2VzLyIgbGlicmFyeURl
cGVuZGVuY2llcyArPSAib295YWxhLmNuZCIgJSAiam9iLXNlcnZlciIgJSAiMC4zLjEiICUgInBy
b3ZpZGVkIiBsaWJyYXJ5RGVwZW5kZW5jaWVzICs9ICJjb20uZ2l0aHViLmZvbW1pbC5uZXRsaWIi
ICUgImFsbCIgJSAiMS4xLjIiIGxpYnJhcnlEZXBlbmRlbmNpZXMgKz0gIm9yZy5hcGFjaGUuc3Bh
cmsiICUlICJzcGFyay1jb3JlIiAlICIxLjAuMCIgbGlicmFyeURlcGVuZGVuY2llcyArKz0gU2Vx
KCAib3JnLnNjYWxhbmxwIiAlJSAiYnJlZXplIiAlICIwLjguMSIsICJvcmcuc2NhbGFubHAiICUl
ICJicmVlemUtbmF0aXZlcyIgJSAiMC44LjEiICkgcmVzb2x2ZXJzICsrPSBTZXEoICJTb25hdHlw
ZSBTbmFwc2hvdHMiIGF0ICJodHRwczovL29zcy5zb25hdHlwZS5vcmcvY29udGVudC9yZXBvc2l0
b3JpZXMvc25hcHNob3RzLyIsICJTb25hdHlwZSBSZWxlYXNlcyIgYXQgImh0dHBzOi8vb3NzLnNv
bmF0eXBlLm9yZy9jb250ZW50L3JlcG9zaXRvcmllcy9yZWxlYXNlcy8iICkgc2NhbGFWZXJzaW9u
IDo9ICIyLjEwLjMiCj4+Pj4gRW5kIDw8PDwKCgpJcyB0aGVyZSBhbnl0aGluZyBJIG1pc3NlZD8K
CgpCUgpLaXRhZXY=
------=_Part_62015_1197251378.1407056611431--


From dev-return-8687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 09:07:28 2014
Return-Path: <dev-return-8687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 236AB111A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 09:07:28 +0000 (UTC)
Received: (qmail 69521 invoked by uid 500); 3 Aug 2014 09:07:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69452 invoked by uid 500); 3 Aug 2014 09:07:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69440 invoked by uid 99); 3 Aug 2014 09:07:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:07:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kitaev@126.com designates 220.181.15.35 as permitted sender)
Received: from [220.181.15.35] (HELO m15-35.126.com) (220.181.15.35)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:07:20 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=126.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=hNdPb
	thls1r56w/DsAP3Wp/uEzvzAd0adi89ggaTNFw=; b=GATN9PsuW9B672+2/u0kI
	jdsKacQo9rDqL3hpGyHwnTxOFAIb7ukw+qKYLeFsfnr9/Tl3UuCqcR3KGzKx9ivB
	804qH9v4BifbOGrzlXkIAvuXPjO7OWhF7g/k5n236d3ZZSIAK3rUuwQiIgG1ngDm
	SXkj9VL3/4HLwhdtEwbwUs=
Received: from kitaev$126.com ( [116.230.9.45] ) by ajax-webmail-wmsvr35
 (Coremail) ; Sun, 3 Aug 2014 17:06:58 +0800 (CST)
X-Originating-IP: [116.230.9.45]
Date: Sun, 3 Aug 2014 17:06:58 +0800 (CST)
From: jun  <kitaev@126.com>
To: dev-spark <dev@spark.apache.org>
Subject: Re:Intellij IDEA can not recognize the MLlib package
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20140422(26738.6344) Copyright (c) 2002-2014 www.mailtech.cn 126com
In-Reply-To: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
References: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
X-CM-CTRLDATA: aGyHzGZvb3Rlcl9odG09MTcxNjo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_62462_1581075575.1407056818547"
MIME-Version: 1.0
Message-ID: <2e75577.3e2b.1479b1f3174.Coremail.kitaev@126.com>
X-CM-TRANSID:I8qowEApaUKy+91Tw+MNAA--.9182W
X-CM-SenderInfo: 5nlwtvby6rjloofrz/1tbiDw3cxE0vUwWXKgABsS
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_62462_1581075575.1407056818547
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: base64

U29ycnkgdGhlIGNvbG9yIGlzIG1pc3NpbmcuIHRoZSAibWxsaWIiIGlzIHJlZCB3b3JkIGFuZCAi
aW1wb3J0IiBzZW50ZW5jZSBpcyBncmV5Lj4+aW1wb3J0IG9yZy5hcGFjaGUuc3BhcmsubWxsaWIu
cmVjb21tZW5kYXRpb24uQUxTCgpBdCAyMDE0LTA4LTAzIDA1OjAzOjMxLCBqdW4iIDxraXRhZXZA
MTI2LmNvbT4gd3JvdGU6ID5IaSwgPiA+ID5JIGhhdmUgc3RhcnRlZCBteSBzcGFyayBleHBsb3Jh
dGlvbiBpbiBpbnRlbGxpaiBJREVBIGxvY2FsIG1vZGVsIGFuZCB3YW50IHRvIGZvY3VzIG9uIE1M
bGliIHBhcnQuID5idXQgd2hlbiBJIHB1dCBzb21lIGV4YW1wbGUgY29kZXMgaW4gSURFQSwgSXQg
Y2FuIG5vdCByZWNvZ25pemUgbWxsaWIgcGFja2FnZSwganVzdCBsb29zIGxpa2UgdGhhdDogPiA+
ID4+ID4+IGltcG9ydCBvcmcuYXBhY2hlLnNwYXJrLlNwYXJrQ29udGV4dCA+PmltcG9ydCBvcmcu
YXBhY2hlLnNwYXJrLm1sbGliLnJlY29tbWVuZGF0aW9uLkFMUyA+PiA+ID4gPkkgaGF2YSBjb25m
aWd1cmVkIHRoZSBicmVlemUgaW4gYnVpbGQuc2J0IGZpbGUgYW5kIGFsc28gaW5zdGFsbCB0aGUg
bWluZ3cgZ2NjICYgZ2ZvcnRyYW4gbGliLiBIZXJlIGlzIG15IGJ1aWxkLnNidDogPiA+ID4+Pj4+
IGJ1aWxkLnNidCA8PDw8ID5uYW1lIDo9ICJTcGFya01MbGliTG9jYWwiIHZlcnNpb24gOj0gIjEu
MCIgcmVzb2x2ZXJzICs9ICJPb3lhbGEgQmludHJheSIgYXQgImh0dHA6Ly9kbC5iaW50cmF5LmNv
bS9vb3lhbGEvbWF2ZW4iIHJlc29sdmVycyArPSAiQWtrYSBSZXBvc2l0b3J5IiBhdCAiaHR0cDov
L3JlcG8uYWtrYS5pby9yZWxlYXNlcy8iIGxpYnJhcnlEZXBlbmRlbmNpZXMgKz0gIm9veWFsYS5j
bmQiICUgImpvYi1zZXJ2ZXIiICUgIjAuMy4xIiAlICJwcm92aWRlZCIgbGlicmFyeURlcGVuZGVu
Y2llcyArPSAiY29tLmdpdGh1Yi5mb21taWwubmV0bGliIiAlICJhbGwiICUgIjEuMS4yIiBsaWJy
YXJ5RGVwZW5kZW5jaWVzICs9ICJvcmcuYXBhY2hlLnNwYXJrIiAlJSAic3BhcmstY29yZSIgJSAi
MS4wLjAiIGxpYnJhcnlEZXBlbmRlbmNpZXMgKys9IFNlcSggIm9yZy5zY2FsYW5scCIgJSUgImJy
ZWV6ZSIgJSAiMC44LjEiLCAib3JnLnNjYWxhbmxwIiAlJSAiYnJlZXplLW5hdGl2ZXMiICUgIjAu
OC4xIiApIHJlc29sdmVycyArKz0gU2VxKCAiU29uYXR5cGUgU25hcHNob3RzIiBhdCAiaHR0cHM6
Ly9vc3Muc29uYXR5cGUub3JnL2NvbnRlbnQvcmVwb3NpdG9yaWVzL3NuYXBzaG90cy8iLCAiU29u
YXR5cGUgUmVsZWFzZXMiIGF0ICJodHRwczovL29zcy5zb25hdHlwZS5vcmcvY29udGVudC9yZXBv
c2l0b3JpZXMvcmVsZWFzZXMvIiApIHNjYWxhVmVyc2lvbiA6PSAiMi4xMC4zIiA+Pj4+PiBFbmQg
PDw8PCA+ID4gPklzIHRoZXJlIGFueXRoaW5nIEkgbWlzc2VkPyA+ID4gPkJSID5LaXRhZXY=
------=_Part_62462_1581075575.1407056818547--


From dev-return-8688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 09:07:44 2014
Return-Path: <dev-return-8688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13C1B111A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 09:07:44 +0000 (UTC)
Received: (qmail 70621 invoked by uid 500); 3 Aug 2014 09:07:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70558 invoked by uid 500); 3 Aug 2014 09:07:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70547 invoked by uid 99); 3 Aug 2014 09:07:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:07:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.169 as permitted sender)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:07:42 +0000
Received: by mail-vc0-f169.google.com with SMTP id le20so9186505vcb.14
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 02:07:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=+ABFrtGj7P5xLSigZEzRC1CChYXyCwemwXRWvkA5XeY=;
        b=IU92mkqIKahyEeJBbuERbFI8ojs09C4tWWRjgxptYlPC0uz9l5rWuolodAjDXBLFxu
         9sNGt+meGFRfhingmZfX/Hf/yhjKzy+iMj0D9ViSeG5Uqc9weEA+gmtcgoehkHZlrCWZ
         O3i1qU3PAsm8W/vhNSmZfST+C/A2UjLbCPa8Tbmv4N7aV/LmTRAcPD3CvDORHlB7ZOEd
         P94VnDbkN5Rs831z60yeI6HRCmXa7m2QNUr6wUJkICJ1KfdYrQbJHcvWRA6uCRsmiVpV
         MP+hqZG4cFZm71GaKljBHzqZEOYZBztE3a+Kzb3D6z4HMfOQCuFPHWAn1ebnLvJ13Sut
         wERw==
X-Gm-Message-State: ALoCoQlxMg8uL/JjRCraMGUUl9aNkQmN4Xdl3Y7fM/RGhARGaY+rm6u8df7dscOXXMKZ0ysXbzJo
X-Received: by 10.52.53.33 with SMTP id y1mr10424135vdo.31.1407056837165; Sun,
 03 Aug 2014 02:07:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Sun, 3 Aug 2014 02:06:57 -0700 (PDT)
In-Reply-To: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
References: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 3 Aug 2014 10:06:57 +0100
Message-ID: <CAMAsSdK73hCkhTMgULpMtv-aOGn0Yz2gx_q3Xa5YhtS65jFX8Q@mail.gmail.com>
Subject: Re: Intellij IDEA can not recognize the MLlib package
To: jun <kitaev@126.com>
Cc: dev-spark <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

You missed the mllib artifact? that would certainly explain it! all I
see is core.

On Sun, Aug 3, 2014 at 10:03 AM, jun <kitaev@126.com> wrote:
> Hi,
>
>
> I have started my spark exploration in intellij IDEA local model and want=
 to focus on MLlib part.
> but when I put some example codes in IDEA, It can not recognize mllib pac=
kage, just loos like that:
>
>
>>
>> import org.apache.spark.SparkContext
>>import org.apache.spark.mllib.recommendation.ALS
>>
>
>
> I hava configured the breeze in build.sbt file and also install the mingw=
 gcc & gfortran lib. Here is my build.sbt:
>
>
>>>>> build.sbt <<<<
> name :=3D "SparkMLlibLocal" version :=3D "1.0" resolvers +=3D "Ooyala Bin=
tray" at "http://dl.bintray.com/ooyala/maven" resolvers +=3D "Akka Reposito=
ry" at "http://repo.akka.io/releases/" libraryDependencies +=3D "ooyala.cnd=
" % "job-server" % "0.3.1" % "provided" libraryDependencies +=3D "com.githu=
b.fommil.netlib" % "all" % "1.1.2" libraryDependencies +=3D "org.apache.spa=
rk" %% "spark-core" % "1.0.0" libraryDependencies ++=3D Seq( "org.scalanlp"=
 %% "breeze" % "0.8.1", "org.scalanlp" %% "breeze-natives" % "0.8.1" ) reso=
lvers ++=3D Seq( "Sonatype Snapshots" at "https://oss.sonatype.org/content/=
repositories/snapshots/", "Sonatype Releases" at "https://oss.sonatype.org/=
content/repositories/releases/" ) scalaVersion :=3D "2.10.3"
>>>>> End <<<<
>
>
> Is there anything I missed?
>
>
> BR
> Kitaev

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 09:33:40 2014
Return-Path: <dev-return-8689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2CCF711200
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 09:33:40 +0000 (UTC)
Received: (qmail 97318 invoked by uid 500); 3 Aug 2014 09:33:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97276 invoked by uid 500); 3 Aug 2014 09:33:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97263 invoked by uid 99); 3 Aug 2014 09:33:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:33:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kitaev@126.com designates 220.181.15.35 as permitted sender)
Received: from [220.181.15.35] (HELO m15-35.126.com) (220.181.15.35)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 09:33:36 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=126.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=LtRAd
	iLimaL9szEGfNxg7V0BQbvlROq9z0Hvp7G9AAM=; b=TXnr2DcCz7TMiyw5Zd1Z+
	aWwRmzWUcFW/6yC71/u3BexZGF1WXpW2yLtbuV3uJMvhTz8270qSr9ud61ZgEWbt
	85lLUV7nkbkchY34L0G1g0XoZ707xYlR9nKetjpi78NRqv13aM426Mp9Ge059TV1
	lvX7aZG+kRg1O9SG/KFa8g=
Received: from kitaev$126.com ( [116.230.9.45] ) by ajax-webmail-wmsvr35
 (Coremail) ; Sun, 3 Aug 2014 17:33:05 +0800 (CST)
X-Originating-IP: [116.230.9.45]
Date: Sun, 3 Aug 2014 17:33:05 +0800 (CST)
From: jun  <kitaev@126.com>
To: "Sean Owen" <sowen@cloudera.com>
Cc: dev-spark <dev@spark.apache.org>
Subject: Re:Re: Intellij IDEA can not recognize the MLlib package
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20140422(26738.6344) Copyright (c) 2002-2014 www.mailtech.cn 126com
In-Reply-To: <CAMAsSdKcsKNvAewMrZYNG5DhSxeH6GycXtHt6FdQBqEt38kySQ@mail.gmail.com>
References: <e867128.3dba.1479b1c0867.Coremail.kitaev@126.com>
 <2e75577.3e2b.1479b1f3174.Coremail.kitaev@126.com>
 <CAMAsSdKcsKNvAewMrZYNG5DhSxeH6GycXtHt6FdQBqEt38kySQ@mail.gmail.com>
X-CM-CTRLDATA: 7wSoT2Zvb3Rlcl9odG09MjEzMDo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_66131_255628648.1407058385518"
MIME-Version: 1.0
Message-ID: <78d349e2.41dd.1479b371a6e.Coremail.kitaev@126.com>
X-CM-TRANSID:I8qowECZ2ULSAd5TLOkNAA--.15083W
X-CM-SenderInfo: 5nlwtvby6rjloofrz/1tbiDx3cxE0vUwXGyAAAsw
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_66131_255628648.1407058385518
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: base64
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==

------=_Part_66131_255628648.1407058385518--


From dev-return-8690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug  3 20:36:09 2014
Return-Path: <dev-return-8690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 908C411A65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Aug 2014 20:36:09 +0000 (UTC)
Received: (qmail 77108 invoked by uid 500); 3 Aug 2014 20:36:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77040 invoked by uid 500); 3 Aug 2014 20:36:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77028 invoked by uid 99); 3 Aug 2014 20:36:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 20:36:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Aug 2014 20:36:04 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so6685146wgh.34
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 13:35:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=K3X3Wn5ej85Q1bpZ8Cna3NH+LudtgrSlvHn5H9r3Bh4=;
        b=SpYcrNbTBoDJHV+euIMudhcDHgLln5gSUrpwdakIG+M8NaR/ePgdKL34S0je8ILMKd
         XsufC8cXbMtdNYCEz6ttALc0GQnpWWSJUXWO1WfZ7vNHB2dnHFGSB3nJdUH0KoWZkYeK
         r/HVWetjLVdhkAIb/9aB5kj2P5tVeVNNxdFRBbx6YSr45UomGClZPzfnoxLDi+F16XoA
         Hc1E93K5h5rD/uR3Xf8ksatoBDdMET/N1US9wL4PKYTMFmv/qatNqiiVP8LlYq7MvH/K
         R5b6e0IJqw+NAkV2c2LZwATCkf49iA3GbKRKkwEy5kBD2zEcvVLdBhxFArd8N12zBre0
         7Mdw==
X-Received: by 10.194.6.134 with SMTP id b6mr26418845wja.64.1407098142991;
 Sun, 03 Aug 2014 13:35:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sun, 3 Aug 2014 13:35:02 -0700 (PDT)
In-Reply-To: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 3 Aug 2014 16:35:02 -0400
Message-ID: <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
Subject: Re: -1s on pull requests?
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b450116fa2aef04ffbf909d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b450116fa2aef04ffbf909d
Content-Type: text/plain; charset=UTF-8

On Mon, Jul 21, 2014 at 4:44 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
wrote:

> This also happens when something accidentally gets merged after the tests
> have started but before tests have passed.
>

Some improvements to SparkQA <https://github.com/SparkQA> could help with
this. May I suggest:

   1. Include the commit hash in the "tests have started/completed"
   messages, so that it's clear what code exactly is/has been tested for each
   test cycle.
   2. "Pin" a message to the start or end of the PR that is updated with
   the status of the PR. "Testing not complete"; "New commits since last
   test"; "Tests failed"; etc. It should be easy for committers to get the
   status of the PR at a glance, without scrolling through the comment history.

Nick

--047d7b450116fa2aef04ffbf909d--

From dev-return-8691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 00:14:23 2014
Return-Path: <dev-return-8691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0EE5E11D02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 00:14:23 +0000 (UTC)
Received: (qmail 9813 invoked by uid 500); 4 Aug 2014 00:14:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9758 invoked by uid 500); 4 Aug 2014 00:14:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9745 invoked by uid 99); 4 Aug 2014 00:14:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 00:14:22 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of SRS0=O.ztoP=46=nirvana-international.com=pritish@eigbox.net designates 66.96.190.8 as permitted sender)
Received: from [66.96.190.8] (HELO bosmailout08.eigbox.net) (66.96.190.8)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 00:14:20 +0000
Received: from bosmailscan11.eigbox.net ([10.20.15.11])
	by bosmailout08.eigbox.net with esmtp (Exim)
	id 1XE5uU-0008Gp-Vo
	for dev@spark.apache.org; Sun, 03 Aug 2014 20:13:54 -0400
Received: from [10.115.3.32] (helo=bosimpout12)
	by bosmailscan11.eigbox.net with esmtp (Exim)
	id 1XE5uU-0004YS-TA
	for dev@spark.apache.org; Sun, 03 Aug 2014 20:13:54 -0400
Received: from bosoxweb05.eigbox.net ([10.20.16.132])
	by bosimpout12 with 
	id aQDr1o0062qxdGy01QDu9u; Sun, 03 Aug 2014 20:13:54 -0400
X-Authority-Analysis: v=2.1 cv=UPBTHUvy c=1 sm=1 tr=0
 a=27UItO6z3w1W2TFkx1gH7Q==:117 a=QPcu4mC3AAAA:8 a=Pi6aMINpWpYA:10
 a=fnPIPC-HWbsA:10 a=r5dXRqAEvkAA:10 a=ctHiq1sxn34A:10 a=wPDyFdB5xvgA:10
 a=IkcTkHD0fZMA:10 a=RYXLKYPDAAAA:8 a=BRBGA3hWAAAA:8 a=ZSg0DG7QbRxzApuOB34A:9
 a=QEXdDO2ut3YA:10 a=NqPMGhbpAn0A:10 a=QvPcIV659ggA:10
Received: from localhost ([127.0.0.1] helo=bosoxweb05.eigbox.net)
	by bosoxweb05.eigbox.net with esmtp (Exim 4.72)
	(envelope-from <pritish@nirvana-international.com>)
	id 1XE5uR-000271-MR
	for dev@spark.apache.org; Sun, 03 Aug 2014 20:13:51 -0400
Date: Sun, 3 Aug 2014 20:13:51 -0400 (EDT)
From: pritish <pritish@nirvana-international.com>
Reply-To: pritish <pritish@nirvana-international.com>
To: dev@spark.apache.org
Message-ID: <150926535.187806.1407111231694.open-xchange@bosoxweb05.eigbox.net>
In-Reply-To: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
References: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
Subject: I would like to contribute
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Priority: 3
Importance: Medium
X-Mailer: Open-Xchange Mailer v7.4.2-Rev20
X-Originating-IP: 98.169.89.152
X-Virus-Checked: Checked by ClamAV on apache.org

Hi

We would like to contribute to Spark but we are not sure how. We can offer
project management, release management to begin with. Please advice on how to
get engaged.

Thank you!!

Regards
Pritish
Nirvana International Inc.
Big Data, Hadoop, Oracle EBS and IT Solutions
VA - SWaM, MD - MBE Certified Company
pritish@nirvana-international.com
http://www.nirvana-international.com

> On August 2, 2014 at 9:04 PM Anand Avati <avati@gluster.org> wrote:
>
>
> We are currently blocked on non availability of the following external
> dependencies for porting Spark to Scala 2.11 [SPARK-1812 Jira]:
>
> - akka-*_2.11 (2.3.4-shaded-protobuf from org.spark-project). The shaded
> protobuf needs to be 2.5.0, and the shading is needed because Hadoop1
> specifically needs protobuf 2.4. Issues arising because of this
> incompatibility is already explained in SPARK-1812 Jira.
>
> - chill_2.11 (0.4 from com.twitter) for core
> - algebird_2.11 (0.7 from com.twitter) for examples
> - kafka_2.11 (0.8 from org.apache) for external/kafka and examples
> - akka-zeromq_2.11 (2.3.4 from com.typesafe, but probably not needed if a
> shaded-protobuf version is released from org.spark-project)
>
> First,
> Who do I pester to get org.spark-project artifacts published for the akka
> shaded-protobuf version?
>
> Second,
> In the past what has been the convention to request/pester external
> projects to re-release artifacts in a new Scala version?
>
> Thanks!

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 00:24:20 2014
Return-Path: <dev-return-8692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 60F8E11D20
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 00:24:20 +0000 (UTC)
Received: (qmail 15814 invoked by uid 500); 4 Aug 2014 00:24:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15754 invoked by uid 500); 4 Aug 2014 00:24:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15742 invoked by uid 99); 4 Aug 2014 00:24:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 00:24:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.160.174 as permitted sender)
Received: from [209.85.160.174] (HELO mail-yk0-f174.google.com) (209.85.160.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 00:24:15 +0000
Received: by mail-yk0-f174.google.com with SMTP id q9so3719107ykb.5
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 17:23:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=6svX8YS+zyOh6S0JGfNthdcsKcT6etF7Z0VDqz4sUS4=;
        b=vX7Q/0N7Xwc/bPlYxkKG17nI9/1PGNbvwb8RfilXkSTv7oLqsGV6tsVHzFlDQ43zN1
         kFi47bYw1FjlyzTjiJGIkLD9I7RJk/t+sK6FQXIXWvFVGewdF6xrpOUDKKSvr86EoEdJ
         wHKN4sC3L8W51naknG/5LloQIsUStrsT2XdYwB81bEhDQRmBak7kOzFfWcjfvlaSTk96
         JOGMkHwVbTHXD4CVDtNFfIiFvvrZOsW1WblnvCZvEIksyo8k30pA10YC1fHLc7oSeKcD
         OgJWba1o1TA9zeKLclLVFdxIfgqp6sFpNmScjzCp6VyXUjzYoJAOIjEzVIRP7SJqKP1W
         /n4A==
X-Received: by 10.236.62.130 with SMTP id y2mr33940571yhc.47.1407111834158;
        Sun, 03 Aug 2014 17:23:54 -0700 (PDT)
Received: from joshs-mbp.att.net ([2602:306:cdd1:b10:a4bc:1caa:81c9:4ac7])
        by mx.google.com with ESMTPSA id p55sm27228507yhh.34.2014.08.03.17.23.50
        for <dev@spark.apache.org>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Sun, 03 Aug 2014 17:23:51 -0700 (PDT)
Date: Sun, 3 Aug 2014 17:23:49 -0700
From: Josh Rosen <rosenville@gmail.com>
To: dev@spark.apache.org
Message-ID: <etPan.53ded295.507ed7ab.9d@joshs-mbp.att.net>
In-Reply-To: <150926535.187806.1407111231694.open-xchange@bosoxweb05.eigbox.net>
References: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
 <150926535.187806.1407111231694.open-xchange@bosoxweb05.eigbox.net>
Subject: Re: I would like to contribute
X-Mailer: Airmail Beta (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53ded295_2eb141f2_9d"
X-Virus-Checked: Checked by ClamAV on apache.org

--53ded295_2eb141f2_9d
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

The Contributing to Spark guide on the Spark Wiki provides a good overview on how to start contributing:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


On August 3, 2014 at 5:14:23 PM, pritish (pritish@nirvana-international.com) wrote:

Hi 

We would like to contribute to Spark but we are not sure how. We can offer 
project management, release management to begin with. Please advice on how to 
get engaged. 

Thank you!! 

Regards 
Pritish 
Nirvana International Inc. 
Big Data, Hadoop, Oracle EBS and IT Solutions 
VA - SWaM, MD - MBE Certified Company 
pritish@nirvana-international.com 
http://www.nirvana-international.com 

> On August 2, 2014 at 9:04 PM Anand Avati <avati@gluster.org> wrote: 
> 
> 
> We are currently blocked on non availability of the following external 
> dependencies for porting Spark to Scala 2.11 [SPARK-1812 Jira]: 
> 
> - akka-*_2.11 (2.3.4-shaded-protobuf from org.spark-project). The shaded 
> protobuf needs to be 2.5.0, and the shading is needed because Hadoop1 
> specifically needs protobuf 2.4. Issues arising because of this 
> incompatibility is already explained in SPARK-1812 Jira. 
> 
> - chill_2.11 (0.4 from com.twitter) for core 
> - algebird_2.11 (0.7 from com.twitter) for examples 
> - kafka_2.11 (0.8 from org.apache) for external/kafka and examples 
> - akka-zeromq_2.11 (2.3.4 from com.typesafe, but probably not needed if a 
> shaded-protobuf version is released from org.spark-project) 
> 
> First, 
> Who do I pester to get org.spark-project artifacts published for the akka 
> shaded-protobuf version? 
> 
> Second, 
> In the past what has been the convention to request/pester external 
> projects to re-release artifacts in a new Scala version? 
> 
> Thanks! 

--------------------------------------------------------------------- 
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
For additional commands, e-mail: dev-help@spark.apache.org 


--53ded295_2eb141f2_9d--


From dev-return-8693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 00:41:13 2014
Return-Path: <dev-return-8693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B2F511D59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 00:41:13 +0000 (UTC)
Received: (qmail 27989 invoked by uid 500); 4 Aug 2014 00:41:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27932 invoked by uid 500); 4 Aug 2014 00:41:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 24115 invoked by uid 99); 4 Aug 2014 00:35:09 -0000
X-ASF-Spam-Status: No, hits=6.2 required=10.0
	tests=DCC_CHECK,HTML_MESSAGE,MIME_HTML_MOSTLY,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of guruyvs@yahoo.com designates 98.139.213.162 as permitted sender)
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 548693.91634.bm@omp1047.mail.bf1.yahoo.com
X-YMail-OSG: 4Dq6MPwVM1l0wdf9d1naBKeoq3Lq_d8iNvAaFxrfwZxjuU_
 CYmANfB7wBrBUc2CzwmxFVP_ya4GQ.50PheprgmP1QqqTRm0LOK323g62NkZ
 czPQS4BT49Dzfh07PkwaozvGEXMoeAy_Dbk7.xcqXiltToJTSBEFoel9XM6n
 RgkHzKFtkC0KNEwf8ELeFWV1DexVe5vZQ0.ATHVNrgZJ2uyeg3Or0W2bvBAz
 9f.bVRSttRv_ndjEkIQdU_UZ6kz5pKZFY_7mpkx8gS9957wypIEedx5D.4K6
 41Yu4WcgbaBVwZ4PIHqu064T6.k_XBS0yFLVE26vLiyiFbqVlB0dJMS6hhIP
 NqAiyXoICB.1eYy_xTNdGzH5YRJZ.ZOhHLYgGgZ.zgHu_FkpOmbls12bpyAK
 2t6CDENLZq5ZDCSI7w4.uyj1rYSpcDTRVzWNsh9gfUqF8hDfEmMug8m5pch2
 .mui.aFATTCLsDJl52oEQ_MsDhDAaP2VuXDnkk8VaT6VQG11KbC1p_W8xtAi
 Ki4IliJAczm_yg0GqnQ4AMUAlE7n2Hscm3k5Q56ZAVYL_vxU-
X-Rocket-MIMEInfo: 002.001,CgEwAQEBAQ--
X-Mailer: YahooMailWebService/0.8.198.689
Message-ID: <1407112479.65969.YahooMailNeo@web160404.mail.bf1.yahoo.com>
Date: Sun, 3 Aug 2014 17:34:39 -0700
From: Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>
Reply-To: Gurumurthy Yeleswarapu <guruyvs@yahoo.com>
Subject: (send this email to subscribe)
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="630850330-1850852545-1407112479=:65969"
X-Virus-Checked: Checked by ClamAV on apache.org

--630850330-1850852545-1407112479=:65969
Content-Type: text/plain; charset=us-ascii



--630850330-1850852545-1407112479=:65969--

From dev-return-8694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 02:05:41 2014
Return-Path: <dev-return-8694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 015BF11E8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 02:05:41 +0000 (UTC)
Received: (qmail 3854 invoked by uid 500); 4 Aug 2014 02:05:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3800 invoked by uid 500); 4 Aug 2014 02:05:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 32372 invoked by uid 99); 4 Aug 2014 00:45:40 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ausoniapositive@gmail.com designates 74.125.82.47 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=jymFXAmDrLkOpN0c9GnFClSy/8hHWLGtmcL6sR+b9gA=;
        b=y963d+g56CQY1500SleTdPNSStbYNyBXd/H/pzHXq3GdCBdpe+zRq3EzDOLmp7Sb23
         XSM0lICUh0+u5ZNakYZasjV21qoXiwDkfW1qp4vDRiCemFaiQcQTdxj/CoTgt7d47uWf
         +PAzgl3vSIedmzcclU6qt1mPOToPI+vQyu+0tR0cMzap6jAVhghV8j9LP6Rd26V12J1Q
         FReYMjJRfV6q9ADClsA3sOzpZWGfonye6IDdGjTAw+7/mpLPc3TRXnmygwrXpTaKHK1Y
         H/aFl6kk2o41+lqP9HaqOCnY9N2kUt/A2WCINHYZkI3tR2EOmeV0+VY135An5b//Aytf
         EA+w==
MIME-Version: 1.0
X-Received: by 10.180.39.33 with SMTP id m1mr25305265wik.82.1407113113819;
 Sun, 03 Aug 2014 17:45:13 -0700 (PDT)
Sender: ausoniapositive@gmail.com
Date: Mon, 4 Aug 2014 09:45:13 +0900
X-Google-Sender-Auth: OrnRxs4UjCOWLFlTssRS-n4sCDM
Message-ID: <CAETF4DG8mgVTTuA-3Cpy=G6wFuFOY_0mxXm2WazR8PYdSneRMw@mail.gmail.com>
Subject: spark
From: =?UTF-8?B?7KGw7J247IiY?= <ausonia@cyberdigm.co.kr>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134d9324ee29f04ffc30df0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d9324ee29f04ffc30df0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: base64
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--001a1134d9324ee29f04ffc30df0--

From dev-return-8695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 03:29:41 2014
Return-Path: <dev-return-8695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45B111101E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 03:29:41 +0000 (UTC)
Received: (qmail 4336 invoked by uid 500); 4 Aug 2014 03:29:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4278 invoked by uid 500); 4 Aug 2014 03:29:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4263 invoked by uid 99); 4 Aug 2014 03:29:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:29:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:29:38 +0000
Received: by mail-oa0-f52.google.com with SMTP id o6so4640489oag.11
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 20:29:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ST9NE8tagiWOBAhD5HBuAJFwpNvdYPETIdHgRYBX+ME=;
        b=Wjq/zudQWAXT/7+BDr1fX5qxYtq9jcKme439O1Avgbkn9VI71d2Zio/nZS8J9viofr
         FCFyB3bDK9loiDDLMoQDro5v+Db8qn/9Z3B0uQUoZhPfFc5dSM199mY5wfbi0BwIPwsz
         6rtGnmnGUcXTKJQn5C4FjMiYqLo2ecVHCwqM0uZPTBMplfCh4NqMI8j/Lv6Ob9A0PvXo
         25wzfd1NHMN8A1bXdwG4nr8bdD0Atl97ug8tA0R0CQKx+OgTGyAurefa4QVmBq+56wve
         T076RWBPpGH8Ruh7EVFo60h2KqaLPh8SjmYthh97rcUtuXYbRWCnFeplUvgVoJn+2C5I
         6zlw==
MIME-Version: 1.0
X-Received: by 10.182.226.202 with SMTP id ru10mr486324obc.77.1407122953278;
 Sun, 03 Aug 2014 20:29:13 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 3 Aug 2014 20:29:13 -0700 (PDT)
In-Reply-To: <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
	<CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
Date: Sun, 3 Aug 2014 20:29:13 -0700
Message-ID: <CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c32efac91d2604ffc55797
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c32efac91d2604ffc55797
Content-Type: text/plain; charset=ISO-8859-1

>
>    1. Include the commit hash in the "tests have started/completed"
>    messages, so that it's clear what code exactly is/has been tested for
> each
>    test cycle.
>

Great idea - I think this is easy to do given the current architecture. We
already have access to the commit ID in the same script that posts the
comments.

   2. "Pin" a message to the start or end of the PR that is updated with
>    the status of the PR. "Testing not complete"; "New commits since last
>    test"; "Tests failed"; etc. It should be easy for committers to get the
>    status of the PR at a glance, without scrolling through the comment
> history.
>

This also is a good idea - I think this would be doable since the github
API allows us to edit comments, but it's a bit tricker. I think it would
require first making an API call to get the "status comment" ID and then
updating it.


>
> Nick
>

Nick - Any interest in doing these? this is all doable from within the
spark repo itself because our QA harness scripts are in there:

https://github.com/apache/spark/blob/master/dev/run-tests-jenkins

If not, could you make a JIRA for them and put it under "Project Infra".

- Patrick

--001a11c32efac91d2604ffc55797--

From dev-return-8696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 03:49:21 2014
Return-Path: <dev-return-8696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 741B011064
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 03:49:21 +0000 (UTC)
Received: (qmail 20498 invoked by uid 500); 4 Aug 2014 03:49:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20435 invoked by uid 500); 4 Aug 2014 03:49:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20423 invoked by uid 99); 4 Aug 2014 03:49:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:49:20 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:49:12 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id CB319261329
	for <dev@spark.apache.org>; Mon,  4 Aug 2014 11:48:49 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id C0A34260B51;
	Mon,  4 Aug 2014 11:48:49 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id bDYTIhJDo32U; Mon,  4 Aug 2014 11:48:49 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.82])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id A19E7260BE1;
	Mon,  4 Aug 2014 11:48:49 +0800 (GMT-8)
Message-ID: <53DF02A4.8090905@sjtu.edu.cn>
Date: Mon, 04 Aug 2014 11:48:52 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
CC: xiaodi@sjtu.edu.cn
Subject: Compiling Spark master (6ba6c3eb) with sbt/sbt assembly
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

On the latest pull today (6ba6c3ebfe9a47351a50e45271e241140b09bf10) meet 
assembly problem.

$ ./sbt/sbt assembly
Using /usr/lib/jvm/java-7-oracle as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[info] Loading project definition from ~/spark/project/project
[info] Loading project definition from 
~/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
[warn] Multiple resolvers having different access mechanism configured 
with same name 'sbt-plugin-releases'. To avoid conflict, Remove 
duplicate project resolvers (`resolvers`) or rename publishing resolv
er (`publishTo`).
[info] Loading project definition from ~/spark/project
[info] Set current project to spark-parent (in build file:~/spark/)
[info] Compiling 372 Scala sources and 35 Java sources to 
~/spark/core/target/scala-2.10/classes...
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:116: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]       stageData.taskData.put(taskInfo.taskId, new 
TaskUIData(taskInfo))
[error]                                               ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:134: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
[error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
[error]       val execSummary = 
execSummaryMap.getOrElseUpdate(info.executorId, new ExecutorSummary)
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:163: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]       val taskData = 
stageData.taskData.getOrElseUpdate(info.taskId, new TaskUIData(info))
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:180: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
[error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
[error]     val execSummary = 
stageData.executorSummary.getOrElseUpdate(execId, new ExecutorSummary)
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:109: type 
mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData => 
Seq[scala.xml.Node]
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData => 
Seq[scala.xml.Node]
[error] Error occurred in an application involving default arguments.
[error]         taskHeaders, taskRow(hasInput, hasShuffleRead, 
hasShuffleWrite, hasBytesSpilled), tasks)
[error]                             ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:119: constructor 
cannot be instantiated to expected type;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]           val serializationTimes = validTasks.map { case 
TaskUIData(_, metrics, _) =>
[error]                                                          ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:120: not 
found: value metrics
[error]             metrics.get.resultSerializationTime.toDouble
[error]             ^

I think the code doesn't make correct reference to the updated structure.

"core/src/main/scala/org/apache/spark/ui/jobs/UIData.scala" is 
introduced in commit 72e9021eaf26f31a82120505f8b764b18fbe8d48

Larry

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 03:59:33 2014
Return-Path: <dev-return-8697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 67BB41109C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 03:59:33 +0000 (UTC)
Received: (qmail 30130 invoked by uid 500); 4 Aug 2014 03:59:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30005 invoked by uid 500); 4 Aug 2014 03:59:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28988 invoked by uid 99); 4 Aug 2014 03:59:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:59:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 03:59:28 +0000
Received: by mail-oi0-f43.google.com with SMTP id u20so4212482oif.30
        for <multiple recipients>; Sun, 03 Aug 2014 20:59:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rJTF0IREPYQB8SWf8jSSJ/T80iNbpEX7SOszMttq+3A=;
        b=DmKRKf5EdRqXkfK90qo689383ylPvSoXUTD6HJe+i4aN/leGmpxGgxSQSuT3dMPtbe
         ME2yWHn6IvlgN4IVSmZioJWvhQSxfzoez5WqI7r4hOs7D75MmkWgG3DohwGNyTb0NZ/q
         zycG5N4z5j1oHeckzflIECZ2MeBg0+ZQqeOmbUj6UVprrYN7Yx82XS/Zod3CL/t2tz5u
         WBcSEdj0CVXPQrsyrc74Gr5gV1Ip9ekXt6RiDEo491UY4z+4qR91K4U1Ji2qH9P1982K
         lAO9BaLuiF1q9L5nXWhRdl0M3du5eq8ZKlukgQ538xyios7R8hLfFULms7Rmn4zHEj0B
         3flw==
MIME-Version: 1.0
X-Received: by 10.182.24.38 with SMTP id r6mr28640139obf.10.1407124743560;
 Sun, 03 Aug 2014 20:59:03 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 3 Aug 2014 20:59:03 -0700 (PDT)
In-Reply-To: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
Date: Sun, 3 Aug 2014 20:59:03 -0700
Message-ID: <CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c29c827ea33404ffc5c28e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29c827ea33404ffc5c28e
Content-Type: text/plain; charset=ISO-8859-1

I'll let TD chime on on this one, but I'm guessing this would be a welcome
addition. It's great to see community effort on adding new
streams/receivers, adding a Java API for receivers was something we did
specifically to allow this :)

- Patrick


On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
dibyendu.bhattachary@gmail.com> wrote:

> Hi,
>
> I have implemented a Low Level Kafka Consumer for Spark Streaming using
> Kafka Simple Consumer API. This API will give better control over the Kafka
> offset management and recovery from failures. As the present Spark
> KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
> control over the offset management which is not possible in Kafka HighLevel
> consumer.
>
> This Project is available in below Repo :
>
> https://github.com/dibbhatt/kafka-spark-consumer
>
>
> I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver.
> The KafkaReceiver uses low level Kafka Consumer API (implemented in
> consumer.kafka packages) to fetch messages from Kafka and 'store' it in
> Spark.
>
> The logic will detect number of partitions for a topic and spawn that many
> threads (Individual instances of Consumers). Kafka Consumer uses Zookeeper
> for storing the latest offset for individual partitions, which will help to
> recover in case of failure. The Kafka Consumer logic is tolerant to ZK
> Failures, Kafka Leader of Partition changes, Kafka broker failures,
>  recovery from offset errors and other fail-over aspects.
>
> The consumer.kafka.client.Consumer is the sample Consumer which uses this
> Kafka Receivers to generate DStreams from Kafka and apply a Output
> operation for every messages of the RDD.
>
> We are planning to use this Kafka Spark Consumer to perform Near Real Time
> Indexing of Kafka Messages to target Search Cluster and also Near Real Time
> Aggregation using target NoSQL storage.
>
> Kindly let me know your view. Also if this looks good, can I contribute to
> Spark Streaming project.
>
> Regards,
> Dibyendu
>

--001a11c29c827ea33404ffc5c28e--

From dev-return-8698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 04:10:26 2014
Return-Path: <dev-return-8698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CE81110C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 04:10:26 +0000 (UTC)
Received: (qmail 38609 invoked by uid 500); 4 Aug 2014 04:10:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38548 invoked by uid 500); 4 Aug 2014 04:10:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38535 invoked by uid 99); 4 Aug 2014 04:10:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:10:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:10:23 +0000
Received: by mail-oi0-f41.google.com with SMTP id a141so4213932oig.28
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 21:09:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=w2GPAsluovchwHKGpkyAuN0l7/0eFw03jE4V7bdf7pc=;
        b=hewElk8+UTcnuxpPowZI7C8KHogJMCO2Uvn4ElaWsJehk05uPA1gbQ/pB9cqmt3+Rl
         t0K0KxETokBkV7QyxuZvRybZjc0u4k2ZOIIf1bZz0Ld52Qy7IBvHoQ0Jrd4bPtXkD7ya
         8zPWaNjwbNkPa5JE5HrvinmWlwArg+R14amS+Mp9KhuMdlB1dOyzExJaMek8EP5SNsLk
         mFNSRA8YTH2vXxQmfcEJUMAWMVEylYcwJaDgNtZiO4150Y5XsziQKUZON2xG4r/4BkGz
         J8so6ainmBTX3ZCjbYcBrDvcYflbe5WiKGL3SlvDp7qhWJyYhB7ABZNEqdavLr+nvLck
         PsBA==
MIME-Version: 1.0
X-Received: by 10.182.22.201 with SMTP id g9mr6182631obf.75.1407125398500;
 Sun, 03 Aug 2014 21:09:58 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 3 Aug 2014 21:09:58 -0700 (PDT)
In-Reply-To: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
References: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
Date: Sun, 3 Aug 2014 21:09:58 -0700
Message-ID: <CABPQxssGVanUChfazXUwiyx1JcmtpRyDTjuQmscdyZwM-3ZGCg@mail.gmail.com>
Subject: Re: Scala 2.11 external dependencies
From: Patrick Wendell <pwendell@gmail.com>
To: Anand Avati <avati@gluster.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e90c883fb204ffc5e9c1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e90c883fb204ffc5e9c1
Content-Type: text/plain; charset=ISO-8859-1

Hey Anand,

Thanks for looking into this - it's great to see momentum towards Scala
2.11 and I'd love if this land in Spark 1.2.

For the external dependencies, it would be good to create a sub-task of
SPARK-1812 to track our efforts encouraging other projects to upgrade. In
certain cases (e.g. Kafka) there is fairly late-stage work on this already,
so we can e.g. link to those JIRA's as well. A good starting point is to
just go to their dev list and ask what the status is, most Scala projects
have put at least some thought into this already. Another thing we can do
is submit patches ourselves to those projects to help get them upgraded.
The twitter libraries, e.g., tend to be pretty small and also open to
external contributions.

One other thing in the mix here - Prashant Sharma has also spent some time
looking at this, so it might be good for you two to connect (probably off
list) and sync up. Prashant has contributed to many Scala projects, so he
might have cycles to go and help some of our dependencies get upgraded -
but I won't commit to that on his behalf :).

Regarding Akka - I shaded and published akka as a one-off thing:
https://github.com/pwendell/akka/tree/2.2.3-shaded-proto

Over time we've had to publish our own versions of a small number of
dependencies. It's somewhat high overhead, but it actually works quite well
in terms of avoiding some of the nastier dependency conflicts. At least
better than other alternatives I've seen such as using a shader build
plug-in.

Going forward, I'd actually like to track these in the Spark repo itself.
For instance, we have a bash script in the spark repo that can e.g. check
out akka, apply a few patches or regular expressions, and then you have a
fully shaded dependency that can be published to maven. If you wanted to
take a crack at something like that for akka 2.3.4, be my guest. I can help
with the actual publishing.

- Patrick


On Sat, Aug 2, 2014 at 6:04 PM, Anand Avati <avati@gluster.org> wrote:

> We are currently blocked on non availability of the following external
> dependencies for porting Spark to Scala 2.11 [SPARK-1812 Jira]:
>
> - akka-*_2.11 (2.3.4-shaded-protobuf from org.spark-project). The shaded
> protobuf needs to be 2.5.0, and the shading is needed because Hadoop1
> specifically needs protobuf 2.4. Issues arising because of this
> incompatibility is already explained in SPARK-1812 Jira.
>
> - chill_2.11 (0.4 from com.twitter) for core
> - algebird_2.11 (0.7 from com.twitter) for examples
> - kafka_2.11 (0.8 from org.apache) for external/kafka and examples
> - akka-zeromq_2.11 (2.3.4 from com.typesafe, but probably not needed if a
> shaded-protobuf version is released from org.spark-project)
>
> First,
> Who do I pester to get org.spark-project artifacts published for the akka
> shaded-protobuf version?
>
> Second,
> In the past what has been the convention to request/pester external
> projects to re-release artifacts in a new Scala version?
>
> Thanks!
>

--001a11c2e90c883fb204ffc5e9c1--

From dev-return-8699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 04:13:26 2014
Return-Path: <dev-return-8699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55E37110CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 04:13:26 +0000 (UTC)
Received: (qmail 40734 invoked by uid 500); 4 Aug 2014 04:13:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40660 invoked by uid 500); 4 Aug 2014 04:13:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40648 invoked by uid 99); 4 Aug 2014 04:13:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:13:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:13:23 +0000
Received: by mail-wi0-f178.google.com with SMTP id hi2so4260145wib.5
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 21:12:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=gukxoXPnNcYiWKfZqWhqbAWwKaX67jaSoQmSJVygL5U=;
        b=XW5y0C+79Xa8WKVTFBC75TG580rb+L37UfmWOy4kAL9p/hEZ5dn2EVPO6vxWu+kl2r
         V5NQMLrWqtN5H1qrTwUr4ueOKR9LcAQzFWIDtJ2az188WuQO7BA5OB03GeVnxCzaqfpm
         vB5YXsnQIZ4y8DG7iHYzjYajLij9MQsYxU8xgeHNvIpBGozr/ZuP1pSOXUtj0TulPaK2
         71H96M8ApUhZD4FXfp2baNgv1/1inf8RSVSE0wFKwVTJeOSwHDRCnUw6k6EYMmOuU1Zn
         Dzpmb/6he/CpuF1vROn4Lez4tpTCTaMorXu2daDbIvu870JdrzChkarvBIPzLScWuahe
         0k+A==
X-Received: by 10.180.205.168 with SMTP id lh8mr10423674wic.33.1407125578610;
 Sun, 03 Aug 2014 21:12:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sun, 3 Aug 2014 21:12:18 -0700 (PDT)
In-Reply-To: <CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
 <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com> <CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 4 Aug 2014 00:12:18 -0400
Message-ID: <CAOhmDzdyR0dVJCK_+u0vhE+fi4MHgCLOa=YeD0XaSoCNOnu2Pw@mail.gmail.com>
Subject: Re: -1s on pull requests?
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c38ace447d9504ffc5f4de
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38ace447d9504ffc5f4de
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On Sun, Aug 3, 2014 at 11:29 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:

Nick - Any interest in doing these? this is all doable from within the
> spark repo itself because our QA harness scripts are in there:
>
> https://github.com/apache/spark/blob/master/dev/run-tests-jenkins
>
> If not, could you make a JIRA for them and put it under "Project Infra".
>
I=E2=80=99ll make the JIRA and think about how to do this stuff. I=E2=80=99=
ll have to
understand what that run-tests-jenkins script does and see how easy it is
to extend.

Nick
=E2=80=8B

--001a11c38ace447d9504ffc5f4de--

From dev-return-8700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 04:44:15 2014
Return-Path: <dev-return-8700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 305A111159
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 04:44:15 +0000 (UTC)
Received: (qmail 92840 invoked by uid 500); 4 Aug 2014 04:44:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92785 invoked by uid 500); 4 Aug 2014 04:44:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92770 invoked by uid 99); 4 Aug 2014 04:44:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:44:13 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 04:44:12 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so4110218oig.26
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 21:43:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=So0Ir3NHshEy0Z0buJI2GKmcvGwy9aZV0UfWgFyxE9A=;
        b=bsK6IJTQEJF4U8pUrDbIX4S7XCdA0KLnzjxj2N112VcNJqK4lTKeSlhLqaxwTAmiAk
         wxWhM9tRbZnZNuCljWXacjmdhDxp5E8myLMuUhoHASZYDrkS+Qt21mhuFsOaEWP7OPTR
         5BsMnNV8Gz6pxBbPJ7m0BTIklquuyjxy/HZgHpI3zm4LwxEZNfT7txBbokVX5ez7eZRu
         Wc4DXHWD7iL5aueTiuh9tzvYxkWExXw0aOFEjqnrgHJK4Z2pajnMYoTNZ9wMIrhwS7Od
         TAaMKwyyVLLYd1cy6xqUwF2VytI1euaXGQEjaGvh8YJV8e7JgE2XUsybSWZZP2ZmvOoD
         UhFA==
MIME-Version: 1.0
X-Received: by 10.182.22.201 with SMTP id g9mr6317623obf.75.1407127426777;
 Sun, 03 Aug 2014 21:43:46 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 3 Aug 2014 21:43:46 -0700 (PDT)
In-Reply-To: <CAOhmDzdyR0dVJCK_+u0vhE+fi4MHgCLOa=YeD0XaSoCNOnu2Pw@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
	<CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
	<CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
	<CAOhmDzdyR0dVJCK_+u0vhE+fi4MHgCLOa=YeD0XaSoCNOnu2Pw@mail.gmail.com>
Date: Sun, 3 Aug 2014 21:43:46 -0700
Message-ID: <CABPQxsu=6Hddmk4WNZbkykkbOHsuwLcpe96BOcDL2m8sE0Egkw@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e90c6d47a504ffc662ee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e90c6d47a504ffc662ee
Content-Type: text/plain; charset=ISO-8859-1

Sure thing - feel free to ping me off list if you need pointers. The script
just does string concatenation and a curl to post the comment... I think it
should be pretty accessible!

- Patrick


On Sun, Aug 3, 2014 at 9:12 PM, Nicholas Chammas <nicholas.chammas@gmail.com
> wrote:

> On Sun, Aug 3, 2014 at 11:29 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> Nick - Any interest in doing these? this is all doable from within the
>> spark repo itself because our QA harness scripts are in there:
>>
>> https://github.com/apache/spark/blob/master/dev/run-tests-jenkins
>>
>> If not, could you make a JIRA for them and put it under "Project Infra".
>>
> I'll make the JIRA and think about how to do this stuff. I'll have to
> understand what that run-tests-jenkins script does and see how easy it is
> to extend.
>
> Nick
> 
>

--001a11c2e90c6d47a504ffc662ee--

From dev-return-8701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 06:54:00 2014
Return-Path: <dev-return-8701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78DC8113ED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 06:54:00 +0000 (UTC)
Received: (qmail 46419 invoked by uid 500); 4 Aug 2014 06:54:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46361 invoked by uid 500); 4 Aug 2014 06:54:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46340 invoked by uid 99); 4 Aug 2014 06:53:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 06:53:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pllee@appier.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 06:53:56 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so8330687qgf.20
        for <dev@spark.apache.org>; Sun, 03 Aug 2014 23:53:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bdcRtsn9U26gGRYnmntVzGSormIPOYoIQZuqsBvaxMA=;
        b=VtiMEyeVECGN1xkDYvyYSuHi6kiGQbEtAYODRX7OE4iWFocqDi6OlpKW/V+EXByHY1
         OrVYY3ENy7UgiwzTgtaOgnpA4O0ZkLQuE3xhgfcfZMsY9SxtWdqgG3944O+q1IEWu18i
         1qzMoLCPBRC0gNSeAGm2xA9A0POkjYRUj2/dM=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=bdcRtsn9U26gGRYnmntVzGSormIPOYoIQZuqsBvaxMA=;
        b=bTJO1lLpbla1U+oEimDMLCi30bd6YMiNKaERss3UFhWvUQEdGjEmi4E83o1Ol41wNz
         00A8XsRju5bwA3c6Ywvr5jdef6YZ2K+BDhnpdIlN3D3WR+f/EsjLf4u8Z5kQWJwv5V4x
         0ttI9r7R5UeKpsm0G7e85yIkA16nZNuMCIbak1r3P0x2w981SQl1nsn14fIV6lOKvfw3
         v+WBpHyRTLVjJkgvP2/kSJCPCQUSNQhJ7Phn8MoPzfNfc2UHV/725ynQler63VRYnOhp
         MFmM5O0RI/u0TVmT3aH0sEuxj7sNyaGWTPcSVg39maP70HJVikqtVuvzkLe9HW9AyZF0
         5OzQ==
X-Gm-Message-State: ALoCoQlSrGM6rJHYnqO5vX/qPXJ6mMCUnRDlKo8SS0+WWfQF3rD76JcMJwFZLCp2I2lKdhLwi7E5
MIME-Version: 1.0
X-Received: by 10.224.37.72 with SMTP id w8mr33943692qad.58.1407135210274;
 Sun, 03 Aug 2014 23:53:30 -0700 (PDT)
Received: by 10.140.109.52 with HTTP; Sun, 3 Aug 2014 23:53:30 -0700 (PDT)
In-Reply-To: <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
	<CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
	<CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
	<CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
	<CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
	<CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
Date: Mon, 4 Aug 2014 14:53:30 +0800
Message-ID: <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
Subject: Re: Spark SQL 1.0.1 error on reading fixed length byte array
From: Pei-Lun Lee <pllee@appier.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1e3dc5c1d9e04ffc832d3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1e3dc5c1d9e04ffc832d3
Content-Type: text/plain; charset=UTF-8

Hi,

We have a PR to support fixed length byte array in parquet file.

https://github.com/apache/spark/pull/1737

Can someone help verifying it?

Thanks.

2014-07-15 19:23 GMT+08:00 Pei-Lun Lee <pllee@appier.com>:

> Sorry, should be SPARK-2489
>
>
> 2014-07-15 19:22 GMT+08:00 Pei-Lun Lee <pllee@appier.com>:
>
> Filed SPARK-2446
>>
>>
>>
>> 2014-07-15 16:17 GMT+08:00 Michael Armbrust <michael@databricks.com>:
>>
>> Oh, maybe not.  Please file another JIRA.
>>>
>>>
>>> On Tue, Jul 15, 2014 at 12:34 AM, Pei-Lun Lee <pllee@appier.com> wrote:
>>>
>>>> Hi Michael,
>>>>
>>>> Good to know it is being handled. I tried master branch (9fe693b5) and
>>>> got another error:
>>>>
>>>> scala> sqlContext.parquetFile("/tmp/foo")
>>>> java.lang.RuntimeException: Unsupported parquet datatype optional
>>>> fixed_len_byte_array(4) b
>>>> at scala.sys.package$.error(package.scala:27)
>>>> at
>>>> org.apache.spark.sql.parquet.ParquetTypesConverter$.toPrimitiveDataType(ParquetTypes.scala:58)
>>>>  at
>>>> org.apache.spark.sql.parquet.ParquetTypesConverter$.toDataType(ParquetTypes.scala:109)
>>>> at
>>>> org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$convertToAttributes$1.apply(ParquetTypes.scala:282)
>>>>  at
>>>> org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$convertToAttributes$1.apply(ParquetTypes.scala:279)
>>>>         ......
>>>>
>>>> The avro schema I used is something like:
>>>>
>>>> protocol Test {
>>>>     fixed Bytes4(4);
>>>>
>>>>     record User {
>>>>         string name;
>>>>         int age;
>>>>         union {null, int} i;
>>>>         union {null, int} j;
>>>>         union {null, Bytes4} b;
>>>>         union {null, bytes} c;
>>>>         union {null, int} d;
>>>>     }
>>>> }
>>>>
>>>> Is this case included in SPARK-2446
>>>> <https://issues.apache.org/jira/browse/SPARK-2446>?
>>>>
>>>>
>>>> 2014-07-15 3:54 GMT+08:00 Michael Armbrust <michael@databricks.com>:
>>>>
>>>> This is not supported yet, but there is a PR open to fix it:
>>>>> https://issues.apache.org/jira/browse/SPARK-2446
>>>>>
>>>>>
>>>>> On Mon, Jul 14, 2014 at 4:17 AM, Pei-Lun Lee <pllee@appier.com> wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I am using spark-sql 1.0.1 to load parquet files generated from
>>>>>> method described in:
>>>>>>
>>>>>> https://gist.github.com/massie/7224868
>>>>>>
>>>>>>
>>>>>> When I try to submit a select query with columns of type fixed length
>>>>>> byte array, the following error pops up:
>>>>>>
>>>>>>
>>>>>> 14/07/14 11:09:14 INFO scheduler.DAGScheduler: Failed to run take at
>>>>>> basicOperators.scala:100
>>>>>> org.apache.spark.SparkDriverExecutionException: Execution error
>>>>>>         at
>>>>>> org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:581)
>>>>>>         at
>>>>>> org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:559)
>>>>>> Caused by: parquet.io.ParquetDecodingException: Can not read value at
>>>>>> 0 in block -1 in file s3n://foo/bar/part-r-00000.snappy.parquet
>>>>>>         at
>>>>>> parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:177)
>>>>>>         at
>>>>>> parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:130)
>>>>>>         at
>>>>>> org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:122)
>>>>>>         at
>>>>>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
>>>>>>         at
>>>>>> scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
>>>>>>         at
>>>>>> scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
>>>>>>         at
>>>>>> scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
>>>>>>         at
>>>>>> scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
>>>>>>         at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>>>>>         at
>>>>>> scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>>>>>         at
>>>>>> scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
>>>>>>         at
>>>>>> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
>>>>>>         at
>>>>>> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
>>>>>>         at scala.collection.TraversableOnce$class.to
>>>>>> (TraversableOnce.scala:273)
>>>>>>         at scala.collection.AbstractIterator.to(Iterator.scala:1157)
>>>>>>         at
>>>>>> scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
>>>>>>         at
>>>>>> scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
>>>>>>         at
>>>>>> scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
>>>>>>         at
>>>>>> scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
>>>>>>         at org.apache.spark.rdd.RDD$$anonfun$27.apply(RDD.scala:989)
>>>>>>         at org.apache.spark.rdd.RDD$$anonfun$27.apply(RDD.scala:989)
>>>>>>         at
>>>>>> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1083)
>>>>>>         at
>>>>>> org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:574)
>>>>>>         ... 1 more
>>>>>> Caused by: java.lang.ClassCastException: Expected instance of
>>>>>> primitive converter but got
>>>>>> "org.apache.spark.sql.parquet.CatalystNativeArrayConverter"
>>>>>>         at
>>>>>> parquet.io.api.Converter.asPrimitiveConverter(Converter.java:30)
>>>>>>         at
>>>>>> parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:264)
>>>>>>         at
>>>>>> parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:60)
>>>>>>         at
>>>>>> parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:74)
>>>>>>         at
>>>>>> parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:110)
>>>>>>         at
>>>>>> parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:172)
>>>>>>         ... 24 more
>>>>>>
>>>>>>
>>>>>> Is fixed length byte array supposed to work in this version? I
>>>>>> noticed that other array types like int or string already work.
>>>>>>
>>>>>> Thanks,
>>>>>> --
>>>>>> Pei-Lun
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c1e3dc5c1d9e04ffc832d3--

From dev-return-8702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 08:16:24 2014
Return-Path: <dev-return-8702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E804115A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 08:16:24 +0000 (UTC)
Received: (qmail 52089 invoked by uid 500); 4 Aug 2014 08:16:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51990 invoked by uid 500); 4 Aug 2014 08:16:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50987 invoked by uid 99); 4 Aug 2014 08:16:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:16:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:16:18 +0000
Received: by mail-ob0-f178.google.com with SMTP id nu7so4286836obb.23
        for <multiple recipients>; Mon, 04 Aug 2014 01:15:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=d8ACJSY1eLJ/m/e/U51Gv2mzjJbgyMCfaD1H3OhtVWA=;
        b=t7P7qwGRW1kMQfKTP/ECDiEkYcNDAO8WJh8pj07WNTI7EyL5LrDewtW0ch92irbTM8
         8o2kjPkaLOOn+QrzBHZGd7aKH8GRSzIwymIZiv9lK8x2nKIA//Uj93Lajzuh3Dax5ciJ
         oXaak124X1RhaOpxVBHViMzeRxQ2O7Hj7HbUPamdGd4Xzqs0aWqvCzS42qHe3KMR96wW
         VPNinIn2/mzzRBg/Ypy1KebBFnviDLJsyVLm5PiLGAHCUhP0DoaHLaocf8pJsD2MpI1Y
         DdoNKdePfK0eh8CqhzDDMMtj4LWtBTMv520XURvrQPkrBapTAXoCMin8iE7dhuVG5n/E
         cK/A==
MIME-Version: 1.0
X-Received: by 10.60.51.106 with SMTP id j10mr2091522oeo.77.1407140153480;
 Mon, 04 Aug 2014 01:15:53 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 4 Aug 2014 01:15:53 -0700 (PDT)
In-Reply-To: <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
	<CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
	<CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
	<CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
	<CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
	<CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
	<CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
	<CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
Date: Mon, 4 Aug 2014 01:15:53 -0700
Message-ID: <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Patrick Wendell <pwendell@gmail.com>
To: "Ron's Yahoo!" <zlgonzalez@yahoo.com.invalid>
Cc: user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c30152ff5b2b04ffc95899
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c30152ff5b2b04ffc95899
Content-Type: text/plain; charset=ISO-8859-1

For hortonworks, I believe it should work to just link against the
corresponding upstream version. I.e. just set the Hadoop version to "2.4.0"

Does that work?

- Patrick


On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>
wrote:

> Hi,
>   Not sure whose issue this is, but if I run make-distribution using HDP
> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
> make-distribution.sh), I get a strange error with the exception below. If I
> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
> make-distribution, using the generated assembly all works fine for me.
> Either 1.0.0 or 1.0.1 will work fine.
>
>   Should I file a JIRA or is this a known issue?
>
> Thanks,
> Ron
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted
> due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
> Exception failure in TID 0 on host localhost:
> java.lang.IncompatibleClassChangeError: Found interface
> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
>         org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(
> AvroKeyInputFormat.java:47)
>         org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(
> NewHadoopRDD.scala:111)
>         org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>         org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111
> )
>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>         org.apache.spark.executor.Executor$TaskRunner.run(
> Executor.scala:187)
>         java.util.concurrent.ThreadPoolExecutor.runWorker(
> ThreadPoolExecutor.java:1145)
>         java.util.concurrent.ThreadPoolExecutor$Worker.run(
> ThreadPoolExecutor.java:615)
>         java.lang.Thread.run(Thread.java:745)
>

--001a11c30152ff5b2b04ffc95899--

From dev-return-8703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 08:25:53 2014
Return-Path: <dev-return-8703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C8DD115C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 08:25:53 +0000 (UTC)
Received: (qmail 69312 invoked by uid 500); 4 Aug 2014 08:25:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69254 invoked by uid 500); 4 Aug 2014 08:25:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69238 invoked by uid 99); 4 Aug 2014 08:25:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:25:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.172 as permitted sender)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:25:46 +0000
Received: by mail-vc0-f172.google.com with SMTP id im17so10499305vcb.31
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 01:25:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=yvS9dkyZyHrl9jdsy6t6teuCUpSVgsmlv55pOw1RbBw=;
        b=VCnNyTnVradFf3NddXYWS4OfRueKoLsJXlKKE0HWp61a1NjNQ6on0M6QGi86/8FzcG
         +De+K48dAXb2jfT08jvZbscxD0p8apXCNhG1AaMbNrsIvUCAuedannwBfx5xKxGgBmxq
         FPDB+kEt9b/WvxnXtac+5wqWF2u3kp/bRA8KTln8QLNjYBMUNawN6xMreQHrswRklZDw
         wzLYkmoBTc394F8GELXlWolQgzVonsWPTjkgDhrrSd/hWEF5J706LM/UGJQklczbT4Zc
         ZBF1inILVhvpyH42VwKBBkkeoHbjtzurQNiS0ZV07kjumwpljAiLB8gaLCan3zeiy6p0
         Ty0g==
X-Gm-Message-State: ALoCoQl0QjhMTj6s7VpsH5HRTbnSH6JM3QPZ/CjhMb58U4sqdgnjeusRUnms99xlzdqA2xdYvamT
X-Received: by 10.52.239.6 with SMTP id vo6mr1792842vdc.59.1407140724114; Mon,
 04 Aug 2014 01:25:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 4 Aug 2014 01:25:04 -0700 (PDT)
In-Reply-To: <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
 <CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
 <CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
 <CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
 <CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
 <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
 <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
 <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com> <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 4 Aug 2014 09:25:04 +0100
Message-ID: <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
To: Patrick Wendell <pwendell@gmail.com>
Cc: "Ron's Yahoo!" <zlgonzalez@yahoo.com.invalid>, user@spark.apache.org, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

For any Hadoop 2.4 distro, yes, set hadoop.version but also set
-Phadoop-2.4. http://spark.apache.org/docs/latest/building-with-maven.html

On Mon, Aug 4, 2014 at 9:15 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> For hortonworks, I believe it should work to just link against the
> corresponding upstream version. I.e. just set the Hadoop version to "2.4.0"
>
> Does that work?
>
> - Patrick
>
>
> On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>
> wrote:
>>
>> Hi,
>>   Not sure whose issue this is, but if I run make-distribution using HDP
>> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
>> make-distribution.sh), I get a strange error with the exception below. If I
>> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
>> make-distribution, using the generated assembly all works fine for me.
>> Either 1.0.0 or 1.0.1 will work fine.
>>
>>   Should I file a JIRA or is this a known issue?
>>
>> Thanks,
>> Ron
>>
>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>> due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
>> Exception failure in TID 0 on host localhost:
>> java.lang.IncompatibleClassChangeError: Found interface
>> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
>>
>> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInputFormat.java:47)
>>
>> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
>>         org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>>         org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>         java.lang.Thread.run(Thread.java:745)
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 08:44:43 2014
Return-Path: <dev-return-8704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 677BF1161D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 08:44:43 +0000 (UTC)
Received: (qmail 6911 invoked by uid 500); 4 Aug 2014 08:44:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6862 invoked by uid 500); 4 Aug 2014 08:44:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5783 invoked by uid 99); 4 Aug 2014 08:44:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:44:41 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:44:33 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id 5B81C260F04;
	Mon,  4 Aug 2014 16:44:12 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id 4EE0A260C9C;
	Mon,  4 Aug 2014 16:44:12 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id 3twY3hMLojaL; Mon,  4 Aug 2014 16:44:12 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.82])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id 22850260C90;
	Mon,  4 Aug 2014 16:44:12 +0800 (GMT-8)
Message-ID: <53DF47DE.20802@sjtu.edu.cn>
Date: Mon, 04 Aug 2014 16:44:14 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org, user@spark.apache.org
CC: xiaodi@sjtu.edu.cn
Subject: Re: Compiling Spark master (6ba6c3eb) with sbt/sbt assembly
References: <53DF02A4.8090905@sjtu.edu.cn>
In-Reply-To: <53DF02A4.8090905@sjtu.edu.cn>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I guessed
     ./sbt/sbt clean
and it works fine now.

On 8/4/14, 11:48 AM, Larry Xiao wrote:
> On the latest pull today (6ba6c3ebfe9a47351a50e45271e241140b09bf10) 
> meet assembly problem.
>
> $ ./sbt/sbt assembly
> Using /usr/lib/jvm/java-7-oracle as default JAVA_HOME.
> Note, this will be overridden by -java-home if it is set.
> [info] Loading project definition from ~/spark/project/project
> [info] Loading project definition from 
> ~/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
> [warn] Multiple resolvers having different access mechanism configured 
> with same name 'sbt-plugin-releases'. To avoid conflict, Remove 
> duplicate project resolvers (`resolvers`) or rename publishing resolv
> er (`publishTo`).
> [info] Loading project definition from ~/spark/project
> [info] Set current project to spark-parent (in build file:~/spark/)
> [info] Compiling 372 Scala sources and 35 Java sources to 
> ~/spark/core/target/scala-2.10/classes...
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:116: 
> type mismatch;
> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
> [error]       stageData.taskData.put(taskInfo.taskId, new 
> TaskUIData(taskInfo))
> [error]                                               ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:134: 
> type mismatch;
> [error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
> [error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
> [error]       val execSummary = 
> execSummaryMap.getOrElseUpdate(info.executorId, new ExecutorSummary)
> [error] ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:163: 
> type mismatch;
> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
> [error]       val taskData = 
> stageData.taskData.getOrElseUpdate(info.taskId, new TaskUIData(info))
> [error] ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:180: 
> type mismatch;
> [error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
> [error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
> [error]     val execSummary = 
> stageData.executorSummary.getOrElseUpdate(execId, new ExecutorSummary)
> [error] ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:109: 
> type mismatch;
> [error]  found   : org.apache.spark.ui.jobs.TaskUIData => 
> Seq[scala.xml.Node]
> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData => 
> Seq[scala.xml.Node]
> [error] Error occurred in an application involving default arguments.
> [error]         taskHeaders, taskRow(hasInput, hasShuffleRead, 
> hasShuffleWrite, hasBytesSpilled), tasks)
> [error]                             ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:119: 
> constructor cannot be instantiated to expected type;
> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
> [error]           val serializationTimes = validTasks.map { case 
> TaskUIData(_, metrics, _) =>
> [error]                                                          ^
> [error] 
> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:120: 
> not found: value metrics
> [error]             metrics.get.resultSerializationTime.toDouble
> [error]             ^
>
> I think the code doesn't make correct reference to the updated structure.
>
> "core/src/main/scala/org/apache/spark/ui/jobs/UIData.scala" is 
> introduced in commit 72e9021eaf26f31a82120505f8b764b18fbe8d48
>
> Larry
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 08:48:30 2014
Return-Path: <dev-return-8705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C83BB11636
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 08:48:30 +0000 (UTC)
Received: (qmail 16806 invoked by uid 500); 4 Aug 2014 08:48:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16656 invoked by uid 500); 4 Aug 2014 08:48:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15674 invoked by uid 99); 4 Aug 2014 08:48:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:48:27 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 08:48:20 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id 86028260F04;
	Mon,  4 Aug 2014 16:47:59 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id 79AD9260BBC;
	Mon,  4 Aug 2014 16:47:59 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id DPEIliowl8Kr; Mon,  4 Aug 2014 16:47:59 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.85])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id 5928E260B9B;
	Mon,  4 Aug 2014 16:47:59 +0800 (GMT-8)
Message-ID: <53DF48C1.3010702@sjtu.edu.cn>
Date: Mon, 04 Aug 2014 16:48:01 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org, user@spark.apache.org
CC: xiaodi@sjtu.edu.cn
Subject: Re: Compiling Spark master (6ba6c3eb) with sbt/sbt assembly
References: <53DF02A4.8090905@sjtu.edu.cn> <53DF47DE.20802@sjtu.edu.cn>
In-Reply-To: <53DF47DE.20802@sjtu.edu.cn>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry I mean, I tried this command
     ./sbt/sbt clean
and now it works.

Is it because of cached components no recompiled?

On 8/4/14, 4:44 PM, Larry Xiao wrote:
> I guessed
>     ./sbt/sbt clean
> and it works fine now.
>
> On 8/4/14, 11:48 AM, Larry Xiao wrote:
>> On the latest pull today (6ba6c3ebfe9a47351a50e45271e241140b09bf10) 
>> meet assembly problem.
>>
>> $ ./sbt/sbt assembly
>> Using /usr/lib/jvm/java-7-oracle as default JAVA_HOME.
>> Note, this will be overridden by -java-home if it is set.
>> [info] Loading project definition from ~/spark/project/project
>> [info] Loading project definition from 
>> ~/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
>> [warn] Multiple resolvers having different access mechanism 
>> configured with same name 'sbt-plugin-releases'. To avoid conflict, 
>> Remove duplicate project resolvers (`resolvers`) or rename publishing 
>> resolv
>> er (`publishTo`).
>> [info] Loading project definition from ~/spark/project
>> [info] Set current project to spark-parent (in build file:~/spark/)
>> [info] Compiling 372 Scala sources and 35 Java sources to 
>> ~/spark/core/target/scala-2.10/classes...
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:116: 
>> type mismatch;
>> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
>> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
>> [error]       stageData.taskData.put(taskInfo.taskId, new 
>> TaskUIData(taskInfo))
>> [error]                                               ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:134: 
>> type mismatch;
>> [error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
>> [error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
>> [error]       val execSummary = 
>> execSummaryMap.getOrElseUpdate(info.executorId, new ExecutorSummary)
>> [error] ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:163: 
>> type mismatch;
>> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
>> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
>> [error]       val taskData = 
>> stageData.taskData.getOrElseUpdate(info.taskId, new TaskUIData(info))
>> [error] ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:180: 
>> type mismatch;
>> [error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
>> [error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
>> [error]     val execSummary = 
>> stageData.executorSummary.getOrElseUpdate(execId, new ExecutorSummary)
>> [error] ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:109: 
>> type mismatch;
>> [error]  found   : org.apache.spark.ui.jobs.TaskUIData => 
>> Seq[scala.xml.Node]
>> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData => 
>> Seq[scala.xml.Node]
>> [error] Error occurred in an application involving default arguments.
>> [error]         taskHeaders, taskRow(hasInput, hasShuffleRead, 
>> hasShuffleWrite, hasBytesSpilled), tasks)
>> [error]                             ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:119: 
>> constructor cannot be instantiated to expected type;
>> [error]  found   : org.apache.spark.ui.jobs.TaskUIData
>> [error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
>> [error]           val serializationTimes = validTasks.map { case 
>> TaskUIData(_, metrics, _) =>
>> [error] ^
>> [error] 
>> ~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:120: 
>> not found: value metrics
>> [error]             metrics.get.resultSerializationTime.toDouble
>> [error]             ^
>>
>> I think the code doesn't make correct reference to the updated 
>> structure.
>>
>> "core/src/main/scala/org/apache/spark/ui/jobs/UIData.scala" is 
>> introduced in commit 72e9021eaf26f31a82120505f8b764b18fbe8d48
>>
>> Larry
>>
>
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 14:14:05 2014
Return-Path: <dev-return-8706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 156DC11FDD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 14:14:05 +0000 (UTC)
Received: (qmail 86581 invoked by uid 500); 4 Aug 2014 14:14:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86522 invoked by uid 500); 4 Aug 2014 14:14:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86511 invoked by uid 99); 4 Aug 2014 14:14:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 14:14:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 14:13:56 +0000
Received: by mail-pd0-f170.google.com with SMTP id g10so9767535pdj.1
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 07:13:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:cc:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type
         :content-transfer-encoding;
        bh=JkCigtGbEvurHlKINirDdjMP+PTwLja52AdRSoRG0HE=;
        b=FWDMqys7K7vl3ETJ7CK6CMjuT2/xcx83lrX7nYYQ88iovBXWGOFjQzhB0QpcUkpJ5w
         jSkVe0M3DGOCbNTj3mw9LpInUVjDzLIbUz2nzoLacOPdp3z4ot7pfeRp0OJ2b9ldVsQK
         f+7JamrqurLI77mRAQTsUky2BIdTqufcQJFQIFhnBHKoNBaYwbOwK/1fsaKNLfCpbQph
         ojvvDYNQ9HhJG/25R1QLSzP2ohCtjJojbPRbHvQUZir5hfsblev6FGSIWGaU/GkEIQ85
         hRbbZzZwDLhW5Jf55cqJhI76VBmY29w8UdJgl/Nejx4UJsh+8lz0/F9D3rfd0j6l/DAU
         sA+A==
X-Gm-Message-State: ALoCoQmhC1vmMYqR2Q526cjRdwT3bO/wPbZJ6OJXZKz+3Bt9i2cstMFXTEKlpt9DZr7RstqOWgY/R8cF2sxPzRROAIiLonk5oohxuzWq4J9UF5DFu7fgLns=
X-Received: by 10.68.65.36 with SMTP id u4mr3176098pbs.127.1407161610932;
        Mon, 04 Aug 2014 07:13:30 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id nk1sm26967770pdb.0.2014.08.04.07.13.28
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 04 Aug 2014 07:13:30 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 04 Aug 2014 07:13:23 -0700
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Steve Nunez <snunez@hortonworks.com>
To: Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>
CC: <user@spark.apache.org>,
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <D004E2D9.528E%snunez@hortonworks.com>
Thread-Topic: Issues with HDP 2.4.0.2.1.3.0-563
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
 <CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
 <CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
 <CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
 <CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
 <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
 <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
 <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
 <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
 <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
In-Reply-To: <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
Mime-version: 1.0
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Provided you=B9ve got the HWX repo in your pom.xml, you can build with this
line:

mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0.2.1.1.0-385
-DskipTests clean package

I haven=B9t tried building a distro, but it should be similar.


	- SteveN

On 8/4/14, 1:25, "Sean Owen" <sowen@cloudera.com> wrote:

>For any Hadoop 2.4 distro, yes, set hadoop.version but also set
>-Phadoop-2.4. http://spark.apache.org/docs/latest/building-with-maven.html
>
>On Mon, Aug 4, 2014 at 9:15 AM, Patrick Wendell <pwendell@gmail.com>
>wrote:
>> For hortonworks, I believe it should work to just link against the
>> corresponding upstream version. I.e. just set the Hadoop version to
>>"2.4.0"
>>
>> Does that work?
>>
>> - Patrick
>>
>>
>> On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo!
>><zlgonzalez@yahoo.com.invalid>
>> wrote:
>>>
>>> Hi,
>>>   Not sure whose issue this is, but if I run make-distribution using
>>>HDP
>>> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
>>> make-distribution.sh), I get a strange error with the exception below.
>>>If I
>>> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
>>> make-distribution, using the generated assembly all works fine for me.
>>> Either 1.0.0 or 1.0.1 will work fine.
>>>
>>>   Should I file a JIRA or is this a known issue?
>>>
>>> Thanks,
>>> Ron
>>>
>>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>>> due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
>>> Exception failure in TID 0 on host localhost:
>>> java.lang.IncompatibleClassChangeError: Found interface
>>> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
>>>
>>>=20
>>>org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyI
>>>nputFormat.java:47)
>>>
>>>=20
>>>org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
>>>        =20
>>>org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>>>        =20
>>>org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>        =20
>>>org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>>
>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>>>
>>>=20
>>>java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jav
>>>a:1145)
>>>
>>>=20
>>>java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.ja
>>>va:615)
>>>         java.lang.Thread.run(Thread.java:745)
>>
>>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>



--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 16:45:16 2014
Return-Path: <dev-return-8707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8C489115AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 16:45:16 +0000 (UTC)
Received: (qmail 45272 invoked by uid 500); 4 Aug 2014 16:45:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45166 invoked by uid 500); 4 Aug 2014 16:45:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44360 invoked by uid 99); 4 Aug 2014 16:45:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 16:45:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 16:45:10 +0000
Received: by mail-oi0-f46.google.com with SMTP id i138so4712989oig.5
        for <multiple recipients>; Mon, 04 Aug 2014 09:44:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=C6FIIQ7A+kDAV76nlVRiaw6jemPu9vGHEQXkmTpig9c=;
        b=spXgNp81TZtqUp8dRvPm0X7Or/pqKUNgNoMIJEa721hwSm3nk+VCFCgLbYs4qMLZkU
         FdfR9W34dH6S3+boCB8heYQFLwy82n2XwYrXqfqoVQgb0y1KKI4sYccZXiY5AqB7MKZl
         4OTpyXFu5b4sKQAPdaJs3tG9TbXCHBfriMI5K8IKuhIKNPPFC2soF8JJX4lwF5m/dFM6
         VRfJTkEcVUb1wpVG+V/zA6awA5jmFmOOsL24IJqnZK96VMH5v3od86bkmBD+OYZUj5Y6
         tAwX0JzTuyNnVGaYO5v5Ot5uHjuexwVgfGCqp2UE85697AwlW7LWSZQaSjEffui1FZN4
         tj1w==
MIME-Version: 1.0
X-Received: by 10.60.94.208 with SMTP id de16mr34111010oeb.52.1407170689583;
 Mon, 04 Aug 2014 09:44:49 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 4 Aug 2014 09:44:49 -0700 (PDT)
In-Reply-To: <FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
	<CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
	<CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
	<CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
	<CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
	<CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
	<CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
	<CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
	<CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
	<CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
	<D004E2D9.528E%snunez@hortonworks.com>
	<FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
Date: Mon, 4 Aug 2014 09:44:49 -0700
Message-ID: <CABPQxss=VySKcut05Q-B8fhJ0AXD71UA9tFn2QX24ZV7KNYD6w@mail.gmail.com>
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Patrick Wendell <pwendell@gmail.com>
To: "Ron's Yahoo!" <zlgonzalez@yahoo.com.invalid>
Cc: Steve Nunez <snunez@hortonworks.com>, user@spark.apache.org, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0116009c174e9b04ffd07538
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0116009c174e9b04ffd07538
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Can you try building without any of the special `hadoop.version` flags and
just building only with -Phadoop-2.4? In the past users have reported
issues trying to build random spot versions... I think HW is supposed to be
compatible with the normal 2.4.0 build.


On Mon, Aug 4, 2014 at 8:35 AM, Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>
wrote:

> Thanks, I ensured that $SPARK_HOME/pom.xml had the HDP repository under
> the repositories element. I also confirmed that if the build couldn't fin=
d
> the version, it would fail fast so it seems as if it's able to get the
> versions it needs to build the distribution.
> I ran the following (generated from make-distribution.sh), but it did not
> address the problem, while building with an older version
> (2.4.0.2.1.2.0-402) worked. Any other thing I can try?
>
> mvn clean package -Phadoop-2.4 -Phive -Pyarn
> -Dyarn.version=3D2.4.0.2.1.2.0-563 -Dhadoop.version=3D2.4.0.2.1.3.0-563
> -DskipTests
>
>
> Thanks,
> Ron
>
>
> On Aug 4, 2014, at 7:13 AM, Steve Nunez <snunez@hortonworks.com> wrote:
>
> Provided you=B9ve got the HWX repo in your pom.xml, you can build with th=
is
> line:
>
> mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0.2.1.1.0-385
> -DskipTests clean package
>
> I haven=B9t tried building a distro, but it should be similar.
>
>
> - SteveN
>
> On 8/4/14, 1:25, "Sean Owen" <sowen@cloudera.com> wrote:
>
> For any Hadoop 2.4 distro, yes, set hadoop.version but also set
> -Phadoop-2.4. http://spark.apache.org/docs/latest/building-with-maven.htm=
l
>
> On Mon, Aug 4, 2014 at 9:15 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> For hortonworks, I believe it should work to just link against the
> corresponding upstream version. I.e. just set the Hadoop version to
> "2.4.0"
>
> Does that work?
>
> - Patrick
>
>
> On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo!
> <zlgonzalez@yahoo.com.invalid>
> wrote:
>
>
> Hi,
>  Not sure whose issue this is, but if I run make-distribution using
> HDP
> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
> make-distribution.sh), I get a strange error with the exception below.
> If I
> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
> make-distribution, using the generated assembly all works fine for me.
> Either 1.0.0 or 1.0.1 will work fine.
>
>  Should I file a JIRA or is this a known issue?
>
> Thanks,
> Ron
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted
> due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
> Exception failure in TID 0 on host localhost:
> java.lang.IncompatibleClassChangeError: Found interface
> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
>
>
> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyI
> nputFormat.java:47)
>
>
> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
>
> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>
> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
> org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>        org.apache.spark.scheduler.Task.run(Task.scala:51)
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jav
> a:1145)
>
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.ja
> va:615)
>        java.lang.Thread.run(Thread.java:745)
>
>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
>
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
>
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
>
> and delete it from your system. Thank You.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> For additional commands, e-mail: user-help@spark.apache.org
>
>
>

--089e0116009c174e9b04ffd07538--

From dev-return-8708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 17:09:25 2014
Return-Path: <dev-return-8708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72334116EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 17:09:25 +0000 (UTC)
Received: (qmail 21884 invoked by uid 500); 4 Aug 2014 17:09:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21830 invoked by uid 500); 4 Aug 2014 17:09:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21560 invoked by uid 99); 4 Aug 2014 17:09:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:09:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:09:13 +0000
Received: by mail-oi0-f45.google.com with SMTP id e131so4808626oig.18
        for <multiple recipients>; Mon, 04 Aug 2014 10:08:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=L5iyHS5hfUfiLFxqILZRzQSqbCX0Xi9SGkWp5m57I1s=;
        b=exd9/X+lQR1z1R+DLKS2Fcu3D27leXzhymE5g0aU9zK56ArPCjI0xPyYVOh3+Kv2Bu
         WgF6Lk/1ES/j1PNg96ybzabdt526SxnbhIZrGmZhXo/dG1pXWVNV/FUPYd7VS76UTrzP
         4oAvrAznkMDE768xwy9X5+EE7sVtfOisoBpzx0Rgc/Qamni8rSZvbGcAqw+O9rl2Denw
         /bMJDI8G8yc0X1wgQTCdGLs+pdkfgwezsUtmJvt7/hXFh1r9Ky/W7S2GGvpabE4KEdRH
         xU1PNm0yG1xLA/GDW1ud/HuRSRrHkbdnflXv3QN12ExDfMYV3wQUa30LDY8F2EEdba+5
         mzUw==
MIME-Version: 1.0
X-Received: by 10.60.42.226 with SMTP id r2mr34506577oel.69.1407172133008;
 Mon, 04 Aug 2014 10:08:53 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 4 Aug 2014 10:08:52 -0700 (PDT)
In-Reply-To: <763EE8DE-2294-4FF6-898C-4EF4FC4B8CCF@yahoo.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
	<CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
	<CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
	<CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
	<CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
	<CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
	<CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
	<CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
	<CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
	<CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
	<D004E2D9.528E%snunez@hortonworks.com>
	<FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
	<CABPQxss=VySKcut05Q-B8fhJ0AXD71UA9tFn2QX24ZV7KNYD6w@mail.gmail.com>
	<6701C9FA-EEA1-4605-BB70-D47D885465BF@yahoo.com>
	<763EE8DE-2294-4FF6-898C-4EF4FC4B8CCF@yahoo.com>
Date: Mon, 4 Aug 2014 10:08:52 -0700
Message-ID: <CABPQxss=UhqkhoicS5rb3vYNEZMYmj1ZveQFZkYiRxC-kruLyg@mail.gmail.com>
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Patrick Wendell <pwendell@gmail.com>
To: "Ron's Yahoo!" <zlgonzalez@yahoo.com>
Cc: "Ron's Yahoo!" <zlgonzalez@yahoo.com.invalid>, Steve Nunez <snunez@hortonworks.com>, 
	user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c20c4c207e4b04ffd0cbce
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c20c4c207e4b04ffd0cbce
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Ah I see, yeah you might need to set hadoop.version and yarn.version. I
thought he profile set this automatically.


On Mon, Aug 4, 2014 at 10:02 AM, Ron's Yahoo! <zlgonzalez@yahoo.com> wrote:

> I meant yarn and hadoop defaulted to 1.0.4 so the yarn build fails since
> 1.0.4 doesn't exist for yarn...
>
> Thanks,
> Ron
>
> On Aug 4, 2014, at 10:01 AM, Ron's Yahoo! <zlgonzalez@yahoo.com> wrote:
>
> That failed since it defaulted the versions for yarn and hadoop
> I'll give it a try with just 2.4.0 for both yarn and hadoop...
>
> Thanks,
> Ron
>
> On Aug 4, 2014, at 9:44 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
> Can you try building without any of the special `hadoop.version` flags an=
d
> just building only with -Phadoop-2.4? In the past users have reported
> issues trying to build random spot versions... I think HW is supposed to =
be
> compatible with the normal 2.4.0 build.
>
>
> On Mon, Aug 4, 2014 at 8:35 AM, Ron's Yahoo! <zlgonzalez@yahoo.com.invali=
d
> > wrote:
>
>> Thanks, I ensured that $SPARK_HOME/pom.xml had the HDP repository under
>> the repositories element. I also confirmed that if the build couldn't fi=
nd
>> the version, it would fail fast so it seems as if it's able to get the
>> versions it needs to build the distribution.
>> I ran the following (generated from make-distribution.sh), but it did no=
t
>> address the problem, while building with an older version
>> (2.4.0.2.1.2.0-402) worked. Any other thing I can try?
>>
>> mvn clean package -Phadoop-2.4 -Phive -Pyarn
>> -Dyarn.version=3D2.4.0.2.1.2.0-563 -Dhadoop.version=3D2.4.0.2.1.3.0-563
>> -DskipTests
>>
>>
>> Thanks,
>> Ron
>>
>>
>> On Aug 4, 2014, at 7:13 AM, Steve Nunez <snunez@hortonworks.com> wrote:
>>
>> Provided you=B9ve got the HWX repo in your pom.xml, you can build with t=
his
>> line:
>>
>> mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0.2.1.1.0-385
>> -DskipTests clean package
>>
>> I haven=B9t tried building a distro, but it should be similar.
>>
>>
>> - SteveN
>>
>> On 8/4/14, 1:25, "Sean Owen" <sowen@cloudera.com> wrote:
>>
>> For any Hadoop 2.4 distro, yes, set hadoop.version but also set
>> -Phadoop-2.4.
>> http://spark.apache.org/docs/latest/building-with-maven.html
>>
>> On Mon, Aug 4, 2014 at 9:15 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> For hortonworks, I believe it should work to just link against the
>> corresponding upstream version. I.e. just set the Hadoop version to
>> "2.4.0"
>>
>> Does that work?
>>
>> - Patrick
>>
>>
>> On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo!
>> <zlgonzalez@yahoo.com.invalid>
>> wrote:
>>
>>
>> Hi,
>>  Not sure whose issue this is, but if I run make-distribution using
>> HDP
>> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
>> make-distribution.sh), I get a strange error with the exception below.
>> If I
>> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
>> make-distribution, using the generated assembly all works fine for me.
>> Either 1.0.0 or 1.0.1 will work fine.
>>
>>  Should I file a JIRA or is this a known issue?
>>
>> Thanks,
>> Ron
>>
>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>> due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
>> Exception failure in TID 0 on host localhost:
>> java.lang.IncompatibleClassChangeError: Found interface
>> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
>>
>>
>> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyI
>> nputFormat.java:47)
>>
>>
>> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
>>
>> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>>
>> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>> org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>>        org.apache.spark.scheduler.Task.run(Task.scala:51)
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>>
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jav
>> a:1145)
>>
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.ja
>> va:615)
>>        java.lang.Thread.run(Thread.java:745)
>>
>>
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>>
>>
>> --
>> CONFIDENTIALITY NOTICE
>> NOTICE: This message is intended for the use of the individual or entity
>> to
>> which it is addressed and may contain information that is confidential,
>> privileged and exempt from disclosure under applicable law. If the reade=
r
>>
>> of this message is not the intended recipient, you are hereby notified
>> that
>> any printing, copying, dissemination, distribution, disclosure or
>> forwarding of this communication is strictly prohibited. If you have
>> received this communication in error, please contact the sender
>> immediately
>> and delete it from your system. Thank You.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>>
>>
>
>
>

--001a11c20c4c207e4b04ffd0cbce--

From dev-return-8709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 17:21:57 2014
Return-Path: <dev-return-8709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F29FD11786
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 17:21:57 +0000 (UTC)
Received: (qmail 59484 invoked by uid 500); 4 Aug 2014 17:21:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59423 invoked by uid 500); 4 Aug 2014 17:21:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59412 invoked by uid 99); 4 Aug 2014 17:21:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:21:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.220.42 as permitted sender)
Received: from [209.85.220.42] (HELO mail-pa0-f42.google.com) (209.85.220.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:21:53 +0000
Received: by mail-pa0-f42.google.com with SMTP id lf10so10432385pab.1
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 10:21:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:cc:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type;
        bh=2y5o0EIkDG7F4iHH47jDPS1YUOnoeQmCz3VpSOsi1UU=;
        b=UYoaIYSeao9A2g4NBYSKCSAvRGkVzBNQp0iazUGqZkRjb63HL7651m7E8kWuOigkP+
         BcwhuDhRTAWTi7U+5YihMEKt1Ltj7mM2lQsZLIQwrQzE7r8v3PvTzgy7tGJ4OujvCEPm
         26i2qZJZ19OVLTxuh1ecQKg25kTe6hM5/8ZexxrZjDFEa5KHqH7BW6ihxwG4BNUY4b9+
         DfjbPZ95CuRY27YMBxygW/0N9smxkmwnE+578BmDbJ3OACk3SMnyAG6EnLyTgKY0Ce93
         GDxBFriYSHzqVnhOXE4EKYDzZZyIoHw/X9PR1VY/Ky4YCRixLWMRCKv9639EjgNm8Ykq
         swUw==
X-Gm-Message-State: ALoCoQl1pEjLVW4FEDnpefiCJKBheSWYstsyZ9mGvVd+wLC9XrKHF/sI9f1e0fXU9NZGtjFMbR1MuqyTpHlBJoQvbR6cdDEQrjhSViRluAfljOXjceRMeUw=
X-Received: by 10.70.132.38 with SMTP id or6mr25604145pdb.24.1407172887668;
        Mon, 04 Aug 2014 10:21:27 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id po5sm27588840pdb.55.2014.08.04.10.21.24
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 04 Aug 2014 10:21:27 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 04 Aug 2014 10:21:19 -0700
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Steve Nunez <snunez@hortonworks.com>
To: Patrick Wendell <pwendell@gmail.com>,
	Ron's Yahoo! <zlgonzalez@yahoo.com>
CC: <user@spark.apache.org>,
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <D0050F02.52E7%snunez@hortonworks.com>
Thread-Topic: Issues with HDP 2.4.0.2.1.3.0-563
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
 <CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
 <CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
 <CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
 <CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
 <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
 <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
 <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
 <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
 <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
 <D004E2D9.528E%snunez@hortonworks.com>
 <FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
 <CABPQxss=VySKcut05Q-B8fhJ0AXD71UA9tFn2QX24ZV7KNYD6w@mail.gmail.com>
 <6701C9FA-EEA1-4605-BB70-D47D885465BF@yahoo.com>
 <763EE8DE-2294-4FF6-898C-4EF4FC4B8CCF@yahoo.com>
 <CABPQxss=UhqkhoicS5rb3vYNEZMYmj1ZveQFZkYiRxC-kruLyg@mail.gmail.com>
In-Reply-To: <CABPQxss=UhqkhoicS5rb3vYNEZMYmj1ZveQFZkYiRxC-kruLyg@mail.gmail.com>
Mime-version: 1.0
Content-type: multipart/alternative;
	boundary="B_3489992485_8091734"
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3489992485_8091734
Content-type: text/plain; charset=EUC-KR
Content-transfer-encoding: quoted-printable

I don=A1=AFt think there is an hwx profile, but there probably should be.

- Steve

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Monday, August 4, 2014 at 10:08
To:  Ron's Yahoo! <zlgonzalez@yahoo.com>
Cc:  Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>, Steve Nunez
<snunez@hortonworks.com>, <user@spark.apache.org>, "dev@spark.apache.org"
<dev@spark.apache.org>
Subject:  Re: Issues with HDP 2.4.0.2.1.3.0-563

Ah I see, yeah you might need to set hadoop.version and yarn.version. I
thought he profile set this automatically.


On Mon, Aug 4, 2014 at 10:02 AM, Ron's Yahoo! <zlgonzalez@yahoo.com> wrote:
> I meant yarn and hadoop defaulted to 1.0.4 so the yarn build fails since =
1.0.4
> doesn=A1=AFt exist for yarn...
>=20
> Thanks,
> Ron
>=20
> On Aug 4, 2014, at 10:01 AM, Ron's Yahoo! <zlgonzalez@yahoo.com> wrote:
>=20
>> That failed since it defaulted the versions for yarn and hadoop
>> I=A1=AFll give it a try with just 2.4.0 for both yarn and hadoop=A1=A6
>>=20
>> Thanks,
>> Ron
>>=20
>> On Aug 4, 2014, at 9:44 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>=20
>>> Can you try building without any of the special `hadoop.version` flags =
and
>>> just building only with -Phadoop-2.4? In the past users have reported i=
ssues
>>> trying to build random spot versions... I think HW is supposed to be
>>> compatible with the normal 2.4.0 build.
>>>=20
>>>=20
>>> On Mon, Aug 4, 2014 at 8:35 AM, Ron's Yahoo! <zlgonzalez@yahoo.com.inva=
lid>
>>> wrote:
>>>> Thanks, I ensured that $SPARK_HOME/pom.xml had the HDP repository unde=
r the
>>>> repositories element. I also confirmed that if the build couldn=A1=AFt=
 find the
>>>> version, it would fail fast so it seems as if it=A1=AFs able to get th=
e versions
>>>> it needs to build the distribution.
>>>> I ran the following (generated from make-distribution.sh), but it did =
not
>>>> address the problem, while building with an older version
>>>> (2.4.0.2.1.2.0-402) worked. Any other thing I can try?
>>>>=20
>>>> mvn clean package -Phadoop-2.4 -Phive -Pyarn
>>>> -Dyarn.version=3D2.4.0.2.1.2.0-563 -Dhadoop.version=3D2.4.0.2.1.3.0-56=
3
>>>> -DskipTests
>>>>=20
>>>>=20
>>>> Thanks,
>>>> Ron
>>>>=20
>>>>=20
>>>> On Aug 4, 2014, at 7:13 AM, Steve Nunez <snunez@hortonworks.com> wrote=
:
>>>>=20
>>>>> Provided you=A9=F6ve got the HWX repo in your pom.xml, you can build =
with this
>>>>> line:
>>>>>=20
>>>>> mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0.2.1.1.0-385
>>>>> -DskipTests clean package
>>>>>=20
>>>>> I haven=A9=F6t tried building a distro, but it should be similar.
>>>>>=20
>>>>>=20
>>>>> - SteveN
>>>>>=20
>>>>> On 8/4/14, 1:25, "Sean Owen" <sowen@cloudera.com> wrote:
>>>>>=20
>>>>>> For any Hadoop 2.4 distro, yes, set hadoop.version but also set
>>>>>> -Phadoop-2.4.
>>>>>> http://spark.apache.org/docs/latest/building-with-maven.html
>>>>>>=20
>>>>>> On Mon, Aug 4, 2014 at 9:15 AM, Patrick Wendell <pwendell@gmail.com>
>>>>>> wrote:
>>>>>>> For hortonworks, I believe it should work to just link against the
>>>>>>> corresponding upstream version. I.e. just set the Hadoop version to
>>>>>>> "2.4.0"
>>>>>>>=20
>>>>>>> Does that work?
>>>>>>>=20
>>>>>>> - Patrick
>>>>>>>=20
>>>>>>>=20
>>>>>>> On Mon, Aug 4, 2014 at 12:13 AM, Ron's Yahoo!
>>>>>>> <zlgonzalez@yahoo.com.invalid>
>>>>>>> wrote:
>>>>>>>=20
>>>>>>> Hi,
>>>>>>>  Not sure whose issue this is, but if I run make-distribution using
>>>>>>> HDP
>>>>>>> 2.4.0.2.1.3.0-563 as the hadoop version (replacing it in
>>>>>>> make-distribution.sh), I get a strange error with the exception bel=
ow.
>>>>>>> If I
>>>>>>> use a slightly older version of HDP (2.4.0.2.1.2.0-402) with
>>>>>>> make-distribution, using the generated assembly all works fine for =
me.
>>>>>>> Either 1.0.0 or 1.0.1 will work fine.
>>>>>>>=20
>>>>>>>  Should I file a JIRA or is this a known issue?
>>>>>>>=20
>>>>>>> Thanks,
>>>>>>> Ron
>>>>>>>=20
>>>>>>> Exception in thread "main" org.apache.spark.SparkException: Job abo=
rted
>>>>>>> due to stage failure: Task 0.0:0 failed 1 times, most recent failur=
e:
>>>>>>> Exception failure in TID 0 on host localhost:
>>>>>>> java.lang.IncompatibleClassChangeError: Found interface
>>>>>>> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expec=
ted
>>>>>>>=20
>>>>>>>=20
>>>>>>> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(Avr=
oKeyI
>>>>>>> nputFormat.java:47)
>>>>>>>=20
>>>>>>>=20
>>>>>>> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala=
:111)
>>>>>>>=20
>>>>>>> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
>>>>>>>=20
>>>>>>> org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
>>>>>>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:2=
62)
>>>>>>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>>>>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>>>>>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:2=
62)
>>>>>>>=20
>>>>>>> org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>>>>>>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>>>>>>        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>>>>>>>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:2=
62)
>>>>>>>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>>>>=20
>>>>>>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>>>>>>>        org.apache.spark.scheduler.Task.run(Task.scala:51)
>>>>>>>=20
>>>>>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:18=
7)
>>>>>>>=20
>>>>>>>=20
>>>>>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecuto=
r.jav
>>>>>>> a:1145)
>>>>>>>=20
>>>>>>>=20
>>>>>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecut=
or.ja
>>>>>>> va:615)
>>>>>>>        java.lang.Thread.run(Thread.java:745)
>>>>>>>=20
>>>>>>>=20
>>>>>>=20
>>>>>> --------------------------------------------------------------------=
-
>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>> --=20
>>>>> CONFIDENTIALITY NOTICE
>>>>> NOTICE: This message is intended for the use of the individual or ent=
ity
>>>>> to=20
>>>>> which it is addressed and may contain information that is confidentia=
l,
>>>>> privileged and exempt from disclosure under applicable law. If the re=
ader
>>>>> of this message is not the intended recipient, you are hereby notifie=
d
>>>>> that=20
>>>>> any printing, copying, dissemination, distribution, disclosure or
>>>>> forwarding of this communication is strictly prohibited. If you have
>>>>> received this communication in error, please contact the sender
>>>>> immediately=20
>>>>> and delete it from your system. Thank You.
>>>>>=20
>>>>> ---------------------------------------------------------------------
>>>>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: user-help@spark.apache.org
>>>>=20
>>>=20
>>=20
>=20




--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

--B_3489992485_8091734--



From dev-return-8710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 17:23:30 2014
Return-Path: <dev-return-8710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BDF3117AA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 17:23:30 +0000 (UTC)
Received: (qmail 66375 invoked by uid 500); 4 Aug 2014 17:23:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66325 invoked by uid 500); 4 Aug 2014 17:23:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66293 invoked by uid 99); 4 Aug 2014 17:23:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:23:29 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:23:25 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so5383646wiv.13
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 10:23:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=6lGXBsskqaby9jFINYjF4bF7YDHjiJvEO39i5Lj2x/8=;
        b=bT5+YrrEKCcFcYI300tI/J8ToXwZJflJ9YTjJqyI1Cm/X2h2zEWFQQ7FuJrN+FJI+P
         Q9UZBQiZpI3W9/rLN4rPkUmfDb5C8K+axoEgZw/b9vWB2F69N9OdTQ+y8IKvfef/XMDF
         RIEuHBZi+3FTU3GSMAyDD8n8UXa/CxDDj1KkuC7Ngufgva1L1IIjncvQ+9x5Xow07xrD
         vjZ7s73NF4YBwuctlhAfKZw+nkHUY8aFKheAhzs/kui+0QJb7fxKHeoAIW/8Nj+O1ymT
         6rqSrod5MuQ+9LlQk6v6QhnbF7n1xqdANSzMPlMH0r2avKO5lsiLe3yCrtuX+w1pB/qF
         fHSA==
X-Gm-Message-State: ALoCoQmg7UlFt2wACLQL7QEukL8Y4EgmM/sZyoCdT8h79CdM9SPxern2NAyhFa0IexErxzOcOdPF
MIME-Version: 1.0
X-Received: by 10.180.73.139 with SMTP id l11mr31138459wiv.30.1407172980604;
 Mon, 04 Aug 2014 10:23:00 -0700 (PDT)
Received: by 10.180.94.34 with HTTP; Mon, 4 Aug 2014 10:23:00 -0700 (PDT)
Date: Mon, 4 Aug 2014 10:23:00 -0700
Message-ID: <CAFZt-EQES0-eqRoYaPLJUPQ3gK4MbrbKQvZSQvV5PyKkk9crvw@mail.gmail.com>
Subject: Problems running modified spark version on ec2 cluster
From: Matt Forbes <matt@tellapart.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043c07dea5b87d04ffd0fd67
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c07dea5b87d04ffd0fd67
Content-Type: text/plain; charset=UTF-8

I'm trying to run a forked version of mllib where I am experimenting with a
boosted trees implementation. Here is what I've tried, but can't seem to
get working properly:

*Directory layout:*

src/spark-dev  (spark github fork)
  pom.xml - I've tried changing the version to 1.2 arbitrarily in core and
mllib
src/forestry  (test driver)
  pom.xml - depends on spark-core and spark-mllib with version 1.2

*spark-defaults.conf:*

spark.master                    spark://
ec2-54-224-112-117.compute-1.amazonaws.com:7077
spark.verbose                   true
spark.files.userClassPathFirst  false  # I've tried both true and false here
spark.executor-memory           6G
spark.jars
 spark-mllib_2.10-1.2.0-SNAPSHOT.jar,spark-core_2.10-1.2.0-SNAPSHOT.jar,spark-streaming_2.10-1.2.0-SNAPSHOT.jar

*Build and run script:*

MASTER=root@ec2-54-224-112-117.compute-1.amazonaws.com
PRIMARY_JAR=forestry-main-1.0-SNAPSHOT-jar-with-dependencies.jar
FORESTRY_DIR=~/src/forestry-main
SPARK_DIR=~/src/spark-dev
cd $SPARK_DIR
mvn -T8 -DskipTests -pl core,mllib,streaming install
cd $FORESTRY_DIR
mvn -T8 -DskipTests package
rsync --progress
~/src/spark-dev/mllib/target/spark-mllib_2.10-1.2.0-SNAPSHOT.jar $MASTER:
rsync --progress
~/src/spark-dev/core/target/spark-core_2.10-1.2.0-SNAPSHOT.jar $MASTER:
rsync --progress
~/src/spark-dev/streaming/target/spark-streaming_2.10-1.2.0-SNAPSHOT.jar
$MASTER:
rsync --progress ~/src/forestry-main/target/$PRIMARY_JAR $MASTER:
rsync --progress ~/src/forestry-main/spark-defaults.conf $MASTER:spark/conf
ssh $MASTER "spark/bin/spark-submit $PRIMARY_JAR --class forestry.TreeTest
--verbose"

In spark-dev/mllib I've added a new class, GradientBoostingTree, which I'm
referencing from TreeTest in my test driver. The driver pulls some data
from s3, converts to LabeledPoint, and then calls
GradientBoostingTree.train(...) identically to how DecisionTree works. This
is all fine until it we call examples.map { x => tree.predict(x.features) }
where tree is a DecisionTree that I've also modified in my fork. At this
point, the workers blow up because they can't find a new method I've added
to the tree.model.Node class. My suspicion is that maybe the workers have
deserialized the DecisionTreeModel into a different version of mllib that
doesn't have my changes?

Is my setup all wrong? I'm using an EC2 cluster because it is so easy to
startup and manage, maybe I need to fully distribute my new version of
spark to all the workers before starting the job? Is there an easy way to
do that?

--f46d043c07dea5b87d04ffd0fd67--

From dev-return-8711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 17:36:42 2014
Return-Path: <dev-return-8711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 68EC611854
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 17:36:42 +0000 (UTC)
Received: (qmail 7307 invoked by uid 500); 4 Aug 2014 17:36:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7220 invoked by uid 500); 4 Aug 2014 17:36:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6159 invoked by uid 99); 4 Aug 2014 17:36:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:36:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:36:38 +0000
Received: by mail-vc0-f173.google.com with SMTP id hy10so11590504vcb.4
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 10:36:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=4RzhkT1k3nzdcfV8hNJYNYGILlZ+zHtG75x2EGvgArM=;
        b=UoMiKs7FRx0nnrqFm/Tu4WTgHnGmwPPfOeJOpZb/0u0rWSkN3WPwKBez6MLWXOlrZo
         okbXLYrvMcemOZM6aAVi43hfI1hrmOFoRhnoeJekZDmmV35SY5orcTYeuvWW8curukwX
         7qiAjl65oaXGtdYdvb7GT81+qPv8ZwpJA9SmAK2Xx5zwOp8xr/SJx6F3sBYWVDEYpxxq
         CLdAiwFNe26dcAzrDH2lsLIUC/BfExe6curDJUalj1emr3vDlyggV+8GQJQIOCBXpZYU
         zXoAna9ZVgFnffyB3zi/WgRNIzB8xknC9Hd/nB04IJQijlce2GpJuiSWV4Jj/VvCFv2M
         xo3A==
X-Gm-Message-State: ALoCoQnycGBI2gPkuEArK9vRBXbh4QTI2f5gKYs3JUYiL1V4dFVVoALlir6DiYSNq4sbVjEXlQ1e
X-Received: by 10.52.168.134 with SMTP id zw6mr21502472vdb.37.1407173773084;
 Mon, 04 Aug 2014 10:36:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 4 Aug 2014 10:35:52 -0700 (PDT)
In-Reply-To: <D0050F02.52E7%snunez@hortonworks.com>
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
 <CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
 <CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
 <CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
 <CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
 <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
 <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
 <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com> <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
 <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
 <D004E2D9.528E%snunez@hortonworks.com> <FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
 <CABPQxss=VySKcut05Q-B8fhJ0AXD71UA9tFn2QX24ZV7KNYD6w@mail.gmail.com>
 <6701C9FA-EEA1-4605-BB70-D47D885465BF@yahoo.com> <763EE8DE-2294-4FF6-898C-4EF4FC4B8CCF@yahoo.com>
 <CABPQxss=UhqkhoicS5rb3vYNEZMYmj1ZveQFZkYiRxC-kruLyg@mail.gmail.com> <D0050F02.52E7%snunez@hortonworks.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 4 Aug 2014 18:35:52 +0100
Message-ID: <CAMAsSdJgN4LcKK5=B6RtVouP18phObo2vD-w_wpz5gcCxVV+1g@mail.gmail.com>
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
To: Steve Nunez <snunez@hortonworks.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "Ron's Yahoo!" <zlgonzalez@yahoo.com>, user@spark.apache.org, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

What would such a profile do though? In general building for a
specific vendor version means setting hadoop.verison and/or
yarn.version. Any hard-coded value is unlikely to match what a
particular user needs. Setting protobuf versions and so on is already
done by the generic profiles.

In a similar vein, I am not clear on why there's a mapr profile in the
build. Its versions are about to be out of date and won't work with
upcoming Hbase changes for example.

(Elsewhere in the build I think it wouldn't hurt to clear out
cloudera-specific profiles and releases too -- they're not in the pom
but are in the distribution script. It's the vendor's problem.)

This isn't any argument about being purist but just that I am not sure
these are things that the project can meaningfully bother with.

It makes sense to set vendor repos in the pom for convenience, and
makes sense to run smoke tests in Jenkins against particular versions.

$0.02
Sean

On Mon, Aug 4, 2014 at 6:21 PM, Steve Nunez <snunez@hortonworks.com> wrote:
> I don=E2=80=99t think there is an hwx profile, but there probably should =
be.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 17:39:51 2014
Return-Path: <dev-return-8712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 174FC1186B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 17:39:51 +0000 (UTC)
Received: (qmail 18902 invoked by uid 500); 4 Aug 2014 17:39:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18785 invoked by uid 500); 4 Aug 2014 17:39:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18034 invoked by uid 99); 4 Aug 2014 17:39:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:39:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.192.175 as permitted sender)
Received: from [209.85.192.175] (HELO mail-pd0-f175.google.com) (209.85.192.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 17:39:46 +0000
Received: by mail-pd0-f175.google.com with SMTP id r10so9992069pdi.20
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 10:39:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:cc:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type
         :content-transfer-encoding;
        bh=WuLLKsywq68ZWtMCZPqAGlKjakKjARYywj4C/xlXKTs=;
        b=GZn0E1au5Vz3k0WjmREzxPaB4yWha4W15ZfC/JALXIWFmldeb/Cfy6OeBNfWor6YOV
         AzxiU4cVsI8FUqvrBldd98L4yqgUt+yXmjPdcotBOqSue+XRenKmDU/jjWq8fjjXNyNv
         12m8ud6Hond2BSbH8BWdtQKeq6K24mW9UGMdYkXwVN+WMOOs9USyqStVtZ0WBidyQ+8u
         32R+rxyLM1P6tOYLeB8bCZlOHOpXy4rTPoiaQOsjv8xspVCoMB1AfQVQo8iDFQDxq1/O
         3RLXSNkjsksYoeI1gul2Sx69dKZ9gboExmn2K069aWb0EWIURCXPZbSRQwCcV//UlYw9
         cgQw==
X-Gm-Message-State: ALoCoQlhP+96e8QClxzxPklSp+myj1KYDYJ4ZGvQMUjI/gpIEkPS8bnSJUEI6v6Q3iTuzX42HMi1RzLva+Dw7u2KIg2mmQ6f8o10Lyq5yIsKQdK2Bt4qhPI=
X-Received: by 10.67.4.1 with SMTP id ca1mr23359338pad.50.1407173961106;
        Mon, 04 Aug 2014 10:39:21 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id fn2sm65387660pab.22.2014.08.04.10.39.17
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 04 Aug 2014 10:39:20 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 04 Aug 2014 10:39:13 -0700
Subject: Re: Issues with HDP 2.4.0.2.1.3.0-563
From: Steve Nunez <snunez@hortonworks.com>
To: Sean Owen <sowen@cloudera.com>
CC: <user@spark.apache.org>,
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <D00512CD.5301%snunez@hortonworks.com>
Thread-Topic: Issues with HDP 2.4.0.2.1.3.0-563
References: <CANrtgzURdHQeE_SoqTLhsM3aj_Xv3Ks0dVs4Uzum3bOAr0LPkw@mail.gmail.com>
 <CAAswR-5_sUtFjOzhsyLXhf29imf_Wuv++vYVBUPLN9fo0S6iFA@mail.gmail.com>
 <CANrtgzUJ6rXK1qmG1LQZ2Lww96goh7GdY=CwVtNaM44EBESv-Q@mail.gmail.com>
 <CAAswR-6c3irx4dHv66L4ZrmhufsNXj8NojZZLV4tSAviB2fECg@mail.gmail.com>
 <CANrtgzU1JiM3wYCsXTWjrt4QoEGzt0ny-UieR1ZHRONb0gs25Q@mail.gmail.com>
 <CANrtgzWe6ZU7+g-5+jZQMqK0U8tZaQTQhJrhvk57+qfC_hYHGQ@mail.gmail.com>
 <CANrtgzUWTVV4Eg3LBF3Svn6NXvAJmj-AmdMyQme8PcCoOt0uTQ@mail.gmail.com>
 <CF8E8E71-7E52-4F8B-AAD3-112EC2A3014C@yahoo.com>
 <CABPQxssFhH6dP-uwp8eOq2U_bQdas2VMNmzYTHFooFxKKzKGtA@mail.gmail.com>
 <CAMAsSdKkybvi7QNXKX34Zf0dRsd1BVtm0nUJMyrbTbiDAhtv0w@mail.gmail.com>
 <D004E2D9.528E%snunez@hortonworks.com>
 <FDBB5EBE-B314-4F33-BED4-1DA407D1C463@yahoo.com>
 <CABPQxss=VySKcut05Q-B8fhJ0AXD71UA9tFn2QX24ZV7KNYD6w@mail.gmail.com>
 <6701C9FA-EEA1-4605-BB70-D47D885465BF@yahoo.com>
 <763EE8DE-2294-4FF6-898C-4EF4FC4B8CCF@yahoo.com>
 <CABPQxss=UhqkhoicS5rb3vYNEZMYmj1ZveQFZkYiRxC-kruLyg@mail.gmail.com>
 <D0050F02.52E7%snunez@hortonworks.com>
 <CAMAsSdJgN4LcKK5=B6RtVouP18phObo2vD-w_wpz5gcCxVV+1g@mail.gmail.com>
In-Reply-To: <CAMAsSdJgN4LcKK5=B6RtVouP18phObo2vD-w_wpz5gcCxVV+1g@mail.gmail.com>
Mime-version: 1.0
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hmm. Fair enough. I hadn=B9t given that answer much thought and on
reflection think you=B9re right in that a profile would just be a bad hack.



On 8/4/14, 10:35, "Sean Owen" <sowen@cloudera.com> wrote:

>What would such a profile do though? In general building for a
>specific vendor version means setting hadoop.verison and/or
>yarn.version. Any hard-coded value is unlikely to match what a
>particular user needs. Setting protobuf versions and so on is already
>done by the generic profiles.
>
>In a similar vein, I am not clear on why there's a mapr profile in the
>build. Its versions are about to be out of date and won't work with
>upcoming Hbase changes for example.
>
>(Elsewhere in the build I think it wouldn't hurt to clear out
>cloudera-specific profiles and releases too -- they're not in the pom
>but are in the distribution script. It's the vendor's problem.)
>
>This isn't any argument about being purist but just that I am not sure
>these are things that the project can meaningfully bother with.
>
>It makes sense to set vendor repos in the pom for convenience, and
>makes sense to run smoke tests in Jenkins against particular versions.
>
>$0.02
>Sean
>
>On Mon, Aug 4, 2014 at 6:21 PM, Steve Nunez <snunez@hortonworks.com>
>wrote:
>> I don=B9t think there is an hwx profile, but there probably should be.
>>



--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 19:14:46 2014
Return-Path: <dev-return-8713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A980011E18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 19:14:46 +0000 (UTC)
Received: (qmail 13495 invoked by uid 500); 4 Aug 2014 19:14:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13431 invoked by uid 500); 4 Aug 2014 19:14:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13420 invoked by uid 99); 4 Aug 2014 19:14:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 19:14:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 19:14:43 +0000
Received: by mail-wg0-f42.google.com with SMTP id l18so7946715wgh.13
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 12:14:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=8b4P+YGEAY7QV3ogbtSC+ogF+4ZwmzPrDbVgICn/r1k=;
        b=FzDCFldpK/pf7hYK9RbFwKjH1LmvuBhqzPeLiR6ogHvsj0kdljYESTZsPcepe0HTKe
         wHgAI89NIPSCWfktWcnHFyXvYvCzC5+u3Y1P2So+SN4LLhJ28zzXUkzt6stzGkD4p4Ss
         EX3RVR+1gC30xWe8sgBhIg1eMQpAxANvDG82L0o1WCFINy+s+4oLO2a1ggIvMYzfhXc+
         +1rogallsavFXsXGzNfOvmaqM06j4Lc2J6sj0KmNKCBvh69w0APfDBzSEZ+zHKIUyhab
         tXCL/2QLKXuqVqy5phnzUUSU0D8BHGOM4pf2cKMuupHpsHE2jj8G8OZGr05bZlkYWotu
         nN6w==
X-Gm-Message-State: ALoCoQlOV9Rn2Dw495KXMXM1wFznssvoxACA2ZVKuXdNcxbitfcX+EGEjx2a9HDKcBbDWr1TvBmu
MIME-Version: 1.0
X-Received: by 10.180.95.68 with SMTP id di4mr12797091wib.60.1407179654761;
 Mon, 04 Aug 2014 12:14:14 -0700 (PDT)
Received: by 10.180.94.34 with HTTP; Mon, 4 Aug 2014 12:14:14 -0700 (PDT)
In-Reply-To: <CAFZt-EQES0-eqRoYaPLJUPQ3gK4MbrbKQvZSQvV5PyKkk9crvw@mail.gmail.com>
References: <CAFZt-EQES0-eqRoYaPLJUPQ3gK4MbrbKQvZSQvV5PyKkk9crvw@mail.gmail.com>
Date: Mon, 4 Aug 2014 12:14:14 -0700
Message-ID: <CAFZt-ESvpJN044HDxgh-b3=XxL7j7Rk-DB1Ru6h1v=N=V5P0jw@mail.gmail.com>
Subject: Re: Problems running modified spark version on ec2 cluster
From: Matt Forbes <matt@tellapart.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04138dbf7526ce04ffd28b9d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04138dbf7526ce04ffd28b9d
Content-Type: text/plain; charset=UTF-8

After rummaging through the worker instances I noticed they were using the
assembly jar (which I hadn't noticed before). Now instead of using the core
and mllib jars individually, I'm just overwriting the assembly jar in the
master and using spark-ec2/copy-dir. For posterity, my run script is:

MASTER=root@ec2-54-224-110-72.compute-1.amazonaws.com
PRIMARY_JAR=forestry-main-1.0-SNAPSHOT-jar-with-dependencies.jar
ASSEMBLY_SRC=spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.4.jar
ASSEMBLY_DEST=spark-assembly-1.0.1-hadoop1.0.4.jar
FORESTRY_DIR=~/src/forestry-main
SPARK_DIR=~/src/spark-dev
cd $SPARK_DIR
mvn -T8 -DskipTests -pl core,mllib,assembly install
cd $FORESTRY_DIR
mvn -T8 -DskipTests package
rsync --progress ~/src/spark-dev/assembly/target/scala-2.10/$ASSEMBLY_SRC
$MASTER:spark/lib/$ASSEMBLY_DEST
rsync --progress ~/src/forestry-main/target/$PRIMARY_JAR $MASTER:
rsync --progress ~/src/forestry-main/spark-defaults.conf $MASTER:spark/conf
ssh $MASTER "spark-ec2/copy-dir --delete /root/spark/lib"
ssh $MASTER "spark/bin/spark-submit $PRIMARY_JAR --class
com.ttforbes.TreeTest --verbose"



On Mon, Aug 4, 2014 at 10:23 AM, Matt Forbes <matt@tellapart.com> wrote:

> I'm trying to run a forked version of mllib where I am experimenting with
> a boosted trees implementation. Here is what I've tried, but can't seem to
> get working properly:
>
> *Directory layout:*
>
> src/spark-dev  (spark github fork)
>   pom.xml - I've tried changing the version to 1.2 arbitrarily in core and
> mllib
> src/forestry  (test driver)
>   pom.xml - depends on spark-core and spark-mllib with version 1.2
>
> *spark-defaults.conf:*
>
> spark.master                    spark://
> ec2-54-224-112-117.compute-1.amazonaws.com:7077
> spark.verbose                   true
> spark.files.userClassPathFirst  false  # I've tried both true and false
> here
> spark.executor-memory           6G
> spark.jars
>  spark-mllib_2.10-1.2.0-SNAPSHOT.jar,spark-core_2.10-1.2.0-SNAPSHOT.jar,spark-streaming_2.10-1.2.0-SNAPSHOT.jar
>
> *Build and run script:*
>
> MASTER=root@ec2-54-224-112-117.compute-1.amazonaws.com
> PRIMARY_JAR=forestry-main-1.0-SNAPSHOT-jar-with-dependencies.jar
> FORESTRY_DIR=~/src/forestry-main
> SPARK_DIR=~/src/spark-dev
> cd $SPARK_DIR
> mvn -T8 -DskipTests -pl core,mllib,streaming install
> cd $FORESTRY_DIR
> mvn -T8 -DskipTests package
> rsync --progress
> ~/src/spark-dev/mllib/target/spark-mllib_2.10-1.2.0-SNAPSHOT.jar $MASTER:
> rsync --progress
> ~/src/spark-dev/core/target/spark-core_2.10-1.2.0-SNAPSHOT.jar $MASTER:
> rsync --progress
> ~/src/spark-dev/streaming/target/spark-streaming_2.10-1.2.0-SNAPSHOT.jar
> $MASTER:
> rsync --progress ~/src/forestry-main/target/$PRIMARY_JAR $MASTER:
> rsync --progress ~/src/forestry-main/spark-defaults.conf $MASTER:spark/conf
> ssh $MASTER "spark/bin/spark-submit $PRIMARY_JAR --class forestry.TreeTest
> --verbose"
>
> In spark-dev/mllib I've added a new class, GradientBoostingTree, which I'm
> referencing from TreeTest in my test driver. The driver pulls some data
> from s3, converts to LabeledPoint, and then calls
> GradientBoostingTree.train(...) identically to how DecisionTree works. This
> is all fine until it we call examples.map { x => tree.predict(x.features) }
> where tree is a DecisionTree that I've also modified in my fork. At this
> point, the workers blow up because they can't find a new method I've added
> to the tree.model.Node class. My suspicion is that maybe the workers have
> deserialized the DecisionTreeModel into a different version of mllib that
> doesn't have my changes?
>
> Is my setup all wrong? I'm using an EC2 cluster because it is so easy to
> startup and manage, maybe I need to fully distribute my new version of
> spark to all the workers before starting the job? Is there an easy way to
> do that?
>
>
>
>
>
>
>

--f46d04138dbf7526ce04ffd28b9d--

From dev-return-8714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 20:01:43 2014
Return-Path: <dev-return-8714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8247B11047
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 20:01:43 +0000 (UTC)
Received: (qmail 30982 invoked by uid 500); 4 Aug 2014 20:01:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30927 invoked by uid 500); 4 Aug 2014 20:01:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30914 invoked by uid 99); 4 Aug 2014 20:01:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 20:01:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anand.avati@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 20:01:40 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so4961764obc.33
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 13:01:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=i+2YkBYB/Ax75SMH2tKJHtQP+V/o3e+im25XOIR8YLQ=;
        b=aUd+ALtzWElVsOoMr1I121VtaDOuNU8EAg3+H2NRB4pWnzAS4oDr//Nh11/mJyBj/U
         zKn2MXaLOtiajaaIXjnIGjgGVXAaJ241u1Y2xqrP4uH+mHrpBUmEEPa+w+G2UofYfYLf
         fqzw2RiiqhitZRX650LoI/xsVESYM5E/MKU+p6QOyaAKEag00Ih/XoX3x86ltcCfwh/z
         86usAP/oKu7ZHhmHxuQs9gaE29sfRvNze0e0XtmcFLG70yIsb+6DYO4HVP+kSoKGuGES
         gNHRQ4kuGphaFBSYG4993pAhDWtw2uBlHDC6FJZ7h9x5dNdrEU22pCv/wB2oDkCzrttQ
         v9oA==
MIME-Version: 1.0
X-Received: by 10.182.28.66 with SMTP id z2mr36053715obg.19.1407182475584;
 Mon, 04 Aug 2014 13:01:15 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.202.226.147 with HTTP; Mon, 4 Aug 2014 13:01:15 -0700 (PDT)
In-Reply-To: <CABPQxssGVanUChfazXUwiyx1JcmtpRyDTjuQmscdyZwM-3ZGCg@mail.gmail.com>
References: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
	<CABPQxssGVanUChfazXUwiyx1JcmtpRyDTjuQmscdyZwM-3ZGCg@mail.gmail.com>
Date: Mon, 4 Aug 2014 13:01:15 -0700
X-Google-Sender-Auth: 44DB5-ZchIu8jOsg3VS9GKTjDus
Message-ID: <CAFboF2xLS18iWWxkBm1NxG65AZoLdG9L8vipdu_uRBLWo-62aw@mail.gmail.com>
Subject: Re: Scala 2.11 external dependencies
From: Anand Avati <avati@gluster.org>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b09e976f8a04ffd3337b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b09e976f8a04ffd3337b
Content-Type: text/plain; charset=UTF-8

On Sun, Aug 3, 2014 at 9:09 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Anand,
>
> Thanks for looking into this - it's great to see momentum towards Scala
> 2.11 and I'd love if this land in Spark 1.2.
>
> For the external dependencies, it would be good to create a sub-task of
> SPARK-1812 to track our efforts encouraging other projects to upgrade. In
> certain cases (e.g. Kafka) there is fairly late-stage work on this already,
> so we can e.g. link to those JIRA's as well. A good starting point is to
> just go to their dev list and ask what the status is, most Scala projects
> have put at least some thought into this already. Another thing we can do
> is submit patches ourselves to those projects to help get them upgraded.
> The twitter libraries, e.g., tend to be pretty small and also open to
> external contributions.
>
> One other thing in the mix here - Prashant Sharma has also spent some time
> looking at this, so it might be good for you two to connect (probably off
> list) and sync up. Prashant has contributed to many Scala projects, so he
> might have cycles to go and help some of our dependencies get upgraded -
> but I won't commit to that on his behalf :).
>
> Regarding Akka - I shaded and published akka as a one-off thing:
> https://github.com/pwendell/akka/tree/2.2.3-shaded-proto
>
> Over time we've had to publish our own versions of a small number of
> dependencies. It's somewhat high overhead, but it actually works quite well
> in terms of avoiding some of the nastier dependency conflicts. At least
> better than other alternatives I've seen such as using a shader build
> plug-in.
>
> Going forward, I'd actually like to track these in the Spark repo itself.
> For instance, we have a bash script in the spark repo that can e.g. check
> out akka, apply a few patches or regular expressions, and then you have a
> fully shaded dependency that can be published to maven. If you wanted to
> take a crack at something like that for akka 2.3.4, be my guest. I can help
> with the actual publishing.
>

Will give it a try, thanks!

--089e0158b09e976f8a04ffd3337b--

From dev-return-8715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 22:04:38 2014
Return-Path: <dev-return-8715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9768D11582
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 22:04:38 +0000 (UTC)
Received: (qmail 66451 invoked by uid 500); 4 Aug 2014 22:04:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66391 invoked by uid 500); 4 Aug 2014 22:04:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66379 invoked by uid 99); 4 Aug 2014 22:04:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 22:04:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 22:04:32 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so67173wgg.16
        for <dev@spark.apache.org>; Mon, 04 Aug 2014 15:04:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ONqqw+r+EAMs2J0VjxOEm9mkEz61pIr6jmivXPpQrrE=;
        b=RI74wDT3s1zvgOGVh9oXQ0CRprgOQaNsP8YoUEinXwEs4x6NWFagHYj85ezkipWC3Y
         fXG0dLZJ+YsNCcUQMIHnMwxYdkttVRynLvHcBKk37wSCPKeflM0CmTTtx3xyYIZv993D
         Yq1Wo5hVAB250g06UEmvRGWR5CkFOBkPbuiwNHEYLq7A9IBHtGjd+VitvAW8l+9dmaJk
         JxECIaVNt5qOTUo+vlmeUSdSS5MQN/f8OOv/LV2DY1TPSWMeSgIVn0Vma6HoOgWQcKP3
         D7+fLqmQYE8RjpwFUeoKWYAdmyRB0b25FeCcVPDxnXLKn0Y6zS134ZE6zWQfjOzA0k8u
         Ji6g==
X-Gm-Message-State: ALoCoQnfRBEBoDB+Ku/TkPnXG2dqDig2xmfdoUvb7LL8WSfXf6gxoTgk3k47cTIGxZ/BAsTYl8Y5
X-Received: by 10.195.18.8 with SMTP id gi8mr34741724wjd.75.1407189850857;
 Mon, 04 Aug 2014 15:04:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Mon, 4 Aug 2014 15:03:50 -0700 (PDT)
In-Reply-To: <CADnDY-X8BpPnX0D7cV_zM9ewiuFNw6eZBuEhrxfpsxketew=fw@mail.gmail.com>
References: <CADnDY-X8BpPnX0D7cV_zM9ewiuFNw6eZBuEhrxfpsxketew=fw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 4 Aug 2014 15:03:50 -0700
Message-ID: <CAPh_B=Y45mm=Kq+USZZss5BO411d+fbyiH+csaD4CPUavsxi6A@mail.gmail.com>
Subject: Re: Interested in contributing to GraphX in Python
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Ankur Dave <ankur@databricks.com>, Joseph Gonzalez <joseph.e.gonzalez@gmail.com>
Content-Type: multipart/alternative; boundary=001a1130cc4631369004ffd4eb5f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1130cc4631369004ffd4eb5f
Content-Type: text/plain; charset=UTF-8

Thanks for your interest.

I think the main challenge is if we have to call Python functions per
record, it can be pretty expensive to serialize/deserialize across
boundaries of the Python process and JVM process.  I don't know if there is
a good way to solve this problem yet.





On Fri, Aug 1, 2014 at 11:06 AM, Rajiv Abraham <rajiv.abraham@gmail.com>
wrote:

> Hi,
> I just saw Ankur's GraphX presentation and it looks very exciting! I would
> like to contribute to a Python version of GraphX. I checked out JIRA and
> Github but I did not find much info.
>
> - Are there limitations currently to port GraphX in Python? (e.g. Maybe the
> Python Spark RDD API is incomplete or not refactored for GraphX as compared
> to the Scala version)
> - If I had to start, could  I take inspiration from the Scala version and
> try to emulate it in Python?
> - Otherwise any suggestions of  starter tasks regarding GraphX in Python
> would be appreciated
>
>
>
> --
> Take care,
> Rajiv
>

--001a1130cc4631369004ffd4eb5f--

From dev-return-8716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug  4 22:52:22 2014
Return-Path: <dev-return-8716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4A5611768
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Aug 2014 22:52:22 +0000 (UTC)
Received: (qmail 71147 invoked by uid 500); 4 Aug 2014 22:52:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71060 invoked by uid 500); 4 Aug 2014 22:52:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70159 invoked by uid 99); 4 Aug 2014 22:52:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 22:52:20 +0000
X-ASF-Spam-Status: No, hits=2.0 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yanfang724@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Aug 2014 22:52:18 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so124429qae.20
        for <multiple recipients>; Mon, 04 Aug 2014 15:51:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=9cwejXVTIA/160AlZol98voy2eN2j9K2/0XBhT1VuyY=;
        b=ylzUjfCZQsJn7IGPibbvTSFUZcMhw/wQZ5USYJi/EArUzX1ge+hhiMGppsQxtwXBcc
         +B1RUnEBSnYctyH89p4lQsSlTt7JFeyRYF/ruRK2Ulja7Wd89Ww6J+T4a/UyfjAQTY8l
         //24RlqiPtv7cs0dR2Sz/YEjUADl0C2j0fd8/+Jh78ojQkr/k4/ZpZJv3g8VtGU19/Wr
         WGo+dlVuE2sgZF61UC4QtTLrR78t3HB0xLrdiw9Yhpzf3nIoBDI5nLiNJjjS5PbbUd3x
         6TAfz1RCdY+oQJgwoOQhVZ/Uy2CbbaV3XmbWPW+P/tGiQK1FG0FQlsJwic/+g8lhFeeU
         Oi8A==
X-Received: by 10.229.178.202 with SMTP id bn10mr41193601qcb.6.1407192713452;
 Mon, 04 Aug 2014 15:51:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.19.76 with HTTP; Mon, 4 Aug 2014 15:51:33 -0700 (PDT)
In-Reply-To: <CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
 <CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
From: Yan Fang <yanfang724@gmail.com>
Date: Mon, 4 Aug 2014 15:51:33 -0700
Message-ID: <CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
To: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2be92d0e0be04ffd595ee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2be92d0e0be04ffd595ee
Content-Type: text/plain; charset=UTF-8

Another suggestion that may help is that, you can consider use Kafka to
store the latest offset instead of Zookeeper. There are at least two
benefits: 1) lower the workload of ZK 2) support replay from certain
offset. This is how Samza <http://samza.incubator.apache.org/> deals with
the Kafka offset, the doc is here
<http://samza.incubator.apache.org/learn/documentation/0.7.0/container/checkpointing.html>
.
Thank you.

Cheers,

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108


On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'll let TD chime on on this one, but I'm guessing this would be a welcome
> addition. It's great to see community effort on adding new
> streams/receivers, adding a Java API for receivers was something we did
> specifically to allow this :)
>
> - Patrick
>
>
> On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
> dibyendu.bhattachary@gmail.com> wrote:
>
>> Hi,
>>
>> I have implemented a Low Level Kafka Consumer for Spark Streaming using
>> Kafka Simple Consumer API. This API will give better control over the Kafka
>> offset management and recovery from failures. As the present Spark
>> KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
>> control over the offset management which is not possible in Kafka HighLevel
>> consumer.
>>
>> This Project is available in below Repo :
>>
>> https://github.com/dibbhatt/kafka-spark-consumer
>>
>>
>> I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver.
>> The KafkaReceiver uses low level Kafka Consumer API (implemented in
>> consumer.kafka packages) to fetch messages from Kafka and 'store' it in
>> Spark.
>>
>> The logic will detect number of partitions for a topic and spawn that
>> many threads (Individual instances of Consumers). Kafka Consumer uses
>> Zookeeper for storing the latest offset for individual partitions, which
>> will help to recover in case of failure. The Kafka Consumer logic is
>> tolerant to ZK Failures, Kafka Leader of Partition changes, Kafka broker
>> failures,  recovery from offset errors and other fail-over aspects.
>>
>> The consumer.kafka.client.Consumer is the sample Consumer which uses this
>> Kafka Receivers to generate DStreams from Kafka and apply a Output
>> operation for every messages of the RDD.
>>
>> We are planning to use this Kafka Spark Consumer to perform Near Real
>> Time Indexing of Kafka Messages to target Search Cluster and also Near Real
>> Time Aggregation using target NoSQL storage.
>>
>> Kindly let me know your view. Also if this looks good, can I contribute
>> to Spark Streaming project.
>>
>> Regards,
>> Dibyendu
>>
>
>

--001a11c2be92d0e0be04ffd595ee--

From dev-return-8717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 00:02:27 2014
Return-Path: <dev-return-8717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D2F6119EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 00:02:27 +0000 (UTC)
Received: (qmail 40123 invoked by uid 500); 5 Aug 2014 00:02:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40060 invoked by uid 500); 5 Aug 2014 00:02:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40048 invoked by uid 99); 5 Aug 2014 00:02:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 00:02:26 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 00:02:23 +0000
Received: by mail-ob0-f182.google.com with SMTP id wm4so131787obc.27
        for <dev@spark.incubator.apache.org>; Mon, 04 Aug 2014 17:01:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=CfXK1gaVD7Pq09ullHdRLo7lEP5waV4uy8ws+1tRcbw=;
        b=w9J9qzI4o5pqzv57Ay4vU/wshOhtH+BC9XCMQ1kdYcDHFYk9O0VU06Sm6AtOim0N7S
         771c+i0v9p3pNEoGjcnGVKMiLnOH9izIRh8vfMtBvxIC4TY00S9FbpeT5NNTkNCVr6KB
         pbjtIRhnngh3ytParOOLGPiYX5mXtXueYxRp7ct9DfoQnENeSFmPOCBOw5l/ZjuD+HuW
         Ie6DHaG1QK4pYHIcWGi4ZyyToxjHvrm1GT8YC5Ls6Yvh27bEtP7kSAqDUFBHUTQF+5Ov
         wWtxlhbXSJplnewpD2AErGEqwakzgkZijszAviNXxuF/nS7i5qGV7ylsnfSueWikBAJc
         pg6Q==
MIME-Version: 1.0
X-Received: by 10.182.28.66 with SMTP id z2mr119020obg.19.1407196918382; Mon,
 04 Aug 2014 17:01:58 -0700 (PDT)
Received: by 10.76.35.134 with HTTP; Mon, 4 Aug 2014 17:01:58 -0700 (PDT)
Date: Mon, 4 Aug 2014 17:01:58 -0700
Message-ID: <CAPud8Tp_T93A5qmbqKTFivJ2Qwfm9DYz9epToEAmbBg3FRTCog@mail.gmail.com>
Subject: "log" overloaded in SparkContext/ Spark 1.0.x
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b09e745b2a04ffd69055
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b09e745b2a04ffd69055
Content-Type: text/plain; charset=UTF-8

it would seem the code like

import o.a.spark.SparkContext._
import math._

....

a = log(b)

does not seem to compile anymore with Spark 1.0.x since SparkContext._ also
exposes a `log` function. Which happens a lot to a guy like me.

obvious workaround is to use something like

import o.a.spark.SparkContext.{log => sparkLog,  _}

but wouldn't it be easier just to avoid so expected clash in the first
place?

thank you.
-d

--089e0158b09e745b2a04ffd69055--

From dev-return-8718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 00:42:28 2014
Return-Path: <dev-return-8718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72C4211B69
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 00:42:28 +0000 (UTC)
Received: (qmail 38877 invoked by uid 500); 5 Aug 2014 00:42:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38818 invoked by uid 500); 5 Aug 2014 00:42:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38803 invoked by uid 99); 5 Aug 2014 00:42:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 00:42:27 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 00:42:23 +0000
Received: by mail-pa0-f45.google.com with SMTP id eu11so284487pac.32
        for <dev@spark.incubator.apache.org>; Mon, 04 Aug 2014 17:41:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=Q/Ssn94ozYtNDEge83zfsX1GvH1YImjsx+H45KhjTKo=;
        b=ZTfVZFRgKf/1XSy9uk0CaY6C0op9O8OUyHuXx/xPu6vO1dnp93Fx3cH6PNh3R329Nx
         ZJAfWEsYWoZ6HVFoLSM+SoJwUqAlcu91mpss2r0Ra2Lnc9XpoT7y+Nx8pSfO8ahgblOQ
         l623e/XNyzsbLOjHHer/Bca0pEv5o1Kyr2ynSneh6/0ZbAbcSlkzMGlQoJd2tYaJ3ntE
         6VSIfXAcmeVRDEvwvJY/QjvfXj8Ge4QnA/6MjNcYEkWXbJdEiZ11xsEj4/hxieF17ws/
         Yr+ASrQO7LMSBw3r/EhjHh+iHfWc69lt7eciK3OcKOa3rprLhY+LHCIMkZH/UZrxxr/L
         7YXA==
X-Received: by 10.68.68.162 with SMTP id x2mr340695pbt.84.1407199317699;
        Mon, 04 Aug 2014 17:41:57 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id oy3sm124193pdb.79.2014.08.04.17.41.55
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 04 Aug 2014 17:41:56 -0700 (PDT)
Date: Mon, 4 Aug 2014 17:41:54 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: "=?utf-8?Q?dev=40spark.incubator.apache.org?="
 <dev@spark.incubator.apache.org>, Dmitriy Lyubimov <dlieu.7@gmail.com>
Message-ID: <etPan.53e02852.3352255a.c956@mbp-3>
In-Reply-To: <CAPud8Tp_T93A5qmbqKTFivJ2Qwfm9DYz9epToEAmbBg3FRTCog@mail.gmail.com>
References: <CAPud8Tp_T93A5qmbqKTFivJ2Qwfm9DYz9epToEAmbBg3FRTCog@mail.gmail.com>
Subject: Re: "log" overloaded in SparkContext/ Spark 1.0.x
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53e02852_109cf92e_c956"
X-Virus-Checked: Checked by ClamAV on apache.org

--53e02852_109cf92e_c956
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hah, weird. "log" should be protected actually (look at trait Logging). Is your class extending SparkContext or somehow being placed in the org.apache.spark package? Or maybe the Scala compiler looks at it anyway.. in that case we can rename it. Please open a JIRA for it if that's the case.

On August 4, 2014 at 5:02:27 PM, Dmitriy Lyubimov (dlieu.7@gmail.com) wrote:

it would seem the code like 

import o.a.spark.SparkContext._ 
import math._ 

.... 

a = log(b) 

does not seem to compile anymore with Spark 1.0.x since SparkContext._ also 
exposes a `log` function. Which happens a lot to a guy like me. 

obvious workaround is to use something like 

import o.a.spark.SparkContext.{log => sparkLog, _} 

but wouldn't it be easier just to avoid so expected clash in the first 
place? 

thank you. 
-d 

--53e02852_109cf92e_c956--


From dev-return-8719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 06:57:40 2014
Return-Path: <dev-return-8719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2343F1079B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 06:57:40 +0000 (UTC)
Received: (qmail 57105 invoked by uid 500); 5 Aug 2014 06:57:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57044 invoked by uid 500); 5 Aug 2014 06:57:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57028 invoked by uid 99); 5 Aug 2014 06:57:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 06:57:38 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 06:57:31 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id D5E1F26006B;
	Tue,  5 Aug 2014 14:57:09 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id CAD3A260BD1;
	Tue,  5 Aug 2014 14:57:09 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id Z_2kp3gl5z5O; Tue,  5 Aug 2014 14:57:09 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.85])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id A79AD260BCE;
	Tue,  5 Aug 2014 14:57:09 +0800 (GMT-8)
Message-ID: <53E08045.80301@sjtu.edu.cn>
Date: Tue, 05 Aug 2014 14:57:09 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: shijiaxin.cn@gmail.com, dev@spark.apache.org
CC: xiaodi@sjtu.edu.cn
Subject: GraphX partitioning and threading details
Content-Type: multipart/alternative;
 boundary="------------090707060005000305070008"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------090707060005000305070008
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit

Hi all,

about GraphX partitioning details and possible optimization.

  * Can you tell how are partitions distributed to nodes? And inside
    worker, how does partitions get allocated to threads?
      o Is it possible to make manual configuration, like partition A =>
        node 1, thread 1
  * How is memory organized among threads?
      o Can we exploit the shared memory to combine mirror cache on same
        node into one?
      o (our experiment shows that more partitions requires much more
        memory)

Larry


--------------090707060005000305070008--

From dev-return-8720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 09:15:10 2014
Return-Path: <dev-return-8720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C6D010CA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 09:15:10 +0000 (UTC)
Received: (qmail 82140 invoked by uid 500); 5 Aug 2014 09:15:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82098 invoked by uid 500); 5 Aug 2014 09:15:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80955 invoked by uid 99); 5 Aug 2014 09:15:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 09:15:07 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=FREEMAIL_REPLY,HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dibyendu.bhattachary@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 09:15:03 +0000
Received: by mail-ob0-f178.google.com with SMTP id nu7so453003obb.37
        for <multiple recipients>; Tue, 05 Aug 2014 02:14:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Q9IVvpivTGMcr7z0aSjB6qDWCdnryA7d4YxDCK6RS0g=;
        b=UoUduVguCrc1/3fEqniN1YyMhaChb/fZ4RIXPvwDxa6UHnhL4+RHK56Cs6aqjOtEWj
         LnJ3MHOoFHmn+f/7sJu3Xq7FIZVc/Ti+pghSAMvIzZBNkA6tbzOzdlqbIy5RmDMBeMAs
         cuwrjJUTc7gYdqEA6EcuabyyOhlfxY0eAatpgBtHeRzV61cWxhYO/V5etZmN+3j784vd
         yjgURrjQTKWyJLVqG5FvO0wqwwwkVWEv1ru4CDj7wGJkEymvraAnQt3e5m5tfHBpDLQ8
         2CRl/qXGie7q5aNC682WYAz1hKsj7nJArkYT//fwPmoFlmy9qXydZR3+iFYaRVWutpa3
         5KFg==
MIME-Version: 1.0
X-Received: by 10.182.126.233 with SMTP id nb9mr3449273obb.46.1407230076635;
 Tue, 05 Aug 2014 02:14:36 -0700 (PDT)
Received: by 10.76.76.169 with HTTP; Tue, 5 Aug 2014 02:14:36 -0700 (PDT)
In-Reply-To: <CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
	<CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
	<CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
	<CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
Date: Tue, 5 Aug 2014 14:44:36 +0530
Message-ID: <CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
From: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
To: Jonathan Hodges <hodgesz@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Cc: user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1e550d5cf4a04ffde48c8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1e550d5cf4a04ffde48c8
Content-Type: text/plain; charset=UTF-8

Thanks Jonathan,

Yes, till non-ZK based offset management is available in Kafka, I need to
maintain the offset in ZK. And yes, both cases explicit commit is
necessary. I modified the Low Level Kafka Spark Consumer little bit to have
Receiver spawns threads for every partition of the topic and perform the
'store' operation in multiple threads. It would be good if the
receiver.store methods are made thread safe..which is not now presently .

Waiting for TD's comment on this Kafka Spark Low Level consumer.


Regards,
Dibyendu



On Tue, Aug 5, 2014 at 5:32 AM, Jonathan Hodges <hodgesz@gmail.com> wrote:

> Hi Yan,
>
> That is a good suggestion.  I believe non-Zookeeper offset management will
> be a feature in the upcoming Kafka 0.8.2 release tentatively scheduled for
> September.
>
>
> https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management
>
> That should make this fairly easy to implement, but it will still require
> explicit offset commits to avoid data loss which is different than the
> current KafkaUtils implementation.
>
> Jonathan
>
>
>
>
>
> On Mon, Aug 4, 2014 at 4:51 PM, Yan Fang <yanfang724@gmail.com> wrote:
>
>> Another suggestion that may help is that, you can consider use Kafka to
>> store the latest offset instead of Zookeeper. There are at least two
>> benefits: 1) lower the workload of ZK 2) support replay from certain
>> offset. This is how Samza <http://samza.incubator.apache.org/> deals
>> with the Kafka offset, the doc is here
>> <http://samza.incubator.apache.org/learn/documentation/0.7.0/container/checkpointing.html> .
>> Thank you.
>>
>> Cheers,
>>
>> Fang, Yan
>> yanfang724@gmail.com
>> +1 (206) 849-4108
>>
>>
>> On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> I'll let TD chime on on this one, but I'm guessing this would be a
>>> welcome addition. It's great to see community effort on adding new
>>> streams/receivers, adding a Java API for receivers was something we did
>>> specifically to allow this :)
>>>
>>> - Patrick
>>>
>>>
>>> On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
>>> dibyendu.bhattachary@gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> I have implemented a Low Level Kafka Consumer for Spark Streaming using
>>>> Kafka Simple Consumer API. This API will give better control over the Kafka
>>>> offset management and recovery from failures. As the present Spark
>>>> KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
>>>> control over the offset management which is not possible in Kafka HighLevel
>>>> consumer.
>>>>
>>>> This Project is available in below Repo :
>>>>
>>>> https://github.com/dibbhatt/kafka-spark-consumer
>>>>
>>>>
>>>> I have implemented a Custom Receiver
>>>> consumer.kafka.client.KafkaReceiver. The KafkaReceiver uses low level Kafka
>>>> Consumer API (implemented in consumer.kafka packages) to fetch messages
>>>> from Kafka and 'store' it in Spark.
>>>>
>>>> The logic will detect number of partitions for a topic and spawn that
>>>> many threads (Individual instances of Consumers). Kafka Consumer uses
>>>> Zookeeper for storing the latest offset for individual partitions, which
>>>> will help to recover in case of failure. The Kafka Consumer logic is
>>>> tolerant to ZK Failures, Kafka Leader of Partition changes, Kafka broker
>>>> failures,  recovery from offset errors and other fail-over aspects.
>>>>
>>>> The consumer.kafka.client.Consumer is the sample Consumer which uses
>>>> this Kafka Receivers to generate DStreams from Kafka and apply a Output
>>>> operation for every messages of the RDD.
>>>>
>>>> We are planning to use this Kafka Spark Consumer to perform Near Real
>>>> Time Indexing of Kafka Messages to target Search Cluster and also Near Real
>>>> Time Aggregation using target NoSQL storage.
>>>>
>>>> Kindly let me know your view. Also if this looks good, can I contribute
>>>> to Spark Streaming project.
>>>>
>>>> Regards,
>>>> Dibyendu
>>>>
>>>
>>>
>>
>

--001a11c1e550d5cf4a04ffde48c8--

From dev-return-8721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 13:10:54 2014
Return-Path: <dev-return-8721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87C921145F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 13:10:54 +0000 (UTC)
Received: (qmail 7796 invoked by uid 500); 5 Aug 2014 13:10:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7743 invoked by uid 500); 5 Aug 2014 13:10:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7730 invoked by uid 99); 5 Aug 2014 13:10:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 13:10:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 13:10:49 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so925593qge.32
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 06:10:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vCAPqLPMyZhrW2iKKmR4tJwXk1CTE/RGvuAht+2Lnqw=;
        b=doWLUeNqLvZ7NMOLy/d6z1JZ7Rp6vOsEdHA8bera5bN/L0/NwjY97Q0LCNP5aVYvhW
         0mZYRvC4B9LJQgXMDlblYBv/H2dMJTgaJpr8gW2mPpRYET/bR30g8nXud1cTxK6WvZZT
         g54ZBN8Q3VSFAGjMdDFh7Ph4SxNnkmsOgfk57F/IPsWn7ooIZ7MyPG00Ft1lxwVholtx
         R/SiVoetrFW3y0bpFQan8KIvHWm1KNjnSSpcuw3IyBn7lGNdEpfNxRqi8F4YrsjXkoAA
         Rl2HH3zSlHDzjdEsVPEro1w+fUt/hZvpeowC3T2yArOzqSqK/123IT42jIwfm5VvX8sr
         hXZA==
MIME-Version: 1.0
X-Received: by 10.140.49.77 with SMTP id p71mr5213153qga.86.1407244228587;
 Tue, 05 Aug 2014 06:10:28 -0700 (PDT)
Received: by 10.140.24.50 with HTTP; Tue, 5 Aug 2014 06:10:28 -0700 (PDT)
In-Reply-To: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
Date: Tue, 5 Aug 2014 18:40:28 +0530
Message-ID: <CAJiQeYKYV0NBd9+hPDWJCEnoKyrK37JV6W+5nD65KLc_wvRYcA@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Just came across this mail, thanks for initiating this discussion Kay.
To add; another issue which recurs is very rapid commit's: before most
contributors have had a chance to even look at the changes proposed.
There is not much prior discussion on the jira or pr, and the time
between submitting the PR and committing it is < 12 hours.

Particularly relevant when contributors are not on US timezones and/or
colocated; I have raised this a few times before when the commit had
other side effects not considered.
On flip side we have PR's which have been languishing for weeks with
little or no activity from committers side - making the contribution
stale; so too long a delay is also definitely not the direction to
take either !



Regards,
Mridul



On Tue, Jul 22, 2014 at 2:14 AM, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:
> Hi all,
>
> As the number of committers / contributors on Spark has increased, there
> are cases where pull requests get merged before all the review comments
> have been addressed. This happens say when one committer points out a
> problem with the pull request, and another committer doesn't see the
> earlier comment and merges the PR before the comment has been addressed.
>  This is especially tricky for pull requests with a large number of
> comments, because it can be difficult to notice early comments describing
> blocking issues.
>
> This also happens when something accidentally gets merged after the tests
> have started but before tests have passed.
>
> Do folks have ideas on how we can handle this issue? Are there other
> projects that have good ways of handling this? It looks like for Hadoop,
> people can -1 / +1 on the JIRA.
>
> -Kay

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 14:32:31 2014
Return-Path: <dev-return-8722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFE14116FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 14:32:31 +0000 (UTC)
Received: (qmail 38319 invoked by uid 500); 5 Aug 2014 14:32:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38267 invoked by uid 500); 5 Aug 2014 14:32:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38250 invoked by uid 99); 5 Aug 2014 14:32:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 14:32:30 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saisai.shao@intel.com designates 192.55.52.115 as permitted sender)
Received: from [192.55.52.115] (HELO mga14.intel.com) (192.55.52.115)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 14:32:27 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by fmsmga103.fm.intel.com with ESMTP; 05 Aug 2014 07:24:53 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.01,805,1400050800"; 
   d="scan'208,217";a="572004106"
Received: from fmsmsx104.amr.corp.intel.com ([10.19.9.35])
  by fmsmga001.fm.intel.com with ESMTP; 05 Aug 2014 07:32:00 -0700
Received: from shsmsx101.ccr.corp.intel.com (10.239.4.153) by
 FMSMSX104.amr.corp.intel.com (10.19.9.35) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Tue, 5 Aug 2014 07:32:00 -0700
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.97]) by
 SHSMSX101.ccr.corp.intel.com ([169.254.1.252]) with mapi id 14.03.0195.001;
 Tue, 5 Aug 2014 22:31:58 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Low Level Kafka Consumer for Spark
Thread-Topic: Low Level Kafka Consumer for Spark
Thread-Index: AQHPrnSkVhPmSJHvZUast8tM7EvgFJu/TdaAgAE8aoCAABPvgIAAmiUAgADW9KA=
Date: Tue, 5 Aug 2014 14:31:57 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F7026F9413@SHSMSX104.ccr.corp.intel.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
	<CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
	<CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
	<CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
 <CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
In-Reply-To: <CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_64474308D680D540A4D8151B0F7C03F7026F9413SHSMSX104ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_64474308D680D540A4D8151B0F7C03F7026F9413SHSMSX104ccrcor_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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=

--_000_64474308D680D540A4D8151B0F7C03F7026F9413SHSMSX104ccrcor_--

From dev-return-8723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 15:22:52 2014
Return-Path: <dev-return-8723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EFD7118B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 15:22:52 +0000 (UTC)
Received: (qmail 43948 invoked by uid 500); 5 Aug 2014 15:22:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43892 invoked by uid 500); 5 Aug 2014 15:22:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43872 invoked by uid 99); 5 Aug 2014 15:22:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:22:51 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:22:44 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s75FMLlw016291
	for <dev@spark.apache.org>; Tue, 5 Aug 2014 11:22:21 -0400
Date: Tue, 5 Aug 2014 11:22:21 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <188925664.2035338.1407252141803.JavaMail.zimbra@redhat.com>
In-Reply-To: <1548155869.2033949.1407251958042.JavaMail.zimbra@redhat.com>
Subject: any interest in something like rdd.parent[T](n)  (equivalent to
 firstParent[T] for n==0) ?
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.6]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC36 (Linux)/8.0.6_GA_5922)
Thread-Topic: any interest in something like rdd.parent[T](n) (equivalent to firstParent[T] for n==0) ?
Thread-Index: CR1RaAb582EqKx+ErniqJtQ0nTo3tQ==
X-Virus-Checked: Checked by ClamAV on apache.org

Not that  rdd.dependencies(n).rdd.asInstanceOf[RDD[T]]  is terrible, but rdd.parent[T](n) better captures the intent.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 15:39:17 2014
Return-Path: <dev-return-8724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED8C511926
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 15:39:17 +0000 (UTC)
Received: (qmail 88648 invoked by uid 500); 5 Aug 2014 15:39:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88601 invoked by uid 500); 5 Aug 2014 15:39:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88588 invoked by uid 99); 5 Aug 2014 15:39:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:39:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dibyendu.bhattachary@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:39:11 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so745346oig.11
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 08:38:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Zx6ttdJAJrGRrfcbz0W6hzk9QZVHtkiIhbue7kDgAGE=;
        b=djxQteujmnlDV1xR73NMHzlH5h9m/Nj2cxueVmwQTm3elxo7Kfqgr4tFLEOdpMO6gJ
         I2MeWXbxHk2EAXREkRQHDTwSGN8/5IOekBwjtxsPhKWGLvREIqCdNfnlIzyvnMVAoMkV
         l4IiRGANJOeZhk8leCSuDGnhdbQG6tDK//s99fd8VRFFnfDxkwF3NjEHGGYzjiZpS2F6
         OGBSyYsgVzzYS2cJdKWOIAF2AEyRZ1i1GWpWuPRBro0iCS3B7GMwfLpo2MEgULykU0WO
         CBo7LpUxkAfBhMyIrjvG5IIJUCHqKOF0N6+AShUevjZa3LkU39Wl9KYF+XMagSUCl6/f
         weUw==
MIME-Version: 1.0
X-Received: by 10.60.132.14 with SMTP id oq14mr6841846oeb.8.1407253131324;
 Tue, 05 Aug 2014 08:38:51 -0700 (PDT)
Received: by 10.76.76.169 with HTTP; Tue, 5 Aug 2014 08:38:51 -0700 (PDT)
In-Reply-To: <64474308D680D540A4D8151B0F7C03F7026F9413@SHSMSX104.ccr.corp.intel.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
	<CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
	<CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
	<CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
	<CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
	<64474308D680D540A4D8151B0F7C03F7026F9413@SHSMSX104.ccr.corp.intel.com>
Date: Tue, 5 Aug 2014 21:08:51 +0530
Message-ID: <CAFiYKR89TZ21tsjmp+cJ7UQEFg6=zDgYGJwMTQ4_bB7nj8DusA@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
From: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
To: "Shao, Saisai" <saisai.shao@intel.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b41cd2e0077be04ffe3a765
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b41cd2e0077be04ffe3a765
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi

This fault tolerant aspect already taken care in the Kafka-Spark Consumer
code , like if Leader of a partition changes etc.. in ZkCoordinator.java.
Basically it does a refresh of PartitionManagers every X seconds to make
sure Partition details is correct and consumer don't fail.

Dib


On Tue, Aug 5, 2014 at 8:01 PM, Shao, Saisai <saisai.shao@intel.com> wrote:

> Hi,
>
> I think this is an awesome feature for Spark Streaming Kafka interface to
> offer user the controllability of partition offset, so user can have more
> applications based on this.
>
> What I concern is that if we want to do offset management, fault tolerant
> related control and others, we have to take the role as current
> ZookeeperConsumerConnect did, that would be a big field we should take ca=
re
> of, for example when node is failed, how to pass current partition to
> another consumer and some others. I=E2=80=99m not sure what is your thoug=
ht?
>
> Thanks
> Jerry
>
> From: Dibyendu Bhattacharya [mailto:dibyendu.bhattachary@gmail.com]
> Sent: Tuesday, August 05, 2014 5:15 PM
> To: Jonathan Hodges; dev@spark.apache.org
> Cc: user
> Subject: Re: Low Level Kafka Consumer for Spark
>
> Thanks Jonathan,
>
> Yes, till non-ZK based offset management is available in Kafka, I need to
> maintain the offset in ZK. And yes, both cases explicit commit is
> necessary. I modified the Low Level Kafka Spark Consumer little bit to ha=
ve
> Receiver spawns threads for every partition of the topic and perform the
> 'store' operation in multiple threads. It would be good if the
> receiver.store methods are made thread safe..which is not now presently .
>
> Waiting for TD's comment on this Kafka Spark Low Level consumer.
>
>
> Regards,
> Dibyendu
>
>
> On Tue, Aug 5, 2014 at 5:32 AM, Jonathan Hodges <hodgesz@gmail.com<mailto=
:
> hodgesz@gmail.com>> wrote:
> Hi Yan,
>
> That is a good suggestion.  I believe non-Zookeeper offset management wil=
l
> be a feature in the upcoming Kafka 0.8.2 release tentatively scheduled fo=
r
> September.
>
>
> https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset=
+Management
>
> That should make this fairly easy to implement, but it will still require
> explicit offset commits to avoid data loss which is different than the
> current KafkaUtils implementation.
>
> Jonathan
>
>
>
>
> On Mon, Aug 4, 2014 at 4:51 PM, Yan Fang <yanfang724@gmail.com<mailto:
> yanfang724@gmail.com>> wrote:
> Another suggestion that may help is that, you can consider use Kafka to
> store the latest offset instead of Zookeeper. There are at least two
> benefits: 1) lower the workload of ZK 2) support replay from certain
> offset. This is how Samza<http://samza.incubator.apache.org/> deals with
> the Kafka offset, the doc is here<
> http://samza.incubator.apache.org/learn/documentation/0.7.0/container/che=
ckpointing.html>
> . Thank you.
>
> Cheers,
>
> Fang, Yan
> yanfang724@gmail.com<mailto:yanfang724@gmail.com>
> +1 (206) 849-4108<tel:%2B1%20%28206%29%20849-4108>
>
> On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com
> <mailto:pwendell@gmail.com>> wrote:
> I'll let TD chime on on this one, but I'm guessing this would be a welcom=
e
> addition. It's great to see community effort on adding new
> streams/receivers, adding a Java API for receivers was something we did
> specifically to allow this :)
>
> - Patrick
>
> On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
> dibyendu.bhattachary@gmail.com<mailto:dibyendu.bhattachary@gmail.com>>
> wrote:
> Hi,
>
> I have implemented a Low Level Kafka Consumer for Spark Streaming using
> Kafka Simple Consumer API. This API will give better control over the Kaf=
ka
> offset management and recovery from failures. As the present Spark
> KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
> control over the offset management which is not possible in Kafka HighLev=
el
> consumer.
>
> This Project is available in below Repo :
>
> https://github.com/dibbhatt/kafka-spark-consumer
>
>
> I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver.
> The KafkaReceiver uses low level Kafka Consumer API (implemented in
> consumer.kafka packages) to fetch messages from Kafka and 'store' it in
> Spark.
>
> The logic will detect number of partitions for a topic and spawn that man=
y
> threads (Individual instances of Consumers). Kafka Consumer uses Zookeepe=
r
> for storing the latest offset for individual partitions, which will help =
to
> recover in case of failure. The Kafka Consumer logic is tolerant to ZK
> Failures, Kafka Leader of Partition changes, Kafka broker failures,
>  recovery from offset errors and other fail-over aspects.
>
> The consumer.kafka.client.Consumer is the sample Consumer which uses this
> Kafka Receivers to generate DStreams from Kafka and apply a Output
> operation for every messages of the RDD.
>
> We are planning to use this Kafka Spark Consumer to perform Near Real Tim=
e
> Indexing of Kafka Messages to target Search Cluster and also Near Real Ti=
me
> Aggregation using target NoSQL storage.
>
> Kindly let me know your view. Also if this looks good, can I contribute t=
o
> Spark Streaming project.
>
> Regards,
> Dibyendu
>
>
>
>
>

--047d7b41cd2e0077be04ffe3a765--

From dev-return-8725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 15:49:38 2014
Return-Path: <dev-return-8725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 719C111997
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 15:49:38 +0000 (UTC)
Received: (qmail 21243 invoked by uid 500); 5 Aug 2014 15:49:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21187 invoked by uid 500); 5 Aug 2014 15:49:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21175 invoked by uid 99); 5 Aug 2014 15:49:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:49:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 15:49:32 +0000
Received: by mail-we0-f172.google.com with SMTP id x48so1216583wes.31
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 08:49:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=KvSCWYwAMe9VfEHUapB/5jWzoOrXWI6nMV59Sin9YVI=;
        b=u3keS045JJHVftMI3EQI7usEdpeTOqaRx686KrrZHXbIvjUSQzqEPf4GHBYBv9lzS2
         BomimUCjWeB61uv/SH8OVNhxXRXiKfBQDtkANblIwzTiAMInvIDSfN+wPr2YL3oFTjrp
         Twh13S77wxnau5IxozNfpTkmVAEj+jeV6J+kW0BPAZOZuxutDa1dxr2c1CW2IAnhxokl
         MobYBzQQW3sXL4Mxs1JcaZy0wy0fUkSYXrgjoPvFdOXgbU/p3CU3oajArDHKtglGHbfM
         8CwyqM8EpB01A2we1V/Jw2+8oS6LT8wglpFO5rJHWJWGiTkCzUCJytIlZ52bfifTAf64
         AKRA==
X-Received: by 10.180.77.193 with SMTP id u1mr1261317wiw.45.1407253746752;
 Tue, 05 Aug 2014 08:49:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 5 Aug 2014 08:48:26 -0700 (PDT)
In-Reply-To: <CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
 <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com> <CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 5 Aug 2014 11:48:26 -0400
Message-ID: <CAOhmDzeP97E81-68_=azvHP2qrVXxc0deNO0Qcxcwuwa+N4ZxA@mail.gmail.com>
Subject: Re: -1s on pull requests?
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043d6759af259a04ffe3cbf5
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043d6759af259a04ffe3cbf5
Content-Type: text/plain; charset=UTF-8

>
> 1. Include the commit hash in the "tests have started/completed"
>

FYI: Looks like Xiangrui's already got a JIRA issue for this.

SPARK-2622: Add Jenkins build numbers to SparkQA messages
<https://issues.apache.org/jira/browse/SPARK-2622>

2. "Pin" a message to the start or end of the PR


Should new JIRA issues for this item fall under the following umbrella
issue?

SPARK-2230: Improvements to Jenkins QA Harness
<https://issues.apache.org/jira/browse/SPARK-2230>

Nick

--f46d043d6759af259a04ffe3cbf5--

From dev-return-8726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 16:05:35 2014
Return-Path: <dev-return-8726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E711E11A42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 16:05:34 +0000 (UTC)
Received: (qmail 83463 invoked by uid 500); 5 Aug 2014 16:05:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83410 invoked by uid 500); 5 Aug 2014 16:05:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83394 invoked by uid 99); 5 Aug 2014 16:05:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 16:05:33 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 16:05:27 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 89DE319B
	for <dev@spark.apache.org>; Tue,  5 Aug 2014 16:05:01 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 5 Aug 2014 16:04:21 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Tue, 5 Aug 2014 16:04:20 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Spark maven project with the latest Spark jars
Thread-Topic: Spark maven project with the latest Spark jars
Thread-Index: Ac+wxcCNx/+2uWHfSY2WRafKTeIlbQ==
Date: Tue, 5 Aug 2014 16:04:20 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCD42EA@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCD42EAG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCD42EAG4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I'm trying to create a maven project that references the latest build of Sp=
ark.
1)downloaded sources and compiled the latest version of Spark.
2)added new spark-core jar to the a new local maven repo
3)created Scala maven project with net.alchim31.maven (scala-archetype-simp=
le v 1.5)
4)added dependency to the new spark-core inside the pom.xml
5)I create SparkContext in the code of this project: val sc =3D new SparkCo=
ntext("local", "test")
6)When I run it, I get the error:
Error:scalac: bad symbolic reference. A signature in RDD.class refers to te=
rm io
in package org.apache.hadoop which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling RD=
D.class.

This problem doesn't occur if I reference the spark-core from the maven rep=
o. What am I doing wrong?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCD42EAG4W3292americas_--

From dev-return-8727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 16:10:53 2014
Return-Path: <dev-return-8727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6824B11A6C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 16:10:53 +0000 (UTC)
Received: (qmail 5965 invoked by uid 500); 5 Aug 2014 16:10:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5908 invoked by uid 500); 5 Aug 2014 16:10:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5896 invoked by uid 99); 5 Aug 2014 16:10:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 16:10:52 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 16:10:47 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so1256869qgf.34
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 09:10:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lyDjtbgpX2mYT8p82CKZBOHDZhRtIEWGf5ENjoQBzsw=;
        b=VZBOtWIEy2YOmsAI/MBtr4ywLj+ATpJNeR3rZI4wGPZAGY2ymC4y9wC1wFpbanEyF9
         M3uGSEcwdz9FM0h1dER/S++1Of1ejk02Lzp7fM8xDxOOiJQoMpFUlFvjGaDuf6MkM5eB
         VouB0sl0cVo/IxO6kmkzmGb/nzwJNG05grQoKeaz66wAjYxgtXrY2nZMC86Nlp1rQjd8
         NBJUBe/3i2JdnR1e5p2uwGRsXxhM5VY/4Hh6ORSDikN2y0a6lxF6qysWwkTRVUUqiZGE
         itgd2SkCzVzEEf7oCJGS+j7eE7l1JFCAWaKIniWbc7Q3s4ykFg0RKwy6d4hcQmt6CdXa
         whsA==
MIME-Version: 1.0
X-Received: by 10.224.104.1 with SMTP id m1mr7208429qao.81.1407255022136; Tue,
 05 Aug 2014 09:10:22 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Tue, 5 Aug 2014 09:10:22 -0700 (PDT)
In-Reply-To: <CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
Date: Tue, 5 Aug 2014 09:10:22 -0700
Message-ID: <CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c248e0b3f92604ffe417e8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c248e0b3f92604ffe417e8
Content-Type: text/plain; charset=UTF-8

I created the assembly file but still it wants to pick the mllib from the
cluster:

jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
QuadraticMinimizer

org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class

/Users/v606014/dist-1.0.1/bin/spark-submit --master
spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath outputPath

Exception in thread "main" java.lang.NoSuchMethodError:
org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;

Now if I force it to use the jar that I gave
using spark.files.userClassPathFirst, then it fails on some serialization
issues...
A simple solution is to cherry pick the files I need from spark branch to
the application branch but I am not sure that's the right thing to do...

The way userClassPathFirst is behaving, there might be bugs in it...

Any suggestions will be appreciated....

Thanks.
Deb


On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Yes, that should work. spark-mllib-1.1.0 should be compatible with
> spark-core-1.0.1.
>
> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Let me try it...
> >
> > Will this be fixed if I generate a assembly file with mllib-1.1.0
> SNAPSHOT
> > jar and other dependencies with the rest of the application code ?
> >
> >
> >
> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >>
> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
> >> sure whether it could solve your problem. -Xiangrui
> >>
> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > Hi,
> >> >
> >> > I have deployed spark stable 1.0.1 on the cluster but I have new code
> >> > that
> >> > I added in mllib-1.1.0-SNAPSHOT.
> >> >
> >> > I am trying to access the new code using spark-submit as follows:
> >> >
> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
> >> > --executor-memory 16g --total-executor-cores 16 --jars
> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
> >> > --qpProblem 2 inputPath outputPath
> >> >
> >> > I can see the jars are getting added to httpServer as expected:
> >> >
> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
> >> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> with
> >> > timestamp 1406998204236
> >> >
> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
> >> > 1406998204237
> >> >
> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
> >> > timestamp
> >> > 1406998204238
> >> >
> >> > But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I
> >> > think
> >> > it's picking up the mllib from cluster which is at 1.0.1...
> >> >
> >> > Please help. I will ask for a PR tomorrow but internally we want to
> >> > generate results from the new code.
> >> >
> >> > Thanks.
> >> >
> >> > Deb
> >
> >
>

--001a11c248e0b3f92604ffe417e8--

From dev-return-8728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 17:33:00 2014
Return-Path: <dev-return-8728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1C4F11D8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 17:32:59 +0000 (UTC)
Received: (qmail 11850 invoked by uid 500); 5 Aug 2014 17:32:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11796 invoked by uid 500); 5 Aug 2014 17:32:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11780 invoked by uid 99); 5 Aug 2014 17:32:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 17:32:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 17:32:55 +0000
Received: by mail-ie0-f171.google.com with SMTP id at1so1452140iec.30
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 10:32:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tq3laVMkDdUv5NA3q9LzLCEOgX6oWzNtE38MhXaA2Hg=;
        b=TePwkbGFZeZXPSw4jIdK3Lz8El+yRK1ui0rOBy8x7aldb4dV9Dex0Dx3lGrl7w9GxE
         jgfkA6pmcD/3VLrSlQ7J+JuAgLqJ3tulJ2BhmeORX7NMw7G/EAZu/Cm9Ae/m2q0TEiJb
         n0eRYf27jtF8oN0XFXgTjmtFJZ+cm3Q+946TuFciCkzUHS096f94Zf1RfMwR93HTrYu9
         4tTrlh+WgsBGT9JCxZx+Bl+4deWcHzGCix2z7NZiZjsNgehmApZamgeqX4M8HnhVfIUb
         sIBnehwbSHruze4ov4DebLIt/jYjVFHlM6UFBK6E89vIYG9x6liB7BbH4diBqB8LCa7H
         TKzw==
MIME-Version: 1.0
X-Received: by 10.42.40.144 with SMTP id l16mr7337991ice.27.1407259954400;
 Tue, 05 Aug 2014 10:32:34 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Tue, 5 Aug 2014 10:32:34 -0700 (PDT)
In-Reply-To: <CAOhmDzeP97E81-68_=azvHP2qrVXxc0deNO0Qcxcwuwa+N4ZxA@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
	<CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
	<CABPQxssy0ri2QAz=cc9Tx+EXYWARm7pNcVm8apqCwc-esLbO4Q@mail.gmail.com>
	<CAOhmDzeP97E81-68_=azvHP2qrVXxc0deNO0Qcxcwuwa+N4ZxA@mail.gmail.com>
Date: Tue, 5 Aug 2014 10:32:34 -0700
Message-ID: <CAJgQjQ9vVWdfHMZO+rXV0Yv3_HHDO9pShT5AZ3DtUMfgryOEGg@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Xiangrui Meng <mengxr@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I think the build number is included in the SparkQA message, for
example: https://github.com/apache/spark/pull/1788

The build number 17941 is in the URL
"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17941/consoleFull".
Just need to be careful to match the number.

Another solution is to kill running Jenkins jobs if there is a code change.

On Tue, Aug 5, 2014 at 8:48 AM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
>>
>> 1. Include the commit hash in the "tests have started/completed"
>>
>
> FYI: Looks like Xiangrui's already got a JIRA issue for this.
>
> SPARK-2622: Add Jenkins build numbers to SparkQA messages
> <https://issues.apache.org/jira/browse/SPARK-2622>
>
> 2. "Pin" a message to the start or end of the PR
>
>
> Should new JIRA issues for this item fall under the following umbrella
> issue?
>
> SPARK-2230: Improvements to Jenkins QA Harness
> <https://issues.apache.org/jira/browse/SPARK-2230>
>
> Nick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 17:37:31 2014
Return-Path: <dev-return-8729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B7B111DD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 17:37:31 +0000 (UTC)
Received: (qmail 23996 invoked by uid 500); 5 Aug 2014 17:37:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23938 invoked by uid 500); 5 Aug 2014 17:37:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23926 invoked by uid 99); 5 Aug 2014 17:37:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 17:37:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 17:37:22 +0000
Received: by mail-ie0-f178.google.com with SMTP id rd18so1458916iec.9
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 10:37:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mBD4sgKOkyfc2/q7UKIUffJ4s2uowqml4Unl+gAbgYg=;
        b=BjN4KnKzMt1grxJHNI+h+46NeVsEjad6hjKvInVxiwOpTk3aPSVw9iJAN1MNcln6zY
         qigGcGep/BgNpcQLx4UwwrmfmGANykK99RDUmJ1cOQL/7z4iOAY+cOhQWJTodDpdEpBH
         9rJ0CMgLCFachjGrgHoVtbo3b+J6DkgL9h6BnzIAIy65XngiqvURLdDW5vJgTgU6Kczj
         kMC6Ypa+Q+LL1gOpDUFULjnlSaO8iiJ7NM7MiGUwOOKErwjfvfemak252md3ynS53sPW
         upgR0lxGybT3VTtC/8oFdkOyO0AuXCrLWk4VIL7Swe5smG8y7/tBVkUxBnkHSmM3s/Tl
         Hhtg==
MIME-Version: 1.0
X-Received: by 10.50.143.101 with SMTP id sd5mr50763453igb.18.1407260221834;
 Tue, 05 Aug 2014 10:37:01 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Tue, 5 Aug 2014 10:37:01 -0700 (PDT)
In-Reply-To: <CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
Date: Tue, 5 Aug 2014 10:37:01 -0700
Message-ID: <CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

If you cannot change the Spark jar deployed on the cluster, an easy
solution would be renaming ALS in your jar. If userClassPathFirst
doesn't work, could you create a JIRA and attach the log? Thanks!
-Xiangrui

On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> I created the assembly file but still it wants to pick the mllib from the
> cluster:
>
> jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
> QuadraticMinimizer
>
> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>
> /Users/v606014/dist-1.0.1/bin/spark-submit --master
> spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
> ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath outputPath
>
> Exception in thread "main" java.lang.NoSuchMethodError:
> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>
> Now if I force it to use the jar that I gave using
> spark.files.userClassPathFirst, then it fails on some serialization
> issues...
>
> A simple solution is to cherry pick the files I need from spark branch to
> the application branch but I am not sure that's the right thing to do...
>
> The way userClassPathFirst is behaving, there might be bugs in it...
>
> Any suggestions will be appreciated....
>
> Thanks.
> Deb
>
>
> On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>> spark-core-1.0.1.
>>
>> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Let me try it...
>> >
>> > Will this be fixed if I generate a assembly file with mllib-1.1.0
>> > SNAPSHOT
>> > jar and other dependencies with the rest of the application code ?
>> >
>> >
>> >
>> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> >>
>> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
>> >> sure whether it could solve your problem. -Xiangrui
>> >>
>> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>> >> <debasish.das83@gmail.com>
>> >> wrote:
>> >> > Hi,
>> >> >
>> >> > I have deployed spark stable 1.0.1 on the cluster but I have new code
>> >> > that
>> >> > I added in mllib-1.1.0-SNAPSHOT.
>> >> >
>> >> > I am trying to access the new code using spark-submit as follows:
>> >> >
>> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
>> >> > --executor-memory 16g --total-executor-cores 16 --jars
>> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
>> >> > --qpProblem 2 inputPath outputPath
>> >> >
>> >> > I can see the jars are getting added to httpServer as expected:
>> >> >
>> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>> >> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>> >> > with
>> >> > timestamp 1406998204236
>> >> >
>> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
>> >> > 1406998204237
>> >> >
>> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
>> >> > timestamp
>> >> > 1406998204238
>> >> >
>> >> > But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I
>> >> > think
>> >> > it's picking up the mllib from cluster which is at 1.0.1...
>> >> >
>> >> > Please help. I will ask for a PR tomorrow but internally we want to
>> >> > generate results from the new code.
>> >> >
>> >> > Thanks.
>> >> >
>> >> > Deb
>> >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 19:14:43 2014
Return-Path: <dev-return-8730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 52D1711194
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 19:14:43 +0000 (UTC)
Received: (qmail 8329 invoked by uid 500); 5 Aug 2014 19:14:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8269 invoked by uid 500); 5 Aug 2014 19:14:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8250 invoked by uid 99); 5 Aug 2014 19:14:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 19:14:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 19:14:39 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so2373389vcb.19
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 12:14:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=AnfH7Mq2luf0VDFU/1NFACVTtqDEIzZPqBDipLas5CA=;
        b=pC6FcIuwazLbha00DuCWnPUIvGr+r0IhnnDDCAach7woI5I4IdghSCGMaJjWsFEw3e
         G1XMxEZZ9JWKKFsB3MVNRPpXxnBHUJ+AgvTgAoajpVBmxz0bcPR6PaROcUdh/EQ55XUD
         0k0m8BDBw0ZapcjadTTEps67KS1NApEhwKlZzm5Dli2OdYFx9NcuMbm85HbrNfi8v5F3
         UKWMADTJ2tDCSp5rHs+Rk10UPfTfAGSYpz7xmq0qvvArlj/PZD7SIZfEQYCRhpPP41XW
         0PwrsDIfEBCC7e3Gvg8SP5Pgp9i78V7u8S99mEkaQcl+WETDEQvj8bjZ4Qx++fvxgSdC
         U5Mg==
MIME-Version: 1.0
X-Received: by 10.53.12.225 with SMTP id et1mr5196964vdd.5.1407266054494; Tue,
 05 Aug 2014 12:14:14 -0700 (PDT)
Received: by 10.52.114.194 with HTTP; Tue, 5 Aug 2014 12:14:14 -0700 (PDT)
Date: Tue, 5 Aug 2014 12:14:14 -0700
Message-ID: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
Subject: Tiny curiosity question on closing the jdbc connection
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133fe38485f9a04ffe6a9f6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133fe38485f9a04ffe6a9f6
Content-Type: text/plain; charset=UTF-8

Within its compute.close method, the JdbcRDD class has this interesting
logic for closing jdbc connection:


      try {
        if (null != conn && ! stmt.isClosed()) conn.close()
        logInfo("closed connection")
      } catch {
        case e: Exception => logWarning("Exception closing connection", e)
      }

Notice that the second check is on stmt  having been closed - not on the
connection.

I would wager this were not a simple oversight and there were some
motivation for this logic- curious if anyone would be able to shed some
light?   My particular interest is that I have written custom ORM's in jdbc
since late 90's  and never did it this way.

--001a1133fe38485f9a04ffe6a9f6--

From dev-return-8731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 20:04:32 2014
Return-Path: <dev-return-8731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EE59112CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 20:04:32 +0000 (UTC)
Received: (qmail 31459 invoked by uid 500); 5 Aug 2014 20:04:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31396 invoked by uid 500); 5 Aug 2014 20:04:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31385 invoked by uid 99); 5 Aug 2014 20:04:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:04:31 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:04:24 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so1979580wiv.13
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 13:04:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=hjepBGTqT2ZZ+nA9NjMkKe2lXUw9z9X//WcbmF4Gb58=;
        b=lE7xbhkXxaH7uJzsqTms+KRjpbLR8Hf6iZ8/GNrMkOrOsLGSeql3ssKMlkYbHQ0Bwb
         taEWMt+Ouf264z4w1dqozAUnzEiPEZM2p6k8kfZMAcJCYYKBQt+U9gouSVZHgcuuKS5F
         xh/bJNJOj9ikOacVQ5zP9tUI7XJr11fn/Td/Hz3X1mmIoh+CRonp+4xTNt2TfoygTP9U
         jkmp6DbfMs3uOeuGUAl3o1CZ4M1OheXQjxMEdTo9lvUHtAwJF9PPEiqj9GKAxpEwu3lG
         KqzEVJ2+x3yA/p5zP/QSATN+BSuN7FxpPWg/noc0X6ooa2pPRAowItWVUreKKAHzfBZy
         wd/A==
X-Gm-Message-State: ALoCoQmr7QjYAfYNwD3PKfw+VLwmzajHfUP/SyqBTDC5pAHpAmlYouqq/H04Nsup4LHCOzspS7nA
X-Received: by 10.180.106.99 with SMTP id gt3mr20585273wib.1.1407269042539;
 Tue, 05 Aug 2014 13:04:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Tue, 5 Aug 2014 13:03:42 -0700 (PDT)
In-Reply-To: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 5 Aug 2014 13:03:42 -0700
Message-ID: <CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04428e56626cfc04ffe75b6e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428e56626cfc04ffe75b6e
Content-Type: text/plain; charset=UTF-8

I'm pretty sure it is an oversight. Would you like to submit a pull request
to fix that?



On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com> wrote:

> Within its compute.close method, the JdbcRDD class has this interesting
> logic for closing jdbc connection:
>
>
>       try {
>         if (null != conn && ! stmt.isClosed()) conn.close()
>         logInfo("closed connection")
>       } catch {
>         case e: Exception => logWarning("Exception closing connection", e)
>       }
>
> Notice that the second check is on stmt  having been closed - not on the
> connection.
>
> I would wager this were not a simple oversight and there were some
> motivation for this logic- curious if anyone would be able to shed some
> light?   My particular interest is that I have written custom ORM's in jdbc
> since late 90's  and never did it this way.
>

--f46d04428e56626cfc04ffe75b6e--

From dev-return-8732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 20:12:27 2014
Return-Path: <dev-return-8732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5768211320
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 20:12:27 +0000 (UTC)
Received: (qmail 58548 invoked by uid 500); 5 Aug 2014 20:12:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58488 invoked by uid 500); 5 Aug 2014 20:12:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58476 invoked by uid 99); 5 Aug 2014 20:12:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:12:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:12:20 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so2530118vcb.13
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 13:12:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=QBQb+XrRGV3PO8Nw6vCxrbZLWjYPjihJtFXQogaY+vE=;
        b=T7/yC0ZZCLz1diOKOLA9xmw4DO2Cf7oYK6jw6+fVe0PlnLlVimrjIrZlKrfMSR5CML
         2FRQyjgVITN+AZaxmjJgvl44uVTsyR/wPtAv7t+nTTDLqDCTHocXHO8Pqcb2L2hzggNG
         EweV2IFhi7e8USrEjOcm4LssW2IgjvNWU4242BZ5tUC18tMafnAagys7+iVzCWzmRLwn
         jH4gYstoF8HpB8N5Yz+H936b9aa/+IpEGStc2bKxXqpETLU0Jl0ZtM2erh3UuQp2/Xpb
         s+swgo3jid1iXE/sG86K1B9aQLHirJXt1P569Oi++VRtwfmPL7kGpY3rIa3nJfvuw7Cv
         REbw==
MIME-Version: 1.0
X-Received: by 10.220.74.195 with SMTP id v3mr6478811vcj.23.1407269519918;
 Tue, 05 Aug 2014 13:11:59 -0700 (PDT)
Received: by 10.52.114.194 with HTTP; Tue, 5 Aug 2014 13:11:59 -0700 (PDT)
In-Reply-To: <CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
	<CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
Date: Tue, 5 Aug 2014 13:11:59 -0700
Message-ID: <CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
From: Stephen Boesch <javadba@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01634bf0d68bc504ffe777f4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634bf0d68bc504ffe777f4
Content-Type: text/plain; charset=UTF-8

Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
Another small concern: there appears to be no exception handling from the
creation of the prepared statement (line 74) through to the executeQuery
(line 86).   In case of error/exception it would seem to be leaking
connections (/statements).  If that were the case then I would include a
small patch for the exception trapping in that section of code as well.
 BTW I was looking at this code for another reason, not intending to be a
bother ;)




2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:

> I'm pretty sure it is an oversight. Would you like to submit a pull
> request to fix that?
>
>
>
> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
>> Within its compute.close method, the JdbcRDD class has this interesting
>> logic for closing jdbc connection:
>>
>>
>>       try {
>>         if (null != conn && ! stmt.isClosed()) conn.close()
>>         logInfo("closed connection")
>>       } catch {
>>         case e: Exception => logWarning("Exception closing connection", e)
>>       }
>>
>> Notice that the second check is on stmt  having been closed - not on the
>> connection.
>>
>> I would wager this were not a simple oversight and there were some
>> motivation for this logic- curious if anyone would be able to shed some
>> light?   My particular interest is that I have written custom ORM's in
>> jdbc
>> since late 90's  and never did it this way.
>>
>
>

--089e01634bf0d68bc504ffe777f4--

From dev-return-8733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 20:16:02 2014
Return-Path: <dev-return-8733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC76E113C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 20:16:02 +0000 (UTC)
Received: (qmail 67674 invoked by uid 500); 5 Aug 2014 20:16:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67631 invoked by uid 500); 5 Aug 2014 20:16:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67620 invoked by uid 99); 5 Aug 2014 20:16:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:16:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:15:57 +0000
Received: by mail-wi0-f169.google.com with SMTP id n3so8418556wiv.4
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 13:15:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=0ySmbUmB17nNtggguqti9mzWQg74dpplTbbkEdAVPb4=;
        b=P53GikUmleyVEY46gzTCz9u0jXbTBZAOprjiQTqw5yzoXATppNIW9xN31ISuGrnELq
         de1qlQNKl//Ab4pfNfjlH1EUb7JvKoTf/Ii6cnq+ZFbZMKX1vfaqN8rrdshNZUHIMCma
         DnPrNmLCbnq49pZcgzga/upgQZllUe4bK3kDxmKRCtUUDk57jdRr/HhxriZpKVBSmQm1
         h0arWbgawNTUeW6oJBAnfucG0fzEuMpF4TW4k6RgWD5gqNaj8YfZ9mf0AUOOzVQLws1c
         1aLTHocjm0Iq256JgmtWnwNICetJP7/7JWRT2D5kI2aICPVa4VAGWcJBRTWUG2xXY7mR
         6Mxg==
X-Gm-Message-State: ALoCoQn4TSp7jZSubTm4EIi0fOm58V8RAqv3FADDQhUsF1uAw/PgZQK9oH1utt0BbXAZKJMI0aV7
X-Received: by 10.195.18.8 with SMTP id gi8mr8943822wjd.75.1407269731053; Tue,
 05 Aug 2014 13:15:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Tue, 5 Aug 2014 13:15:10 -0700 (PDT)
In-Reply-To: <CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
 <CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com> <CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 5 Aug 2014 13:15:10 -0700
Message-ID: <CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1130cc466c48b504ffe78480
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1130cc466c48b504ffe78480
Content-Type: text/plain; charset=UTF-8

Thanks. Those are definitely great problems to fix!



On Tue, Aug 5, 2014 at 1:11 PM, Stephen Boesch <javadba@gmail.com> wrote:

> Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
> Another small concern: there appears to be no exception handling from the
> creation of the prepared statement (line 74) through to the executeQuery
> (line 86).   In case of error/exception it would seem to be leaking
> connections (/statements).  If that were the case then I would include a
> small patch for the exception trapping in that section of code as well.
>  BTW I was looking at this code for another reason, not intending to be a
> bother ;)
>
>
>
>
> 2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>
> I'm pretty sure it is an oversight. Would you like to submit a pull
>> request to fix that?
>>
>>
>>
>> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com>
>> wrote:
>>
>>> Within its compute.close method, the JdbcRDD class has this interesting
>>> logic for closing jdbc connection:
>>>
>>>
>>>       try {
>>>         if (null != conn && ! stmt.isClosed()) conn.close()
>>>         logInfo("closed connection")
>>>       } catch {
>>>         case e: Exception => logWarning("Exception closing connection",
>>> e)
>>>       }
>>>
>>> Notice that the second check is on stmt  having been closed - not on the
>>> connection.
>>>
>>> I would wager this were not a simple oversight and there were some
>>> motivation for this logic- curious if anyone would be able to shed some
>>> light?   My particular interest is that I have written custom ORM's in
>>> jdbc
>>> since late 90's  and never did it this way.
>>>
>>
>>
>

--001a1130cc466c48b504ffe78480--

From dev-return-8734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 20:58:39 2014
Return-Path: <dev-return-8734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A3FB114E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 20:58:39 +0000 (UTC)
Received: (qmail 68947 invoked by uid 500); 5 Aug 2014 20:58:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68879 invoked by uid 500); 5 Aug 2014 20:58:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68861 invoked by uid 99); 5 Aug 2014 20:58:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:58:38 +0000
X-ASF-Spam-Status: No, hits=3.4 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 20:58:33 +0000
Received: by mail-oa0-f53.google.com with SMTP id j17so1143598oag.40
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 13:58:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:cc:content-type;
        bh=Gkd9aO5MhSJbwrC7sND4irvK42h3UcCmoa/P2Kf/y3I=;
        b=KGtkRHTDYtNusMpEY9X8ZyrGPqAyUHF3qCFdrSw3Ju9xPU9jIsaGeSil2+Bv27Ck/Q
         gHP4J44paKFhEVBURFchZmjcjWLU4AguAFsBNf3XKoiW7VOf5AE7IAhcM8OFtcDMLrVw
         k/Dq4HEiywC8CQCzgI8eK4kOhDxav0fKKBG3egmn70+w97H9oODK62yd0SYUrxdE8S1F
         SocWZzL5bkIbvGLEtpN8in4BBjKu10SN4Zfhg8EGKSL8W4RNq1fSTDhw6OHnQGX9P0Zu
         PQ5/5wcrEwV1u9xXatMqh4OmXUYrUadJ80dMDxT2SOT/VmGppC1NEyP7iZ5fUy8LycGp
         I6ng==
X-Gm-Message-State: ALoCoQkUjzt7/8W/K94BdeAI7lhRDebQj4uf92HKo8Z6FcNl6Ng7FugcS33Fvz4P+Rt7rQUXEMph
MIME-Version: 1.0
X-Received: by 10.182.209.5 with SMTP id mi5mt4326684obc.33.1407272292239;
 Tue, 05 Aug 2014 13:58:12 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Tue, 5 Aug 2014 13:58:12 -0700 (PDT)
In-Reply-To: <CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
	<CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
	<CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
	<CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com>
Date: Tue, 5 Aug 2014 15:58:12 -0500
Message-ID: <CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
From: Cody Koeninger <cody@koeninger.org>
Cc: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8ff1cf9217285a04ffe81dec
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1cf9217285a04ffe81dec
Content-Type: text/plain; charset=UTF-8

The stmt.isClosed just looks like stupidity on my part, no secret
motivation :)  Thanks for noticing it.

As for the leaking in the case of malformed statements, isn't that
addressed by

context.addOnCompleteCallback{ () => closeIfNeeded() }

or am I misunderstanding?


On Tue, Aug 5, 2014 at 3:15 PM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks. Those are definitely great problems to fix!
>
>
>
> On Tue, Aug 5, 2014 at 1:11 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
> > Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
> > Another small concern: there appears to be no exception handling from the
> > creation of the prepared statement (line 74) through to the executeQuery
> > (line 86).   In case of error/exception it would seem to be leaking
> > connections (/statements).  If that were the case then I would include a
> > small patch for the exception trapping in that section of code as well.
> >  BTW I was looking at this code for another reason, not intending to be a
> > bother ;)
> >
> >
> >
> >
> > 2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:
> >
> > I'm pretty sure it is an oversight. Would you like to submit a pull
> >> request to fix that?
> >>
> >>
> >>
> >> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com>
> >> wrote:
> >>
> >>> Within its compute.close method, the JdbcRDD class has this interesting
> >>> logic for closing jdbc connection:
> >>>
> >>>
> >>>       try {
> >>>         if (null != conn && ! stmt.isClosed()) conn.close()
> >>>         logInfo("closed connection")
> >>>       } catch {
> >>>         case e: Exception => logWarning("Exception closing connection",
> >>> e)
> >>>       }
> >>>
> >>> Notice that the second check is on stmt  having been closed - not on
> the
> >>> connection.
> >>>
> >>> I would wager this were not a simple oversight and there were some
> >>> motivation for this logic- curious if anyone would be able to shed some
> >>> light?   My particular interest is that I have written custom ORM's in
> >>> jdbc
> >>> since late 90's  and never did it this way.
> >>>
> >>
> >>
> >
>

--e89a8ff1cf9217285a04ffe81dec--

From dev-return-8735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 21:01:27 2014
Return-Path: <dev-return-8735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5268E1151D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 21:01:27 +0000 (UTC)
Received: (qmail 82550 invoked by uid 500); 5 Aug 2014 21:01:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82495 invoked by uid 500); 5 Aug 2014 21:01:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82483 invoked by uid 99); 5 Aug 2014 21:01:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:01:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:01:21 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so2570285vcb.24
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 14:01:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=RSs7p1Unv7gB8NFjmTIMpBfmRMAYudw9yQAvymqhLZs=;
        b=uStZmJsaeRiidafhYC6BEVAp2ht9zRr4qRK6KkAH7hfmJ1v653MSTZCmawuAsd/2DL
         tHKXGlXgIcaSd9MmgupAeAe0+G0ug1AgzSjRIAhtBdqi5eFQanmAsPDh5pTSXvqn/AzD
         r5TGPwSYsCt1zYm4wB9VH4PK3qKSLM/8vsKwg6led9dXrxr4F9K+4vopwh2bmaS7NeNb
         KHimYcW4bCCoEK0VBYnrjTmJ3hHuAKr0Uw/XTIuxUn7spHbgcEssgrrKQFgHlV2o9f8a
         SiK2yhOOqAzoev2yMNfRlJ0IzB+oS09AwPjqen004BQ8EtIC9oRbbE0+bEbHp8f/zkR9
         neig==
MIME-Version: 1.0
X-Received: by 10.220.92.5 with SMTP id p5mr6739032vcm.7.1407272460854; Tue,
 05 Aug 2014 14:01:00 -0700 (PDT)
Received: by 10.52.114.194 with HTTP; Tue, 5 Aug 2014 14:01:00 -0700 (PDT)
In-Reply-To: <CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
	<CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
	<CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
	<CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com>
	<CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
Date: Tue, 5 Aug 2014 14:01:00 -0700
Message-ID: <CACkSZy0dyE-Q0SQH0YPDOFJqYbZqFPKn5uEe4MZaS=4Gq_97Yg@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
From: Stephen Boesch <javadba@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01633d6c21ad2e04ffe827fa
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01633d6c21ad2e04ffe827fa
Content-Type: text/plain; charset=UTF-8

Hi yes that callback takes care of it. thanks!


2014-08-05 13:58 GMT-07:00 Cody Koeninger <cody@koeninger.org>:

> The stmt.isClosed just looks like stupidity on my part, no secret
> motivation :)  Thanks for noticing it.
>
> As for the leaking in the case of malformed statements, isn't that
> addressed by
>
> context.addOnCompleteCallback{ () => closeIfNeeded() }
>
> or am I misunderstanding?
>
>
> On Tue, Aug 5, 2014 at 3:15 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Thanks. Those are definitely great problems to fix!
>>
>>
>>
>> On Tue, Aug 5, 2014 at 1:11 PM, Stephen Boesch <javadba@gmail.com> wrote:
>>
>> > Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
>> > Another small concern: there appears to be no exception handling from
>> the
>> > creation of the prepared statement (line 74) through to the executeQuery
>> > (line 86).   In case of error/exception it would seem to be leaking
>> > connections (/statements).  If that were the case then I would include a
>> > small patch for the exception trapping in that section of code as well.
>> >  BTW I was looking at this code for another reason, not intending to be
>> a
>> > bother ;)
>> >
>> >
>> >
>> >
>> > 2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>> >
>> > I'm pretty sure it is an oversight. Would you like to submit a pull
>> >> request to fix that?
>> >>
>> >>
>> >>
>> >> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com>
>> >> wrote:
>> >>
>> >>> Within its compute.close method, the JdbcRDD class has this
>> interesting
>> >>> logic for closing jdbc connection:
>> >>>
>> >>>
>> >>>       try {
>> >>>         if (null != conn && ! stmt.isClosed()) conn.close()
>> >>>         logInfo("closed connection")
>> >>>       } catch {
>> >>>         case e: Exception => logWarning("Exception closing
>> connection",
>> >>> e)
>> >>>       }
>> >>>
>> >>> Notice that the second check is on stmt  having been closed - not on
>> the
>> >>> connection.
>> >>>
>> >>> I would wager this were not a simple oversight and there were some
>> >>> motivation for this logic- curious if anyone would be able to shed
>> some
>> >>> light?   My particular interest is that I have written custom ORM's in
>> >>> jdbc
>> >>> since late 90's  and never did it this way.
>> >>>
>> >>
>> >>
>> >
>>
>
>

--089e01633d6c21ad2e04ffe827fa--

From dev-return-8736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 21:02:19 2014
Return-Path: <dev-return-8736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4A4911521
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 21:02:19 +0000 (UTC)
Received: (qmail 84846 invoked by uid 500); 5 Aug 2014 21:02:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84798 invoked by uid 500); 5 Aug 2014 21:02:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84787 invoked by uid 99); 5 Aug 2014 21:02:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:02:18 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:02:14 +0000
Received: by mail-wi0-f171.google.com with SMTP id hi2so7748409wib.10
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 14:01:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=jg1Q7g1268QBvwBpmEPxYzxEltjjKOd59mba9mRP1a0=;
        b=Mdx50v3Aii5CETLYWaKOIKbLtzoNpSoS1sTZfh7hj/8J3WOwIbN6o1zTmOFRyVz8d5
         H9mQdtX3p6XqWRAJSLdcl/5ilO+emTJD4p+cIk0f5IoXThtwG8e4Asm4O9hwQP4O50AD
         YDuCkdk0um4V546w3ks8/bVXkl8e5yQpZ5/82cxIzsDpcvShzYEyOKxODrmmfIUMTbyX
         J/ZLrW/OO6eFS3TRUoGSFPtP6t7z/mVX3IVWSCPOXBIFirGHBbYj9DOzARn+NPO3xYKS
         W0vzRY7KtslKW6ii22ELzmQDisrRab7aflc2ASjbiFyY7nwT9Xfq1oE0lzsskUZCqLxP
         LNNg==
X-Gm-Message-State: ALoCoQl3ZPPFh3Nb39GEU/LyYJ7mE+TNHZl2eD3L77I4LxtUFZdi5Ol6pqTLyCkUKd2kWcM1O/zz
X-Received: by 10.180.95.66 with SMTP id di2mr10336164wib.60.1407272513314;
 Tue, 05 Aug 2014 14:01:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Tue, 5 Aug 2014 14:01:33 -0700 (PDT)
In-Reply-To: <CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
 <CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
 <CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
 <CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com> <CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 5 Aug 2014 14:01:33 -0700
Message-ID: <CAPh_B=aVXZkLqUSUp_TLNMFQoT7bgtVXCU5eW3GwYE_PT1CAyQ@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
To: Cody Koeninger <cody@koeninger.org>
Cc: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04182524423d4904ffe82a80
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04182524423d4904ffe82a80
Content-Type: text/plain; charset=UTF-8

Yes it is. I actually commented on it:
https://github.com/apache/spark/pull/1792/files#r15840899



On Tue, Aug 5, 2014 at 1:58 PM, Cody Koeninger <cody@koeninger.org> wrote:

> The stmt.isClosed just looks like stupidity on my part, no secret
> motivation :)  Thanks for noticing it.
>
> As for the leaking in the case of malformed statements, isn't that
> addressed by
>
> context.addOnCompleteCallback{ () => closeIfNeeded() }
>
> or am I misunderstanding?
>
>
> On Tue, Aug 5, 2014 at 3:15 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Thanks. Those are definitely great problems to fix!
> >
> >
> >
> > On Tue, Aug 5, 2014 at 1:11 PM, Stephen Boesch <javadba@gmail.com>
> wrote:
> >
> > > Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
> > > Another small concern: there appears to be no exception handling from
> the
> > > creation of the prepared statement (line 74) through to the
> executeQuery
> > > (line 86).   In case of error/exception it would seem to be leaking
> > > connections (/statements).  If that were the case then I would include
> a
> > > small patch for the exception trapping in that section of code as well.
> > >  BTW I was looking at this code for another reason, not intending to
> be a
> > > bother ;)
> > >
> > >
> > >
> > >
> > > 2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:
> > >
> > > I'm pretty sure it is an oversight. Would you like to submit a pull
> > >> request to fix that?
> > >>
> > >>
> > >>
> > >> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com>
> > >> wrote:
> > >>
> > >>> Within its compute.close method, the JdbcRDD class has this
> interesting
> > >>> logic for closing jdbc connection:
> > >>>
> > >>>
> > >>>       try {
> > >>>         if (null != conn && ! stmt.isClosed()) conn.close()
> > >>>         logInfo("closed connection")
> > >>>       } catch {
> > >>>         case e: Exception => logWarning("Exception closing
> connection",
> > >>> e)
> > >>>       }
> > >>>
> > >>> Notice that the second check is on stmt  having been closed - not on
> > the
> > >>> connection.
> > >>>
> > >>> I would wager this were not a simple oversight and there were some
> > >>> motivation for this logic- curious if anyone would be able to shed
> some
> > >>> light?   My particular interest is that I have written custom ORM's
> in
> > >>> jdbc
> > >>> since late 90's  and never did it this way.
> > >>>
> > >>
> > >>
> > >
> >
>

--f46d04182524423d4904ffe82a80--

From dev-return-8737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 21:27:04 2014
Return-Path: <dev-return-8737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6FA5211611
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 21:27:04 +0000 (UTC)
Received: (qmail 44538 invoked by uid 500); 5 Aug 2014 21:27:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44480 invoked by uid 500); 5 Aug 2014 21:27:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44468 invoked by uid 99); 5 Aug 2014 21:27:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:27:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:27:02 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so2683889vcb.19
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 14:26:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=AkwKNPprTYL4HakB5SJPAVrBbeLC0+r6hIo11Qiw4Kw=;
        b=zI9Vb89VGMK95TXxqNDgVwqIG1BcPc9WYURXG+kJ9ISUtSigvpZKZIWGZOoSvUvtiB
         kAibvkDY1x6jPXERLrS/l3pc+dftoiIbGWZLsRMAExCk3DbMzgbrz9eXXtMfzAkRO+a/
         xEn4u8jnwnbVoOEL53OQf9xh3aItJzZ8vI+tZTPdCrjNYneancAS1f7nIfhsre9XAeVc
         Clo/Nc/641tzW17afS4oXINEpElK7OvP9CyWE15Go5BE5kUSr0pQ4vuSYcVhuMQx6yN5
         twuMabCUaOil2g/ePgjXYq/FGG7ZurTQ3Is4oR92p5M+0LdAtIAhA4fdBM3FoCvfJbdn
         CBCA==
MIME-Version: 1.0
X-Received: by 10.52.162.74 with SMTP id xy10mr4604746vdb.51.1407273997017;
 Tue, 05 Aug 2014 14:26:37 -0700 (PDT)
Received: by 10.52.114.194 with HTTP; Tue, 5 Aug 2014 14:26:36 -0700 (PDT)
In-Reply-To: <CAPh_B=aVXZkLqUSUp_TLNMFQoT7bgtVXCU5eW3GwYE_PT1CAyQ@mail.gmail.com>
References: <CACkSZy0HgQ22LtCtrpMKGaiq=-DiBtzJ68TFW=fReBkh=ZHCsQ@mail.gmail.com>
	<CAPh_B=a3Xu4B9icJ9P8g7+c01d8uBjeu+3U7oyuBJjppd=WXZQ@mail.gmail.com>
	<CACkSZy2r7ML4e-2h_V7+9eNpPTeky89g8Wo_ogd6eGAOKPbdBg@mail.gmail.com>
	<CAPh_B=YVYqiNc8i9cKhYsN2wA+ZiuiOYBMV9+YH2kDrVBuBbsg@mail.gmail.com>
	<CAKWX9VVv6bnANPW6-ZV_1EMAq_-Q3FcVxAjbUPXcALWC=copXQ@mail.gmail.com>
	<CAPh_B=aVXZkLqUSUp_TLNMFQoT7bgtVXCU5eW3GwYE_PT1CAyQ@mail.gmail.com>
Date: Tue, 5 Aug 2014 14:26:36 -0700
Message-ID: <CACkSZy3Vz4VptYX0PVBZoPzpfimvXsD=T+izxv4dx9CAOuQS8g@mail.gmail.com>
Subject: Re: Tiny curiosity question on closing the jdbc connection
From: Stephen Boesch <javadba@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Cody Koeninger <cody@koeninger.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01538648b1a75504ffe8824f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01538648b1a75504ffe8824f
Content-Type: text/plain; charset=UTF-8

The existing callback does take care of it:  within the DAGScheduler  there
is a finally block to ensure the callbacks are executed.

      try {
        val result = job.func(taskContext, rdd.iterator(split, taskContext))
        job.listener.taskSucceeded(0, result)
      } finally {
        taskContext.executeOnCompleteCallbacks()
      }

So I have removed that exception handling code from the  PR and updated the
JIRA.


2014-08-05 14:01 GMT-07:00 Reynold Xin <rxin@databricks.com>:

> Yes it is. I actually commented on it:
> https://github.com/apache/spark/pull/1792/files#r15840899
>
>
>
> On Tue, Aug 5, 2014 at 1:58 PM, Cody Koeninger <cody@koeninger.org> wrote:
>
>> The stmt.isClosed just looks like stupidity on my part, no secret
>> motivation :)  Thanks for noticing it.
>>
>> As for the leaking in the case of malformed statements, isn't that
>> addressed by
>>
>> context.addOnCompleteCallback{ () => closeIfNeeded() }
>>
>> or am I misunderstanding?
>>
>>
>> On Tue, Aug 5, 2014 at 3:15 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> > Thanks. Those are definitely great problems to fix!
>> >
>> >
>> >
>> > On Tue, Aug 5, 2014 at 1:11 PM, Stephen Boesch <javadba@gmail.com>
>> wrote:
>> >
>> > > Thanks Reynold, Ted Yu did mention offline and I put in a jira
>> already.
>> > > Another small concern: there appears to be no exception handling from
>> the
>> > > creation of the prepared statement (line 74) through to the
>> executeQuery
>> > > (line 86).   In case of error/exception it would seem to be leaking
>> > > connections (/statements).  If that were the case then I would
>> include a
>> > > small patch for the exception trapping in that section of code as
>> well.
>> > >  BTW I was looking at this code for another reason, not intending to
>> be a
>> > > bother ;)
>> > >
>> > >
>> > >
>> > >
>> > > 2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>> > >
>> > > I'm pretty sure it is an oversight. Would you like to submit a pull
>> > >> request to fix that?
>> > >>
>> > >>
>> > >>
>> > >> On Tue, Aug 5, 2014 at 12:14 PM, Stephen Boesch <javadba@gmail.com>
>> > >> wrote:
>> > >>
>> > >>> Within its compute.close method, the JdbcRDD class has this
>> interesting
>> > >>> logic for closing jdbc connection:
>> > >>>
>> > >>>
>> > >>>       try {
>> > >>>         if (null != conn && ! stmt.isClosed()) conn.close()
>> > >>>         logInfo("closed connection")
>> > >>>       } catch {
>> > >>>         case e: Exception => logWarning("Exception closing
>> connection",
>> > >>> e)
>> > >>>       }
>> > >>>
>> > >>> Notice that the second check is on stmt  having been closed - not on
>> > the
>> > >>> connection.
>> > >>>
>> > >>> I would wager this were not a simple oversight and there were some
>> > >>> motivation for this logic- curious if anyone would be able to shed
>> some
>> > >>> light?   My particular interest is that I have written custom ORM's
>> in
>> > >>> jdbc
>> > >>> since late 90's  and never did it this way.
>> > >>>
>> > >>
>> > >>
>> > >
>> >
>>
>
>

--089e01538648b1a75504ffe8824f--

From dev-return-8738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 21:43:37 2014
Return-Path: <dev-return-8738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7D4A116F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 21:43:37 +0000 (UTC)
Received: (qmail 6869 invoked by uid 500); 5 Aug 2014 21:43:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6806 invoked by uid 500); 5 Aug 2014 21:43:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6792 invoked by uid 99); 5 Aug 2014 21:43:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:43:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of guruyvs@yahoo.com designates 72.30.238.192 as permitted sender)
Received: from [72.30.238.192] (HELO nm34.bullet.mail.bf1.yahoo.com) (72.30.238.192)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 21:43:26 +0000
Received: from [98.139.215.141] by nm34.bullet.mail.bf1.yahoo.com with NNFMP; 05 Aug 2014 21:43:05 -0000
Received: from [98.139.212.196] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 05 Aug 2014 21:43:05 -0000
Received: from [127.0.0.1] by omp1005.mail.bf1.yahoo.com with NNFMP; 05 Aug 2014 21:43:05 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 252942.93781.bm@omp1005.mail.bf1.yahoo.com
Received: (qmail 32563 invoked by uid 60001); 5 Aug 2014 21:43:05 -0000
X-YMail-OSG: _4rxRYoVM1legy5LSOGPa96k2qgWvk5Mga8EmqQBWMIrINH
 yXzx6SoM4.S97hQIItlLbYkComqi7w9LL5l6loMoDs1MSijpJI714J_k0qA7
 nspKmlVrW88GVLZNWW5SXhkDsJc689uSNKBMXkPqLb.ibVpIdfP1_fCP_EcH
 0nNWmi0eXq4YoiuD4hZTJjgBH3nBrFjEgA9e2xiNdxWEEJkUQ9VpXs7TPwjN
 ccqf5l5p.Uf1fvhio4lRfvvkSzR_oAFScl.NVxpDdMWkk00qPysMVT0fSE.x
 kWOlTxlOgJhJ4FwtThRy.luIARrepgRQVjX4AkGZpoNMTQ2Dp8xRQxMgt7T8
 jee2y8t8lBUm1jaU0lqxl4kC6g8_UrQz_kMHZ.0YXTQW5kj6ttrciujWyaVR
 Z7_kgX_jkFJ3zHB5.uyQZ1SQWl51E1cIdf6heBp_law5pMmMvns7djvcLHO6
 YwBFSsfztECNGBXBfoQJN8wberGq.f5PJp3oNFHr.IvOIxKC4gFw.raiZi1w
 EAS4hZAKqJyX2HVWjIrvaDhlcdC2EBM_HSp2aT8dMKSdN5dWP
Received: from [12.250.117.158] by web160403.mail.bf1.yahoo.com via HTTP; Tue, 05 Aug 2014 14:43:04 PDT
X-Rocket-MIMEInfo: 002.001,SW0gbmV3IHRvIFNwYXJrIGNvbW11bml0eS4gQWN0aXZlbHkgd29ya2luZyBvbiBIYWRvb3AgZWNvIHN5c3RlbSAoIG1vcmUgc3BlY2lmaWNhbGx5IFlBUk4pLiBJJ20gdmVyeSBrZWVuIG9uIGdldHRpbmcgbXkgaGFuZHMgZGlydGlseSB3aXRoIFNwYXJrLiBQbGVhc2UgbGV0IG1lIGtub3cgYW55IHBvaW50ZXJzIHRvIHN0YXJ0IHdpdGguCgpUaGFua3MgaW4gYWR2YW5jZQpCZXN0IHJlZ2FyZHMKR3VydSBZZWxlc3dhcmFwdQEwAQEBAQ--
X-Mailer: YahooMailWebService/0.8.198.689
Message-ID: <1407274984.35858.YahooMailNeo@web160403.mail.bf1.yahoo.com>
Date: Tue, 5 Aug 2014 14:43:04 -0700
From: Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>
Reply-To: Gurumurthy Yeleswarapu <guruyvs@yahoo.com>
Subject: Hello All
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="1466435115-1472993464-1407274984=:35858"
X-Virus-Checked: Checked by ClamAV on apache.org

--1466435115-1472993464-1407274984=:35858
Content-Type: text/plain; charset=us-ascii

Im new to Spark community. Actively working on Hadoop eco system ( more specifically YARN). I'm very keen on getting my hands dirtily with Spark. Please let me know any pointers to start with.

Thanks in advance
Best regards
Guru Yeleswarapu
--1466435115-1472993464-1407274984=:35858--

From dev-return-8739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug  5 22:52:54 2014
Return-Path: <dev-return-8739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 062B2119A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Aug 2014 22:52:54 +0000 (UTC)
Received: (qmail 91561 invoked by uid 500); 5 Aug 2014 22:52:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91504 invoked by uid 500); 5 Aug 2014 22:52:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91492 invoked by uid 99); 5 Aug 2014 22:52:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 22:52:53 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of byavuz@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Aug 2014 22:52:45 +0000
Received: from codegreen2.stanford.edu (codegreen2.Stanford.EDU [171.67.224.3])
	(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by smtp.stanford.edu (Postfix) with ESMTPS id 6EFAE102105;
	Tue,  5 Aug 2014 15:52:24 -0700 (PDT)
Received: from codegreen2.stanford.edu (localhost.localdomain [127.0.0.1])
	by codegreen2.stanford.edu (Postfix) with ESMTP id 580C265;
	Tue,  5 Aug 2014 15:52:24 -0700 (PDT)
Received: from smtp.stanford.edu (smtp2.Stanford.EDU [171.67.219.82])
	(using TLSv1 with cipher ADH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by codegreen2.stanford.edu (Postfix) with ESMTP id 4D06865;
	Tue,  5 Aug 2014 15:52:24 -0700 (PDT)
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 377D9342439;
	Tue,  5 Aug 2014 15:52:24 -0700 (PDT)
Received: from zm01.stanford.edu (zm01.Stanford.EDU [171.67.219.145])
	by smtp.stanford.edu (Postfix) with ESMTP id 1420E34242D;
	Tue,  5 Aug 2014 15:52:24 -0700 (PDT)
Date: Tue, 5 Aug 2014 15:52:23 -0700 (PDT)
From: Burak Yavuz <byavuz@stanford.edu>
To: Gurumurthy Yeleswarapu <guruyvs@yahoo.com>
Cc: dev@spark.apache.org
Message-ID: <679322872.2544132.1407279143772.JavaMail.zimbra@stanford.edu>
In-Reply-To: <1407274984.35858.YahooMailNeo@web160403.mail.bf1.yahoo.com>
References: <1407274984.35858.YahooMailNeo@web160403.mail.bf1.yahoo.com>
Subject: Re: Hello All
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [67.164.94.237]
X-Mailer: Zimbra 8.0.7_GA_6021 (ZimbraWebClient - GC36 (Mac)/8.0.7_GA_6021)
X-Authenticated-User: byavuz@stanford.edu
Thread-Topic: Hello All
Thread-Index: ZUUe6CnK9yqAJ7bLIu0UrPefZ7YR4w==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Guru,

Take a look at:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

It has all the information you need on how to contribute to Spark. Also take a look at:
https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel

where a list of issues exist that need fixing. You can also request or propose new additions to Spark.

Happy coding!
Burak

----- Original Message -----
From: "Gurumurthy Yeleswarapu" <guruyvs@yahoo.com.INVALID>
To: dev@spark.apache.org
Sent: Tuesday, August 5, 2014 2:43:04 PM
Subject: Hello All

Im new to Spark community. Actively working on Hadoop eco system ( more specifically YARN). I'm very keen on getting my hands dirtily with Spark. Please let me know any pointers to start with.

Thanks in advance
Best regards
Guru Yeleswarapu


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 00:28:00 2014
Return-Path: <dev-return-8740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56DE311CD8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 00:28:00 +0000 (UTC)
Received: (qmail 22409 invoked by uid 500); 6 Aug 2014 00:27:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22346 invoked by uid 500); 6 Aug 2014 00:27:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22334 invoked by uid 99); 6 Aug 2014 00:27:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 00:27:59 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 00:27:57 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so1274515oah.37
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 17:27:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=5WlFW4QF65Ye6lVEPdQNt4kOrC4MysiIgJAMCOXii9w=;
        b=ob0vKBjRFMP8uzV+qSqnxhxrIHUwGyToJtyW4mk1UGs7TDXZF6jDhBqIQROAohKdbY
         hH04tAi5Rln1P3jY0RVV9jD+07Y/NKtSdBNoWu0BFsBS71v+u5CbeZuqXQ0zKbvCCVJf
         VJJh8MIUIb2oHxUXV7Umyfu3Vy9cLgLCuVrNaP02FvwpwX7MwNFu7Ld9zTMLg7TYquvT
         31n9IyAludXfUKGc9wZSncS2Cjgdko+JhZqXUpdz/c2uVi6167JPDO+ahhq2vVPcwxpf
         5FmFoa46iitb0WKS6i1UdlkcQkv+P2wcuh0Cpef/DjnhBENp2Dhx+pDC1714FoXrLuZ2
         Uz6Q==
MIME-Version: 1.0
X-Received: by 10.182.68.104 with SMTP id v8mr9235618obt.26.1407284852684;
 Tue, 05 Aug 2014 17:27:32 -0700 (PDT)
Received: by 10.76.35.134 with HTTP; Tue, 5 Aug 2014 17:27:32 -0700 (PDT)
Date: Tue, 5 Aug 2014 17:27:32 -0700
Message-ID: <CAPud8ToQgKRt5ZYeLe0MNm0Mr=+e8WpLtM1i8f6A8q_-uwG-Rg@mail.gmail.com>
Subject: Unit test best practice for Spark-derived projects
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8fb20204bdfe9404ffeb097c
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb20204bdfe9404ffeb097c
Content-Type: text/plain; charset=UTF-8

Hello,

I 've been switching Mahout from Spark 0.9 to Spark 1.0.x [1] and noticed
that tests now run much slower compared to 0.9 with CPU running idle most
of the time. I had to conclude that most of that time is spent on tearing
down/resetting Spark context which apparently now takes significantly
longer time in local mode than before.

Q1 --- Is there a way to mitigate long session startup times with local
context?

Q2 -- Our unit tests are basically mixing in a rip-off of
LocalSparkContext, and we are using local[3]. Looking into 1.0.x code, i
 noticed that a lot of Spark unit test code has switched to
SharedSparkContext (i.e. no context reset between individual tests). Is
that now recommended practice to write Spark-based unit tests?

Q3 -- Any other reasons that i may have missed for degraded test
performance?


[1] https://github.com/apache/mahout/pull/40

thank you in advance.
-Dmitriy

--e89a8fb20204bdfe9404ffeb097c--

From dev-return-8741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 01:00:15 2014
Return-Path: <dev-return-8741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7DA2411DF7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 01:00:15 +0000 (UTC)
Received: (qmail 19538 invoked by uid 500); 6 Aug 2014 01:00:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19471 invoked by uid 500); 6 Aug 2014 01:00:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19453 invoked by uid 99); 6 Aug 2014 01:00:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 01:00:14 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 01:00:08 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so1996206qgd.13
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 17:59:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=E5Hler8bKlSI//0xcj9mYGCr5cDz0HI9XzzYpMBzSrE=;
        b=r3nZru6NJe+amSvcTkNgu/2FS514Z3R6r/hYJhj4bs0G35HbxK5myHSMXiSohnT2VX
         /swG+bWeP4fiWJKcCrqn5/dQI/hSz8/eYaabeoSi5EkeySkxIJOTstDDcqdinJDEMNre
         EvGkFFGNm/yZc+9tZx7DixGbXgYuEVdNTzkFpmiQDz2QpxDzDNiSS6goATwet9BMRTtC
         kguqrzf5YiSJCfQLJYnvqdmX5DBa5Tvn5NXYvjKMR+sJyDMb0NumciLt09oNvK21ZxIf
         kCEy+4Nppl0ZnRbLnUdzbPd44QH1Rq3sfjtMs0iU4L5EyFsHak1frbtsdwcO9VwMjZws
         IUgw==
MIME-Version: 1.0
X-Received: by 10.140.93.161 with SMTP id d30mr10716141qge.53.1407286788307;
 Tue, 05 Aug 2014 17:59:48 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Tue, 5 Aug 2014 17:59:48 -0700 (PDT)
In-Reply-To: <CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
Date: Tue, 5 Aug 2014 17:59:48 -0700
Message-ID: <CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113b97761d42ab04ffeb7dc0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b97761d42ab04ffeb7dc0
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

I used your idea and kept a cherry picked version of ALS.scala in my
application and call it ALSQp.scala...this is a OK workaround for now till
a version adds up to master for example...

For the bug with userClassPathFirst, looks like Koert already found this
issue in the following JIRA:

https://issues.apache.org/jira/browse/SPARK-1863

By the way the userClassPathFirst feature is very useful since I am sure
the deployed version of spark on a production cluster will always be the
last stable (core at 1.0.1 in my case) and people would like to deploy
SNAPSHOT versions of libraries that build on top of spark core (mllib,
streaming etc)...

Another way is to have a build option that deploys only the core and not
the libraries built upon core...

Do we have an option like that in make-distribution script ?

Thanks.
Deb


On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> If you cannot change the Spark jar deployed on the cluster, an easy
> solution would be renaming ALS in your jar. If userClassPathFirst
> doesn't work, could you create a JIRA and attach the log? Thanks!
> -Xiangrui
>
> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > I created the assembly file but still it wants to pick the mllib from the
> > cluster:
> >
> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
> > QuadraticMinimizer
> >
> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
> >
> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath outputPath
> >
> > Exception in thread "main" java.lang.NoSuchMethodError:
> >
> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
> >
> > Now if I force it to use the jar that I gave using
> > spark.files.userClassPathFirst, then it fails on some serialization
> > issues...
> >
> > A simple solution is to cherry pick the files I need from spark branch to
> > the application branch but I am not sure that's the right thing to do...
> >
> > The way userClassPathFirst is behaving, there might be bugs in it...
> >
> > Any suggestions will be appreciated....
> >
> > Thanks.
> > Deb
> >
> >
> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >>
> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
> >> spark-core-1.0.1.
> >>
> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > Let me try it...
> >> >
> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
> >> > SNAPSHOT
> >> > jar and other dependencies with the rest of the application code ?
> >> >
> >> >
> >> >
> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >> >>
> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
> >> >> sure whether it could solve your problem. -Xiangrui
> >> >>
> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
> >> >> <debasish.das83@gmail.com>
> >> >> wrote:
> >> >> > Hi,
> >> >> >
> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have new
> code
> >> >> > that
> >> >> > I added in mllib-1.1.0-SNAPSHOT.
> >> >> >
> >> >> > I am trying to access the new code using spark-submit as follows:
> >> >> >
> >> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda
> 1.0
> >> >> > --qpProblem 2 inputPath outputPath
> >> >> >
> >> >> > I can see the jars are getting added to httpServer as expected:
> >> >> >
> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> at
> >> >> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> >> >> > with
> >> >> > timestamp 1406998204236
> >> >> >
> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
> >> >> > 1406998204237
> >> >> >
> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
> >> >> > timestamp
> >> >> > 1406998204238
> >> >> >
> >> >> > But the job still can't access code form mllib-1.1.0
> SNAPSHOT.jar...I
> >> >> > think
> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
> >> >> >
> >> >> > Please help. I will ask for a PR tomorrow but internally we want to
> >> >> > generate results from the new code.
> >> >> >
> >> >> > Thanks.
> >> >> >
> >> >> > Deb
> >> >
> >> >
> >
> >
>

--001a113b97761d42ab04ffeb7dc0--

From dev-return-8742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 06:43:15 2014
Return-Path: <dev-return-8742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 84BBC116D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 06:43:15 +0000 (UTC)
Received: (qmail 36203 invoked by uid 500); 6 Aug 2014 06:43:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36150 invoked by uid 500); 6 Aug 2014 06:43:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36134 invoked by uid 99); 6 Aug 2014 06:43:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 06:43:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 06:43:10 +0000
Received: by mail-ob0-f169.google.com with SMTP id nu7so1511698obb.0
        for <dev@spark.apache.org>; Tue, 05 Aug 2014 23:42:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=9VXkqxbP1dQa9H8EASMhD15Bu8FFOw0oC1wTY3KLzBE=;
        b=ztsL9IX0a5xMatAUKwieSSW8WEkRFCVzZFuL2jt8jZALBF11t+Yxre18BzFqOYZwiY
         okRJx2rSpiwJ9jXlhuM12UoWj1iUe5q+plFPdn272Y79tRqZU/wjqtxL/qPbSjdeGThf
         bac2HvJLfa6qn/BkhxoG2YcFO+HdIhSYNjG5e1u3g6ZblmylOZOfUeYAADN84F8ny4G9
         DzTTV60VnzSsZQNj2XOreQa34/9sEhmeskH/nAYVJzLigBNXwOvlqlzGWiV8QGsPhpLw
         lzpOXvy2Oylqbo+KJ/Uaf9s58hxULaxvm3NyFOJ6lstCMSA7yymr+2GwFuXG8cq4Oczi
         GzqQ==
MIME-Version: 1.0
X-Received: by 10.182.81.200 with SMTP id c8mr12578451oby.35.1407307369922;
 Tue, 05 Aug 2014 23:42:49 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.202.226.147 with HTTP; Tue, 5 Aug 2014 23:42:49 -0700 (PDT)
In-Reply-To: <CAFboF2xLS18iWWxkBm1NxG65AZoLdG9L8vipdu_uRBLWo-62aw@mail.gmail.com>
References: <CAFboF2yzhoD=mEPrv8shuLUnqU2DbpDpJn7xcSXyWPmShq6wEw@mail.gmail.com>
	<CABPQxssGVanUChfazXUwiyx1JcmtpRyDTjuQmscdyZwM-3ZGCg@mail.gmail.com>
	<CAFboF2xLS18iWWxkBm1NxG65AZoLdG9L8vipdu_uRBLWo-62aw@mail.gmail.com>
Date: Tue, 5 Aug 2014 23:42:49 -0700
X-Google-Sender-Auth: Y_kyg2p4srVIDebbD8MifIhmTV4
Message-ID: <CAFboF2wfBZS=gUYH1D_kwr9=J78oxSgDBmN8Wgr5LMvtaDPvMQ@mail.gmail.com>
Subject: Re: Scala 2.11 external dependencies
From: Anand Avati <avati@gluster.org>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2e4e3edfc76e04fff04737
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e4e3edfc76e04fff04737
Content-Type: text/plain; charset=UTF-8

On Mon, Aug 4, 2014 at 1:01 PM, Anand Avati <avati@gluster.org> wrote:

>
>
>
> On Sun, Aug 3, 2014 at 9:09 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey Anand,
>>
>> Thanks for looking into this - it's great to see momentum towards Scala
>> 2.11 and I'd love if this land in Spark 1.2.
>>
>> For the external dependencies, it would be good to create a sub-task of
>> SPARK-1812 to track our efforts encouraging other projects to upgrade. In
>> certain cases (e.g. Kafka) there is fairly late-stage work on this already,
>> so we can e.g. link to those JIRA's as well. A good starting point is to
>> just go to their dev list and ask what the status is, most Scala projects
>> have put at least some thought into this already. Another thing we can do
>> is submit patches ourselves to those projects to help get them upgraded.
>> The twitter libraries, e.g., tend to be pretty small and also open to
>> external contributions.
>>
>> One other thing in the mix here - Prashant Sharma has also spent some
>> time looking at this, so it might be good for you two to connect (probably
>> off list) and sync up. Prashant has contributed to many Scala projects, so
>> he might have cycles to go and help some of our dependencies get upgraded -
>> but I won't commit to that on his behalf :).
>>
>> Regarding Akka - I shaded and published akka as a one-off thing:
>> https://github.com/pwendell/akka/tree/2.2.3-shaded-proto
>>
>> Over time we've had to publish our own versions of a small number of
>> dependencies. It's somewhat high overhead, but it actually works quite well
>> in terms of avoiding some of the nastier dependency conflicts. At least
>> better than other alternatives I've seen such as using a shader build
>> plug-in.
>>
>> Going forward, I'd actually like to track these in the Spark repo itself.
>> For instance, we have a bash script in the spark repo that can e.g. check
>> out akka, apply a few patches or regular expressions, and then you have a
>> fully shaded dependency that can be published to maven. If you wanted to
>> take a crack at something like that for akka 2.3.4, be my guest. I can help
>> with the actual publishing.
>>
>
> Will give it a try, thanks!
>
>
Patrick, I have a set of scripts at https://github.com/avati/spark-shaded
which generate shaded artifacts for akka 2.3.4 and transitive dependencies
under org.spark-project group for both Scala 2.10 and 2.11 in your local
maven repo ($HOME/.m2/...) Publishing these artifacts is necessary for
https://github.com/apache/spark/pull/1685. Once merged, bumping up Scala
version to 2.11 will be smooth w.r.t akka (verified it on my system)

If you think it will be useful, I can submit a PR of those scripts into
spark.git/extras or somewhere.

Thanks

--047d7b2e4e3edfc76e04fff04737--

From dev-return-8743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 15:29:15 2014
Return-Path: <dev-return-8743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C66BC11645
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 15:29:15 +0000 (UTC)
Received: (qmail 58579 invoked by uid 500); 6 Aug 2014 15:29:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58520 invoked by uid 500); 6 Aug 2014 15:29:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58506 invoked by uid 99); 6 Aug 2014 15:29:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 15:29:14 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 15:29:09 +0000
Received: by mail-qa0-f45.google.com with SMTP id cm18so2553526qab.32
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 08:28:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Zodhhcq2BNvU5vMFUxYjSWwvEtDMKH2a4YrsiGUs5Pg=;
        b=TcvkFl1f38YGwFtPyLNwCyY4dMJKV7kjtoFH4+b9hB4FAiLTAxwERMOsaYZ6acQLpr
         EyNwxqAD9Zt3a20cUp9FGeS47NE+ABO/tmMkdIEbMfCpfxhIKKMDzk26JWTcFrwUADGA
         LdafB9UqyFnIyGI9y/NNEtO+b/astIeZpvgXN1v4n8LfvxS6j8wY4dJGqEP4M5u6rSws
         Q3T41oING1fPTiRZEZhXrF0Zea6fo1WAba9enM2V1aWKxfvh65FLJZTwQfEiWIHZZvaB
         YVe21RyucXj9qcRj7xD6jiAqEs+tsQ3OFoqdYQYa1a8nKwVLnXz9BvWeiUIlOgkNnGIH
         WbvQ==
MIME-Version: 1.0
X-Received: by 10.224.156.194 with SMTP id y2mr5236638qaw.15.1407338928892;
 Wed, 06 Aug 2014 08:28:48 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 6 Aug 2014 08:28:48 -0700 (PDT)
In-Reply-To: <CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
Date: Wed, 6 Aug 2014 08:28:48 -0700
Message-ID: <CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01536e54ef753804fff7a078
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01536e54ef753804fff7a078
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

Maintaining another file will be a pain later so I deployed spark 1.0.1
without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
along with the code changes for quadratic optimization...

Later the plan is to patch the snapshot mllib with the deployed stable
mllib...

There are 5 variants that I am experimenting with around 400M ratings
(daily data, monthly data I will update in few days)...

1. LS
2. NNLS
3. Quadratic with bounds
4. Quadratic with L1
5. Quadratic with equality and positivity

Now the ALS 1.1.0 snapshot runs fine but after completion on this step
ALS.scala:311

// Materialize usersOut and productsOut.
usersOut.count()

I am getting from one of the executors: java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2

I am debugging it further but I was wondering if this is due to RDD
compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?

I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
cluster has Java 1.7.0_45.

The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that Java
version mismatch cause this ?

Stack traces are below

Thanks.
Deb


Executor stacktrace:

org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)

        org.apache.spark.scheduler.Task.run(Task.scala:51)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

 at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi Xiangrui,
>
> I used your idea and kept a cherry picked version of ALS.scala in my
> application and call it ALSQp.scala...this is a OK workaround for now till
> a version adds up to master for example...
>
> For the bug with userClassPathFirst, looks like Koert already found this
> issue in the following JIRA:
>
> https://issues.apache.org/jira/browse/SPARK-1863
>
> By the way the userClassPathFirst feature is very useful since I am sure
> the deployed version of spark on a production cluster will always be the
> last stable (core at 1.0.1 in my case) and people would like to deploy
> SNAPSHOT versions of libraries that build on top of spark core (mllib,
> streaming etc)...
>
> Another way is to have a build option that deploys only the core and not
> the libraries built upon core...
>
> Do we have an option like that in make-distribution script ?
>
> Thanks.
> Deb
>
>
> On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> If you cannot change the Spark jar deployed on the cluster, an easy
>> solution would be renaming ALS in your jar. If userClassPathFirst
>> doesn't work, could you create a JIRA and attach the log? Thanks!
>> -Xiangrui
>>
>> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > I created the assembly file but still it wants to pick the mllib from
>> the
>> > cluster:
>> >
>> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
>> > QuadraticMinimizer
>> >
>> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>> >
>> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>> outputPath
>> >
>> > Exception in thread "main" java.lang.NoSuchMethodError:
>> >
>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>> >
>> > Now if I force it to use the jar that I gave using
>> > spark.files.userClassPathFirst, then it fails on some serialization
>> > issues...
>> >
>> > A simple solution is to cherry pick the files I need from spark branch
>> to
>> > the application branch but I am not sure that's the right thing to do...
>> >
>> > The way userClassPathFirst is behaving, there might be bugs in it...
>> >
>> > Any suggestions will be appreciated....
>> >
>> > Thanks.
>> > Deb
>> >
>> >
>> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>
>> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>> >> spark-core-1.0.1.
>> >>
>> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>> debasish.das83@gmail.com>
>> >> wrote:
>> >> > Let me try it...
>> >> >
>> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
>> >> > SNAPSHOT
>> >> > jar and other dependencies with the rest of the application code ?
>> >> >
>> >> >
>> >> >
>> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >> >>
>> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
>> >> >> sure whether it could solve your problem. -Xiangrui
>> >> >>
>> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>> >> >> <debasish.das83@gmail.com>
>> >> >> wrote:
>> >> >> > Hi,
>> >> >> >
>> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have new
>> code
>> >> >> > that
>> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>> >> >> >
>> >> >> > I am trying to access the new code using spark-submit as follows:
>> >> >> >
>> >> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
>> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda
>> 1.0
>> >> >> > --qpProblem 2 inputPath outputPath
>> >> >> >
>> >> >> > I can see the jars are getting added to httpServer as expected:
>> >> >> >
>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> >
>> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>> >> >> >
>> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>> >> >> > with
>> >> >> > timestamp 1406998204236
>> >> >> >
>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>> timestamp
>> >> >> > 1406998204237
>> >> >> >
>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
>> >> >> > timestamp
>> >> >> > 1406998204238
>> >> >> >
>> >> >> > But the job still can't access code form mllib-1.1.0
>> SNAPSHOT.jar...I
>> >> >> > think
>> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
>> >> >> >
>> >> >> > Please help. I will ask for a PR tomorrow but internally we want
>> to
>> >> >> > generate results from the new code.
>> >> >> >
>> >> >> > Thanks.
>> >> >> >
>> >> >> > Deb
>> >> >
>> >> >
>> >
>> >
>>
>
>

--089e01536e54ef753804fff7a078--

From dev-return-8744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 16:09:52 2014
Return-Path: <dev-return-8744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A97A117A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 16:09:52 +0000 (UTC)
Received: (qmail 50294 invoked by uid 500); 6 Aug 2014 16:09:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49860 invoked by uid 500); 6 Aug 2014 16:09:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49412 invoked by uid 99); 6 Aug 2014 16:09:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:09:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:09:46 +0000
Received: by mail-ig0-f177.google.com with SMTP id hn18so3089732igb.4
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 09:09:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=GzoPs3lT2+uSqxPUfghnbD4Me0b8pKdu1pbdfa6V/+I=;
        b=k5nB5dqCig7mstK5+trOBbbby6sPRrXp7jgrpqPkFfyYDzVUUOfuHk/ZWaF1g9HNyF
         nJLtOu8SbsvdUDTSFS2xxfWKvB/yvlhEJtiGSjXp0h1H3Fqfm9V35CX2N76nzh+nB0ji
         i3RN8by8sKtqdmHjGUTJJUyjlFdGB2KJ0Zv7YxAREdiP2/kLzIMz1GYqsAGlcJCDW8PE
         6smU5/O740q6D+Bi0Faz0rKhcIm9NBykeD5eYnCFjvjcodGiw+wjJTnpq9ilkWfppzw1
         mBsBTdPz4WV/mqfjPDLMXLL8kWgy0Cg6Ivy6bDC8khm+S9QugK+Bh22ThO2vNQ8fOwCq
         yaKg==
MIME-Version: 1.0
X-Received: by 10.42.207.68 with SMTP id fx4mr16195072icb.67.1407341365798;
 Wed, 06 Aug 2014 09:09:25 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Wed, 6 Aug 2014 09:09:25 -0700 (PDT)
In-Reply-To: <CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
Date: Wed, 6 Aug 2014 09:09:25 -0700
Message-ID: <CAJgQjQ_bAjknFejRfQ+DqOrpfWc6CCG0SkUkJ6M+RHm5V55Fkw@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

One thing I like to clarify is that we do not support running a newer
version of a Spark component on top of a older version of Spark core.
I don't remember any code change in MLlib that requires Spark v1.1 but
I might miss some PRs. There were changes to CoGroup, which may be
relevant:

https://github.com/apache/spark/commits/master/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala

Btw, for the constrained optimization, I'm really interested in how
they differ in the final recommendation? It would be great if you can
test prec@k or ndcg@k metrics.

Best,
Xiangrui

On Wed, Aug 6, 2014 at 8:28 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Xiangrui,
>
> Maintaining another file will be a pain later so I deployed spark 1.0.1
> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT along
> with the code changes for quadratic optimization...
>
> Later the plan is to patch the snapshot mllib with the deployed stable
> mllib...
>
> There are 5 variants that I am experimenting with around 400M ratings (daily
> data, monthly data I will update in few days)...
>
> 1. LS
> 2. NNLS
> 3. Quadratic with bounds
> 4. Quadratic with L1
> 5. Quadratic with equality and positivity
>
> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
> ALS.scala:311
>
> // Materialize usersOut and productsOut.
> usersOut.count()
>
> I am getting from one of the executors: java.lang.ClassCastException:
> scala.Tuple1 cannot be cast to scala.Product2
>
> I am debugging it further but I was wondering if this is due to RDD
> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>
> I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
> cluster has Java 1.7.0_45.
>
> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that Java
> version mismatch cause this ?
>
> Stack traces are below
>
> Thanks.
> Deb
>
>
> Executor stacktrace:
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>
>         scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>
>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>
>         java.lang.Thread.run(Thread.java:744)
>
> Driver stacktrace:
>
> at
> org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>
> at scala.Option.foreach(Option.scala:236)
>
> at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>
> at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>
> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>
> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
> at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
>
> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>>
>> Hi Xiangrui,
>>
>> I used your idea and kept a cherry picked version of ALS.scala in my
>> application and call it ALSQp.scala...this is a OK workaround for now till a
>> version adds up to master for example...
>>
>> For the bug with userClassPathFirst, looks like Koert already found this
>> issue in the following JIRA:
>>
>> https://issues.apache.org/jira/browse/SPARK-1863
>>
>> By the way the userClassPathFirst feature is very useful since I am sure
>> the deployed version of spark on a production cluster will always be the
>> last stable (core at 1.0.1 in my case) and people would like to deploy
>> SNAPSHOT versions of libraries that build on top of spark core (mllib,
>> streaming etc)...
>>
>> Another way is to have a build option that deploys only the core and not
>> the libraries built upon core...
>>
>> Do we have an option like that in make-distribution script ?
>>
>> Thanks.
>> Deb
>>
>>
>> On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>> If you cannot change the Spark jar deployed on the cluster, an easy
>>> solution would be renaming ALS in your jar. If userClassPathFirst
>>> doesn't work, could you create a JIRA and attach the log? Thanks!
>>> -Xiangrui
>>>
>>> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>> > I created the assembly file but still it wants to pick the mllib from
>>> > the
>>> > cluster:
>>> >
>>> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
>>> > QuadraticMinimizer
>>> >
>>> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>>> >
>>> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>>> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>>> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>>> > outputPath
>>> >
>>> > Exception in thread "main" java.lang.NoSuchMethodError:
>>> >
>>> > org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>>> >
>>> > Now if I force it to use the jar that I gave using
>>> > spark.files.userClassPathFirst, then it fails on some serialization
>>> > issues...
>>> >
>>> > A simple solution is to cherry pick the files I need from spark branch
>>> > to
>>> > the application branch but I am not sure that's the right thing to
>>> > do...
>>> >
>>> > The way userClassPathFirst is behaving, there might be bugs in it...
>>> >
>>> > Any suggestions will be appreciated....
>>> >
>>> > Thanks.
>>> > Deb
>>> >
>>> >
>>> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
>>> > wrote:
>>> >>
>>> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>>> >> spark-core-1.0.1.
>>> >>
>>> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das
>>> >> <debasish.das83@gmail.com>
>>> >> wrote:
>>> >> > Let me try it...
>>> >> >
>>> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
>>> >> > SNAPSHOT
>>> >> > jar and other dependencies with the rest of the application code ?
>>> >> >
>>> >> >
>>> >> >
>>> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
>>> >> > wrote:
>>> >> >>
>>> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
>>> >> >> sure whether it could solve your problem. -Xiangrui
>>> >> >>
>>> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>>> >> >> <debasish.das83@gmail.com>
>>> >> >> wrote:
>>> >> >> > Hi,
>>> >> >> >
>>> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have new
>>> >> >> > code
>>> >> >> > that
>>> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>>> >> >> >
>>> >> >> > I am trying to access the new code using spark-submit as follows:
>>> >> >> >
>>> >> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
>>> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>>> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>>> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda
>>> >> >> > 1.0
>>> >> >> > --qpProblem 2 inputPath outputPath
>>> >> >> >
>>> >> >> > I can see the jars are getting added to httpServer as expected:
>>> >> >> >
>>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> >
>>> >> >> > file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>>> >> >> >
>>> >> >> > http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>>> >> >> > with
>>> >> >> > timestamp 1406998204236
>>> >> >> >
>>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>>> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>>> >> >> > timestamp
>>> >> >> > 1406998204237
>>> >> >> >
>>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>>> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
>>> >> >> > timestamp
>>> >> >> > 1406998204238
>>> >> >> >
>>> >> >> > But the job still can't access code form mllib-1.1.0
>>> >> >> > SNAPSHOT.jar...I
>>> >> >> > think
>>> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
>>> >> >> >
>>> >> >> > Please help. I will ask for a PR tomorrow but internally we want
>>> >> >> > to
>>> >> >> > generate results from the new code.
>>> >> >> >
>>> >> >> > Thanks.
>>> >> >> >
>>> >> >> > Deb
>>> >> >
>>> >> >
>>> >
>>> >
>>
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 16:34:16 2014
Return-Path: <dev-return-8745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A000811890
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 16:34:16 +0000 (UTC)
Received: (qmail 25238 invoked by uid 500); 6 Aug 2014 16:34:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25179 invoked by uid 500); 6 Aug 2014 16:34:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25165 invoked by uid 99); 6 Aug 2014 16:34:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:34:15 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:34:11 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so2750931qaq.28
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 09:33:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pCNRwmbpah9V93UYWkmCSi9t+Uxn8aUge3uIf8OeQLI=;
        b=lHbvkl1oleQmH1WZtgJaBr0eRpmB4uSVo24agYoCju14CnTDi/M2+y9dIFcYPq0VvP
         H8PRAsX7i8Cxy2+Tg6TF3s+Tt73P3wljHUi+zac5h4XW5gmMW5yoCugrS1tEvFWsK3NC
         QMPK0nPmQI83bo2VU7ULsLIVT2SNrML1wNncm0SHF8U240Pk/gBI3MZhxjkoPqRC4OWd
         mhCKOFliazUX2LpzR8VYvKvr0Yi5BtbjT4M9qBncPSYpGUJRHZgMx6pDdaz0xHhsxO42
         R7kQaqYt7ByjoHPTqpf6ERinLyZj5sosnCkleOge7MBpsxH2AGc8pDqRKF88wWhROMs8
         7O+g==
MIME-Version: 1.0
X-Received: by 10.224.80.10 with SMTP id r10mr18981675qak.24.1407342830053;
 Wed, 06 Aug 2014 09:33:50 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 6 Aug 2014 09:33:49 -0700 (PDT)
In-Reply-To: <CAJgQjQ_bAjknFejRfQ+DqOrpfWc6CCG0SkUkJ6M+RHm5V55Fkw@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAJgQjQ_bAjknFejRfQ+DqOrpfWc6CCG0SkUkJ6M+RHm5V55Fkw@mail.gmail.com>
Date: Wed, 6 Aug 2014 09:33:49 -0700
Message-ID: <CA+B-+fyE0paF62xORz7PN11ebA_vu7J82WSY-=uXVFJGBPoUog@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2dba8768b8a04fff889ff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2dba8768b8a04fff889ff
Content-Type: text/plain; charset=UTF-8

Ok...let me look into it a bit more and most likely I will deploy the Spark
v1.1 and then use mllib 1.1 SNAPSHOT jar with it so that we follow your
guideline of not running newer spark component on older version of spark
core...

That should solve this issue unless it is related to Java versions....

I am also keen to see the final recommendation within L1 and
Positivity....I will compute the metrics

Our plan is to use scalable matrix factorization as an engine to do
clustering, feature extraction, topic modeling and auto encoders (single
layer to start with). So these algorithms are not really constrained to
recommendation use-cases...



On Wed, Aug 6, 2014 at 9:09 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> One thing I like to clarify is that we do not support running a newer
> version of a Spark component on top of a older version of Spark core.
> I don't remember any code change in MLlib that requires Spark v1.1 but
> I might miss some PRs. There were changes to CoGroup, which may be
> relevant:
>
>
> https://github.com/apache/spark/commits/master/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
>
> Btw, for the constrained optimization, I'm really interested in how
> they differ in the final recommendation? It would be great if you can
> test prec@k or ndcg@k metrics.
>
> Best,
> Xiangrui
>
> On Wed, Aug 6, 2014 at 8:28 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Xiangrui,
> >
> > Maintaining another file will be a pain later so I deployed spark 1.0.1
> > without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
> along
> > with the code changes for quadratic optimization...
> >
> > Later the plan is to patch the snapshot mllib with the deployed stable
> > mllib...
> >
> > There are 5 variants that I am experimenting with around 400M ratings
> (daily
> > data, monthly data I will update in few days)...
> >
> > 1. LS
> > 2. NNLS
> > 3. Quadratic with bounds
> > 4. Quadratic with L1
> > 5. Quadratic with equality and positivity
> >
> > Now the ALS 1.1.0 snapshot runs fine but after completion on this step
> > ALS.scala:311
> >
> > // Materialize usersOut and productsOut.
> > usersOut.count()
> >
> > I am getting from one of the executors: java.lang.ClassCastException:
> > scala.Tuple1 cannot be cast to scala.Product2
> >
> > I am debugging it further but I was wondering if this is due to RDD
> > compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
> >
> > I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
> > cluster has Java 1.7.0_45.
> >
> > The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that
> Java
> > version mismatch cause this ?
> >
> > Stack traces are below
> >
> > Thanks.
> > Deb
> >
> >
> > Executor stacktrace:
> >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
> >
> >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
> >
> >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >
> >
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >
> >         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> >
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
> >
> >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
> >
> >
> >
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
> >
> >
> >
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
> >
> >
> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
> >
> >
> >
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
> >
> >         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> >
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> >
> >         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >
> >         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >
> >
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
> >
> >
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
> >
> >         org.apache.spark.scheduler.Task.run(Task.scala:51)
> >
> >
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
> >
> >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >
> >         java.lang.Thread.run(Thread.java:744)
> >
> > Driver stacktrace:
> >
> > at
> > org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
> >
> > at
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >
> > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >
> > at scala.Option.foreach(Option.scala:236)
> >
> > at
> >
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
> >
> > at
> >
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
> >
> > at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >
> > at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >
> > at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >
> > at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >
> > at
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >
> > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >
> > at
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >
> > at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >
> > at
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >
> >
> >
> > On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >>
> >> Hi Xiangrui,
> >>
> >> I used your idea and kept a cherry picked version of ALS.scala in my
> >> application and call it ALSQp.scala...this is a OK workaround for now
> till a
> >> version adds up to master for example...
> >>
> >> For the bug with userClassPathFirst, looks like Koert already found this
> >> issue in the following JIRA:
> >>
> >> https://issues.apache.org/jira/browse/SPARK-1863
> >>
> >> By the way the userClassPathFirst feature is very useful since I am sure
> >> the deployed version of spark on a production cluster will always be the
> >> last stable (core at 1.0.1 in my case) and people would like to deploy
> >> SNAPSHOT versions of libraries that build on top of spark core (mllib,
> >> streaming etc)...
> >>
> >> Another way is to have a build option that deploys only the core and not
> >> the libraries built upon core...
> >>
> >> Do we have an option like that in make-distribution script ?
> >>
> >> Thanks.
> >> Deb
> >>
> >>
> >> On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>
> >>> If you cannot change the Spark jar deployed on the cluster, an easy
> >>> solution would be renaming ALS in your jar. If userClassPathFirst
> >>> doesn't work, could you create a JIRA and attach the log? Thanks!
> >>> -Xiangrui
> >>>
> >>> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com
> >
> >>> wrote:
> >>> > I created the assembly file but still it wants to pick the mllib from
> >>> > the
> >>> > cluster:
> >>> >
> >>> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
> >>> > QuadraticMinimizer
> >>> >
> >>> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
> >>> >
> >>> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
> >>> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
> >>> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
> >>> > outputPath
> >>> >
> >>> > Exception in thread "main" java.lang.NoSuchMethodError:
> >>> >
> >>> >
> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
> >>> >
> >>> > Now if I force it to use the jar that I gave using
> >>> > spark.files.userClassPathFirst, then it fails on some serialization
> >>> > issues...
> >>> >
> >>> > A simple solution is to cherry pick the files I need from spark
> branch
> >>> > to
> >>> > the application branch but I am not sure that's the right thing to
> >>> > do...
> >>> >
> >>> > The way userClassPathFirst is behaving, there might be bugs in it...
> >>> >
> >>> > Any suggestions will be appreciated....
> >>> >
> >>> > Thanks.
> >>> > Deb
> >>> >
> >>> >
> >>> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
> >>> > wrote:
> >>> >>
> >>> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
> >>> >> spark-core-1.0.1.
> >>> >>
> >>> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das
> >>> >> <debasish.das83@gmail.com>
> >>> >> wrote:
> >>> >> > Let me try it...
> >>> >> >
> >>> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
> >>> >> > SNAPSHOT
> >>> >> > jar and other dependencies with the rest of the application code ?
> >>> >> >
> >>> >> >
> >>> >> >
> >>> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
> >>> >> > wrote:
> >>> >> >>
> >>> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm
> not
> >>> >> >> sure whether it could solve your problem. -Xiangrui
> >>> >> >>
> >>> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
> >>> >> >> <debasish.das83@gmail.com>
> >>> >> >> wrote:
> >>> >> >> > Hi,
> >>> >> >> >
> >>> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have
> new
> >>> >> >> > code
> >>> >> >> > that
> >>> >> >> > I added in mllib-1.1.0-SNAPSHOT.
> >>> >> >> >
> >>> >> >> > I am trying to access the new code using spark-submit as
> follows:
> >>> >> >> >
> >>> >> >> > spark-job --class
> com.verizon.bda.mllib.recommendation.ALSDriver
> >>> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
> >>> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> >>> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
> --lambda
> >>> >> >> > 1.0
> >>> >> >> > --qpProblem 2 inputPath outputPath
> >>> >> >> >
> >>> >> >> > I can see the jars are getting added to httpServer as expected:
> >>> >> >> >
> >>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>> >> >> >
> >>> >> >> >
> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
> >>> >> >> >
> >>> >> >> >
> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> >>> >> >> > with
> >>> >> >> > timestamp 1406998204236
> >>> >> >> >
> >>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> >>> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
> >>> >> >> > timestamp
> >>> >> >> > 1406998204237
> >>> >> >> >
> >>> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> >>> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
> with
> >>> >> >> > timestamp
> >>> >> >> > 1406998204238
> >>> >> >> >
> >>> >> >> > But the job still can't access code form mllib-1.1.0
> >>> >> >> > SNAPSHOT.jar...I
> >>> >> >> > think
> >>> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
> >>> >> >> >
> >>> >> >> > Please help. I will ask for a PR tomorrow but internally we
> want
> >>> >> >> > to
> >>> >> >> > generate results from the new code.
> >>> >> >> >
> >>> >> >> > Thanks.
> >>> >> >> >
> >>> >> >> > Deb
> >>> >> >
> >>> >> >
> >>> >
> >>> >
> >>
> >>
> >
>

--001a11c2dba8768b8a04fff889ff--

From dev-return-8746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 16:46:02 2014
Return-Path: <dev-return-8746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7BA2E118F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 16:46:02 +0000 (UTC)
Received: (qmail 66403 invoked by uid 500); 6 Aug 2014 16:46:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66345 invoked by uid 500); 6 Aug 2014 16:46:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66333 invoked by uid 99); 6 Aug 2014 16:46:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:46:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 16:45:58 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so3026804qge.11
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 09:45:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=zRcVA7MedT1dDeXBvXBUhByJdyjAOVziPjTTLSiUovw=;
        b=iL/IGZYVmRY/B/SCNaclLglKM1nAESUGz+SQYJy7nCvmrjRiRai/OcM4ctZskbeLIP
         zwdlomobAnuVCCI8JMXS2CJe8T4tZl0XOXTjmu4E9/oz+V75rOceGuKiSHRBvGsi0gK3
         RG3nPKRsVAs6MpTtIU7ir1YVXijU9KHS+9tVYrRELQjJSskz0DVjMh2pq3H/njs422j4
         612ufmSu6PzOr0gKpfZrcaEBj2o+2ncs5xX4BgYFAU/XcqlwkA/Y8qtVQp2RZ1PeATzt
         j8syQ6ah3vSIjG5ktqLfxcODKj1jsVT6eIBScIHU/6k22H4fIn1CLBOuPizLCbEX+rwd
         dJ4Q==
X-Gm-Message-State: ALoCoQlO99O2A08QqcgCA0Qv4kYtRcKUelmdC0DuA2mpiWkID/RhxqjWUl9x2AGK99sm3YgqBWYF
MIME-Version: 1.0
X-Received: by 10.224.136.200 with SMTP id s8mr18811039qat.44.1407343526605;
 Wed, 06 Aug 2014 09:45:26 -0700 (PDT)
Received: by 10.229.183.130 with HTTP; Wed, 6 Aug 2014 09:45:26 -0700 (PDT)
Received: by 10.229.183.130 with HTTP; Wed, 6 Aug 2014 09:45:26 -0700 (PDT)
In-Reply-To: <CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
Date: Wed, 6 Aug 2014 09:45:26 -0700
Message-ID: <CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: DB Tsai <dbtsai@dbtsai.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2caa2fb27c704fff8b210
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2caa2fb27c704fff8b210
Content-Type: text/plain; charset=UTF-8

One related question, is mllib jar independent from hadoop version (doesnt
use hadoop api directly)? Can I use mllib jar compile for one version of
hadoop and use it in another version of hadoop?

Sent from my Google Nexus 5
On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com> wrote:

> Hi Xiangrui,
>
> Maintaining another file will be a pain later so I deployed spark 1.0.1
> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
> along with the code changes for quadratic optimization...
>
> Later the plan is to patch the snapshot mllib with the deployed stable
> mllib...
>
> There are 5 variants that I am experimenting with around 400M ratings
> (daily data, monthly data I will update in few days)...
>
> 1. LS
> 2. NNLS
> 3. Quadratic with bounds
> 4. Quadratic with L1
> 5. Quadratic with equality and positivity
>
> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
> ALS.scala:311
>
> // Materialize usersOut and productsOut.
> usersOut.count()
>
> I am getting from one of the executors: java.lang.ClassCastException:
> scala.Tuple1 cannot be cast to scala.Product2
>
> I am debugging it further but I was wondering if this is due to RDD
> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>
> I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
> cluster has Java 1.7.0_45.
>
> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that Java
> version mismatch cause this ?
>
> Stack traces are below
>
> Thanks.
> Deb
>
>
> Executor stacktrace:
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>
>
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
>
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>
>         scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>
>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>
>
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>
>
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>
>         java.lang.Thread.run(Thread.java:744)
>
> Driver stacktrace:
>
> at org.apache.spark.scheduler.DAGScheduler.org
>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>
> at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>
> at scala.Option.foreach(Option.scala:236)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>
> at
>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>
> at
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>
> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
> at
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>
> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
>  at
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Hi Xiangrui,
> >
> > I used your idea and kept a cherry picked version of ALS.scala in my
> > application and call it ALSQp.scala...this is a OK workaround for now
> till
> > a version adds up to master for example...
> >
> > For the bug with userClassPathFirst, looks like Koert already found this
> > issue in the following JIRA:
> >
> > https://issues.apache.org/jira/browse/SPARK-1863
> >
> > By the way the userClassPathFirst feature is very useful since I am sure
> > the deployed version of spark on a production cluster will always be the
> > last stable (core at 1.0.1 in my case) and people would like to deploy
> > SNAPSHOT versions of libraries that build on top of spark core (mllib,
> > streaming etc)...
> >
> > Another way is to have a build option that deploys only the core and not
> > the libraries built upon core...
> >
> > Do we have an option like that in make-distribution script ?
> >
> > Thanks.
> > Deb
> >
> >
> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> If you cannot change the Spark jar deployed on the cluster, an easy
> >> solution would be renaming ALS in your jar. If userClassPathFirst
> >> doesn't work, could you create a JIRA and attach the log? Thanks!
> >> -Xiangrui
> >>
> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com>
> >> wrote:
> >> > I created the assembly file but still it wants to pick the mllib from
> >> the
> >> > cluster:
> >> >
> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
> >> > QuadraticMinimizer
> >> >
> >> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
> >> >
> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
> >> outputPath
> >> >
> >> > Exception in thread "main" java.lang.NoSuchMethodError:
> >> >
> >>
> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
> >> >
> >> > Now if I force it to use the jar that I gave using
> >> > spark.files.userClassPathFirst, then it fails on some serialization
> >> > issues...
> >> >
> >> > A simple solution is to cherry pick the files I need from spark branch
> >> to
> >> > the application branch but I am not sure that's the right thing to
> do...
> >> >
> >> > The way userClassPathFirst is behaving, there might be bugs in it...
> >> >
> >> > Any suggestions will be appreciated....
> >> >
> >> > Thanks.
> >> > Deb
> >> >
> >> >
> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>
> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
> >> >> spark-core-1.0.1.
> >> >>
> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
> >> debasish.das83@gmail.com>
> >> >> wrote:
> >> >> > Let me try it...
> >> >> >
> >> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
> >> >> > SNAPSHOT
> >> >> > jar and other dependencies with the rest of the application code ?
> >> >> >
> >> >> >
> >> >> >
> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >> >>
> >> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm not
> >> >> >> sure whether it could solve your problem. -Xiangrui
> >> >> >>
> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
> >> >> >> <debasish.das83@gmail.com>
> >> >> >> wrote:
> >> >> >> > Hi,
> >> >> >> >
> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have new
> >> code
> >> >> >> > that
> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
> >> >> >> >
> >> >> >> > I am trying to access the new code using spark-submit as
> follows:
> >> >> >> >
> >> >> >> > spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
> --lambda
> >> 1.0
> >> >> >> > --qpProblem 2 inputPath outputPath
> >> >> >> >
> >> >> >> > I can see the jars are getting added to httpServer as expected:
> >> >> >> >
> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> >> >
> >> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
> >> >> >> >
> >> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> >> >> >> > with
> >> >> >> > timestamp 1406998204236
> >> >> >> >
> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
> >> timestamp
> >> >> >> > 1406998204237
> >> >> >> >
> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
> >> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with
> >> >> >> > timestamp
> >> >> >> > 1406998204238
> >> >> >> >
> >> >> >> > But the job still can't access code form mllib-1.1.0
> >> SNAPSHOT.jar...I
> >> >> >> > think
> >> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
> >> >> >> >
> >> >> >> > Please help. I will ask for a PR tomorrow but internally we want
> >> to
> >> >> >> > generate results from the new code.
> >> >> >> >
> >> >> >> > Thanks.
> >> >> >> >
> >> >> >> > Deb
> >> >> >
> >> >> >
> >> >
> >> >
> >>
> >
> >
>

--001a11c2caa2fb27c704fff8b210--

From dev-return-8747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 19:02:18 2014
Return-Path: <dev-return-8747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6BD1711F81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 19:02:18 +0000 (UTC)
Received: (qmail 61782 invoked by uid 500); 6 Aug 2014 19:02:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61729 invoked by uid 500); 6 Aug 2014 19:02:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61715 invoked by uid 99); 6 Aug 2014 19:02:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 19:02:17 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 19:02:12 +0000
Received: by mail-qg0-f53.google.com with SMTP id q107so3138859qgd.40
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 12:01:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DZHuSw02jnieyJXO6xYCyoXJFC/2IPY5j0lzY87/L5w=;
        b=IIb/n5E/vN137NH0BgHxRWFbLE6e05YyUilsAjSfKWqjtd5LZG7zLjQKSDOcmR3i/f
         RKh54qxBiYFsBV0p0yB8W9m30Hs7viMi5epLeB4WBqMntx0Vumb1SzXoU+J//m8PLQ5l
         u7VyP2WI8vH5AXAzzg8bmyCwDWtf/K5+gT7LCmm3zQr3bobiBf00+ZcZCqQ139+ppgSB
         0IP/lyZQXsjP+QRxmRHr/5UU+NLtmqaBw7yr6sqXONvVBGnUzr8riwcpeMbwVmfH3Xzd
         lJJfwWAee9Y3epSvZ38k2qP8/8s7KZNtt1DcY81VlbBQKrQb1qZQsF/UnZ2dk6Pq2yEZ
         UUtQ==
MIME-Version: 1.0
X-Received: by 10.229.38.3 with SMTP id z3mr19727023qcd.17.1407351711902; Wed,
 06 Aug 2014 12:01:51 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 6 Aug 2014 12:01:51 -0700 (PDT)
In-Reply-To: <CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
Date: Wed, 6 Aug 2014 12:01:51 -0700
Message-ID: <CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c29a4adcd2f104fffa9a27
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29a4adcd2f104fffa9a27
Content-Type: text/plain; charset=UTF-8

I did not play with Hadoop settings...everything is compiled with
2.3.0CDH5.0.2 for me...

I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98 but
there was no profile for CDH in the pom...but that's unrelated to this !


On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:

> One related question, is mllib jar independent from hadoop version (doesnt
> use hadoop api directly)? Can I use mllib jar compile for one version of
> hadoop and use it in another version of hadoop?
>
> Sent from my Google Nexus 5
> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com> wrote:
>
>> Hi Xiangrui,
>>
>> Maintaining another file will be a pain later so I deployed spark 1.0.1
>> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
>> along with the code changes for quadratic optimization...
>>
>> Later the plan is to patch the snapshot mllib with the deployed stable
>> mllib...
>>
>> There are 5 variants that I am experimenting with around 400M ratings
>> (daily data, monthly data I will update in few days)...
>>
>> 1. LS
>> 2. NNLS
>> 3. Quadratic with bounds
>> 4. Quadratic with L1
>> 5. Quadratic with equality and positivity
>>
>> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
>> ALS.scala:311
>>
>> // Materialize usersOut and productsOut.
>> usersOut.count()
>>
>> I am getting from one of the executors: java.lang.ClassCastException:
>> scala.Tuple1 cannot be cast to scala.Product2
>>
>> I am debugging it further but I was wondering if this is due to RDD
>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>>
>> I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
>> cluster has Java 1.7.0_45.
>>
>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that
>> Java
>> version mismatch cause this ?
>>
>> Stack traces are below
>>
>> Thanks.
>> Deb
>>
>>
>> Executor stacktrace:
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>>
>>
>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>
>>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>
>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>
>>
>>
>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>
>>
>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>
>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>
>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>
>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>>
>>
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>
>>
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>
>>         java.lang.Thread.run(Thread.java:744)
>>
>> Driver stacktrace:
>>
>> at org.apache.spark.scheduler.DAGScheduler.org
>>
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>
>> at
>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>
>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>
>> at scala.Option.foreach(Option.scala:236)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>
>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>
>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>
>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>
>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>
>> at
>>
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>
>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>
>> at
>>
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>
>> at
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>
>>  at
>>
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>
>>
>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>> > Hi Xiangrui,
>> >
>> > I used your idea and kept a cherry picked version of ALS.scala in my
>> > application and call it ALSQp.scala...this is a OK workaround for now
>> till
>> > a version adds up to master for example...
>> >
>> > For the bug with userClassPathFirst, looks like Koert already found this
>> > issue in the following JIRA:
>> >
>> > https://issues.apache.org/jira/browse/SPARK-1863
>> >
>> > By the way the userClassPathFirst feature is very useful since I am sure
>> > the deployed version of spark on a production cluster will always be the
>> > last stable (core at 1.0.1 in my case) and people would like to deploy
>> > SNAPSHOT versions of libraries that build on top of spark core (mllib,
>> > streaming etc)...
>> >
>> > Another way is to have a build option that deploys only the core and not
>> > the libraries built upon core...
>> >
>> > Do we have an option like that in make-distribution script ?
>> >
>> > Thanks.
>> > Deb
>> >
>> >
>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >
>> >> If you cannot change the Spark jar deployed on the cluster, an easy
>> >> solution would be renaming ALS in your jar. If userClassPathFirst
>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
>> >> -Xiangrui
>> >>
>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <debasish.das83@gmail.com
>> >
>> >> wrote:
>> >> > I created the assembly file but still it wants to pick the mllib from
>> >> the
>> >> > cluster:
>> >> >
>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
>> >> > QuadraticMinimizer
>> >> >
>> >> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>> >> >
>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>> >> outputPath
>> >> >
>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>> >> >
>> >>
>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>> >> >
>> >> > Now if I force it to use the jar that I gave using
>> >> > spark.files.userClassPathFirst, then it fails on some serialization
>> >> > issues...
>> >> >
>> >> > A simple solution is to cherry pick the files I need from spark
>> branch
>> >> to
>> >> > the application branch but I am not sure that's the right thing to
>> do...
>> >> >
>> >> > The way userClassPathFirst is behaving, there might be bugs in it...
>> >> >
>> >> > Any suggestions will be appreciated....
>> >> >
>> >> > Thanks.
>> >> > Deb
>> >> >
>> >> >
>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
>> >> wrote:
>> >> >>
>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>> >> >> spark-core-1.0.1.
>> >> >>
>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>> >> debasish.das83@gmail.com>
>> >> >> wrote:
>> >> >> > Let me try it...
>> >> >> >
>> >> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
>> >> >> > SNAPSHOT
>> >> >> > jar and other dependencies with the rest of the application code ?
>> >> >> >
>> >> >> >
>> >> >> >
>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com>
>> >> wrote:
>> >> >> >>
>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm
>> not
>> >> >> >> sure whether it could solve your problem. -Xiangrui
>> >> >> >>
>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>> >> >> >> <debasish.das83@gmail.com>
>> >> >> >> wrote:
>> >> >> >> > Hi,
>> >> >> >> >
>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have
>> new
>> >> code
>> >> >> >> > that
>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>> >> >> >> >
>> >> >> >> > I am trying to access the new code using spark-submit as
>> follows:
>> >> >> >> >
>> >> >> >> > spark-job --class
>> com.verizon.bda.mllib.recommendation.ALSDriver
>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>> --lambda
>> >> 1.0
>> >> >> >> > --qpProblem 2 inputPath outputPath
>> >> >> >> >
>> >> >> >> > I can see the jars are getting added to httpServer as expected:
>> >> >> >> >
>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> >> >
>> >> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>> >> >> >> >
>> >> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>> >> >> >> > with
>> >> >> >> > timestamp 1406998204236
>> >> >> >> >
>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>> >> timestamp
>> >> >> >> > 1406998204237
>> >> >> >> >
>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>> >> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>> with
>> >> >> >> > timestamp
>> >> >> >> > 1406998204238
>> >> >> >> >
>> >> >> >> > But the job still can't access code form mllib-1.1.0
>> >> SNAPSHOT.jar...I
>> >> >> >> > think
>> >> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
>> >> >> >> >
>> >> >> >> > Please help. I will ask for a PR tomorrow but internally we
>> want
>> >> to
>> >> >> >> > generate results from the new code.
>> >> >> >> >
>> >> >> >> > Thanks.
>> >> >> >> >
>> >> >> >> > Deb
>> >> >> >
>> >> >> >
>> >> >
>> >> >
>> >>
>> >
>> >
>>
>

--001a11c29a4adcd2f104fffa9a27--

From dev-return-8748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 19:29:51 2014
Return-Path: <dev-return-8748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A5E51109E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 19:29:51 +0000 (UTC)
Received: (qmail 53962 invoked by uid 500); 6 Aug 2014 19:29:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53905 invoked by uid 500); 6 Aug 2014 19:29:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53892 invoked by uid 99); 6 Aug 2014 19:29:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 19:29:50 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.162 as permitted sender)
Received: from [98.139.213.162] (HELO nm19-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.162)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 19:29:42 +0000
Received: from [98.139.215.141] by nm19.bullet.mail.bf1.yahoo.com with NNFMP; 06 Aug 2014 19:29:21 -0000
Received: from [98.139.212.199] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 06 Aug 2014 19:29:21 -0000
Received: from [127.0.0.1] by omp1008.mail.bf1.yahoo.com with NNFMP; 06 Aug 2014 19:29:21 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 198317.90368.bm@omp1008.mail.bf1.yahoo.com
Received: (qmail 85468 invoked by uid 60001); 6 Aug 2014 19:29:21 -0000
X-YMail-OSG: 32VZY6YVM1mEQn5FepgsssIRaZUPTG9JblpXzMekd9hr49P
 KAfIFcxhWv2_BGhQTpf_EHBYQ2WjEeHsi.gsHMNgT0RKmyK0Yy01klDl_ivs
 OgWQuN2Y49bTGXtEH6GIdxwTSZNj5q4ntlX4miS9WsQUrF05u1jzSS6FhKTt
 8.oy4.w_1aAP9wOVatLTGQH4b8UGShljIhciPm4NMSTlQ87f2JOpkO10uYwk
 BxdgLIZaD.jDxReP1k4SOo59C4cyIFt4AgyUt9tyTtde6diMJBVwUmmj43bG
 1pLI6O_wVnN37pApBjpJMj1P7joWku1S9aXjrPg9WIMEqvbnYWRKCVU9U1fj
 tWQV48abivxeu.oYknB5lGTj3hp73vo4ZqSVQ1fwbAXeVxoz..vKmJNN2TTP
 1Hd4LakNsFKYyL2LpRo.EHJ.CN9k0IaDHqJoNdu_dIkQNvpCgyDHIOb70zhk
 qyjceC3ba_byI6YcTGlg3UuWWGP649WanwBoum53sC_PK3SurMthl3XYE.KN
 MOKWZ1DXl9Fm1WVI2fQwxcNhoyy2CFQVfhHlHkm5FDQcwarRM_rxaHpQ-
Received: from [12.130.144.124] by web162401.mail.bf1.yahoo.com via HTTP; Wed, 06 Aug 2014 12:29:21 PDT
X-Rocket-MIMEInfo: 002.001,SGksCsKgIEknbSB0cnlpbmcgdG8gZ2V0IHRoZSBhcGFjaGUgc3BhcmsgdHJ1bmsgY29tcGlsaW5nIGluIG15IEVjbGlwc2UsIGJ1dCBJIGNhbid0IHNlZW0gdG8gZ2V0IGl0IGdvaW5nLiBJbiBwYXJ0aWN1bGFyLCBJJ3ZlIHRyaWVkIHNidC9zYnQgZWNsaXBzZSwgYnV0IGl0IGRvZXNuJ3Qgc2VlbSB0byBjcmVhdGUgdGhlIGVjbGlwc2UgcGllY2VzIGZvciB5YXJuIGFuZCBvdGhlciBwcm9qZWN0cy4gRG9pbmcgbXZuIGVjbGlwc2U6ZWNsaXBzZSBvbiB5YXJuIHNlZW1zIHRvIGZhaWwgYXMgd2VsbCBhcyBzYnQBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
Message-ID: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
Date: Wed, 6 Aug 2014 12:29:21 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: Buidling spark in Eclipse Kepler
To: Dev <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-1911863903-1058680285-1407353361=:9795"
X-Virus-Checked: Checked by ClamAV on apache.org

---1911863903-1058680285-1407353361=:9795
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi,=0A=A0 I'm trying to get the apache spark trunk compiling in my Eclipse,=
 but I can't seem to get it going. In particular, I've tried sbt/sbt eclips=
e, but it doesn't seem to create the eclipse pieces for yarn and other proj=
ects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt ec=
lipse just for yarn fails. Is there some documentation available for eclips=
e? I've gone through the ones on the site, but to no avail.=0A=A0 Any tips?=
=0A=0AThanks,=0ARon
---1911863903-1058680285-1407353361=:9795--

From dev-return-8749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 20:17:57 2014
Return-Path: <dev-return-8749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E830111306
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 20:17:56 +0000 (UTC)
Received: (qmail 16519 invoked by uid 500); 6 Aug 2014 20:17:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16467 invoked by uid 500); 6 Aug 2014 20:17:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16450 invoked by uid 99); 6 Aug 2014 20:17:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:17:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.174 as permitted sender)
Received: from [209.85.160.174] (HELO mail-yk0-f174.google.com) (209.85.160.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:17:52 +0000
Received: by mail-yk0-f174.google.com with SMTP id q9so2084058ykb.19
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 13:17:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=lzQPIg3mYYjZ1c1bYuaX2aXSh/9Fv3pzqHy3jZcfg30=;
        b=Yq+iiYQQfslEVGdS/YRbXUDtoa+sSfqFfk0xL1+U9KQ4AyNIOuMjArEYQMlI+tBDsj
         SYqDk+DydtAU/9vYbtcZYYNdKdyVNh3O9NXgDZZCklmqIqvvlsOYDAUkwdgfyFAVGRfS
         BQ/oNd98EUjOxVbRB9j20Qz29Tg+oRk2FBICEqaKjK2Wn2ocGLQLsS7bNwxeWLvPhp1Q
         HDX0i2P+I4Bama0y1pNdy0f5CBb24YJGKwqCY9RnMB2ChnWMKKHq564qHyNvTQrIQxF1
         As+nrE1tszTFxsDU2kBdCECn43CZc2/2V34bGlcvdXpsNm7qTodWmwl1mNqEFOnSlYB/
         dlSA==
MIME-Version: 1.0
X-Received: by 10.236.228.161 with SMTP id f31mr20178902yhq.44.1407356247651;
 Wed, 06 Aug 2014 13:17:27 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Wed, 6 Aug 2014 13:17:27 -0700 (PDT)
Date: Wed, 6 Aug 2014 13:17:27 -0700
Message-ID: <CALte62wH1wxPQNm-zwhh1RMimGZJ7S_eeW6VgKgs9CW9DDgY0w@mail.gmail.com>
Subject: compilation error in Catalyst module
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133352636b98f04fffba95e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133352636b98f04fffba95e
Content-Type: text/plain; charset=UTF-8

I refreshed my workspace.
I got the following error with this command:

mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install

[ERROR] bad symbolic reference. A signature in package.class refers to term
scalalogging
in package com.typesafe which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
package.class.
[ERROR]
/homes/hortonzy/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/package.scala:36:
bad symbolic reference. A signature in package.class refers to term slf4j
in value com.typesafe.scalalogging which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
package.class.
[ERROR] package object trees extends Logging {
[ERROR]                              ^
[ERROR] two errors found

Has anyone else seen the above ?

Thanks

--001a1133352636b98f04fffba95e--

From dev-return-8750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 20:37:12 2014
Return-Path: <dev-return-8750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F184113B3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 20:37:12 +0000 (UTC)
Received: (qmail 77467 invoked by uid 500); 6 Aug 2014 20:37:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77410 invoked by uid 500); 6 Aug 2014 20:37:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77397 invoked by uid 99); 6 Aug 2014 20:37:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:37:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of concretevitamin@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:37:08 +0000
Received: by mail-wi0-f171.google.com with SMTP id hi2so9482926wib.4
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 13:36:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=6CS9AtRxnSW7jUoGWaSW3JXbBLbXLQhgYbuYY/WMxbI=;
        b=rRr6te55JnOqbec6BeR47Rmb2bvN8/eP4HLZZl7+fwKzpIDflBD0cePQSSWigcj+/J
         vf1/veHnxH6z72LfpedZy0r1GsxnniZmaf+pcwU/dgsZHzB7EajC4OO9sYOfpETCSTXX
         yoo/wEahNBiCv/4/JMG0nZ28SO9KPt1QDpaMehCbm1hE/gnCkYkuaKeYtwSkvE933yG7
         MwbW2qZ1WXofDJX5HMArPO6xBGQuxKlj0DimCRQe/eK1xtKSn03LppOlb9d26DwWlob/
         mlvqNlnbuEJEW0mQrrPOQaJjaL80dLymQggbFMJWFhgwJppozJS82nSKejQrBI/Z8OKh
         J0kg==
MIME-Version: 1.0
X-Received: by 10.194.184.230 with SMTP id ex6mr18474362wjc.83.1407357404396;
 Wed, 06 Aug 2014 13:36:44 -0700 (PDT)
Sender: concretevitamin@gmail.com
Received: by 10.216.62.129 with HTTP; Wed, 6 Aug 2014 13:36:44 -0700 (PDT)
In-Reply-To: <CALte62wH1wxPQNm-zwhh1RMimGZJ7S_eeW6VgKgs9CW9DDgY0w@mail.gmail.com>
References: <CALte62wH1wxPQNm-zwhh1RMimGZJ7S_eeW6VgKgs9CW9DDgY0w@mail.gmail.com>
Date: Wed, 6 Aug 2014 13:36:44 -0700
X-Google-Sender-Auth: ZxfFQPDXqD1S3sxeNpOWdhePIKk
Message-ID: <CAG2+eogpXfb-OYve97OUBt5_ec1OAB77o8RRv73Fv4ZjfzauYw@mail.gmail.com>
Subject: Re: compilation error in Catalyst module
From: Zongheng Yang <zongheng.y@gmail.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Ted,

By refreshing do you mean you have done 'mvn clean'?

On Wed, Aug 6, 2014 at 1:17 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> I refreshed my workspace.
> I got the following error with this command:
>
> mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install
>
> [ERROR] bad symbolic reference. A signature in package.class refers to term
> scalalogging
> in package com.typesafe which is not available.
> It may be completely missing from the current classpath, or the version on
> the classpath might be incompatible with the version used when compiling
> package.class.
> [ERROR]
> /homes/hortonzy/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/package.scala:36:
> bad symbolic reference. A signature in package.class refers to term slf4j
> in value com.typesafe.scalalogging which is not available.
> It may be completely missing from the current classpath, or the version on
> the classpath might be incompatible with the version used when compiling
> package.class.
> [ERROR] package object trees extends Logging {
> [ERROR]                              ^
> [ERROR] two errors found
>
> Has anyone else seen the above ?
>
> Thanks

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 20:43:47 2014
Return-Path: <dev-return-8751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4ABA113DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 20:43:47 +0000 (UTC)
Received: (qmail 96671 invoked by uid 500); 6 Aug 2014 20:43:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96632 invoked by uid 500); 6 Aug 2014 20:43:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96621 invoked by uid 99); 6 Aug 2014 20:43:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:43:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:43:41 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so4844596vcb.8
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 13:43:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=6rCUFS4kHsYkbUOIcjkSN9Wk+CTTXk6Y8Avh6wMtJ24=;
        b=G7yjuHiYsgog/AjV0J4UlnuUS50Fc5AzycnoVSbdzP2sZCPCejqtPu8dvbPvDkv8Qc
         c1ZYxu4ma1xg92U6whML+rffpazsgsHQDrurrYRyz1Wabm5ew963lIKQQsAGEzBl4/Rc
         U/miCrXAszuK7qarChmaaH2T8LKo5kVNcd6oQ2A+jVPQcwuRpPPZYAto/rBi7Kqk4pUt
         HFRV3SIzPKi6lNrI3oLHQ5VbrmdGl1i9sb1jfh2uom0u0FhLRiEpNLm2LOGwuONwmBFo
         9WXexl4GnMC1/x6cR46piQe8vWjyVBHchF34KzZzTXb9sVebK3dli8yNTH0BM0z8fSjJ
         q1dw==
X-Gm-Message-State: ALoCoQli2SmtZBB7137Bf7FcgPhXauNMcSgj8gHFS5vTDEUgYRgPu9iiewMPPUi61MSVkT7Apnjy
X-Received: by 10.220.105.201 with SMTP id u9mr12761159vco.11.1407357800485;
 Wed, 06 Aug 2014 13:43:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Wed, 6 Aug 2014 13:43:00 -0700 (PDT)
In-Reply-To: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 6 Aug 2014 21:43:00 +0100
Message-ID: <CAMAsSdJfOHRVN11Au2u-QdBobd=m9OO4d0+ZEJk75CORPDZUQw@mail.gmail.com>
Subject: Re: Buidling spark in Eclipse Kepler
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I think your best bet by far is to consume the Maven build as-is from
within Eclipse. I wouldn't try to export a project config from the
build as there is plenty to get lost in translation.

Certainly this works well with IntelliJ, and by the by, if you have a
choice, I would strongly recommend IntelliJ over Eclipse for working
with Maven and Scala.

On Wed, Aug 6, 2014 at 8:29 PM, Ron Gonzalez
<zlgonzalez@yahoo.com.invalid> wrote:
> Hi,
>   I'm trying to get the apache spark trunk compiling in my Eclipse, but I=
 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but=
 it doesn't seem to create the eclipse pieces for yarn and other projects. =
Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse =
just for yarn fails. Is there some documentation available for eclipse? I'v=
e gone through the ones on the site, but to no avail.
>   Any tips?
>
> Thanks,
> Ron

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 20:54:31 2014
Return-Path: <dev-return-8752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A511011446
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 20:54:31 +0000 (UTC)
Received: (qmail 27882 invoked by uid 500); 6 Aug 2014 20:54:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27833 invoked by uid 500); 6 Aug 2014 20:54:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27817 invoked by uid 99); 6 Aug 2014 20:54:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:54:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.171 as permitted sender)
Received: from [209.85.160.171] (HELO mail-yk0-f171.google.com) (209.85.160.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 20:54:25 +0000
Received: by mail-yk0-f171.google.com with SMTP id 19so2102680ykq.16
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 13:54:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=TDb0gE92l56mWOaePrrQ7G+TK+fQL/yd8ihTZK5j1Dg=;
        b=aJgN4lUuv3/uQaQFfe5QE5bGSaZMvGd7UauQlroQNm4W5pgVeWN0MrfBbJTc5gVIRv
         RWV23kpDDS6T5BORCHoQB6DyqHQQgbtUWo5R3dPJXWmGETAaE0E8w74DS9FY4a6k7CcF
         Qkd3UdwDc1r2JO522Nh4I22SQ0Gfx/DzetNMJudfP2td11OzNmLs7DllYPg2mDzdzjTY
         mCZ4jT7/c2uRh8m9/FvAXF6BEqxq+u8ryBizM5OusPbG2vvMGoCiy9djXfBDXXZiMJxn
         ras0A356AjYhqSnwK+kph5qzJ+mlOWBdgcmRsEkVbpigKnrNbMlcUr8DApzOyadbTv9T
         JQpw==
MIME-Version: 1.0
X-Received: by 10.236.145.4 with SMTP id o4mr8838441yhj.188.1407358444531;
 Wed, 06 Aug 2014 13:54:04 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Wed, 6 Aug 2014 13:54:04 -0700 (PDT)
In-Reply-To: <CAG2+eogpXfb-OYve97OUBt5_ec1OAB77o8RRv73Fv4ZjfzauYw@mail.gmail.com>
References: <CALte62wH1wxPQNm-zwhh1RMimGZJ7S_eeW6VgKgs9CW9DDgY0w@mail.gmail.com>
	<CAG2+eogpXfb-OYve97OUBt5_ec1OAB77o8RRv73Fv4ZjfzauYw@mail.gmail.com>
Date: Wed, 6 Aug 2014 13:54:04 -0700
Message-ID: <CALte62yt-pT3sGRpyuWCVdoOyU6z6y_QZxkmfZ_wJJCq0AjZBQ@mail.gmail.com>
Subject: Re: compilation error in Catalyst module
From: Ted Yu <yuzhihong@gmail.com>
To: Zongheng Yang <zongheng.y@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30426fc228798f04fffc2cc5
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30426fc228798f04fffc2cc5
Content-Type: text/plain; charset=UTF-8

Forgot to do that step.

Now compilation passes.


On Wed, Aug 6, 2014 at 1:36 PM, Zongheng Yang <zongheng.y@gmail.com> wrote:

> Hi Ted,
>
> By refreshing do you mean you have done 'mvn clean'?
>
> On Wed, Aug 6, 2014 at 1:17 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> > I refreshed my workspace.
> > I got the following error with this command:
> >
> > mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install
> >
> > [ERROR] bad symbolic reference. A signature in package.class refers to
> term
> > scalalogging
> > in package com.typesafe which is not available.
> > It may be completely missing from the current classpath, or the version
> on
> > the classpath might be incompatible with the version used when compiling
> > package.class.
> > [ERROR]
> >
> /homes/hortonzy/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/package.scala:36:
> > bad symbolic reference. A signature in package.class refers to term slf4j
> > in value com.typesafe.scalalogging which is not available.
> > It may be completely missing from the current classpath, or the version
> on
> > the classpath might be incompatible with the version used when compiling
> > package.class.
> > [ERROR] package object trees extends Logging {
> > [ERROR]                              ^
> > [ERROR] two errors found
> >
> > Has anyone else seen the above ?
> >
> > Thanks
>

--20cf30426fc228798f04fffc2cc5--

From dev-return-8753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug  6 23:17:23 2014
Return-Path: <dev-return-8753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3297411A69
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Aug 2014 23:17:23 +0000 (UTC)
Received: (qmail 68175 invoked by uid 500); 6 Aug 2014 23:17:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68114 invoked by uid 500); 6 Aug 2014 23:17:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68095 invoked by uid 99); 6 Aug 2014 23:17:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 23:17:22 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Aug 2014 23:17:20 +0000
Received: by mail-ig0-f177.google.com with SMTP id hn18so3654734igb.4
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 16:16:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=gQGgpj3RQPxojVw9d8BOCXNS0jYQyJzYRajY4A1pJ/s=;
        b=MBMkuQZ7k11hDvGPmnqNm7Z9+eNQJau50tuk1oDTT+IYzz2Mw2LOSX0Zj1Skqn5xgn
         GgR02r5NDJu2rryChlBRt3AVd8ylvfD3PyBu49tZFOKTW0Hvc9L0KS9xn4B1oQ+GN2T7
         nr5+YeO+1sBnL4a2frZjUljSoT5qFRBJIHOVuGt1VjB0Tlyox0PomL2PQTjpMLCEH/v3
         wBnRxuozlXxQSIFY3q1rt0e4lJjRoreX8MhuD4HyeeLsEtaFaUl7c4s687QGL2SBk5+c
         AeD4SkTIQtRV1BnZ3eErNJ1L0vfcVaZ9Azj3moPOBm1XbziaVkJFgxFpBcGfc14Iq7jz
         WC6g==
X-Received: by 10.50.32.10 with SMTP id e10mr23270303igi.7.1407367014720; Wed,
 06 Aug 2014 16:16:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.129.10 with HTTP; Wed, 6 Aug 2014 16:16:24 -0700 (PDT)
In-Reply-To: <CAFiYKR89TZ21tsjmp+cJ7UQEFg6=zDgYGJwMTQ4_bB7nj8DusA@mail.gmail.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
 <CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
 <CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
 <CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
 <CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
 <64474308D680D540A4D8151B0F7C03F7026F9413@SHSMSX104.ccr.corp.intel.com> <CAFiYKR89TZ21tsjmp+cJ7UQEFg6=zDgYGJwMTQ4_bB7nj8DusA@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 6 Aug 2014 16:16:24 -0700
Message-ID: <CAMwrk0=8rH7Cfn+BSbq8O=nVqKsE4vZy=cUgdCzg=i2=FJWQFw@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
To: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
Cc: "Shao, Saisai" <saisai.shao@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b11198ffb533204fffe2a5d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b11198ffb533204fffe2a5d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Dibyendu,

This is really awesome. I am still yet to go through the code to understand
the details, but I want to do it really soon. In particular, I want to
understand the improvements, over the existing Kafka receiver.

And its fantastic to see such contributions from the community. :)

TD

On Tue, Aug 5, 2014 at 8:38 AM, Dibyendu Bhattacharya <
dibyendu.bhattachary@gmail.com> wrote:

> Hi
>
> This fault tolerant aspect already taken care in the Kafka-Spark Consumer
> code , like if Leader of a partition changes etc.. in ZkCoordinator.java.
> Basically it does a refresh of PartitionManagers every X seconds to make
> sure Partition details is correct and consumer don't fail.
>
> Dib
>
>
> On Tue, Aug 5, 2014 at 8:01 PM, Shao, Saisai <saisai.shao@intel.com>
> wrote:
>
> > Hi,
> >
> > I think this is an awesome feature for Spark Streaming Kafka interface =
to
> > offer user the controllability of partition offset, so user can have mo=
re
> > applications based on this.
> >
> > What I concern is that if we want to do offset management, fault tolera=
nt
> > related control and others, we have to take the role as current
> > ZookeeperConsumerConnect did, that would be a big field we should take
> care
> > of, for example when node is failed, how to pass current partition to
> > another consumer and some others. I=E2=80=99m not sure what is your tho=
ught?
> >
> > Thanks
> > Jerry
> >
> > From: Dibyendu Bhattacharya [mailto:dibyendu.bhattachary@gmail.com]
> > Sent: Tuesday, August 05, 2014 5:15 PM
> > To: Jonathan Hodges; dev@spark.apache.org
> > Cc: user
> > Subject: Re: Low Level Kafka Consumer for Spark
> >
> > Thanks Jonathan,
> >
> > Yes, till non-ZK based offset management is available in Kafka, I need =
to
> > maintain the offset in ZK. And yes, both cases explicit commit is
> > necessary. I modified the Low Level Kafka Spark Consumer little bit to
> have
> > Receiver spawns threads for every partition of the topic and perform th=
e
> > 'store' operation in multiple threads. It would be good if the
> > receiver.store methods are made thread safe..which is not now presently=
 .
> >
> > Waiting for TD's comment on this Kafka Spark Low Level consumer.
> >
> >
> > Regards,
> > Dibyendu
> >
> >
> > On Tue, Aug 5, 2014 at 5:32 AM, Jonathan Hodges <hodgesz@gmail.com
> <mailto:
> > hodgesz@gmail.com>> wrote:
> > Hi Yan,
> >
> > That is a good suggestion.  I believe non-Zookeeper offset management
> will
> > be a feature in the upcoming Kafka 0.8.2 release tentatively scheduled
> for
> > September.
> >
> >
> >
> https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset=
+Management
> >
> > That should make this fairly easy to implement, but it will still requi=
re
> > explicit offset commits to avoid data loss which is different than the
> > current KafkaUtils implementation.
> >
> > Jonathan
> >
> >
> >
> >
> > On Mon, Aug 4, 2014 at 4:51 PM, Yan Fang <yanfang724@gmail.com<mailto:
> > yanfang724@gmail.com>> wrote:
> > Another suggestion that may help is that, you can consider use Kafka to
> > store the latest offset instead of Zookeeper. There are at least two
> > benefits: 1) lower the workload of ZK 2) support replay from certain
> > offset. This is how Samza<http://samza.incubator.apache.org/> deals wit=
h
> > the Kafka offset, the doc is here<
> >
> http://samza.incubator.apache.org/learn/documentation/0.7.0/container/che=
ckpointing.html
> >
> > . Thank you.
> >
> > Cheers,
> >
> > Fang, Yan
> > yanfang724@gmail.com<mailto:yanfang724@gmail.com>
> > +1 (206) 849-4108<tel:%2B1%20%28206%29%20849-4108>
> >
> > On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com
> > <mailto:pwendell@gmail.com>> wrote:
> > I'll let TD chime on on this one, but I'm guessing this would be a
> welcome
> > addition. It's great to see community effort on adding new
> > streams/receivers, adding a Java API for receivers was something we did
> > specifically to allow this :)
> >
> > - Patrick
> >
> > On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
> > dibyendu.bhattachary@gmail.com<mailto:dibyendu.bhattachary@gmail.com>>
> > wrote:
> > Hi,
> >
> > I have implemented a Low Level Kafka Consumer for Spark Streaming using
> > Kafka Simple Consumer API. This API will give better control over the
> Kafka
> > offset management and recovery from failures. As the present Spark
> > KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
> > control over the offset management which is not possible in Kafka
> HighLevel
> > consumer.
> >
> > This Project is available in below Repo :
> >
> > https://github.com/dibbhatt/kafka-spark-consumer
> >
> >
> > I have implemented a Custom Receiver consumer.kafka.client.KafkaReceive=
r.
> > The KafkaReceiver uses low level Kafka Consumer API (implemented in
> > consumer.kafka packages) to fetch messages from Kafka and 'store' it in
> > Spark.
> >
> > The logic will detect number of partitions for a topic and spawn that
> many
> > threads (Individual instances of Consumers). Kafka Consumer uses
> Zookeeper
> > for storing the latest offset for individual partitions, which will hel=
p
> to
> > recover in case of failure. The Kafka Consumer logic is tolerant to ZK
> > Failures, Kafka Leader of Partition changes, Kafka broker failures,
> >  recovery from offset errors and other fail-over aspects.
> >
> > The consumer.kafka.client.Consumer is the sample Consumer which uses th=
is
> > Kafka Receivers to generate DStreams from Kafka and apply a Output
> > operation for every messages of the RDD.
> >
> > We are planning to use this Kafka Spark Consumer to perform Near Real
> Time
> > Indexing of Kafka Messages to target Search Cluster and also Near Real
> Time
> > Aggregation using target NoSQL storage.
> >
> > Kindly let me know your view. Also if this looks good, can I contribute
> to
> > Spark Streaming project.
> >
> > Regards,
> > Dibyendu
> >
> >
> >
> >
> >
>

--047d7b11198ffb533204fffe2a5d--

From dev-return-8754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 01:11:59 2014
Return-Path: <dev-return-8754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BAA2311EA5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 01:11:59 +0000 (UTC)
Received: (qmail 24360 invoked by uid 500); 7 Aug 2014 01:11:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24293 invoked by uid 500); 7 Aug 2014 01:11:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24280 invoked by uid 99); 7 Aug 2014 01:11:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:11:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:11:53 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so3485505wes.19
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 18:11:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=Fl/AV21xolVUaR0YOZMlXUkQW7L3wiJV6GvTw8aYSXE=;
        b=ifh2v+iYrZgajVhfc2dgQS/0jilzd5XHKzKzJshLPPSudk3CcUig9dWnErAIJgbuEC
         QNk99usc+bDh5M97oOwGvFcufsIQyBWOjgAEFPQwQ5vMN2iDF5ICYToeKLZzIe1e03vT
         3qfeXNTq92MdfeX+twci89eLgJIj5w7PWjvHyyOJp1B1yr7g8AbeT1j+tC7tpM3VW7up
         zHX8uutdZIGGxH9C392W8o194Z8R/p/XWLF5DuzqgbkyPX37VaoUCTG/cJUSwNqBSeSu
         PuLRTbGToYzfgTpi9oY5dMKk/t57+cDJBtO63VHiAANpo+2DAmMXAyXmzZP97gsCo4GR
         XH8A==
X-Gm-Message-State: ALoCoQlgvI4iFGMH8Tg/Tnpnr07xg46YszP8QqnLUz5WwBhDHkBfGu/LpQq5RBjPOXPp2VbkkY62
MIME-Version: 1.0
X-Received: by 10.195.13.79 with SMTP id ew15mr19241218wjd.19.1407373891988;
 Wed, 06 Aug 2014 18:11:31 -0700 (PDT)
Received: by 10.180.94.34 with HTTP; Wed, 6 Aug 2014 18:11:31 -0700 (PDT)
Date: Wed, 6 Aug 2014 18:11:31 -0700
Message-ID: <CAFZt-ETM5dp+2VBTv_1_KaCg0N27oE0g3OoRhhxK2hB-t-1+Mg@mail.gmail.com>
Subject: Documentation confusing or incorrect for decision trees?
From: Matt Forbes <matt@tellapart.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd093ae6058b04ffffc4c2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd093ae6058b04ffffc4c2
Content-Type: text/plain; charset=UTF-8

I found the section on ordering categorical features really interesting,
but the A, B, C example seemed inconsistent. Am I interpreting this passage
wrong, or are there typos? Aren't the split candidates A | C, B and A, C |
B ?

For example, for a binary classification problem with one categorical
feature with three categories A, B and C with corresponding proportion of
label 1 as 0.2, 0.6 and 0.4, the categorical features are ordered as A
followed by C followed B or A, B, C. The two split candidates are A | C, B
and A , B | C where | denotes the split.

--047d7bfd093ae6058b04ffffc4c2--

From dev-return-8755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 01:48:12 2014
Return-Path: <dev-return-8755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1AC4D11F73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 01:48:12 +0000 (UTC)
Received: (qmail 99444 invoked by uid 500); 7 Aug 2014 01:48:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99370 invoked by uid 500); 7 Aug 2014 01:48:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99342 invoked by uid 99); 7 Aug 2014 01:48:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:48:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:48:06 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so3398647wes.25
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 18:47:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=sNXQ2eIc4ZBVYpM8OtoV13a9G1DRVdWkE3l43K99wwQ=;
        b=CQc6PpGTAFiqGrBAdHEQP8CxYuYP6CneJZYvaFdAgM8bbjP9Dfm3vtdGbXGohacaWJ
         oFFu5H5FFMaZEzGfn3JWWcHmVFAwufDKw0i41NWahK4KONiEV10cTyKDHIzLBfB0rKJf
         qVH3h38qGoFiLfk97uN2OhL8tBtSfFnE5WtOsuqqwOEceMuDDkZIJ4gVdnY+t11UF3Du
         4/nnG+nEhEwOsl7+ffXCKEo8CEFL6uQLf+8bkgJY9pVHmIUBxD8Nh/PXnlcS6vVAjGKg
         awEdXw0ycUgUouze4ymx8QYTN0YTBec9LM572/YL7snOaX1S1mz9DpHRJ1zyg+Mc7jFY
         L2fg==
MIME-Version: 1.0
X-Received: by 10.194.95.66 with SMTP id di2mr14261032wjb.47.1407376065551;
 Wed, 06 Aug 2014 18:47:45 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Wed, 6 Aug 2014 18:47:45 -0700 (PDT)
Date: Thu, 7 Aug 2014 11:47:45 +1000
Message-ID: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
Subject: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb0498e73e6f60500004624
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb0498e73e6f60500004624
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Spark devs,

I=E2=80=99ve posted an issue on JIRA (
https://issues.apache.org/jira/browse/SPARK-2878) which occurs when using
Kryo serialisation with a custom Kryo registrator to register custom
classes with Kryo.  This is an insidious issue that non-deterministically
causes Kryo to have different ID number =3D> class name maps on different
nodes, which then causes weird exceptions (ClassCastException,
ClassNotFoundException, ArrayIndexOutOfBoundsException) at deserialisation
time.  I=E2=80=99ve created a reliable reproduction for the issue here:
https://github.com/GrahamDennis/spark-kryo-serialisation

I=E2=80=99m happy to try and put a pull request together to try and address=
 this,
but it=E2=80=99s not obvious to me the right way to solve this and I=E2=80=
=99d like to get
feedback / ideas on how to address this.

The root cause of the problem is a "Failed to run spark.kryo.registrator=E2=
=80=9D
error which non-deterministically occurs in some executor processes during
operation.  My custom Kryo registrator is in the application jar, and it is
accessible on the worker nodes.  This is demonstrated by the fact that most
of the time the custom kryo registrator is successfully run.

What=E2=80=99s happening is that Kryo serialisation/deserialisation is happ=
ening
most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D thread=
, which has the
thread's class loader set to contain the application jar.  This happens in
`org.apache.spark.executor.Executor.TaskRunner.run`, and from what I can
tell, it is only these threads that have access to the application jar
(that contains the custom Kryo registrator).  However, the
ConnectionManager threads sometimes need to serialise/deserialise objects
to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects haven=E2=80=
=99t previously been
serialised.  As the ConnectionManager threads don=E2=80=99t have the applic=
ation
jar available from their class loader, when it tries to look up the custom
Kryo registrator, this fails.  Spark then swallows this exception, which
results in a different ID number =E2=80=94> class mapping for this kryo ins=
tance,
and this then causes deserialisation errors later on a different node.

A related issue to the issue reported in SPARK-2878 is that Spark probably
shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kryo regis=
trators.
 The user has explicitly specified this class, and if it deterministically
can=E2=80=99t be found, then it may cause problems at serialisation /
deserialisation time.  If only sometimes it can=E2=80=99t be found (as in t=
his
case), then it leads to a data corruption issue later on.  Either way,
we=E2=80=99re better off dying due to the ClassNotFound exception earlier, =
than the
weirder errors later on.

I have some ideas on potential solutions to this issue, but I=E2=80=99m kee=
n for
experienced eyes to critique these approaches:

1. The simplest approach to fixing this would be to just make the
application jar available to the connection manager threads, but I=E2=80=99=
m
guessing it=E2=80=99s a design decision to isolate the application jar to j=
ust the
executor task runner threads.  Also, I don=E2=80=99t know if there are any =
other
threads that might be interacting with kryo serialisation / deserialisation=
.
2. Before looking up the custom Kryo registrator, change the thread=E2=80=
=99s class
loader to include the application jar, then restore the class loader after
the kryo registrator has been run.  I don=E2=80=99t know if this would have=
 any
other side-effects.
3. Always serialise / deserialise on the existing TaskRunner threads,
rather than delaying serialisation until later, when it can be done only if
needed.  This approach would probably have negative performance
consequences.
4. Create a new dedicated thread pool for lazy serialisation /
deserialisation that has the application jar on the class path.
 Serialisation / deserialisation would be the only thing these threads do,
and this would minimise conflicts / interactions between the application
jar and other jars.

#4 sounds like the best approach to me, but I think would require
considerable knowledge of Spark internals, which is beyond me at present.
 Does anyone have any better (and ideally simpler) ideas?

Cheers,

Graham

--047d7bb0498e73e6f60500004624--

From dev-return-8756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 01:54:27 2014
Return-Path: <dev-return-8756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6FCB811F96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 01:54:27 +0000 (UTC)
Received: (qmail 7417 invoked by uid 500); 7 Aug 2014 01:54:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7356 invoked by uid 500); 7 Aug 2014 01:54:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7344 invoked by uid 99); 7 Aug 2014 01:54:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:54:26 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 01:54:24 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so3407770wgg.28
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 18:53:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Sx+AbJ6T4QZZyfkkGk9D5JxKW46H/fit/2SDXJhH51E=;
        b=Fa51BmOrDAp6ejpKVK/spmJ11T86+H62C4VCX0hD7cDaq/BqbsDvWlPvWzojImnjgT
         cpDNqrcxeFjWkI2SNM1CSh2uDwkcBmEa9S5FoJ2HmaJu7jGwA1I4WTf07xW9Mb5CDoXh
         90dgCW0B2Q7Zm4fPE5RqxV9HuvRI5TLFpW9dqEogOxHysJNRMmIee8N+LwSdUsSSm3hK
         WVpz7qMTAV06S+FBMPiWeSdkqcSHTrSyP6FAfqZ/rl25zj0sctZqnN7/MZa3PQlqs/ll
         YmlEt1SadolkPYFoWw3ACt9NqDZRn4DZEsDZeygudKV4SClhkdXTCaAJm0DSZbzUCpOk
         CfJg==
X-Gm-Message-State: ALoCoQl67S4VuqHdQdU+BzHnZFYaAhXLHsJElcRznTV6ND8NDO/8lO9UhyGFgozKD5kVggLE6/u7
X-Received: by 10.180.95.66 with SMTP id di2mr20776533wib.60.1407376439565;
 Wed, 06 Aug 2014 18:53:59 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Wed, 6 Aug 2014 18:53:39 -0700 (PDT)
In-Reply-To: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 6 Aug 2014 18:53:39 -0700
Message-ID: <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
To: Graham Dennis <graham.dennis@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04182524bee3d80500005c20
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04182524bee3d80500005c20
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I don't think it was a conscious design decision to not include the
application classes in the connection manager serializer. We should fix
that. Where is it deserializing data in that thread?

4 might make sense in the long run, but it adds a lot of complexity to the
code base (whole separate code base, task queue, blocking/non-blocking
logic within task threads) that can be error prone, so I think it is best
to stay away from that right now.





On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> Hi Spark devs,
>
> I=E2=80=99ve posted an issue on JIRA (
> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when using
> Kryo serialisation with a custom Kryo registrator to register custom
> classes with Kryo.  This is an insidious issue that non-deterministically
> causes Kryo to have different ID number =3D> class name maps on different
> nodes, which then causes weird exceptions (ClassCastException,
> ClassNotFoundException, ArrayIndexOutOfBoundsException) at deserialisatio=
n
> time.  I=E2=80=99ve created a reliable reproduction for the issue here:
> https://github.com/GrahamDennis/spark-kryo-serialisation
>
> I=E2=80=99m happy to try and put a pull request together to try and addre=
ss this,
> but it=E2=80=99s not obvious to me the right way to solve this and I=E2=
=80=99d like to get
> feedback / ideas on how to address this.
>
> The root cause of the problem is a "Failed to run spark.kryo.registrator=
=E2=80=9D
> error which non-deterministically occurs in some executor processes durin=
g
> operation.  My custom Kryo registrator is in the application jar, and it =
is
> accessible on the worker nodes.  This is demonstrated by the fact that mo=
st
> of the time the custom kryo registrator is successfully run.
>
> What=E2=80=99s happening is that Kryo serialisation/deserialisation is ha=
ppening
> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D thre=
ad, which has the
> thread's class loader set to contain the application jar.  This happens i=
n
> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what I can
> tell, it is only these threads that have access to the application jar
> (that contains the custom Kryo registrator).  However, the
> ConnectionManager threads sometimes need to serialise/deserialise objects
> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects haven=E2=
=80=99t previously been
> serialised.  As the ConnectionManager threads don=E2=80=99t have the appl=
ication
> jar available from their class loader, when it tries to look up the custo=
m
> Kryo registrator, this fails.  Spark then swallows this exception, which
> results in a different ID number =E2=80=94> class mapping for this kryo i=
nstance,
> and this then causes deserialisation errors later on a different node.
>
> A related issue to the issue reported in SPARK-2878 is that Spark probabl=
y
> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kryo reg=
istrators.
>  The user has explicitly specified this class, and if it deterministicall=
y
> can=E2=80=99t be found, then it may cause problems at serialisation /
> deserialisation time.  If only sometimes it can=E2=80=99t be found (as in=
 this
> case), then it leads to a data corruption issue later on.  Either way,
> we=E2=80=99re better off dying due to the ClassNotFound exception earlier=
, than the
> weirder errors later on.
>
> I have some ideas on potential solutions to this issue, but I=E2=80=99m k=
een for
> experienced eyes to critique these approaches:
>
> 1. The simplest approach to fixing this would be to just make the
> application jar available to the connection manager threads, but I=E2=80=
=99m
> guessing it=E2=80=99s a design decision to isolate the application jar to=
 just the
> executor task runner threads.  Also, I don=E2=80=99t know if there are an=
y other
> threads that might be interacting with kryo serialisation /
> deserialisation.
> 2. Before looking up the custom Kryo registrator, change the thread=E2=80=
=99s class
> loader to include the application jar, then restore the class loader afte=
r
> the kryo registrator has been run.  I don=E2=80=99t know if this would ha=
ve any
> other side-effects.
> 3. Always serialise / deserialise on the existing TaskRunner threads,
> rather than delaying serialisation until later, when it can be done only =
if
> needed.  This approach would probably have negative performance
> consequences.
> 4. Create a new dedicated thread pool for lazy serialisation /
> deserialisation that has the application jar on the class path.
>  Serialisation / deserialisation would be the only thing these threads do=
,
> and this would minimise conflicts / interactions between the application
> jar and other jars.
>
> #4 sounds like the best approach to me, but I think would require
> considerable knowledge of Spark internals, which is beyond me at present.
>  Does anyone have any better (and ideally simpler) ideas?
>
> Cheers,
>
> Graham
>

--f46d04182524bee3d80500005c20--

From dev-return-8757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 02:02:20 2014
Return-Path: <dev-return-8757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 886AA11FC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 02:02:20 +0000 (UTC)
Received: (qmail 20306 invoked by uid 500); 7 Aug 2014 02:02:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20248 invoked by uid 500); 7 Aug 2014 02:02:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20229 invoked by uid 99); 7 Aug 2014 02:02:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 02:02:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 02:02:15 +0000
Received: by mail-we0-f175.google.com with SMTP id t60so3506380wes.20
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 19:01:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=m8ICuB6Pw6sXBWxm1nDSUMwrGEWlCDRNbghbptkLpis=;
        b=faTMcVG4kClb+hYehvazDjv8DTG4RbZntaiYTGd724+6x5LTSdDFcVAisVcjls/NhG
         2xVATCco4m5Pf7xXJ5iBe7Qgv0gU5Gdv+0sD898LgSqE44BH/+zVYEuYsWgLYz3D3Uaa
         w0p4sLz2SwQ1IfnhtTe5H6JdHfAMo9dFC80flzy7pjfw9Ka/Ywu/Zns2TpfdLT3vv5bt
         kZ4bjKPUWbslL8hTAJ0PrhR7Q1W8eD/L7ovqk3tYgh/qTBIh0gSoyyrfVKXXfscGcy2l
         JIloBXy6ezBHimxWZJrhm5emrAko/szuy1XlAsc+4PnC7ta8KC3mvbqJZq7l84DRDhyp
         S3SA==
MIME-Version: 1.0
X-Received: by 10.180.187.141 with SMTP id fs13mr20915033wic.57.1407376913667;
 Wed, 06 Aug 2014 19:01:53 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Wed, 6 Aug 2014 19:01:53 -0700 (PDT)
In-Reply-To: <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
Date: Thu, 7 Aug 2014 12:01:53 +1000
Message-ID: <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c381e4011ea50500007930
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c381e4011ea50500007930
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

See my comment on https://issues.apache.org/jira/browse/SPARK-2878 for the
full stacktrace, but it's in the BlockManager/BlockManagerWorker where it's
trying to fulfil a "getBlock" request for another node.  The objects that
would be in the block haven't yet been serialised, and that then causes the
deserialisation to happen on that thread.  See MemoryStore.scala:102.


On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:

> I don't think it was a conscious design decision to not include the
> application classes in the connection manager serializer. We should fix
> that. Where is it deserializing data in that thread?
>
> 4 might make sense in the long run, but it adds a lot of complexity to th=
e
> code base (whole separate code base, task queue, blocking/non-blocking
> logic within task threads) that can be error prone, so I think it is best
> to stay away from that right now.
>
>
>
>
>
> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> Hi Spark devs,
>>
>> I=E2=80=99ve posted an issue on JIRA (
>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when usin=
g
>> Kryo serialisation with a custom Kryo registrator to register custom
>> classes with Kryo.  This is an insidious issue that non-deterministicall=
y
>> causes Kryo to have different ID number =3D> class name maps on differen=
t
>> nodes, which then causes weird exceptions (ClassCastException,
>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at deserialisati=
on
>> time.  I=E2=80=99ve created a reliable reproduction for the issue here:
>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>
>> I=E2=80=99m happy to try and put a pull request together to try and addr=
ess this,
>> but it=E2=80=99s not obvious to me the right way to solve this and I=E2=
=80=99d like to get
>> feedback / ideas on how to address this.
>>
>> The root cause of the problem is a "Failed to run spark.kryo.registrator=
=E2=80=9D
>> error which non-deterministically occurs in some executor processes duri=
ng
>> operation.  My custom Kryo registrator is in the application jar, and it
>> is
>> accessible on the worker nodes.  This is demonstrated by the fact that
>> most
>> of the time the custom kryo registrator is successfully run.
>>
>> What=E2=80=99s happening is that Kryo serialisation/deserialisation is h=
appening
>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D thr=
ead, which has the
>> thread's class loader set to contain the application jar.  This happens =
in
>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what I can
>> tell, it is only these threads that have access to the application jar
>> (that contains the custom Kryo registrator).  However, the
>> ConnectionManager threads sometimes need to serialise/deserialise object=
s
>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects haven=E2=
=80=99t previously been
>> serialised.  As the ConnectionManager threads don=E2=80=99t have the app=
lication
>> jar available from their class loader, when it tries to look up the cust=
om
>> Kryo registrator, this fails.  Spark then swallows this exception, which
>> results in a different ID number =E2=80=94> class mapping for this kryo =
instance,
>> and this then causes deserialisation errors later on a different node.
>>
>> A related issue to the issue reported in SPARK-2878 is that Spark probab=
ly
>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kryo
>> registrators.
>>  The user has explicitly specified this class, and if it deterministical=
ly
>> can=E2=80=99t be found, then it may cause problems at serialisation /
>> deserialisation time.  If only sometimes it can=E2=80=99t be found (as i=
n this
>> case), then it leads to a data corruption issue later on.  Either way,
>> we=E2=80=99re better off dying due to the ClassNotFound exception earlie=
r, than
>> the
>> weirder errors later on.
>>
>> I have some ideas on potential solutions to this issue, but I=E2=80=99m =
keen for
>> experienced eyes to critique these approaches:
>>
>> 1. The simplest approach to fixing this would be to just make the
>> application jar available to the connection manager threads, but I=E2=80=
=99m
>> guessing it=E2=80=99s a design decision to isolate the application jar t=
o just the
>> executor task runner threads.  Also, I don=E2=80=99t know if there are a=
ny other
>> threads that might be interacting with kryo serialisation /
>> deserialisation.
>> 2. Before looking up the custom Kryo registrator, change the thread=E2=
=80=99s
>> class
>> loader to include the application jar, then restore the class loader aft=
er
>> the kryo registrator has been run.  I don=E2=80=99t know if this would h=
ave any
>> other side-effects.
>> 3. Always serialise / deserialise on the existing TaskRunner threads,
>> rather than delaying serialisation until later, when it can be done only
>> if
>> needed.  This approach would probably have negative performance
>> consequences.
>> 4. Create a new dedicated thread pool for lazy serialisation /
>> deserialisation that has the application jar on the class path.
>>  Serialisation / deserialisation would be the only thing these threads d=
o,
>> and this would minimise conflicts / interactions between the application
>> jar and other jars.
>>
>> #4 sounds like the best approach to me, but I think would require
>> considerable knowledge of Spark internals, which is beyond me at present=
.
>>  Does anyone have any better (and ideally simpler) ideas?
>>
>> Cheers,
>>
>> Graham
>>
>
>

--001a11c381e4011ea50500007930--

From dev-return-8758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 02:23:20 2014
Return-Path: <dev-return-8758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0826C11036
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 02:23:20 +0000 (UTC)
Received: (qmail 47477 invoked by uid 500); 7 Aug 2014 02:23:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47423 invoked by uid 500); 7 Aug 2014 02:23:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47376 invoked by uid 99); 7 Aug 2014 02:23:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 02:23:18 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 72.30.239.75 as permitted sender)
Received: from [72.30.239.75] (HELO nm34-vm3.bullet.mail.bf1.yahoo.com) (72.30.239.75)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 02:23:12 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=DPy77BhkJJ3TWLv/mERpfHv8CUzDINBb+jVobSHEHMKlFwVHrub1VT8Kw8RcNEEBl1gLPBbdN4VO+xNqfzeoJmhMVFNyC2NTkC22O/JxlMTXWkI/nDbi7tclxqNBUGS98YU6fzs5Zfws57GoY/7xJ1Lscr9Eb6Ij/fgymIlndm8A996cMniIJJV3F6xscA/4M1lhMjmkkVkuouQsTFhgnAEgQQF2vKc4uZrhFm7CpykjTKyiFqOHZvxxdU94JSHcchr1WYjQHN9VS+y7e/ul2gWmn3VlENQgoVh7tKZsrOv8tth9F88cphgVXywscHiynicFV4X0kNGJt2v7GIB75Q==;
Received: from [98.139.215.143] by nm34.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 02:22:51 -0000
Received: from [68.142.230.65] by tm14.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 02:22:51 -0000
Received: from [127.0.0.1] by smtp222.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 02:22:51 -0000
X-Yahoo-Newman-Id: 709070.72343.bm@smtp222.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: KcdbeR8VM1kTzFsx7MftWmc6yAzGo0KXhmUKmbEwOVHbTUW
 XK1PUdclwlGWKBza1qe433g5bSySX0o9exCNfziI.X_KRjiyRxqayg91EXYj
 rXh8M1unfXQ8sR298Ulxr8GySUYingskKVO0WomD6MhdOQPKxAHq7Or0XhSL
 OlbsGmQ8Bl3.mLdyXMOPfJ9kGZs5m9vCTr7XunPrPXwfEZNTStp4FgoHYgB_
 OHVZOkWwqAw_DwEUqZ94epqiwadk62vUmeD6aXFsiE7P3F4k7uUqv63LHQPW
 iBioV.5KNfSUm473HrW4hRMJ_7MYLqJ41k4YFvkupxmHlSNuWAEX5L6xPLXP
 q8lGlu9ILWrxtiXLtC2qtFtqS3gejIPc6RLV_Kuw8sGNueGHAwOXVYs4zJBI
 aJyhSEc.WEOsOFYFz0aBWGU26SYsiIgRToYG9GFJy5G0qlIgdweQIXkJsCoP
 6x0t0TnEioO3C8MLZGHZt_DOQiCYvfLoLj_GhfZIH9RDYUyjwnfM5iit7Hgd
 zIO4jLGAWO8Q3MJOTHY30AN9Otrck
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Buidling spark in Eclipse Kepler
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
X-Mailer: iPad Mail (11D257)
In-Reply-To: <CAMAsSdJfOHRVN11Au2u-QdBobd=m9OO4d0+ZEJk75CORPDZUQw@mail.gmail.com>
Date: Wed, 6 Aug 2014 19:22:49 -0700
Cc: Dev <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <F965CA09-F03B-4D40-B5A9-D47945E35847@yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com> <CAMAsSdJfOHRVN11Au2u-QdBobd=m9OO4d0+ZEJk75CORPDZUQw@mail.gmail.com>
To: Sean Owen <sowen@cloudera.com>
X-Virus-Checked: Checked by ClamAV on apache.org

Ok I'll give it a little more time, and if I can't get it going, I'll switch=
. I am indeed a little disappointed in the Scala IDE plugin for Eclipse so I=
 think switching to IntelliJ might be my best bet.

Thanks,
Ron

Sent from my iPad

> On Aug 6, 2014, at 1:43 PM, Sean Owen <sowen@cloudera.com> wrote:
>=20
> I think your best bet by far is to consume the Maven build as-is from
> within Eclipse. I wouldn't try to export a project config from the
> build as there is plenty to get lost in translation.
>=20
> Certainly this works well with IntelliJ, and by the by, if you have a
> choice, I would strongly recommend IntelliJ over Eclipse for working
> with Maven and Scala.
>=20
> On Wed, Aug 6, 2014 at 8:29 PM, Ron Gonzalez
> <zlgonzalez@yahoo.com.invalid> wrote:
>> Hi,
>>  I'm trying to get the apache spark trunk compiling in my Eclipse, but I c=
an't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it=
 doesn't seem to create the eclipse pieces for yarn and other projects. Doin=
g mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just f=
or yarn fails. Is there some documentation available for eclipse? I've gone t=
hrough the ones on the site, but to no avail.
>>  Any tips?
>>=20
>> Thanks,
>> Ron

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 03:54:29 2014
Return-Path: <dev-return-8759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 712AC1124A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 03:54:29 +0000 (UTC)
Received: (qmail 82143 invoked by uid 500); 7 Aug 2014 03:54:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82076 invoked by uid 500); 7 Aug 2014 03:54:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82054 invoked by uid 99); 7 Aug 2014 03:54:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 03:54:27 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.212.154 as permitted sender)
Received: from [98.139.212.154] (HELO nm3-vm0.bullet.mail.bf1.yahoo.com) (98.139.212.154)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 03:54:22 +0000
Received: from [66.196.81.174] by nm3.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 03:53:57 -0000
Received: from [98.139.212.215] by tm20.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 03:53:57 -0000
Received: from [127.0.0.1] by omp1024.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 03:53:57 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 392871.31277.bm@omp1024.mail.bf1.yahoo.com
Received: (qmail 83251 invoked by uid 60001); 7 Aug 2014 03:53:57 -0000
X-YMail-OSG: CSDc15QVM1m6X0Pjb7znHJZe5kr4XMFug3ruOEoqQBervxm
 .Hv4q9QFejWKKpjp9._0TViD_LjL6yXRa8DwtENAp.6nOdmZQ4..REhZOpo.
 5cZXhAITw7neH9BPQ3BYcSCa1UgikUT2cTGKuiPp._d7nVwbF9RJ2vuRZOFS
 tkx3TzLIDIpGbAJQ.zAthrBnKmwCMFFW60LCTSR3rc5DejY6obc3enXHgrqq
 UsHrQOfej0eU2jM5P2hum24UAlFWfkCbUEdGV.hauLYR5SPs8QlTIsZaR8nJ
 TIQNsUrF1Px3wdVqNxaOnuAuqr6D.S1s6ymYrhk4G.4K0WGFYAPf0kzzigOd
 EY7GTJ5S4IVaL066IMVcXsrORq96noHXBX5rA.JBkuryBqNls8rbA4REeGfk
 WEGcMXF18gT.Nz4iCfgg6UP3OovS2VKVQimgvSrtHsloHcXNiON8hqDi6Zxr
 dZwRgZjBhQd1s99DmUJUet.T4f2QCjaZLERoP7mOHvLimIeTSJ.P7u2DbRjr
 ugPHfkovQFsPeK6_EczlCz75_jYWZnoIPAg9wo_aDy.nrKYnC9csW
Received: from [75.37.202.33] by web162402.mail.bf1.yahoo.com via HTTP; Wed, 06 Aug 2014 20:53:57 PDT
X-Rocket-MIMEInfo: 002.001,U28gSSBkb3dubG9hZGVkIGNvbW11bml0eSBlZGl0aW9uIG9mIEludGVsbGlKLCBhbmQgcmFuIHNidC9zYnQgZ2VuLWlkZWEuCkkgdGhlbiBpbXBvcnRlZCB0aGUgcG9tLnhtbCBmaWxlLgpJJ20gc3RpbGwgZ2V0dGluZyBhbGwgc29ydHMgb2YgZXJyb3JzIGZyb20gSW50ZWxsaUogYWJvdXQgdW5yZXNvbHZlZCBkZXBlbmRlbmNpZXMuCkFueSBzdWdnZXN0aW9ucz8KClRoYW5rcywKUm9uCgoKT24gV2VkbmVzZGF5LCBBdWd1c3QgNiwgMjAxNCAxMjoyOSBQTSwgUm9uIEdvbnphbGV6IDx6bGdvbnphbGV6QHlhaG8BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
Message-ID: <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com>
Date: Wed, 6 Aug 2014 20:53:57 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: Re: Buidling spark in Eclipse Kepler
To: Ron Gonzalez <zlgonzalez@yahoo.com>, Dev <dev@spark.apache.org>
In-Reply-To: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-853603208-99187638-1407383637=:74750"
X-Virus-Checked: Checked by ClamAV on apache.org

---853603208-99187638-1407383637=:74750
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.=0A=
I then imported the pom.xml file.=0AI'm still getting all sorts of errors f=
rom IntelliJ about unresolved dependencies.=0AAny suggestions?=0A=0AThanks,=
=0ARon=0A=0A=0AOn Wednesday, August 6, 2014 12:29 PM, Ron Gonzalez <zlgonza=
lez@yahoo.com.INVALID> wrote:=0A =0A=0A=0AHi,=0A=A0 I'm trying to get the a=
pache spark trunk compiling in my Eclipse, but I can't seem to get it going=
. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create =
the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse o=
n yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is the=
re some documentation available for eclipse? I've gone through the ones on =
the site, but to no avail.=0A=A0 Any tips?=0A=0AThanks,=0ARon
---853603208-99187638-1407383637=:74750--

From dev-return-8760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 04:27:18 2014
Return-Path: <dev-return-8760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9277911376
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 04:27:18 +0000 (UTC)
Received: (qmail 32194 invoked by uid 500); 7 Aug 2014 04:27:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32138 invoked by uid 500); 7 Aug 2014 04:27:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32124 invoked by uid 99); 7 Aug 2014 04:27:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 04:27:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 04:27:12 +0000
Received: by mail-qg0-f51.google.com with SMTP id a108so3739252qge.24
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 21:26:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=PZKliFh/zvvDQv6MGg0M3JzP9jnFZuldWN2l46d7gow=;
        b=iouXUrhaxQIMRiONkkQZHjbI0dGi1Aw1X/lQfMl/gwsSlEY4uttnbYVoB8nYnNIMnv
         H1T7fY4i4ww2qv44rgFXFY7yS5Z1FDsqtTUq68RaBzDq8nHPKGiWejmbeya4wXPDc2M1
         UwvaiOp2U2fuEjLAdkZeRLQL4r/qYxS4TkV0ln0EW6nRtDk06E+JRSDRHBEYGcUTR8sP
         WKgU38RssnukG3sJCgtdZFzoaBV23pcWIYvRGyi7IAIxHniHw4TlKEZ+5JAC7rtq8um8
         fEHwR/+YmM8LmocszA6AWj9OW0NWwQvZHXBUqil/T2pcRCwfljtqnzxB2RKsayvJYuqY
         KfVA==
X-Gm-Message-State: ALoCoQmYQ1k+VpOfoW71kLdVwYp214Rzq3SHCJLfDAK+T7XuhpuW5VqApJqUtU/5sI++Ij8ZQPCg
MIME-Version: 1.0
X-Received: by 10.140.96.229 with SMTP id k92mr9206855qge.38.1407385611227;
 Wed, 06 Aug 2014 21:26:51 -0700 (PDT)
Received: by 10.229.183.130 with HTTP; Wed, 6 Aug 2014 21:26:51 -0700 (PDT)
Received: by 10.229.183.130 with HTTP; Wed, 6 Aug 2014 21:26:51 -0700 (PDT)
In-Reply-To: <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
	<1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com>
Date: Wed, 6 Aug 2014 21:26:51 -0700
Message-ID: <CAEYYnxYw_b=dkEfhHBmF29iYH03tEhS4UGjV-rRKQikz+-MGbw@mail.gmail.com>
Subject: Re: Buidling spark in Eclipse Kepler
From: DB Tsai <dbtsai@dbtsai.com>
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113976606b6dc70500027f01
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113976606b6dc70500027f01
Content-Type: text/plain; charset=UTF-8

After sbt gen-idea , you can open the intellji project directly without
going through pom.xml

If u want to compile inside intellji, you have to remove one of the messo
jar. This is an open issue, and u can find the detail in JIRA.

Sent from my Google Nexus 5
On Aug 6, 2014 8:54 PM, "Ron Gonzalez" <zlgonzalez@yahoo.com.invalid> wrote:

> So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.
> I then imported the pom.xml file.
> I'm still getting all sorts of errors from IntelliJ about unresolved
> dependencies.
> Any suggestions?
>
> Thanks,
> Ron
>
>
> On Wednesday, August 6, 2014 12:29 PM, Ron Gonzalez
> <zlgonzalez@yahoo.com.INVALID> wrote:
>
>
>
> Hi,
>   I'm trying to get the apache spark trunk compiling in my Eclipse, but I
> can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but
> it doesn't seem to create the eclipse pieces for yarn and other projects.
> Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse
> just for yarn fails. Is there some documentation available for eclipse?
> I've gone through the ones on the site, but to no avail.
>   Any tips?
>
> Thanks,
> Ron

--001a113976606b6dc70500027f01--

From dev-return-8761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 04:39:17 2014
Return-Path: <dev-return-8761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 659B611396
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 04:39:17 +0000 (UTC)
Received: (qmail 46810 invoked by uid 500); 7 Aug 2014 04:39:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46757 invoked by uid 500); 7 Aug 2014 04:39:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46742 invoked by uid 99); 7 Aug 2014 04:39:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 04:39:16 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.161 as permitted sender)
Received: from [98.139.213.161] (HELO nm24-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.161)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 04:39:10 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=to6nqiMQTnC6PqMcZR+8JXTp/R7HYED5r3jf8xygcw9IzzyOuOUeRLFU4HJggcwyE35INtntp30V6abl/ZyI41Yl5K0O034KXSma/9E9tbIWvk7fnYpkq94vIav//pVOmYT59jS/q0Vn48DDiDiL7dPE2ax5cNsS1B3hn1EzEDvOpEqzI1r8p4BlrGNC9rXU7q3/ZEINsM2c2pJdDHlSb6tKQTkb79jsB1yMgfwjaK5nCD7hffh5mUmfpt5Kr7g2Y80piEkk+1I71QGlO1hMZkOQlWA1AQHEn1XNNrEhk6SQt/Rmv4lHXymMiz7ReunA7QL2/v/NJFP/O0NUkp2t6A==;
Received: from [66.196.81.173] by nm24.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 04:38:49 -0000
Received: from [68.142.230.71] by tm19.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 04:38:49 -0000
Received: from [127.0.0.1] by smtp228.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 04:38:49 -0000
X-Yahoo-Newman-Id: 511305.29659.bm@smtp228.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: D0peiwoVM1lINXTiu3sfLofj6rhLNb3fM5entEjvdAdgG_c
 _Fr..imouQIdf1Z7bdwMvwxqdcxpbWXjN1.MrmJPT6OYYBXcwSgfjD0yFebL
 9r.tRCQFbqvUaYz_3MpOMnfwgDhBfiqk2z9FMXaBDcXmWlJYnxK7qxWF6QY_
 0X4MZ_Vv87rW6bvpqnXoo3xap_qw.4ud4QtVTmW_BIIe_PvtL.5KdL5m4j.2
 IrKC.sBE7EeuPMW_NfmGnPF8JZN6.8.6qilGrvUlwP6KGRpURk5eDnM.C1SW
 CFHboGLGPLsRjXrT1Dsi71FimJdiLfzjpqdT6MJgtMZrILJ16HzyvucGw7n4
 8qkcx8qAhXOrgqPF41DCwHEi2BxXkdE7VNZfdRYuNFnkvGRWYwMXF_GMg4YJ
 THBUl0BIlnK1449HfESSkREp8i.NBE2lLmjGFN09kYi7kaYOJOGm695s2MIL
 AK4Ey1hnkZkNKPdVy4yy55rBnifAMnQgL3F.t9kSccsg00hN2xAXKrB8vUYE
 DbFQYcEZo99MV5qWBkxIqFZ9gYA--
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Content-Type: multipart/alternative;
	boundary=Apple-Mail-FE46B7F7-A93D-4EDB-B3DF-D58D96EC7184
Mime-Version: 1.0 (1.0)
Subject: Re: Buidling spark in Eclipse Kepler
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
X-Mailer: iPad Mail (11D257)
In-Reply-To: <CAEYYnxYw_b=dkEfhHBmF29iYH03tEhS4UGjV-rRKQikz+-MGbw@mail.gmail.com>
Date: Wed, 6 Aug 2014 21:38:47 -0700
Cc: Dev <dev@spark.apache.org>
Content-Transfer-Encoding: 7bit
Message-Id: <969AF704-574D-40C5-B4C2-7DF51766669F@yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com> <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com> <CAEYYnxYw_b=dkEfhHBmF29iYH03tEhS4UGjV-rRKQikz+-MGbw@mail.gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-FE46B7F7-A93D-4EDB-B3DF-D58D96EC7184
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

Thanks, will give that a try.

Sent from my iPad

> On Aug 6, 2014, at 9:26 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>=20
> After sbt gen-idea , you can open the intellji project directly without go=
ing through pom.xml
>=20
> If u want to compile inside intellji, you have to remove one of the messo j=
ar. This is an open issue, and u can find the detail in JIRA.
>=20
> Sent from my Google Nexus 5
>=20
>> On Aug 6, 2014 8:54 PM, "Ron Gonzalez" <zlgonzalez@yahoo.com.invalid> wro=
te:
>> So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.
>> I then imported the pom.xml file.
>> I'm still getting all sorts of errors from IntelliJ about unresolved depe=
ndencies.
>> Any suggestions?
>>=20
>> Thanks,
>> Ron
>>=20
>>=20
>> On Wednesday, August 6, 2014 12:29 PM, Ron Gonzalez <zlgonzalez@yahoo.com=
.INVALID> wrote:
>>=20
>>=20
>>=20
>> Hi,
>>   I'm trying to get the apache spark trunk compiling in my Eclipse, but I=
 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but i=
t doesn't seem to create the eclipse pieces for yarn and other projects. Doi=
ng mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just=
 for yarn fails. Is there some documentation available for eclipse? I've gon=
e through the ones on the site, but to no avail.
>>   Any tips?
>>=20
>> Thanks,
>> Ron

--Apple-Mail-FE46B7F7-A93D-4EDB-B3DF-D58D96EC7184--

From dev-return-8762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 06:15:54 2014
Return-Path: <dev-return-8762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8816F115BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 06:15:54 +0000 (UTC)
Received: (qmail 3649 invoked by uid 500); 7 Aug 2014 06:15:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3588 invoked by uid 500); 7 Aug 2014 06:15:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3533 invoked by uid 99); 7 Aug 2014 06:15:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:15:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:15:51 +0000
Received: by mail-ob0-f177.google.com with SMTP id wp18so2498853obc.8
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 23:15:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=3TPVKejxVv6uunBT+F+r43xUEUPVrXxeuCNLFyn2hXM=;
        b=OOmdpWqCzI8dbhD/Jaf1LUanr+ECaiC24COnmEIvAVfD5JQEwb6/N2NwvCCzOuPuJn
         bIxdm/F8Y1GMktNlAxsHpyfTBokXvHjuQF9ine7WYbs1YwXNn/j/o0953bDyNlx/twdl
         s3WFWFOk8CHzUX+cB1zmcZhtipypCi0poF7gj6JbM0UtLL/K13XolyS2mOfQ+7rmJ4/F
         Dc8vwdt9goGDQK81bQur+eA5nKAmVblQqU0FUHkvEYxDDGjIWqN0Z2GaAhCCAXNTlEA+
         zJ41qgPjhP6S3iWp0J2fo5ifrW6SMV09nsIh0YjXlrJFGts6/9w8rLviQn2WaKZJWsbV
         FGwQ==
X-Gm-Message-State: ALoCoQnZMgidcJi/XCjnQfA0wqF4Xx6L6U0avtqZrIvtW45y5DOCgiFNOswAcqtbAnfWH2seu6uc
X-Received: by 10.182.219.116 with SMTP id pn20mr598042obc.86.1407392126169;
 Wed, 06 Aug 2014 23:15:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.60.142.170 with HTTP; Wed, 6 Aug 2014 23:15:05 -0700 (PDT)
In-Reply-To: <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com> <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 7 Aug 2014 07:15:05 +0100
Message-ID: <CAMAsSdLH6NtdTizQ+hNqZ0A4OFDyLHmGMM6SmY2r9c3-p1wqHw@mail.gmail.com>
Subject: Re: Buidling spark in Eclipse Kepler
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

(Don't use gen-idea, just open it directly as a Maven project in IntelliJ.)

On Thu, Aug 7, 2014 at 4:53 AM, Ron Gonzalez
<zlgonzalez@yahoo.com.invalid> wrote:
> So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.
> I then imported the pom.xml file.
> I'm still getting all sorts of errors from IntelliJ about unresolved depe=
ndencies.
> Any suggestions?
>
> Thanks,
> Ron
>
>
> On Wednesday, August 6, 2014 12:29 PM, Ron Gonzalez <zlgonzalez@yahoo.com=
.INVALID> wrote:
>
>
>
> Hi,
>   I'm trying to get the apache spark trunk compiling in my Eclipse, but I=
 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but=
 it doesn't seem to create the eclipse pieces for yarn and other projects. =
Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse =
just for yarn fails. Is there some documentation available for eclipse? I'v=
e gone through the ones on the site, but to no avail.
>   Any tips?
>
> Thanks,
> Ron

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 06:19:55 2014
Return-Path: <dev-return-8763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF097115F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 06:19:55 +0000 (UTC)
Received: (qmail 14843 invoked by uid 500); 7 Aug 2014 06:19:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14785 invoked by uid 500); 7 Aug 2014 06:19:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14772 invoked by uid 99); 7 Aug 2014 06:19:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:19:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:19:50 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so2293523oig.26
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 23:19:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=uk3L0BsOeQnQXMNtML0E4wcAKG3Y3jaLTGuaj/F6HmQ=;
        b=Tqv5XjFWxVdGvBKMH5nv5+06N3XCRdQy/FEHlAjzpG02NBVuSC6ZhaYea/drMAq/5E
         1sjKgHtIP9p5rIIs+VfSpWhS60GPWvGiF6F5U4K5TbIogNOBZWrO3iVGR8yKDBsOMzLd
         wxn6XKqIOuju0SfdZ3r9yp/m3S5N9i4z3XdCBHIHFjfzwvA8HxVxgsmnCT8jqYXPWKgf
         +g9Tw/cjrzQoZr6SRzV6rAHTHdqxIeA+pWx7EWwNsaaqPn/Bs5gjiZvLCEdZUpx5kDxJ
         OQoGM5W+F1yhRGzBHhUxwxSDwG0M9SQ+NVBvsa5DvWzsLwfDTvvIWU2vbgNpZo2E8Xl1
         IVSA==
X-Gm-Message-State: ALoCoQkDrGvxsPA8F0F9/FbZPxU5EfMFi8KmT58iYVXpWVJyVJGSgQll1RqUGZtSeV6MRBC4qbb/
X-Received: by 10.60.57.36 with SMTP id f4mr20842848oeq.10.1407392370174; Wed,
 06 Aug 2014 23:19:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.60.142.170 with HTTP; Wed, 6 Aug 2014 23:19:10 -0700 (PDT)
In-Reply-To: <CAFZt-ETM5dp+2VBTv_1_KaCg0N27oE0g3OoRhhxK2hB-t-1+Mg@mail.gmail.com>
References: <CAFZt-ETM5dp+2VBTv_1_KaCg0N27oE0g3OoRhhxK2hB-t-1+Mg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 7 Aug 2014 07:19:10 +0100
Message-ID: <CAMAsSd+d=MpOs7RCPbzTP1XrEWsC118QPhVYanmXLuBsdrdejA@mail.gmail.com>
Subject: Re: Documentation confusing or incorrect for decision trees?
To: Matt Forbes <matt@tellapart.com>
Cc: dev-spark <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's definitely just a typo. The ordered categories are A, C, B so the
other split can't be A | B, C. Just open a PR.

On Thu, Aug 7, 2014 at 2:11 AM, Matt Forbes <matt@tellapart.com> wrote:
> I found the section on ordering categorical features really interesting,
> but the A, B, C example seemed inconsistent. Am I interpreting this passage
> wrong, or are there typos? Aren't the split candidates A | C, B and A, C |
> B ?
>
> For example, for a binary classification problem with one categorical
> feature with three categories A, B and C with corresponding proportion of
> label 1 as 0.2, 0.6 and 0.4, the categorical features are ordered as A
> followed by C followed B or A, B, C. The two split candidates are A | C, B
> and A , B | C where | denotes the split.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 06:23:48 2014
Return-Path: <dev-return-8764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1FF0E11611
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 06:23:48 +0000 (UTC)
Received: (qmail 29899 invoked by uid 500); 7 Aug 2014 06:23:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29835 invoked by uid 500); 7 Aug 2014 06:23:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29820 invoked by uid 99); 7 Aug 2014 06:23:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:23:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:23:44 +0000
Received: by mail-ob0-f181.google.com with SMTP id va2so2651248obc.12
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 23:23:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=l5l9psOwXRS0QMnH/q78rZJ+uJ8DalfuWco6++IY2co=;
        b=dU6i0WRkjnr7kngO+WqhH3RH//v2Bjx6eoyC9VPR0SXjEzXtCqtC7c6y5blLrPplaP
         z3KC5efDeE96WPxQsyhWwNSPP5u1bbv6VeX6Dsah/jT6xm7aE6sRGTIce7HUfQKG/D5l
         tEj5Awgeja/y7V9Um9nEgSEZI+zxt+VqKZDmwMEVSRKiolDDiCdJrsxmu3QkeBusD35C
         AZ9OdP/hnJ5/5kdEjiTUV5JEFHNnsSjQlxr6kw8NT6P7L7y8I4de6pGRrQy8sMSCnva1
         ghrpcQw1lm1h3sTmtzMkLfJFMfRBIyW3eDXm70FS1LJJnc4pZqEU1lZRt9nLrPAdPwGf
         BtAA==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr18751621oes.78.1407392599131;
 Wed, 06 Aug 2014 23:23:19 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Wed, 6 Aug 2014 23:23:19 -0700 (PDT)
Date: Wed, 6 Aug 2014 23:23:19 -0700
Message-ID: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
Subject: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c249aeee4c0f0500041f48
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c249aeee4c0f0500041f48
Content-Type: text/plain; charset=ISO-8859-1

Hi All,

I've packaged and published a snapshot release of Spark 1.1 for testing.
This is being distributed to the community for QA and preview purposes. It
is not yet an official RC for voting. Going forward, we'll do preview
releases like this for testing ahead of official votes.

The tag of this release is v1.1.0-snapshot1 (commit d428d8):
*https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0
<https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0>*

The release files, including signatures, digests, etc can be found at:
*http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
<http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/>*

Release artifacts are signed with the following key:
*https://people.apache.org/keys/committer/pwendell.asc
<https://people.apache.org/keys/committer/pwendell.asc>*

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1025/
<https://repository.apache.org/content/repositories/orgapachespark-1024/>

NOTE: Due to SPARK-2899, docs are not yet available for this release. Docs
will be posted ASAP.

To learn more about Apache Spark, please see
http://spark.apache.org/

--001a11c249aeee4c0f0500041f48--

From dev-return-8765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 06:25:13 2014
Return-Path: <dev-return-8765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 486F31161B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 06:25:13 +0000 (UTC)
Received: (qmail 32109 invoked by uid 500); 7 Aug 2014 06:25:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32053 invoked by uid 500); 7 Aug 2014 06:25:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32022 invoked by uid 99); 7 Aug 2014 06:25:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:25:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 06:25:07 +0000
Received: by mail-ob0-f173.google.com with SMTP id vb8so2638709obc.18
        for <dev@spark.apache.org>; Wed, 06 Aug 2014 23:24:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=3hNF7Ljwo9zgwSrOaRH2FXlqQCFzIkL8PBnQpgUuq/c=;
        b=GeSvaCyD2jHsNa+Bg2vM3kbdVBlsS006x3xxkOfXxpDLG/3FFXTV7tLVppiZhtiixM
         NTxZHE+WqPMhRk5GZU2r1f/PImXbZ42VsvG23//ONkz1eJrLceTnSo1Y90k01Q94vlsQ
         KfPVDvOeORTB8YWGj2VjGQncCVYB8UT6UnFWpveMJhw6FfsyxUxY+K8IuXaG/4+kGZVO
         B0SyTFf8aYPMWBn9wx+rgoybdyNNqyuSS5InZRJjTTwVflRRJ5gD1/eZ8+0qiiz3xdWC
         2Df1yFfPskur2LjaekR8s5iMUHdUoBjQIl3jlLQYDprArCB2o3Zae3gOOxZyfb8VCC0J
         OhOg==
MIME-Version: 1.0
X-Received: by 10.182.108.228 with SMTP id hn4mr20862570obb.73.1407392686487;
 Wed, 06 Aug 2014 23:24:46 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Wed, 6 Aug 2014 23:24:46 -0700 (PDT)
In-Reply-To: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
References: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
Date: Wed, 6 Aug 2014 23:24:46 -0700
Message-ID: <CABPQxsvSF3iPC1QHamx3QjwWjyMORSnxie+F2fwMAycXFsAN5w@mail.gmail.com>
Subject: Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Minor correction: the encoded URL in the staging repo link was wrong.
The correct repo is:
https://repository.apache.org/content/repositories/orgapachespark-1025/


On Wed, Aug 6, 2014 at 11:23 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:
>
> Hi All,
>
> I've packaged and published a snapshot release of Spark 1.1 for testing. =
This is being distributed to the community for QA and preview purposes. It =
is not yet an official RC for voting. Going forward, we'll do preview relea=
ses like this for testing ahead of official votes.
>
> The tag of this release is v1.1.0-snapshot1 (commit d428d8):
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dd428=
d88418d385d1d04e1b0adcb6b068efe9c7b0
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1025/
>
> NOTE: Due to SPARK-2899, docs are not yet available for this release. Doc=
s will be posted ASAP.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 14:08:25 2014
Return-Path: <dev-return-8766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EF2AC2B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 14:08:25 +0000 (UTC)
Received: (qmail 13811 invoked by uid 500); 7 Aug 2014 14:08:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13754 invoked by uid 500); 7 Aug 2014 14:08:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13741 invoked by uid 99); 7 Aug 2014 14:08:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 14:08:24 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 14:08:20 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1XFOMJ-0005co-4a
	for dev@spark.incubator.apache.org; Thu, 07 Aug 2014 07:07:59 -0700
Date: Thu, 7 Aug 2014 07:07:59 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407420479129-7730.post@n3.nabble.com>
In-Reply-To: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com>
Subject: Re: Buidling spark in Eclipse Kepler
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Ron,

I was able to build core in Eclipse following these steps:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse

I was working only on core, so I know that works in Eclipse Juno.
I haven't tried yarn or other Eclipse releases.
Are you able to build *core* in Eclipse Kepler?

In my view, tool independence is a good thing.
I'll do what I can to support Eclipse.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Buidling-spark-in-Eclipse-Kepler-tp7712p7730.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 17:30:09 2014
Return-Path: <dev-return-8767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4411CD3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 17:30:09 +0000 (UTC)
Received: (qmail 89841 invoked by uid 500); 7 Aug 2014 17:30:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89778 invoked by uid 500); 7 Aug 2014 17:30:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89756 invoked by uid 99); 7 Aug 2014 17:30:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 17:30:08 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 17:30:07 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1XFRVW-00048E-4G
	for dev@spark.incubator.apache.org; Thu, 07 Aug 2014 10:29:42 -0700
Date: Thu, 7 Aug 2014 10:29:42 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407432582119-7731.post@n3.nabble.com>
In-Reply-To: <CAPud8ToQgKRt5ZYeLe0MNm0Mr=+e8WpLtM1i8f6A8q_-uwG-Rg@mail.gmail.com>
References: <CAPud8ToQgKRt5ZYeLe0MNm0Mr=+e8WpLtM1i8f6A8q_-uwG-Rg@mail.gmail.com>
Subject: Re: Unit test best practice for Spark-derived projects
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

How long does it take to get a spark context?
I found that if you don't have a network connection (reverse DNS lookup most
likely), it can take up 30 seconds to start up locally. I think a hosts file
entry is sufficient.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Unit-test-best-practice-for-Spark-derived-projects-tp7704p7731.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 17:46:12 2014
Return-Path: <dev-return-8768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 909E6CDE5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 17:46:12 +0000 (UTC)
Received: (qmail 46306 invoked by uid 500); 7 Aug 2014 17:46:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46245 invoked by uid 500); 7 Aug 2014 17:46:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46215 invoked by uid 99); 7 Aug 2014 17:46:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 17:46:11 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 17:46:10 +0000
Received: by mail-oi0-f41.google.com with SMTP id a141so2938758oig.0
        for <dev@spark.incubator.apache.org>; Thu, 07 Aug 2014 10:45:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=TwODer9GcTtz1BX1euqsMXsQwFZhjx7Akoep5tSXnDA=;
        b=y55rZgKV2OpQ/p/JxpeWRN5cNqUX6kErN70KlJIf4xcI4mdcfhzv0eeVc+4+SAnqnf
         fSu8rdHyL8OLqXTjyC0bKfaXHA7ajZDzN35GB+ez2p1DDaK7dt8CKEKdR8WxIRSeifa3
         aE1Zt7y7NVq9hIS+WHwNIRaCWXWgP6JwEBTmA23Ykp2ZEraJiXM1RAdv2CUAESR9ZfA3
         omnv/3T6WyBEnpADIdGXNZztMbUkU1/Fz5HKUxEtdd33CmOSARYvUX5kyeko/YFuoZp5
         lOfw6b0/Wum0nBXq6/JyCvZXjhDXjoWcXP6y48BUYi2czIm/R6Hq3W6FESmwJjjGZf71
         GusA==
MIME-Version: 1.0
X-Received: by 10.60.33.35 with SMTP id o3mr25440741oei.7.1407433545616; Thu,
 07 Aug 2014 10:45:45 -0700 (PDT)
Received: by 10.76.35.134 with HTTP; Thu, 7 Aug 2014 10:45:45 -0700 (PDT)
In-Reply-To: <1407432582119-7731.post@n3.nabble.com>
References: <CAPud8ToQgKRt5ZYeLe0MNm0Mr=+e8WpLtM1i8f6A8q_-uwG-Rg@mail.gmail.com>
	<1407432582119-7731.post@n3.nabble.com>
Date: Thu, 7 Aug 2014 10:45:45 -0700
Message-ID: <CAPud8TqwZVo+gdN2cta-c=HDPq3cuPJssYabgZVJEyPLAHVApA@mail.gmail.com>
Subject: Re: Unit test best practice for Spark-derived projects
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: Madhu <madhu@madhu.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e011842a68812d905000da8d7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011842a68812d905000da8d7
Content-Type: text/plain; charset=UTF-8

Thanks.

let me check this hypothesis (i have dhcp connection on a private net but
consequently not sure if there's an inverse).


On Thu, Aug 7, 2014 at 10:29 AM, Madhu <madhu@madhu.com> wrote:

> How long does it take to get a spark context?
> I found that if you don't have a network connection (reverse DNS lookup
> most
> likely), it can take up 30 seconds to start up locally. I think a hosts
> file
> entry is sufficient.
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Unit-test-best-practice-for-Spark-derived-projects-tp7704p7731.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e011842a68812d905000da8d7--

From dev-return-8769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 21:36:27 2014
Return-Path: <dev-return-8769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0371911866
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 21:36:27 +0000 (UTC)
Received: (qmail 42803 invoked by uid 500); 7 Aug 2014 21:36:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42748 invoked by uid 500); 7 Aug 2014 21:36:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42736 invoked by uid 99); 7 Aug 2014 21:36:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 21:36:24 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 21:36:22 +0000
Received: by mail-ob0-f173.google.com with SMTP id vb8so3394066obc.32
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 14:35:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zEitBd+E3YPE7Os3ApVP5+sYqAahy+oP9TLvswMy4nQ=;
        b=l5iO6/fWlRY6F/sPS/T3ZrR7yjoG6ff9rcwmnFKqQqNt0y4OJ1jq0OsQRUU9jEydl+
         slG/s2xV2rqTbxFK5hEG68K6ysFHmuWpQJhcfLkkV45eGM5zbPbYeJIVIsA1kz9gMtrD
         Z3+5QrDPwoFAWgA5ovdn+tNWdYqzTGtndFMZazOrtxQb2Txd350htBZkQbHq/Zwgv5io
         EYMdd9wixCpzBlESIh+3hdToUoEZKolGBNhZdxLrSCrOkcL6Ig/l2i4/wEmEcGpuGKY4
         AQlMwN/24R3Nuhof75ZwO3tT86I3KthfFAgG60hXgUjaBNhE6KXxX7Db5fIkZYr3AuYQ
         XIbg==
X-Gm-Message-State: ALoCoQmI5Et3mLXC5kmb8B1XvR4emehFL4xEwPaW2hlKO/g+afu1CBh2zAqiyPuJA22RhacwlmWW
MIME-Version: 1.0
X-Received: by 10.182.144.131 with SMTP id sm3mr25474160obb.3.1407447356631;
 Thu, 07 Aug 2014 14:35:56 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Thu, 7 Aug 2014 14:35:56 -0700 (PDT)
In-Reply-To: <CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
Date: Thu, 7 Aug 2014 16:35:56 -0500
Message-ID: <CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Cody Koeninger <cody@koeninger.org>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158aba2bb8379050010df34
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158aba2bb8379050010df34
Content-Type: text/plain; charset=UTF-8

Just wanted to check in on this, see if I should file a bug report
regarding the mesos argument propagation.


On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org> wrote:

> 1. I've tried with and without escaping equals sign, it doesn't affect the
> results.
>
> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for getting
> system properties set in the local shell (although not for executors).
>
> 3. We're using the default fine-grained mesos mode, not setting
> spark.mesos.coarse, so it doesn't seem immediately related to that ticket.
> Should I file a bug report?
>
>
> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> The third issue may be related to this:
>> https://issues.apache.org/jira/browse/SPARK-2022
>>
>> We can take a look at this during the bug fix period for the 1.1
>> release next week. If we come up with a fix we can backport it into
>> the 1.0 branch also.
>>
>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > Thanks for digging around here. I think there are a few distinct issues.
>> >
>> > 1. Properties containing the '=' character need to be escaped.
>> > I was able to load properties fine as long as I escape the '='
>> > character. But maybe we should document this:
>> >
>> > == spark-defaults.conf ==
>> > spark.foo a\=B
>> > == shell ==
>> > scala> sc.getConf.get("spark.foo")
>> > res2: String = a=B
>> >
>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
>> > don't affect the driver when running in client mode (always the case
>> > for mesos). We should probably document this. In this case you need to
>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
>> >
>> > 3. Arguments aren't propagated on Mesos (this might be because of the
>> > other issues, or a separate bug).
>> >
>> > - Patrick
>> >
>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>> >> In addition, spark.executor.extraJavaOptions does not seem to behave
>> as I
>> >> would expect; java arguments don't seem to be propagated to executors.
>> >>
>> >>
>> >> $ cat conf/spark-defaults.conf
>> >>
>> >> spark.master
>> >>
>> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> >>
>> >>
>> >> $ ./bin/spark-shell
>> >>
>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
>> >> res0: String = -Dfoo.bar.baz=23
>> >>
>> >> scala> sc.parallelize(1 to 100).map{ i => (
>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
>> >>      |  System.getProperty("foo.bar.baz")
>> >>      | )}.collect
>> >>
>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
>> >> (dn-02.mxstg,null), ...
>> >>
>> >>
>> >>
>> >> Note that this is a mesos deployment, although I wouldn't expect that
>> to
>> >> affect the availability of spark.driver.extraJavaOptions in a local
>> spark
>> >> shell.
>> >>
>> >>
>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>> >>
>> >>> Either whitespace or equals sign are valid properties file formats.
>> >>> Here's an example:
>> >>>
>> >>> $ cat conf/spark-defaults.conf
>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> >>>
>> >>> $ ./bin/spark-shell -v
>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> >>> Adding default property:
>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>> >>>
>> >>>
>> >>> scala>  System.getProperty("foo.bar.baz")
>> >>> res0: String = null
>> >>>
>> >>>
>> >>> If you add double quotes, the resulting string value will have double
>> >>> quotes.
>> >>>
>> >>>
>> >>> $ cat conf/spark-defaults.conf
>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>> >>>
>> >>> $ ./bin/spark-shell -v
>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> >>> Adding default property:
>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>> >>>
>> >>> scala>  System.getProperty("foo.bar.baz")
>> >>> res0: String = null
>> >>>
>> >>>
>> >>> Neither one of those affects the issue; the underlying problem in my
>> case
>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
>> >>> spark-defaults.conf before the java process is started.
>> >>>
>> >>> Here's an example of the process running when only
>> spark-defaults.conf is
>> >>> being used:
>> >>>
>> >>> $ ps -ef | grep spark
>> >>>
>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
>> ./bin/spark-shell -v
>> >>>
>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
>> /usr/local/java/bin/java
>> >>> -cp
>> >>>
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>> >>> org.apache.spark.repl.Main
>> >>>
>> >>>
>> >>> Here's an example of it when the command line --driver-java-options is
>> >>> used (and thus things work):
>> >>>
>> >>>
>> >>> $ ps -ef | grep spark
>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
>> ./bin/spark-shell -v
>> >>> --driver-java-options -Dfoo.bar.baz=23
>> >>>
>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
>> /usr/local/java/bin/java
>> >>> -cp
>> >>>
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
>> org.apache.spark.repl.Main
>> >>>
>> >>>
>> >>>
>> >>>
>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
>> >>> wrote:
>> >>>
>> >>>> Cody - in your example you are using the '=' character, but in our
>> >>>> documentation and tests we use a whitespace to separate the key and
>> >>>> value in the defaults file.
>> >>>>
>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
>> >>>>
>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> >>>>
>> >>>> I'm not sure if the java properties file parser will try to interpret
>> >>>> the equals sign. If so you might need to do this.
>> >>>>
>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>> >>>>
>> >>>> Do those work for you?
>> >>>>
>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com
>> >
>> >>>> wrote:
>> >>>> > Hi Cody,
>> >>>> >
>> >>>> > Could you file a bug for this if there isn't one already?
>> >>>> >
>> >>>> > For system properties SparkSubmit should be able to read those
>> >>>> > settings and do the right thing, but that obviously won't work for
>> >>>> > other JVM options... the current code should work fine in cluster
>> mode
>> >>>> > though, since the driver is a different process. :-)
>> >>>> >
>> >>>> >
>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
>> cody@koeninger.org>
>> >>>> wrote:
>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
>> properties
>> >>>> via
>> >>>> >> -D.
>> >>>> >>
>> >>>> >> This was used for properties that varied on a
>> >>>> per-deployment-environment
>> >>>> >> basis, but needed to be available in the spark shell and workers.
>> >>>> >>
>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
>> deprecated,
>> >>>> and
>> >>>> >> replaced by spark-defaults.conf and command line arguments to
>> >>>> spark-submit
>> >>>> >> or spark-shell.
>> >>>> >>
>> >>>> >> However, setting spark.driver.extraJavaOptions and
>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
>> >>>> replacement
>> >>>> >> for SPARK_JAVA_OPTS:
>> >>>> >>
>> >>>> >>
>> >>>> >> $ cat conf/spark-defaults.conf
>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>> >>>> >>
>> >>>> >> $ ./bin/spark-shell
>> >>>> >>
>> >>>> >> scala> System.getProperty("foo.bar.baz")
>> >>>> >> res0: String = null
>> >>>> >>
>> >>>> >>
>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>> >>>> >>
>> >>>> >> scala> System.getProperty("foo.bar.baz")
>> >>>> >> res0: String = 23
>> >>>> >>
>> >>>> >>
>> >>>> >> Looking through the shell scripts for spark-submit and
>> spark-class, I
>> >>>> can
>> >>>> >> see why this is; parsing spark-defaults.conf from bash could be
>> >>>> brittle.
>> >>>> >>
>> >>>> >> But from an ergonomic point of view, it's a step back to go from a
>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
>> >>>> command
>> >>>> >> line arguments.
>> >>>> >>
>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell with
>> the
>> >>>> >> appropriate arguments, but I wanted to bring the issue up to see
>> if
>> >>>> anyone
>> >>>> >> else had run into it,
>> >>>> >> or had any direction for a general solution (beyond parsing java
>> >>>> properties
>> >>>> >> files from bash).
>> >>>> >
>> >>>> >
>> >>>> >
>> >>>> > --
>> >>>> > Marcelo
>> >>>>
>> >>>
>> >>>
>>
>
>

--089e0158aba2bb8379050010df34--

From dev-return-8770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 21:47:40 2014
Return-Path: <dev-return-8770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E9E0118C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 21:47:40 +0000 (UTC)
Received: (qmail 84393 invoked by uid 500); 7 Aug 2014 21:47:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84331 invoked by uid 500); 7 Aug 2014 21:47:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84320 invoked by uid 99); 7 Aug 2014 21:47:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 21:47:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 21:47:35 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so5101485qgd.37
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 14:47:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=osr4vgW1yHzboOIy5gChqGqCjQGF08DJmGzLJomq/mY=;
        b=UTpC+bIh/eN80Nh+c6l3DSvqMH3xedIO4RnRQoFeNE8ajUxQ+tjPlr8NHgzHin8BiW
         ekaP8wtne1GCDMW40+1gnCBCslxUJJL0cxaCwO1YYCSwcDA1ug4KU6yyS3JC0jeeWeYP
         GqYnhwHQBVfzlvaUUYV10V0YmaU3cah/dcB7a/Derxim1WNS8CjIuydijctGiM4zPxdi
         B0xUlWhnGyKmLFFzAA3RolZHdLvPOMjIRLJLYtc6YQRW8bSjK+KVOFd/px8J1DfKKab0
         H4xeIxfW8LBXe4npU5jgWRynw5zG9/hFq80oGrg5csXDwE0tencnAwMFyFnaGBaDSxCO
         YyTw==
X-Gm-Message-State: ALoCoQkhdiN8hEczR/Y+jIsgGqi0QvlYCMwOgpbW+Xp8IUXkm6MCJNjILL8zWt/HypJ4OBBr8U7x
MIME-Version: 1.0
X-Received: by 10.224.129.201 with SMTP id p9mr31786235qas.75.1407448034724;
 Thu, 07 Aug 2014 14:47:14 -0700 (PDT)
Received: by 10.229.199.194 with HTTP; Thu, 7 Aug 2014 14:47:14 -0700 (PDT)
In-Reply-To: <CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
Date: Thu, 7 Aug 2014 14:47:14 -0700
Message-ID: <CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Andrew has been working on a fix:
https://github.com/apache/spark/pull/1770

On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org> wrote:
> Just wanted to check in on this, see if I should file a bug report
> regarding the mesos argument propagation.
>
>
> On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org> wrote:
>
>> 1. I've tried with and without escaping equals sign, it doesn't affect the
>> results.
>>
>> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for getting
>> system properties set in the local shell (although not for executors).
>>
>> 3. We're using the default fine-grained mesos mode, not setting
>> spark.mesos.coarse, so it doesn't seem immediately related to that ticket.
>> Should I file a bug report?
>>
>>
>> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> The third issue may be related to this:
>>> https://issues.apache.org/jira/browse/SPARK-2022
>>>
>>> We can take a look at this during the bug fix period for the 1.1
>>> release next week. If we come up with a fix we can backport it into
>>> the 1.0 branch also.
>>>
>>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>> > Thanks for digging around here. I think there are a few distinct issues.
>>> >
>>> > 1. Properties containing the '=' character need to be escaped.
>>> > I was able to load properties fine as long as I escape the '='
>>> > character. But maybe we should document this:
>>> >
>>> > == spark-defaults.conf ==
>>> > spark.foo a\=B
>>> > == shell ==
>>> > scala> sc.getConf.get("spark.foo")
>>> > res2: String = a=B
>>> >
>>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
>>> > don't affect the driver when running in client mode (always the case
>>> > for mesos). We should probably document this. In this case you need to
>>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
>>> >
>>> > 3. Arguments aren't propagated on Mesos (this might be because of the
>>> > other issues, or a separate bug).
>>> >
>>> > - Patrick
>>> >
>>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>> >> In addition, spark.executor.extraJavaOptions does not seem to behave
>>> as I
>>> >> would expect; java arguments don't seem to be propagated to executors.
>>> >>
>>> >>
>>> >> $ cat conf/spark-defaults.conf
>>> >>
>>> >> spark.master
>>> >>
>>> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
>>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
>>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> >>
>>> >>
>>> >> $ ./bin/spark-shell
>>> >>
>>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
>>> >> res0: String = -Dfoo.bar.baz=23
>>> >>
>>> >> scala> sc.parallelize(1 to 100).map{ i => (
>>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
>>> >>      |  System.getProperty("foo.bar.baz")
>>> >>      | )}.collect
>>> >>
>>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
>>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
>>> >> (dn-02.mxstg,null), ...
>>> >>
>>> >>
>>> >>
>>> >> Note that this is a mesos deployment, although I wouldn't expect that
>>> to
>>> >> affect the availability of spark.driver.extraJavaOptions in a local
>>> spark
>>> >> shell.
>>> >>
>>> >>
>>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>> >>
>>> >>> Either whitespace or equals sign are valid properties file formats.
>>> >>> Here's an example:
>>> >>>
>>> >>> $ cat conf/spark-defaults.conf
>>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> >>>
>>> >>> $ ./bin/spark-shell -v
>>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> >>> Adding default property:
>>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>> >>>
>>> >>>
>>> >>> scala>  System.getProperty("foo.bar.baz")
>>> >>> res0: String = null
>>> >>>
>>> >>>
>>> >>> If you add double quotes, the resulting string value will have double
>>> >>> quotes.
>>> >>>
>>> >>>
>>> >>> $ cat conf/spark-defaults.conf
>>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>> >>>
>>> >>> $ ./bin/spark-shell -v
>>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> >>> Adding default property:
>>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>>> >>>
>>> >>> scala>  System.getProperty("foo.bar.baz")
>>> >>> res0: String = null
>>> >>>
>>> >>>
>>> >>> Neither one of those affects the issue; the underlying problem in my
>>> case
>>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
>>> >>> spark-defaults.conf before the java process is started.
>>> >>>
>>> >>> Here's an example of the process running when only
>>> spark-defaults.conf is
>>> >>> being used:
>>> >>>
>>> >>> $ ps -ef | grep spark
>>> >>>
>>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
>>> ./bin/spark-shell -v
>>> >>>
>>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
>>> /usr/local/java/bin/java
>>> >>> -cp
>>> >>>
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>>> >>> org.apache.spark.repl.Main
>>> >>>
>>> >>>
>>> >>> Here's an example of it when the command line --driver-java-options is
>>> >>> used (and thus things work):
>>> >>>
>>> >>>
>>> >>> $ ps -ef | grep spark
>>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
>>> ./bin/spark-shell -v
>>> >>> --driver-java-options -Dfoo.bar.baz=23
>>> >>>
>>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
>>> /usr/local/java/bin/java
>>> >>> -cp
>>> >>>
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
>>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
>>> org.apache.spark.repl.Main
>>> >>>
>>> >>>
>>> >>>
>>> >>>
>>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
>>> >>> wrote:
>>> >>>
>>> >>>> Cody - in your example you are using the '=' character, but in our
>>> >>>> documentation and tests we use a whitespace to separate the key and
>>> >>>> value in the defaults file.
>>> >>>>
>>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
>>> >>>>
>>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> >>>>
>>> >>>> I'm not sure if the java properties file parser will try to interpret
>>> >>>> the equals sign. If so you might need to do this.
>>> >>>>
>>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>> >>>>
>>> >>>> Do those work for you?
>>> >>>>
>>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com
>>> >
>>> >>>> wrote:
>>> >>>> > Hi Cody,
>>> >>>> >
>>> >>>> > Could you file a bug for this if there isn't one already?
>>> >>>> >
>>> >>>> > For system properties SparkSubmit should be able to read those
>>> >>>> > settings and do the right thing, but that obviously won't work for
>>> >>>> > other JVM options... the current code should work fine in cluster
>>> mode
>>> >>>> > though, since the driver is a different process. :-)
>>> >>>> >
>>> >>>> >
>>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
>>> cody@koeninger.org>
>>> >>>> wrote:
>>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
>>> properties
>>> >>>> via
>>> >>>> >> -D.
>>> >>>> >>
>>> >>>> >> This was used for properties that varied on a
>>> >>>> per-deployment-environment
>>> >>>> >> basis, but needed to be available in the spark shell and workers.
>>> >>>> >>
>>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
>>> deprecated,
>>> >>>> and
>>> >>>> >> replaced by spark-defaults.conf and command line arguments to
>>> >>>> spark-submit
>>> >>>> >> or spark-shell.
>>> >>>> >>
>>> >>>> >> However, setting spark.driver.extraJavaOptions and
>>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
>>> >>>> replacement
>>> >>>> >> for SPARK_JAVA_OPTS:
>>> >>>> >>
>>> >>>> >>
>>> >>>> >> $ cat conf/spark-defaults.conf
>>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>> >>>> >>
>>> >>>> >> $ ./bin/spark-shell
>>> >>>> >>
>>> >>>> >> scala> System.getProperty("foo.bar.baz")
>>> >>>> >> res0: String = null
>>> >>>> >>
>>> >>>> >>
>>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>>> >>>> >>
>>> >>>> >> scala> System.getProperty("foo.bar.baz")
>>> >>>> >> res0: String = 23
>>> >>>> >>
>>> >>>> >>
>>> >>>> >> Looking through the shell scripts for spark-submit and
>>> spark-class, I
>>> >>>> can
>>> >>>> >> see why this is; parsing spark-defaults.conf from bash could be
>>> >>>> brittle.
>>> >>>> >>
>>> >>>> >> But from an ergonomic point of view, it's a step back to go from a
>>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
>>> >>>> command
>>> >>>> >> line arguments.
>>> >>>> >>
>>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell with
>>> the
>>> >>>> >> appropriate arguments, but I wanted to bring the issue up to see
>>> if
>>> >>>> anyone
>>> >>>> >> else had run into it,
>>> >>>> >> or had any direction for a general solution (beyond parsing java
>>> >>>> properties
>>> >>>> >> files from bash).
>>> >>>> >
>>> >>>> >
>>> >>>> >
>>> >>>> > --
>>> >>>> > Marcelo
>>> >>>>
>>> >>>
>>> >>>
>>>
>>
>>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 23:29:27 2014
Return-Path: <dev-return-8771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62AEC11CA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 23:29:27 +0000 (UTC)
Received: (qmail 42004 invoked by uid 500); 7 Aug 2014 23:29:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41947 invoked by uid 500); 7 Aug 2014 23:29:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41935 invoked by uid 99); 7 Aug 2014 23:29:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 23:29:26 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 23:29:25 +0000
Received: by mail-ob0-f172.google.com with SMTP id wn1so3460842obc.17
        for <dev@spark.incubator.apache.org>; Thu, 07 Aug 2014 16:29:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=vcPKFb/9vHQfYMuoXq9t8BLuO7SYgw7w7KKvzDDkyY8=;
        b=QI8CiwSjBVcvhYoUeeRR6kk8ATSEqVB57c4b9Af7larwLTHm2RS58UEfmtROMdWHBO
         kPHU3wnHQbvfcniL2PFYoIOXnqUzqo/mJpK6Sns1WFaeQbj+oj22UqeRAp4lFUyuhwP5
         Cvw7HLbuazkwscMbGmHCxtWQztwwBYMFB5Of079wCCK6KR5moKd5utSvfX9/bx+LJem1
         aP7rUNluepGgxji7H8JOrjXYzgBkZdtxbQVsjdzOwXMQj3oXb6z/yuK5QCTMIaMiiHl2
         QgMyvOCf4Oi5/ACDA+of1ncyqZDnw6pDDldEY6hcWYykVcGikgEwxK7F8hnzXIGanRib
         Lo8g==
MIME-Version: 1.0
X-Received: by 10.182.181.42 with SMTP id dt10mr26733762obc.69.1407454140377;
 Thu, 07 Aug 2014 16:29:00 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 7 Aug 2014 16:29:00 -0700 (PDT)
In-Reply-To: <CAPud8TqwZVo+gdN2cta-c=HDPq3cuPJssYabgZVJEyPLAHVApA@mail.gmail.com>
References: <CAPud8ToQgKRt5ZYeLe0MNm0Mr=+e8WpLtM1i8f6A8q_-uwG-Rg@mail.gmail.com>
	<1407432582119-7731.post@n3.nabble.com>
	<CAPud8TqwZVo+gdN2cta-c=HDPq3cuPJssYabgZVJEyPLAHVApA@mail.gmail.com>
Date: Thu, 7 Aug 2014 16:29:00 -0700
Message-ID: <CABPQxsvMKPJCKsWsbNbbnO+c55EfkzcVMEONVO3dJXph3GiE5g@mail.gmail.com>
Subject: Re: Unit test best practice for Spark-derived projects
From: Patrick Wendell <pwendell@gmail.com>
To: Dmitriy Lyubimov <dlieu.7@gmail.com>
Cc: Madhu <madhu@madhu.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

In the past I've found if I do a jstack when running some tests, it
sits forever inside of a hostname resolution step or something. I
never narrowed it down, though.

- Patrick

On Thu, Aug 7, 2014 at 10:45 AM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:
> Thanks.
>
> let me check this hypothesis (i have dhcp connection on a private net but
> consequently not sure if there's an inverse).
>
>
> On Thu, Aug 7, 2014 at 10:29 AM, Madhu <madhu@madhu.com> wrote:
>
>> How long does it take to get a spark context?
>> I found that if you don't have a network connection (reverse DNS lookup
>> most
>> likely), it can take up 30 seconds to start up locally. I think a hosts
>> file
>> entry is sufficient.
>>
>>
>>
>> -----
>> --
>> Madhu
>> https://www.linkedin.com/in/msiddalingaiah
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Unit-test-best-practice-for-Spark-derived-projects-tp7704p7731.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug  7 23:59:21 2014
Return-Path: <dev-return-8772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AC96C11D7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Aug 2014 23:59:21 +0000 (UTC)
Received: (qmail 835 invoked by uid 500); 7 Aug 2014 23:59:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 776 invoked by uid 500); 7 Aug 2014 23:59:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 763 invoked by uid 99); 7 Aug 2014 23:59:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 23:59:20 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zlgonzalez@yahoo.com designates 216.109.114.175 as permitted sender)
Received: from [216.109.114.175] (HELO nm41-vm8.bullet.mail.bf1.yahoo.com) (216.109.114.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Aug 2014 23:59:16 +0000
Received: from [66.196.81.173] by nm41.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 23:58:51 -0000
Received: from [98.139.212.226] by tm19.bullet.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 23:58:51 -0000
Received: from [127.0.0.1] by omp1035.mail.bf1.yahoo.com with NNFMP; 07 Aug 2014 23:58:51 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 748083.7190.bm@omp1035.mail.bf1.yahoo.com
Received: (qmail 76585 invoked by uid 60001); 7 Aug 2014 23:58:51 -0000
X-YMail-OSG: wGRpXfAVM1kUBprFtkpqESROfh.U6KwXnc6YZGAHyN7.eaw
 MndIC2m4oHLWdm_4UYKfkIl8XkeLicgYG79huKh5GZabiDXtTEplh4sABKp2
 cy7nmzu_EKwjd115cyzjhZcFw74sTcgiRUasR.zEQWD7lREE6E37MTxihMSn
 cjNFIwN5_VicIUw.UVSqikwcY3lkWJQoQai2sIlpIx.jbCf.hruYD3SEhV9q
 wEtKnoz1iWraVIXJuC4uvmXlZh61Y.l7O0cIWz8Byz3IFhmb5ZcYulrS1JkM
 6QEnT_BoXmpvNCI_IPPVgz.YfcNhgBP9VGt00z6fRD94D7vAct7cOtoxNOPX
 urNWbCKs9.LP59zv9OukDmgFpH4n8v6Eavn9KATmXPy6_2jZoYsZ1RHd2Z6e
 5lb68BiS62We17hxJkY73aDmvaIq.b_rf.vOLUnioML_kPKuxxE7EG6pbG_Y
 .sxEaWTu6ZM7pwC6GUis3RM3rO5fB4dL1L_xVnIQH1ZwC9r7Z20we06sq8Iq
 COELF7Zj5R86bPZBpUBbFyxS2MnQYPglVqriqYX5jvyUsUcE-
Received: from [216.38.154.194] by web162406.mail.bf1.yahoo.com via HTTP; Thu, 07 Aug 2014 16:58:51 PDT
X-Rocket-MIMEInfo: 002.001,U28gSSBvcGVuZWQgaXQgYXMgYSBtYXZlbiBwcm9qZWN0IChJIG9wZW5lZCBpdCB1c2luZyB0aGUgdG9wLWxldmVsIHBvbS54bWwgZmlsZSksIGJ1dCByZWJ1aWxkaW5nIHRoZSBwcm9qZWN0IGVuZHMgdXAgaW4gYWxsIHNvcnRzIG9mIGVycm9ycyBhYm91dCB1bnJlc29sdmVkIGRlcGVuZGVuY2llcy4KClRoYW5rcywKUm9uCgoKT24gV2VkbmVzZGF5LCBBdWd1c3QgNiwgMjAxNCAxMToxNSBQTSwgU2VhbiBPd2VuIDxzb3dlbkBjbG91ZGVyYS5jb20.IHdyb3RlOgogCgoKKERvbid0IHVzZSBnZW4taWRlYSwganUBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
References: <1407353361.9795.YahooMailNeo@web162401.mail.bf1.yahoo.com> <1407383637.74750.YahooMailNeo@web162402.mail.bf1.yahoo.com> <CAMAsSdLH6NtdTizQ+hNqZ0A4OFDyLHmGMM6SmY2r9c3-p1wqHw@mail.gmail.com>
Message-ID: <1407455931.75335.YahooMailNeo@web162406.mail.bf1.yahoo.com>
Date: Thu, 7 Aug 2014 16:58:51 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: Re: Buidling spark in Eclipse Kepler
To: Sean Owen <sowen@cloudera.com>
Cc: Dev <dev@spark.apache.org>
In-Reply-To: <CAMAsSdLH6NtdTizQ+hNqZ0A4OFDyLHmGMM6SmY2r9c3-p1wqHw@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-747684034-1133169280-1407455931=:75335"
X-Virus-Checked: Checked by ClamAV on apache.org

---747684034-1133169280-1407455931=:75335
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

So I opened it as a maven project (I opened it using the top-level pom.xml =
file), but rebuilding the project ends up in all sorts of errors about unre=
solved dependencies.=0A=0AThanks,=0ARon=0A=0A=0AOn Wednesday, August 6, 201=
4 11:15 PM, Sean Owen <sowen@cloudera.com> wrote:=0A =0A=0A=0A(Don't use ge=
n-idea, just open it directly as a Maven project in IntelliJ.)=0A=0A=0AOn T=
hu, Aug 7, 2014 at 4:53 AM, Ron Gonzalez=0A<zlgonzalez@yahoo.com.invalid> w=
rote:=0A> So I downloaded community edition of IntelliJ, and ran sbt/sbt ge=
n-idea.=0A> I then imported the pom.xml file.=0A> I'm still getting all sor=
ts of errors from IntelliJ about unresolved dependencies.=0A> Any suggestio=
ns?=0A>=0A> Thanks,=0A> Ron=0A>=0A>=0A> On Wednesday, August 6, 2014 12:29 =
PM, Ron Gonzalez <zlgonzalez@yahoo.com.INVALID> wrote:=0A>=0A>=0A>=0A> Hi,=
=0A>=A0  I'm trying to get the apache spark trunk compiling in my Eclipse, =
but I can't seem to get it going. In particular, I've tried sbt/sbt eclipse=
, but it doesn't seem to create the eclipse pieces for yarn and other proje=
cts. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt ecl=
ipse just for yarn fails. Is there some documentation available for eclipse=
? I've gone through the ones on the site, but to no avail.=0A>=A0  Any tips=
?=0A>=0A> Thanks,=0A> Ron=0A=0A--------------------------------------------=
-------------------------=0ATo unsubscribe, e-mail: dev-unsubscribe@spark.a=
pache.org=0AFor additional commands, e-mail: dev-help@spark.apache.org
---747684034-1133169280-1407455931=:75335--

From dev-return-8773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 00:31:27 2014
Return-Path: <dev-return-8773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7083411E90
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 00:31:27 +0000 (UTC)
Received: (qmail 56465 invoked by uid 500); 8 Aug 2014 00:31:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56408 invoked by uid 500); 8 Aug 2014 00:31:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56396 invoked by uid 99); 8 Aug 2014 00:31:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 00:31:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 00:31:24 +0000
Received: by mail-qg0-f43.google.com with SMTP id a108so5261195qge.2
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 17:30:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ZMmZ0vNxxO7+mfIKXsCw0A6DgLuxQWj6zwRfTXSVlLo=;
        b=k755STX69Tl3uKDAx94FqaNBR7W5/OkTSYV5PKATq3LIAPzZhDX5IKm3MSHPkmNV1v
         dB9eqmVOh2zvx1dgfZ57nqlY5lYGHcpxq+HI9/A3+JM9vIe2wTpKh+aVz51Dx0lNw3pM
         +AroT2zSW40gWYwerqCsZ3qOA4d2OGehKsq3h/p/tj4p/Fg+Ue5tQ6fgN2DLz0fJewB9
         GzuGoYmldYixUkvOr+Whf1eJPAyNT2ph1Ohi/dIuh6WpQorktKCLmIBoQ1aqNuAXKsjd
         dvURFNQLLYq1HYza+rKQttCy/m2JaqD0w3B8oLeOxI18UhhOUnOGXf7KON53ojMzk+qG
         J24g==
MIME-Version: 1.0
X-Received: by 10.140.43.245 with SMTP id e108mr18097513qga.76.1407457859035;
 Thu, 07 Aug 2014 17:30:59 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Thu, 7 Aug 2014 17:30:58 -0700 (PDT)
In-Reply-To: <CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
	<CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
Date: Thu, 7 Aug 2014 20:30:58 -0400
Message-ID: <CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Gary Malouf <malouf.gary@gmail.com>
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Cody Koeninger <cody@koeninger.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a6664b9682b0500135180
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a6664b9682b0500135180
Content-Type: text/plain; charset=UTF-8

Can this be cherry-picked for 1.1 if everything works out?  In my opinion,
it could be qualified as a bug fix.


On Thu, Aug 7, 2014 at 5:47 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:

> Andrew has been working on a fix:
> https://github.com/apache/spark/pull/1770
>
> On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org> wrote:
> > Just wanted to check in on this, see if I should file a bug report
> > regarding the mesos argument propagation.
> >
> >
> > On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org>
> wrote:
> >
> >> 1. I've tried with and without escaping equals sign, it doesn't affect
> the
> >> results.
> >>
> >> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for getting
> >> system properties set in the local shell (although not for executors).
> >>
> >> 3. We're using the default fine-grained mesos mode, not setting
> >> spark.mesos.coarse, so it doesn't seem immediately related to that
> ticket.
> >> Should I file a bug report?
> >>
> >>
> >> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >>> The third issue may be related to this:
> >>> https://issues.apache.org/jira/browse/SPARK-2022
> >>>
> >>> We can take a look at this during the bug fix period for the 1.1
> >>> release next week. If we come up with a fix we can backport it into
> >>> the 1.0 branch also.
> >>>
> >>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <pwendell@gmail.com>
> >>> wrote:
> >>> > Thanks for digging around here. I think there are a few distinct
> issues.
> >>> >
> >>> > 1. Properties containing the '=' character need to be escaped.
> >>> > I was able to load properties fine as long as I escape the '='
> >>> > character. But maybe we should document this:
> >>> >
> >>> > == spark-defaults.conf ==
> >>> > spark.foo a\=B
> >>> > == shell ==
> >>> > scala> sc.getConf.get("spark.foo")
> >>> > res2: String = a=B
> >>> >
> >>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
> >>> > don't affect the driver when running in client mode (always the case
> >>> > for mesos). We should probably document this. In this case you need
> to
> >>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
> >>> >
> >>> > 3. Arguments aren't propagated on Mesos (this might be because of the
> >>> > other issues, or a separate bug).
> >>> >
> >>> > - Patrick
> >>> >
> >>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org>
> >>> wrote:
> >>> >> In addition, spark.executor.extraJavaOptions does not seem to behave
> >>> as I
> >>> >> would expect; java arguments don't seem to be propagated to
> executors.
> >>> >>
> >>> >>
> >>> >> $ cat conf/spark-defaults.conf
> >>> >>
> >>> >> spark.master
> >>> >>
> >>>
> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
> >>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
> >>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> >>
> >>> >>
> >>> >> $ ./bin/spark-shell
> >>> >>
> >>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
> >>> >> res0: String = -Dfoo.bar.baz=23
> >>> >>
> >>> >> scala> sc.parallelize(1 to 100).map{ i => (
> >>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
> >>> >>      |  System.getProperty("foo.bar.baz")
> >>> >>      | )}.collect
> >>> >>
> >>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
> >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
> >>> >> (dn-02.mxstg,null), ...
> >>> >>
> >>> >>
> >>> >>
> >>> >> Note that this is a mesos deployment, although I wouldn't expect
> that
> >>> to
> >>> >> affect the availability of spark.driver.extraJavaOptions in a local
> >>> spark
> >>> >> shell.
> >>> >>
> >>> >>
> >>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org
> >
> >>> wrote:
> >>> >>
> >>> >>> Either whitespace or equals sign are valid properties file formats.
> >>> >>> Here's an example:
> >>> >>>
> >>> >>> $ cat conf/spark-defaults.conf
> >>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> >>>
> >>> >>> $ ./bin/spark-shell -v
> >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> >>> Adding default property:
> >>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>> >>>
> >>> >>>
> >>> >>> scala>  System.getProperty("foo.bar.baz")
> >>> >>> res0: String = null
> >>> >>>
> >>> >>>
> >>> >>> If you add double quotes, the resulting string value will have
> double
> >>> >>> quotes.
> >>> >>>
> >>> >>>
> >>> >>> $ cat conf/spark-defaults.conf
> >>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>> >>>
> >>> >>> $ ./bin/spark-shell -v
> >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> >>> Adding default property:
> >>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
> >>> >>>
> >>> >>> scala>  System.getProperty("foo.bar.baz")
> >>> >>> res0: String = null
> >>> >>>
> >>> >>>
> >>> >>> Neither one of those affects the issue; the underlying problem in
> my
> >>> case
> >>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
> >>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
> >>> >>> spark-defaults.conf before the java process is started.
> >>> >>>
> >>> >>> Here's an example of the process running when only
> >>> spark-defaults.conf is
> >>> >>> being used:
> >>> >>>
> >>> >>> $ ps -ef | grep spark
> >>> >>>
> >>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
> >>> ./bin/spark-shell -v
> >>> >>>
> >>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
> >>> /usr/local/java/bin/java
> >>> >>> -cp
> >>> >>>
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
> >>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
> >>> >>> org.apache.spark.repl.Main
> >>> >>>
> >>> >>>
> >>> >>> Here's an example of it when the command line
> --driver-java-options is
> >>> >>> used (and thus things work):
> >>> >>>
> >>> >>>
> >>> >>> $ ps -ef | grep spark
> >>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
> >>> ./bin/spark-shell -v
> >>> >>> --driver-java-options -Dfoo.bar.baz=23
> >>> >>>
> >>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
> >>> /usr/local/java/bin/java
> >>> >>> -cp
> >>> >>>
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
> >>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
> >>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
> >>> org.apache.spark.repl.Main
> >>> >>>
> >>> >>>
> >>> >>>
> >>> >>>
> >>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <
> pwendell@gmail.com>
> >>> >>> wrote:
> >>> >>>
> >>> >>>> Cody - in your example you are using the '=' character, but in our
> >>> >>>> documentation and tests we use a whitespace to separate the key
> and
> >>> >>>> value in the defaults file.
> >>> >>>>
> >>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
> >>> >>>>
> >>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> >>>>
> >>> >>>> I'm not sure if the java properties file parser will try to
> interpret
> >>> >>>> the equals sign. If so you might need to do this.
> >>> >>>>
> >>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>> >>>>
> >>> >>>> Do those work for you?
> >>> >>>>
> >>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <
> vanzin@cloudera.com
> >>> >
> >>> >>>> wrote:
> >>> >>>> > Hi Cody,
> >>> >>>> >
> >>> >>>> > Could you file a bug for this if there isn't one already?
> >>> >>>> >
> >>> >>>> > For system properties SparkSubmit should be able to read those
> >>> >>>> > settings and do the right thing, but that obviously won't work
> for
> >>> >>>> > other JVM options... the current code should work fine in
> cluster
> >>> mode
> >>> >>>> > though, since the driver is a different process. :-)
> >>> >>>> >
> >>> >>>> >
> >>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
> >>> cody@koeninger.org>
> >>> >>>> wrote:
> >>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
> >>> properties
> >>> >>>> via
> >>> >>>> >> -D.
> >>> >>>> >>
> >>> >>>> >> This was used for properties that varied on a
> >>> >>>> per-deployment-environment
> >>> >>>> >> basis, but needed to be available in the spark shell and
> workers.
> >>> >>>> >>
> >>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
> >>> deprecated,
> >>> >>>> and
> >>> >>>> >> replaced by spark-defaults.conf and command line arguments to
> >>> >>>> spark-submit
> >>> >>>> >> or spark-shell.
> >>> >>>> >>
> >>> >>>> >> However, setting spark.driver.extraJavaOptions and
> >>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
> >>> >>>> replacement
> >>> >>>> >> for SPARK_JAVA_OPTS:
> >>> >>>> >>
> >>> >>>> >>
> >>> >>>> >> $ cat conf/spark-defaults.conf
> >>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>> >>>> >>
> >>> >>>> >> $ ./bin/spark-shell
> >>> >>>> >>
> >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>> >>>> >> res0: String = null
> >>> >>>> >>
> >>> >>>> >>
> >>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
> >>> >>>> >>
> >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>> >>>> >> res0: String = 23
> >>> >>>> >>
> >>> >>>> >>
> >>> >>>> >> Looking through the shell scripts for spark-submit and
> >>> spark-class, I
> >>> >>>> can
> >>> >>>> >> see why this is; parsing spark-defaults.conf from bash could be
> >>> >>>> brittle.
> >>> >>>> >>
> >>> >>>> >> But from an ergonomic point of view, it's a step back to go
> from a
> >>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to
> requiring
> >>> >>>> command
> >>> >>>> >> line arguments.
> >>> >>>> >>
> >>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell with
> >>> the
> >>> >>>> >> appropriate arguments, but I wanted to bring the issue up to
> see
> >>> if
> >>> >>>> anyone
> >>> >>>> >> else had run into it,
> >>> >>>> >> or had any direction for a general solution (beyond parsing
> java
> >>> >>>> properties
> >>> >>>> >> files from bash).
> >>> >>>> >
> >>> >>>> >
> >>> >>>> >
> >>> >>>> > --
> >>> >>>> > Marcelo
> >>> >>>>
> >>> >>>
> >>> >>>
> >>>
> >>
> >>
>
>
>
> --
> Marcelo
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113a6664b9682b0500135180--

From dev-return-8774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 02:43:05 2014
Return-Path: <dev-return-8774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50AAC1124E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 02:43:05 +0000 (UTC)
Received: (qmail 18105 invoked by uid 500); 8 Aug 2014 02:43:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18042 invoked by uid 500); 8 Aug 2014 02:43:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18029 invoked by uid 99); 8 Aug 2014 02:43:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:43:04 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:43:01 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so6236290pdj.28
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 19:42:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wSxP/AbtZVSln0REM+NRhNrYS0BIeUPOZnsE8ZmZ8B4=;
        b=TgKy/jsQntmtUPazibiKntLQNsi6hpWN/VDyeH1PzsO4XLItUii8gBi1Ocz6/fcoUx
         HIshTsOZq7jqVfD2NYjMYg10qa+n08QUwG4/cPB0A8+q+ViHINkpC7ssDuFxeCABDZUc
         eoEe4faKeF2WCLCspKKO6X1iFTAdxaEj0k4WuVECvwmMTbCKLdnbZLnSpq3LzceQz4Y4
         hryKpzQ/KpBs9l9T+avmBYdjJ4bX/tei+CD8+EKspIjdNo1Xo2yBAXU59lKZ3Y0TmcWu
         0+YeTyLQVHu2PE8R4S+LWn3ydPXU/ObdfH7jzQroQSyLlntcbOAcBChwdlOTrMkqzTQR
         OZkw==
X-Gm-Message-State: ALoCoQkhW4e7Duizuh2pLeoX6jMe0ozQQd3yGx7M5m+xQ4Q6G2IFV/MorJ/p+w+zfgI7SyzayOlb
MIME-Version: 1.0
X-Received: by 10.70.88.15 with SMTP id bc15mr1003309pdb.63.1407465755853;
 Thu, 07 Aug 2014 19:42:35 -0700 (PDT)
Received: by 10.70.4.133 with HTTP; Thu, 7 Aug 2014 19:42:35 -0700 (PDT)
In-Reply-To: <CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
	<CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
	<CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
Date: Thu, 7 Aug 2014 19:42:35 -0700
Message-ID: <CAMJOb8=nJ6_n-j9qFix1=biYhaZgLbUW1zhbc5UiET7efnh-3w@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Andrew Or <andrew@databricks.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: Marcelo Vanzin <vanzin@cloudera.com>, Cody Koeninger <cody@koeninger.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c21f0e695c2a0500152865
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c21f0e695c2a0500152865
Content-Type: text/plain; charset=UTF-8

Thanks Marcelo, I have moved the changes to a new PR to describe the
problems more clearly: https://github.com/apache/spark/pull/1845

@Gary Yeah, the goal is to get this into 1.1 as a bug fix.


2014-08-07 17:30 GMT-07:00 Gary Malouf <malouf.gary@gmail.com>:

> Can this be cherry-picked for 1.1 if everything works out?  In my opinion,
> it could be qualified as a bug fix.
>
>
> On Thu, Aug 7, 2014 at 5:47 PM, Marcelo Vanzin <vanzin@cloudera.com>
> wrote:
>
> > Andrew has been working on a fix:
> > https://github.com/apache/spark/pull/1770
> >
> > On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> > > Just wanted to check in on this, see if I should file a bug report
> > > regarding the mesos argument propagation.
> > >
> > >
> > > On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org>
> > wrote:
> > >
> > >> 1. I've tried with and without escaping equals sign, it doesn't affect
> > the
> > >> results.
> > >>
> > >> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for
> getting
> > >> system properties set in the local shell (although not for executors).
> > >>
> > >> 3. We're using the default fine-grained mesos mode, not setting
> > >> spark.mesos.coarse, so it doesn't seem immediately related to that
> > ticket.
> > >> Should I file a bug report?
> > >>
> > >>
> > >> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >>
> > >>> The third issue may be related to this:
> > >>> https://issues.apache.org/jira/browse/SPARK-2022
> > >>>
> > >>> We can take a look at this during the bug fix period for the 1.1
> > >>> release next week. If we come up with a fix we can backport it into
> > >>> the 1.0 branch also.
> > >>>
> > >>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <
> pwendell@gmail.com>
> > >>> wrote:
> > >>> > Thanks for digging around here. I think there are a few distinct
> > issues.
> > >>> >
> > >>> > 1. Properties containing the '=' character need to be escaped.
> > >>> > I was able to load properties fine as long as I escape the '='
> > >>> > character. But maybe we should document this:
> > >>> >
> > >>> > == spark-defaults.conf ==
> > >>> > spark.foo a\=B
> > >>> > == shell ==
> > >>> > scala> sc.getConf.get("spark.foo")
> > >>> > res2: String = a=B
> > >>> >
> > >>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
> > >>> > don't affect the driver when running in client mode (always the
> case
> > >>> > for mesos). We should probably document this. In this case you need
> > to
> > >>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
> > >>> >
> > >>> > 3. Arguments aren't propagated on Mesos (this might be because of
> the
> > >>> > other issues, or a separate bug).
> > >>> >
> > >>> > - Patrick
> > >>> >
> > >>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <
> cody@koeninger.org>
> > >>> wrote:
> > >>> >> In addition, spark.executor.extraJavaOptions does not seem to
> behave
> > >>> as I
> > >>> >> would expect; java arguments don't seem to be propagated to
> > executors.
> > >>> >>
> > >>> >>
> > >>> >> $ cat conf/spark-defaults.conf
> > >>> >>
> > >>> >> spark.master
> > >>> >>
> > >>>
> >
> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
> > >>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
> > >>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> > >>> >>
> > >>> >>
> > >>> >> $ ./bin/spark-shell
> > >>> >>
> > >>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
> > >>> >> res0: String = -Dfoo.bar.baz=23
> > >>> >>
> > >>> >> scala> sc.parallelize(1 to 100).map{ i => (
> > >>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
> > >>> >>      |  System.getProperty("foo.bar.baz")
> > >>> >>      | )}.collect
> > >>> >>
> > >>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
> > >>> >> (dn-02.mxstg,null), ...
> > >>> >>
> > >>> >>
> > >>> >>
> > >>> >> Note that this is a mesos deployment, although I wouldn't expect
> > that
> > >>> to
> > >>> >> affect the availability of spark.driver.extraJavaOptions in a
> local
> > >>> spark
> > >>> >> shell.
> > >>> >>
> > >>> >>
> > >>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <
> cody@koeninger.org
> > >
> > >>> wrote:
> > >>> >>
> > >>> >>> Either whitespace or equals sign are valid properties file
> formats.
> > >>> >>> Here's an example:
> > >>> >>>
> > >>> >>> $ cat conf/spark-defaults.conf
> > >>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> > >>> >>>
> > >>> >>> $ ./bin/spark-shell -v
> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> > >>> >>> Adding default property:
> > >>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> > >>> >>>
> > >>> >>>
> > >>> >>> scala>  System.getProperty("foo.bar.baz")
> > >>> >>> res0: String = null
> > >>> >>>
> > >>> >>>
> > >>> >>> If you add double quotes, the resulting string value will have
> > double
> > >>> >>> quotes.
> > >>> >>>
> > >>> >>>
> > >>> >>> $ cat conf/spark-defaults.conf
> > >>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> > >>> >>>
> > >>> >>> $ ./bin/spark-shell -v
> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> > >>> >>> Adding default property:
> > >>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
> > >>> >>>
> > >>> >>> scala>  System.getProperty("foo.bar.baz")
> > >>> >>> res0: String = null
> > >>> >>>
> > >>> >>>
> > >>> >>> Neither one of those affects the issue; the underlying problem in
> > my
> > >>> case
> > >>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
> > >>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
> > >>> >>> spark-defaults.conf before the java process is started.
> > >>> >>>
> > >>> >>> Here's an example of the process running when only
> > >>> spark-defaults.conf is
> > >>> >>> being used:
> > >>> >>>
> > >>> >>> $ ps -ef | grep spark
> > >>> >>>
> > >>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
> > >>> ./bin/spark-shell -v
> > >>> >>>
> > >>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
> > >>> /usr/local/java/bin/java
> > >>> >>> -cp
> > >>> >>>
> > >>>
> >
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> > >>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
> > >>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
> > >>> >>> org.apache.spark.repl.Main
> > >>> >>>
> > >>> >>>
> > >>> >>> Here's an example of it when the command line
> > --driver-java-options is
> > >>> >>> used (and thus things work):
> > >>> >>>
> > >>> >>>
> > >>> >>> $ ps -ef | grep spark
> > >>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
> > >>> ./bin/spark-shell -v
> > >>> >>> --driver-java-options -Dfoo.bar.baz=23
> > >>> >>>
> > >>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
> > >>> /usr/local/java/bin/java
> > >>> >>> -cp
> > >>> >>>
> > >>>
> >
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> > >>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path=
> -Xms512m
> > >>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
> > >>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
> > >>> org.apache.spark.repl.Main
> > >>> >>>
> > >>> >>>
> > >>> >>>
> > >>> >>>
> > >>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <
> > pwendell@gmail.com>
> > >>> >>> wrote:
> > >>> >>>
> > >>> >>>> Cody - in your example you are using the '=' character, but in
> our
> > >>> >>>> documentation and tests we use a whitespace to separate the key
> > and
> > >>> >>>> value in the defaults file.
> > >>> >>>>
> > >>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
> > >>> >>>>
> > >>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> > >>> >>>>
> > >>> >>>> I'm not sure if the java properties file parser will try to
> > interpret
> > >>> >>>> the equals sign. If so you might need to do this.
> > >>> >>>>
> > >>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> > >>> >>>>
> > >>> >>>> Do those work for you?
> > >>> >>>>
> > >>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <
> > vanzin@cloudera.com
> > >>> >
> > >>> >>>> wrote:
> > >>> >>>> > Hi Cody,
> > >>> >>>> >
> > >>> >>>> > Could you file a bug for this if there isn't one already?
> > >>> >>>> >
> > >>> >>>> > For system properties SparkSubmit should be able to read those
> > >>> >>>> > settings and do the right thing, but that obviously won't work
> > for
> > >>> >>>> > other JVM options... the current code should work fine in
> > cluster
> > >>> mode
> > >>> >>>> > though, since the driver is a different process. :-)
> > >>> >>>> >
> > >>> >>>> >
> > >>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
> > >>> cody@koeninger.org>
> > >>> >>>> wrote:
> > >>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
> > >>> properties
> > >>> >>>> via
> > >>> >>>> >> -D.
> > >>> >>>> >>
> > >>> >>>> >> This was used for properties that varied on a
> > >>> >>>> per-deployment-environment
> > >>> >>>> >> basis, but needed to be available in the spark shell and
> > workers.
> > >>> >>>> >>
> > >>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
> > >>> deprecated,
> > >>> >>>> and
> > >>> >>>> >> replaced by spark-defaults.conf and command line arguments to
> > >>> >>>> spark-submit
> > >>> >>>> >> or spark-shell.
> > >>> >>>> >>
> > >>> >>>> >> However, setting spark.driver.extraJavaOptions and
> > >>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is
> not a
> > >>> >>>> replacement
> > >>> >>>> >> for SPARK_JAVA_OPTS:
> > >>> >>>> >>
> > >>> >>>> >>
> > >>> >>>> >> $ cat conf/spark-defaults.conf
> > >>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> > >>> >>>> >>
> > >>> >>>> >> $ ./bin/spark-shell
> > >>> >>>> >>
> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> > >>> >>>> >> res0: String = null
> > >>> >>>> >>
> > >>> >>>> >>
> > >>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
> > >>> >>>> >>
> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> > >>> >>>> >> res0: String = 23
> > >>> >>>> >>
> > >>> >>>> >>
> > >>> >>>> >> Looking through the shell scripts for spark-submit and
> > >>> spark-class, I
> > >>> >>>> can
> > >>> >>>> >> see why this is; parsing spark-defaults.conf from bash could
> be
> > >>> >>>> brittle.
> > >>> >>>> >>
> > >>> >>>> >> But from an ergonomic point of view, it's a step back to go
> > from a
> > >>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to
> > requiring
> > >>> >>>> command
> > >>> >>>> >> line arguments.
> > >>> >>>> >>
> > >>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell
> with
> > >>> the
> > >>> >>>> >> appropriate arguments, but I wanted to bring the issue up to
> > see
> > >>> if
> > >>> >>>> anyone
> > >>> >>>> >> else had run into it,
> > >>> >>>> >> or had any direction for a general solution (beyond parsing
> > java
> > >>> >>>> properties
> > >>> >>>> >> files from bash).
> > >>> >>>> >
> > >>> >>>> >
> > >>> >>>> >
> > >>> >>>> > --
> > >>> >>>> > Marcelo
> > >>> >>>>
> > >>> >>>
> > >>> >>>
> > >>>
> > >>
> > >>
> >
> >
> >
> > --
> > Marcelo
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a11c21f0e695c2a0500152865--

From dev-return-8775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 02:52:18 2014
Return-Path: <dev-return-8775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7294C1126F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 02:52:18 +0000 (UTC)
Received: (qmail 26560 invoked by uid 500); 8 Aug 2014 02:52:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26502 invoked by uid 500); 8 Aug 2014 02:52:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26484 invoked by uid 99); 8 Aug 2014 02:52:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:52:17 +0000
X-ASF-Spam-Status: No, hits=0.2 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.140 as permitted sender)
Received: from [202.81.31.140] (HELO e23smtp07.au.ibm.com) (202.81.31.140)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:52:11 +0000
Received: from /spool/local
	by e23smtp07.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Fri, 8 Aug 2014 12:51:43 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp07.au.ibm.com (202.81.31.204) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 8 Aug 2014 12:51:41 +1000
Received: from d23relay06.au.ibm.com (d23relay06.au.ibm.com [9.185.63.219])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id 556E72BB0055
	for <dev@spark.apache.org>; Fri,  8 Aug 2014 12:51:41 +1000 (EST)
Received: from d23av03.au.ibm.com (d23av03.au.ibm.com [9.190.234.97])
	by d23relay06.au.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s782qIAG15532054
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 12:52:18 +1000
Received: from d23av03.au.ibm.com (localhost [127.0.0.1])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s782pekm005403
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 12:51:40 +1000
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s782peRo005376
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 12:51:40 +1000
In-Reply-To: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Re: Fine-Grained Scheduler on Yarn
X-KeepSent: 1B922427:882F1394-48257D2E:000F97EF;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Fri, 8 Aug 2014 10:50:45 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6|November 21, 2013) at
 08/08/2014 10:50:51,
	Serialize complete at 08/08/2014 10:50:51
Content-Type: multipart/related; boundary="=_related 000FB51E48257D2E_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14080802-0260-0000-0000-0000000C8B0D
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 000FB51E48257D2E_=
Content-Type: multipart/alternative; boundary="=_alternative 000FB51E48257D2E_="


--=_alternative 000FB51E48257D2E_=
Content-Type: text/plain; charset="US-ASCII"

Any one know the answer?
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Jun Feng Liu/China/IBM 
2014/08/07 15:37

To
dev@spark.apache.org, 
cc

Subject
Fine-Grained Scheduler on Yarn





Hi, there

Just aware right now Spark only support fine grained scheduler on Mesos 
with MesosSchedulerBackend. The Yarn schedule sounds like only works on 
coarse-grained model. Is there any plan to implement fine-grained 
scheduler for YARN? Or there is any technical issue block us to do that.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 

--=_alternative 000FB51E48257D2E_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 000FB51E48257D2E_=--
--=_related 000FB51E48257D2E_=--


From dev-return-8776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 02:55:57 2014
Return-Path: <dev-return-8776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90B0211275
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 02:55:57 +0000 (UTC)
Received: (qmail 32385 invoked by uid 500); 8 Aug 2014 02:55:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32322 invoked by uid 500); 8 Aug 2014 02:55:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32311 invoked by uid 99); 8 Aug 2014 02:55:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:55:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 02:55:54 +0000
Received: by mail-pa0-f54.google.com with SMTP id fa1so6458329pad.13
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 19:55:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=u+u/rwQXmWab2069K/gWg+4XZcUB2fXDIrotF41+zvQ=;
        b=VvSHriC+DJCi4pjG1F5urpSQhveI+zGBz76KkqItb7JrVw/V0S3xY+/24xnGR8Z+Jd
         9ZCHDjt1fVaaAexRzsoRektqvPkYarNTTDAWtHR2tp7u+tanZYOjtLYW3FdUBAlsjsVU
         sO7IIZ7L2xWC0ef0on+4mAPQBlJLRhdTtBlfeavT7JUVd3QQePcfdW6IXkBtQTwzMibs
         Z8dPTygWay46NAjECoA0T583C/kSy9Ao9nkCPDD2268+wzpdKxwkKygxA2xh2AE9If2q
         8lybNKWxBhd8DZiOrf66Jy5XE3B+UR4Cfg32yUoM+su0OpR5Jd2n/fgbPIZTRloROwFI
         0sag==
X-Gm-Message-State: ALoCoQmp9WzhuvbZoRag0jmR6cqOqmsuYd2zfUpllLvJWazJOldW2wA4I7gJb0onnZgkc7lY2SvH
MIME-Version: 1.0
X-Received: by 10.69.17.230 with SMTP id gh6mr21629955pbd.0.1407466528510;
 Thu, 07 Aug 2014 19:55:28 -0700 (PDT)
Received: by 10.70.4.133 with HTTP; Thu, 7 Aug 2014 19:55:28 -0700 (PDT)
In-Reply-To: <CAMJOb8=nJ6_n-j9qFix1=biYhaZgLbUW1zhbc5UiET7efnh-3w@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
	<CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
	<CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
	<CAMJOb8=nJ6_n-j9qFix1=biYhaZgLbUW1zhbc5UiET7efnh-3w@mail.gmail.com>
Date: Thu, 7 Aug 2014 19:55:28 -0700
Message-ID: <CAMJOb8n_3TR-3-MWKUV5s7pPpG_b=W1C4hM5hB8FT52y+6aELw@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Andrew Or <andrew@databricks.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: Marcelo Vanzin <vanzin@cloudera.com>, Cody Koeninger <cody@koeninger.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c444772dc205001556b4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c444772dc205001556b4
Content-Type: text/plain; charset=UTF-8

@Cody I took a quick glance at the Mesos code and it appears that we
currently do not even pass extra java options to executors except in coarse
grained mode, and even in this mode we do not pass them to executors
correctly. I have filed a related JIRA here:
https://issues.apache.org/jira/browse/SPARK-2921. This is a somewhat
serious limitation and we will try to fix this for 1.1.

-Andrew


2014-08-07 19:42 GMT-07:00 Andrew Or <andrew@databricks.com>:

> Thanks Marcelo, I have moved the changes to a new PR to describe the
> problems more clearly: https://github.com/apache/spark/pull/1845
>
> @Gary Yeah, the goal is to get this into 1.1 as a bug fix.
>
>
> 2014-08-07 17:30 GMT-07:00 Gary Malouf <malouf.gary@gmail.com>:
>
> Can this be cherry-picked for 1.1 if everything works out?  In my opinion,
>> it could be qualified as a bug fix.
>>
>>
>> On Thu, Aug 7, 2014 at 5:47 PM, Marcelo Vanzin <vanzin@cloudera.com>
>> wrote:
>>
>> > Andrew has been working on a fix:
>> > https://github.com/apache/spark/pull/1770
>> >
>> > On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>> > > Just wanted to check in on this, see if I should file a bug report
>> > > regarding the mesos argument propagation.
>> > >
>> > >
>> > > On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org>
>> > wrote:
>> > >
>> > >> 1. I've tried with and without escaping equals sign, it doesn't
>> affect
>> > the
>> > >> results.
>> > >>
>> > >> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for
>> getting
>> > >> system properties set in the local shell (although not for
>> executors).
>> > >>
>> > >> 3. We're using the default fine-grained mesos mode, not setting
>> > >> spark.mesos.coarse, so it doesn't seem immediately related to that
>> > ticket.
>> > >> Should I file a bug report?
>> > >>
>> > >>
>> > >> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com
>> >
>> > >> wrote:
>> > >>
>> > >>> The third issue may be related to this:
>> > >>> https://issues.apache.org/jira/browse/SPARK-2022
>> > >>>
>> > >>> We can take a look at this during the bug fix period for the 1.1
>> > >>> release next week. If we come up with a fix we can backport it into
>> > >>> the 1.0 branch also.
>> > >>>
>> > >>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <
>> pwendell@gmail.com>
>> > >>> wrote:
>> > >>> > Thanks for digging around here. I think there are a few distinct
>> > issues.
>> > >>> >
>> > >>> > 1. Properties containing the '=' character need to be escaped.
>> > >>> > I was able to load properties fine as long as I escape the '='
>> > >>> > character. But maybe we should document this:
>> > >>> >
>> > >>> > == spark-defaults.conf ==
>> > >>> > spark.foo a\=B
>> > >>> > == shell ==
>> > >>> > scala> sc.getConf.get("spark.foo")
>> > >>> > res2: String = a=B
>> > >>> >
>> > >>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
>> > >>> > don't affect the driver when running in client mode (always the
>> case
>> > >>> > for mesos). We should probably document this. In this case you
>> need
>> > to
>> > >>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
>> > >>> >
>> > >>> > 3. Arguments aren't propagated on Mesos (this might be because of
>> the
>> > >>> > other issues, or a separate bug).
>> > >>> >
>> > >>> > - Patrick
>> > >>> >
>> > >>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <
>> cody@koeninger.org>
>> > >>> wrote:
>> > >>> >> In addition, spark.executor.extraJavaOptions does not seem to
>> behave
>> > >>> as I
>> > >>> >> would expect; java arguments don't seem to be propagated to
>> > executors.
>> > >>> >>
>> > >>> >>
>> > >>> >> $ cat conf/spark-defaults.conf
>> > >>> >>
>> > >>> >> spark.master
>> > >>> >>
>> > >>>
>> >
>> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
>> > >>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
>> > >>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> > >>> >>
>> > >>> >>
>> > >>> >> $ ./bin/spark-shell
>> > >>> >>
>> > >>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
>> > >>> >> res0: String = -Dfoo.bar.baz=23
>> > >>> >>
>> > >>> >> scala> sc.parallelize(1 to 100).map{ i => (
>> > >>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
>> > >>> >>      |  System.getProperty("foo.bar.baz")
>> > >>> >>      | )}.collect
>> > >>> >>
>> > >>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
>> > >>> >> (dn-02.mxstg,null), ...
>> > >>> >>
>> > >>> >>
>> > >>> >>
>> > >>> >> Note that this is a mesos deployment, although I wouldn't expect
>> > that
>> > >>> to
>> > >>> >> affect the availability of spark.driver.extraJavaOptions in a
>> local
>> > >>> spark
>> > >>> >> shell.
>> > >>> >>
>> > >>> >>
>> > >>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <
>> cody@koeninger.org
>> > >
>> > >>> wrote:
>> > >>> >>
>> > >>> >>> Either whitespace or equals sign are valid properties file
>> formats.
>> > >>> >>> Here's an example:
>> > >>> >>>
>> > >>> >>> $ cat conf/spark-defaults.conf
>> > >>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> > >>> >>>
>> > >>> >>> $ ./bin/spark-shell -v
>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> > >>> >>> Adding default property:
>> > >>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
>> > >>> >>> res0: String = null
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> If you add double quotes, the resulting string value will have
>> > double
>> > >>> >>> quotes.
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> $ cat conf/spark-defaults.conf
>> > >>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>> > >>> >>>
>> > >>> >>> $ ./bin/spark-shell -v
>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> > >>> >>> Adding default property:
>> > >>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>> > >>> >>>
>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
>> > >>> >>> res0: String = null
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> Neither one of those affects the issue; the underlying problem
>> in
>> > my
>> > >>> case
>> > >>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>> > >>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
>> > >>> >>> spark-defaults.conf before the java process is started.
>> > >>> >>>
>> > >>> >>> Here's an example of the process running when only
>> > >>> spark-defaults.conf is
>> > >>> >>> being used:
>> > >>> >>>
>> > >>> >>> $ ps -ef | grep spark
>> > >>> >>>
>> > >>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
>> > >>> ./bin/spark-shell -v
>> > >>> >>>
>> > >>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
>> > >>> /usr/local/java/bin/java
>> > >>> >>> -cp
>> > >>> >>>
>> > >>>
>> >
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> > >>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>> > >>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>> > >>> >>> org.apache.spark.repl.Main
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> Here's an example of it when the command line
>> > --driver-java-options is
>> > >>> >>> used (and thus things work):
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> $ ps -ef | grep spark
>> > >>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
>> > >>> ./bin/spark-shell -v
>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23
>> > >>> >>>
>> > >>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
>> > >>> /usr/local/java/bin/java
>> > >>> >>> -cp
>> > >>> >>>
>> > >>>
>> >
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> > >>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path=
>> -Xms512m
>> > >>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
>> > >>> org.apache.spark.repl.Main
>> > >>> >>>
>> > >>> >>>
>> > >>> >>>
>> > >>> >>>
>> > >>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <
>> > pwendell@gmail.com>
>> > >>> >>> wrote:
>> > >>> >>>
>> > >>> >>>> Cody - in your example you are using the '=' character, but in
>> our
>> > >>> >>>> documentation and tests we use a whitespace to separate the key
>> > and
>> > >>> >>>> value in the defaults file.
>> > >>> >>>>
>> > >>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
>> > >>> >>>>
>> > >>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>> > >>> >>>>
>> > >>> >>>> I'm not sure if the java properties file parser will try to
>> > interpret
>> > >>> >>>> the equals sign. If so you might need to do this.
>> > >>> >>>>
>> > >>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>> > >>> >>>>
>> > >>> >>>> Do those work for you?
>> > >>> >>>>
>> > >>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <
>> > vanzin@cloudera.com
>> > >>> >
>> > >>> >>>> wrote:
>> > >>> >>>> > Hi Cody,
>> > >>> >>>> >
>> > >>> >>>> > Could you file a bug for this if there isn't one already?
>> > >>> >>>> >
>> > >>> >>>> > For system properties SparkSubmit should be able to read
>> those
>> > >>> >>>> > settings and do the right thing, but that obviously won't
>> work
>> > for
>> > >>> >>>> > other JVM options... the current code should work fine in
>> > cluster
>> > >>> mode
>> > >>> >>>> > though, since the driver is a different process. :-)
>> > >>> >>>> >
>> > >>> >>>> >
>> > >>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
>> > >>> cody@koeninger.org>
>> > >>> >>>> wrote:
>> > >>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
>> > >>> properties
>> > >>> >>>> via
>> > >>> >>>> >> -D.
>> > >>> >>>> >>
>> > >>> >>>> >> This was used for properties that varied on a
>> > >>> >>>> per-deployment-environment
>> > >>> >>>> >> basis, but needed to be available in the spark shell and
>> > workers.
>> > >>> >>>> >>
>> > >>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
>> > >>> deprecated,
>> > >>> >>>> and
>> > >>> >>>> >> replaced by spark-defaults.conf and command line arguments
>> to
>> > >>> >>>> spark-submit
>> > >>> >>>> >> or spark-shell.
>> > >>> >>>> >>
>> > >>> >>>> >> However, setting spark.driver.extraJavaOptions and
>> > >>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is
>> not a
>> > >>> >>>> replacement
>> > >>> >>>> >> for SPARK_JAVA_OPTS:
>> > >>> >>>> >>
>> > >>> >>>> >>
>> > >>> >>>> >> $ cat conf/spark-defaults.conf
>> > >>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>> > >>> >>>> >>
>> > >>> >>>> >> $ ./bin/spark-shell
>> > >>> >>>> >>
>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
>> > >>> >>>> >> res0: String = null
>> > >>> >>>> >>
>> > >>> >>>> >>
>> > >>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>> > >>> >>>> >>
>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
>> > >>> >>>> >> res0: String = 23
>> > >>> >>>> >>
>> > >>> >>>> >>
>> > >>> >>>> >> Looking through the shell scripts for spark-submit and
>> > >>> spark-class, I
>> > >>> >>>> can
>> > >>> >>>> >> see why this is; parsing spark-defaults.conf from bash
>> could be
>> > >>> >>>> brittle.
>> > >>> >>>> >>
>> > >>> >>>> >> But from an ergonomic point of view, it's a step back to go
>> > from a
>> > >>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to
>> > requiring
>> > >>> >>>> command
>> > >>> >>>> >> line arguments.
>> > >>> >>>> >>
>> > >>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell
>> with
>> > >>> the
>> > >>> >>>> >> appropriate arguments, but I wanted to bring the issue up to
>> > see
>> > >>> if
>> > >>> >>>> anyone
>> > >>> >>>> >> else had run into it,
>> > >>> >>>> >> or had any direction for a general solution (beyond parsing
>> > java
>> > >>> >>>> properties
>> > >>> >>>> >> files from bash).
>> > >>> >>>> >
>> > >>> >>>> >
>> > >>> >>>> >
>> > >>> >>>> > --
>> > >>> >>>> > Marcelo
>> > >>> >>>>
>> > >>> >>>
>> > >>> >>>
>> > >>>
>> > >>
>> > >>
>> >
>> >
>> >
>> > --
>> > Marcelo
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>

--089e0158c444772dc205001556b4--

From dev-return-8777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 04:43:13 2014
Return-Path: <dev-return-8777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0875114AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 04:43:13 +0000 (UTC)
Received: (qmail 80223 invoked by uid 500); 8 Aug 2014 04:43:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80177 invoked by uid 500); 8 Aug 2014 04:43:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80165 invoked by uid 99); 8 Aug 2014 04:43:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 04:43:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 04:43:04 +0000
Received: by mail-oi0-f45.google.com with SMTP id e131so3329015oig.32
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 21:42:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/w+kqKispE2sDDyN0zTSIIu8dIUX3IAVPA/21fFDPpA=;
        b=PaR8/ffLeg+kBYNJAQfwRc+fegriIxOEz+c4JdON28I4aseEUA5A+ULsVeZ5VxbPL4
         y9s9YimU/lUCG0HRXjdorSn2FFCfYxYonm7vrvtYHWf5EX060eKYPOPRvjDWoIdLPMX0
         OReKlognMhAhs395/Qf5pepHJfB+90R2X9Ry+2DZLp0/Jv45OgA6SBc2x3CP1FO5SUh0
         a/1568ubaQcMtPXmqhI1mLVbITmPrpoyOTDBSsjScg+vatdZzrYxjbHYPy095jdMXii0
         xtxRZekqwJNXsd4b0G92z5hHv+iQrm/cDYp2F6RoLCjc4tvnmU49DpaxL1olEiN5/5UP
         NJlQ==
MIME-Version: 1.0
X-Received: by 10.182.94.230 with SMTP id df6mr28147329obb.36.1407472959138;
 Thu, 07 Aug 2014 21:42:39 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 7 Aug 2014 21:42:39 -0700 (PDT)
In-Reply-To: <CAMJOb8n_3TR-3-MWKUV5s7pPpG_b=W1C4hM5hB8FT52y+6aELw@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
	<CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
	<CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
	<CAMJOb8=nJ6_n-j9qFix1=biYhaZgLbUW1zhbc5UiET7efnh-3w@mail.gmail.com>
	<CAMJOb8n_3TR-3-MWKUV5s7pPpG_b=W1C4hM5hB8FT52y+6aELw@mail.gmail.com>
Date: Thu, 7 Aug 2014 21:42:39 -0700
Message-ID: <CABPQxssZ+i=nPyfd2QNq46LxzPSdAqAP-ocfvsAd4LQKuU98=w@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Patrick Wendell <pwendell@gmail.com>
To: Andrew Or <andrew@databricks.com>
Cc: Gary Malouf <malouf.gary@gmail.com>, Marcelo Vanzin <vanzin@cloudera.com>, 
	Cody Koeninger <cody@koeninger.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Andrew - I think your JIRA may duplicate existing work:
https://github.com/apache/spark/pull/1513


On Thu, Aug 7, 2014 at 7:55 PM, Andrew Or <andrew@databricks.com> wrote:
> @Cody I took a quick glance at the Mesos code and it appears that we
> currently do not even pass extra java options to executors except in coarse
> grained mode, and even in this mode we do not pass them to executors
> correctly. I have filed a related JIRA here:
> https://issues.apache.org/jira/browse/SPARK-2921. This is a somewhat
> serious limitation and we will try to fix this for 1.1.
>
> -Andrew
>
>
> 2014-08-07 19:42 GMT-07:00 Andrew Or <andrew@databricks.com>:
>
>> Thanks Marcelo, I have moved the changes to a new PR to describe the
>> problems more clearly: https://github.com/apache/spark/pull/1845
>>
>> @Gary Yeah, the goal is to get this into 1.1 as a bug fix.
>>
>>
>> 2014-08-07 17:30 GMT-07:00 Gary Malouf <malouf.gary@gmail.com>:
>>
>> Can this be cherry-picked for 1.1 if everything works out?  In my opinion,
>>> it could be qualified as a bug fix.
>>>
>>>
>>> On Thu, Aug 7, 2014 at 5:47 PM, Marcelo Vanzin <vanzin@cloudera.com>
>>> wrote:
>>>
>>> > Andrew has been working on a fix:
>>> > https://github.com/apache/spark/pull/1770
>>> >
>>> > On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>> > > Just wanted to check in on this, see if I should file a bug report
>>> > > regarding the mesos argument propagation.
>>> > >
>>> > >
>>> > > On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <cody@koeninger.org>
>>> > wrote:
>>> > >
>>> > >> 1. I've tried with and without escaping equals sign, it doesn't
>>> affect
>>> > the
>>> > >> results.
>>> > >>
>>> > >> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for
>>> getting
>>> > >> system properties set in the local shell (although not for
>>> executors).
>>> > >>
>>> > >> 3. We're using the default fine-grained mesos mode, not setting
>>> > >> spark.mesos.coarse, so it doesn't seem immediately related to that
>>> > ticket.
>>> > >> Should I file a bug report?
>>> > >>
>>> > >>
>>> > >> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com
>>> >
>>> > >> wrote:
>>> > >>
>>> > >>> The third issue may be related to this:
>>> > >>> https://issues.apache.org/jira/browse/SPARK-2022
>>> > >>>
>>> > >>> We can take a look at this during the bug fix period for the 1.1
>>> > >>> release next week. If we come up with a fix we can backport it into
>>> > >>> the 1.0 branch also.
>>> > >>>
>>> > >>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <
>>> pwendell@gmail.com>
>>> > >>> wrote:
>>> > >>> > Thanks for digging around here. I think there are a few distinct
>>> > issues.
>>> > >>> >
>>> > >>> > 1. Properties containing the '=' character need to be escaped.
>>> > >>> > I was able to load properties fine as long as I escape the '='
>>> > >>> > character. But maybe we should document this:
>>> > >>> >
>>> > >>> > == spark-defaults.conf ==
>>> > >>> > spark.foo a\=B
>>> > >>> > == shell ==
>>> > >>> > scala> sc.getConf.get("spark.foo")
>>> > >>> > res2: String = a=B
>>> > >>> >
>>> > >>> > 2. spark.driver.extraJavaOptions, when set in the properties file,
>>> > >>> > don't affect the driver when running in client mode (always the
>>> case
>>> > >>> > for mesos). We should probably document this. In this case you
>>> need
>>> > to
>>> > >>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
>>> > >>> >
>>> > >>> > 3. Arguments aren't propagated on Mesos (this might be because of
>>> the
>>> > >>> > other issues, or a separate bug).
>>> > >>> >
>>> > >>> > - Patrick
>>> > >>> >
>>> > >>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <
>>> cody@koeninger.org>
>>> > >>> wrote:
>>> > >>> >> In addition, spark.executor.extraJavaOptions does not seem to
>>> behave
>>> > >>> as I
>>> > >>> >> would expect; java arguments don't seem to be propagated to
>>> > executors.
>>> > >>> >>
>>> > >>> >>
>>> > >>> >> $ cat conf/spark-defaults.conf
>>> > >>> >>
>>> > >>> >> spark.master
>>> > >>> >>
>>> > >>>
>>> >
>>> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
>>> > >>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
>>> > >>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> > >>> >>
>>> > >>> >>
>>> > >>> >> $ ./bin/spark-shell
>>> > >>> >>
>>> > >>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
>>> > >>> >> res0: String = -Dfoo.bar.baz=23
>>> > >>> >>
>>> > >>> >> scala> sc.parallelize(1 to 100).map{ i => (
>>> > >>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
>>> > >>> >>      |  System.getProperty("foo.bar.baz")
>>> > >>> >>      | )}.collect
>>> > >>> >>
>>> > >>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
>>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
>>> > >>> >> (dn-02.mxstg,null), ...
>>> > >>> >>
>>> > >>> >>
>>> > >>> >>
>>> > >>> >> Note that this is a mesos deployment, although I wouldn't expect
>>> > that
>>> > >>> to
>>> > >>> >> affect the availability of spark.driver.extraJavaOptions in a
>>> local
>>> > >>> spark
>>> > >>> >> shell.
>>> > >>> >>
>>> > >>> >>
>>> > >>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <
>>> cody@koeninger.org
>>> > >
>>> > >>> wrote:
>>> > >>> >>
>>> > >>> >>> Either whitespace or equals sign are valid properties file
>>> formats.
>>> > >>> >>> Here's an example:
>>> > >>> >>>
>>> > >>> >>> $ cat conf/spark-defaults.conf
>>> > >>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> > >>> >>>
>>> > >>> >>> $ ./bin/spark-shell -v
>>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> > >>> >>> Adding default property:
>>> > >>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
>>> > >>> >>> res0: String = null
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> If you add double quotes, the resulting string value will have
>>> > double
>>> > >>> >>> quotes.
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> $ cat conf/spark-defaults.conf
>>> > >>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>> > >>> >>>
>>> > >>> >>> $ ./bin/spark-shell -v
>>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> > >>> >>> Adding default property:
>>> > >>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>>> > >>> >>>
>>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
>>> > >>> >>> res0: String = null
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> Neither one of those affects the issue; the underlying problem
>>> in
>>> > my
>>> > >>> case
>>> > >>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>>> > >>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
>>> > >>> >>> spark-defaults.conf before the java process is started.
>>> > >>> >>>
>>> > >>> >>> Here's an example of the process running when only
>>> > >>> spark-defaults.conf is
>>> > >>> >>> being used:
>>> > >>> >>>
>>> > >>> >>> $ ps -ef | grep spark
>>> > >>> >>>
>>> > >>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
>>> > >>> ./bin/spark-shell -v
>>> > >>> >>>
>>> > >>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
>>> > >>> /usr/local/java/bin/java
>>> > >>> >>> -cp
>>> > >>> >>>
>>> > >>>
>>> >
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> > >>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>>> > >>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>>> > >>> >>> org.apache.spark.repl.Main
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> Here's an example of it when the command line
>>> > --driver-java-options is
>>> > >>> >>> used (and thus things work):
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> $ ps -ef | grep spark
>>> > >>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
>>> > >>> ./bin/spark-shell -v
>>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23
>>> > >>> >>>
>>> > >>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
>>> > >>> /usr/local/java/bin/java
>>> > >>> >>> -cp
>>> > >>> >>>
>>> > >>>
>>> >
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> > >>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path=
>>> -Xms512m
>>> > >>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
>>> > >>> org.apache.spark.repl.Main
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>>
>>> > >>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <
>>> > pwendell@gmail.com>
>>> > >>> >>> wrote:
>>> > >>> >>>
>>> > >>> >>>> Cody - in your example you are using the '=' character, but in
>>> our
>>> > >>> >>>> documentation and tests we use a whitespace to separate the key
>>> > and
>>> > >>> >>>> value in the defaults file.
>>> > >>> >>>>
>>> > >>> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
>>> > >>> >>>>
>>> > >>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>> > >>> >>>>
>>> > >>> >>>> I'm not sure if the java properties file parser will try to
>>> > interpret
>>> > >>> >>>> the equals sign. If so you might need to do this.
>>> > >>> >>>>
>>> > >>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>> > >>> >>>>
>>> > >>> >>>> Do those work for you?
>>> > >>> >>>>
>>> > >>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <
>>> > vanzin@cloudera.com
>>> > >>> >
>>> > >>> >>>> wrote:
>>> > >>> >>>> > Hi Cody,
>>> > >>> >>>> >
>>> > >>> >>>> > Could you file a bug for this if there isn't one already?
>>> > >>> >>>> >
>>> > >>> >>>> > For system properties SparkSubmit should be able to read
>>> those
>>> > >>> >>>> > settings and do the right thing, but that obviously won't
>>> work
>>> > for
>>> > >>> >>>> > other JVM options... the current code should work fine in
>>> > cluster
>>> > >>> mode
>>> > >>> >>>> > though, since the driver is a different process. :-)
>>> > >>> >>>> >
>>> > >>> >>>> >
>>> > >>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
>>> > >>> cody@koeninger.org>
>>> > >>> >>>> wrote:
>>> > >>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
>>> > >>> properties
>>> > >>> >>>> via
>>> > >>> >>>> >> -D.
>>> > >>> >>>> >>
>>> > >>> >>>> >> This was used for properties that varied on a
>>> > >>> >>>> per-deployment-environment
>>> > >>> >>>> >> basis, but needed to be available in the spark shell and
>>> > workers.
>>> > >>> >>>> >>
>>> > >>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
>>> > >>> deprecated,
>>> > >>> >>>> and
>>> > >>> >>>> >> replaced by spark-defaults.conf and command line arguments
>>> to
>>> > >>> >>>> spark-submit
>>> > >>> >>>> >> or spark-shell.
>>> > >>> >>>> >>
>>> > >>> >>>> >> However, setting spark.driver.extraJavaOptions and
>>> > >>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is
>>> not a
>>> > >>> >>>> replacement
>>> > >>> >>>> >> for SPARK_JAVA_OPTS:
>>> > >>> >>>> >>
>>> > >>> >>>> >>
>>> > >>> >>>> >> $ cat conf/spark-defaults.conf
>>> > >>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>> > >>> >>>> >>
>>> > >>> >>>> >> $ ./bin/spark-shell
>>> > >>> >>>> >>
>>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
>>> > >>> >>>> >> res0: String = null
>>> > >>> >>>> >>
>>> > >>> >>>> >>
>>> > >>> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>>> > >>> >>>> >>
>>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
>>> > >>> >>>> >> res0: String = 23
>>> > >>> >>>> >>
>>> > >>> >>>> >>
>>> > >>> >>>> >> Looking through the shell scripts for spark-submit and
>>> > >>> spark-class, I
>>> > >>> >>>> can
>>> > >>> >>>> >> see why this is; parsing spark-defaults.conf from bash
>>> could be
>>> > >>> >>>> brittle.
>>> > >>> >>>> >>
>>> > >>> >>>> >> But from an ergonomic point of view, it's a step back to go
>>> > from a
>>> > >>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to
>>> > requiring
>>> > >>> >>>> command
>>> > >>> >>>> >> line arguments.
>>> > >>> >>>> >>
>>> > >>> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell
>>> with
>>> > >>> the
>>> > >>> >>>> >> appropriate arguments, but I wanted to bring the issue up to
>>> > see
>>> > >>> if
>>> > >>> >>>> anyone
>>> > >>> >>>> >> else had run into it,
>>> > >>> >>>> >> or had any direction for a general solution (beyond parsing
>>> > java
>>> > >>> >>>> properties
>>> > >>> >>>> >> files from bash).
>>> > >>> >>>> >
>>> > >>> >>>> >
>>> > >>> >>>> >
>>> > >>> >>>> > --
>>> > >>> >>>> > Marcelo
>>> > >>> >>>>
>>> > >>> >>>
>>> > >>> >>>
>>> > >>>
>>> > >>
>>> > >>
>>> >
>>> >
>>> >
>>> > --
>>> > Marcelo
>>> >
>>> > ---------------------------------------------------------------------
>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>> >
>>> >
>>>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 04:44:08 2014
Return-Path: <dev-return-8778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1909E114C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 04:44:08 +0000 (UTC)
Received: (qmail 85823 invoked by uid 500); 8 Aug 2014 04:44:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85766 invoked by uid 500); 8 Aug 2014 04:44:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85754 invoked by uid 99); 8 Aug 2014 04:44:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 04:44:07 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 04:44:04 +0000
Received: by mail-oi0-f50.google.com with SMTP id a141so3302161oig.37
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 21:43:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5Dmu8VVNFAevALfezgBTFe+zAtYb/G+P0J0SbBLXQDo=;
        b=DeBfSO68rtawVNM/AVscrQ06ufWKIUhr2fgGrS/1ouF8QbRia/E7tlXUeuSuY3PsSW
         00jWuY5bdTv1ynz2x55jDS0EGWP2RBXhFlVET27E7zfvGC7yr/UFw+L07nOk0nIe9Zzv
         PJJTQFByRSNAZ6z+kNqheTawxEUNme4EI6wshbHM4wmHGLYQ4UCSxt0iQ4GC9C5uYuEh
         Mmh+VgAov/vWf/FkY/TlHwo5RV7MLue5+3f69dJGd0cV3iQ85rypdP67jTBlCySgu7or
         i1i12zUJQg8nK2kfYlBT7NrDBauDDLWcq6WwsPMzsvhW6hfFryTvZagU8z4ElssIVw9g
         sR0g==
MIME-Version: 1.0
X-Received: by 10.182.18.101 with SMTP id v5mr27917572obd.64.1407473019004;
 Thu, 07 Aug 2014 21:43:39 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 7 Aug 2014 21:43:38 -0700 (PDT)
In-Reply-To: <OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
Date: Thu, 7 Aug 2014 21:43:38 -0700
Message-ID: <CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>
Subject: Re: Fine-Grained Scheduler on Yarn
From: Patrick Wendell <pwendell@gmail.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33060542cdd050016d93d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33060542cdd050016d93d
Content-Type: text/plain; charset=ISO-8859-1

The current YARN is equivalent to what is called "fine grained" mode in
Mesos. The scheduling of tasks happens totally inside of the Spark driver.


On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Any one know the answer?
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Jun Feng Liu/China/IBM*
>
> 2014/08/07 15:37
>   To
> dev@spark.apache.org,
> cc
>   Subject
> Fine-Grained Scheduler on Yarn
>
>
>
> Hi, there
>
> Just aware right now Spark only support fine grained scheduler on Mesos
> with MesosSchedulerBackend. The Yarn schedule sounds like only works on
> coarse-grained model. Is there any plan to implement fine-grained scheduler
> for YARN? Or there is any technical issue block us to do that.
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>

--001a11c33060542cdd050016d93d--

From dev-return-8779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 05:11:25 2014
Return-Path: <dev-return-8779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B558711557
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 05:11:25 +0000 (UTC)
Received: (qmail 20062 invoked by uid 500); 8 Aug 2014 05:11:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20003 invoked by uid 500); 8 Aug 2014 05:11:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19988 invoked by uid 99); 8 Aug 2014 05:11:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:11:24 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:11:20 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so3652839obc.19
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 22:10:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=OxY9Nxd7lPc4BzbpqsiTUDix7LXlKkLOneZUWwxdM2U=;
        b=DyTTQ3zmB5pKC/xRvw9kvk0p7QvBqoCzzO0HhxFZ0p6hMsOSHavbjaUaIIMbLhUUQe
         xrZ/yttDSQU67kRYJxFovtYuP0Ahew7PrDLMtbx1vmvUTORSvz4txQl2Za/n4lIcvRzE
         m8q9bwtmcxQaXUwTnaifMC16fpTuvQnutV+zsr/TCNmvRqR9rYEB6TquEivSlnSCSyw1
         si305UV2zzRfBRJ/DWwAg8U/GmeTMduTYiihRWh7HofY4Gulcph9ykrxaM4OtMH8++74
         lI3ZlGZw6kjfCcoCFC7cihPsMUIIw5bK9Hz7Tio+MM1xGD9ZDJaI7m9wA1XF+kizSt8Z
         e3ag==
MIME-Version: 1.0
X-Received: by 10.182.137.195 with SMTP id qk3mr8125421obb.5.1407474655801;
 Thu, 07 Aug 2014 22:10:55 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 7 Aug 2014 22:10:55 -0700 (PDT)
In-Reply-To: <CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
	<CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>
Date: Thu, 7 Aug 2014 22:10:55 -0700
Message-ID: <CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
Subject: Re: Fine-Grained Scheduler on Yarn
From: Patrick Wendell <pwendell@gmail.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c361f6e3b4dc0500173a47
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c361f6e3b4dc0500173a47
Content-Type: text/plain; charset=ISO-8859-1

Hey sorry about that - what I said was the opposite of what is true.

The current YARN mode is equivalent to "coarse grained" mesos. There is no
fine-grained scheduling on YARN at the moment. I'm not sure YARN supports
scheduling in units other than containers. Fine-grained scheduling requires
scheduling at the granularity of individual cores.


On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> The current YARN is equivalent to what is called "fine grained" mode in
> Mesos. The scheduling of tasks happens totally inside of the Spark driver.
>
>
> On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
>> Any one know the answer?
>>
>> Best Regards
>>
>>
>> *Jun Feng Liu*
>> IBM China Systems & Technology Laboratory in Beijing
>>
>>   ------------------------------
>>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>>
>> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>> [image: IBM]
>>
>> BLD 28,ZGC Software Park
>> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
>> China
>>
>>
>>
>>
>>
>>  *Jun Feng Liu/China/IBM*
>>
>> 2014/08/07 15:37
>>   To
>> dev@spark.apache.org,
>> cc
>>   Subject
>> Fine-Grained Scheduler on Yarn
>>
>>
>>
>> Hi, there
>>
>> Just aware right now Spark only support fine grained scheduler on Mesos
>> with MesosSchedulerBackend. The Yarn schedule sounds like only works on
>> coarse-grained model. Is there any plan to implement fine-grained scheduler
>> for YARN? Or there is any technical issue block us to do that.
>>
>> Best Regards
>>
>>
>> *Jun Feng Liu*
>> IBM China Systems & Technology Laboratory in Beijing
>>
>>   ------------------------------
>>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>>
>> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>> [image: IBM]
>>
>> BLD 28,ZGC Software Park
>> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
>> China
>>
>>
>>
>>
>>
>

--001a11c361f6e3b4dc0500173a47--

From dev-return-8780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 05:19:29 2014
Return-Path: <dev-return-8780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABA8B1157D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 05:19:29 +0000 (UTC)
Received: (qmail 35971 invoked by uid 500); 8 Aug 2014 05:19:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35902 invoked by uid 500); 8 Aug 2014 05:19:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35891 invoked by uid 99); 8 Aug 2014 05:19:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:19:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:19:26 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so482056pdi.17
        for <dev@spark.apache.org>; Thu, 07 Aug 2014 22:19:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=l6O1mBLlL55OhKcvmIUGsVzT79CWpPNeGW2sGT4FRrY=;
        b=RK9ZSvkG0DAWkBxnS1phBpjoesJg/BB+yPjRr2L49VEXNvyuNXceCAR/bJVQQZDTIH
         2nGMarL6m/BV0AtPOxZtqRapetJW42+/lDAr7qgvIYvkLO6hRoIdLwfQ/XxoiCBST1G+
         16j5jpsQEBH8exQUL7QBF6Gx4eq0vHPHHClI79zU4pkc22+izE4iOqyAyrghzkal5d5P
         DfauCIBPBTAJahEwotu9aWKjzQmhjNlk5bKXu8RQweMmFx64VizFRZrfEKI+Uo9OTSRT
         LAB6ocYNJDXyW7ZH/bscUGN/942xK4UfqXw7xN8pto4IHiX/H9gzC3wZFkSaVriUJ196
         s3Bg==
X-Gm-Message-State: ALoCoQlRYSh5wFonB/q1PUFtOzuOBlzEe5Q5P/jMUvUiEdVnW/JtSPMom9J/FGTYZEeGf0R+a/sS
MIME-Version: 1.0
X-Received: by 10.69.18.203 with SMTP id go11mr6529125pbd.50.1407475140325;
 Thu, 07 Aug 2014 22:19:00 -0700 (PDT)
Received: by 10.70.4.133 with HTTP; Thu, 7 Aug 2014 22:19:00 -0700 (PDT)
In-Reply-To: <CABPQxssZ+i=nPyfd2QNq46LxzPSdAqAP-ocfvsAd4LQKuU98=w@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
	<CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
	<CAKWX9VXjKpsEmJEqymf6Hy+hcbvruVFMJf+JzOqU=SvJGNkS8Q@mail.gmail.com>
	<CAAOnQ7tVEXRJaZ9EZ-ppRWRXwudKhaV3_m7rp0Fdu+GcABPvyA@mail.gmail.com>
	<CAGOvqipBpgo=6Snj-kDPV5KABPTeK6SUmqebv7Z2e+d7LdgEug@mail.gmail.com>
	<CAMJOb8=nJ6_n-j9qFix1=biYhaZgLbUW1zhbc5UiET7efnh-3w@mail.gmail.com>
	<CAMJOb8n_3TR-3-MWKUV5s7pPpG_b=W1C4hM5hB8FT52y+6aELw@mail.gmail.com>
	<CABPQxssZ+i=nPyfd2QNq46LxzPSdAqAP-ocfvsAd4LQKuU98=w@mail.gmail.com>
Date: Thu, 7 Aug 2014 22:19:00 -0700
Message-ID: <CAMJOb8=ytGqmn4k0zUiFn1zBL9dm1-14SK6ZLf-Hi=00a0k27A@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Andrew Or <andrew@databricks.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Gary Malouf <malouf.gary@gmail.com>, Marcelo Vanzin <vanzin@cloudera.com>, 
	Cody Koeninger <cody@koeninger.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11364e74c50c0d05001757d5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11364e74c50c0d05001757d5
Content-Type: text/plain; charset=UTF-8

Ah, great to know this is already being fixed. Thanks Patrick, I have
marked my JIRA as a duplicate.


2014-08-07 21:42 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

> Andrew - I think your JIRA may duplicate existing work:
> https://github.com/apache/spark/pull/1513
>
>
> On Thu, Aug 7, 2014 at 7:55 PM, Andrew Or <andrew@databricks.com> wrote:
> > @Cody I took a quick glance at the Mesos code and it appears that we
> > currently do not even pass extra java options to executors except in
> coarse
> > grained mode, and even in this mode we do not pass them to executors
> > correctly. I have filed a related JIRA here:
> > https://issues.apache.org/jira/browse/SPARK-2921. This is a somewhat
> > serious limitation and we will try to fix this for 1.1.
> >
> > -Andrew
> >
> >
> > 2014-08-07 19:42 GMT-07:00 Andrew Or <andrew@databricks.com>:
> >
> >> Thanks Marcelo, I have moved the changes to a new PR to describe the
> >> problems more clearly: https://github.com/apache/spark/pull/1845
> >>
> >> @Gary Yeah, the goal is to get this into 1.1 as a bug fix.
> >>
> >>
> >> 2014-08-07 17:30 GMT-07:00 Gary Malouf <malouf.gary@gmail.com>:
> >>
> >> Can this be cherry-picked for 1.1 if everything works out?  In my
> opinion,
> >>> it could be qualified as a bug fix.
> >>>
> >>>
> >>> On Thu, Aug 7, 2014 at 5:47 PM, Marcelo Vanzin <vanzin@cloudera.com>
> >>> wrote:
> >>>
> >>> > Andrew has been working on a fix:
> >>> > https://github.com/apache/spark/pull/1770
> >>> >
> >>> > On Thu, Aug 7, 2014 at 2:35 PM, Cody Koeninger <cody@koeninger.org>
> >>> wrote:
> >>> > > Just wanted to check in on this, see if I should file a bug report
> >>> > > regarding the mesos argument propagation.
> >>> > >
> >>> > >
> >>> > > On Thu, Jul 31, 2014 at 8:35 AM, Cody Koeninger <
> cody@koeninger.org>
> >>> > wrote:
> >>> > >
> >>> > >> 1. I've tried with and without escaping equals sign, it doesn't
> >>> affect
> >>> > the
> >>> > >> results.
> >>> > >>
> >>> > >> 2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for
> >>> getting
> >>> > >> system properties set in the local shell (although not for
> >>> executors).
> >>> > >>
> >>> > >> 3. We're using the default fine-grained mesos mode, not setting
> >>> > >> spark.mesos.coarse, so it doesn't seem immediately related to that
> >>> > ticket.
> >>> > >> Should I file a bug report?
> >>> > >>
> >>> > >>
> >>> > >> On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <
> pwendell@gmail.com
> >>> >
> >>> > >> wrote:
> >>> > >>
> >>> > >>> The third issue may be related to this:
> >>> > >>> https://issues.apache.org/jira/browse/SPARK-2022
> >>> > >>>
> >>> > >>> We can take a look at this during the bug fix period for the 1.1
> >>> > >>> release next week. If we come up with a fix we can backport it
> into
> >>> > >>> the 1.0 branch also.
> >>> > >>>
> >>> > >>> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <
> >>> pwendell@gmail.com>
> >>> > >>> wrote:
> >>> > >>> > Thanks for digging around here. I think there are a few
> distinct
> >>> > issues.
> >>> > >>> >
> >>> > >>> > 1. Properties containing the '=' character need to be escaped.
> >>> > >>> > I was able to load properties fine as long as I escape the '='
> >>> > >>> > character. But maybe we should document this:
> >>> > >>> >
> >>> > >>> > == spark-defaults.conf ==
> >>> > >>> > spark.foo a\=B
> >>> > >>> > == shell ==
> >>> > >>> > scala> sc.getConf.get("spark.foo")
> >>> > >>> > res2: String = a=B
> >>> > >>> >
> >>> > >>> > 2. spark.driver.extraJavaOptions, when set in the properties
> file,
> >>> > >>> > don't affect the driver when running in client mode (always the
> >>> case
> >>> > >>> > for mesos). We should probably document this. In this case you
> >>> need
> >>> > to
> >>> > >>> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
> >>> > >>> >
> >>> > >>> > 3. Arguments aren't propagated on Mesos (this might be because
> of
> >>> the
> >>> > >>> > other issues, or a separate bug).
> >>> > >>> >
> >>> > >>> > - Patrick
> >>> > >>> >
> >>> > >>> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <
> >>> cody@koeninger.org>
> >>> > >>> wrote:
> >>> > >>> >> In addition, spark.executor.extraJavaOptions does not seem to
> >>> behave
> >>> > >>> as I
> >>> > >>> >> would expect; java arguments don't seem to be propagated to
> >>> > executors.
> >>> > >>> >>
> >>> > >>> >>
> >>> > >>> >> $ cat conf/spark-defaults.conf
> >>> > >>> >>
> >>> > >>> >> spark.master
> >>> > >>> >>
> >>> > >>>
> >>> >
> >>>
> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
> >>> > >>> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
> >>> > >>> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> > >>> >>
> >>> > >>> >>
> >>> > >>> >> $ ./bin/spark-shell
> >>> > >>> >>
> >>> > >>> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
> >>> > >>> >> res0: String = -Dfoo.bar.baz=23
> >>> > >>> >>
> >>> > >>> >> scala> sc.parallelize(1 to 100).map{ i => (
> >>> > >>> >>      |  java.net.InetAddress.getLocalHost.getHostName,
> >>> > >>> >>      |  System.getProperty("foo.bar.baz")
> >>> > >>> >>      | )}.collect
> >>> > >>> >>
> >>> > >>> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
> >>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >>> > >>> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
> >>> > >>> >> (dn-02.mxstg,null), ...
> >>> > >>> >>
> >>> > >>> >>
> >>> > >>> >>
> >>> > >>> >> Note that this is a mesos deployment, although I wouldn't
> expect
> >>> > that
> >>> > >>> to
> >>> > >>> >> affect the availability of spark.driver.extraJavaOptions in a
> >>> local
> >>> > >>> spark
> >>> > >>> >> shell.
> >>> > >>> >>
> >>> > >>> >>
> >>> > >>> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <
> >>> cody@koeninger.org
> >>> > >
> >>> > >>> wrote:
> >>> > >>> >>
> >>> > >>> >>> Either whitespace or equals sign are valid properties file
> >>> formats.
> >>> > >>> >>> Here's an example:
> >>> > >>> >>>
> >>> > >>> >>> $ cat conf/spark-defaults.conf
> >>> > >>> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> > >>> >>>
> >>> > >>> >>> $ ./bin/spark-shell -v
> >>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> > >>> >>> Adding default property:
> >>> > >>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
> >>> > >>> >>> res0: String = null
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> If you add double quotes, the resulting string value will
> have
> >>> > double
> >>> > >>> >>> quotes.
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> $ cat conf/spark-defaults.conf
> >>> > >>> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>> > >>> >>>
> >>> > >>> >>> $ ./bin/spark-shell -v
> >>> > >>> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> > >>> >>> Adding default property:
> >>> > >>> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
> >>> > >>> >>>
> >>> > >>> >>> scala>  System.getProperty("foo.bar.baz")
> >>> > >>> >>> res0: String = null
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> Neither one of those affects the issue; the underlying
> problem
> >>> in
> >>> > my
> >>> > >>> case
> >>> > >>> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS
> and
> >>> > >>> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
> >>> > >>> >>> spark-defaults.conf before the java process is started.
> >>> > >>> >>>
> >>> > >>> >>> Here's an example of the process running when only
> >>> > >>> spark-defaults.conf is
> >>> > >>> >>> being used:
> >>> > >>> >>>
> >>> > >>> >>> $ ps -ef | grep spark
> >>> > >>> >>>
> >>> > >>> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash
> >>> > >>> ./bin/spark-shell -v
> >>> > >>> >>>
> >>> > >>> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
> >>> > >>> /usr/local/java/bin/java
> >>> > >>> >>> -cp
> >>> > >>> >>>
> >>> > >>>
> >>> >
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> > >>> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
> >>> > >>> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
> >>> > >>> >>> org.apache.spark.repl.Main
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> Here's an example of it when the command line
> >>> > --driver-java-options is
> >>> > >>> >>> used (and thus things work):
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> $ ps -ef | grep spark
> >>> > >>> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash
> >>> > >>> ./bin/spark-shell -v
> >>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23
> >>> > >>> >>>
> >>> > >>> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
> >>> > >>> /usr/local/java/bin/java
> >>> > >>> >>> -cp
> >>> > >>> >>>
> >>> > >>>
> >>> >
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> > >>> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path=
> >>> -Xms512m
> >>> > >>> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
> >>> > >>> >>> --driver-java-options -Dfoo.bar.baz=23 --class
> >>> > >>> org.apache.spark.repl.Main
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <
> >>> > pwendell@gmail.com>
> >>> > >>> >>> wrote:
> >>> > >>> >>>
> >>> > >>> >>>> Cody - in your example you are using the '=' character, but
> in
> >>> our
> >>> > >>> >>>> documentation and tests we use a whitespace to separate the
> key
> >>> > and
> >>> > >>> >>>> value in the defaults file.
> >>> > >>> >>>>
> >>> > >>> >>>> docs:
> http://spark.apache.org/docs/latest/configuration.html
> >>> > >>> >>>>
> >>> > >>> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>> > >>> >>>>
> >>> > >>> >>>> I'm not sure if the java properties file parser will try to
> >>> > interpret
> >>> > >>> >>>> the equals sign. If so you might need to do this.
> >>> > >>> >>>>
> >>> > >>> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>> > >>> >>>>
> >>> > >>> >>>> Do those work for you?
> >>> > >>> >>>>
> >>> > >>> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <
> >>> > vanzin@cloudera.com
> >>> > >>> >
> >>> > >>> >>>> wrote:
> >>> > >>> >>>> > Hi Cody,
> >>> > >>> >>>> >
> >>> > >>> >>>> > Could you file a bug for this if there isn't one already?
> >>> > >>> >>>> >
> >>> > >>> >>>> > For system properties SparkSubmit should be able to read
> >>> those
> >>> > >>> >>>> > settings and do the right thing, but that obviously won't
> >>> work
> >>> > for
> >>> > >>> >>>> > other JVM options... the current code should work fine in
> >>> > cluster
> >>> > >>> mode
> >>> > >>> >>>> > though, since the driver is a different process. :-)
> >>> > >>> >>>> >
> >>> > >>> >>>> >
> >>> > >>> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <
> >>> > >>> cody@koeninger.org>
> >>> > >>> >>>> wrote:
> >>> > >>> >>>> >> We were previously using SPARK_JAVA_OPTS to set java
> system
> >>> > >>> properties
> >>> > >>> >>>> via
> >>> > >>> >>>> >> -D.
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> This was used for properties that varied on a
> >>> > >>> >>>> per-deployment-environment
> >>> > >>> >>>> >> basis, but needed to be available in the spark shell and
> >>> > workers.
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
> >>> > >>> deprecated,
> >>> > >>> >>>> and
> >>> > >>> >>>> >> replaced by spark-defaults.conf and command line
> arguments
> >>> to
> >>> > >>> >>>> spark-submit
> >>> > >>> >>>> >> or spark-shell.
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> However, setting spark.driver.extraJavaOptions and
> >>> > >>> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is
> >>> not a
> >>> > >>> >>>> replacement
> >>> > >>> >>>> >> for SPARK_JAVA_OPTS:
> >>> > >>> >>>> >>
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> $ cat conf/spark-defaults.conf
> >>> > >>> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> $ ./bin/spark-shell
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>> > >>> >>>> >> res0: String = null
> >>> > >>> >>>> >>
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> $ ./bin/spark-shell --driver-java-options
> "-Dfoo.bar.baz=23"
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>> > >>> >>>> >> res0: String = 23
> >>> > >>> >>>> >>
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> Looking through the shell scripts for spark-submit and
> >>> > >>> spark-class, I
> >>> > >>> >>>> can
> >>> > >>> >>>> >> see why this is; parsing spark-defaults.conf from bash
> >>> could be
> >>> > >>> >>>> brittle.
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> But from an ergonomic point of view, it's a step back to
> go
> >>> > from a
> >>> > >>> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to
> >>> > requiring
> >>> > >>> >>>> command
> >>> > >>> >>>> >> line arguments.
> >>> > >>> >>>> >>
> >>> > >>> >>>> >> I can solve this with an ad-hoc script to wrap
> spark-shell
> >>> with
> >>> > >>> the
> >>> > >>> >>>> >> appropriate arguments, but I wanted to bring the issue
> up to
> >>> > see
> >>> > >>> if
> >>> > >>> >>>> anyone
> >>> > >>> >>>> >> else had run into it,
> >>> > >>> >>>> >> or had any direction for a general solution (beyond
> parsing
> >>> > java
> >>> > >>> >>>> properties
> >>> > >>> >>>> >> files from bash).
> >>> > >>> >>>> >
> >>> > >>> >>>> >
> >>> > >>> >>>> >
> >>> > >>> >>>> > --
> >>> > >>> >>>> > Marcelo
> >>> > >>> >>>>
> >>> > >>> >>>
> >>> > >>> >>>
> >>> > >>>
> >>> > >>
> >>> > >>
> >>> >
> >>> >
> >>> >
> >>> > --
> >>> > Marcelo
> >>> >
> >>> > ---------------------------------------------------------------------
> >>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> > For additional commands, e-mail: dev-help@spark.apache.org
> >>> >
> >>> >
> >>>
> >>
> >>
>

--001a11364e74c50c0d05001757d5--

From dev-return-8781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 05:57:35 2014
Return-Path: <dev-return-8781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF2511164A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 05:57:35 +0000 (UTC)
Received: (qmail 173 invoked by uid 500); 8 Aug 2014 05:57:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 109 invoked by uid 500); 8 Aug 2014 05:57:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99997 invoked by uid 99); 8 Aug 2014 05:57:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:57:28 +0000
X-ASF-Spam-Status: No, hits=0.2 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.146 as permitted sender)
Received: from [202.81.31.146] (HELO e23smtp04.au.ibm.com) (202.81.31.146)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 05:57:20 +0000
Received: from /spool/local
	by e23smtp04.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Fri, 8 Aug 2014 15:56:58 +1000
Received: from d23dlp03.au.ibm.com (202.81.31.214)
	by e23smtp04.au.ibm.com (202.81.31.210) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 8 Aug 2014 15:56:57 +1000
Received: from d23relay06.au.ibm.com (d23relay06.au.ibm.com [9.185.63.219])
	by d23dlp03.au.ibm.com (Postfix) with ESMTP id 950DA3578053
	for <dev@spark.apache.org>; Fri,  8 Aug 2014 15:56:56 +1000 (EST)
Received: from d23av05.au.ibm.com (d23av05.au.ibm.com [9.190.234.119])
	by d23relay06.au.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s785vXn212976194
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 15:57:33 +1000
Received: from d23av05.au.ibm.com (localhost [127.0.0.1])
	by d23av05.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s785uukU020726
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 15:56:56 +1000
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av05.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s785utCw020705
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 15:56:55 +1000
In-Reply-To: <CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
	<CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com> <CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Re: Fine-Grained Scheduler on Yarn
X-KeepSent: E4B5E67A:527C58A9-48257D2E:001FC9BF;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OFE4B5E67A.527C58A9-ON48257D2E.001FC9BF-48257D2E.0020AB17@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Fri, 8 Aug 2014 13:56:00 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6|November 21, 2013) at
 08/08/2014 13:56:05,
	Serialize complete at 08/08/2014 13:56:05
Content-Type: multipart/related; boundary="=_related 0020AB1448257D2E_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14080805-9264-0000-0000-0000000F6A36
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 0020AB1448257D2E_=
Content-Type: multipart/alternative; boundary="=_alternative 0020AB1448257D2E_="


--=_alternative 0020AB1448257D2E_=
Content-Type: text/plain; charset="US-ASCII"

Thanks for echo on this. Possible to adjust resource based on container 
numbers? e.g to allocate more container when driver need more resources 
and return some resource by delete some container when parts of container 
already have enough cores/memory 
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Patrick Wendell <pwendell@gmail.com> 
2014/08/08 13:10

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
"dev@spark.apache.org" <dev@spark.apache.org>
Subject
Re: Fine-Grained Scheduler on Yarn






Hey sorry about that - what I said was the opposite of what is true.

The current YARN mode is equivalent to "coarse grained" mesos. There is no 
fine-grained scheduling on YARN at the moment. I'm not sure YARN supports 
scheduling in units other than containers. Fine-grained scheduling 
requires scheduling at the granularity of individual cores.


On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell <pwendell@gmail.com> 
wrote:
The current YARN is equivalent to what is called "fine grained" mode in 
Mesos. The scheduling of tasks happens totally inside of the Spark driver.


On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
Any one know the answer?
Best Regards 
  
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing 



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com 


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

  



Jun Feng Liu/China/IBM 
2014/08/07 15:37 


To
dev@spark.apache.org, 
cc

Subject
Fine-Grained Scheduler on Yarn







Hi, there 

Just aware right now Spark only support fine grained scheduler on Mesos 
with MesosSchedulerBackend. The Yarn schedule sounds like only works on 
coarse-grained model. Is there any plan to implement fine-grained 
scheduler for YARN? Or there is any technical issue block us to do that.
Best Regards 
  
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing 



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com 


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

  



--=_alternative 0020AB1448257D2E_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64

PGZvbnQgc2l6ZT0yIGZhY2U9InNhbnMtc2VyaWYiPlRoYW5rcyBmb3IgZWNobyBvbiB0aGlzLiBQ
b3NzaWJsZSB0byBhZGp1c3QNCnJlc291cmNlIGJhc2VkIG9uIGNvbnRhaW5lciBudW1iZXJzPyBl
LmcgdG8gYWxsb2NhdGUgbW9yZSBjb250YWluZXIgd2hlbg0KZHJpdmVyIG5lZWQgbW9yZSByZXNv
dXJjZXMgYW5kIHJldHVybiBzb21lIHJlc291cmNlIGJ5IGRlbGV0ZSBzb21lIGNvbnRhaW5lcg0K
d2hlbiBwYXJ0cyBvZiBjb250YWluZXIgYWxyZWFkeSBoYXZlIGVub3VnaCBjb3Jlcy9tZW1vcnkg
PGJyPg0KPC9mb250Pjxmb250IHNpemU9MSBmYWNlPSJBcmlhbCI+IDwvZm9udD4NCjxwPjxmb250
IHNpemU9MSBmYWNlPSJBcmlhbCI+QmVzdCBSZWdhcmRzPC9mb250Pg0KPHA+PGZvbnQgc2l6ZT0x
IGZhY2U9IkFyaWFsIj4mbmJzcDs8L2ZvbnQ+DQo8YnI+PGZvbnQgc2l6ZT0zIGNvbG9yPSM4Zjhm
OGYgZmFjZT0iQXJpYWwiPjxiPkp1biBGZW5nIExpdTwvYj48L2ZvbnQ+PGZvbnQgc2l6ZT0xIGZh
Y2U9IkFyaWFsIj48YnI+DQpJQk0gQ2hpbmEgU3lzdGVtcyAmYW1wOyBUZWNobm9sb2d5IExhYm9y
YXRvcnkgaW4gQmVpamluZzwvZm9udD4NCjxwPg0KPHRhYmxlPg0KPHRyPg0KPHRkIGNvbHNwYW49
Mz4NCjxkaXYgYWxpZ249Y2VudGVyPg0KPGhyIG5vc2hhZGU+PC9kaXY+DQo8dHI+DQo8dGQgcm93
c3Bhbj0yPjxpbWcgc3JjPWNpZDpfMl8xMkJEQUQ2NDEyODY5NUZDMDAyMEFCMTE0ODI1N0QyRSBh
bHQ9IjJEIGJhcmNvZGUgLSBlbmNvZGVkIHdpdGggY29udGFjdCBpbmZvcm1hdGlvbiI+DQo8dGQ+
PGZvbnQgc2l6ZT0xIGNvbG9yPSM0MTgxYzAgZmFjZT0iy87M5SI+PGI+UGhvbmU6IDwvYj48L2Zv
bnQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM1ZjVmNWYgZmFjZT0iy87M5SI+ODYtMTAtODI0NTI2ODMN
CjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzQxODFjMD48Yj48YnI+DQpFLW1haWw6PC9iPjwv
Zm9udD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1Zj4gPC9mb250PjxhIGhyZWY9bWFpbHRvOmxp
dWp1bmZAY24uaWJtLmNvbSB0YXJnZXQ9X2JsYW5rPjxmb250IHNpemU9MSBjb2xvcj0jNWY1ZjVm
IGZhY2U9IsvOzOUiPjx1PmxpdWp1bmZAY24uaWJtLmNvbTwvdT48L2ZvbnQ+PC9hPg0KPHRkIHJv
d3NwYW49Mj4NCjxkaXYgYWxpZ249cmlnaHQ+PGltZyBzcmM9Y2lkOl8xXzEyQkRCNzEwMTJCREIz
M0MwMDIwQUIxMTQ4MjU3RDJFIHdpZHRoPTMyIGhlaWdodD0zMiBhbHQ9SUJNPjxmb250IHNpemU9
MSBjb2xvcj0jNWY1ZjVmPjxicj4NCjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1ZiBm
YWNlPSLLzszlIj48YnI+DQpCTEQgMjgsWkdDIFNvZnR3YXJlIFBhcmsgPGJyPg0KTm8uOCBSZC5E
b25nIEJlaSBXYW5nIFdlc3QsIERpc3QuSGFpZGlhbiBCZWlqaW5nIDEwMDE5MyA8YnI+DQpDaGlu
YSA8L2ZvbnQ+PC9kaXY+DQo8dHI+DQo8dGQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM1ZjVmNWY+Jm5i
c3A7PC9mb250PjwvdGFibGU+DQo8YnI+DQo8cD48Zm9udCBzaXplPTM+Jm5ic3A7PC9mb250Pg0K
PGJyPg0KPGJyPg0KPGJyPg0KPHRhYmxlIHdpZHRoPTEwMCU+DQo8dHIgdmFsaWduPXRvcD4NCjx0
ZCB3aWR0aD00MCU+PGZvbnQgc2l6ZT0xIGZhY2U9InNhbnMtc2VyaWYiPjxiPlBhdHJpY2sgV2Vu
ZGVsbCAmbHQ7cHdlbmRlbGxAZ21haWwuY29tJmd0OzwvYj4NCjwvZm9udD4NCjxwPjxmb250IHNp
emU9MSBmYWNlPSJzYW5zLXNlcmlmIj4yMDE0LzA4LzA4IDEzOjEwPC9mb250Pg0KPHRkIHdpZHRo
PTU5JT4NCjx0YWJsZSB3aWR0aD0xMDAlPg0KPHRyIHZhbGlnbj10b3A+DQo8dGQ+DQo8ZGl2IGFs
aWduPXJpZ2h0Pjxmb250IHNpemU9MSBmYWNlPSJzYW5zLXNlcmlmIj5UbzwvZm9udD48L2Rpdj4N
Cjx0ZD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+SnVuIEZlbmcgTGl1L0NoaW5hL0lC
TUBJQk1DTiwgPC9mb250Pg0KPHRyIHZhbGlnbj10b3A+DQo8dGQ+DQo8ZGl2IGFsaWduPXJpZ2h0
Pjxmb250IHNpemU9MSBmYWNlPSJzYW5zLXNlcmlmIj5jYzwvZm9udD48L2Rpdj4NCjx0ZD48Zm9u
dCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+JnF1b3Q7ZGV2QHNwYXJrLmFwYWNoZS5vcmcmcXVv
dDsgJmx0O2RldkBzcGFyay5hcGFjaGUub3JnJmd0OzwvZm9udD4NCjx0ciB2YWxpZ249dG9wPg0K
PHRkPg0KPGRpdiBhbGlnbj1yaWdodD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+U3Vi
amVjdDwvZm9udD48L2Rpdj4NCjx0ZD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+UmU6
IEZpbmUtR3JhaW5lZCBTY2hlZHVsZXIgb24gWWFybjwvZm9udD48L3RhYmxlPg0KPGJyPg0KPHRh
YmxlPg0KPHRyIHZhbGlnbj10b3A+DQo8dGQ+DQo8dGQ+PC90YWJsZT4NCjxicj48L3RhYmxlPg0K
PGJyPg0KPGJyPg0KPGJyPjxmb250IHNpemU9Mz5IZXkgc29ycnkgYWJvdXQgdGhhdCAtIHdoYXQg
SSBzYWlkIHdhcyB0aGUgb3Bwb3NpdGUgb2YNCndoYXQgaXMgdHJ1ZS48L2ZvbnQ+DQo8YnI+DQo8
YnI+PGZvbnQgc2l6ZT0zPlRoZSBjdXJyZW50IFlBUk4gbW9kZSBpcyBlcXVpdmFsZW50IHRvICZx
dW90O2NvYXJzZSBncmFpbmVkJnF1b3Q7DQptZXNvcy4gVGhlcmUgaXMgbm8gZmluZS1ncmFpbmVk
IHNjaGVkdWxpbmcgb24gWUFSTiBhdCB0aGUgbW9tZW50LiBJJ20gbm90DQpzdXJlIFlBUk4gc3Vw
cG9ydHMgc2NoZWR1bGluZyBpbiB1bml0cyBvdGhlciB0aGFuIGNvbnRhaW5lcnMuIEZpbmUtZ3Jh
aW5lZA0Kc2NoZWR1bGluZyByZXF1aXJlcyBzY2hlZHVsaW5nIGF0IHRoZSBncmFudWxhcml0eSBv
ZiBpbmRpdmlkdWFsIGNvcmVzLjwvZm9udD4NCjxicj48Zm9udCBzaXplPTM+PGJyPg0KPC9mb250
Pg0KPGJyPjxmb250IHNpemU9Mz5PbiBUaHUsIEF1ZyA3LCAyMDE0IGF0IDk6NDMgUE0sIFBhdHJp
Y2sgV2VuZGVsbCAmbHQ7PC9mb250PjxhIGhyZWY9bWFpbHRvOnB3ZW5kZWxsQGdtYWlsLmNvbSB0
YXJnZXQ9X2JsYW5rPjxmb250IHNpemU9MyBjb2xvcj1ibHVlPjx1PnB3ZW5kZWxsQGdtYWlsLmNv
bTwvdT48L2ZvbnQ+PC9hPjxmb250IHNpemU9Mz4mZ3Q7DQp3cm90ZTo8L2ZvbnQ+DQo8YnI+PGZv
bnQgc2l6ZT0zPlRoZSBjdXJyZW50IFlBUk4gaXMgZXF1aXZhbGVudCB0byB3aGF0IGlzIGNhbGxl
ZCAmcXVvdDtmaW5lDQpncmFpbmVkJnF1b3Q7IG1vZGUgaW4gTWVzb3MuIFRoZSBzY2hlZHVsaW5n
IG9mIHRhc2tzIGhhcHBlbnMgdG90YWxseSBpbnNpZGUNCm9mIHRoZSBTcGFyayBkcml2ZXIuPC9m
b250Pg0KPGJyPjxmb250IHNpemU9Mz48YnI+DQo8L2ZvbnQ+DQo8YnI+PGZvbnQgc2l6ZT0zPk9u
IFRodSwgQXVnIDcsIDIwMTQgYXQgNzo1MCBQTSwgSnVuIEZlbmcgTGl1ICZsdDs8L2ZvbnQ+PGEg
aHJlZj1tYWlsdG86bGl1anVuZkBjbi5pYm0uY29tIHRhcmdldD1fYmxhbms+PGZvbnQgc2l6ZT0z
IGNvbG9yPWJsdWU+PHU+bGl1anVuZkBjbi5pYm0uY29tPC91PjwvZm9udD48L2E+PGZvbnQgc2l6
ZT0zPiZndDsNCndyb3RlOjwvZm9udD4NCjxicj48Zm9udCBzaXplPTMgZmFjZT0ic2Fucy1zZXJp
ZiI+QW55IG9uZSBrbm93IHRoZSBhbnN3ZXI/PC9mb250Pg0KPGJyPjxmb250IHNpemU9MSBmYWNl
PSJBcmlhbCI+QmVzdCBSZWdhcmRzPC9mb250Pjxmb250IHNpemU9Mz4gPC9mb250Pg0KPHA+PGZv
bnQgc2l6ZT0xIGZhY2U9IkFyaWFsIj4mbmJzcDs8L2ZvbnQ+PGZvbnQgc2l6ZT0zPiA8L2ZvbnQ+
PGZvbnQgc2l6ZT0zIGNvbG9yPSM4ZjhmOGYgZmFjZT0iQXJpYWwiPjxiPjxicj4NCkp1biBGZW5n
IExpdTwvYj48L2ZvbnQ+PGZvbnQgc2l6ZT0xIGZhY2U9IkFyaWFsIj48YnI+DQpJQk0gQ2hpbmEg
U3lzdGVtcyAmYW1wOyBUZWNobm9sb2d5IExhYm9yYXRvcnkgaW4gQmVpamluZzwvZm9udD48Zm9u
dCBzaXplPTM+DQo8L2ZvbnQ+DQo8cD4NCjx0YWJsZT4NCjx0cj4NCjx0ZCBjb2xzcGFuPTM+DQo8
ZGl2IGFsaWduPWNlbnRlcj4NCjxociBub3NoYWRlPjwvZGl2Pg0KPHRyPg0KPHRkIHJvd3NwYW49
Mj4NCjx0ZD48Zm9udCBzaXplPTEgY29sb3I9IzQxODFjMCBmYWNlPSLLzszlIj48Yj5QaG9uZTog
PC9iPjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1ZiBmYWNlPSLLzszlIj44Ni0xMC04
MjQ1MjY4Mw0KPC9mb250Pjxmb250IHNpemU9MSBjb2xvcj0jNDE4MWMwPjxiPjxicj4NCkUtbWFp
bDo8L2I+PC9mb250Pjxmb250IHNpemU9MSBjb2xvcj0jNWY1ZjVmPiA8L2ZvbnQ+PGEgaHJlZj1t
YWlsdG86bGl1anVuZkBjbi5pYm0uY29tIHRhcmdldD1fYmxhbms+PGZvbnQgc2l6ZT0xIGNvbG9y
PSM1ZjVmNWYgZmFjZT0iy87M5SI+PHU+bGl1anVuZkBjbi5pYm0uY29tPC91PjwvZm9udD48L2E+
PGZvbnQgc2l6ZT0zPg0KPC9mb250Pg0KPHRkIHJvd3NwYW49Mj4NCjxkaXYgYWxpZ249cmlnaHQ+
PGZvbnQgc2l6ZT0xIGNvbG9yPSM1ZjVmNWYgZmFjZT0iy87M5SI+PGJyPg0KPGJyPg0KQkxEIDI4
LFpHQyBTb2Z0d2FyZSBQYXJrIDxicj4NCk5vLjggUmQuRG9uZyBCZWkgV2FuZyBXZXN0LCBEaXN0
LkhhaWRpYW4gQmVpamluZyAxMDAxOTMgPGJyPg0KQ2hpbmEgPC9mb250PjwvZGl2Pg0KPHRyPg0K
PHRkPjxmb250IHNpemU9MSBjb2xvcj0jNWY1ZjVmPiZuYnNwOzwvZm9udD48L3RhYmxlPg0KPGJy
Pg0KPHA+PGZvbnQgc2l6ZT0zPiZuYnNwOyA8YnI+DQo8YnI+DQo8L2ZvbnQ+DQo8cD4NCjx0YWJs
ZSB3aWR0aD0xMDAlPg0KPHRyIHZhbGlnbj10b3A+DQo8dGQgd2lkdGg9NDElPjxmb250IHNpemU9
MSBmYWNlPSJzYW5zLXNlcmlmIj48Yj5KdW4gRmVuZyBMaXUvQ2hpbmEvSUJNPC9iPg0KPC9mb250
Pg0KPHA+PGZvbnQgc2l6ZT0xIGZhY2U9InNhbnMtc2VyaWYiPjIwMTQvMDgvMDcgMTU6Mzc8L2Zv
bnQ+PGZvbnQgc2l6ZT0zPg0KPC9mb250Pg0KPHRkIHdpZHRoPTU4JT4NCjxicj4NCjx0YWJsZSB3
aWR0aD0xMDAlPg0KPHRyIHZhbGlnbj10b3A+DQo8dGQgd2lkdGg9MTklPg0KPGRpdiBhbGlnbj1y
aWdodD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+VG88L2ZvbnQ+PC9kaXY+DQo8dGQg
d2lkdGg9ODAlPjxhIGhyZWY9bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnIHRhcmdldD1fYmxh
bms+PGZvbnQgc2l6ZT0xIGNvbG9yPWJsdWUgZmFjZT0ic2Fucy1zZXJpZiI+PHU+ZGV2QHNwYXJr
LmFwYWNoZS5vcmc8L3U+PC9mb250PjwvYT48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1zZXJpZiI+
LA0KPC9mb250Pg0KPHRyIHZhbGlnbj10b3A+DQo8dGQ+DQo8ZGl2IGFsaWduPXJpZ2h0Pjxmb250
IHNpemU9MSBmYWNlPSJzYW5zLXNlcmlmIj5jYzwvZm9udD48L2Rpdj4NCjx0ZD4NCjx0ciB2YWxp
Z249dG9wPg0KPHRkPg0KPGRpdiBhbGlnbj1yaWdodD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1z
ZXJpZiI+U3ViamVjdDwvZm9udD48L2Rpdj4NCjx0ZD48Zm9udCBzaXplPTEgZmFjZT0ic2Fucy1z
ZXJpZiI+RmluZS1HcmFpbmVkIFNjaGVkdWxlciBvbiBZYXJuPC9mb250PjwvdGFibGU+DQo8YnI+
DQo8YnI+DQo8dGFibGU+DQo8dHIgdmFsaWduPXRvcD4NCjx0ZD4NCjx0ZD48L3RhYmxlPg0KPGJy
PjwvdGFibGU+DQo8YnI+PGZvbnQgc2l6ZT0zPjxicj4NCjwvZm9udD48Zm9udCBzaXplPTMgZmFj
ZT0ic2Fucy1zZXJpZiI+PGJyPg0KSGksIHRoZXJlPC9mb250Pjxmb250IHNpemU9Mz4gPGJyPg0K
PC9mb250Pjxmb250IHNpemU9MyBmYWNlPSJzYW5zLXNlcmlmIj48YnI+DQpKdXN0IGF3YXJlIHJp
Z2h0IG5vdyBTcGFyayBvbmx5IHN1cHBvcnQgZmluZSBncmFpbmVkIHNjaGVkdWxlciBvbiBNZXNv
cw0Kd2l0aCBNZXNvc1NjaGVkdWxlckJhY2tlbmQuIFRoZSBZYXJuIHNjaGVkdWxlIHNvdW5kcyBs
aWtlIG9ubHkgd29ya3Mgb24NCmNvYXJzZS1ncmFpbmVkIG1vZGVsLiBJcyB0aGVyZSBhbnkgcGxh
biB0byBpbXBsZW1lbnQgZmluZS1ncmFpbmVkIHNjaGVkdWxlcg0KZm9yIFlBUk4/IE9yIHRoZXJl
IGlzIGFueSB0ZWNobmljYWwgaXNzdWUgYmxvY2sgdXMgdG8gZG8gdGhhdC48L2ZvbnQ+DQo8cD48
Zm9udCBzaXplPTEgZmFjZT0iQXJpYWwiPkJlc3QgUmVnYXJkczwvZm9udD48Zm9udCBzaXplPTM+
IDwvZm9udD4NCjxwPjxmb250IHNpemU9MSBmYWNlPSJBcmlhbCI+Jm5ic3A7PC9mb250Pjxmb250
IHNpemU9Mz4gPC9mb250Pjxmb250IHNpemU9MyBjb2xvcj0jOGY4ZjhmIGZhY2U9IkFyaWFsIj48
Yj48YnI+DQpKdW4gRmVuZyBMaXU8L2I+PC9mb250Pjxmb250IHNpemU9MSBmYWNlPSJBcmlhbCI+
PGJyPg0KSUJNIENoaW5hIFN5c3RlbXMgJmFtcDsgVGVjaG5vbG9neSBMYWJvcmF0b3J5IGluIEJl
aWppbmc8L2ZvbnQ+PGZvbnQgc2l6ZT0zPg0KPC9mb250Pg0KPHA+DQo8dGFibGU+DQo8dHI+DQo8
dGQgY29sc3Bhbj0zPg0KPGRpdiBhbGlnbj1jZW50ZXI+DQo8aHIgbm9zaGFkZT48L2Rpdj4NCjx0
cj4NCjx0ZCByb3dzcGFuPTI+DQo8dGQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM0MTgxYzAgZmFjZT0i
y87M5SI+PGI+UGhvbmU6IDwvYj48L2ZvbnQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM1ZjVmNWYgZmFj
ZT0iy87M5SI+ODYtMTAtODI0NTI2ODMNCjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzQxODFj
MD48Yj48YnI+DQpFLW1haWw6PC9iPjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1Zj4g
PC9mb250PjxhIGhyZWY9bWFpbHRvOmxpdWp1bmZAY24uaWJtLmNvbSB0YXJnZXQ9X2JsYW5rPjxm
b250IHNpemU9MSBjb2xvcj0jNWY1ZjVmIGZhY2U9IsvOzOUiPjx1PmxpdWp1bmZAY24uaWJtLmNv
bTwvdT48L2ZvbnQ+PC9hPjxmb250IHNpemU9Mz4NCjwvZm9udD4NCjx0ZCByb3dzcGFuPTI+DQo8
ZGl2IGFsaWduPXJpZ2h0Pjxmb250IHNpemU9MSBjb2xvcj0jNWY1ZjVmIGZhY2U9IsvOzOUiPjxi
cj4NCjxicj4NCkJMRCAyOCxaR0MgU29mdHdhcmUgUGFyayA8YnI+DQpOby44IFJkLkRvbmcgQmVp
IFdhbmcgV2VzdCwgRGlzdC5IYWlkaWFuIEJlaWppbmcgMTAwMTkzIDxicj4NCkNoaW5hIDwvZm9u
dD48L2Rpdj4NCjx0cj4NCjx0ZD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1Zj4mbmJzcDs8L2Zv
bnQ+PC90YWJsZT4NCjxicj4NCjxwPjxmb250IHNpemU9Mz4mbmJzcDsgPC9mb250Pg0KPHA+DQo8
YnI+DQo8YnI+DQo=
--=_alternative 0020AB1448257D2E_=--
--=_related 0020AB1448257D2E_=--


From dev-return-8782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:00:03 2014
Return-Path: <dev-return-8782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64215117D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:00:03 +0000 (UTC)
Received: (qmail 8424 invoked by uid 500); 8 Aug 2014 07:00:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8361 invoked by uid 500); 8 Aug 2014 07:00:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 95798 invoked by uid 99); 8 Aug 2014 04:58:02 -0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:from:to:cc:message-id:subject:mime-version
         :content-type:content-transfer-encoding:content-disposition;
        bh=JOlye3fL6azgqA4i4DtDTvi8vqcSzeYkVFc9IDpt8lg=;
        b=POye9CbQwRkWQsqrd/iT4lNsIzsm+G0efMPjljEACqRl9XRyKGgFI3kQL99on5mKWz
         hPFx3JLuLWGDG4+iwQ7wfZtbncnrgf42nFGeouFDXKEXlakzwjJumDqCVTSFyddldsxY
         duEBARxbdzha1wouc2uuttOsNhek7wt+A3MZY7PNOQqzGIWxta/zZ7a/59E7GFysaaeF
         fGPL1I2v7SNVF81IfLlc8HNV0roTMohsTbBVAXjkcGUpD1kn1NkvTgk2vtWuWmhnJBIE
         q+jHPiPBnrbM5OQWPZduAki84GfM4sMtrbydu8zv3owF9r7r6ExUKOH/7Sw/mFFfDSJ6
         hTsA==
X-Gm-Message-State: ALoCoQkLHVK2iD5nlw47vw8PUomDq3683iIZnx/4q8+3xPKX+8ks2GV6WXZsf9cfUUkt5MiUcmzz
X-Received: by 10.68.68.207 with SMTP id y15mr21881194pbt.25.1407473853076;
        Thu, 07 Aug 2014 21:57:33 -0700 (PDT)
Date: Thu, 7 Aug 2014 21:57:30 -0700
From: Matei Zaharia <matei@databricks.com>
To: "=?utf-8?Q?dev=40spark.incubator.apache.org?="
 <dev@spark.incubator.apache.org>
Cc: Joseph Gonzalez <jegonzal@eecs.berkeley.edu>, andrew@databricks.com
Message-ID: <etPan.53e458bb.515f007c.119a7@mbp-3.local>
Subject: Welcoming two new committers
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline
X-Virus-Checked: Checked by ClamAV on apache.org

Hi everyone,

The PMC recently voted to add two new committers and PMC members: Joey Gonzalez and Andrew Or. Both have been huge contributors in the past year -- Joey on much of GraphX as well as quite a bit of the initial work in MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!

Matei




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:00:10 2014
Return-Path: <dev-return-8783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A160117DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:00:10 +0000 (UTC)
Received: (qmail 9500 invoked by uid 500); 8 Aug 2014 07:00:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9440 invoked by uid 500); 8 Aug 2014 07:00:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 72068 invoked by uid 99); 8 Aug 2014 05:39:45 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of joseph.e.gonzalez@gmail.com designates 209.85.220.174 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=r0Y5Hx4gHOcUB1Xpr4NIDo/ZwIpxcd6uT+5ZntuQiSE=;
        b=NxEMHgO4ddxJyM4Cc5b1B2I1gQqqA427pJbsRiosxnL8V9SmPCQ6pw5ogDGA1wv5Lk
         A9jFANjqaVk1OevCBBoh5Of49DU5li319pGLeC9/eK+DzWJ4dimQ8nY3tIjAT1L0g/Gq
         OdHf85QTv+k8HqAO2S265e08RBOP+9nSVbP7mEwrI3yqJjLzdunQO/AUWekyQzO2gc8P
         hmY6/ZsRobchNmb7yeqOQHdEP3hXRBgjciQLkSM5oSOdPYopX7zSUSsWXIbJhFRoyRjO
         XGKmy+jrNpcs0n6kr4uytKvz0FHZRCO+VUQlLIjPxwx/7opFznOG72jXpGWh0RMQ+mMG
         DMEQ==
MIME-Version: 1.0
X-Received: by 10.220.190.197 with SMTP id dj5mr20020539vcb.19.1407476360544;
 Thu, 07 Aug 2014 22:39:20 -0700 (PDT)
Sender: joseph.e.gonzalez@gmail.com
In-Reply-To: <etPan.53e458bb.515f007c.119a7@mbp-3.local>
References: <etPan.53e458bb.515f007c.119a7@mbp-3.local>
Date: Thu, 7 Aug 2014 22:39:20 -0700
X-Google-Sender-Auth: n28bLXIvL8Uy7Jbj72IfFOjrpmU
Message-ID: <CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
Subject: Re: Welcoming two new committers
From: Joseph Gonzalez <jegonzal@eecs.berkeley.edu>
To: Matei Zaharia <matei@databricks.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, andrew@databricks.com
Content-Type: multipart/alternative; boundary=001a11c1c272800857050017a07a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c272800857050017a07a
Content-Type: text/plain; charset=UTF-8

Hi Everyone,

Thank you for inviting me to be a committer.  I look forward to working
with everyone to ensure the continued success of the Spark project.

Thanks!
Joey




On Thu, Aug 7, 2014 at 9:57 PM, Matei Zaharia <matei@databricks.com> wrote:

> Hi everyone,
>
> The PMC recently voted to add two new committers and PMC members: Joey
> Gonzalez and Andrew Or. Both have been huge contributors in the past year
> -- Joey on much of GraphX as well as quite a bit of the initial work in
> MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!
>
> Matei
>
>
>
>

--001a11c1c272800857050017a07a--

From dev-return-8784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:15:23 2014
Return-Path: <dev-return-8784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 99A8B1182E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:15:23 +0000 (UTC)
Received: (qmail 33424 invoked by uid 500); 8 Aug 2014 07:15:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33360 invoked by uid 500); 8 Aug 2014 07:15:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33349 invoked by uid 99); 8 Aug 2014 07:15:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:15:22 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:15:17 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so5011321qae.20
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 00:14:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=zWb1dfAi+XDhxGNHfpOHW3aKqn4QXQKyIo9RIxbpJz4=;
        b=jUGeqjfzoBWdIURiJ9oBjDSdB7k3yzuqkkGFQRdj4bH3VghUYiWvWpk958LRQKrGrk
         WEhTkhENjHbu3HcwhOA5SQAwME4mlNqB9kOu5Chqg53w7dxbQDBYoE7RoRfVnljCK8F4
         YUgMze9L3AS2ks4NUI/4eUicK89i1dcz55D0O1+IdS373kMjfhLXneCUluEBnQcSNCTx
         6Mt0HHVjp4HpkiQjz/XjGib/6YJm5GYpRtkfQRQHXsZI3pg4rR/YWwe1unc/HTH0jjKi
         lmrN49JHiaFQP8BvaOtABUOcJyn6FjWirqToltevBA4qOZlFrNhD1HSU1PY9cPXkd1zk
         dJEg==
X-Gm-Message-State: ALoCoQkkgjWAjTLSUyOyCF41KETfCwN322C1wGhc6CrSr8/LuoYNxiBevrymPL7T4kwMyiBhcNem
MIME-Version: 1.0
X-Received: by 10.140.31.72 with SMTP id e66mr20437671qge.89.1407482096386;
 Fri, 08 Aug 2014 00:14:56 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Fri, 8 Aug 2014 00:14:56 -0700 (PDT)
In-Reply-To: <OFE4B5E67A.527C58A9-ON48257D2E.001FC9BF-48257D2E.0020AB17@cn.ibm.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
	<CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>
	<CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
	<OFE4B5E67A.527C58A9-ON48257D2E.001FC9BF-48257D2E.0020AB17@cn.ibm.com>
Date: Fri, 8 Aug 2014 00:14:56 -0700
Message-ID: <CACBYxKL6rw9G6MYYmGKMgq4raMgdxpPbte-janTHZA9rj1-Fow@mail.gmail.com>
Subject: Re: Fine-Grained Scheduler on Yarn
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a9af06225ae050018f6d9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9af06225ae050018f6d9
Content-Type: text/plain; charset=UTF-8

Hi Jun,

Spark currently doesn't have that feature, i.e. it aims for a fixed number
of executors per application regardless of resource usage, but it's
definitely worth considering.  We could start more executors when we have a
large backlog of tasks and shut some down when we're underutilized.

The fine-grained task scheduling is blocked on work from YARN that will
allow changing the CPU allocation of a YARN container dynamically.  The
relevant JIRA for this dependency is YARN-1197, though YARN-1488 might
serve this purpose as well if it comes first.

-Sandy


On Thu, Aug 7, 2014 at 10:56 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Thanks for echo on this. Possible to adjust resource based on container
> numbers? e.g to allocate more container when driver need more resources and
> return some resource by delete some container when parts of container
> already have enough cores/memory
>
> Best Regards
>
>
> *Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information]
> *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Patrick Wendell <pwendell@gmail.com <pwendell@gmail.com>>*
>
> 2014/08/08 13:10
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> "dev@spark.apache.org" <dev@spark.apache.org>
> Subject
> Re: Fine-Grained Scheduler on Yarn
>
>
>
>
> Hey sorry about that - what I said was the opposite of what is true.
>
> The current YARN mode is equivalent to "coarse grained" mesos. There is no
> fine-grained scheduling on YARN at the moment. I'm not sure YARN supports
> scheduling in units other than containers. Fine-grained scheduling requires
> scheduling at the granularity of individual cores.
>
>
> On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell <*pwendell@gmail.com*
> <pwendell@gmail.com>> wrote:
> The current YARN is equivalent to what is called "fine grained" mode in
> Mesos. The scheduling of tasks happens totally inside of the Spark driver.
>
>
> On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <*liujunf@cn.ibm.com*
> <liujunf@cn.ibm.com>> wrote:
> Any one know the answer?
> Best Regards
>
>
> * Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>   *Jun Feng Liu/China/IBM*
>
> 2014/08/07 15:37
>
>   To
> *dev@spark.apache.org* <dev@spark.apache.org>,
> cc
>   Subject
> Fine-Grained Scheduler on Yarn
>
>
>
>
>
> Hi, there
>
> Just aware right now Spark only support fine grained scheduler on Mesos
> with MesosSchedulerBackend. The Yarn schedule sounds like only works on
> coarse-grained model. Is there any plan to implement fine-grained scheduler
> for YARN? Or there is any technical issue block us to do that.
>
> Best Regards
>
>
> * Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>
>

--001a113a9af06225ae050018f6d9--

From dev-return-8785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:15:44 2014
Return-Path: <dev-return-8785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 270AC11831
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:15:44 +0000 (UTC)
Received: (qmail 34730 invoked by uid 500); 8 Aug 2014 07:15:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34671 invoked by uid 500); 8 Aug 2014 07:15:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34656 invoked by uid 99); 8 Aug 2014 07:15:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:15:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:15:38 +0000
Received: by mail-ie0-f175.google.com with SMTP id x19so5941028ier.20
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 00:15:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=BbdxddH5KWBmHdVTOc0KM/bTgCh6kSiLlTbW3w39mB4=;
        b=FimmbDnMQhLfRylX3X3EXwC5VWWpoBaDROrBu1FROeP5jc5GVoTVZkU5/o8xIN+pDz
         r+RLxAQbvJ1/Wt6MaMTKFXSez2GWd156e8RSqVPfd2Otynwtk0k8iEBYsF9lZICowXAS
         AVLI6dl81u4E1GKsa16iw4cnY7DuuskG9AWts=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=BbdxddH5KWBmHdVTOc0KM/bTgCh6kSiLlTbW3w39mB4=;
        b=ZDlQkam4cQc40WCUBIQy0ur8abbDIXkn7o4YMSNKBKrRf0Tp90eQiqoHjI4kkraYOF
         hwNrav7BGXKW1xbrsdbB76r8m1sBCnBqjTxtPCKgYzIVOb3dI/98qhMq5PcLOWBQbOnT
         rKqcMgDNPb0nGZvXlHyizw4fYllmpaFe0H1T2XX5GYKJCBAJnCqIYbHt4MaVcutGeC3/
         hcapsesOkDTu0PXfs9jeIhqzjyYPYkMuYEj55Je45awm5TJwAy08Vse72Bpx95GHwTWJ
         nWBaXA8I23MuVb/eJnMR5fweQiOOk0P+gQ7fQkzqHX5uUk6vhVfDagBJtlcSzf3ESFe+
         ATqA==
X-Gm-Message-State: ALoCoQnPKscIOwaEkPvFO7cd4pR2/53Y88+RJQX09Qn5FlBMuMkk+1hf77ZsQXLPCiZCRggCvPL5
X-Received: by 10.50.33.100 with SMTP id q4mr2575902igi.8.1407482115481; Fri,
 08 Aug 2014 00:15:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.238.20 with HTTP; Fri, 8 Aug 2014 00:14:55 -0700 (PDT)
X-Originating-IP: [76.21.63.199]
In-Reply-To: <CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
References: <etPan.53e458bb.515f007c.119a7@mbp-3.local> <CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Fri, 8 Aug 2014 00:14:55 -0700
Message-ID: <CAGh_TuMkcvw=Rb94jLYeSh-pcGDZ-cEi2FBRMbwPbagfTObQ4A@mail.gmail.com>
Subject: Re: Welcoming two new committers
To: Joseph Gonzalez <jegonzal@eecs.berkeley.edu>
Cc: Matei Zaharia <matei@databricks.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, andrew@databricks.com
Content-Type: multipart/alternative; boundary=089e0158b034858622050018f7c8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b034858622050018f7c8
Content-Type: text/plain; charset=UTF-8

+1 Joey & Andrew :)

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com> [ah-'DAY-tao]
linkedin.com/in/ctnguyen



On Thu, Aug 7, 2014 at 10:39 PM, Joseph Gonzalez <jegonzal@eecs.berkeley.edu
> wrote:

> Hi Everyone,
>
> Thank you for inviting me to be a committer.  I look forward to working
> with everyone to ensure the continued success of the Spark project.
>
> Thanks!
> Joey
>
>
>
>
> On Thu, Aug 7, 2014 at 9:57 PM, Matei Zaharia <matei@databricks.com>
> wrote:
>
> > Hi everyone,
> >
> > The PMC recently voted to add two new committers and PMC members: Joey
> > Gonzalez and Andrew Or. Both have been huge contributors in the past year
> > -- Joey on much of GraphX as well as quite a bit of the initial work in
> > MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!
> >
> > Matei
> >
> >
> >
> >
>

--089e0158b034858622050018f7c8--

From dev-return-8786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:19:18 2014
Return-Path: <dev-return-8786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31ADD11866
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:19:18 +0000 (UTC)
Received: (qmail 44233 invoked by uid 500); 8 Aug 2014 07:19:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44171 invoked by uid 500); 8 Aug 2014 07:19:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44159 invoked by uid 99); 8 Aug 2014 07:19:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:19:17 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:19:14 +0000
Received: by mail-qg0-f42.google.com with SMTP id j5so5609911qga.15
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 00:18:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=OHH1hvHjJgHTOXvjelC9Dys0fZRcywmLuozBq28V8Fo=;
        b=lhWL1wlTmrhp1HiHnuSvmfIN3Ad/NLm2oj5dQhzzuSxvxr0KyG+pqZ0Di+kL4G79nl
         2lFDajeFty0eTUv5MVmntKkEEqvgMZeHtIDroHC8eQpNg002sNfXWuFZNcXBwGS5YTyw
         fMkjebQixGPD06Xp/leEGKip68Gn0sPduV28gjA3w1hEsipfeqf2eNAU8b0rYMsUpEv7
         WNx3TPMgolr/3YUhpobI+vMhWYq9SJkZrGHAYdcazRXsUGcppV8cvk0kw5DpbTzqi4CR
         2wF+EfalM9pko4Kjp33M6DirUu3L1jKLBhdQl7TA7WtTVMBYLUX4dxZ2aROfIQiWrsgO
         G7Dg==
MIME-Version: 1.0
X-Received: by 10.224.104.1 with SMTP id m1mr35398830qao.81.1407482328844;
 Fri, 08 Aug 2014 00:18:48 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Fri, 8 Aug 2014 00:18:48 -0700 (PDT)
In-Reply-To: <CABPQxsvSF3iPC1QHamx3QjwWjyMORSnxie+F2fwMAycXFsAN5w@mail.gmail.com>
References: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
	<CABPQxsvSF3iPC1QHamx3QjwWjyMORSnxie+F2fwMAycXFsAN5w@mail.gmail.com>
Date: Fri, 8 Aug 2014 00:18:48 -0700
Message-ID: <CA+B-+fyU-80UkjJ-rB-po83mxMuekkAt6qPH5dXLAGKG-oxxCg@mail.gmail.com>
Subject: Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted
From: Debasish Das <debasish.das83@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c248e03d100e0500190492
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c248e03d100e0500190492
Content-Type: text/plain; charset=UTF-8

Hi Patrick,

I am testing the 1.1 branch but I see lot of protobuf warnings while
building the jars:

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

I am compiling for 2.3.0cdh5.0.2...Later when running the jobs I am getting
a protobuf error:

Exception in thread "main" java.lang.VerifyError: class
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto
overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;

Is there a protobuf issue on this branch ?

Also on the branch at least I am noticing the following:

Welcome to

      ____              __

     / __/__  ___ _____/ /__

    _\ \/ _ \/ _ `/ __/  '_/

   /___/ .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT

      /_/

Won't it be 1.1.0-SNAPSHOT ?

Thanks.

Deb


On Wed, Aug 6, 2014 at 11:24 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Minor correction: the encoded URL in the staging repo link was wrong.
> The correct repo is:
> https://repository.apache.org/content/repositories/orgapachespark-1025/
>
>
> On Wed, Aug 6, 2014 at 11:23 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> > Hi All,
> >
> > I've packaged and published a snapshot release of Spark 1.1 for testing.
> This is being distributed to the community for QA and preview purposes. It
> is not yet an official RC for voting. Going forward, we'll do preview
> releases like this for testing ahead of official votes.
> >
> > The tag of this release is v1.1.0-snapshot1 (commit d428d8):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0
> >
> > The release files, including signatures, digests, etc can be found at:
> > http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1025/
> >
> > NOTE: Due to SPARK-2899, docs are not yet available for this release.
> Docs will be posted ASAP.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c248e03d100e0500190492--

From dev-return-8787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:26:05 2014
Return-Path: <dev-return-8787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C0B611880
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:26:05 +0000 (UTC)
Received: (qmail 57140 invoked by uid 500); 8 Aug 2014 07:26:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57084 invoked by uid 500); 8 Aug 2014 07:26:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57071 invoked by uid 99); 8 Aug 2014 07:26:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:26:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of scrapcodes@gmail.com designates 209.85.220.169 as permitted sender)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:25:58 +0000
Received: by mail-vc0-f169.google.com with SMTP id le20so7911708vcb.0
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 00:25:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=8Yhb8mFQdHQw2mslctY6oTxPpPiGXmHRnT+Fjog9t38=;
        b=jJDRW044Hv3Ut9er/mE6DY1Cwv4wUMPElDKVuLPFrcFBcoKtO7bopgX8jnmGuhLSXo
         /iUwMHlVogFkkdsJCBCv++FiFA7Q6kY10HRi7RvGQ4udLyHZn2zhkw4IXsi7xTZxr3cV
         IaZWc6CJ/R4TopJNfp25lT/GKAynFgha6av2r6PMdIO3jOkZVXtg7wpmmgM1h6cgEA1V
         VW5mmrY9Og5rHnIIymwx7bBR/3RilSrbo7Aam+uCrwtt08pDqLFMUdQd4knNPCI9NjYK
         zmf5Ery+R29BWl9fKhgq0EtMLURPPXtbMZuW2Ip0lIiwe5a/v/v3CIEttUjmwNg9syST
         6y+w==
X-Received: by 10.220.190.197 with SMTP id dj5mr20387201vcb.19.1407482738031;
 Fri, 08 Aug 2014 00:25:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.231.10 with HTTP; Fri, 8 Aug 2014 00:25:17 -0700 (PDT)
In-Reply-To: <CA+B-+fyU-80UkjJ-rB-po83mxMuekkAt6qPH5dXLAGKG-oxxCg@mail.gmail.com>
References: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
 <CABPQxsvSF3iPC1QHamx3QjwWjyMORSnxie+F2fwMAycXFsAN5w@mail.gmail.com> <CA+B-+fyU-80UkjJ-rB-po83mxMuekkAt6qPH5dXLAGKG-oxxCg@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Fri, 8 Aug 2014 12:55:17 +0530
Message-ID: <CAOYDGoAx+XuudF5wUPH18SJOgcTj8A8p0qPZim0gDHQrpDZbLg@mail.gmail.com>
Subject: Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted
To: Debasish Das <debasish.das83@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1c272a0c25b0500191c0f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c272a0c25b0500191c0f
Content-Type: text/plain; charset=UTF-8

Yeah this should be changed. You can change the banner in the repl,
printWelcome function. Mind sending a PR ?

I think this should be a one place change in the future (Not sure how
feasible it is). Volunteers ?


Prashant Sharma




On Fri, Aug 8, 2014 at 12:48 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi Patrick,
>
> I am testing the 1.1 branch but I see lot of protobuf warnings while
> building the jars:
>
> [warn] Class com.google.protobuf.Parser not found - continuing with a stub.
>
> [warn] Class com.google.protobuf.Parser not found - continuing with a stub.
>
> [warn] Class com.google.protobuf.Parser not found - continuing with a stub.
>
> [warn] Class com.google.protobuf.Parser not found - continuing with a stub.
>
> I am compiling for 2.3.0cdh5.0.2...Later when running the jobs I am getting
> a protobuf error:
>
> Exception in thread "main" java.lang.VerifyError: class
>
> org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto
> overrides final method
> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
>
> Is there a protobuf issue on this branch ?
>
> Also on the branch at least I am noticing the following:
>
> Welcome to
>
>       ____              __
>
>      / __/__  ___ _____/ /__
>
>     _\ \/ _ \/ _ `/ __/  '_/
>
>    /___/ .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT
>
>       /_/
>
> Won't it be 1.1.0-SNAPSHOT ?
>
> Thanks.
>
> Deb
>
>
> On Wed, Aug 6, 2014 at 11:24 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Minor correction: the encoded URL in the staging repo link was wrong.
> > The correct repo is:
> > https://repository.apache.org/content/repositories/orgapachespark-1025/
> >
> >
> > On Wed, Aug 6, 2014 at 11:23 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > >
> > > Hi All,
> > >
> > > I've packaged and published a snapshot release of Spark 1.1 for
> testing.
> > This is being distributed to the community for QA and preview purposes.
> It
> > is not yet an official RC for voting. Going forward, we'll do preview
> > releases like this for testing ahead of official votes.
> > >
> > > The tag of this release is v1.1.0-snapshot1 (commit d428d8):
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0
> > >
> > > The release files, including signatures, digests, etc can be found at:
> > > http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/pwendell.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1025/
> > >
> > > NOTE: Due to SPARK-2899, docs are not yet available for this release.
> > Docs will be posted ASAP.
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a11c1c272a0c25b0500191c0f--

From dev-return-8788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:28:26 2014
Return-Path: <dev-return-8788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9140911887
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:28:26 +0000 (UTC)
Received: (qmail 60803 invoked by uid 500); 8 Aug 2014 07:28:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60742 invoked by uid 500); 8 Aug 2014 07:28:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60730 invoked by uid 99); 8 Aug 2014 07:28:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:28:25 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 184.105.206.27 as permitted sender)
Received: from [184.105.206.27] (HELO smtpbg302.qq.com) (184.105.206.27)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:28:20 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1407482873; bh=UihMpfdeMm4lGUzfVZnGYVmzX/xXR4uED7dCBstTelY=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Cc:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE:X-QQ-FName:X-QQ-LocalIP;
	b=k4JKhvvm+5fLPuMWJpsH6B5SUysqw30w0ugAg55qm/9kAtE0GUhMb00yHkjyBmwH0
	 Wzr4r1EndMr1gLq7Fxd8u5nfSpGHhx4kSbEz+h1GMbFhtwjnSQmp+YY4dGOMl4/yZy
	 r26DPq7GsNGII4czPJCqMvmIkrs37y1uiaH/nf50=
X-QQ-FEAT: 6gzCEv7TRslGbiIE/+8DLdeBHX0aT4B9FATc8+UYeYGegNQ/Hu3y3+4tijVj2
	K2w/pSlZqwpMwTTin+kNXl9qYTQU4zQq/qadKfX5biO0dMN8nxYkNfQuS5uHhtrGDXrg+9j
	brGSBr1QemYZbNmeyQlKsAzptkmmIgRWu51xIuFsjCjyeVpmb6Q1oNltW5zB16LVIv/KfoF
	7UjKBK6wdMQ==
X-QQ-SSF: 000000000000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <CA+B-+fyU-80UkjJ-rB-po83mxMuekkAt6qPH5dXLAGKG-oxxCg@mail.gmail.com>
References: <CABPQxssfFPgZOGPK8ohgq8mvMtGgWqHh9q3aoi_1=sQphJ-gvg@mail.gmail.com>
	<CABPQxsvSF3iPC1QHamx3QjwWjyMORSnxie+F2fwMAycXFsAN5w@mail.gmail.com>
	<CA+B-+fyU-80UkjJ-rB-po83mxMuekkAt6qPH5dXLAGKG-oxxCg@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1407482869t1463759
From: "=?gb18030?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?gb18030?B?RGViYXNpc2ggRGFz?=" <debasish.das83@gmail.com>, "=?gb18030?B?UGF0cmljayBXZW5kZWxs?=" <pwendell@gmail.com>
Cc: "=?gb18030?B?ZGV2?=" <dev@spark.apache.org>
Subject: =?gb18030?B?u9i4tKO6IFtTTkFQU0hPVF0gU25hcHNob3QxIG9m?=
 =?gb18030?B?IFNwYXJrIDEuMS4wIGhhcyBiZWVuIHBvc3RlZA==?=
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53E47BF5_09082880_03D23DFE"
Content-Transfer-Encoding: 8Bit
Date: Fri, 8 Aug 2014 15:27:49 +0800
X-Priority: 3
Message-ID: <tencent_417CAAFB2947B0C202486305@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 57796048
X-QQ-SENDSIZE: 520
X-QQ-FName: C099372C899D4D3C8D592389F5AA2FCC
X-QQ-LocalIP: 112.95.241.173
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53E47BF5_09082880_03D23DFE
Content-Type: text/plain;
	charset="gb18030"
Content-Transfer-Encoding: base64
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------=_NextPart_53E47BF5_09082880_03D23DFE--


From dev-return-8789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:38:44 2014
Return-Path: <dev-return-8789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB0C3118C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:38:43 +0000 (UTC)
Received: (qmail 79657 invoked by uid 500); 8 Aug 2014 07:38:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79595 invoked by uid 500); 8 Aug 2014 07:38:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79583 invoked by uid 99); 8 Aug 2014 07:38:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:38:42 +0000
X-ASF-Spam-Status: No, hits=-2.5 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.148 as permitted sender)
Received: from [202.81.31.148] (HELO e23smtp06.au.ibm.com) (202.81.31.148)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:38:37 +0000
Received: from /spool/local
	by e23smtp06.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Fri, 8 Aug 2014 17:38:10 +1000
Received: from d23dlp03.au.ibm.com (202.81.31.214)
	by e23smtp06.au.ibm.com (202.81.31.212) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 8 Aug 2014 17:38:09 +1000
Received: from d23relay05.au.ibm.com (d23relay05.au.ibm.com [9.190.235.152])
	by d23dlp03.au.ibm.com (Postfix) with ESMTP id 520213578053
	for <dev@spark.apache.org>; Fri,  8 Aug 2014 17:38:09 +1000 (EST)
Received: from d23av01.au.ibm.com (d23av01.au.ibm.com [9.190.234.96])
	by d23relay05.au.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s787Ev4h49545278
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 17:14:58 +1000
Received: from d23av01.au.ibm.com (localhost [127.0.0.1])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s787c8ov003368
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 17:38:08 +1000
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s787c7Y1003328
	for <dev@spark.apache.org>; Fri, 8 Aug 2014 17:38:07 +1000
In-Reply-To: <CACBYxKL6rw9G6MYYmGKMgq4raMgdxpPbte-janTHZA9rj1-Fow@mail.gmail.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
	<CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>	<CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
	<OFE4B5E67A.527C58A9-ON48257D2E.001FC9BF-48257D2E.0020AB17@cn.ibm.com> <CACBYxKL6rw9G6MYYmGKMgq4raMgdxpPbte-janTHZA9rj1-Fow@mail.gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
        Patrick Wendell <pwendell@gmail.com>
MIME-Version: 1.0
Subject: Re: Fine-Grained Scheduler on Yarn
X-KeepSent: BFBFA484:90BE4C7B-48257D2E:00291498;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OFBFBFA484.90BE4C7B-ON48257D2E.00291498-48257D2E.0029EF09@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Fri, 8 Aug 2014 15:37:12 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6|November 21, 2013) at
 08/08/2014 15:37:18,
	Serialize complete at 08/08/2014 15:37:18
Content-Type: multipart/related; boundary="=_related 0029EEFA48257D2E_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14080807-7014-0000-0000-0000000CEB01
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 0029EEFA48257D2E_=
Content-Type: multipart/alternative; boundary="=_alternative 0029EF0048257D2E_="


--=_alternative 0029EF0048257D2E_=
Content-Type: text/plain; charset="US-ASCII"

Yes, I think we need both level resource control (container numbers and 
dynamically change container resources), which can make the resource 
utilization much more effective, especially when we have more types work 
load share the same infrastructure. 

Is there anyway I can observe the tasks backlog in schedulerbackend? 
Sounds like scheduler backend be triggered during new taskset submitted. I 
did not figured if there is a way to check the whole backlog tasks inside 
it. I am interesting to implement some policy in schedulerbackend and test 
to see how useful it is going to be.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Sandy Ryza <sandy.ryza@cloudera.com> 
2014/08/08 15:14

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" 
<dev@spark.apache.org>
Subject
Re: Fine-Grained Scheduler on Yarn






Hi Jun,

Spark currently doesn't have that feature, i.e. it aims for a fixed number
of executors per application regardless of resource usage, but it's
definitely worth considering.  We could start more executors when we have 
a
large backlog of tasks and shut some down when we're underutilized.

The fine-grained task scheduling is blocked on work from YARN that will
allow changing the CPU allocation of a YARN container dynamically.  The
relevant JIRA for this dependency is YARN-1197, though YARN-1488 might
serve this purpose as well if it comes first.

-Sandy


On Thu, Aug 7, 2014 at 10:56 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Thanks for echo on this. Possible to adjust resource based on container
> numbers? e.g to allocate more container when driver need more resources 
and
> return some resource by delete some container when parts of container
> already have enough cores/memory
>
> Best Regards
>
>
> *Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information]
> *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Patrick Wendell <pwendell@gmail.com <pwendell@gmail.com>>*
>
> 2014/08/08 13:10
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> "dev@spark.apache.org" <dev@spark.apache.org>
> Subject
> Re: Fine-Grained Scheduler on Yarn
>
>
>
>
> Hey sorry about that - what I said was the opposite of what is true.
>
> The current YARN mode is equivalent to "coarse grained" mesos. There is 
no
> fine-grained scheduling on YARN at the moment. I'm not sure YARN 
supports
> scheduling in units other than containers. Fine-grained scheduling 
requires
> scheduling at the granularity of individual cores.
>
>
> On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell <*pwendell@gmail.com*
> <pwendell@gmail.com>> wrote:
> The current YARN is equivalent to what is called "fine grained" mode in
> Mesos. The scheduling of tasks happens totally inside of the Spark 
driver.
>
>
> On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <*liujunf@cn.ibm.com*
> <liujunf@cn.ibm.com>> wrote:
> Any one know the answer?
> Best Regards
>
>
> * Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>   *Jun Feng Liu/China/IBM*
>
> 2014/08/07 15:37
>
>   To
> *dev@spark.apache.org* <dev@spark.apache.org>,
> cc
>   Subject
> Fine-Grained Scheduler on Yarn
>
>
>
>
>
> Hi, there
>
> Just aware right now Spark only support fine grained scheduler on Mesos
> with MesosSchedulerBackend. The Yarn schedule sounds like only works on
> coarse-grained model. Is there any plan to implement fine-grained 
scheduler
> for YARN? Or there is any technical issue block us to do that.
>
> Best Regards
>
>
> * Jun Feng Liu*
>
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  *Phone: *86-10-82452683
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>
>


--=_alternative 0029EF0048257D2E_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 0029EF0048257D2E_=--
--=_related 0029EEFA48257D2E_=--


From dev-return-8790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 07:50:23 2014
Return-Path: <dev-return-8790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0CC6A118F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 07:50:23 +0000 (UTC)
Received: (qmail 99935 invoked by uid 500); 8 Aug 2014 07:50:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99874 invoked by uid 500); 8 Aug 2014 07:50:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99486 invoked by uid 99); 8 Aug 2014 07:50:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:50:13 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 07:50:10 +0000
Received: by mail-qa0-f49.google.com with SMTP id dc16so5122080qab.8
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 00:49:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=t3Lv+qn48hkOB8J8YzlSG+t82g9fvA5/sQpYKiZSsYc=;
        b=BCql0X8WY9qYUw27Y5IlcfubLT52O+sGZCYET04VXvlHegHetajnFqDw/KxkyAwMmw
         b7GN85vIj0jCXvVHBG4z0a2oDBO4yp4Nt+sCIDOkpqxnblPW1VjCZzq2Evs4+lTgvRDI
         a5GiohuBPXTbc8gedRbrSO+o9+GiOh7b7zO6JX7/veNSgeq/eLRLs71HOQNRzqUqPxij
         1DPnEz8Ug57dTVHGCdo4ak2BUZ5V7YO+0p9XdRqB5ZfX2XN6CG6Omv49goeqCuapxsN+
         tQ8Q70Y97eIx6OM7Yyuk96xGqzeBtezHbX8uAXbwJ2VDNQ8nLffW/KKO2oXvCKiGuuyb
         YHxA==
X-Gm-Message-State: ALoCoQmAUg7ajZAczwzpCnULnT1wReZPexs0Ue7Atb9CP9ywkELVR/5zTT/MTH0BGvPugYsKYH3v
MIME-Version: 1.0
X-Received: by 10.140.84.138 with SMTP id l10mr20480141qgd.51.1407484184576;
 Fri, 08 Aug 2014 00:49:44 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Fri, 8 Aug 2014 00:49:44 -0700 (PDT)
In-Reply-To: <OFBFBFA484.90BE4C7B-ON48257D2E.00291498-48257D2E.0029EF09@cn.ibm.com>
References: <OFC79ECCF9.7307DF54-ON48257D2D.0029A9D9-48257D2D.0029FDEC@LocalDomain>
	<OF1B922427.882F1394-ON48257D2E.000F97EF-48257D2E.000FB521@cn.ibm.com>
	<CABPQxst0tuNROWiR3rjkcAJUqgCwNM5XsqCWN0O=RLRTU1n+SQ@mail.gmail.com>
	<CABPQxss=fs_E3JL-nay3N32_Mag6UULz+cw3uaeua5feEvPhpQ@mail.gmail.com>
	<OFE4B5E67A.527C58A9-ON48257D2E.001FC9BF-48257D2E.0020AB17@cn.ibm.com>
	<CACBYxKL6rw9G6MYYmGKMgq4raMgdxpPbte-janTHZA9rj1-Fow@mail.gmail.com>
	<OFBFBFA484.90BE4C7B-ON48257D2E.00291498-48257D2E.0029EF09@cn.ibm.com>
Date: Fri, 8 Aug 2014 00:49:44 -0700
Message-ID: <CACBYxKKTfKeY4He+y7-wJfwgOaZYoMafReO5RdfVM_iPvdHt2w@mail.gmail.com>
Subject: Re: Fine-Grained Scheduler on Yarn
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/related; boundary=001a11c115a0df12ee050019727f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c115a0df12ee050019727f
Content-Type: multipart/alternative; boundary=001a11c115a0df12eb050019727e

--001a11c115a0df12eb050019727e
Content-Type: text/plain; charset=UTF-8

I think that would be useful work.  I don't know the minute details of this
code, but in general TaskSchedulerImpl keeps track of pending tasks.  Tasks
are organized into TaskSets, each of which corresponds to a particular
stage.  Each TaskSet has a TaskSetManager, which directly tracks the
pending tasks for that stage.

-Sandy


On Fri, Aug 8, 2014 at 12:37 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Yes, I think we need both level resource control (container numbers and
> dynamically change container resources), which can make the resource
> utilization much more effective, especially when we have more types work
> load share the same infrastructure.
>
> Is there anyway I can observe the tasks backlog in schedulerbackend?
> Sounds like scheduler backend be triggered during new taskset submitted. I
> did not figured if there is a way to check the whole backlog tasks inside
> it. I am interesting to implement some policy in schedulerbackend and test
> to see how useful it is going to be.
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Sandy Ryza <sandy.ryza@cloudera.com <sandy.ryza@cloudera.com>>*
>
> 2014/08/08 15:14
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <
> dev@spark.apache.org>
> Subject
> Re: Fine-Grained Scheduler on Yarn
>
>
>
>
> Hi Jun,
>
> Spark currently doesn't have that feature, i.e. it aims for a fixed number
> of executors per application regardless of resource usage, but it's
> definitely worth considering.  We could start more executors when we have a
> large backlog of tasks and shut some down when we're underutilized.
>
> The fine-grained task scheduling is blocked on work from YARN that will
> allow changing the CPU allocation of a YARN container dynamically.  The
> relevant JIRA for this dependency is YARN-1197, though YARN-1488 might
> serve this purpose as well if it comes first.
>
> -Sandy
>
>
> On Thu, Aug 7, 2014 at 10:56 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
> > Thanks for echo on this. Possible to adjust resource based on container
> > numbers? e.g to allocate more container when driver need more resources
> and
> > return some resource by delete some container when parts of container
> > already have enough cores/memory
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
>
> >
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
>
> >  [image: 2D barcode - encoded with contact information]
> > *Phone: *86-10-82452683
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
> >  *Patrick Wendell <pwendell@gmail.com <pwendell@gmail.com>>*
>
> >
> > 2014/08/08 13:10
> >   To
> > Jun Feng Liu/China/IBM@IBMCN,
> > cc
> > "dev@spark.apache.org" <dev@spark.apache.org>
> > Subject
> > Re: Fine-Grained Scheduler on Yarn
> >
> >
> >
> >
> > Hey sorry about that - what I said was the opposite of what is true.
> >
> > The current YARN mode is equivalent to "coarse grained" mesos. There is
> no
> > fine-grained scheduling on YARN at the moment. I'm not sure YARN supports
> > scheduling in units other than containers. Fine-grained scheduling
> requires
> > scheduling at the granularity of individual cores.
> >
> >
> > On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell <*pwendell@gmail.com*
>
> > <pwendell@gmail.com>> wrote:
> > The current YARN is equivalent to what is called "fine grained" mode in
> > Mesos. The scheduling of tasks happens totally inside of the Spark
> driver.
> >
> >
> > On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu <*liujunf@cn.ibm.com*
>
> > <liujunf@cn.ibm.com>> wrote:
> > Any one know the answer?
> > Best Regards
> >
> >
> > * Jun Feng Liu*
>
> >
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  *Phone: *86-10-82452683
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
> >
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >   *Jun Feng Liu/China/IBM*
> >
> > 2014/08/07 15:37
> >
> >   To
> > *dev@spark.apache.org* <dev@spark.apache.org>,
>
> > cc
> >   Subject
> > Fine-Grained Scheduler on Yarn
> >
> >
> >
> >
> >
> > Hi, there
> >
> > Just aware right now Spark only support fine grained scheduler on Mesos
> > with MesosSchedulerBackend. The Yarn schedule sounds like only works on
> > coarse-grained model. Is there any plan to implement fine-grained
> scheduler
> > for YARN? Or there is any technical issue block us to do that.
> >
> > Best Regards
> >
> >
> > * Jun Feng Liu*
>
> >
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  *Phone: *86-10-82452683
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>
> >
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
> >
> >
>
>

--001a11c115a0df12eb050019727e
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">I think that would be useful work. =C2=A0I don&#39;t know =
the minute details of this code, but in general TaskSchedulerImpl keeps tra=
ck of pending tasks. =C2=A0Tasks are organized into TaskSets, each of which=
 corresponds to a particular stage. =C2=A0Each TaskSet has a TaskSetManager=
, which directly tracks the pending tasks for that stage.<div>
<br></div><div>-Sandy</div></div><div class=3D"gmail_extra"><br><br><div cl=
ass=3D"gmail_quote">On Fri, Aug 8, 2014 at 12:37 AM, Jun Feng Liu <span dir=
=3D"ltr">&lt;<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_blank">liujun=
f@cn.ibm.com</a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><font face=3D"sans-serif">Yes, I think we ne=
ed both level resource
control (container numbers and dynamically change container resources),
which can make the resource utilization much more effective, especially
when we have more types work load share the same infrastructure. </font>
<br>
<br><font face=3D"sans-serif">Is there anyway I can observe the tasks
backlog in schedulerbackend? Sounds like scheduler backend be triggered
during new taskset submitted. I did not figured if there is a way to check
the whole backlog tasks inside it. I am interesting to implement some polic=
y
in schedulerbackend and test to see how useful it is going to be.<br>
</font><font size=3D"1" face=3D"Arial"> </font>
<p><font size=3D"1" face=3D"Arial">Best Regards</font>
</p><p></p><div class=3D""><font size=3D"1" face=3D"Arial">=C2=A0</font>
<br><font size=3D"3" color=3D"#8f8f8f" face=3D"Arial"><b>Jun Feng Liu</b></=
font><font size=3D"1" face=3D"Arial"><br>
IBM China Systems &amp; Technology Laboratory in Beijing</font>
</div><p>
</p><table>
<tbody><tr>
<td colspan=3D"3">
<div align=3D"center">
<hr noshade></div>
</td></tr><tr>
<td rowspan=3D"2"><img src=3D"cid:_2_12BF645812BF60840029EEED48257D2E" alt=
=3D"2D barcode - encoded with contact information">
</td><td><font size=3D"1" color=3D"#4181c0" face=3D"=E5=AE=8B=E4=BD=93"><b>=
Phone: </b></font><font size=3D"1" color=3D"#5f5f5f" face=3D"=E5=AE=8B=E4=
=BD=93">86-10-82452683
</font><font size=3D"1" color=3D"#4181c0"><b><br>
E-mail:</b></font><font size=3D"1" color=3D"#5f5f5f"> </font><a href=3D"mai=
lto:liujunf@cn.ibm.com" target=3D"_blank"><font size=3D"1" color=3D"#5f5f5f=
" face=3D"=E5=AE=8B=E4=BD=93"><u>liujunf@cn.ibm.com</u></font></a>
</td><td rowspan=3D"2">
<div align=3D"right"><img src=3D"cid:_1_12BF6E0412BF6A300029EEED48257D2E" w=
idth=3D"32" height=3D"32" alt=3D"IBM"><font size=3D"1" color=3D"#5f5f5f"><b=
r>
</font><font size=3D"1" color=3D"#5f5f5f" face=3D"=E5=AE=8B=E4=BD=93"><br>
BLD 28,ZGC Software Park <br>
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 <br>
China </font></div>
</td></tr><tr>
<td><font size=3D"1" color=3D"#5f5f5f">=C2=A0</font></td></tr></tbody></tab=
le>
<br>
<p><font size=3D"3">=C2=A0</font>
<br>
<br>
<br>
</p><p></p><table width=3D"100%">
<tbody><tr valign=3D"top">
<td width=3D"40%"><font size=3D"1" face=3D"sans-serif"><b>Sandy Ryza &lt;<a=
 href=3D"mailto:sandy.ryza@cloudera.com" target=3D"_blank">sandy.ryza@cloud=
era.com</a>&gt;</b>
</font>
<p><font size=3D"1" face=3D"sans-serif">2014/08/08 15:14</font>
</p></td><td width=3D"59%">
<table width=3D"100%">
<tbody><tr valign=3D"top">
<td>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">To</font></div>
</td><td><font size=3D"1" face=3D"sans-serif">Jun Feng Liu/China/IBM@IBMCN,=
 </font>
</td></tr><tr valign=3D"top">
<td>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">cc</font></div>
</td><td><font size=3D"1" face=3D"sans-serif">Patrick Wendell &lt;<a href=
=3D"mailto:pwendell@gmail.com" target=3D"_blank">pwendell@gmail.com</a>&gt;=
,
&quot;<a href=3D"mailto:dev@spark.apache.org" target=3D"_blank">dev@spark.a=
pache.org</a>&quot; &lt;<a href=3D"mailto:dev@spark.apache.org" target=3D"_=
blank">dev@spark.apache.org</a>&gt;</font>
</td></tr><tr valign=3D"top">
<td>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">Subject</font></d=
iv>
</td><td><font size=3D"1" face=3D"sans-serif">Re: Fine-Grained Scheduler on=
 Yarn</font></td></tr></tbody></table>
<br>
<table>
<tbody><tr valign=3D"top">
<td>
</td><td></td></tr></tbody></table>
<br></td></tr></tbody></table>
<br>
<br>
<br><tt><font><div class=3D"">Hi Jun,<br>
<br>
Spark currently doesn&#39;t have that feature, i.e. it aims for a fixed num=
ber<br>
of executors per application regardless of resource usage, but it&#39;s<br>
definitely worth considering. =C2=A0We could start more executors when
we have a<br>
large backlog of tasks and shut some down when we&#39;re underutilized.<br>
<br>
The fine-grained task scheduling is blocked on work from YARN that will<br>
allow changing the CPU allocation of a YARN container dynamically. =C2=A0Th=
e<br>
relevant JIRA for this dependency is YARN-1197, though YARN-1488 might<br>
serve this purpose as well if it comes first.<br>
<br>
-Sandy<br>
<br>
<br>
On Thu, Aug 7, 2014 at 10:56 PM, Jun Feng Liu &lt;<a href=3D"mailto:liujunf=
@cn.ibm.com" target=3D"_blank">liujunf@cn.ibm.com</a>&gt;
wrote:<br>
<br>
&gt; Thanks for echo on this. Possible to adjust resource based on containe=
r<br>
&gt; numbers? e.g to allocate more container when driver need more resource=
s
and<br>
&gt; return some resource by delete some container when parts of container<=
br>
&gt; already have enough cores/memory<br>
&gt;<br>
&gt; Best Regards<br>
&gt;<br>
&gt;<br></div>
&gt; *Jun Feng Liu*<div class=3D""><br>
&gt;<br>
&gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt;<br></div>
&gt; =C2=A0 ------------------------------<div class=3D""><br>
&gt; =C2=A0[image: 2D barcode - encoded with contact information]<br></div>
&gt; *Phone: *86-10-82452683<br>
&gt; * E-mail:* *<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_blank">li=
ujunf@cn.ibm.com</a>* &lt;<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_=
blank">liujunf@cn.ibm.com</a>&gt;<div class=3D""><br>
&gt; [image: IBM]<br>
&gt;<br>
&gt; BLD 28,ZGC Software Park<br>
&gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; China<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br></div>
&gt; =C2=A0*Patrick Wendell &lt;<a href=3D"mailto:pwendell@gmail.com" targe=
t=3D"_blank">pwendell@gmail.com</a> &lt;<a href=3D"mailto:pwendell@gmail.co=
m" target=3D"_blank">pwendell@gmail.com</a>&gt;&gt;*<div class=3D""><br>
&gt;<br>
&gt; 2014/08/08 13:10<br>
&gt; =C2=A0 To<br>
&gt; Jun Feng Liu/China/IBM@IBMCN,<br>
&gt; cc<br>
&gt; &quot;<a href=3D"mailto:dev@spark.apache.org" target=3D"_blank">dev@sp=
ark.apache.org</a>&quot; &lt;<a href=3D"mailto:dev@spark.apache.org" target=
=3D"_blank">dev@spark.apache.org</a>&gt;<br>
&gt; Subject<br>
&gt; Re: Fine-Grained Scheduler on Yarn<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Hey sorry about that - what I said was the opposite of what is true.<b=
r>
&gt;<br>
&gt; The current YARN mode is equivalent to &quot;coarse grained&quot;
mesos. There is no<br>
&gt; fine-grained scheduling on YARN at the moment. I&#39;m not sure YARN s=
upports<br>
&gt; scheduling in units other than containers. Fine-grained scheduling
requires<br>
&gt; scheduling at the granularity of individual cores.<br>
&gt;<br>
&gt;<br></div>
&gt; On Thu, Aug 7, 2014 at 9:43 PM, Patrick Wendell &lt;*<a href=3D"mailto=
:pwendell@gmail.com" target=3D"_blank">pwendell@gmail.com</a>*<div class=3D=
""><br>
&gt; &lt;<a href=3D"mailto:pwendell@gmail.com" target=3D"_blank">pwendell@g=
mail.com</a>&gt;&gt; wrote:<br>
&gt; The current YARN is equivalent to what is called &quot;fine grained&qu=
ot;
mode in<br>
&gt; Mesos. The scheduling of tasks happens totally inside of the Spark
driver.<br>
&gt;<br>
&gt;<br></div>
&gt; On Thu, Aug 7, 2014 at 7:50 PM, Jun Feng Liu &lt;*<a href=3D"mailto:li=
ujunf@cn.ibm.com" target=3D"_blank">liujunf@cn.ibm.com</a>*<div class=3D"">=
<br>
&gt; &lt;<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_blank">liujunf@cn=
.ibm.com</a>&gt;&gt; wrote:<br>
&gt; Any one know the answer?<br>
&gt; Best Regards<br>
&gt;<br>
&gt;<br></div>
&gt; * Jun Feng Liu*<div class=3D""><br>
&gt;<br>
&gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt;<br></div>
&gt; =C2=A0 ------------------------------<br>
&gt; =C2=A0*Phone: *86-10-82452683<br>
&gt; * E-mail:* *<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_blank">li=
ujunf@cn.ibm.com</a>* &lt;<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_=
blank">liujunf@cn.ibm.com</a>&gt;<div class=3D""><br>
&gt;<br>
&gt;<br>
&gt; BLD 28,ZGC Software Park<br>
&gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; China<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br></div><div class=3D"">
&gt; =C2=A0 *Jun Feng Liu/China/IBM*<br>
&gt;<br>
&gt; 2014/08/07 15:37<br>
&gt;<br>
&gt; =C2=A0 To<br></div>
&gt; *<a href=3D"mailto:dev@spark.apache.org" target=3D"_blank">dev@spark.a=
pache.org</a>* &lt;<a href=3D"mailto:dev@spark.apache.org" target=3D"_blank=
">dev@spark.apache.org</a>&gt;,<div class=3D""><br>
&gt; cc<br>
&gt; =C2=A0 Subject<br>
&gt; Fine-Grained Scheduler on Yarn<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Hi, there<br>
&gt;<br>
&gt; Just aware right now Spark only support fine grained scheduler on
Mesos<br>
&gt; with MesosSchedulerBackend. The Yarn schedule sounds like only works
on<br>
&gt; coarse-grained model. Is there any plan to implement fine-grained
scheduler<br>
&gt; for YARN? Or there is any technical issue block us to do that.<br>
&gt;<br>
&gt; Best Regards<br>
&gt;<br>
&gt;<br></div>
&gt; * Jun Feng Liu*<div class=3D""><br>
&gt;<br>
&gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt;<br></div>
&gt; =C2=A0 ------------------------------<br>
&gt; =C2=A0*Phone: *86-10-82452683<br>
&gt; * E-mail:* *<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_blank">li=
ujunf@cn.ibm.com</a>* &lt;<a href=3D"mailto:liujunf@cn.ibm.com" target=3D"_=
blank">liujunf@cn.ibm.com</a>&gt;<div class=3D""><br>
&gt;<br>
&gt;<br>
&gt; BLD 28,ZGC Software Park<br>
&gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; China<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
</div></font></tt>
<br>
<p></p><p></p><p></p><p></p></blockquote></div><br></div>

--001a11c115a0df12eb050019727e--
--001a11c115a0df12ee050019727f--

From dev-return-8791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 08:40:34 2014
Return-Path: <dev-return-8791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7154211A40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 08:40:34 +0000 (UTC)
Received: (qmail 249 invoked by uid 500); 8 Aug 2014 08:40:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 185 invoked by uid 500); 8 Aug 2014 08:40:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 173 invoked by uid 99); 8 Aug 2014 08:40:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 08:40:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.223.182 as permitted sender)
Received: from [209.85.223.182] (HELO mail-ie0-f182.google.com) (209.85.223.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 08:40:29 +0000
Received: by mail-ie0-f182.google.com with SMTP id y20so6169551ier.13
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 01:40:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=3UwCaS3O3xb25x7nFsc2yb7KB5Gy0LuUeeUu5vJsKbc=;
        b=SXktLPKvkr7tr/ZkzXK86ssmzyJfxHo0yqR3YmICrkGC99KtogzJogzmOpSGDoR9TR
         f6Bq+t1tc2CuPcgEA4glyGSxd+X9sDPyIvi3VbxJMK3Vv7xVbQMzk8UGmWtDuwG0JKwR
         8weOfnadJqkQZM+DKW9eIDGSDG6glbqay0FzCviGSZi0exME7Z8GhdEFk0R20wulU8tJ
         oqhfWmpagW0J7LHT7ttMFeEW8QUds3iU/ZF8NY8IkxMVUKcfhXdA2Xs7+7mvrBwbngln
         tW1ir16HoAEe7/0XcLVBdq01Mx6cCTVCmpnCiKqtj4XHCQlr8RELpBi34XlBqPG9aV1j
         uxmA==
MIME-Version: 1.0
X-Received: by 10.50.253.195 with SMTP id ac3mr2295306igd.18.1407487209353;
 Fri, 08 Aug 2014 01:40:09 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Fri, 8 Aug 2014 01:40:09 -0700 (PDT)
In-Reply-To: <CAGh_TuMkcvw=Rb94jLYeSh-pcGDZ-cEi2FBRMbwPbagfTObQ4A@mail.gmail.com>
References: <etPan.53e458bb.515f007c.119a7@mbp-3.local>
	<CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
	<CAGh_TuMkcvw=Rb94jLYeSh-pcGDZ-cEi2FBRMbwPbagfTObQ4A@mail.gmail.com>
Date: Fri, 8 Aug 2014 01:40:09 -0700
Message-ID: <CAJgQjQ-=-Sty97TFNisjue38bYo8aVVcfxC3Jrf_NNvyHsUD4g@mail.gmail.com>
Subject: Re: Welcoming two new committers
From: Xiangrui Meng <mengxr@gmail.com>
To: Christopher Nguyen <ctn@adatao.com>
Cc: Joseph Gonzalez <jegonzal@eecs.berkeley.edu>, Matei Zaharia <matei@databricks.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, andrew@databricks.com
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Congrats, Joey & Andrew!!

-Xiangrui

On Fri, Aug 8, 2014 at 12:14 AM, Christopher Nguyen <ctn@adatao.com> wrote:
> +1 Joey & Andrew :)
>
> --
> Christopher T. Nguyen
> Co-founder & CEO, Adatao <http://adatao.com> [ah-'DAY-tao]
> linkedin.com/in/ctnguyen
>
>
>
> On Thu, Aug 7, 2014 at 10:39 PM, Joseph Gonzalez <jegonzal@eecs.berkeley.edu
>> wrote:
>
>> Hi Everyone,
>>
>> Thank you for inviting me to be a committer.  I look forward to working
>> with everyone to ensure the continued success of the Spark project.
>>
>> Thanks!
>> Joey
>>
>>
>>
>>
>> On Thu, Aug 7, 2014 at 9:57 PM, Matei Zaharia <matei@databricks.com>
>> wrote:
>>
>> > Hi everyone,
>> >
>> > The PMC recently voted to add two new committers and PMC members: Joey
>> > Gonzalez and Andrew Or. Both have been huge contributors in the past year
>> > -- Joey on much of GraphX as well as quite a bit of the initial work in
>> > MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!
>> >
>> > Matei
>> >
>> >
>> >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 10:24:44 2014
Return-Path: <dev-return-8792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C40A11D5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 10:24:44 +0000 (UTC)
Received: (qmail 50575 invoked by uid 500); 8 Aug 2014 10:24:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50509 invoked by uid 500); 8 Aug 2014 10:24:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50495 invoked by uid 99); 8 Aug 2014 10:24:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 10:24:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of scrapcodes@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 10:24:41 +0000
Received: by mail-vc0-f180.google.com with SMTP id ij19so7862897vcb.25
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 03:24:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZMApSFkXMuALugAyfbhQLSx30U6HjYdIO3UAwshWiF8=;
        b=D8siZ+xHKaZzaj+QrpTVjawITHfu8nLCtNpCIF4JlnAyWuTsa8oQaaBCmlCR8V3Lq5
         0mHqvYT0JmuNxHZtZBl0RJRlLEnQd7kMI6YKQYSgBBlgzFDB2jxAdwc2p5D9vC5fJIXr
         kDtRtQ86d4akVZbSltHHuQA9LSDP1HgpRfj5L+66B7dibfpmTzH8CVgJfb8jEN0ne6i0
         DtoSUMNSnP9i1S4qrw4uTBEOg2xDiF0csD1ZwsM4L+raPRIcndjbc8Dq9q2c9v1Q4HV0
         +bEs8jsM14A/55GKj7rFDVusnBa7ZheacwcDc3rN4hRgIs68UjJnjzAmrzOzN9rJ7hEX
         Rdeg==
X-Received: by 10.220.2.136 with SMTP id 8mr21099841vcj.17.1407493456753; Fri,
 08 Aug 2014 03:24:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.231.10 with HTTP; Fri, 8 Aug 2014 03:23:55 -0700 (PDT)
In-Reply-To: <CAJgQjQ-=-Sty97TFNisjue38bYo8aVVcfxC3Jrf_NNvyHsUD4g@mail.gmail.com>
References: <etPan.53e458bb.515f007c.119a7@mbp-3.local> <CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
 <CAGh_TuMkcvw=Rb94jLYeSh-pcGDZ-cEi2FBRMbwPbagfTObQ4A@mail.gmail.com> <CAJgQjQ-=-Sty97TFNisjue38bYo8aVVcfxC3Jrf_NNvyHsUD4g@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Fri, 8 Aug 2014 15:53:55 +0530
Message-ID: <CAOYDGoArRHhgpx3wZzhzZU--dB5zBnwvbpobQopn4vDW0nTreQ@mail.gmail.com>
Subject: Re: Welcoming two new committers
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Christopher Nguyen <ctn@adatao.com>, Joseph Gonzalez <jegonzal@eecs.berkeley.edu>, 
	Matei Zaharia <matei@databricks.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, andrew@databricks.com
Content-Type: multipart/alternative; boundary=001a11c3bee283790105001b9bda
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3bee283790105001b9bda
Content-Type: text/plain; charset=UTF-8

Congratulations Andrew and Joey.

Prashant Sharma




On Fri, Aug 8, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Congrats, Joey & Andrew!!
>
> -Xiangrui
>
> On Fri, Aug 8, 2014 at 12:14 AM, Christopher Nguyen <ctn@adatao.com>
> wrote:
> > +1 Joey & Andrew :)
> >
> > --
> > Christopher T. Nguyen
> > Co-founder & CEO, Adatao <http://adatao.com> [ah-'DAY-tao]
> > linkedin.com/in/ctnguyen
> >
> >
> >
> > On Thu, Aug 7, 2014 at 10:39 PM, Joseph Gonzalez <
> jegonzal@eecs.berkeley.edu
> >> wrote:
> >
> >> Hi Everyone,
> >>
> >> Thank you for inviting me to be a committer.  I look forward to working
> >> with everyone to ensure the continued success of the Spark project.
> >>
> >> Thanks!
> >> Joey
> >>
> >>
> >>
> >>
> >> On Thu, Aug 7, 2014 at 9:57 PM, Matei Zaharia <matei@databricks.com>
> >> wrote:
> >>
> >> > Hi everyone,
> >> >
> >> > The PMC recently voted to add two new committers and PMC members: Joey
> >> > Gonzalez and Andrew Or. Both have been huge contributors in the past
> year
> >> > -- Joey on much of GraphX as well as quite a bit of the initial work
> in
> >> > MLlib, and Andrew on Spark Core. Join me in welcoming them as
> committers!
> >> >
> >> > Matei
> >> >
> >> >
> >> >
> >> >
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c3bee283790105001b9bda--

From dev-return-8793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 16:03:09 2014
Return-Path: <dev-return-8793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F1F2410562
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 16:03:08 +0000 (UTC)
Received: (qmail 80365 invoked by uid 500); 8 Aug 2014 16:03:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80292 invoked by uid 500); 8 Aug 2014 16:03:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80274 invoked by uid 99); 8 Aug 2014 16:03:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:03:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:02:58 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so1275858wib.2
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 09:02:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=pOf5vKMomzIJf7IuhAf4OBO6CjZAuaU//Wpl/+KauEA=;
        b=KSnvJqPDj2Uyrt6GutWkp0cHXvmAJH/bJIOYMTyQMOagKz2FIwic+pHHuttvMSwH0O
         ISwsq+p4s6xJqh9eYh0/PJcNbUap4K6Aw5vAOz3qRoTnoufxEVgJ1HvjBGuUEiLnNJzE
         NStsVOBs1uVJozKJdeaefhwSyv6UjvbA64MTs9bZFnFKRTwdk4j7hF7UpX+wjYASFfqP
         jD2mOujBfdupgY9qn1hj39wVVQKh9ohPLwaVIfVWyFQumURz70IGwSCzZUzIjUH9kj8V
         DL2W5Szo9wrjeBaqU5PqMcuKTbFjEGRiZb3ZUjATPVrPPXMuZZfW03oyAYSbOO/MDz7R
         wElA==
X-Received: by 10.194.57.132 with SMTP id i4mr32312236wjq.6.1407513757869;
 Fri, 08 Aug 2014 09:02:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 8 Aug 2014 09:01:57 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 8 Aug 2014 12:01:57 -0400
Message-ID: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
Subject: Unit tests in < 5 minutes
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba9782e8def000500205543
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9782e8def000500205543
Content-Type: text/plain; charset=UTF-8

Howdy,

Do we think it's both feasible and worthwhile to invest in getting our unit
tests to finish in under 5 minutes (or something similarly brief) when run
by Jenkins?

Unit tests currently seem to take anywhere from 30 min to 2 hours. As
people add more tests, I imagine this time will only grow. I think it would
be better for both contributors and reviewers if they didn't have to wait
so long for test results; PR reviews would be shorter, if nothing else.

I don't know how how this is normally done, but maybe it wouldn't be too
much work to get a test cycle to feel lighter.

Most unit tests are independent and can be run concurrently, right? Would
it make sense to build a given patch on many servers at once and send
disjoint sets of unit tests to each?

I'd be interested in working on something like that if possible (and
sensible).

Nick

--047d7ba9782e8def000500205543--

From dev-return-8794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 16:15:30 2014
Return-Path: <dev-return-8794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF8DA105A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 16:15:30 +0000 (UTC)
Received: (qmail 98416 invoked by uid 500); 8 Aug 2014 16:15:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98376 invoked by uid 500); 8 Aug 2014 16:15:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98365 invoked by uid 99); 8 Aug 2014 16:15:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:15:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:15:01 +0000
Received: by mail-ig0-f182.google.com with SMTP id c1so1270520igq.15
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 09:15:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=SVJpx5446RIJwyc/7gdTbMbWArY6OWISqfAvZXkvC0g=;
        b=kl7U5+cwFjLH+3s1SRlomc0IfyWtbwh240a5qzgICVr5M+sIWkTF297s3+91/V40qT
         CE09zTVsaxVWXUwlSiitPLoXqcWmJITZwalmL01ESfx3SkwuIT8TUJo1CsVy4DkQjyLe
         0qdxsMkdCq6vMvoe8zPwAt0dIUxblKF9g6ggVT6enwY6DKNyiqcf8V+UgZi6WcPUN5wA
         i7/ETf0nOunLH8FFeLJb9eZhXtukDuqzb+t4IWZACjuNR39BwOVwtWGk2b0WHNZ4ocOv
         jt2H4pfL+Em7zkLAinSXppAOE+S4F6x1APwcH+XN9lzasmXYsnCssHENxvVVL0p7ZF8/
         GIfA==
X-Gm-Message-State: ALoCoQm0KWICySnq0t5SvXrsR6z/4k8KiIQLdXFvW63iqUqAJCli6OWPqll53stuNDBNqYrVjnPv
X-Received: by 10.42.148.73 with SMTP id q9mr4472444icv.88.1407514500132; Fri,
 08 Aug 2014 09:15:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Fri, 8 Aug 2014 09:14:40 -0700 (PDT)
In-Reply-To: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 8 Aug 2014 17:14:40 +0100
Message-ID: <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

A common approach is to separate unit tests from integration tests.
Maven has support for this distinction. I'm not sure it helps a lot
though, since it only helps you to not run integration tests all the
time. But lots of Spark tests are integration-test-like and are
important to run to know a change works.

I haven't heard of a plugin to run different test suites remotely on
many machines, but I would not be surprised if it exists.

The Jenkins servers aren't CPU-bound as far as I can tell. It's that
the tests spend a lot of time waiting for bits to start up or
complete. That implies the existing tests could be sped up by just
running in parallel locally. I recall someone recently proposed this?

And I think the problem with that is simply that some of the tests
collide with each other, by opening up the same port at the same time
for example. I know that kind of problem is being attacked even right
now. But if all the tests were made parallel friendly, I imagine
parallelism could be enabled and speed up builds greatly without any
remote machines.


On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Howdy,
>
> Do we think it's both feasible and worthwhile to invest in getting our unit
> tests to finish in under 5 minutes (or something similarly brief) when run
> by Jenkins?
>
> Unit tests currently seem to take anywhere from 30 min to 2 hours. As
> people add more tests, I imagine this time will only grow. I think it would
> be better for both contributors and reviewers if they didn't have to wait
> so long for test results; PR reviews would be shorter, if nothing else.
>
> I don't know how how this is normally done, but maybe it wouldn't be too
> much work to get a test cycle to feel lighter.
>
> Most unit tests are independent and can be run concurrently, right? Would
> it make sense to build a given patch on many servers at once and send
> disjoint sets of unit tests to each?
>
> I'd be interested in working on something like that if possible (and
> sensible).
>
> Nick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 16:48:26 2014
Return-Path: <dev-return-8795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8764E106A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 16:48:26 +0000 (UTC)
Received: (qmail 59595 invoked by uid 500); 8 Aug 2014 16:48:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59541 invoked by uid 500); 8 Aug 2014 16:48:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59529 invoked by uid 99); 8 Aug 2014 16:48:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:48:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.171 as permitted sender)
Received: from [209.85.160.171] (HELO mail-yk0-f171.google.com) (209.85.160.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 16:47:59 +0000
Received: by mail-yk0-f171.google.com with SMTP id 19so4021172ykq.16
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 09:47:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+tE/H0n0aE+HhScPgYC3un0wvitAYBC3C8ymoXB+H9g=;
        b=iVh3afIo4IEm1t/WRod2f9LOhNAEK2bOyMbfgp4NAR22bD0+QUIT6jO9mx63j0XVCj
         FXpWw51kIRa0g/C3aXjslpLDfZUtFm73HO1L1axqEEPdEH2MAktOcgT4CtHY/fN8VZFb
         QiZPfSAfNhciDbfpA0QoYlpr0QVl/YSQ12AtcS5p7AVucaRAnUk7oCAcJUnWmsCPleQz
         PyWG6EBRFIYhKgTibaHwP5gDmgi2w6EyplqouDoIwwZEzaj1m7JcwBXi3K2dX6HepyB5
         bz+1Fhsb8DwlaRjO3L/q/9G9IK1sAxsymP/aGJWqZcIG64PIU2W47KdxCC29twY8eZ//
         6y5w==
MIME-Version: 1.0
X-Received: by 10.236.228.161 with SMTP id f31mr16306433yhq.44.1407516478375;
 Fri, 08 Aug 2014 09:47:58 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Fri, 8 Aug 2014 09:47:58 -0700 (PDT)
In-Reply-To: <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
Date: Fri, 8 Aug 2014 09:47:58 -0700
Message-ID: <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Ted Yu <yuzhihong@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11333526b58e17050020f7f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11333526b58e17050020f7f8
Content-Type: text/plain; charset=UTF-8

How about using parallel execution feature of maven-surefire-plugin
(assuming all the tests were made parallel friendly) ?

http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html

Cheers


On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:

> A common approach is to separate unit tests from integration tests.
> Maven has support for this distinction. I'm not sure it helps a lot
> though, since it only helps you to not run integration tests all the
> time. But lots of Spark tests are integration-test-like and are
> important to run to know a change works.
>
> I haven't heard of a plugin to run different test suites remotely on
> many machines, but I would not be surprised if it exists.
>
> The Jenkins servers aren't CPU-bound as far as I can tell. It's that
> the tests spend a lot of time waiting for bits to start up or
> complete. That implies the existing tests could be sped up by just
> running in parallel locally. I recall someone recently proposed this?
>
> And I think the problem with that is simply that some of the tests
> collide with each other, by opening up the same port at the same time
> for example. I know that kind of problem is being attacked even right
> now. But if all the tests were made parallel friendly, I imagine
> parallelism could be enabled and speed up builds greatly without any
> remote machines.
>
>
> On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > Howdy,
> >
> > Do we think it's both feasible and worthwhile to invest in getting our
> unit
> > tests to finish in under 5 minutes (or something similarly brief) when
> run
> > by Jenkins?
> >
> > Unit tests currently seem to take anywhere from 30 min to 2 hours. As
> > people add more tests, I imagine this time will only grow. I think it
> would
> > be better for both contributors and reviewers if they didn't have to wait
> > so long for test results; PR reviews would be shorter, if nothing else.
> >
> > I don't know how how this is normally done, but maybe it wouldn't be too
> > much work to get a test cycle to feel lighter.
> >
> > Most unit tests are independent and can be run concurrently, right? Would
> > it make sense to build a given patch on many servers at once and send
> > disjoint sets of unit tests to each?
> >
> > I'd be interested in working on something like that if possible (and
> > sensible).
> >
> > Nick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11333526b58e17050020f7f8--

From dev-return-8796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:11:32 2014
Return-Path: <dev-return-8796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C157B10766
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:11:32 +0000 (UTC)
Received: (qmail 12441 invoked by uid 500); 8 Aug 2014 17:11:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12381 invoked by uid 500); 8 Aug 2014 17:11:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12367 invoked by uid 99); 8 Aug 2014 17:11:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:11:31 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:11:27 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so5813903wgh.35
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:11:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/3JhlVCsh/lO6uL/kc99LrcEKqV4Dlc5O+CRnJElZ0I=;
        b=gk9pVBLf0i88090DKJ60vN3SzXUWfeBHBt339TkzrzIx4Z4tUFmnfLNzELlRRLW00W
         D2XHtlaHj6tT2TeT+9HMmIh7CSrXd4NQaMxEIUwb8U1c/GuYDE7qYWAO1PvR6LLltd0W
         H9na3AHkIBLvlyTyqgxkMiSO6sq7AB3sXneJDFwJrEXojMmyvCMm9vX37I5hQ07tbWnO
         4Exb/HAlh96fNx3gJnXgteLxtDiieNIjM1TbzVDkPkXvFqWQYqUqQDDD8fG26TCW+YG/
         Uhln2KPToVQm9Ez12bIKWzVrv134KOIYCQ3H6WQ0WQ7P9PBjuqPTGYQJl6aI65r7KpxJ
         9VbA==
X-Gm-Message-State: ALoCoQnlVptzi0hWhuwTZX41+PbOV4WnrkVZo9VJXu28sc83AgrWj2i5x+n19u42rkSIvvMrH0b0
X-Received: by 10.194.206.67 with SMTP id lm3mr34019751wjc.70.1407517865246;
 Fri, 08 Aug 2014 10:11:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:10:45 -0700 (PDT)
In-Reply-To: <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com> <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:10:45 -0700
Message-ID: <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Ted Yu <yuzhihong@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bacc15e5f99680500214a92
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacc15e5f99680500214a92
Content-Type: text/plain; charset=UTF-8

ScalaTest actually has support for parallelization built-in. We can use
that.

The main challenge is to make sure all the test suites can work in parallel
when running along side each other.


On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:

> How about using parallel execution feature of maven-surefire-plugin
> (assuming all the tests were made parallel friendly) ?
>
>
> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html
>
> Cheers
>
>
> On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > A common approach is to separate unit tests from integration tests.
> > Maven has support for this distinction. I'm not sure it helps a lot
> > though, since it only helps you to not run integration tests all the
> > time. But lots of Spark tests are integration-test-like and are
> > important to run to know a change works.
> >
> > I haven't heard of a plugin to run different test suites remotely on
> > many machines, but I would not be surprised if it exists.
> >
> > The Jenkins servers aren't CPU-bound as far as I can tell. It's that
> > the tests spend a lot of time waiting for bits to start up or
> > complete. That implies the existing tests could be sped up by just
> > running in parallel locally. I recall someone recently proposed this?
> >
> > And I think the problem with that is simply that some of the tests
> > collide with each other, by opening up the same port at the same time
> > for example. I know that kind of problem is being attacked even right
> > now. But if all the tests were made parallel friendly, I imagine
> > parallelism could be enabled and speed up builds greatly without any
> > remote machines.
> >
> >
> > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
> > <nicholas.chammas@gmail.com> wrote:
> > > Howdy,
> > >
> > > Do we think it's both feasible and worthwhile to invest in getting our
> > unit
> > > tests to finish in under 5 minutes (or something similarly brief) when
> > run
> > > by Jenkins?
> > >
> > > Unit tests currently seem to take anywhere from 30 min to 2 hours. As
> > > people add more tests, I imagine this time will only grow. I think it
> > would
> > > be better for both contributors and reviewers if they didn't have to
> wait
> > > so long for test results; PR reviews would be shorter, if nothing else.
> > >
> > > I don't know how how this is normally done, but maybe it wouldn't be
> too
> > > much work to get a test cycle to feel lighter.
> > >
> > > Most unit tests are independent and can be run concurrently, right?
> Would
> > > it make sense to build a given patch on many servers at once and send
> > > disjoint sets of unit tests to each?
> > >
> > > I'd be interested in working on something like that if possible (and
> > > sensible).
> > >
> > > Nick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7bacc15e5f99680500214a92--

From dev-return-8797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:12:41 2014
Return-Path: <dev-return-8797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 983791077F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:12:41 +0000 (UTC)
Received: (qmail 18406 invoked by uid 500); 8 Aug 2014 17:12:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18354 invoked by uid 500); 8 Aug 2014 17:12:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18336 invoked by uid 99); 8 Aug 2014 17:12:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:12:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.212.181 as permitted sender)
Received: from [98.139.212.181] (HELO nm22.bullet.mail.bf1.yahoo.com) (98.139.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:12:31 +0000
Received: from [66.196.81.174] by nm22.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:12:10 -0000
Received: from [98.139.212.247] by tm20.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:12:10 -0000
Received: from [127.0.0.1] by omp1056.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:12:10 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 163969.30446.bm@omp1056.mail.bf1.yahoo.com
Received: (qmail 67088 invoked by uid 60001); 8 Aug 2014 17:12:10 -0000
X-YMail-OSG: SqNs2z4VM1nJGuZmfKXK.R0MefSXtTKPfVJ74_NY2RuO5HG
 LQsbCdqPd4UQUarXTkFQZv55KttBEFdy7C6hStLXZ046WehuuDlkbUP4E_Vc
 Tr8Vn7eIRDg_5rtSZQFHvJyQIbeAT_FDlvJBC3RIB25WXylw6QUb9mPy0Dfb
 MRK3J3SCfEdkfDNTAAi7YVbNuuOHJwfl.Wwm0EnWo_.EflVKdq0OPKDTHzfk
 qOlv9ss8YVbEQEsHot1yFCnyhniLrwHLh_kY27drj_5OYR50ucvW15rAYqyP
 augisM68kVvl42floiNG9qCc5A6w8TnR4SF_5MsBLt.P54T7afE239jBgDay
 UCgJjEoDjtbt_pqJkt2qpx.wgjT2y2dcHCp1fVNXyBHJM_uFvPylKo_c6qjP
 KzGV3JnKDYg9K4AkKLUWUijWynbw0oA8v8yf3QP4jS4OYM6B36c7nD8DaPyN
 oCfZUEmjmOdwLCjDUbcmpLfsjdxwL.JaPd2tN3EQpQUzfL2I4tnGdKCit377
 1xJMlC_eL7MuiLUxRt23q4yMXuRD8D3nt5oLGIVN9MLSXlnQkFc2iUyE-
Received: from [12.130.144.124] by web162405.mail.bf1.yahoo.com via HTTP; Fri, 08 Aug 2014 10:12:09 PDT
X-Rocket-MIMEInfo: 002.001,SGksCkkgaGF2ZSBhIHJ1bm5pbmcgc3BhcmsgYXBwIGFnYWluc3QgdGhlIHJlbGVhc2VkIHZlcnNpb24gb2YgMS4wLjEuIEkgcmVjZW50bHkgZGVjaWRlZCB0byB0cnkgYW5kIHVwZ3JhZGUgdG8gdGhlIHRydW5rIHZlcnNpb24uIEludGVyZXN0aW5nbHkgZW5vdWdoLCBhZnRlciBidWlsZGluZyB0aGUgMS4xLjAtU05BUFNIT1QgYXNzZW1ibHksIHJlcGxhY2luZyBpdCBhcyBteSBhc3NlbWJseSBpbiBteSBhcHAgY2F1c2VkIGVycm9ycy4gSW4gcGFydGljdWxhciwgaXQgc2VlbXMgS3J5byBzZXJpYWxpemF0aW8BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
Message-ID: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
Date: Fri, 8 Aug 2014 10:12:09 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: 1.1.0-SNAPSHOT possible regression
To: Dev <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="473274965-46544895-1407517929=:40821"
X-Virus-Checked: Checked by ClamAV on apache.org

--473274965-46544895-1407517929=:40821
Content-Type: text/plain; charset=us-ascii

Hi,
I have a running spark app against the released version of 1.0.1. I recently decided to try and upgrade to the trunk version. Interestingly enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my assembly in my app caused errors. In particular, it seems Kryo serialization isn't taking. Replacing it with 1.0.1 automatically gets it working again.

Any thoughts? Is this a known issue?

Thanks,
Ron

at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137) at scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124) at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at
 java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at org.apache.spark.util.Utils$.serialize(Utils.scala:64) at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232) at org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85) at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66) at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36) at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29) at
 org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
--473274965-46544895-1407517929=:40821--

From dev-return-8798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:13:07 2014
Return-Path: <dev-return-8798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2EDA10785
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:13:07 +0000 (UTC)
Received: (qmail 19682 invoked by uid 500); 8 Aug 2014 17:13:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19624 invoked by uid 500); 8 Aug 2014 17:13:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19571 invoked by uid 99); 8 Aug 2014 17:13:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:13:06 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:13:02 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so1345164wiv.13
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:12:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=4SOYuqdLlceidpg9OyrIO7n11vye/WYEjhZjaHTrbi4=;
        b=dhcNYpZcI2gp7ebqKMpQ1HizEIpf7wFgcJ0wSaTJGePhKeLdjxdKxHDiXLzZ3j8jqJ
         MgPanTgguRK99D9nM/IGj/IZMP0XolBc7CqortBTkWz680JuBsefeJIwCyYm1Y6iJgZu
         Ac0GlqqDVkcJdkzT+76uKOgWYraSsbehjeHTVgse9Q3FG7vpFqSS4AuFnnS+VYHkRWzg
         PS4xlbPsi0UUwOB+Sop5x3CKwohFEQey/otUaXodt9Q3v8UUGQfW9uCCtuAjAhvxIr+c
         MPJT1guNnp5bO8doX2jrSu/ae64SYCiTyvUIvw7s0ZgvZvajNwpw1zs3OoEmZsohMRan
         FJGA==
X-Gm-Message-State: ALoCoQmfrInqCDbU2srMDte17suVCN81kOfZmV4xoIqR+9MuX38MrI9RGZfQflVeZOBqZtLfiDC4
X-Received: by 10.194.192.201 with SMTP id hi9mr34061545wjc.28.1407517961229;
 Fri, 08 Aug 2014 10:12:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:12:21 -0700 (PDT)
In-Reply-To: <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com> <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:12:21 -0700
Message-ID: <CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Ted Yu <yuzhihong@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b8739a8181f41050021504b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b8739a8181f41050021504b
Content-Type: text/plain; charset=UTF-8

Nick,

Would you like to file a ticket to track this?

I think the first baby step is to log the amount of time each test cases
take. This is supposed to happen already (see the flag), but somehow the
time are not showing. If you have some time to figure that out, that'd be
great.

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L350




On Fri, Aug 8, 2014 at 10:10 AM, Reynold Xin <rxin@databricks.com> wrote:

> ScalaTest actually has support for parallelization built-in. We can use
> that.
>
> The main challenge is to make sure all the test suites can work in
> parallel when running along side each other.
>
>
> On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> How about using parallel execution feature of maven-surefire-plugin
>> (assuming all the tests were made parallel friendly) ?
>>
>>
>> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html
>>
>> Cheers
>>
>>
>> On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> > A common approach is to separate unit tests from integration tests.
>> > Maven has support for this distinction. I'm not sure it helps a lot
>> > though, since it only helps you to not run integration tests all the
>> > time. But lots of Spark tests are integration-test-like and are
>> > important to run to know a change works.
>> >
>> > I haven't heard of a plugin to run different test suites remotely on
>> > many machines, but I would not be surprised if it exists.
>> >
>> > The Jenkins servers aren't CPU-bound as far as I can tell. It's that
>> > the tests spend a lot of time waiting for bits to start up or
>> > complete. That implies the existing tests could be sped up by just
>> > running in parallel locally. I recall someone recently proposed this?
>> >
>> > And I think the problem with that is simply that some of the tests
>> > collide with each other, by opening up the same port at the same time
>> > for example. I know that kind of problem is being attacked even right
>> > now. But if all the tests were made parallel friendly, I imagine
>> > parallelism could be enabled and speed up builds greatly without any
>> > remote machines.
>> >
>> >
>> > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
>> > <nicholas.chammas@gmail.com> wrote:
>> > > Howdy,
>> > >
>> > > Do we think it's both feasible and worthwhile to invest in getting our
>> > unit
>> > > tests to finish in under 5 minutes (or something similarly brief) when
>> > run
>> > > by Jenkins?
>> > >
>> > > Unit tests currently seem to take anywhere from 30 min to 2 hours. As
>> > > people add more tests, I imagine this time will only grow. I think it
>> > would
>> > > be better for both contributors and reviewers if they didn't have to
>> wait
>> > > so long for test results; PR reviews would be shorter, if nothing
>> else.
>> > >
>> > > I don't know how how this is normally done, but maybe it wouldn't be
>> too
>> > > much work to get a test cycle to feel lighter.
>> > >
>> > > Most unit tests are independent and can be run concurrently, right?
>> Would
>> > > it make sense to build a given patch on many servers at once and send
>> > > disjoint sets of unit tests to each?
>> > >
>> > > I'd be interested in working on something like that if possible (and
>> > > sensible).
>> > >
>> > > Nick
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>

--047d7b8739a8181f41050021504b--

From dev-return-8799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:15:44 2014
Return-Path: <dev-return-8799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F9BA10798
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:15:44 +0000 (UTC)
Received: (qmail 23956 invoked by uid 500); 8 Aug 2014 17:15:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23898 invoked by uid 500); 8 Aug 2014 17:15:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23887 invoked by uid 99); 8 Aug 2014 17:15:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:15:43 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:15:38 +0000
Received: by mail-we0-f182.google.com with SMTP id k48so5985432wev.27
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:15:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CrKeUBGYsaDnCYF3B3lXjWBfNviZUaC4WL3C/nvqTH8=;
        b=NZIftlKoEva0DTSyOHJJ4KsKahkLmCq49yQ18Dg6CVYCM1IPo+ea1DXffT9jkWRRIp
         wzV2X/+mysZzcjSrrjLlLFbVp4aXDrTy+icTFdJA9cBxd1jgjzQNi1A1fKh5sT15u9o0
         qz85YUh09dBGppDw7VIVH+EJgYx6wKPiRjSaoDtuICOjkORwfoyow7HvtFUjF2O7ywG4
         SNJqp4MhhbYv1dknxI5nAfTRqURvJLnKjUiU5Dx4tlaXojsJLMry5yRIQMR5tFgaGRt4
         3RfgIA4WYbCEK+TbPm2EAtldfjNiCcmZZ+duiIKTi2thuSQax1BnH2XbUqmzAEqEswmu
         LpmA==
X-Gm-Message-State: ALoCoQmzJCR22O49iZw4J063a1yEygwkd8pOawMTU7yErzWUqqvrUDBSiTv/GRDSipBuUYz8HoM1
X-Received: by 10.194.206.67 with SMTP id lm3mr34045091wjc.70.1407518117242;
 Fri, 08 Aug 2014 10:15:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:14:57 -0700 (PDT)
In-Reply-To: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:14:57 -0700
Message-ID: <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bacc15e64adab050021599c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacc15e64adab050021599c
Content-Type: text/plain; charset=UTF-8

Pasting a better formatted trace:



at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at
scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606) at
 java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
at
org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
at
org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
at
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
at
 org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)


On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <zlgonzalez@yahoo.com.invalid>
wrote:

> Hi,
> I have a running spark app against the released version of 1.0.1. I
> recently decided to try and upgrade to the trunk version. Interestingly
> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
> assembly in my app caused errors. In particular, it seems Kryo
> serialization isn't taking. Replacing it with 1.0.1 automatically gets it
> working again.
>
> Any thoughts? Is this a known issue?
>
> Thanks,
> Ron
>
> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
> at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
> at
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606) at
>  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
> at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
> at
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
> at
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
> at
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
> at
>  org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)

--047d7bacc15e64adab050021599c--

From dev-return-8800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:16:30 2014
Return-Path: <dev-return-8800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBA3710799
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:16:30 +0000 (UTC)
Received: (qmail 25289 invoked by uid 500); 8 Aug 2014 17:16:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25231 invoked by uid 500); 8 Aug 2014 17:16:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25219 invoked by uid 99); 8 Aug 2014 17:16:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:16:30 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:16:04 +0000
Received: by mail-wi0-f177.google.com with SMTP id ho1so1353874wib.4
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:16:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=yumb2rhj5ZLARx78SHGJJHyf5F7PhRWGKyqDf8vSk1U=;
        b=fy4sPYFsYJDmLP9l3CAJ2VOIzzbmcSAlGz+eB4ouoDZoQnhYIlkWtYGNdzlO3a4cA0
         b30kCr4xlZTrJQiE8LjDB+1JqwOKelYzfdenqLaeZc1ZTr7+eBeseze5TkgpiT9wQ6jg
         aazvkulHgJ0r2n49xq8NpwWkKIYaQNgypskrCBpz0n+q6UXhnZod9K5cWYrhCn3rP+l9
         SC4OqoyM4lAJ5TmkTleXngU7VOUXqqjgdV66eMOYxy+tZUf6BL1XkTdkuLRmIJOynMtA
         LnYDcBM3o7dYwVNbZEvMutYc9s4vjraieLO1DLwdAqS6zubm6HgfzPjedt7kO7F6Q9V9
         pe+g==
X-Gm-Message-State: ALoCoQlYjvTLGN1zdmcRzsMVS+jVpEIQPybr1LemYaaF3WF4Tnr5tczMzoHnPJ0VjE/0toLLYekd
X-Received: by 10.180.187.7 with SMTP id fo7mr725232wic.4.1407518162613; Fri,
 08 Aug 2014 10:16:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:15:41 -0700 (PDT)
In-Reply-To: <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com> <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:15:41 -0700
Message-ID: <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c385441910b10500215cc3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c385441910b10500215cc3
Content-Type: text/plain; charset=UTF-8

Looks like you didn't actually paste the exception message. Do you mind
doing that?



On Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databricks.com> wrote:

> Pasting a better formatted trace:
>
>
>
> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
> at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
> at
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
> at
> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
> at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606) at
>  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
> at
> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
> at
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
> at
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
> at
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
> at
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
> at
>
>  org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>
>
> On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <
> zlgonzalez@yahoo.com.invalid> wrote:
>
>> Hi,
>> I have a running spark app against the released version of 1.0.1. I
>> recently decided to try and upgrade to the trunk version. Interestingly
>> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
>> assembly in my app caused errors. In particular, it seems Kryo
>> serialization isn't taking. Replacing it with 1.0.1 automatically gets it
>> working again.
>>
>> Any thoughts? Is this a known issue?
>>
>> Thanks,
>> Ron
>>
>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
>> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
>> at
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
>> at
>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
>> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
>> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
>> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>> at
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>> at java.lang.reflect.Method.invoke(Method.java:606) at
>>  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
>> at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
>> at
>> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
>> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
>> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
>> at
>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
>> at
>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
>> at
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
>> at
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
>> at
>>  org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
>> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>
>
>

--001a11c385441910b10500215cc3--

From dev-return-8801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:20:35 2014
Return-Path: <dev-return-8801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 373B6107DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:20:35 +0000 (UTC)
Received: (qmail 38045 invoked by uid 500); 8 Aug 2014 17:20:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37990 invoked by uid 500); 8 Aug 2014 17:20:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37964 invoked by uid 99); 8 Aug 2014 17:20:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:20:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkeywal@gmail.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:20:08 +0000
Received: by mail-we0-f175.google.com with SMTP id t60so5971023wes.34
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:20:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=If0kbaVOloZZBTaa8vJNWacvNv9s/NRMKZBLDypuP9M=;
        b=dC6oqoGR+lNmqBwifGPD3pqjzsHg3o6RYbGCube2J3P6fdVYA907Qy/PlWa9xwZN1g
         dg3VAhXOMkmimwceMi2lGvNQ/dLBv0TH1w4W+G2xGxQ55KF62YMdXu2F1h+nESMW6zRY
         WcdThZEpI+KljBZuyzYUIic4EAagK5luwfd33V/fuMMtPkESNRA5oTnJKG2z4VRtnsFv
         K4h1kEu0Njb6vlU7g9vWqQsAub/s1UU/DUPdl6zAzujOS7Rv+WIlk1DaowTAfkDdihX7
         DoaRZaXgFoBgWkdQj1DdI4V1tuFVH8c6DvSaMJpVtJ/wJviepMTxLC2zW9JjEBcACJ5P
         zTcg==
X-Received: by 10.180.97.129 with SMTP id ea1mr5676331wib.29.1407518407803;
 Fri, 08 Aug 2014 10:20:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.205.194 with HTTP; Fri, 8 Aug 2014 10:19:47 -0700 (PDT)
In-Reply-To: <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com> <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
From: Nicolas Liochon <nkeywal@gmail.com>
Date: Fri, 8 Aug 2014 19:19:47 +0200
Message-ID: <CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Reynold Xin <rxin@databricks.com>
Cc: Ted Yu <yuzhihong@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04426f18b64a040500216aaa
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04426f18b64a040500216aaa
Content-Type: text/plain; charset=UTF-8

fwiw, when we did this work in HBase, we categorized the tests. Then some
tests can share a single jvm, while some others need to be isolated in
their own jvm. Nevertheless surefire can still run them in parallel by
starting/stopping several jvm.

Nicolas


On Fri, Aug 8, 2014 at 7:10 PM, Reynold Xin <rxin@databricks.com> wrote:

> ScalaTest actually has support for parallelization built-in. We can use
> that.
>
> The main challenge is to make sure all the test suites can work in parallel
> when running along side each other.
>
>
> On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > How about using parallel execution feature of maven-surefire-plugin
> > (assuming all the tests were made parallel friendly) ?
> >
> >
> >
> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html
> >
> > Cheers
> >
> >
> > On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> > > A common approach is to separate unit tests from integration tests.
> > > Maven has support for this distinction. I'm not sure it helps a lot
> > > though, since it only helps you to not run integration tests all the
> > > time. But lots of Spark tests are integration-test-like and are
> > > important to run to know a change works.
> > >
> > > I haven't heard of a plugin to run different test suites remotely on
> > > many machines, but I would not be surprised if it exists.
> > >
> > > The Jenkins servers aren't CPU-bound as far as I can tell. It's that
> > > the tests spend a lot of time waiting for bits to start up or
> > > complete. That implies the existing tests could be sped up by just
> > > running in parallel locally. I recall someone recently proposed this?
> > >
> > > And I think the problem with that is simply that some of the tests
> > > collide with each other, by opening up the same port at the same time
> > > for example. I know that kind of problem is being attacked even right
> > > now. But if all the tests were made parallel friendly, I imagine
> > > parallelism could be enabled and speed up builds greatly without any
> > > remote machines.
> > >
> > >
> > > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
> > > <nicholas.chammas@gmail.com> wrote:
> > > > Howdy,
> > > >
> > > > Do we think it's both feasible and worthwhile to invest in getting
> our
> > > unit
> > > > tests to finish in under 5 minutes (or something similarly brief)
> when
> > > run
> > > > by Jenkins?
> > > >
> > > > Unit tests currently seem to take anywhere from 30 min to 2 hours. As
> > > > people add more tests, I imagine this time will only grow. I think it
> > > would
> > > > be better for both contributors and reviewers if they didn't have to
> > wait
> > > > so long for test results; PR reviews would be shorter, if nothing
> else.
> > > >
> > > > I don't know how how this is normally done, but maybe it wouldn't be
> > too
> > > > much work to get a test cycle to feel lighter.
> > > >
> > > > Most unit tests are independent and can be run concurrently, right?
> > Would
> > > > it make sense to build a given patch on many servers at once and send
> > > > disjoint sets of unit tests to each?
> > > >
> > > > I'd be interested in working on something like that if possible (and
> > > > sensible).
> > > >
> > > > Nick
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > >
> >
>

--f46d04426f18b64a040500216aaa--

From dev-return-8802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:37:58 2014
Return-Path: <dev-return-8802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B48410887
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:37:58 +0000 (UTC)
Received: (qmail 72614 invoked by uid 500); 8 Aug 2014 17:37:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72546 invoked by uid 500); 8 Aug 2014 17:37:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72495 invoked by uid 99); 8 Aug 2014 17:37:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:37:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 72.30.239.214 as permitted sender)
Received: from [72.30.239.214] (HELO nm40-vm6.bullet.mail.bf1.yahoo.com) (72.30.239.214)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:37:50 +0000
Received: from [98.139.212.149] by nm40.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:37:29 -0000
Received: from [98.139.212.205] by tm6.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:37:29 -0000
Received: from [127.0.0.1] by omp1014.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:37:29 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 611119.97361.bm@omp1014.mail.bf1.yahoo.com
Received: (qmail 32517 invoked by uid 60001); 8 Aug 2014 17:37:29 -0000
X-YMail-OSG: SAfbHpgVM1nP.HFAWRv_gieMuc_rrcvECMAyR.AiGvRy1j2
 kxr8b0fEEhrpcL0oLx0bH2IpcKS36FaOCgrHqK2sNmDHyyG.lvv5xUal6tm6
 Zckk3W8Es4Hc3_XWWZ1rqj2b6KHE34eUz4gx6zs7n2ceZSwMf.QI6cu8yDFu
 8LTqWi5rQOQU4Fe0bXVLlwb8wJTghO1HHAcdk4GDgtwxnfXL0WL7WX_QLpQC
 YFZ8SLDCRhosClL3hCf22GCtpTDTW_I80YRrwXO0jXpKnUTOtI1RhryzxEGQ
 aojeP_SGdOQyLo0HEE.K4yiW2vsj2MBhypIMf82gBpW16RqESsINzfysJvKa
 Jzwd2UTxupX.I.D5olPDn0uVzQykzWqhyvnNkwP__fWsUyhtE3IfHsBn5bZ1
 WOU98gOTHJpNEV1ilExgmsfP2jKgX9vm4k_W2H9mswMnkNQfXwOBY1NtYO.O
 c7dHcgxRwyS8AQUKEFIAGUl_ARoi1FbKD0zH_WmKzGIWr_teBmPT_78pHaDv
 OiEn8ZaR7q1X02RLW1RE0bjuadQiBCtxN.3kXdBYleq1qOKIF
Received: from [12.130.144.124] by web162402.mail.bf1.yahoo.com via HTTP; Fri, 08 Aug 2014 10:37:29 PDT
X-Rocket-MIMEInfo: 002.001,T29wcywgZXhjZXB0aW9uIGlzIGJlbG93LgpGb3IgbG9jYWwsIGl0IHdvcmtzIGFuZCB0aGF0J3MgdGhlIGNhc2Ugc2luY2UgVG9ycmVudEJyb2FkY2FzdCBoYXMgaWYgIWlzTG9jYWwsIHRoZW4gdGhhdCdzIHRoZSBvbmx5IHRpbWUgdGhlIGJyb2FkY2FzdCBhY3R1YWxseSBoYXBwZW5zLiBJdCByZWFsbHkgc2VlbXMgYXMgaWYgdGhlIEtyeW8gd3JhcHBlciBkaWRuJ3Qga2ljayBpbiBmb3Igc29tZSByZWFzb24uIERvIHdlIGhhdmUgYSB1bml0IHRlc3QgdGhhdCB0ZXN0cyB0aGUgS3J5byBzZXJpYWxpemF0aW8BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com> <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com> <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com>
Message-ID: <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com>
Date: Fri, 8 Aug 2014 10:37:29 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Reynold Xin <rxin@databricks.com>
Cc: Dev <dev@spark.apache.org>
In-Reply-To: <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-853603208-1273404997-1407519449=:35539"
X-Virus-Checked: Checked by ClamAV on apache.org

---853603208-1273404997-1407519449=:35539
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Oops, exception is below.=0AFor local, it works and that's the case since T=
orrentBroadcast has if !isLocal, then that's the only time the broadcast ac=
tually happens. It really seems as if the Kryo wrapper didn't kick in for s=
ome reason. Do we have a unit test that tests the Kryo serialization that I=
 can give a try?=0AThanks,=0ARon=0A=0AException in thread "Driver" java.lan=
g.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl=
.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Nati=
veMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.i=
nvoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.inv=
oke(Method.java:606) at org.apache.spark.deploy.yarn.ApplicationMaster$$ano=
n$2.run(ApplicationMaster.scala:180)=0ACaused by: java.io.NotSerializableEx=
ception: org.apache.avro.generic.GenericData$Record - custom writeObject da=
ta (class "scala.collection.mutable.HashMap")=0A=0A=0AOn Friday, August 8, =
2014 10:16 AM, Reynold Xin <rxin@databricks.com> wrote:=0A =0A=0A=0ALooks l=
ike you didn't actually paste the exception message. Do you mind=0Adoing th=
at?=0A=0A=0A=0A=0AOn Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databr=
icks.com> wrote:=0A=0A> Pasting a better formatted trace:=0A>=0A>=0A>=0A> a=
t java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)=0A>=
 at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)=0A>=
 at=0A> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashM=
ap.scala:137)=0A> at=0A> scala.collection.mutable.HashMap$$anonfun$writeObj=
ect$1.apply(HashMap.scala:135)=0A> at=0A> scala.collection.mutable.HashTabl=
e$class.foreachEntry(HashTable.scala:226)=0A> at scala.collection.mutable.H=
ashMap.foreachEntry(HashMap.scala:39)=0A> at=0A> scala.collection.mutable.H=
ashTable$class.serializeTo(HashTable.scala:124)=0A> at scala.collection.mut=
able.HashMap.serializeTo(HashMap.scala:39)=0A> at scala.collection.mutable.=
HashMap.writeObject(HashMap.scala:135)=0A> at sun.reflect.NativeMethodAcces=
sorImpl.invoke0(Native Method)=0A> at=0A> sun.reflect.NativeMethodAccessorI=
mpl.invoke(NativeMethodAccessorImpl.java:57)=0A> at=0A> sun.reflect.Delegat=
ingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)=0A> at j=
ava.lang.reflect.Method.invoke(Method.java:606) at=0A>=A0 java.io.ObjectStr=
eamClass.invokeWriteObject(ObjectStreamClass.java:988)=0A> at=0A> java.io.O=
bjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)=0A> at=0A> =
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431=
)=0A> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:11=
77)=0A> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:3=
47)=0A> at org.apache.spark.util.Utils$.serialize(Utils.scala:64)=0A> at=0A=
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)=0A> at=0A> org.apache.spark.broadcast.TorrentBroadcast.sendB=
roadcast(TorrentBroadcast.scala:85)=0A> at=0A> org.apache.spark.broadcast.T=
orrentBroadcast.<init>(TorrentBroadcast.scala:66)=0A> at=0A> org.apache.spa=
rk.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.s=
cala:36)=0A> at=0A> org.apache.spark.broadcast.TorrentBroadcastFactory.newB=
roadcast(TorrentBroadcastFactory.scala:29)=0A> at=0A>=0A>=A0 org.apache.spa=
rk.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)=0A> a=
t org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)=0A>=0A>=
=0A> On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <=0A> zlgonzalez@yahoo.c=
om.invalid> wrote:=0A>=0A>> Hi,=0A>> I have a running spark app against the=
 released version of 1.0.1. I=0A>> recently decided to try and upgrade to t=
he trunk version. Interestingly=0A>> enough, after building the 1.1.0-SNAPS=
HOT assembly, replacing it as my=0A>> assembly in my app caused errors. In =
particular, it seems Kryo=0A>> serialization isn't taking. Replacing it wit=
h 1.0.1 automatically gets it=0A>> working again.=0A>>=0A>> Any thoughts? I=
s this a known issue?=0A>>=0A>> Thanks,=0A>> Ron=0A>>=0A>> at java.io.Objec=
tOutputStream.writeObject0(ObjectOutputStream.java:1180)=0A>> at java.io.Ob=
jectOutputStream.writeObject(ObjectOutputStream.java:347) at=0A>> scala.col=
lection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)=0A>=
> at=0A>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(Has=
hMap.scala:135)=0A>> at=0A>> scala.collection.mutable.HashTable$class.forea=
chEntry(HashTable.scala:226)=0A>> at scala.collection.mutable.HashMap.forea=
chEntry(HashMap.scala:39) at=0A>> scala.collection.mutable.HashTable$class.=
serializeTo(HashTable.scala:124)=0A>> at scala.collection.mutable.HashMap.s=
erializeTo(HashMap.scala:39) at=0A>> scala.collection.mutable.HashMap.write=
Object(HashMap.scala:135) at=0A>> sun.reflect.NativeMethodAccessorImpl.invo=
ke0(Native Method) at=0A>> sun.reflect.NativeMethodAccessorImpl.invoke(Nati=
veMethodAccessorImpl.java:57)=0A>> at=0A>> sun.reflect.DelegatingMethodAcce=
ssorImpl.invoke(DelegatingMethodAccessorImpl.java:43)=0A>> at java.lang.ref=
lect.Method.invoke(Method.java:606) at=0A>>=A0 java.io.ObjectStreamClass.in=
vokeWriteObject(ObjectStreamClass.java:988)=0A>> at java.io.ObjectOutputStr=
eam.writeSerialData(ObjectOutputStream.java:1495)=0A>> at=0A>> java.io.Obje=
ctOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)=0A>> at ja=
va.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at=0A>>=
 java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at=0A>=
> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at=0A>> org.apache=
.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:23=
2)=0A>> at=0A>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(T=
orrentBroadcast.scala:85)=0A>> at=0A>> org.apache.spark.broadcast.TorrentBr=
oadcast.<init>(TorrentBroadcast.scala:66)=0A>> at=0A>> org.apache.spark.bro=
adcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:3=
6)=0A>> at=0A>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroad=
cast(TorrentBroadcastFactory.scala:29)=0A>> at=0A>>=A0 org.apache.spark.bro=
adcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)=0A>> at org=
.apache.spark.SparkContext.broadcast(SparkContext.scala:809)=0A>=0A>=0A>
---853603208-1273404997-1407519449=:35539--

From dev-return-8803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:41:35 2014
Return-Path: <dev-return-8803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47CD0108C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:41:35 +0000 (UTC)
Received: (qmail 84783 invoked by uid 500); 8 Aug 2014 17:41:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84725 invoked by uid 500); 8 Aug 2014 17:41:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84714 invoked by uid 99); 8 Aug 2014 17:41:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:41:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:41:30 +0000
Received: by mail-wi0-f181.google.com with SMTP id bs8so1377126wib.14
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:41:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=7bn2Q6Sy+fO5LYVIWGoo8wMSV7zNsy7CS0PSxsr62qE=;
        b=AgsZ6hMHz+FkofLWqszDQOPUyKtlSHIAm3bwyG95J9aP/kF576zbA9QT6LUVdGU3RV
         mg1jGB1Sb4bh9LAGHbYozvd+0g/N0hkoGTl5MWp2Tp5Vebehgb7pIRu0xgZP6+5Xqm1b
         gWm0OWG/LX81fZ0fMZqrXh4crGYJ5WWZpLlpXG1ItsbyufJ7zweBxU/y+aLUoy9kLGP4
         ba1p0xGwAD1G61vqQRW+m3jTz2S1lq5XxG4ZKQY8NVWgnnZDXP7n7ah6fwZV91JR3ANi
         ZbXVDJKWQX8x3wFKJL1DPYMdCVmViZCCClKkmPjxs4Kq2QO/euqWk0MGtHx1FqGe3WOe
         sZfQ==
X-Gm-Message-State: ALoCoQkr5++K9It9i3qSVUFplDdozHzpdzZBqRu/epxHv58RBb7J0oGRu8y7u3Zd1UfmPA8E6pWo
X-Received: by 10.194.206.67 with SMTP id lm3mr34194160wjc.70.1407519667821;
 Fri, 08 Aug 2014 10:41:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:40:47 -0700 (PDT)
In-Reply-To: <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com>
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
 <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
 <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com> <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:40:47 -0700
Message-ID: <CAPh_B=anQsBpvOJnyKXDChUvQo6fXtonfvqEAA79Az5Z12cJ4g@mail.gmail.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bacc15ed0b4bc050021b54a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacc15ed0b4bc050021b54a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes, I'm pretty sure it doesn't actually use the right serializer in
TorrentBroadcast:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/=
spark/broadcast/TorrentBroadcast.scala#L232

And TorrentBroadcast is turned on by default for 1.1 right now. Do you want
to submit a pull request to fix that? This would be a critical fix for 1.1
that's worth doing.



On Fri, Aug 8, 2014 at 10:37 AM, Ron Gonzalez <zlgonzalez@yahoo.com> wrote:

> Oops, exception is below.
>
> For local, it works and that's the case since TorrentBroadcast has if !is=
Local, then that's the only time the broadcast actually happens. It really =
seems as if the Kryo wrapper didn't kick in for some reason. Do we have a u=
nit test that tests the Kryo serialization that I can give a try?
>
> Thanks,
>
> Ron
>
>
> Exception in thread "Driver" java.lang.reflect.InvocationTargetException
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.=
java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces=
sorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(Applicatio=
nMaster.scala:180)
> Caused by: java.io.NotSerializableException: org.apache.avro.generic.Gene=
ricData$Record
> 	- custom writeObject data (class "scala.collection.mutable.HashMap")
>
>
>
>   On Friday, August 8, 2014 10:16 AM, Reynold Xin <rxin@databricks.com>
> wrote:
>
>
> Looks like you didn't actually paste the exception message. Do you mind
> doing that?
>
>
>
> On Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Pasting a better formatted trace:
> >
> >
> >
> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180=
)
> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> > at
> >
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:137)
> > at
> >
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:135)
> > at
> >
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226=
)
> > at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
> > at
> > scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:12=
4)
> > at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
> > at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
> > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > at java.lang.reflect.Method.invoke(Method.java:606) at
> >  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988=
)
> > at
> > java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495=
)
> > at
> >
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:14=
31)
> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177=
)
> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> > at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcas=
t.scala:85)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala=
:66)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:36)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:29)
> > at
> >
> >
> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager=
.scala:62)
> > at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
> >
> >
> > On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <
> > zlgonzalez@yahoo.com.invalid> wrote:
> >
> >> Hi,
> >> I have a running spark app against the released version of 1.0.1. I
> >> recently decided to try and upgrade to the trunk version. Interestingl=
y
> >> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
> >> assembly in my app caused errors. In particular, it seems Kryo
> >> serialization isn't taking. Replacing it with 1.0.1 automatically gets
> it
> >> working again.
> >>
> >> Any thoughts? Is this a known issue?
> >>
> >> Thanks,
> >> Ron
> >>
> >> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:118=
0)
> >> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at
> >>
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:137)
> >> at
> >>
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:135)
> >> at
> >>
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226=
)
> >> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
> >>
> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
> >> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
> >> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
> >> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
> >>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >> at
> >>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >> at java.lang.reflect.Method.invoke(Method.java:606) at
> >>  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:98=
8)
> >> at
> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
> >> at
> >>
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:14=
31)
> >> at
> java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
> >> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
> >> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
> >>
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcas=
t.scala:85)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala=
:66)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:36)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:29)
> >> at
> >>
> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager=
.scala:62)
> >> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
> >
> >
> >
>
>
>

--047d7bacc15ed0b4bc050021b54a--

From dev-return-8804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:43:38 2014
Return-Path: <dev-return-8804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C09EE108D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:43:38 +0000 (UTC)
Received: (qmail 92751 invoked by uid 500); 8 Aug 2014 17:43:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92694 invoked by uid 500); 8 Aug 2014 17:43:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92609 invoked by uid 99); 8 Aug 2014 17:43:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:43:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:43:11 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so5884497wes.25
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:43:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=9fFPI3Kmixp8aEWoJUFIRe9+fBs58mA076hcypuOgEA=;
        b=UI7ghTu09FEs/OEsEfOF2SiTqeYmPeYbdNBY72o7epCOswq+Zg0+jNrwecpvN5F5sW
         J284nmxyEIA9g34I3c565Mhqy0Ndyk/lnm+U3wWc3nDviGPc+bYa6optET1TMesk0Sx4
         vj4iIBty/oZaga861N6Rzpwd7tG/uMkX/XPzuALtVE0H9R34fD53hFW5+vrVKGFaHdF3
         EFZKRVUq37y882WL+8/dLjc7sqVYveedHhljcR9CXM2VTT/0uUPvoLVMehA+BF64eUPv
         wqqugql0nsNorT6t5MgucIIQSFCAKx+tG/9laiJT+q0/0I2XfZtYHlKMIhzGvijBM6NI
         cFHQ==
X-Gm-Message-State: ALoCoQlkqo2igGRM3VhMxgjXLdGw+Cs7IyEBKBPX6ntdgVzWBNsezYzG6WsL0VI+pAk/5QJ7vXx4
X-Received: by 10.180.91.111 with SMTP id cd15mr5406494wib.69.1407519790758;
 Fri, 08 Aug 2014 10:43:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:42:50 -0700 (PDT)
In-Reply-To: <CAPh_B=anQsBpvOJnyKXDChUvQo6fXtonfvqEAA79Az5Z12cJ4g@mail.gmail.com>
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
 <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
 <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com>
 <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com> <CAPh_B=anQsBpvOJnyKXDChUvQo6fXtonfvqEAA79Az5Z12cJ4g@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:42:50 -0700
Message-ID: <CAPh_B=Ybq4aLvy9uUEGBSmxvTU51Z8gFr1VhgAahMMHaLgvBEA@mail.gmail.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04388ebf24991b050021bdcb
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04388ebf24991b050021bdcb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I created a JIRA ticket to track this:
https://issues.apache.org/jira/browse/SPARK-2928

Let me know if you need help with it.



On Fri, Aug 8, 2014 at 10:40 AM, Reynold Xin <rxin@databricks.com> wrote:

> Yes, I'm pretty sure it doesn't actually use the right serializer in
> TorrentBroadcast:
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/broadcast/TorrentBroadcast.scala#L232
>
> And TorrentBroadcast is turned on by default for 1.1 right now. Do you
> want to submit a pull request to fix that? This would be a critical fix f=
or
> 1.1 that's worth doing.
>
>
>
> On Fri, Aug 8, 2014 at 10:37 AM, Ron Gonzalez <zlgonzalez@yahoo.com>
> wrote:
>
>> Oops, exception is below.
>>
>> For local, it works and that's the case since TorrentBroadcast has if !i=
sLocal, then that's the only time the broadcast actually happens. It really=
 seems as if the Kryo wrapper didn't kick in for some reason. Do we have a =
unit test that tests the Kryo serialization that I can give a try?
>>
>> Thanks,
>>
>> Ron
>>
>>
>> Exception in thread "Driver" java.lang.reflect.InvocationTargetException
>> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl=
.java:57)
>> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcce=
ssorImpl.java:43)
>> 	at java.lang.reflect.Method.invoke(Method.java:606)
>> 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(Applicati=
onMaster.scala:180)
>> Caused by: java.io.NotSerializableException: org.apache.avro.generic.Gen=
ericData$Record
>> 	- custom writeObject data (class "scala.collection.mutable.HashMap")
>>
>>
>>
>>   On Friday, August 8, 2014 10:16 AM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>
>>
>> Looks like you didn't actually paste the exception message. Do you mind
>> doing that?
>>
>>
>>
>> On Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databricks.com> wrote=
:
>>
>> > Pasting a better formatted trace:
>> >
>> >
>> >
>> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:118=
0)
>> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
>> > at
>> >
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sc=
ala:137)
>> > at
>> >
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sc=
ala:135)
>> > at
>> >
>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:22=
6)
>> > at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
>> > at
>> >
>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124=
)
>> > at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
>> > at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
>> > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> > at
>> >
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav=
a:57)
>> > at
>> >
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor=
Impl.java:43)
>> > at java.lang.reflect.Method.invoke(Method.java:606) at
>> >  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:98=
8)
>> > at
>> > java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:149=
5)
>> > at
>> >
>> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1=
431)
>> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:117=
7)
>> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
>> > at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
>> > at
>> >
>> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroad=
cast.scala:232)
>> > at
>> >
>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadca=
st.scala:85)
>> > at
>> >
>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scal=
a:66)
>> > at
>> >
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentB=
roadcastFactory.scala:36)
>> > at
>> >
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentB=
roadcastFactory.scala:29)
>> > at
>> >
>> >
>> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManage=
r.scala:62)
>> > at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>> >
>> >
>> > On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <
>> > zlgonzalez@yahoo.com.invalid> wrote:
>> >
>> >> Hi,
>> >> I have a running spark app against the released version of 1.0.1. I
>> >> recently decided to try and upgrade to the trunk version. Interesting=
ly
>> >> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as m=
y
>> >> assembly in my app caused errors. In particular, it seems Kryo
>> >> serialization isn't taking. Replacing it with 1.0.1 automatically get=
s
>> it
>> >> working again.
>> >>
>> >> Any thoughts? Is this a known issue?
>> >>
>> >> Thanks,
>> >> Ron
>> >>
>> >> at
>> java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
>> >> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347=
)
>> at
>> >>
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sc=
ala:137)
>> >> at
>> >>
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sc=
ala:135)
>> >> at
>> >>
>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:22=
6)
>> >> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
>> >>
>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124=
)
>> >> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
>> >> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
>> >> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
>> >>
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav=
a:57)
>> >> at
>> >>
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor=
Impl.java:43)
>> >> at java.lang.reflect.Method.invoke(Method.java:606) at
>> >>
>> java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
>> >> at
>> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
>> >> at
>> >>
>> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1=
431)
>> >> at
>> java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
>> >> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) a=
t
>> >> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
>> >>
>> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroad=
cast.scala:232)
>> >> at
>> >>
>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadca=
st.scala:85)
>> >> at
>> >>
>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scal=
a:66)
>> >> at
>> >>
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentB=
roadcastFactory.scala:36)
>> >> at
>> >>
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentB=
roadcastFactory.scala:29)
>> >> at
>> >>
>> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManage=
r.scala:62)
>> >> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>> >
>> >
>> >
>>
>>
>>
>

--f46d04388ebf24991b050021bdcb--

From dev-return-8805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:50:49 2014
Return-Path: <dev-return-8805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B699610928
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:50:49 +0000 (UTC)
Received: (qmail 6953 invoked by uid 500); 8 Aug 2014 17:50:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6895 invoked by uid 500); 8 Aug 2014 17:50:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6883 invoked by uid 99); 8 Aug 2014 17:50:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:50:48 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.150 as permitted sender)
Received: from [98.139.213.150] (HELO nm5-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.150)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:50:43 +0000
Received: from [98.139.215.143] by nm5.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:50:22 -0000
Received: from [98.139.212.193] by tm14.bullet.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:50:22 -0000
Received: from [127.0.0.1] by omp1002.mail.bf1.yahoo.com with NNFMP; 08 Aug 2014 17:50:22 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 469573.32844.bm@omp1002.mail.bf1.yahoo.com
Received: (qmail 55134 invoked by uid 60001); 8 Aug 2014 17:50:22 -0000
X-YMail-OSG: BdWvVskVM1lBoYbCVdq48FOd0z6xXkwx4sZXy9Q2Zp79Gep
 HRbNrY1NdjQgj.TPniQADXs7Mij3OgPW3GZOJmjHXJdWrgmr5NyiAQX2swwA
 vBfDllph..ApKVGNDKgoSM_Yi6H5nnhLvCjW4GhSQaAzJlClo563rOfawGSp
 OLVyT7PPsgNwOVFgA9Fqfl_0_lqdTEUDZSITz9yE1oGI5dC9qNqg227Ocdct
 9fyEoQ39RHtLVJxTozFrMIrByDMmU365g37ppChpsT7rNBqcYbBayVjaa75U
 EF5pNaShc2X0RmZDa6xCUsrGScNqPpv0Lffx1wWSV5rC2lBYQ2JL3UXYwITj
 IHrKpau7PkMtisjDa0uVRF_MvrJ4vSd96.1aLmSebwMxJ1qt8JxUhRc4Q20E
 Tjn2Y8EuJnyQGgyCPHRXdCtfiNPb3p7xITPIG20Y2NjreBqKATZQAvcIZjkp
 UA7_nU4H0oX82Uf35vxx2CIt1B.lGwIHcn_mGq1_WUD7J3v.IYvTdIgo7cfL
 ckCQ1jo8L0pMHYyhF8aBh2MK4llZrDkjH_gqn9FjVRDtXyUjWfoA9uOp8JAR
 95ZeFaCwqr7pFpVhLwRgY6hYymBoLTBFNsfz.knysfCtJoIeTtyTzbzxYToF
 L6YBOogDXcY4CXIIHy.c_CqSj5vHgmnwN.u9CukSTb7gzH5LSZXyeUxDi5BV
 U6x1vebcOSdNJ7rPV_663m74G_Kk2d87nIGqIdpZ_N6kotGd9SIYeczGHr7n
 3pg6yd39Pkr5SGfYazL4tVUQBEK1N.Q4bJSKvPtlkDWKppiOjHxC70xtZbfI
 .4s0GrJ2Raq9H9QtM
Received: from [12.130.144.124] by web162406.mail.bf1.yahoo.com via HTTP; Fri, 08 Aug 2014 10:50:22 PDT
X-Rocket-MIMEInfo: 002.001,U3VyZSBsZXQgbWUgZ2l2ZSBpdCBhIHRyeS4gQW55IHRpcHM_IEkndmUgb25seSBzdGFydGVkIGxvb2tpbmcgYXQgU3BhcmsgY29kZSBtb3JlIGNsb3NlbHkgcmVjZW50bHkuCkkgY2FuIGNvbXBhcmUgU3BhcmstMS4wLjEgY29kZSBhbmQgc2VlIHdoYXQncyBnb2luZyBvbi4uLgoKVGhhbmtzLApSb24KCgpPbiBGcmlkYXksIEF1Z3VzdCA4LCAyMDE0IDEwOjQzIEFNLCBSZXlub2xkIFhpbiA8cnhpbkBkYXRhYnJpY2tzLmNvbT4gd3JvdGU6CiAKCgpJIGNyZWF0ZWQgYSBKSVJBIHRpY2tldCB0byB0cmFjayB0aGkBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.198.689
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com> <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com> <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com> <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com> <CAPh_B=anQsBpvOJnyKXDChUvQo6fXtonfvqEAA79Az5Z12cJ4g@mail.gmail.com> <CAPh_B=Ybq4aLvy9uUEGBSmxvTU51Z8gFr1VhgAahMMHaLgvBEA@mail.gmail.com>
Message-ID: <1407520222.52114.YahooMailNeo@web162406.mail.bf1.yahoo.com>
Date: Fri, 8 Aug 2014 10:50:22 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
Reply-To: Ron Gonzalez <zlgonzalez@yahoo.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Reynold Xin <rxin@databricks.com>
Cc: Dev <dev@spark.apache.org>
In-Reply-To: <CAPh_B=Ybq4aLvy9uUEGBSmxvTU51Z8gFr1VhgAahMMHaLgvBEA@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-747684034-32925563-1407520222=:52114"
X-Virus-Checked: Checked by ClamAV on apache.org

---747684034-32925563-1407520222=:52114
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Sure let me give it a try. Any tips? I've only started looking at Spark cod=
e more closely recently.=0AI can compare Spark-1.0.1 code and see what's go=
ing on...=0A=0AThanks,=0ARon=0A=0A=0AOn Friday, August 8, 2014 10:43 AM, Re=
ynold Xin <rxin@databricks.com> wrote:=0A =0A=0A=0AI created a JIRA ticket =
to track this:=A0https://issues.apache.org/jira/browse/SPARK-2928=0A=0ALet =
me know if you need help with it.=0A=0A=0A=0A=0AOn Fri, Aug 8, 2014 at 10:4=
0 AM, Reynold Xin <rxin@databricks.com> wrote:=0A=0AYes, I'm pretty sure it=
 doesn't actually use the right serializer in TorrentBroadcast:=A0https://g=
ithub.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/bro=
adcast/TorrentBroadcast.scala#L232=0A>=0A>=0A>And TorrentBroadcast is turne=
d on by default for 1.1 right now. Do you want to submit a pull request to =
fix that? This would be a critical fix for 1.1 that's worth doing.=0A>=0A>=
=0A>=0A>=0A>=0A>On Fri, Aug 8, 2014 at 10:37 AM, Ron Gonzalez <zlgonzalez@y=
ahoo.com> wrote:=0A>=0A>Oops, exception is below.=0A>>For local, it works a=
nd that's the case since TorrentBroadcast has if !isLocal, then that's the =
only time the broadcast actually happens. It really seems as if the Kryo wr=
apper didn't kick in for some reason. Do we have a unit test that tests the=
 Kryo serialization that I can give a try?=0A>>Thanks,=0A>>Ron=0A>>=0A>>=0A=
>>Exception in thread "Driver" java.lang.reflect.InvocationTargetException =
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.refle=
ct.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun=
.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j=
ava:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.s=
park.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:180)=
=0ACaused by: java.io.NotSerializableException: org.apache.avro.generic.Gen=
ericData$Record - custom writeObject data (class "scala.collection.mutable.=
HashMap")=0A>>=0A>>=0A>>=0A>>On Friday, August 8, 2014 10:16 AM, Reynold Xi=
n <rxin@databricks.com> wrote:=0A>> =0A>>=0A>>=0A>>Looks like you didn't ac=
tually paste the exception message. Do you mind=0A>>doing that?=0A>>=0A>>=
=0A>>=0A>>=0A>>On Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databrick=
s.com> wrote:=0A>>=0A>>> Pasting a better formatted trace:=0A>>>=0A>>>=0A>>=
>=0A>>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:=
1180)=0A>>> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.ja=
va:347)=0A>>> at=0A>>> scala.collection.mutable.HashMap$$anonfun$writeObjec=
t$1.apply(HashMap.scala:137)=0A>>> at=0A>>>=0A scala.collection.mutable.Has=
hMap$$anonfun$writeObject$1.apply(HashMap.scala:135)=0A>>> at=0A>>> scala.c=
ollection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)=0A>>> a=
t scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)=0A>>> at=
=0A>>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala=
:124)=0A>>> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:3=
9)=0A>>> at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)=
=0A>>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)=0A>>>=
 at=0A>>>=0A sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccess=
orImpl.java:57)=0A>>> at=0A>>> sun.reflect.DelegatingMethodAccessorImpl.inv=
oke(DelegatingMethodAccessorImpl.java:43)=0A>>> at java.lang.reflect.Method=
.invoke(Method.java:606) at=0A>>>=A0 java.io.ObjectStreamClass.invokeWriteO=
bject(ObjectStreamClass.java:988)=0A>>> at=0A>>> java.io.ObjectOutputStream=
.writeSerialData(ObjectOutputStream.java:1495)=0A>>> at=0A>>> java.io.Objec=
tOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)=0A>>> at ja=
va.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)=0A>>> a=
t=0A java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)=0A=
>>> at org.apache.spark.util.Utils$.serialize(Utils.scala:64)=0A>>> at=0A>>=
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)=0A>>> at=0A>>> org.apache.spark.broadcast.TorrentBroadcast.s=
endBroadcast(TorrentBroadcast.scala:85)=0A>>> at=0A>>> org.apache.spark.bro=
adcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)=0A>>> at=0A>>> or=
g.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadc=
astFactory.scala:36)=0A>>> at=0A>>>=0A org.apache.spark.broadcast.TorrentBr=
oadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)=0A>>> at=0A>>=
>=0A>>>=A0 org.apache.spark.broadcast.BroadcastManager.newBroadcast(Broadca=
stManager.scala:62)=0A>>> at org.apache.spark.SparkContext.broadcast(SparkC=
ontext.scala:809)=0A>>>=0A>>>=0A>>> On Fri, Aug 8, 2014 at 10:12 AM, Ron Go=
nzalez <=0A>>> zlgonzalez@yahoo.com.invalid> wrote:=0A>>>=0A>>>> Hi,=0A>>>>=
 I have a running spark app against the released=0A version of 1.0.1. I=0A>=
>>> recently decided to try and upgrade to the trunk version. Interestingly=
=0A>>>> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as=
 my=0A>>>> assembly in my app caused errors. In particular, it seems Kryo=
=0A>>>> serialization isn't taking. Replacing it with 1.0.1 automatically g=
ets it=0A>>>> working again.=0A>>>>=0A>>>> Any thoughts? Is this a known is=
sue?=0A>>>>=0A>>>> Thanks,=0A>>>> Ron=0A>>>>=0A>>>> at java.io.ObjectOutput=
Stream.writeObject0(ObjectOutputStream.java:1180)=0A>>>> at java.io.ObjectO=
utputStream.writeObject(ObjectOutputStream.java:347) at=0A>>>> scala.collec=
tion.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)=0A>>>>=
 at=0A>>>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(Ha=
shMap.scala:135)=0A>>>> at=0A>>>> scala.collection.mutable.HashTable$class.=
foreachEntry(HashTable.scala:226)=0A>>>> at scala.collection.mutable.HashMa=
p.foreachEntry(HashMap.scala:39) at=0A>>>> scala.collection.mutable.HashTab=
le$class.serializeTo(HashTable.scala:124)=0A>>>> at scala.collection.mutabl=
e.HashMap.serializeTo(HashMap.scala:39) at=0A>>>>=0A scala.collection.mutab=
le.HashMap.writeObject(HashMap.scala:135) at=0A>>>> sun.reflect.NativeMetho=
dAccessorImpl.invoke0(Native Method) at=0A>>>> sun.reflect.NativeMethodAcce=
ssorImpl.invoke(NativeMethodAccessorImpl.java:57)=0A>>>> at=0A>>>> sun.refl=
ect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4=
3)=0A>>>> at java.lang.reflect.Method.invoke(Method.java:606) at=0A>>>>=A0 =
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)=0A>=
>>> at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1=
495)=0A>>>> at=0A>>>>=0A java.io.ObjectOutputStream.writeOrdinaryObject(Obj=
ectOutputStream.java:1431)=0A>>>> at java.io.ObjectOutputStream.writeObject=
0(ObjectOutputStream.java:1177) at=0A>>>> java.io.ObjectOutputStream.writeO=
bject(ObjectOutputStream.java:347) at=0A>>>> org.apache.spark.util.Utils$.s=
erialize(Utils.scala:64) at=0A>>>> org.apache.spark.broadcast.TorrentBroadc=
ast$.blockifyObject(TorrentBroadcast.scala:232)=0A>>>> at=0A>>>> org.apache=
.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)=
=0A>>>> at=0A>>>> org.apache.spark.broadcast.TorrentBroadcast.<init>(Torren=
tBroadcast.scala:66)=0A>>>> at=0A>>>> org.apache.spark.broadcast.TorrentBro=
adcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)=0A>>>> at=0A>>=
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentB=
roadcastFactory.scala:29)=0A>>>> at=0A>>>>=A0 org.apache.spark.broadcast.Br=
oadcastManager.newBroadcast(BroadcastManager.scala:62)=0A>>>> at org.apache=
.spark.SparkContext.broadcast(SparkContext.scala:809)=0A>>>=0A>>>=0A>>>=0A>=
>=0A>>=0A>>=0A>
---747684034-32925563-1407520222=:52114--

From dev-return-8806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 17:51:41 2014
Return-Path: <dev-return-8806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E017E10939
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 17:51:40 +0000 (UTC)
Received: (qmail 12163 invoked by uid 500); 8 Aug 2014 17:51:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12106 invoked by uid 500); 8 Aug 2014 17:51:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12083 invoked by uid 99); 8 Aug 2014 17:51:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:51:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 17:51:36 +0000
Received: by mail-wi0-f173.google.com with SMTP id f8so1418789wiw.12
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 10:51:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=VwxtKFotpAZ0DuKIR9D+ymXCCV56QBkhs8JRb4waCMw=;
        b=iTUxNmbt6gkUU2S+fuj7liHWVEZxr3oockXlERX0LnsP8JPFnhlZqeyNBG6COB7Rjf
         o+iRdZS8JosTSG8O1e28HWKTZpdHrAQzvr2HlbpsRvMWwoa4go5mIBaqCc9so0IIPLRE
         I/ATydz9NuXDfV04tv5dIucakvTVIEEM9oo2ZY6oQTZpYlT83V3dbGPr3DAOvV6VvVrW
         9r3DOsQeO0O4J7ILoUUgosM1/B/Fu9rapUZ0F7sfvvGAmwj3dz3l2YzWUO0fiQpsjWK8
         l7+pQrChLLe3CnkJREv7SX1DdOTqg2H1MgN8aZZlyyCjItb6tEYLfBejuwVP7AKpxg12
         pRnA==
X-Gm-Message-State: ALoCoQmpqxvjyvEyJAOi3mVNP2vVdm2meudX2qzFIQo2uhfOQlyyfv7RASelffM2awVfRQrf/zU4
X-Received: by 10.180.95.66 with SMTP id di2mr5897692wib.60.1407520274525;
 Fri, 08 Aug 2014 10:51:14 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.207.115 with HTTP; Fri, 8 Aug 2014 10:50:53 -0700 (PDT)
In-Reply-To: <1407520222.52114.YahooMailNeo@web162406.mail.bf1.yahoo.com>
References: <1407517929.40821.YahooMailNeo@web162405.mail.bf1.yahoo.com>
 <CAPh_B=agKxAAn9n5r4E_n6Tt0SusT6tZByCUhi06GDFR4iNg8A@mail.gmail.com>
 <CAPh_B=Y_fGG33jt3e1i2vjXH+98xYPybD=HheE3FMg0JtfxRUw@mail.gmail.com>
 <1407519449.35539.YahooMailNeo@web162402.mail.bf1.yahoo.com>
 <CAPh_B=anQsBpvOJnyKXDChUvQo6fXtonfvqEAA79Az5Z12cJ4g@mail.gmail.com>
 <CAPh_B=Ybq4aLvy9uUEGBSmxvTU51Z8gFr1VhgAahMMHaLgvBEA@mail.gmail.com> <1407520222.52114.YahooMailNeo@web162406.mail.bf1.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 8 Aug 2014 10:50:53 -0700
Message-ID: <CAPh_B=aszL0aeCT0nox79-wT2jaPCcjQrun8hdvzCDWDvzNV3g@mail.gmail.com>
Subject: Re: 1.1.0-SNAPSHOT possible regression
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04182524fa4983050021d900
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04182524fa4983050021d900
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Actually apparently there is a pull request for it. Thanks for reporting!

https://github.com/apache/spark/pull/1836



On Fri, Aug 8, 2014 at 10:50 AM, Ron Gonzalez <zlgonzalez@yahoo.com> wrote:

> Sure let me give it a try. Any tips? I've only started looking at Spark
> code more closely recently.
> I can compare Spark-1.0.1 code and see what's going on...
>
> Thanks,
> Ron
>
>
>   On Friday, August 8, 2014 10:43 AM, Reynold Xin <rxin@databricks.com>
> wrote:
>
>
>  I created a JIRA ticket to track this:
> https://issues.apache.org/jira/browse/SPARK-2928
>
> Let me know if you need help with it.
>
>
>
> On Fri, Aug 8, 2014 at 10:40 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> Yes, I'm pretty sure it doesn't actually use the right serializer in
> TorrentBroadcast:
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/broadcast/TorrentBroadcast.scala#L232
>
> And TorrentBroadcast is turned on by default for 1.1 right now. Do you
> want to submit a pull request to fix that? This would be a critical fix f=
or
> 1.1 that's worth doing.
>
>
>
> On Fri, Aug 8, 2014 at 10:37 AM, Ron Gonzalez <zlgonzalez@yahoo.com>
> wrote:
>
>
> Oops, exception is below.
>
> For local, it works and that's the case since TorrentBroadcast has if !is=
Local, then that's the only time the broadcast actually happens. It really =
seems as if the Kryo wrapper didn't kick in for some reason. Do we have a u=
nit test that tests the Kryo serialization that I can give a try?
>
> Thanks,
>
> Ron
>
>
> Exception in thread "Driver" java.lang.reflect.InvocationTargetException
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.=
java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces=
sorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(Applicatio=
nMaster.scala:180)
> Caused by: java.io.NotSerializableException: org.apache.avro.generic.Gene=
ricData$Record
> 	- custom writeObject data (class "scala.collection.mutable.HashMap")
>
>
>
>   On Friday, August 8, 2014 10:16 AM, Reynold Xin <rxin@databricks.com>
> wrote:
>
>
> Looks like you didn't actually paste the exception message. Do you mind
> doing that?
>
>
>
> On Fri, Aug 8, 2014 at 10:14 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Pasting a better formatted trace:
> >
> >
> >
> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180=
)
> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> > at
> >
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:137)
> > at
> >
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:135)
> > at
> >
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226=
)
> > at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
> > at
> > scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:12=
4)
> > at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
> > at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
> > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > at java.lang.reflect.Method.invoke(Method.java:606) at
> >  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988=
)
> > at
> > java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495=
)
> > at
> >
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:14=
31)
> > at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177=
)
> > at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> > at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcas=
t.scala:85)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala=
:66)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:36)
> > at
> >
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:29)
> > at
> >
> >
> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager=
.scala:62)
> > at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
> >
> >
> > On Fri, Aug 8, 2014 at 10:12 AM, Ron Gonzalez <
> > zlgonzalez@yahoo.com.invalid> wrote:
> >
> >> Hi,
> >> I have a running spark app against the released version of 1.0.1. I
> >> recently decided to try and upgrade to the trunk version. Interestingl=
y
> >> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
> >> assembly in my app caused errors. In particular, it seems Kryo
> >> serialization isn't taking. Replacing it with 1.0.1 automatically gets
> it
> >> working again.
> >>
> >> Any thoughts? Is this a known issue?
> >>
> >> Thanks,
> >> Ron
> >>
> >> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:118=
0)
> >> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at
> >>
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:137)
> >> at
> >>
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.sca=
la:135)
> >> at
> >>
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226=
)
> >> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
> >>
> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
> >> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
> >> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
> >> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
> >>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >> at
> >>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >> at java.lang.reflect.Method.invoke(Method.java:606) at
> >>  java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:98=
8)
> >> at
> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
> >> at
> >>
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:14=
31)
> >> at
> java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
> >> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
> >> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
> >>
> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadc=
ast.scala:232)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcas=
t.scala:85)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala=
:66)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:36)
> >> at
> >>
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBr=
oadcastFactory.scala:29)
> >> at
> >>
> org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager=
.scala:62)
> >> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
> >
> >
> >
>
>
>
>
>
>
>

--f46d04182524fa4983050021d900--

From dev-return-8807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 18:25:00 2014
Return-Path: <dev-return-8807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7795C10B1C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 18:25:00 +0000 (UTC)
Received: (qmail 705 invoked by uid 500); 8 Aug 2014 18:25:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 654 invoked by uid 500); 8 Aug 2014 18:25:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 636 invoked by uid 99); 8 Aug 2014 18:24:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 18:24:59 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 18:24:33 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so1496572pdi.17
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 11:24:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=vja/1O1dhd4oto8UpkDrEQgn5wdHFRvg8/CA8KTohvE=;
        b=F1AiwRNNh3pqQfT8kIzkbYVqDfa1yMFOzB1Ymmu1ab5GUot4MLzoWrCQPOqsS9nW+H
         4DLkHZQR5Ncl+nUngOk1wftB7vvqeCPo7QXIsoV2+L9SwJrn3SZtSF/naPSPsWZ9HG9m
         R7hAau+SqHfZhiHSBgit0eyE2muRo/nCUvcqR+ZDWoKs+EJgt9AdhOulwJzoc7gEcyOc
         rloC+/5ssZH65MoSwKgfgMBYS543i8Bk1k3lRzc8ZwjM2Kq083oO9KP96qk7kFcmobKt
         elPZbg2aJd2kEqdmn67QgJb0IkpE5HlH4/3WNLkStKnKlIcVRUOxNOnmQjwfMEyuSYQ/
         RjQg==
X-Received: by 10.68.99.101 with SMTP id ep5mr3830095pbb.149.1407522271263;
        Fri, 08 Aug 2014 11:24:31 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id jy4sm3677765pbb.68.2014.08.08.11.24.25
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 08 Aug 2014 11:24:26 -0700 (PDT)
Date: Fri, 8 Aug 2014 11:24:23 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Nicolas Liochon <nkeywal@gmail.com>, Reynold Xin
 <rxin@databricks.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Ted Yu
 <yuzhihong@gmail.com>, dev <dev@spark.apache.org>, Sean Owen
 <sowen@cloudera.com>
Message-ID: <etPan.53e515d7.7c83e458.119a7@mbp-3>
In-Reply-To: <CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
 <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
 <CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53e515d7_257130a3_119a7"
X-Virus-Checked: Checked by ClamAV on apache.org

--53e515d7_257130a3_119a7
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Just as a note, when you're developing stuff, you can use "test-only" in sbt, or the equivalent feature in Maven, to run just some of the tests. This is what I do, I don't wait for Jenkins to run things. 90% of the time if it passes the tests that I know could break stuff, it will pass all of Jenkins.

Jenkins should always be doing all the integration tests, so I don't think it will become *that* much shorter in the long run, though it can certainly be improved.

Matei

On August 8, 2014 at 10:20:35 AM, Nicolas Liochon (nkeywal@gmail.com) wrote:

fwiw, when we did this work in HBase, we categorized the tests. Then some 
tests can share a single jvm, while some others need to be isolated in 
their own jvm. Nevertheless surefire can still run them in parallel by 
starting/stopping several jvm. 

Nicolas 


On Fri, Aug 8, 2014 at 7:10 PM, Reynold Xin <rxin@databricks.com> wrote: 

> ScalaTest actually has support for parallelization built-in. We can use 
> that. 
> 
> The main challenge is to make sure all the test suites can work in parallel 
> when running along side each other. 
> 
> 
> On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote: 
> 
> > How about using parallel execution feature of maven-surefire-plugin 
> > (assuming all the tests were made parallel friendly) ? 
> > 
> > 
> > 
> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html 
> > 
> > Cheers 
> > 
> > 
> > On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote: 
> > 
> > > A common approach is to separate unit tests from integration tests. 
> > > Maven has support for this distinction. I'm not sure it helps a lot 
> > > though, since it only helps you to not run integration tests all the 
> > > time. But lots of Spark tests are integration-test-like and are 
> > > important to run to know a change works. 
> > > 
> > > I haven't heard of a plugin to run different test suites remotely on 
> > > many machines, but I would not be surprised if it exists. 
> > > 
> > > The Jenkins servers aren't CPU-bound as far as I can tell. It's that 
> > > the tests spend a lot of time waiting for bits to start up or 
> > > complete. That implies the existing tests could be sped up by just 
> > > running in parallel locally. I recall someone recently proposed this? 
> > > 
> > > And I think the problem with that is simply that some of the tests 
> > > collide with each other, by opening up the same port at the same time 
> > > for example. I know that kind of problem is being attacked even right 
> > > now. But if all the tests were made parallel friendly, I imagine 
> > > parallelism could be enabled and speed up builds greatly without any 
> > > remote machines. 
> > > 
> > > 
> > > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas 
> > > <nicholas.chammas@gmail.com> wrote: 
> > > > Howdy, 
> > > > 
> > > > Do we think it's both feasible and worthwhile to invest in getting 
> our 
> > > unit 
> > > > tests to finish in under 5 minutes (or something similarly brief) 
> when 
> > > run 
> > > > by Jenkins? 
> > > > 
> > > > Unit tests currently seem to take anywhere from 30 min to 2 hours. As 
> > > > people add more tests, I imagine this time will only grow. I think it 
> > > would 
> > > > be better for both contributors and reviewers if they didn't have to 
> > wait 
> > > > so long for test results; PR reviews would be shorter, if nothing 
> else. 
> > > > 
> > > > I don't know how how this is normally done, but maybe it wouldn't be 
> > too 
> > > > much work to get a test cycle to feel lighter. 
> > > > 
> > > > Most unit tests are independent and can be run concurrently, right? 
> > Would 
> > > > it make sense to build a given patch on many servers at once and send 
> > > > disjoint sets of unit tests to each? 
> > > > 
> > > > I'd be interested in working on something like that if possible (and 
> > > > sensible). 
> > > > 
> > > > Nick 
> > > 
> > > --------------------------------------------------------------------- 
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
> > > For additional commands, e-mail: dev-help@spark.apache.org 
> > > 
> > > 
> > 
> 

--53e515d7_257130a3_119a7--


From dev-return-8808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 18:54:20 2014
Return-Path: <dev-return-8808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11A5110CD2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 18:54:20 +0000 (UTC)
Received: (qmail 87775 invoked by uid 500); 8 Aug 2014 18:54:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87716 invoked by uid 500); 8 Aug 2014 18:54:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87704 invoked by uid 99); 8 Aug 2014 18:54:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 18:54:18 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 18:54:14 +0000
Received: by mail-oa0-f45.google.com with SMTP id i7so4417850oag.18
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 11:53:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=NslmFJzGGrBE4TzHo5zp/+POKRIqxeYKJRKz0TVA6mo=;
        b=agdjpeDEpSR6+HvcqTQFb0/WPaGq1m/mjerFyL70ou3cl4+Ym5fbiqbYXCiAfnppon
         FdLnkzwbfEi2DXMLDKk/lrdNJdzd3o9OCsefhWKfHd73Z+hVlOrxVZjmS7xIx+ZFoyqH
         5SUYxlqTh2Nf4+8yjnULHVjgA0E189JkmdyaYyEHnePu/dZSx/10eB4iruEdtGB4+nIw
         5BFzm5tc/ADNTkEx/SZUUYYaoIQ4Ap5nWP+4JSOsmV/Duvh7JBkZ6TA0Y/7QWc4l3Az7
         VLualUOdvzTl64BZG9CXhfGWJobwfkTQAIIalJgBf/5T9K05cDwaR+IXynumcQ68RaXz
         NYAw==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr5099243oes.78.1407524034157; Fri,
 08 Aug 2014 11:53:54 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 8 Aug 2014 11:53:54 -0700 (PDT)
In-Reply-To: <etPan.53e515d7.7c83e458.119a7@mbp-3>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
	<CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
	<CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
	<CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
	<etPan.53e515d7.7c83e458.119a7@mbp-3>
Date: Fri, 8 Aug 2014 11:53:54 -0700
Message-ID: <CABPQxssgHOaiAinN=nDZfm-ckxMc5NzDhV1SsjAYgxHfQiO5eQ@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Patrick Wendell <pwendell@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Nicolas Liochon <nkeywal@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, Ted Yu <yuzhihong@gmail.com>, 
	dev <dev@spark.apache.org>, Sean Owen <sowen@cloudera.com>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I dug around this a bit a while ago, I think if someone sat down and
profiled the tests it's likely we could find some things to optimize.
In particular, there may be overheads in starting up a local spark
context that could be minimized and speed up all the tests. Also,
there are some tests (especially in Streaming) that take really long,
like 60 seconds for a single test (see some of the new flume tests).
These could almost certainly be optimized.

I think 5 minutes might be out of reach, but something like a 2X
improvement might be possible and would be very valuable if
accomplished.

- Patrick

On Fri, Aug 8, 2014 at 11:24 AM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> Just as a note, when you're developing stuff, you can use "test-only" in =
sbt, or the equivalent feature in Maven, to run just some of the tests. Thi=
s is what I do, I don't wait for Jenkins to run things. 90% of the time if =
it passes the tests that I know could break stuff, it will pass all of Jenk=
ins.
>
> Jenkins should always be doing all the integration tests, so I don't thin=
k it will become *that* much shorter in the long run, though it can certain=
ly be improved.
>
> Matei
>
> On August 8, 2014 at 10:20:35 AM, Nicolas Liochon (nkeywal@gmail.com) wro=
te:
>
> fwiw, when we did this work in HBase, we categorized the tests. Then some
> tests can share a single jvm, while some others need to be isolated in
> their own jvm. Nevertheless surefire can still run them in parallel by
> starting/stopping several jvm.
>
> Nicolas
>
>
> On Fri, Aug 8, 2014 at 7:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> ScalaTest actually has support for parallelization built-in. We can use
>> that.
>>
>> The main challenge is to make sure all the test suites can work in paral=
lel
>> when running along side each other.
>>
>>
>> On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>> > How about using parallel execution feature of maven-surefire-plugin
>> > (assuming all the tests were made parallel friendly) ?
>> >
>> >
>> >
>> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-opt=
ions-and-parallel-execution.html
>> >
>> > Cheers
>> >
>> >
>> > On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:
>> >
>> > > A common approach is to separate unit tests from integration tests.
>> > > Maven has support for this distinction. I'm not sure it helps a lot
>> > > though, since it only helps you to not run integration tests all the
>> > > time. But lots of Spark tests are integration-test-like and are
>> > > important to run to know a change works.
>> > >
>> > > I haven't heard of a plugin to run different test suites remotely on
>> > > many machines, but I would not be surprised if it exists.
>> > >
>> > > The Jenkins servers aren't CPU-bound as far as I can tell. It's that
>> > > the tests spend a lot of time waiting for bits to start up or
>> > > complete. That implies the existing tests could be sped up by just
>> > > running in parallel locally. I recall someone recently proposed this=
?
>> > >
>> > > And I think the problem with that is simply that some of the tests
>> > > collide with each other, by opening up the same port at the same tim=
e
>> > > for example. I know that kind of problem is being attacked even righ=
t
>> > > now. But if all the tests were made parallel friendly, I imagine
>> > > parallelism could be enabled and speed up builds greatly without any
>> > > remote machines.
>> > >
>> > >
>> > > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
>> > > <nicholas.chammas@gmail.com> wrote:
>> > > > Howdy,
>> > > >
>> > > > Do we think it's both feasible and worthwhile to invest in getting
>> our
>> > > unit
>> > > > tests to finish in under 5 minutes (or something similarly brief)
>> when
>> > > run
>> > > > by Jenkins?
>> > > >
>> > > > Unit tests currently seem to take anywhere from 30 min to 2 hours.=
 As
>> > > > people add more tests, I imagine this time will only grow. I think=
 it
>> > > would
>> > > > be better for both contributors and reviewers if they didn't have =
to
>> > wait
>> > > > so long for test results; PR reviews would be shorter, if nothing
>> else.
>> > > >
>> > > > I don't know how how this is normally done, but maybe it wouldn't =
be
>> > too
>> > > > much work to get a test cycle to feel lighter.
>> > > >
>> > > > Most unit tests are independent and can be run concurrently, right=
?
>> > Would
>> > > > it make sense to build a given patch on many servers at once and s=
end
>> > > > disjoint sets of unit tests to each?
>> > > >
>> > > > I'd be interested in working on something like that if possible (a=
nd
>> > > > sensible).
>> > > >
>> > > > Nick
>> > >
>> > > --------------------------------------------------------------------=
-
>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > For additional commands, e-mail: dev-help@spark.apache.org
>> > >
>> > >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 19:00:51 2014
Return-Path: <dev-return-8809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A78810D2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 19:00:51 +0000 (UTC)
Received: (qmail 2909 invoked by uid 500); 8 Aug 2014 19:00:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2839 invoked by uid 500); 8 Aug 2014 19:00:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2827 invoked by uid 99); 8 Aug 2014 19:00:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 19:00:50 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.173 as permitted sender)
Received: from [209.85.192.173] (HELO mail-pd0-f173.google.com) (209.85.192.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 19:00:24 +0000
Received: by mail-pd0-f173.google.com with SMTP id w10so7455744pde.4
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 12:00:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=GuUdGh2eJUHlgIXySEkEY5zNvjvv1EL5l1INBQG6DQ8=;
        b=t6o4NmzSKozHP6drLODOT40XZJxQPsgMIqOLT7xUJxFY5Kb/qDMHmSfFvn1z8s4//q
         mEodZTlew/AbLrvvJEchLn0G3Scw5CS2XxLbi74Cc0go9GmtUZffao5y8jSYnVdnJRfH
         ye11igyNd4TlF1IR7FvUun3VVood27VWzWXao42sxd+i9ZOJt8antJW34WyC1K23U1oM
         b7neUxU2Mtf+ULL2muCNKB5DZCApNgOdSPNaA+xkkWqb3885/MvDmI6mkyRCaNUou7Ks
         I1tYfctzkBKNjKELPHFuOj1BOQYzT2Xated8EI8C7i8AuFP7tqYgVge1jBFb/oiEHZvM
         aoSg==
X-Received: by 10.70.103.4 with SMTP id fs4mr26292186pdb.71.1407524422538;
        Fri, 08 Aug 2014 12:00:22 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id w7sm5494314pds.23.2014.08.08.12.00.21
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Fri, 08 Aug 2014 12:00:22 -0700 (PDT)
Date: Fri, 8 Aug 2014 12:00:20 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>, Patrick Wendell
 <pwendell@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Nicolas Liochon
 <nkeywal@gmail.com>, Ted Yu <yuzhihong@gmail.com>, Reynold Xin
 <rxin@databricks.com>, dev <dev@spark.apache.org>, Sean Owen
 <sowen@cloudera.com>
Message-ID: <etPan.53e51e44.75004e27.a3@joshs-mbp>
In-Reply-To: <CABPQxssgHOaiAinN=nDZfm-ckxMc5NzDhV1SsjAYgxHfQiO5eQ@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
 <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
 <CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
 <etPan.53e515d7.7c83e458.119a7@mbp-3>
 <CABPQxssgHOaiAinN=nDZfm-ckxMc5NzDhV1SsjAYgxHfQiO5eQ@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53e51e44_7941cae5_a3"
X-Virus-Checked: Checked by ClamAV on apache.org

--53e51e44_7941cae5_a3
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

One simple optimization might be to disable the application web UI in tes=
ts that don=E2=80=99t need it. =C2=A0When running tests on my local machi=
ne while also running another Spark shell, I=E2=80=99ve noticed that the =
test logs fill up with errors when the web UI attempts to bind to the def=
ault port, fails, and tries a higher one.

- Josh
On August 8, 2014 at 11:54:24 AM, Patrick Wendell (pwendell=40gmail.com) =
wrote:

I dug around this a bit a while ago, I think if someone sat down and =20
profiled the tests it's likely we could find some things to optimize. =20
In particular, there may be overheads in starting up a local spark =20
context that could be minimized and speed up all the tests. Also, =20
there are some tests (especially in Streaming) that take really long, =20
like 60 seconds for a single test (see some of the new flume tests). =20
These could almost certainly be optimized. =20

I think 5 minutes might be out of reach, but something like a 2X =20
improvement might be possible and would be very valuable if =20
accomplished. =20

- Patrick =20

On =46ri, Aug 8, 2014 at 11:24 AM, Matei Zaharia <matei.zaharia=40gmail.c=
om> wrote: =20
> Just as a note, when you're developing stuff, you can use =22test-only=22=
 in sbt, or the equivalent feature in Maven, to run just some of the test=
s. This is what I do, I don't wait for Jenkins to run things. 90% of the =
time if it passes the tests that I know could break stuff, it will pass a=
ll of Jenkins. =20
> =20
> Jenkins should always be doing all the integration tests, so I don't th=
ink it will become *that* much shorter in the long run, though it can cer=
tainly be improved. =20
> =20
> Matei =20
> =20
> On August 8, 2014 at 10:20:35 AM, Nicolas Liochon (nkeywal=40gmail.com)=
 wrote: =20
> =20
> fwiw, when we did this work in HBase, we categorized the tests. Then so=
me =20
> tests can share a single jvm, while some others need to be isolated in =
=20
> their own jvm. Nevertheless surefire can still run them in parallel by =
=20
> starting/stopping several jvm. =20
> =20
> Nicolas =20
> =20
> =20
> On =46ri, Aug 8, 2014 at 7:10 PM, Reynold Xin <rxin=40databricks.com> w=
rote: =20
> =20
>> ScalaTest actually has support for parallelization built-in. We can us=
e =20
>> that. =20
>> =20
>> The main challenge is to make sure all the test suites can work in par=
allel =20
>> when running along side each other. =20
>> =20
>> =20
>> On =46ri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong=40gmail.com> wrote=
: =20
>> =20
>> > How about using parallel execution feature of maven-surefire-plugin =
=20
>> > (assuming all the tests were made parallel friendly) =3F =20
>> > =20
>> > =20
>> > =20
>> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-o=
ptions-and-parallel-execution.html =20
>> > =20
>> > Cheers =20
>> > =20
>> > =20
>> > On =46ri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen=40cloudera.com> w=
rote: =20
>> > =20
>> > > A common approach is to separate unit tests from integration tests=
. =20
>> > > Maven has support for this distinction. I'm not sure it helps a lo=
t =20
>> > > though, since it only helps you to not run integration tests all t=
he =20
>> > > time. But lots of Spark tests are integration-test-like and are =20
>> > > important to run to know a change works. =20
>> > > =20
>> > > I haven't heard of a plugin to run different test suites remotely =
on =20
>> > > many machines, but I would not be surprised if it exists. =20
>> > > =20
>> > > The Jenkins servers aren't CPU-bound as far as I can tell. It's th=
at =20
>> > > the tests spend a lot of time waiting for bits to start up or =20
>> > > complete. That implies the existing tests could be sped up by just=
 =20
>> > > running in parallel locally. I recall someone recently proposed th=
is=3F =20
>> > > =20
>> > > And I think the problem with that is simply that some of the tests=
 =20
>> > > collide with each other, by opening up the same port at the same t=
ime =20
>> > > for example. I know that kind of problem is being attacked even ri=
ght =20
>> > > now. But if all the tests were made parallel friendly, I imagine =20
>> > > parallelism could be enabled and speed up builds greatly without a=
ny =20
>> > > remote machines. =20
>> > > =20
>> > > =20
>> > > On =46ri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas =20
>> > > <nicholas.chammas=40gmail.com> wrote: =20
>> > > > Howdy, =20
>> > > > =20
>> > > > Do we think it's both feasible and worthwhile to invest in getti=
ng =20
>> our =20
>> > > unit =20
>> > > > tests to finish in under 5 minutes (or something similarly brief=
) =20
>> when =20
>> > > run =20
>> > > > by Jenkins=3F =20
>> > > > =20
>> > > > Unit tests currently seem to take anywhere from 30 min to 2 hour=
s. As =20
>> > > > people add more tests, I imagine this time will only grow. I thi=
nk it =20
>> > > would =20
>> > > > be better for both contributors and reviewers if they didn't hav=
e to =20
>> > wait =20
>> > > > so long for test results; PR reviews would be shorter, if nothin=
g =20
>> else. =20
>> > > > =20
>> > > > I don't know how how this is normally done, but maybe it wouldn'=
t be =20
>> > too =20
>> > > > much work to get a test cycle to feel lighter. =20
>> > > > =20
>> > > > Most unit tests are independent and can be run concurrently, rig=
ht=3F =20
>> > Would =20
>> > > > it make sense to build a given patch on many servers at once and=
 send =20
>> > > > disjoint sets of unit tests to each=3F =20
>> > > > =20
>> > > > I'd be interested in working on something like that if possible =
(and =20
>> > > > sensible). =20
>> > > > =20
>> > > > Nick =20
>> > > =20
>> > > ------------------------------------------------------------------=
--- =20
>> > > To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
>> > > =46or additional commands, e-mail: dev-help=40spark.apache.org =20
>> > > =20
>> > > =20
>> > =20
>> =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--53e51e44_7941cae5_a3--


From dev-return-8810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 20:27:25 2014
Return-Path: <dev-return-8810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB80311133
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 20:27:24 +0000 (UTC)
Received: (qmail 64109 invoked by uid 500); 8 Aug 2014 20:27:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64041 invoked by uid 500); 8 Aug 2014 20:27:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64030 invoked by uid 99); 8 Aug 2014 20:27:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:27:23 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:27:19 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XFqkc-0001yc-Ld
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 13:26:58 -0700
Date: Fri, 8 Aug 2014 13:26:58 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407529618660-7774.post@n3.nabble.com>
In-Reply-To: <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
References: <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com> <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com> <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The API change seems not major. I have locally change it and compiled, but
not test yet. The major problem is still how to solve the hive-exec jar
dependency. I am willing to help on this issue. Is it better stick to the
same way as hive-0.12 until hive-exec is cleaned enough to switch back?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7774.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 20:28:52 2014
Return-Path: <dev-return-8811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BB2E11146
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 20:28:52 +0000 (UTC)
Received: (qmail 67495 invoked by uid 500); 8 Aug 2014 20:28:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67453 invoked by uid 500); 8 Aug 2014 20:28:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67438 invoked by uid 99); 8 Aug 2014 20:28:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:28:51 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:28:26 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XFqm1-00021n-5c
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 13:28:25 -0700
Date: Fri, 8 Aug 2014 13:28:25 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407529705166-7775.post@n3.nabble.com>
In-Reply-To: <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com>
References: <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com> <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com> <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I can compile with no error, but my patch also includes other stuff. 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7775.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 20:55:44 2014
Return-Path: <dev-return-8812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E932611284
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 20:55:44 +0000 (UTC)
Received: (qmail 36885 invoked by uid 500); 8 Aug 2014 20:55:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36823 invoked by uid 500); 8 Aug 2014 20:55:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36812 invoked by uid 99); 8 Aug 2014 20:55:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:55:43 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:55:18 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XFrC1-0003UP-BV
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 13:55:17 -0700
Date: Fri, 8 Aug 2014 13:55:17 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407531317333-7776.post@n3.nabble.com>
In-Reply-To: <1407529705166-7775.post@n3.nabble.com>
References: <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com> <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com> <1407529705166-7775.post@n3.nabble.com>
Subject: Re: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Here is the patch. Please ignore the pom.xml related change, which just for
compiling purpose. I need to further work on this one based on Wandou's
previous work.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7776.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 20:57:31 2014
Return-Path: <dev-return-8813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC6A011299
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 20:57:31 +0000 (UTC)
Received: (qmail 41651 invoked by uid 500); 8 Aug 2014 20:57:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41591 invoked by uid 500); 8 Aug 2014 20:57:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41580 invoked by uid 99); 8 Aug 2014 20:57:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:57:29 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 20:57:25 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XFrDk-0003av-VH
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 13:57:04 -0700
Date: Fri, 8 Aug 2014 13:57:04 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407531424931-7777.post@n3.nabble.com>
In-Reply-To: <1407531317333-7776.post@n3.nabble.com>
References: <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com> <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com> <1407529705166-7775.post@n3.nabble.com> <1407531317333-7776.post@n3.nabble.com>
Subject: Re: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry, forget to upload files. I have never posted before :) hive.diff
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n7777/hive.diff>  



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7777.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 21:03:42 2014
Return-Path: <dev-return-8814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC085112F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 21:03:42 +0000 (UTC)
Received: (qmail 55608 invoked by uid 500); 8 Aug 2014 21:03:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55557 invoked by uid 500); 8 Aug 2014 21:03:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55542 invoked by uid 99); 8 Aug 2014 21:03:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:03:41 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:03:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XFrJl-0004yO-0y
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 14:03:17 -0700
Date: Fri, 8 Aug 2014 14:03:17 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407531797021-7778.post@n3.nabble.com>
Subject: spark-shell is broken! (bad option: '--master')
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

git clone https://github.com/apache/spark.git
mvn -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.3.0-cdh5.1.0
-DskipTests clean package

./bin/spark-shell --master local[2]

then i get

Spark assembly has been built with Hive, including Datanucleus jars on
classpath
bad option: '--master'



unbelievable...




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 21:14:56 2014
Return-Path: <dev-return-8815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 681D01136B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 21:14:56 +0000 (UTC)
Received: (qmail 85347 invoked by uid 500); 8 Aug 2014 21:14:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85276 invoked by uid 500); 8 Aug 2014 21:14:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85263 invoked by uid 99); 8 Aug 2014 21:14:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:14:55 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:14:51 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XFrUc-0005TU-Pd
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 14:14:30 -0700
Date: Fri, 8 Aug 2014 14:14:30 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407532470789-7779.post@n3.nabble.com>
In-Reply-To: <1407531797021-7778.post@n3.nabble.com>
References: <1407531797021-7778.post@n3.nabble.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

maybe this commit is the reason?
https://github.com/apache/spark/commit/a6cd31108f0d73ce6823daafe8447677e03cfd13

i fand some discuss in its PR: https://github.com/apache/spark/pull/1801

important is what vanzin said:
https://github.com/apache/spark/pull/1801#issuecomment-51545117

i tried to use this:
bin/spark-class org.apache.spark.deploy.SparkSubmit --class
org.apache.spark.repl.Main spark-shell --master local[2]

also
bad option: '--master'


but this works

bin/spark-class org.apache.spark.deploy.SparkSubmit --master local[2]
--class org.apache.spark.repl.Main spark-shell





and some more try out:

~/tachyon/spark$ bin/spark-class org.apache.spark.deploy.SparkSubmit
--master local[2] --class org.apache.spark.repl.Main spark-shell --foo bar
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
bad option: '--foo'

~/tachyon/spark$ bin/spark-class org.apache.spark.deploy.SparkSubmit
--master local[2] --foo bar --class org.apache.spark.repl.Main spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
Error: Unrecognized option '--foo'.
Run with --help for usage help or --verbose for debug output



god... no test suite for those shell scripts?

such a issue is really awful~




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7779.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 21:27:17 2014
Return-Path: <dev-return-8816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF487113D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 21:27:17 +0000 (UTC)
Received: (qmail 24124 invoked by uid 500); 8 Aug 2014 21:27:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24073 invoked by uid 500); 8 Aug 2014 21:27:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24041 invoked by uid 99); 8 Aug 2014 21:27:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:27:16 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:26:51 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XFrgY-00062b-Fx
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 14:26:50 -0700
Date: Fri, 8 Aug 2014 14:26:50 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407533210487-7780.post@n3.nabble.com>
In-Reply-To: <1407531797021-7778.post@n3.nabble.com>
References: <1407531797021-7778.post@n3.nabble.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

no one use spark-shell in master branch?

i created a PR as follow up commit of SPARK-2678 and PR #1801:

https://github.com/apache/spark/pull/1861



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7780.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 21:52:12 2014
Return-Path: <dev-return-8817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEDE5114AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 21:52:12 +0000 (UTC)
Received: (qmail 88604 invoked by uid 500); 8 Aug 2014 21:52:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88548 invoked by uid 500); 8 Aug 2014 21:52:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88532 invoked by uid 99); 8 Aug 2014 21:52:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:52:10 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 21:52:06 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so6202674qap.9
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 14:51:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=o+bckI6z0fBwqVncWAYDXgdry+xH6wJjiHBdFDKTTn8=;
        b=WOK0A83rfGx95MQBKZb8EFcL2t7BsLExTA++SOLU6WrEMw4mZYzAlUeb4UUvS2UeCb
         7hWAmvr6xIf+c/q7pfPp2y/iwZWRL5btTSkwXIaknMgIxJgCNKwnyWtwOVpicR3rWQSm
         lJksC1nkvs+ehBE9WdsfLJWLVuFltIdea5gX12vhAeUy75rY8L3cNaJDJdSmMoEKPVRN
         uohb+pB5gS+DNy15PDYOWAwoFQDhugfAHpmOcadE1J79VYFzBRqgM3vIPOcqKaMYLiws
         3EXwRpRCn7lYMmWPrktu9IdJ5sDCSZMZnJ26wD0/CkNAeoFgcsP5CbqjXa4Z6L8KBqmO
         2lKA==
X-Gm-Message-State: ALoCoQlXFZXcqp8kUvQmTfDOw04IBaXFw5ckdoVmMRsIS/NlFQ2bA3SwWXrWKIHg5Sj4wGG5FCp2
X-Received: by 10.140.81.134 with SMTP id f6mr26887330qgd.60.1407534704755;
 Fri, 08 Aug 2014 14:51:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.1.9 with HTTP; Fri, 8 Aug 2014 14:51:24 -0700 (PDT)
In-Reply-To: <1407531424931-7777.post@n3.nabble.com>
References: <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
 <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
 <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
 <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
 <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
 <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
 <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com>
 <1407529705166-7775.post@n3.nabble.com> <1407531317333-7776.post@n3.nabble.com>
 <1407531424931-7777.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 8 Aug 2014 14:51:24 -0700
Message-ID: <CAAswR-6FnrMJtoEiAfOQxLhucdPR3xAVEaO98GHs+SWiBhUafw@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: Zhan Zhang <zhazhan@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c12b5a1621200500253698
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12b5a1621200500253698
Content-Type: text/plain; charset=UTF-8

Could you make a PR as described here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


On Fri, Aug 8, 2014 at 1:57 PM, Zhan Zhang <zhazhan@gmail.com> wrote:

> Sorry, forget to upload files. I have never posted before :) hive.diff
> <
> http://apache-spark-developers-list.1001551.n3.nabble.com/file/n7777/hive.diff
> >
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7777.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c12b5a1621200500253698--

From dev-return-8818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 22:44:00 2014
Return-Path: <dev-return-8818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C30DE1160A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 22:44:00 +0000 (UTC)
Received: (qmail 87987 invoked by uid 500); 8 Aug 2014 22:44:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87922 invoked by uid 500); 8 Aug 2014 22:44:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87910 invoked by uid 99); 8 Aug 2014 22:43:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:43:59 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:43:55 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XFsso-0001Cr-W6
	for dev@spark.incubator.apache.org; Fri, 08 Aug 2014 15:43:35 -0700
Date: Fri, 8 Aug 2014 15:43:34 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407537814989-7782.post@n3.nabble.com>
In-Reply-To: <CAAswR-6FnrMJtoEiAfOQxLhucdPR3xAVEaO98GHs+SWiBhUafw@mail.gmail.com>
References: <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com> <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com> <1407529705166-7775.post@n3.nabble.com> <1407531317333-7776.post@n3.nabble.com> <1407531424931-7777.post@n3.nabble.com> <CAAswR-6FnrMJtoEiAfOQxLhucdPR3xAVEaO98GHs+SWiBhUafw@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Attached the diff the PR SPARK-2706. I am currently working on this problem.
If somebody are also working on this, we can share the load.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p7782.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 22:47:27 2014
Return-Path: <dev-return-8819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E6A911620
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 22:47:27 +0000 (UTC)
Received: (qmail 97697 invoked by uid 500); 8 Aug 2014 22:47:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97638 invoked by uid 500); 8 Aug 2014 22:47:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97624 invoked by uid 99); 8 Aug 2014 22:47:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:47:25 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:47:00 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so6229162qaq.10
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 15:46:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=2IDXx46Yzar0vd1z81W/+EuADvwd8Hk2WE2KvtvlCY8=;
        b=avxJliV0F6H4Aet/si40gg5WlpDJLcEz3a4DVlJWqdJIKmeoM/gDMkGsLUPblRdTRY
         JlsF234aWvTyn4mh7skXkPdY6oBi8QLy4MWARqZC7D6nkN5WgdC9vFef2QHCJw6s9g+i
         gd68/preADzKAsDSAWbZZeAazI+j77LCjNIrgJb6n5YoE8jRLP2sJRq5azq1K7/w975O
         S9l8Tq241MoyTmFrub/GdsvZswZkZ79ixrvyw7XlTXHmViCStI/EwBteP6MHzy8OpDfj
         MRMFSv6+nCt3XUKZ687bEhZnXc8l+icO4Rz2R9e+MSNHUdZQ8XA+AA4BSelX8/4QRW4W
         /B+Q==
X-Gm-Message-State: ALoCoQk2xAtOGftuvC8231SbYSbTNorrANItLzrzUV9UOB9i4rlFReYgtIAG4fY6cv5OeaGJNys1
MIME-Version: 1.0
X-Received: by 10.140.88.41 with SMTP id s38mr26831154qgd.73.1407538019639;
 Fri, 08 Aug 2014 15:46:59 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Fri, 8 Aug 2014 15:46:59 -0700 (PDT)
In-Reply-To: <1407533210487-7780.post@n3.nabble.com>
References: <1407531797021-7778.post@n3.nabble.com>
	<1407533210487-7780.post@n3.nabble.com>
Date: Fri, 8 Aug 2014 15:46:59 -0700
Message-ID: <CACBYxKKnU2Din+nkoTXsxa+uiQZEcRVnfzxdYfCj0b6jjVrPsg@mail.gmail.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: chutium <teng.qiu@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c13e7eab524b050025fb80
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13e7eab524b050025fb80
Content-Type: text/plain; charset=UTF-8

Hi Chutium,

This is currently being addressed in
https://github.com/apache/spark/pull/1825

-Sandy


On Fri, Aug 8, 2014 at 2:26 PM, chutium <teng.qiu@gmail.com> wrote:

> no one use spark-shell in master branch?
>
> i created a PR as follow up commit of SPARK-2678 and PR #1801:
>
> https://github.com/apache/spark/pull/1861
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7780.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c13e7eab524b050025fb80--

From dev-return-8820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 22:48:09 2014
Return-Path: <dev-return-8820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6187B11622
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 22:48:09 +0000 (UTC)
Received: (qmail 99203 invoked by uid 500); 8 Aug 2014 22:48:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99145 invoked by uid 500); 8 Aug 2014 22:48:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99133 invoked by uid 99); 8 Aug 2014 22:48:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:48:08 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:48:04 +0000
Received: by mail-oi0-f45.google.com with SMTP id e131so4137910oig.18
        for <dev@spark.apache.org>; Fri, 08 Aug 2014 15:47:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=dD1A6UPBipwGvMZAfF5LNRe+HgEqBRqTjqhfuyBT8Co=;
        b=ObtcuIAMUTdZY/59U9IRz8ti05bm4u9VyqL63B/a9XZ5JVZp7S9TRw7A+uXHoeQ7Up
         W/7LeA4WKGmaXXdiu7KyTLRjowgO7aPWQMA+GPSTkAWvd5B/46aovisqsf9pfCak+zWk
         ax+F03sbxK8b0Wm/KkHFaxM0+BLfBiEh8/kOt0OQI6c6CSsvRBKYVOJuQPC8f5UM+RzV
         x1oPevk3PmiqTP4t6xL2DbjKjPaNm8plwXywGn8hqBp/IUxqdM3DFw5TRFPju9yPgJHz
         t5DbPD1IGrwiKo35desgxIxPN4ECo+ornJaVUxsNN4O46Wuf4kZCSeTlrlLAAkJMstg3
         FP5Q==
MIME-Version: 1.0
X-Received: by 10.60.51.106 with SMTP id j10mr6236921oeo.77.1407538063828;
 Fri, 08 Aug 2014 15:47:43 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 8 Aug 2014 15:47:43 -0700 (PDT)
In-Reply-To: <etPan.53e51e44.75004e27.a3@joshs-mbp>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
	<CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
	<CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
	<CAPcDmStUekdFdkE10eMGbZ8q38BHC2MYpxfUd+Gp1m7=W8FGnw@mail.gmail.com>
	<etPan.53e515d7.7c83e458.119a7@mbp-3>
	<CABPQxssgHOaiAinN=nDZfm-ckxMc5NzDhV1SsjAYgxHfQiO5eQ@mail.gmail.com>
	<etPan.53e51e44.75004e27.a3@joshs-mbp>
Date: Fri, 8 Aug 2014 15:47:43 -0700
Message-ID: <CABPQxstn_opJaZ_qBk6BSaC1JiEpg97oYcLDnwMxYam1EqWMDA@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Patrick Wendell <pwendell@gmail.com>
To: Josh Rosen <rosenville@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Nicolas Liochon <nkeywal@gmail.com>, Ted Yu <yuzhihong@gmail.com>, 
	Reynold Xin <rxin@databricks.com>, dev <dev@spark.apache.org>, 
	Sean Owen <sowen@cloudera.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Josh - that was actually fixed recently (we just bind to a random port
when running tests).

On Fri, Aug 8, 2014 at 12:00 PM, Josh Rosen <rosenville@gmail.com> wrote:
> One simple optimization might be to disable the application web UI in tests
> that don't need it.  When running tests on my local machine while also
> running another Spark shell, I've noticed that the test logs fill up with
> errors when the web UI attempts to bind to the default port, fails, and
> tries a higher one.
>
> - Josh
>
> On August 8, 2014 at 11:54:24 AM, Patrick Wendell (pwendell@gmail.com)
> wrote:
>
> I dug around this a bit a while ago, I think if someone sat down and
> profiled the tests it's likely we could find some things to optimize.
> In particular, there may be overheads in starting up a local spark
> context that could be minimized and speed up all the tests. Also,
> there are some tests (especially in Streaming) that take really long,
> like 60 seconds for a single test (see some of the new flume tests).
> These could almost certainly be optimized.
>
> I think 5 minutes might be out of reach, but something like a 2X
> improvement might be possible and would be very valuable if
> accomplished.
>
> - Patrick
>
> On Fri, Aug 8, 2014 at 11:24 AM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>> Just as a note, when you're developing stuff, you can use "test-only" in
>> sbt, or the equivalent feature in Maven, to run just some of the tests. This
>> is what I do, I don't wait for Jenkins to run things. 90% of the time if it
>> passes the tests that I know could break stuff, it will pass all of Jenkins.
>>
>> Jenkins should always be doing all the integration tests, so I don't think
>> it will become *that* much shorter in the long run, though it can certainly
>> be improved.
>>
>> Matei
>>
>> On August 8, 2014 at 10:20:35 AM, Nicolas Liochon (nkeywal@gmail.com)
>> wrote:
>>
>> fwiw, when we did this work in HBase, we categorized the tests. Then some
>> tests can share a single jvm, while some others need to be isolated in
>> their own jvm. Nevertheless surefire can still run them in parallel by
>> starting/stopping several jvm.
>>
>> Nicolas
>>
>>
>> On Fri, Aug 8, 2014 at 7:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> ScalaTest actually has support for parallelization built-in. We can use
>>> that.
>>>
>>> The main challenge is to make sure all the test suites can work in
>>> parallel
>>> when running along side each other.
>>>
>>>
>>> On Fri, Aug 8, 2014 at 9:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>>>
>>> > How about using parallel execution feature of maven-surefire-plugin
>>> > (assuming all the tests were made parallel friendly) ?
>>> >
>>> >
>>> >
>>>
>>> http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html
>>> >
>>> > Cheers
>>> >
>>> >
>>> > On Fri, Aug 8, 2014 at 9:14 AM, Sean Owen <sowen@cloudera.com> wrote:
>>> >
>>> > > A common approach is to separate unit tests from integration tests.
>>> > > Maven has support for this distinction. I'm not sure it helps a lot
>>> > > though, since it only helps you to not run integration tests all the
>>> > > time. But lots of Spark tests are integration-test-like and are
>>> > > important to run to know a change works.
>>> > >
>>> > > I haven't heard of a plugin to run different test suites remotely on
>>> > > many machines, but I would not be surprised if it exists.
>>> > >
>>> > > The Jenkins servers aren't CPU-bound as far as I can tell. It's that
>>> > > the tests spend a lot of time waiting for bits to start up or
>>> > > complete. That implies the existing tests could be sped up by just
>>> > > running in parallel locally. I recall someone recently proposed this?
>>> > >
>>> > > And I think the problem with that is simply that some of the tests
>>> > > collide with each other, by opening up the same port at the same time
>>> > > for example. I know that kind of problem is being attacked even right
>>> > > now. But if all the tests were made parallel friendly, I imagine
>>> > > parallelism could be enabled and speed up builds greatly without any
>>> > > remote machines.
>>> > >
>>> > >
>>> > > On Fri, Aug 8, 2014 at 5:01 PM, Nicholas Chammas
>>> > > <nicholas.chammas@gmail.com> wrote:
>>> > > > Howdy,
>>> > > >
>>> > > > Do we think it's both feasible and worthwhile to invest in getting
>>> our
>>> > > unit
>>> > > > tests to finish in under 5 minutes (or something similarly brief)
>>> when
>>> > > run
>>> > > > by Jenkins?
>>> > > >
>>> > > > Unit tests currently seem to take anywhere from 30 min to 2 hours.
>>> > > > As
>>> > > > people add more tests, I imagine this time will only grow. I think
>>> > > > it
>>> > > would
>>> > > > be better for both contributors and reviewers if they didn't have
>>> > > > to
>>> > wait
>>> > > > so long for test results; PR reviews would be shorter, if nothing
>>> else.
>>> > > >
>>> > > > I don't know how how this is normally done, but maybe it wouldn't
>>> > > > be
>>> > too
>>> > > > much work to get a test cycle to feel lighter.
>>> > > >
>>> > > > Most unit tests are independent and can be run concurrently, right?
>>> > Would
>>> > > > it make sense to build a given patch on many servers at once and
>>> > > > send
>>> > > > disjoint sets of unit tests to each?
>>> > > >
>>> > > > I'd be interested in working on something like that if possible
>>> > > > (and
>>> > > > sensible).
>>> > > >
>>> > > > Nick
>>> > >
>>> > > ---------------------------------------------------------------------
>>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > > For additional commands, e-mail: dev-help@spark.apache.org
>>> > >
>>> > >
>>> >
>>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug  8 22:48:59 2014
Return-Path: <dev-return-8821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8C63611629
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Aug 2014 22:48:59 +0000 (UTC)
Received: (qmail 1408 invoked by uid 500); 8 Aug 2014 22:48:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1342 invoked by uid 500); 8 Aug 2014 22:48:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1323 invoked by uid 99); 8 Aug 2014 22:48:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:48:58 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Aug 2014 22:48:33 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so4491046obc.19
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 15:48:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=GDEv69aAQZz4Jw5mSmnjiezqYcLemkx/ER1cXN903Uo=;
        b=lrf+Vyb0uncuWDpgUqluMib3F9Pe4m+bQBUG6OhueuCzEmUKuiFAnonbFMzEv2rcc7
         X76vyde/ATMd6OygsnKEXUxT+Xts1ZEn7nCDVHEFroMeoWfY+At5QeB34L/smerxSKkV
         2uUKmrVaIO1x3cZNeGkxblx2nBU/3X23kb9U8Tp0r123QfMRGQNUUUrPDbm1UI018agV
         H9yELrzUtKD4SqkPwxGRv12kn929E+e5DtGB0XbJEpljrD/xiGLM4yqIqymMtplG1bBS
         PSDscGbSvpemPsh6wl5YfXkxt54j7XxHkOW2sqlVqWUXElN6lHv4+QHkYK9dMEv2NST+
         ftgQ==
MIME-Version: 1.0
X-Received: by 10.182.137.195 with SMTP id qk3mr13783780obb.5.1407538112500;
 Fri, 08 Aug 2014 15:48:32 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 8 Aug 2014 15:48:32 -0700 (PDT)
In-Reply-To: <CACBYxKKnU2Din+nkoTXsxa+uiQZEcRVnfzxdYfCj0b6jjVrPsg@mail.gmail.com>
References: <1407531797021-7778.post@n3.nabble.com>
	<1407533210487-7780.post@n3.nabble.com>
	<CACBYxKKnU2Din+nkoTXsxa+uiQZEcRVnfzxdYfCj0b6jjVrPsg@mail.gmail.com>
Date: Fri, 8 Aug 2014 15:48:32 -0700
Message-ID: <CABPQxsspFAbj2RbhMFdZ-dad13caY2npKbQwCUt3uu11337CLA@mail.gmail.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
From: Patrick Wendell <pwendell@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: chutium <teng.qiu@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Cheng Lian also has a fix for this. I've asked him to make a PR - he
is on China time so it probably won't come until tonight:

https://github.com/liancheng/spark/compare/apache:master...liancheng:spark-2894

On Fri, Aug 8, 2014 at 3:46 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> Hi Chutium,
>
> This is currently being addressed in
> https://github.com/apache/spark/pull/1825
>
> -Sandy
>
>
> On Fri, Aug 8, 2014 at 2:26 PM, chutium <teng.qiu@gmail.com> wrote:
>
>> no one use spark-shell in master branch?
>>
>> i created a PR as follow up commit of SPARK-2678 and PR #1801:
>>
>> https://github.com/apache/spark/pull/1861
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7780.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 01:15:39 2014
Return-Path: <dev-return-8822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D34611A28
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 01:15:39 +0000 (UTC)
Received: (qmail 63689 invoked by uid 500); 9 Aug 2014 01:15:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63620 invoked by uid 500); 9 Aug 2014 01:15:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63608 invoked by uid 99); 9 Aug 2014 01:15:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 01:15:37 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 01:15:33 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so6964725qgd.9
        for <dev@spark.incubator.apache.org>; Fri, 08 Aug 2014 18:15:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=5N/H3k3KylJW4KWJRtHs6cNvPr4+FyGSZPTFoJ0t4XU=;
        b=Xz5/hqokxiXxqwwpNGLzsuGDh+j6s51MGzWaMPXJsnWgqiBMvjDKJcica4djq0jC1u
         yTkGhlINL7eXcdzns3oYDEELo36UqocDPkC4xTLXW1PD6s9mMBji84l9vOPvVq3Jsiv0
         qjLiIN4/Hg1ceieVTV5u5E/0Lva4ptlesF9zA6PDxBaRB5hKXi3S+SuXMw5evxM4aowO
         owpaQsOUqY5bChI0VnGVgCoC2fB/AxagwDIDH+9P4Ex8BDjzrCaQ102Zqs6KQ1Ha4paI
         V5jxSWQhNoeD/L8Ntajv6ncqCVUY9ekPTWbtO661Sm3RFh7rlJRGUJmfGosyPIIg9Xo4
         UJcA==
X-Received: by 10.229.103.70 with SMTP id j6mr42353156qco.27.1407546913047;
 Fri, 08 Aug 2014 18:15:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Fri, 8 Aug 2014 18:14:52 -0700 (PDT)
In-Reply-To: <CABPQxsspFAbj2RbhMFdZ-dad13caY2npKbQwCUt3uu11337CLA@mail.gmail.com>
References: <1407531797021-7778.post@n3.nabble.com> <1407533210487-7780.post@n3.nabble.com>
 <CACBYxKKnU2Din+nkoTXsxa+uiQZEcRVnfzxdYfCj0b6jjVrPsg@mail.gmail.com> <CABPQxsspFAbj2RbhMFdZ-dad13caY2npKbQwCUt3uu11337CLA@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Sat, 9 Aug 2014 09:14:52 +0800
Message-ID: <CAA_qdLorps_UMCre56ra60=rAzmpS+ChwkRcph=rZyJZwy8YRA@mail.gmail.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sandy Ryza <sandy.ryza@cloudera.com>, chutium <teng.qiu@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11330ac2c1c9060500280de5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11330ac2c1c9060500280de5
Content-Type: text/plain; charset=UTF-8

Just opened a PR based on the branch Patrick mentioned for this issue
https://github.com/apache/spark/pull/1864


On Sat, Aug 9, 2014 at 6:48 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Cheng Lian also has a fix for this. I've asked him to make a PR - he
> is on China time so it probably won't come until tonight:
>
>
> https://github.com/liancheng/spark/compare/apache:master...liancheng:spark-2894
>
> On Fri, Aug 8, 2014 at 3:46 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > Hi Chutium,
> >
> > This is currently being addressed in
> > https://github.com/apache/spark/pull/1825
> >
> > -Sandy
> >
> >
> > On Fri, Aug 8, 2014 at 2:26 PM, chutium <teng.qiu@gmail.com> wrote:
> >
> >> no one use spark-shell in master branch?
> >>
> >> i created a PR as follow up commit of SPARK-2678 and PR #1801:
> >>
> >> https://github.com/apache/spark/pull/1861
> >>
> >>
> >>
> >> --
> >> View this message in context:
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7780.html
> >> Sent from the Apache Spark Developers List mailing list archive at
> >> Nabble.com.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11330ac2c1c9060500280de5--

From dev-return-8823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 07:43:41 2014
Return-Path: <dev-return-8823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58D7AC06C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 07:43:41 +0000 (UTC)
Received: (qmail 37564 invoked by uid 500); 9 Aug 2014 07:43:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37505 invoked by uid 500); 9 Aug 2014 07:43:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37493 invoked by uid 99); 9 Aug 2014 07:43:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 07:43:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.50] (HELO mail-pa0-f50.google.com) (209.85.220.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 07:43:36 +0000
Received: by mail-pa0-f50.google.com with SMTP id et14so8341426pad.23
        for <dev@spark.incubator.apache.org>; Sat, 09 Aug 2014 00:43:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=9aaIHx8h1opXIwAIWwLdbtp6G+Za03Ye7iL1iH8QJ8s=;
        b=Ui57AnypF1PjRhcLyapSAOrs0CIm5mFJjke+3XWCezPjmeaGnMSyhEGUhl5Cu7jaYD
         eDRdHyJFvkmd9twjYpk4shf1HJzj6n8XziHyov1F6NxkqD7WFrPhs2lAVqJhP/zsifHe
         3qli5oPqBaHvJWfFBiF8n+JLRmjvBMHZ87YY5TaaBhrU2RQV1zhJfwgiN4cLcFfAbnWw
         l+uQE3UyN2LmPWFnmCvJoDRrstKhyUC2ZFQrMMKRk/poQg+GdCDX8X+yUVQlICctkr+4
         oXy/FMRL3ctHrlholfGPToNRD3RBNiFawQj6dT85kOM/kH6t9GoIdBH2ou8DxvmdmhVb
         5Cbg==
X-Gm-Message-State: ALoCoQl5mgwVqOPDsAkBdCe5vkKjk0RvEgxwRMELTPmEdhQJrsVwRxru1TeuaqqhItl543UkjTNl
MIME-Version: 1.0
X-Received: by 10.66.141.76 with SMTP id rm12mr37176pab.138.1407570195712;
 Sat, 09 Aug 2014 00:43:15 -0700 (PDT)
Received: by 10.70.4.133 with HTTP; Sat, 9 Aug 2014 00:43:15 -0700 (PDT)
In-Reply-To: <CAOYDGoArRHhgpx3wZzhzZU--dB5zBnwvbpobQopn4vDW0nTreQ@mail.gmail.com>
References: <etPan.53e458bb.515f007c.119a7@mbp-3.local>
	<CAJcWD7a9vsTy9w9YSEVB+UJnshdr-TYmOPcQW7h5vPaVTq7XyQ@mail.gmail.com>
	<CAGh_TuMkcvw=Rb94jLYeSh-pcGDZ-cEi2FBRMbwPbagfTObQ4A@mail.gmail.com>
	<CAJgQjQ-=-Sty97TFNisjue38bYo8aVVcfxC3Jrf_NNvyHsUD4g@mail.gmail.com>
	<CAOYDGoArRHhgpx3wZzhzZU--dB5zBnwvbpobQopn4vDW0nTreQ@mail.gmail.com>
Date: Sat, 9 Aug 2014 00:43:15 -0700
Message-ID: <CAMJOb8==AQhrr1LAJ0yOPTOzeDP_gizpA7OkX2BTtsHOSztQug@mail.gmail.com>
Subject: Re: Welcoming two new committers
From: Andrew Or <andrew@databricks.com>
To: Prashant Sharma <scrapcodes@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Christopher Nguyen <ctn@adatao.com>, 
	Joseph Gonzalez <jegonzal@eecs.berkeley.edu>, Matei Zaharia <matei@databricks.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11332ebe83217005002d79f1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11332ebe83217005002d79f1
Content-Type: text/plain; charset=UTF-8

Thanks everyone. I look forward to continuing to work with all of you!


2014-08-08 3:23 GMT-07:00 Prashant Sharma <scrapcodes@gmail.com>:

> Congratulations Andrew and Joey.
>
> Prashant Sharma
>
>
>
>
> On Fri, Aug 8, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> Congrats, Joey & Andrew!!
>>
>> -Xiangrui
>>
>> On Fri, Aug 8, 2014 at 12:14 AM, Christopher Nguyen <ctn@adatao.com>
>> wrote:
>> > +1 Joey & Andrew :)
>> >
>> > --
>> > Christopher T. Nguyen
>> > Co-founder & CEO, Adatao <http://adatao.com> [ah-'DAY-tao]
>> > linkedin.com/in/ctnguyen
>> >
>> >
>> >
>> > On Thu, Aug 7, 2014 at 10:39 PM, Joseph Gonzalez <
>> jegonzal@eecs.berkeley.edu
>> >> wrote:
>> >
>> >> Hi Everyone,
>> >>
>> >> Thank you for inviting me to be a committer.  I look forward to working
>> >> with everyone to ensure the continued success of the Spark project.
>> >>
>> >> Thanks!
>> >> Joey
>> >>
>> >>
>> >>
>> >>
>> >> On Thu, Aug 7, 2014 at 9:57 PM, Matei Zaharia <matei@databricks.com>
>> >> wrote:
>> >>
>> >> > Hi everyone,
>> >> >
>> >> > The PMC recently voted to add two new committers and PMC members:
>> Joey
>> >> > Gonzalez and Andrew Or. Both have been huge contributors in the past
>> year
>> >> > -- Joey on much of GraphX as well as quite a bit of the initial work
>> in
>> >> > MLlib, and Andrew on Spark Core. Join me in welcoming them as
>> committers!
>> >> >
>> >> > Matei
>> >> >
>> >> >
>> >> >
>> >> >
>> >>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a11332ebe83217005002d79f1--

From dev-return-8824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 07:47:09 2014
Return-Path: <dev-return-8824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72A49C082
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 07:47:09 +0000 (UTC)
Received: (qmail 40888 invoked by uid 500); 9 Aug 2014 07:47:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40827 invoked by uid 500); 9 Aug 2014 07:47:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40814 invoked by uid 99); 9 Aug 2014 07:47:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 07:47:08 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 07:46:42 +0000
Received: by mail-we0-f171.google.com with SMTP id p10so6494785wes.30
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 00:46:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7HkNmyiXp5ENR0g4YAnki0XlVhzXT1TchfNml2KQtGg=;
        b=vXSxkcmIu0Bi46/q38CzvUe+yC3CHRZM9yROwM42/1ARcmv92FSkIFbY0gtpl2ZUli
         s1L8T4cJZ5v4K3IYYT7+wtjOtU+cvrmtZCi08px0wvywqciRgk75V7cvECVtiuEyg+SU
         N7QFftcwrCaBUmSIJaJW4Ofa4xvmaCZjXFa5m2T9LcqdBiHWd4+orp4d9Umaoc6kq7yX
         x1FJIFm7cp+bYJQHWiYdShxcashSRpzHSiAVrBDO3L1ruYJWQ87Qz0gX4SDajxlYNKgV
         mYqFi8wVqVe/Y9lonQQAD2sqjOdYFjLGf9sQOYZviCBU69wtbQiEifFbrP7pMi2h+8as
         v5PA==
MIME-Version: 1.0
X-Received: by 10.194.87.134 with SMTP id ay6mr36456401wjb.84.1407570401774;
 Sat, 09 Aug 2014 00:46:41 -0700 (PDT)
Received: by 10.194.162.103 with HTTP; Sat, 9 Aug 2014 00:46:41 -0700 (PDT)
In-Reply-To: <CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
Date: Sat, 9 Aug 2014 00:46:41 -0700
Message-ID: <CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0102ee04cb54d305002d853b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0102ee04cb54d305002d853b
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

Based on your suggestion I moved core and mllib both to 1.1.0-SNAPSHOT...I
am still getting class cast exception:

Exception in thread "main" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 249 in stage 52.0 failed 4 times, most recent
failure: Lost task 249.3 in stage 52.0 (TID 10002,
tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2

I am running ALS.scala merged with my changes. I will try the mllib jar
without my changes next...

Can this be due to the fact that my jars are compiled with Java 1.7_55 but
the cluster JRE is at 1.7_45.

Thanks.

Deb




On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> I did not play with Hadoop settings...everything is compiled with
> 2.3.0CDH5.0.2 for me...
>
> I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98
> but there was no profile for CDH in the pom...but that's unrelated to this !
>
>
> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>
>> One related question, is mllib jar independent from hadoop version
>> (doesnt use hadoop api directly)? Can I use mllib jar compile for one
>> version of hadoop and use it in another version of hadoop?
>>
>> Sent from my Google Nexus 5
>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com> wrote:
>>
>>> Hi Xiangrui,
>>>
>>> Maintaining another file will be a pain later so I deployed spark 1.0.1
>>> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
>>> along with the code changes for quadratic optimization...
>>>
>>> Later the plan is to patch the snapshot mllib with the deployed stable
>>> mllib...
>>>
>>> There are 5 variants that I am experimenting with around 400M ratings
>>> (daily data, monthly data I will update in few days)...
>>>
>>> 1. LS
>>> 2. NNLS
>>> 3. Quadratic with bounds
>>> 4. Quadratic with L1
>>> 5. Quadratic with equality and positivity
>>>
>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
>>> ALS.scala:311
>>>
>>> // Materialize usersOut and productsOut.
>>> usersOut.count()
>>>
>>> I am getting from one of the executors: java.lang.ClassCastException:
>>> scala.Tuple1 cannot be cast to scala.Product2
>>>
>>> I am debugging it further but I was wondering if this is due to RDD
>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>>>
>>> I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
>>> cluster has Java 1.7.0_45.
>>>
>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that
>>> Java
>>> version mismatch cause this ?
>>>
>>> Stack traces are below
>>>
>>> Thanks.
>>> Deb
>>>
>>>
>>> Executor stacktrace:
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>>>
>>>
>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>
>>>
>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>
>>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>
>>>
>>>
>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>>
>>>
>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>
>>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>>
>>>
>>>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>>
>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>>
>>>
>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>>>
>>>
>>>
>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>>
>>>
>>>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>>
>>>         java.lang.Thread.run(Thread.java:744)
>>>
>>> Driver stacktrace:
>>>
>>> at org.apache.spark.scheduler.DAGScheduler.org
>>>
>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>>
>>> at
>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>
>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>
>>> at scala.Option.foreach(Option.scala:236)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>>
>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>>
>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>>
>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>>
>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>>
>>> at
>>>
>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>>
>>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>
>>> at
>>>
>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>
>>> at
>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>
>>>  at
>>>
>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>
>>>
>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>>
>>> > Hi Xiangrui,
>>> >
>>> > I used your idea and kept a cherry picked version of ALS.scala in my
>>> > application and call it ALSQp.scala...this is a OK workaround for now
>>> till
>>> > a version adds up to master for example...
>>> >
>>> > For the bug with userClassPathFirst, looks like Koert already found
>>> this
>>> > issue in the following JIRA:
>>> >
>>> > https://issues.apache.org/jira/browse/SPARK-1863
>>> >
>>> > By the way the userClassPathFirst feature is very useful since I am
>>> sure
>>> > the deployed version of spark on a production cluster will always be
>>> the
>>> > last stable (core at 1.0.1 in my case) and people would like to deploy
>>> > SNAPSHOT versions of libraries that build on top of spark core (mllib,
>>> > streaming etc)...
>>> >
>>> > Another way is to have a build option that deploys only the core and
>>> not
>>> > the libraries built upon core...
>>> >
>>> > Do we have an option like that in make-distribution script ?
>>> >
>>> > Thanks.
>>> > Deb
>>> >
>>> >
>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
>>> wrote:
>>> >
>>> >> If you cannot change the Spark jar deployed on the cluster, an easy
>>> >> solution would be renaming ALS in your jar. If userClassPathFirst
>>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
>>> >> -Xiangrui
>>> >>
>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
>>> debasish.das83@gmail.com>
>>> >> wrote:
>>> >> > I created the assembly file but still it wants to pick the mllib
>>> from
>>> >> the
>>> >> > cluster:
>>> >> >
>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
>>> >> > QuadraticMinimizer
>>> >> >
>>> >> > org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>>> >> >
>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>>> >> outputPath
>>> >> >
>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>>> >> >
>>> >>
>>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>>> >> >
>>> >> > Now if I force it to use the jar that I gave using
>>> >> > spark.files.userClassPathFirst, then it fails on some serialization
>>> >> > issues...
>>> >> >
>>> >> > A simple solution is to cherry pick the files I need from spark
>>> branch
>>> >> to
>>> >> > the application branch but I am not sure that's the right thing to
>>> do...
>>> >> >
>>> >> > The way userClassPathFirst is behaving, there might be bugs in it...
>>> >> >
>>> >> > Any suggestions will be appreciated....
>>> >> >
>>> >> > Thanks.
>>> >> > Deb
>>> >> >
>>> >> >
>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
>>> >> wrote:
>>> >> >>
>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>>> >> >> spark-core-1.0.1.
>>> >> >>
>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>>> >> debasish.das83@gmail.com>
>>> >> >> wrote:
>>> >> >> > Let me try it...
>>> >> >> >
>>> >> >> > Will this be fixed if I generate a assembly file with mllib-1.1.0
>>> >> >> > SNAPSHOT
>>> >> >> > jar and other dependencies with the rest of the application code
>>> ?
>>> >> >> >
>>> >> >> >
>>> >> >> >
>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <mengxr@gmail.com
>>> >
>>> >> wrote:
>>> >> >> >>
>>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm
>>> not
>>> >> >> >> sure whether it could solve your problem. -Xiangrui
>>> >> >> >>
>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>>> >> >> >> <debasish.das83@gmail.com>
>>> >> >> >> wrote:
>>> >> >> >> > Hi,
>>> >> >> >> >
>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have
>>> new
>>> >> code
>>> >> >> >> > that
>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>>> >> >> >> >
>>> >> >> >> > I am trying to access the new code using spark-submit as
>>> follows:
>>> >> >> >> >
>>> >> >> >> > spark-job --class
>>> com.verizon.bda.mllib.recommendation.ALSDriver
>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>>> --lambda
>>> >> 1.0
>>> >> >> >> > --qpProblem 2 inputPath outputPath
>>> >> >> >> >
>>> >> >> >> > I can see the jars are getting added to httpServer as
>>> expected:
>>> >> >> >> >
>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> >> >
>>> >> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>>> >> >> >> >
>>> >> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>>> >> >> >> > with
>>> >> >> >> > timestamp 1406998204236
>>> >> >> >> >
>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>>> >> timestamp
>>> >> >> >> > 1406998204237
>>> >> >> >> >
>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>>> >> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>>> with
>>> >> >> >> > timestamp
>>> >> >> >> > 1406998204238
>>> >> >> >> >
>>> >> >> >> > But the job still can't access code form mllib-1.1.0
>>> >> SNAPSHOT.jar...I
>>> >> >> >> > think
>>> >> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
>>> >> >> >> >
>>> >> >> >> > Please help. I will ask for a PR tomorrow but internally we
>>> want
>>> >> to
>>> >> >> >> > generate results from the new code.
>>> >> >> >> >
>>> >> >> >> > Thanks.
>>> >> >> >> >
>>> >> >> >> > Deb
>>> >> >> >
>>> >> >> >
>>> >> >
>>> >> >
>>> >>
>>> >
>>> >
>>>
>>
>

--089e0102ee04cb54d305002d853b--

From dev-return-8825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 09:01:13 2014
Return-Path: <dev-return-8825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 229B6C249
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 09:01:13 +0000 (UTC)
Received: (qmail 89713 invoked by uid 500); 9 Aug 2014 09:01:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89651 invoked by uid 500); 9 Aug 2014 09:01:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89633 invoked by uid 99); 9 Aug 2014 09:01:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 09:01:12 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gdmeda@outlook.com designates 65.55.111.165 as permitted sender)
Received: from [65.55.111.165] (HELO BLU004-OMC4S26.hotmail.com) (65.55.111.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 09:00:46 +0000
Received: from BLU406-EAS222 ([65.55.111.135]) by BLU004-OMC4S26.hotmail.com with Microsoft SMTPSVC(7.5.7601.22712);
	 Sat, 9 Aug 2014 02:00:45 -0700
X-TMN: [mN4lDSlGp8gPzo+ZYipVN0aEcd6X92/S]
X-Originating-Email: [gdmeda@outlook.com]
Message-ID: <BLU406-EAS222E0C1A4A1047DFB6E8CA7D2EF0@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_08a37293-c094-4e29-a150-b0496a067c01_"
MIME-Version: 1.0
To: Andrew Or <andrew@databricks.com>, Prashant Sharma <scrapcodes@gmail.com>
CC: Xiangrui Meng <mengxr@gmail.com>, Christopher Nguyen <ctn@adatao.com>,
	Joseph Gonzalez <jegonzal@eecs.berkeley.edu>, Matei Zaharia
	<matei@databricks.com>, "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>
From: Guru Medasani <gdmeda@outlook.com>
Subject: RE: Welcoming two new committers
Date: Sat, 9 Aug 2014 04:00:43 -0500
X-OriginalArrivalTime: 09 Aug 2014 09:00:45.0277 (UTC) FILETIME=[67B820D0:01CFB3B0]
X-Virus-Checked: Checked by ClamAV on apache.org

--_08a37293-c094-4e29-a150-b0496a067c01_
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain; charset="utf-8"

Congrats Joey and Andrew!

Sent from my Windows Phone
________________________________
From: Andrew Or<mailto:andrew@databricks.com>
Sent: =E2=80=8E8/=E2=80=8E9/=E2=80=8E2014 2:43 AM
To: Prashant Sharma<mailto:scrapcodes@gmail.com>
Cc: Xiangrui Meng<mailto:mengxr@gmail.com>=3B Christopher Nguyen<mailto:ctn=
@adatao.com>=3B Joseph Gonzalez<mailto:jegonzal@eecs.berkeley.edu>=3B Matei=
 Zaharia<mailto:matei@databricks.com>=3B dev@spark.incubator.apache.org<mai=
lto:dev@spark.incubator.apache.org>
Subject: Re: Welcoming two new committers

Thanks everyone. I look forward to continuing to work with all of you!


2014-08-08 3:23 GMT-07:00 Prashant Sharma <scrapcodes@gmail.com>:

> Congratulations Andrew and Joey.
>
> Prashant Sharma
>
>
>
>
> On Fri=2C Aug 8=2C 2014 at 2:10 PM=2C Xiangrui Meng <mengxr@gmail.com> wr=
ote:
>
>> Congrats=2C Joey & Andrew!!
>>
>> -Xiangrui
>>
>> On Fri=2C Aug 8=2C 2014 at 12:14 AM=2C Christopher Nguyen <ctn@adatao.co=
m>
>> wrote:
>> > +1 Joey & Andrew :)
>> >
>> > --
>> > Christopher T. Nguyen
>> > Co-founder & CEO=2C Adatao <http://adatao.com> [ah-'DAY-tao]
>> > linkedin.com/in/ctnguyen
>> >
>> >
>> >
>> > On Thu=2C Aug 7=2C 2014 at 10:39 PM=2C Joseph Gonzalez <
>> jegonzal@eecs.berkeley.edu
>> >> wrote:
>> >
>> >> Hi Everyone=2C
>> >>
>> >> Thank you for inviting me to be a committer.  I look forward to worki=
ng
>> >> with everyone to ensure the continued success of the Spark project.
>> >>
>> >> Thanks!
>> >> Joey
>> >>
>> >>
>> >>
>> >>
>> >> On Thu=2C Aug 7=2C 2014 at 9:57 PM=2C Matei Zaharia <matei@databricks=
.com>
>> >> wrote:
>> >>
>> >> > Hi everyone=2C
>> >> >
>> >> > The PMC recently voted to add two new committers and PMC members:
>> Joey
>> >> > Gonzalez and Andrew Or. Both have been huge contributors in the pas=
t
>> year
>> >> > -- Joey on much of GraphX as well as quite a bit of the initial wor=
k
>> in
>> >> > MLlib=2C and Andrew on Spark Core. Join me in welcoming them as
>> committers!
>> >> >
>> >> > Matei
>> >> >
>> >> >
>> >> >
>> >> >
>> >>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe=2C e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands=2C e-mail: dev-help@spark.apache.org
>>
>>
>

--_08a37293-c094-4e29-a150-b0496a067c01_--

From dev-return-8826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 13:01:00 2014
Return-Path: <dev-return-8826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADF87C5F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 13:01:00 +0000 (UTC)
Received: (qmail 64754 invoked by uid 500); 9 Aug 2014 13:01:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64689 invoked by uid 500); 9 Aug 2014 13:01:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64677 invoked by uid 99); 9 Aug 2014 13:00:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 13:00:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 13:00:54 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so7104299qgf.6
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 06:00:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=VmHMnpku4DY1fqNUr9URZyDItaXapmxuieQQOztVyD0=;
        b=yOVaKd29sf1RDPsc2r7W4JzPgTQQLHlTo1JKFIpprw5geUtSAwfu8ECx1Lw/u+AItW
         OKiur1/yXjngtYj44BrD23BYM/nNxmJ5d+ipTbsVc889AyDXHsrN6L74B5rQqrPrjSnT
         aBhHbyAjIRUV04qdMSEVrSO1leDwB78Ii5M1PmvGUA1IGewFar7dXxXIwMf/c+W3bdM6
         UQDWYIyA7KoJ5MfNzoL2CGiCdKBmHqo21EQx+VdEx3SwNvP3luLB4YX4k0BuQO/vZy9I
         4YMOkkYIsLF+BzFdiBBme72oEnEvUJlolp3yacIP1nDt30xnHJrlWVSWWppKU4RLjRzO
         kf9A==
MIME-Version: 1.0
X-Received: by 10.140.49.77 with SMTP id p71mr30977910qga.86.1407589233119;
 Sat, 09 Aug 2014 06:00:33 -0700 (PDT)
Received: by 10.140.24.50 with HTTP; Sat, 9 Aug 2014 06:00:33 -0700 (PDT)
In-Reply-To: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
Date: Sat, 9 Aug 2014 18:30:33 +0530
Message-ID: <CAJiQeYLOrp4efyoqJaUVNzCC0Mm9eBsW3Utpm9E05rZSTsfWaA@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Mridul Muralidharan <mridul@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Issue with supporting this imo is the fact that scala-test uses the
same vm for all the tests (surefire plugin supports fork, but
scala-test ignores it iirc).
So different tests would initialize different spark context, and can
potentially step on each others toes.

Regards,
Mridul


On Fri, Aug 8, 2014 at 9:31 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Howdy,
>
> Do we think it's both feasible and worthwhile to invest in getting our unit
> tests to finish in under 5 minutes (or something similarly brief) when run
> by Jenkins?
>
> Unit tests currently seem to take anywhere from 30 min to 2 hours. As
> people add more tests, I imagine this time will only grow. I think it would
> be better for both contributors and reviewers if they didn't have to wait
> so long for test results; PR reviews would be shorter, if nothing else.
>
> I don't know how how this is normally done, but maybe it wouldn't be too
> much work to get a test cycle to feel lighter.
>
> Most unit tests are independent and can be run concurrently, right? Would
> it make sense to build a given patch on many servers at once and send
> disjoint sets of unit tests to each?
>
> I'd be interested in working on something like that if possible (and
> sensible).
>
> Nick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 16:59:44 2014
Return-Path: <dev-return-8827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9D96CA7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 16:59:44 +0000 (UTC)
Received: (qmail 84137 invoked by uid 500); 9 Aug 2014 16:59:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84076 invoked by uid 500); 9 Aug 2014 16:59:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84062 invoked by uid 99); 9 Aug 2014 16:59:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 16:59:44 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 16:59:17 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so6770917qaq.1
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 09:59:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=eyKSKKXgmKwnIC5Bs03wLF0lQ5y0rPKurNWSD+NgKHM=;
        b=InppKYZbpDKdRd61wXg5aQb7wR/RLvpmgJW6C6+dmm0nl0huCnNkBjxAMZbmWbhlYz
         psfXh6U6hgo6l4/H+t+a+8xVziEOYmKLh0YCekuVZvBWxkEpa88RqfGZbupBTUKHFarw
         jWb0iWb/ZoxifQSBDy9PJy1G98T5mw0JUTx5t0AY0ebtCLh5OtS5qqY0in2IEvmFBvCZ
         eDoHwgBfPNJV753KoZLb1uxEZWMxO4d+d9ywvcYA2Uw3OgcSgYuCzq75lC4oQJpJ7flC
         XRe24XPA/ENsR73uP9ZsQaEYypyLXJmCnFPudOGBF/h/vwc9tk2mf/qvKqGtCVX2/srF
         Gqsg==
MIME-Version: 1.0
X-Received: by 10.140.44.67 with SMTP id f61mr33266560qga.44.1407603555885;
 Sat, 09 Aug 2014 09:59:15 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Sat, 9 Aug 2014 09:59:15 -0700 (PDT)
In-Reply-To: <CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
	<CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
Date: Sat, 9 Aug 2014 09:59:15 -0700
Message-ID: <CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113943d8eeebb10500353d3d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113943d8eeebb10500353d3d
Content-Type: text/plain; charset=UTF-8

I validated that I can reproduce this problem with master as well (without
adding any of my mllib changes)...

I separated mllib jar from assembly, deploy the assembly and then I supply
the mllib jar as --jars option to spark-submit...

I get this error:

14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299

Exception in thread "main" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
failure: Lost task 238.3 in stage 40.0 (TID 10002,
tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)

        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)


org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)

        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)

        org.apache.spark.scheduler.Task.run(Task.scala:54)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

I will try now with mllib inside the assembly....If that works then
something is weird here !


On Sat, Aug 9, 2014 at 12:46 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi Xiangrui,
>
> Based on your suggestion I moved core and mllib both to 1.1.0-SNAPSHOT...I
> am still getting class cast exception:
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted
> due to stage failure: Task 249 in stage 52.0 failed 4 times, most recent
> failure: Lost task 249.3 in stage 52.0 (TID 10002,
> tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
> scala.Tuple1 cannot be cast to scala.Product2
>
> I am running ALS.scala merged with my changes. I will try the mllib jar
> without my changes next...
>
> Can this be due to the fact that my jars are compiled with Java 1.7_55 but
> the cluster JRE is at 1.7_45.
>
> Thanks.
>
> Deb
>
>
>
>
> On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> I did not play with Hadoop settings...everything is compiled with
>> 2.3.0CDH5.0.2 for me...
>>
>> I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98
>> but there was no profile for CDH in the pom...but that's unrelated to this !
>>
>>
>> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>
>>> One related question, is mllib jar independent from hadoop version
>>> (doesnt use hadoop api directly)? Can I use mllib jar compile for one
>>> version of hadoop and use it in another version of hadoop?
>>>
>>> Sent from my Google Nexus 5
>>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com> wrote:
>>>
>>>> Hi Xiangrui,
>>>>
>>>> Maintaining another file will be a pain later so I deployed spark 1.0.1
>>>> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
>>>> along with the code changes for quadratic optimization...
>>>>
>>>> Later the plan is to patch the snapshot mllib with the deployed stable
>>>> mllib...
>>>>
>>>> There are 5 variants that I am experimenting with around 400M ratings
>>>> (daily data, monthly data I will update in few days)...
>>>>
>>>> 1. LS
>>>> 2. NNLS
>>>> 3. Quadratic with bounds
>>>> 4. Quadratic with L1
>>>> 5. Quadratic with equality and positivity
>>>>
>>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
>>>> ALS.scala:311
>>>>
>>>> // Materialize usersOut and productsOut.
>>>> usersOut.count()
>>>>
>>>> I am getting from one of the executors: java.lang.ClassCastException:
>>>> scala.Tuple1 cannot be cast to scala.Product2
>>>>
>>>> I am debugging it further but I was wondering if this is due to RDD
>>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>>>>
>>>> I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
>>>> cluster has Java 1.7.0_45.
>>>>
>>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that
>>>> Java
>>>> version mismatch cause this ?
>>>>
>>>> Stack traces are below
>>>>
>>>> Thanks.
>>>> Deb
>>>>
>>>>
>>>> Executor stacktrace:
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>>>>
>>>>
>>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>>
>>>>
>>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>>
>>>>
>>>>
>>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>>>
>>>>
>>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>>>
>>>>
>>>>
>>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>>>
>>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>>>
>>>>
>>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>>>>
>>>>
>>>>
>>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>>>
>>>>
>>>>
>>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>>>
>>>>         java.lang.Thread.run(Thread.java:744)
>>>>
>>>> Driver stacktrace:
>>>>
>>>> at org.apache.spark.scheduler.DAGScheduler.org
>>>>
>>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>>>
>>>> at
>>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>>
>>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>>
>>>> at scala.Option.foreach(Option.scala:236)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>>>
>>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>>>
>>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>>>
>>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>>>
>>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>>>
>>>> at
>>>>
>>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>>>
>>>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>
>>>> at
>>>>
>>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>>
>>>> at
>>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>>
>>>>  at
>>>>
>>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>>
>>>>
>>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <debasish.das83@gmail.com>
>>>> wrote:
>>>>
>>>> > Hi Xiangrui,
>>>> >
>>>> > I used your idea and kept a cherry picked version of ALS.scala in my
>>>> > application and call it ALSQp.scala...this is a OK workaround for now
>>>> till
>>>> > a version adds up to master for example...
>>>> >
>>>> > For the bug with userClassPathFirst, looks like Koert already found
>>>> this
>>>> > issue in the following JIRA:
>>>> >
>>>> > https://issues.apache.org/jira/browse/SPARK-1863
>>>> >
>>>> > By the way the userClassPathFirst feature is very useful since I am
>>>> sure
>>>> > the deployed version of spark on a production cluster will always be
>>>> the
>>>> > last stable (core at 1.0.1 in my case) and people would like to deploy
>>>> > SNAPSHOT versions of libraries that build on top of spark core (mllib,
>>>> > streaming etc)...
>>>> >
>>>> > Another way is to have a build option that deploys only the core and
>>>> not
>>>> > the libraries built upon core...
>>>> >
>>>> > Do we have an option like that in make-distribution script ?
>>>> >
>>>> > Thanks.
>>>> > Deb
>>>> >
>>>> >
>>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
>>>> wrote:
>>>> >
>>>> >> If you cannot change the Spark jar deployed on the cluster, an easy
>>>> >> solution would be renaming ALS in your jar. If userClassPathFirst
>>>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
>>>> >> -Xiangrui
>>>> >>
>>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
>>>> debasish.das83@gmail.com>
>>>> >> wrote:
>>>> >> > I created the assembly file but still it wants to pick the mllib
>>>> from
>>>> >> the
>>>> >> > cluster:
>>>> >> >
>>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
>>>> >> > QuadraticMinimizer
>>>> >> >
>>>> >> >
>>>> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>>>> >> >
>>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>>>> >> outputPath
>>>> >> >
>>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>>>> >> >
>>>> >>
>>>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>>>> >> >
>>>> >> > Now if I force it to use the jar that I gave using
>>>> >> > spark.files.userClassPathFirst, then it fails on some serialization
>>>> >> > issues...
>>>> >> >
>>>> >> > A simple solution is to cherry pick the files I need from spark
>>>> branch
>>>> >> to
>>>> >> > the application branch but I am not sure that's the right thing to
>>>> do...
>>>> >> >
>>>> >> > The way userClassPathFirst is behaving, there might be bugs in
>>>> it...
>>>> >> >
>>>> >> > Any suggestions will be appreciated....
>>>> >> >
>>>> >> > Thanks.
>>>> >> > Deb
>>>> >> >
>>>> >> >
>>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com>
>>>> >> wrote:
>>>> >> >>
>>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible with
>>>> >> >> spark-core-1.0.1.
>>>> >> >>
>>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>>>> >> debasish.das83@gmail.com>
>>>> >> >> wrote:
>>>> >> >> > Let me try it...
>>>> >> >> >
>>>> >> >> > Will this be fixed if I generate a assembly file with
>>>> mllib-1.1.0
>>>> >> >> > SNAPSHOT
>>>> >> >> > jar and other dependencies with the rest of the application
>>>> code ?
>>>> >> >> >
>>>> >> >> >
>>>> >> >> >
>>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <
>>>> mengxr@gmail.com>
>>>> >> wrote:
>>>> >> >> >>
>>>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But I'm
>>>> not
>>>> >> >> >> sure whether it could solve your problem. -Xiangrui
>>>> >> >> >>
>>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>>>> >> >> >> <debasish.das83@gmail.com>
>>>> >> >> >> wrote:
>>>> >> >> >> > Hi,
>>>> >> >> >> >
>>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I have
>>>> new
>>>> >> code
>>>> >> >> >> > that
>>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>>>> >> >> >> >
>>>> >> >> >> > I am trying to access the new code using spark-submit as
>>>> follows:
>>>> >> >> >> >
>>>> >> >> >> > spark-job --class
>>>> com.verizon.bda.mllib.recommendation.ALSDriver
>>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>>>> --lambda
>>>> >> 1.0
>>>> >> >> >> > --qpProblem 2 inputPath outputPath
>>>> >> >> >> >
>>>> >> >> >> > I can see the jars are getting added to httpServer as
>>>> expected:
>>>> >> >> >> >
>>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >> >> >> >
>>>> >> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>>>> >> >> >> >
>>>> >> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>>>> >> >> >> > with
>>>> >> >> >> > timestamp 1406998204236
>>>> >> >> >> >
>>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>>>> >> timestamp
>>>> >> >> >> > 1406998204237
>>>> >> >> >> >
>>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>>>> >> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>>>> with
>>>> >> >> >> > timestamp
>>>> >> >> >> > 1406998204238
>>>> >> >> >> >
>>>> >> >> >> > But the job still can't access code form mllib-1.1.0
>>>> >> SNAPSHOT.jar...I
>>>> >> >> >> > think
>>>> >> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
>>>> >> >> >> >
>>>> >> >> >> > Please help. I will ask for a PR tomorrow but internally we
>>>> want
>>>> >> to
>>>> >> >> >> > generate results from the new code.
>>>> >> >> >> >
>>>> >> >> >> > Thanks.
>>>> >> >> >> >
>>>> >> >> >> > Deb
>>>> >> >> >
>>>> >> >> >
>>>> >> >
>>>> >> >
>>>> >>
>>>> >
>>>> >
>>>>
>>>
>>
>

--001a113943d8eeebb10500353d3d--

From dev-return-8828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 18:01:41 2014
Return-Path: <dev-return-8828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4D315CB66
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 18:01:41 +0000 (UTC)
Received: (qmail 24800 invoked by uid 500); 9 Aug 2014 18:01:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24736 invoked by uid 500); 9 Aug 2014 18:01:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24725 invoked by uid 99); 9 Aug 2014 18:01:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 18:01:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 18:01:36 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so6939820wes.33
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 11:01:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=1RZTmVDNvsp5qeytAygPRuOHBUrQ5CQevKt5TOWSqW8=;
        b=KpNzTkn2mMwZnStyMrgUaOmfRft1VccYQNfirLgOFKwC3OVmsWP3VkwX8WTRo4SzBs
         hVkQClXf07dnN/T8QkC57TaCjqC6pA/7+MMl9mnCABQkoSzDhGGiZKMMZlS2Ka/DGm/6
         hndReo/Z1kIhT1Rt49VgrAXQ4ZGGzHEhN/JGP2HyM0Gp2csgSvjtZ1rBX+3dEzBmPMCh
         Qt1p6+oG3P+zvOhI8cHZ7d6f9sS4O8UbfaVrwsqbtqJ27lj4rIgnabDMW25QZBMYHAIa
         tJcWlsxw4JezdTUCxAgO5HjRTFu74A27uLRogED3fooxiL7ydCO+3hb7ycu7bj1DEPQb
         WXPg==
X-Gm-Message-State: ALoCoQmeDxhNsVNq8HSsyS67VWicp6u6JnNywLtTgnQP86aouv/X/pdU8f3rG4o4chXGQm3RfX6a
MIME-Version: 1.0
X-Received: by 10.194.200.137 with SMTP id js9mr564785wjc.90.1407607274156;
 Sat, 09 Aug 2014 11:01:14 -0700 (PDT)
Received: by 10.180.94.34 with HTTP; Sat, 9 Aug 2014 11:01:14 -0700 (PDT)
In-Reply-To: <CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
	<CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
	<CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
Date: Sat, 9 Aug 2014 11:01:14 -0700
Message-ID: <CAFZt-ESs4+h7-qGT4PGHcaezmONp3PHYhTkhUyHqyqcV-uiz9g@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Matt Forbes <matt@tellapart.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb70c748f48ac0500361b35
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb70c748f48ac0500361b35
Content-Type: text/plain; charset=UTF-8

I was having this same problem early this week and had to include my
changes in the assembly.


On Sat, Aug 9, 2014 at 9:59 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> I validated that I can reproduce this problem with master as well (without
> adding any of my mllib changes)...
>
> I separated mllib jar from assembly, deploy the assembly and then I supply
> the mllib jar as --jars option to spark-submit...
>
> I get this error:
>
> 14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted due
> to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
> failure: Lost task 238.3 in stage 40.0 (TID 10002,
> tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
> scala.Tuple1 cannot be cast to scala.Product2
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)
>
>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>
>
>
> org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
>
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)
>
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
>
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>
>         scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>
>         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>
>         org.apache.spark.scheduler.Task.run(Task.scala:54)
>
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
>
>
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>
>
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>
>         java.lang.Thread.run(Thread.java:744)
>
> Driver stacktrace:
>
> at org.apache.spark.scheduler.DAGScheduler.org
>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
>
> at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>
> at scala.Option.foreach(Option.scala:236)
>
> at
>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
>
> at
>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
>
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>
> at
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>
> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
> at
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>
> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
> at
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
> I will try now with mllib inside the assembly....If that works then
> something is weird here !
>
>
> On Sat, Aug 9, 2014 at 12:46 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Hi Xiangrui,
> >
> > Based on your suggestion I moved core and mllib both to
> 1.1.0-SNAPSHOT...I
> > am still getting class cast exception:
> >
> > Exception in thread "main" org.apache.spark.SparkException: Job aborted
> > due to stage failure: Task 249 in stage 52.0 failed 4 times, most recent
> > failure: Lost task 249.3 in stage 52.0 (TID 10002,
> > tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
> > scala.Tuple1 cannot be cast to scala.Product2
> >
> > I am running ALS.scala merged with my changes. I will try the mllib jar
> > without my changes next...
> >
> > Can this be due to the fact that my jars are compiled with Java 1.7_55
> but
> > the cluster JRE is at 1.7_45.
> >
> > Thanks.
> >
> > Deb
> >
> >
> >
> >
> > On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >
> >> I did not play with Hadoop settings...everything is compiled with
> >> 2.3.0CDH5.0.2 for me...
> >>
> >> I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98
> >> but there was no profile for CDH in the pom...but that's unrelated to
> this !
> >>
> >>
> >> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >>
> >>> One related question, is mllib jar independent from hadoop version
> >>> (doesnt use hadoop api directly)? Can I use mllib jar compile for one
> >>> version of hadoop and use it in another version of hadoop?
> >>>
> >>> Sent from my Google Nexus 5
> >>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com>
> wrote:
> >>>
> >>>> Hi Xiangrui,
> >>>>
> >>>> Maintaining another file will be a pain later so I deployed spark
> 1.0.1
> >>>> without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
> >>>> along with the code changes for quadratic optimization...
> >>>>
> >>>> Later the plan is to patch the snapshot mllib with the deployed stable
> >>>> mllib...
> >>>>
> >>>> There are 5 variants that I am experimenting with around 400M ratings
> >>>> (daily data, monthly data I will update in few days)...
> >>>>
> >>>> 1. LS
> >>>> 2. NNLS
> >>>> 3. Quadratic with bounds
> >>>> 4. Quadratic with L1
> >>>> 5. Quadratic with equality and positivity
> >>>>
> >>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this step
> >>>> ALS.scala:311
> >>>>
> >>>> // Materialize usersOut and productsOut.
> >>>> usersOut.count()
> >>>>
> >>>> I am getting from one of the executors: java.lang.ClassCastException:
> >>>> scala.Tuple1 cannot be cast to scala.Product2
> >>>>
> >>>> I am debugging it further but I was wondering if this is due to RDD
> >>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
> >>>>
> >>>> I have built the jars on my Mac which has Java 1.7.0_55 but the
> deployed
> >>>> cluster has Java 1.7.0_45.
> >>>>
> >>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that
> >>>> Java
> >>>> version mismatch cause this ?
> >>>>
> >>>> Stack traces are below
> >>>>
> >>>> Thanks.
> >>>> Deb
> >>>>
> >>>>
> >>>> Executor stacktrace:
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
> >>>>
> >>>>
> >>>>
> >>>>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >>>>
> >>>>
> >>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
> >>>>
> >>>>
> >>>>
> >>>>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
> >>>>
> >>>>
> >>>>
> >>>>
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
> >>>>
> >>>>
> >>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
> >>>>
> >>>>
> >>>>
> >>>>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> >>>>
> >>>>
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> >>>>
> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
> >>>>
> >>>>
> >>>>
> >>>>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
> >>>>
> >>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
> >>>>
> >>>>
> >>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
> >>>>
> >>>>
> >>>>
> >>>>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>>>
> >>>>
> >>>>
> >>>>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>>>
> >>>>         java.lang.Thread.run(Thread.java:744)
> >>>>
> >>>> Driver stacktrace:
> >>>>
> >>>> at org.apache.spark.scheduler.DAGScheduler.org
> >>>>
> >>>>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
> >>>>
> >>>> at
> >>>>
> >>>>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >>>>
> >>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
> >>>>
> >>>> at scala.Option.foreach(Option.scala:236)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
> >>>>
> >>>> at
> >>>>
> >>>>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
> >>>>
> >>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >>>>
> >>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >>>>
> >>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >>>>
> >>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >>>>
> >>>> at
> >>>>
> >>>>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >>>>
> >>>> at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >>>>
> >>>> at
> >>>>
> >>>>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >>>>
> >>>> at
> >>>>
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >>>>
> >>>>  at
> >>>>
> >>>>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >>>>
> >>>>
> >>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <
> debasish.das83@gmail.com>
> >>>> wrote:
> >>>>
> >>>> > Hi Xiangrui,
> >>>> >
> >>>> > I used your idea and kept a cherry picked version of ALS.scala in my
> >>>> > application and call it ALSQp.scala...this is a OK workaround for
> now
> >>>> till
> >>>> > a version adds up to master for example...
> >>>> >
> >>>> > For the bug with userClassPathFirst, looks like Koert already found
> >>>> this
> >>>> > issue in the following JIRA:
> >>>> >
> >>>> > https://issues.apache.org/jira/browse/SPARK-1863
> >>>> >
> >>>> > By the way the userClassPathFirst feature is very useful since I am
> >>>> sure
> >>>> > the deployed version of spark on a production cluster will always be
> >>>> the
> >>>> > last stable (core at 1.0.1 in my case) and people would like to
> deploy
> >>>> > SNAPSHOT versions of libraries that build on top of spark core
> (mllib,
> >>>> > streaming etc)...
> >>>> >
> >>>> > Another way is to have a build option that deploys only the core and
> >>>> not
> >>>> > the libraries built upon core...
> >>>> >
> >>>> > Do we have an option like that in make-distribution script ?
> >>>> >
> >>>> > Thanks.
> >>>> > Deb
> >>>> >
> >>>> >
> >>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
> >>>> wrote:
> >>>> >
> >>>> >> If you cannot change the Spark jar deployed on the cluster, an easy
> >>>> >> solution would be renaming ALS in your jar. If userClassPathFirst
> >>>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
> >>>> >> -Xiangrui
> >>>> >>
> >>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
> >>>> debasish.das83@gmail.com>
> >>>> >> wrote:
> >>>> >> > I created the assembly file but still it wants to pick the mllib
> >>>> from
> >>>> >> the
> >>>> >> > cluster:
> >>>> >> >
> >>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar |
> grep
> >>>> >> > QuadraticMinimizer
> >>>> >> >
> >>>> >> >
> >>>> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
> >>>> >> >
> >>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
> >>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
> >>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
> >>>> >> outputPath
> >>>> >> >
> >>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
> >>>> >> >
> >>>> >>
> >>>>
> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
> >>>> >> >
> >>>> >> > Now if I force it to use the jar that I gave using
> >>>> >> > spark.files.userClassPathFirst, then it fails on some
> serialization
> >>>> >> > issues...
> >>>> >> >
> >>>> >> > A simple solution is to cherry pick the files I need from spark
> >>>> branch
> >>>> >> to
> >>>> >> > the application branch but I am not sure that's the right thing
> to
> >>>> do...
> >>>> >> >
> >>>> >> > The way userClassPathFirst is behaving, there might be bugs in
> >>>> it...
> >>>> >> >
> >>>> >> > Any suggestions will be appreciated....
> >>>> >> >
> >>>> >> > Thanks.
> >>>> >> > Deb
> >>>> >> >
> >>>> >> >
> >>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
> >
> >>>> >> wrote:
> >>>> >> >>
> >>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible
> with
> >>>> >> >> spark-core-1.0.1.
> >>>> >> >>
> >>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
> >>>> >> debasish.das83@gmail.com>
> >>>> >> >> wrote:
> >>>> >> >> > Let me try it...
> >>>> >> >> >
> >>>> >> >> > Will this be fixed if I generate a assembly file with
> >>>> mllib-1.1.0
> >>>> >> >> > SNAPSHOT
> >>>> >> >> > jar and other dependencies with the rest of the application
> >>>> code ?
> >>>> >> >> >
> >>>> >> >> >
> >>>> >> >> >
> >>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <
> >>>> mengxr@gmail.com>
> >>>> >> wrote:
> >>>> >> >> >>
> >>>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But
> I'm
> >>>> not
> >>>> >> >> >> sure whether it could solve your problem. -Xiangrui
> >>>> >> >> >>
> >>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
> >>>> >> >> >> <debasish.das83@gmail.com>
> >>>> >> >> >> wrote:
> >>>> >> >> >> > Hi,
> >>>> >> >> >> >
> >>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I
> have
> >>>> new
> >>>> >> code
> >>>> >> >> >> > that
> >>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
> >>>> >> >> >> >
> >>>> >> >> >> > I am trying to access the new code using spark-submit as
> >>>> follows:
> >>>> >> >> >> >
> >>>> >> >> >> > spark-job --class
> >>>> com.verizon.bda.mllib.recommendation.ALSDriver
> >>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
> >>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
> >>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
> >>>> --lambda
> >>>> >> 1.0
> >>>> >> >> >> > --qpProblem 2 inputPath outputPath
> >>>> >> >> >> >
> >>>> >> >> >> > I can see the jars are getting added to httpServer as
> >>>> expected:
> >>>> >> >> >> >
> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>>> >> >> >> >
> >>>> >> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> at
> >>>> >> >> >> >
> >>>> >> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
> >>>> >> >> >> > with
> >>>> >> >> >> > timestamp 1406998204236
> >>>> >> >> >> >
> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
> >>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
> >>>> >> timestamp
> >>>> >> >> >> > 1406998204237
> >>>> >> >> >> >
> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
> >>>> >> >> >> > file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar
> at
> >>>> >> >> >> > http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
> >>>> with
> >>>> >> >> >> > timestamp
> >>>> >> >> >> > 1406998204238
> >>>> >> >> >> >
> >>>> >> >> >> > But the job still can't access code form mllib-1.1.0
> >>>> >> SNAPSHOT.jar...I
> >>>> >> >> >> > think
> >>>> >> >> >> > it's picking up the mllib from cluster which is at 1.0.1...
> >>>> >> >> >> >
> >>>> >> >> >> > Please help. I will ask for a PR tomorrow but internally we
> >>>> want
> >>>> >> to
> >>>> >> >> >> > generate results from the new code.
> >>>> >> >> >> >
> >>>> >> >> >> > Thanks.
> >>>> >> >> >> >
> >>>> >> >> >> > Deb
> >>>> >> >> >
> >>>> >> >> >
> >>>> >> >
> >>>> >> >
> >>>> >>
> >>>> >
> >>>> >
> >>>>
> >>>
> >>
> >
>

--047d7bb70c748f48ac0500361b35--

From dev-return-8829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug  9 18:13:21 2014
Return-Path: <dev-return-8829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97537CB99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Aug 2014 18:13:21 +0000 (UTC)
Received: (qmail 36396 invoked by uid 500); 9 Aug 2014 18:13:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36319 invoked by uid 500); 9 Aug 2014 18:13:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36306 invoked by uid 99); 9 Aug 2014 18:13:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 18:13:20 +0000
X-ASF-Spam-Status: No, hits=5.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URIBL_WS_SURBL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Aug 2014 18:12:53 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so6830274qaq.27
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 11:12:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=q2A5qnBK3PENwtKJwcWihzvBjBFU8uGyHmyBJ0R8t0c=;
        b=cEUMTocpjPRZdD6FRqAXllqT10hGyrOKhtmkP5+vUMwcxVjKsDZjL4rsyNvbpYWZfK
         /hYjVw1auXRs4t+HgGtjZPw38Ow8MwKWWvniW7RbiPsdnHq8JIoMFfIkiLoAyGdmQrjI
         2q6BnwwR1fu3cUi461Z87tjAqWfSAnCdZsm1lLLemJ42oVUCamWbLJhd44pmHje5V/Ru
         W1IHqBRAlUFd99lCTxQgsw3T/+G5wrKL+bqNHtKQOCZprXMGNRhVn6mEd+TgKy74hDrK
         yAGZ2WpEyh1DH2k/r0U+ElYcjFnkMm4JqAd4op1WCMhiB7bJB2h5XCQAYYvTYmlUo1AZ
         3NSA==
MIME-Version: 1.0
X-Received: by 10.140.93.161 with SMTP id d30mr33031206qge.53.1407607971937;
 Sat, 09 Aug 2014 11:12:51 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Sat, 9 Aug 2014 11:12:51 -0700 (PDT)
In-Reply-To: <CAFZt-ESs4+h7-qGT4PGHcaezmONp3PHYhTkhUyHqyqcV-uiz9g@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
	<CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
	<CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
	<CAFZt-ESs4+h7-qGT4PGHcaezmONp3PHYhTkhUyHqyqcV-uiz9g@mail.gmail.com>
Date: Sat, 9 Aug 2014 11:12:51 -0700
Message-ID: <CA+B-+fyQ0QXPk-nLQkgUdOaSYUSQp+-v9Mx2Rh+RSZxgQroMZQ@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Matt Forbes <matt@tellapart.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113b97762686c7050036452f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b97762686c7050036452f
Content-Type: text/plain; charset=UTF-8

Including mllib inside assembly worked fine...If I deploy only the core and
send mllib as --jars then this problem shows up...

Xiangrui could you please comment if it is a bug or expected behavior ? I
will create a JIRA if this needs to be tracked...


On Sat, Aug 9, 2014 at 11:01 AM, Matt Forbes <matt@tellapart.com> wrote:

> I was having this same problem early this week and had to include my
> changes in the assembly.
>
>
> On Sat, Aug 9, 2014 at 9:59 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> I validated that I can reproduce this problem with master as well (without
>> adding any of my mllib changes)...
>>
>> I separated mllib jar from assembly, deploy the assembly and then I supply
>> the mllib jar as --jars option to spark-submit...
>>
>> I get this error:
>>
>> 14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299
>>
>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>> due
>> to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
>> failure: Lost task 238.3 in stage 40.0 (TID 10002,
>> tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>> scala.Tuple1 cannot be cast to scala.Product2
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)
>>
>>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>
>>
>>
>> org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>
>>
>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>
>>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>
>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)
>>
>>
>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>
>>
>>
>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>
>>
>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>
>>
>>
>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>
>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>
>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>
>>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>
>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>
>>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
>>
>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>
>>         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>>
>>         org.apache.spark.scheduler.Task.run(Task.scala:54)
>>
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
>>
>>
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>
>>
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>
>>         java.lang.Thread.run(Thread.java:744)
>>
>> Driver stacktrace:
>>
>> at org.apache.spark.scheduler.DAGScheduler.org
>>
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
>>
>> at
>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>
>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>
>> at scala.Option.foreach(Option.scala:236)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
>>
>> at
>>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
>>
>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>
>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>
>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>
>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>
>> at
>>
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>
>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>
>> at
>>
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>
>> at
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>
>> at
>>
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>
>> I will try now with mllib inside the assembly....If that works then
>> something is weird here !
>>
>>
>> On Sat, Aug 9, 2014 at 12:46 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>> > Hi Xiangrui,
>> >
>> > Based on your suggestion I moved core and mllib both to
>> 1.1.0-SNAPSHOT...I
>> > am still getting class cast exception:
>> >
>> > Exception in thread "main" org.apache.spark.SparkException: Job aborted
>> > due to stage failure: Task 249 in stage 52.0 failed 4 times, most recent
>> > failure: Lost task 249.3 in stage 52.0 (TID 10002,
>> > tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>> > scala.Tuple1 cannot be cast to scala.Product2
>> >
>> > I am running ALS.scala merged with my changes. I will try the mllib jar
>> > without my changes next...
>> >
>> > Can this be due to the fact that my jars are compiled with Java 1.7_55
>> but
>> > the cluster JRE is at 1.7_45.
>> >
>> > Thanks.
>> >
>> > Deb
>> >
>> >
>> >
>> >
>> > On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <debasish.das83@gmail.com
>> >
>> > wrote:
>> >
>> >> I did not play with Hadoop settings...everything is compiled with
>> >> 2.3.0CDH5.0.2 for me...
>> >>
>> >> I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98
>> >> but there was no profile for CDH in the pom...but that's unrelated to
>> this !
>> >>
>> >>
>> >> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> >>
>> >>> One related question, is mllib jar independent from hadoop version
>> >>> (doesnt use hadoop api directly)? Can I use mllib jar compile for one
>> >>> version of hadoop and use it in another version of hadoop?
>> >>>
>> >>> Sent from my Google Nexus 5
>> >>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com>
>> wrote:
>> >>>
>> >>>> Hi Xiangrui,
>> >>>>
>> >>>> Maintaining another file will be a pain later so I deployed spark
>> 1.0.1
>> >>>> without mllib and then my application jar bundles mllib
>> 1.1.0-SNAPSHOT
>> >>>> along with the code changes for quadratic optimization...
>> >>>>
>> >>>> Later the plan is to patch the snapshot mllib with the deployed
>> stable
>> >>>> mllib...
>> >>>>
>> >>>> There are 5 variants that I am experimenting with around 400M ratings
>> >>>> (daily data, monthly data I will update in few days)...
>> >>>>
>> >>>> 1. LS
>> >>>> 2. NNLS
>> >>>> 3. Quadratic with bounds
>> >>>> 4. Quadratic with L1
>> >>>> 5. Quadratic with equality and positivity
>> >>>>
>> >>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this
>> step
>> >>>> ALS.scala:311
>> >>>>
>> >>>> // Materialize usersOut and productsOut.
>> >>>> usersOut.count()
>> >>>>
>> >>>> I am getting from one of the executors: java.lang.ClassCastException:
>> >>>> scala.Tuple1 cannot be cast to scala.Product2
>> >>>>
>> >>>> I am debugging it further but I was wondering if this is due to RDD
>> >>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>> >>>>
>> >>>> I have built the jars on my Mac which has Java 1.7.0_55 but the
>> deployed
>> >>>> cluster has Java 1.7.0_45.
>> >>>>
>> >>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can
>> that
>> >>>> Java
>> >>>> version mismatch cause this ?
>> >>>>
>> >>>> Stack traces are below
>> >>>>
>> >>>> Thanks.
>> >>>> Deb
>> >>>>
>> >>>>
>> >>>> Executor stacktrace:
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >>>>
>> >>>>
>> >>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >>>>
>> >>>>
>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>> >>>>
>> >>>>
>> >>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>> >>>>
>> >>>>
>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>> >>>>
>> >>>>
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>> >>>>
>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>> >>>>
>> >>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>> >>>>
>> >>>>
>> >>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> >>>>
>> >>>>         java.lang.Thread.run(Thread.java:744)
>> >>>>
>> >>>> Driver stacktrace:
>> >>>>
>> >>>> at org.apache.spark.scheduler.DAGScheduler.org
>> >>>>
>> >>>>
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >>>>
>> >>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>> >>>>
>> >>>> at scala.Option.foreach(Option.scala:236)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>> >>>>
>> >>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>> >>>>
>> >>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>> >>>>
>> >>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>> >>>>
>> >>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>> >>>>
>> >>>> at
>> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>> >>>>
>> >>>> at
>> >>>>
>> >>>>
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>> >>>>
>> >>>> at
>> >>>>
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>> >>>>
>> >>>>  at
>> >>>>
>> >>>>
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>> >>>>
>> >>>>
>> >>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <
>> debasish.das83@gmail.com>
>> >>>> wrote:
>> >>>>
>> >>>> > Hi Xiangrui,
>> >>>> >
>> >>>> > I used your idea and kept a cherry picked version of ALS.scala in
>> my
>> >>>> > application and call it ALSQp.scala...this is a OK workaround for
>> now
>> >>>> till
>> >>>> > a version adds up to master for example...
>> >>>> >
>> >>>> > For the bug with userClassPathFirst, looks like Koert already found
>> >>>> this
>> >>>> > issue in the following JIRA:
>> >>>> >
>> >>>> > https://issues.apache.org/jira/browse/SPARK-1863
>> >>>> >
>> >>>> > By the way the userClassPathFirst feature is very useful since I am
>> >>>> sure
>> >>>> > the deployed version of spark on a production cluster will always
>> be
>> >>>> the
>> >>>> > last stable (core at 1.0.1 in my case) and people would like to
>> deploy
>> >>>> > SNAPSHOT versions of libraries that build on top of spark core
>> (mllib,
>> >>>> > streaming etc)...
>> >>>> >
>> >>>> > Another way is to have a build option that deploys only the core
>> and
>> >>>> not
>> >>>> > the libraries built upon core...
>> >>>> >
>> >>>> > Do we have an option like that in make-distribution script ?
>> >>>> >
>> >>>> > Thanks.
>> >>>> > Deb
>> >>>> >
>> >>>> >
>> >>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
>> >>>> wrote:
>> >>>> >
>> >>>> >> If you cannot change the Spark jar deployed on the cluster, an
>> easy
>> >>>> >> solution would be renaming ALS in your jar. If userClassPathFirst
>> >>>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
>> >>>> >> -Xiangrui
>> >>>> >>
>> >>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
>> >>>> debasish.das83@gmail.com>
>> >>>> >> wrote:
>> >>>> >> > I created the assembly file but still it wants to pick the mllib
>> >>>> from
>> >>>> >> the
>> >>>> >> > cluster:
>> >>>> >> >
>> >>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar |
>> grep
>> >>>> >> > QuadraticMinimizer
>> >>>> >> >
>> >>>> >> >
>> >>>> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>> >>>> >> >
>> >>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>> >>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>> >>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>> >>>> >> outputPath
>> >>>> >> >
>> >>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>> >>>> >> >
>> >>>> >>
>> >>>>
>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>> >>>> >> >
>> >>>> >> > Now if I force it to use the jar that I gave using
>> >>>> >> > spark.files.userClassPathFirst, then it fails on some
>> serialization
>> >>>> >> > issues...
>> >>>> >> >
>> >>>> >> > A simple solution is to cherry pick the files I need from spark
>> >>>> branch
>> >>>> >> to
>> >>>> >> > the application branch but I am not sure that's the right thing
>> to
>> >>>> do...
>> >>>> >> >
>> >>>> >> > The way userClassPathFirst is behaving, there might be bugs in
>> >>>> it...
>> >>>> >> >
>> >>>> >> > Any suggestions will be appreciated....
>> >>>> >> >
>> >>>> >> > Thanks.
>> >>>> >> > Deb
>> >>>> >> >
>> >>>> >> >
>> >>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <
>> mengxr@gmail.com>
>> >>>> >> wrote:
>> >>>> >> >>
>> >>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible
>> with
>> >>>> >> >> spark-core-1.0.1.
>> >>>> >> >>
>> >>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>> >>>> >> debasish.das83@gmail.com>
>> >>>> >> >> wrote:
>> >>>> >> >> > Let me try it...
>> >>>> >> >> >
>> >>>> >> >> > Will this be fixed if I generate a assembly file with
>> >>>> mllib-1.1.0
>> >>>> >> >> > SNAPSHOT
>> >>>> >> >> > jar and other dependencies with the rest of the application
>> >>>> code ?
>> >>>> >> >> >
>> >>>> >> >> >
>> >>>> >> >> >
>> >>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <
>> >>>> mengxr@gmail.com>
>> >>>> >> wrote:
>> >>>> >> >> >>
>> >>>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But
>> I'm
>> >>>> not
>> >>>> >> >> >> sure whether it could solve your problem. -Xiangrui
>> >>>> >> >> >>
>> >>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>> >>>> >> >> >> <debasish.das83@gmail.com>
>> >>>> >> >> >> wrote:
>> >>>> >> >> >> > Hi,
>> >>>> >> >> >> >
>> >>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I
>> have
>> >>>> new
>> >>>> >> code
>> >>>> >> >> >> > that
>> >>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>> >>>> >> >> >> >
>> >>>> >> >> >> > I am trying to access the new code using spark-submit as
>> >>>> follows:
>> >>>> >> >> >> >
>> >>>> >> >> >> > spark-job --class
>> >>>> com.verizon.bda.mllib.recommendation.ALSDriver
>> >>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>> >>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>> >>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>> >>>> --lambda
>> >>>> >> 1.0
>> >>>> >> >> >> > --qpProblem 2 inputPath outputPath
>> >>>> >> >> >> >
>> >>>> >> >> >> > I can see the jars are getting added to httpServer as
>> >>>> expected:
>> >>>> >> >> >> >
>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >>>> >> >> >> >
>> >>>> >>
>> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>> >>>> >> >> >> >
>> >>>> >>
>> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>> >>>> >> >> >> > with
>> >>>> >> >> >> > timestamp 1406998204236
>> >>>> >> >> >> >
>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>> >>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>> >>>> >> timestamp
>> >>>> >> >> >> > 1406998204237
>> >>>> >> >> >> >
>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>> >>>> >> >> >> >
>> file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>> >>>> >> >> >> >
>> http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>> >>>> with
>> >>>> >> >> >> > timestamp
>> >>>> >> >> >> > 1406998204238
>> >>>> >> >> >> >
>> >>>> >> >> >> > But the job still can't access code form mllib-1.1.0
>> >>>> >> SNAPSHOT.jar...I
>> >>>> >> >> >> > think
>> >>>> >> >> >> > it's picking up the mllib from cluster which is at
>> 1.0.1...
>> >>>> >> >> >> >
>> >>>> >> >> >> > Please help. I will ask for a PR tomorrow but internally
>> we
>> >>>> want
>> >>>> >> to
>> >>>> >> >> >> > generate results from the new code.
>> >>>> >> >> >> >
>> >>>> >> >> >> > Thanks.
>> >>>> >> >> >> >
>> >>>> >> >> >> > Deb
>> >>>> >> >> >
>> >>>> >> >> >
>> >>>> >> >
>> >>>> >> >
>> >>>> >>
>> >>>> >
>> >>>> >
>> >>>>
>> >>>
>> >>
>> >
>>
>
>

--001a113b97762686c7050036452f--

From dev-return-8830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 10 01:12:36 2014
Return-Path: <dev-return-8830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BCF41112D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 10 Aug 2014 01:12:36 +0000 (UTC)
Received: (qmail 38539 invoked by uid 500); 10 Aug 2014 01:12:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38481 invoked by uid 500); 10 Aug 2014 01:12:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38465 invoked by uid 99); 10 Aug 2014 01:12:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 01:12:35 +0000
X-ASF-Spam-Status: No, hits=5.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URIBL_WS_SURBL,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.43 as permitted sender)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 01:12:06 +0000
Received: by mail-qa0-f43.google.com with SMTP id w8so6914425qac.30
        for <dev@spark.apache.org>; Sat, 09 Aug 2014 18:12:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Vz/nO4VcUdzMpRaOqWYpjHRHkjNzFZVEK1lZQ5B5Jsg=;
        b=0f3Gzsh1DlK9NFjdJUNQPiHLxtRCOCuDcHBgiSAqPyxt+w2lH5A436SMDpCcL8cHIv
         iuOgnfE1RdwdTlB2p5QuMxulWYNmuy4FgK0Ij4J+VNmL7QA2yVJE9KzEM1Drrugt+2HH
         XPwMqr030Bh6YsvvNtU4gPX2CGxkDB3LxZgQ3zyQflNk7Lh8fmDAsDs6RWYXgKLbcmht
         11j4Y4iEBy4g08B90yz6Zcwax+3DobgC+QiFhv9j2oA+qnNNjG0Jv83c+M4kHOxsuGiL
         +oIIFX8CDWfTel4SK+KFuJDtf0IGhsZzU6p4EX+nzAAepSJHpfXGjJY86pjAUNjZJKUw
         01bg==
MIME-Version: 1.0
X-Received: by 10.224.104.1 with SMTP id m1mr51696439qao.81.1407633124884;
 Sat, 09 Aug 2014 18:12:04 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Sat, 9 Aug 2014 18:12:04 -0700 (PDT)
In-Reply-To: <CA+B-+fyQ0QXPk-nLQkgUdOaSYUSQp+-v9Mx2Rh+RSZxgQroMZQ@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
	<CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
	<CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
	<CAFZt-ESs4+h7-qGT4PGHcaezmONp3PHYhTkhUyHqyqcV-uiz9g@mail.gmail.com>
	<CA+B-+fyQ0QXPk-nLQkgUdOaSYUSQp+-v9Mx2Rh+RSZxgQroMZQ@mail.gmail.com>
Date: Sat, 9 Aug 2014 18:12:04 -0700
Message-ID: <CA+B-+fw0WP4arriG3Lgqpi4LEoeeERBiz1MUK_Q_vLssPqnE3g@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Matt Forbes <matt@tellapart.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c248e06223fd05003c2033
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c248e06223fd05003c2033
Content-Type: text/plain; charset=UTF-8

Actually nope it did not work fine...

With multiple ALS iteration, I am getting the same error (with or without
my mllib changes)....

Exception in thread "main" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 206 in stage 52.0 failed 4 times, most recent
failure: Lost task 206.3 in stage 52.0 (TID 9999,
tblpmidn42adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)

        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)


org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)

        org.apache.spark.scheduler.Task.run(Task.scala:54)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

The behavior is consistent in standalone and yarn mode...

I am at the following checkin: commit
ec79063fad44751a6689f5e58d47886babeaecff

I also tested yarn deployment and I will use standalone mode to deploy
stable spark release (1.0.1 right now) and all the mllib changes I can test
on our datasets through yarn deployment...it works fine...

By the way, let me try if I can reproduce this issue on MovieLensALS
locally....Most likely it is a bug

Thanks.

Deb


On Sat, Aug 9, 2014 at 11:12 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Including mllib inside assembly worked fine...If I deploy only the core
> and send mllib as --jars then this problem shows up...
>
> Xiangrui could you please comment if it is a bug or expected behavior ? I
> will create a JIRA if this needs to be tracked...
>
>
> On Sat, Aug 9, 2014 at 11:01 AM, Matt Forbes <matt@tellapart.com> wrote:
>
>> I was having this same problem early this week and had to include my
>> changes in the assembly.
>>
>>
>> On Sat, Aug 9, 2014 at 9:59 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>>> I validated that I can reproduce this problem with master as well
>>> (without
>>> adding any of my mllib changes)...
>>>
>>> I separated mllib jar from assembly, deploy the assembly and then I
>>> supply
>>> the mllib jar as --jars option to spark-submit...
>>>
>>> I get this error:
>>>
>>> 14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299
>>>
>>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>>> due
>>> to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
>>> failure: Lost task 238.3 in stage 40.0 (TID 10002,
>>> tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>>> scala.Tuple1 cannot be cast to scala.Product2
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)
>>>
>>>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>>
>>>
>>>
>>> org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>
>>>
>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>
>>>
>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>
>>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)
>>>
>>>
>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>
>>>
>>>
>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>>
>>>
>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>>
>>>
>>>
>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>
>>>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>
>>>
>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>>
>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>
>>>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
>>>
>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>>
>>>
>>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>>>
>>>         org.apache.spark.scheduler.Task.run(Task.scala:54)
>>>
>>>
>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
>>>
>>>
>>>
>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>>
>>>
>>>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>>
>>>         java.lang.Thread.run(Thread.java:744)
>>>
>>> Driver stacktrace:
>>>
>>> at org.apache.spark.scheduler.DAGScheduler.org
>>>
>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
>>>
>>> at
>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>
>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>>
>>> at scala.Option.foreach(Option.scala:236)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
>>>
>>> at
>>>
>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
>>>
>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>>
>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>>
>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>>
>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>>
>>> at
>>>
>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>>
>>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>
>>> at
>>>
>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>
>>> at
>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>
>>> at
>>>
>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>
>>> I will try now with mllib inside the assembly....If that works then
>>> something is weird here !
>>>
>>>
>>> On Sat, Aug 9, 2014 at 12:46 AM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>>
>>> > Hi Xiangrui,
>>> >
>>> > Based on your suggestion I moved core and mllib both to
>>> 1.1.0-SNAPSHOT...I
>>> > am still getting class cast exception:
>>> >
>>> > Exception in thread "main" org.apache.spark.SparkException: Job aborted
>>> > due to stage failure: Task 249 in stage 52.0 failed 4 times, most
>>> recent
>>> > failure: Lost task 249.3 in stage 52.0 (TID 10002,
>>> > tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>>> > scala.Tuple1 cannot be cast to scala.Product2
>>> >
>>> > I am running ALS.scala merged with my changes. I will try the mllib jar
>>> > without my changes next...
>>> >
>>> > Can this be due to the fact that my jars are compiled with Java 1.7_55
>>> but
>>> > the cluster JRE is at 1.7_45.
>>> >
>>> > Thanks.
>>> >
>>> > Deb
>>> >
>>> >
>>> >
>>> >
>>> > On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <
>>> debasish.das83@gmail.com>
>>> > wrote:
>>> >
>>> >> I did not play with Hadoop settings...everything is compiled with
>>> >> 2.3.0CDH5.0.2 for me...
>>> >>
>>> >> I did try to bump the version number of HBase from 0.94 to 0.96 or
>>> 0.98
>>> >> but there was no profile for CDH in the pom...but that's unrelated to
>>> this !
>>> >>
>>> >>
>>> >> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>> >>
>>> >>> One related question, is mllib jar independent from hadoop version
>>> >>> (doesnt use hadoop api directly)? Can I use mllib jar compile for one
>>> >>> version of hadoop and use it in another version of hadoop?
>>> >>>
>>> >>> Sent from my Google Nexus 5
>>> >>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com>
>>> wrote:
>>> >>>
>>> >>>> Hi Xiangrui,
>>> >>>>
>>> >>>> Maintaining another file will be a pain later so I deployed spark
>>> 1.0.1
>>> >>>> without mllib and then my application jar bundles mllib
>>> 1.1.0-SNAPSHOT
>>> >>>> along with the code changes for quadratic optimization...
>>> >>>>
>>> >>>> Later the plan is to patch the snapshot mllib with the deployed
>>> stable
>>> >>>> mllib...
>>> >>>>
>>> >>>> There are 5 variants that I am experimenting with around 400M
>>> ratings
>>> >>>> (daily data, monthly data I will update in few days)...
>>> >>>>
>>> >>>> 1. LS
>>> >>>> 2. NNLS
>>> >>>> 3. Quadratic with bounds
>>> >>>> 4. Quadratic with L1
>>> >>>> 5. Quadratic with equality and positivity
>>> >>>>
>>> >>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this
>>> step
>>> >>>> ALS.scala:311
>>> >>>>
>>> >>>> // Materialize usersOut and productsOut.
>>> >>>> usersOut.count()
>>> >>>>
>>> >>>> I am getting from one of the executors:
>>> java.lang.ClassCastException:
>>> >>>> scala.Tuple1 cannot be cast to scala.Product2
>>> >>>>
>>> >>>> I am debugging it further but I was wondering if this is due to RDD
>>> >>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>>> >>>>
>>> >>>> I have built the jars on my Mac which has Java 1.7.0_55 but the
>>> deployed
>>> >>>> cluster has Java 1.7.0_45.
>>> >>>>
>>> >>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can
>>> that
>>> >>>> Java
>>> >>>> version mismatch cause this ?
>>> >>>>
>>> >>>> Stack traces are below
>>> >>>>
>>> >>>> Thanks.
>>> >>>> Deb
>>> >>>>
>>> >>>>
>>> >>>> Executor stacktrace:
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>> >>>>
>>> >>>>
>>> >>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>> >>>>
>>> >>>>
>>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>> >>>>
>>> >>>>
>>> >>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>> >>>>
>>> >>>>
>>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>> >>>>
>>> >>>>
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>> >>>>
>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>> >>>>
>>> >>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>> >>>>
>>> >>>>
>>> >>>>
>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>> >>>>
>>> >>>>         java.lang.Thread.run(Thread.java:744)
>>> >>>>
>>> >>>> Driver stacktrace:
>>> >>>>
>>> >>>> at org.apache.spark.scheduler.DAGScheduler.org
>>> >>>>
>>> >>>>
>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>> >>>>
>>> >>>> at
>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>> >>>>
>>> >>>> at scala.Option.foreach(Option.scala:236)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>> >>>>
>>> >>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>> >>>>
>>> >>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>> >>>>
>>> >>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>> >>>>
>>> >>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>> >>>>
>>> >>>> at
>>> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> >>>>
>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>> >>>>
>>> >>>> at
>>> >>>>
>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>> >>>>
>>> >>>>  at
>>> >>>>
>>> >>>>
>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>> >>>>
>>> >>>>
>>> >>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <
>>> debasish.das83@gmail.com>
>>> >>>> wrote:
>>> >>>>
>>> >>>> > Hi Xiangrui,
>>> >>>> >
>>> >>>> > I used your idea and kept a cherry picked version of ALS.scala in
>>> my
>>> >>>> > application and call it ALSQp.scala...this is a OK workaround for
>>> now
>>> >>>> till
>>> >>>> > a version adds up to master for example...
>>> >>>> >
>>> >>>> > For the bug with userClassPathFirst, looks like Koert already
>>> found
>>> >>>> this
>>> >>>> > issue in the following JIRA:
>>> >>>> >
>>> >>>> > https://issues.apache.org/jira/browse/SPARK-1863
>>> >>>> >
>>> >>>> > By the way the userClassPathFirst feature is very useful since I
>>> am
>>> >>>> sure
>>> >>>> > the deployed version of spark on a production cluster will always
>>> be
>>> >>>> the
>>> >>>> > last stable (core at 1.0.1 in my case) and people would like to
>>> deploy
>>> >>>> > SNAPSHOT versions of libraries that build on top of spark core
>>> (mllib,
>>> >>>> > streaming etc)...
>>> >>>> >
>>> >>>> > Another way is to have a build option that deploys only the core
>>> and
>>> >>>> not
>>> >>>> > the libraries built upon core...
>>> >>>> >
>>> >>>> > Do we have an option like that in make-distribution script ?
>>> >>>> >
>>> >>>> > Thanks.
>>> >>>> > Deb
>>> >>>> >
>>> >>>> >
>>> >>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com>
>>> >>>> wrote:
>>> >>>> >
>>> >>>> >> If you cannot change the Spark jar deployed on the cluster, an
>>> easy
>>> >>>> >> solution would be renaming ALS in your jar. If userClassPathFirst
>>> >>>> >> doesn't work, could you create a JIRA and attach the log? Thanks!
>>> >>>> >> -Xiangrui
>>> >>>> >>
>>> >>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
>>> >>>> debasish.das83@gmail.com>
>>> >>>> >> wrote:
>>> >>>> >> > I created the assembly file but still it wants to pick the
>>> mllib
>>> >>>> from
>>> >>>> >> the
>>> >>>> >> > cluster:
>>> >>>> >> >
>>> >>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar |
>>> grep
>>> >>>> >> > QuadraticMinimizer
>>> >>>> >> >
>>> >>>> >> >
>>> >>>> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>>> >>>> >> >
>>> >>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>>> >>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>>> >>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>>> >>>> >> outputPath
>>> >>>> >> >
>>> >>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>>> >>>> >> >
>>> >>>> >>
>>> >>>>
>>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>>> >>>> >> >
>>> >>>> >> > Now if I force it to use the jar that I gave using
>>> >>>> >> > spark.files.userClassPathFirst, then it fails on some
>>> serialization
>>> >>>> >> > issues...
>>> >>>> >> >
>>> >>>> >> > A simple solution is to cherry pick the files I need from spark
>>> >>>> branch
>>> >>>> >> to
>>> >>>> >> > the application branch but I am not sure that's the right
>>> thing to
>>> >>>> do...
>>> >>>> >> >
>>> >>>> >> > The way userClassPathFirst is behaving, there might be bugs in
>>> >>>> it...
>>> >>>> >> >
>>> >>>> >> > Any suggestions will be appreciated....
>>> >>>> >> >
>>> >>>> >> > Thanks.
>>> >>>> >> > Deb
>>> >>>> >> >
>>> >>>> >> >
>>> >>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <
>>> mengxr@gmail.com>
>>> >>>> >> wrote:
>>> >>>> >> >>
>>> >>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be compatible
>>> with
>>> >>>> >> >> spark-core-1.0.1.
>>> >>>> >> >>
>>> >>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>>> >>>> >> debasish.das83@gmail.com>
>>> >>>> >> >> wrote:
>>> >>>> >> >> > Let me try it...
>>> >>>> >> >> >
>>> >>>> >> >> > Will this be fixed if I generate a assembly file with
>>> >>>> mllib-1.1.0
>>> >>>> >> >> > SNAPSHOT
>>> >>>> >> >> > jar and other dependencies with the rest of the application
>>> >>>> code ?
>>> >>>> >> >> >
>>> >>>> >> >> >
>>> >>>> >> >> >
>>> >>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <
>>> >>>> mengxr@gmail.com>
>>> >>>> >> wrote:
>>> >>>> >> >> >>
>>> >>>> >> >> >> You can try enabling "spark.files.userClassPathFirst". But
>>> I'm
>>> >>>> not
>>> >>>> >> >> >> sure whether it could solve your problem. -Xiangrui
>>> >>>> >> >> >>
>>> >>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>>> >>>> >> >> >> <debasish.das83@gmail.com>
>>> >>>> >> >> >> wrote:
>>> >>>> >> >> >> > Hi,
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I
>>> have
>>> >>>> new
>>> >>>> >> code
>>> >>>> >> >> >> > that
>>> >>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > I am trying to access the new code using spark-submit as
>>> >>>> follows:
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > spark-job --class
>>> >>>> com.verizon.bda.mllib.recommendation.ALSDriver
>>> >>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>>> >>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>>> >>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>>> >>>> --lambda
>>> >>>> >> 1.0
>>> >>>> >> >> >> > --qpProblem 2 inputPath outputPath
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > I can see the jars are getting added to httpServer as
>>> >>>> expected:
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >>>> >> >> >> >
>>> >>>> >>
>>> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>>> >>>> >> >> >> >
>>> >>>> >>
>>> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>>> >>>> >> >> >> > with
>>> >>>> >> >> >> > timestamp 1406998204236
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>>> >>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with
>>> >>>> >> timestamp
>>> >>>> >> >> >> > 1406998204237
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>> >>>> >> >> >> >
>>> file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>>> >>>> >> >> >> >
>>> http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>>> >>>> with
>>> >>>> >> >> >> > timestamp
>>> >>>> >> >> >> > 1406998204238
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > But the job still can't access code form mllib-1.1.0
>>> >>>> >> SNAPSHOT.jar...I
>>> >>>> >> >> >> > think
>>> >>>> >> >> >> > it's picking up the mllib from cluster which is at
>>> 1.0.1...
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > Please help. I will ask for a PR tomorrow but internally
>>> we
>>> >>>> want
>>> >>>> >> to
>>> >>>> >> >> >> > generate results from the new code.
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > Thanks.
>>> >>>> >> >> >> >
>>> >>>> >> >> >> > Deb
>>> >>>> >> >> >
>>> >>>> >> >> >
>>> >>>> >> >
>>> >>>> >> >
>>> >>>> >>
>>> >>>> >
>>> >>>> >
>>> >>>>
>>> >>>
>>> >>
>>> >
>>>
>>
>>
>

--001a11c248e06223fd05003c2033--

From dev-return-8831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 10 10:41:56 2014
Return-Path: <dev-return-8831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A59911806
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 10 Aug 2014 10:41:56 +0000 (UTC)
Received: (qmail 23166 invoked by uid 500); 10 Aug 2014 10:41:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23103 invoked by uid 500); 10 Aug 2014 10:41:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23092 invoked by uid 99); 10 Aug 2014 10:41:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 10:41:54 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of amnon.is@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 10:41:29 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <amnon.is@gmail.com>)
	id 1XGQZ6-0000sk-6U
	for dev@spark.incubator.apache.org; Sun, 10 Aug 2014 03:41:28 -0700
Date: Sun, 10 Aug 2014 03:41:28 -0700 (PDT)
From: amnonkhen <amnon.is@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407667288137-7795.post@n3.nabble.com>
Subject: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am loading a csv text file from s3 into spark, filtering and mapping the
records and writing the result to s3.

I have tried several input sizes: 100k rows, 1M rows & 3.5M rows. The former
two finish successfully while the latter (3.5M rows) hangs in some weird
state in which the job stages monitor web app (the one in port 4040) stops ,
and the command line console gets stuck and does not even respond to ctrl-c.
The Master's web monitoring app still responds and shows the state as
FINISHED.

In s3, I see an empty directory with a single zero-sized entry
_temporary_$folder$. The s3 url is given using the s3n:// protocol.

I did not see any error in the logs in the web console. I also tried several
cluster sizes (1 master + 1 worker, 1 master + 5 workers) and got to the
same state.

Has anyone encountered such an issue? Any idea what's going on?

I also posted this question to Stack Overflow: 
http://stackoverflow.com/questions/25226419/saveastextfile-to-s3-on-spark-does-not-work-just-hangs
<http://stackoverflow.com/questions/25226419/saveastextfile-to-s3-on-spark-does-not-work-just-hangs>  



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 10 19:13:08 2014
Return-Path: <dev-return-8832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9CD8611024
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 10 Aug 2014 19:13:08 +0000 (UTC)
Received: (qmail 84396 invoked by uid 500); 10 Aug 2014 19:13:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84359 invoked by uid 500); 10 Aug 2014 19:13:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 94918 invoked by uid 99); 10 Aug 2014 15:44:58 -0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of xuite627@gmail.com designates 209.85.216.51 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=5pqYDfUwMVolw0yW6AJJxh7Hfx0Pg+hQzJqUZVCzFeY=;
        b=B1t4UUXIAs+aQ0m5P2vCJSPEhyqnaqpmTPvPTtg0mUqS+shxmlXBC8GxUJ4KJjJLMo
         TfPgLwR1JEXvM9UVQf+0CHjLXtETUNY0GVx1FnBgc+JWIFmq2EM1hRd+kSinwiYPwk9h
         hfWM7PvB7fFc5icuX+DsAoE47tbUXbp4vylZeHBcfAw/2wdbTdLIbrqpURn93M+aOxBD
         wZqrN13kRAYKI1kYPu7+CVel0mSNGVzEXq8kODP1Kl7Hn9hMY8d4+oexrHtkXD+4kqP+
         zpvesEVBdPwzDlh/k+kbMR5wJdxFLTYAr29ZFdtJkt5glqXRJc4S++5IHt8rq+Z3D7mA
         8I1A==
X-Received: by 10.140.90.40 with SMTP id w37mr40796063qgd.52.1407685471900;
 Sun, 10 Aug 2014 08:44:31 -0700 (PDT)
MIME-Version: 1.0
From: =?UTF-8?B?5p2O5a6c6Iqz?= <xuite627@gmail.com>
Date: Sun, 10 Aug 2014 23:44:10 +0800
Message-ID: <CADzqkaAkGqCG5nmWKmhLzC0yfSjvW5Dia37yzf_BS1ZC7y2gUg@mail.gmail.com>
Subject: fair scheduler
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c11852821c3f05004850a6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c11852821c3f05004850a6
Content-Type: text/plain; charset=UTF-8

Hi

I am trying to switch from FIFO to FAIR with standalone mode.

my environment:
hadoop 1.2.1
spark 0.8.0 using stanalone mode

and i modified the code..........

ClusterScheduler.scala  -> System.getProperty("spark.scheduler.mode",
"FAIR"))
SchedulerBuilder.scala  ->
val DEFAULT_SCHEDULING_MODE = SchedulingMode.FAIR

LocalScheduler.scala ->
System.getProperty("spark.scheduler.mode", "FAIR)

spark-env.sh ->
export SPARK_JAVA_OPTS="-Dspark.scheduler.mode=FAIR"
export SPARK_JAVA_OPTS=" -Dspark.scheduler.mode=FAIR" ./run-example
org.apache.spark.examples.SparkPi spark://streaming1:7077


but it's not work
i want to switch from fifo to fair
how can i  do??

Regards
Crystal Lee

--001a11c11852821c3f05004850a6--

From dev-return-8833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 10 22:49:51 2014
Return-Path: <dev-return-8833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7202113CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 10 Aug 2014 22:49:50 +0000 (UTC)
Received: (qmail 42018 invoked by uid 500); 10 Aug 2014 22:49:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41966 invoked by uid 500); 10 Aug 2014 22:49:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41954 invoked by uid 99); 10 Aug 2014 22:49:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 22:49:49 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 10 Aug 2014 22:49:44 +0000
Received: by mail-pa0-f53.google.com with SMTP id rd3so9996969pab.40
        for <dev@spark.apache.org>; Sun, 10 Aug 2014 15:49:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=x8+YLs1POQgV3majW7EfLb207OCqlMh/r12MAmk7KRQ=;
        b=ZAwvc6CX8rnTIwAJx611O+nHlWbKjbtWwZTE3dpQ71K8Ogm/mrZ8oPJ5AbpLDQht+1
         fBIyLxW+2aZDVuumqrhFEAYzvVlva77CDo9bJcR2EwxvR/075F7SkvuVEvXxLUE5KnG9
         lC6Ia6VIU90l7TFBpRbvdThoztLguG0RyCcZhbI1ChrYwlghBqdM3TLRTMxxttfGjwc9
         JzH6EjOze4v0NWhgSBkQqyTV3PerPkhoXBa/8rXsSagHi+30g1UFhqhaf9+6YnoBpGdN
         suCLe3jfD83jIAi0UJtHggxnOZH5DKCjO5P9js/d2oY5XD5G1jieDD4wGP1dPnZEiifx
         jsHA==
X-Received: by 10.68.204.4 with SMTP id ku4mr38968849pbc.39.1407710963534;
        Sun, 10 Aug 2014 15:49:23 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id n3sm14534025pde.47.2014.08.10.15.49.16
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 10 Aug 2014 15:49:17 -0700 (PDT)
Date: Sun, 10 Aug 2014 15:49:16 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: =?utf-8?Q?=E6=9D=8E=E5=AE=9C=E8=8A=B3?= <xuite627@gmail.com>, 
 dev@spark.apache.org
Message-ID: <etPan.53e7f6ec.2ae8944a.97@mbp-3>
In-Reply-To: <CADzqkaAkGqCG5nmWKmhLzC0yfSjvW5Dia37yzf_BS1ZC7y2gUg@mail.gmail.com>
References: <CADzqkaAkGqCG5nmWKmhLzC0yfSjvW5Dia37yzf_BS1ZC7y2gUg@mail.gmail.com>
Subject: Re: fair scheduler
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53e7f6ec_625558ec_97"
X-Virus-Checked: Checked by ClamAV on apache.org

--53e7f6ec_625558ec_97
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi Crystal,

The fair scheduler is only for jobs running concurrently within the same =
SparkContext (i.e. within an application), not for separate applications =
on the standalone cluster manager. It has no effect there. To run more of=
 those concurrently, you need to set a cap on how many cores they each gr=
ab with spark.cores.max.

Matei

On August 10, 2014 at 12:13:08 PM, =E6=9D=8E=E5=AE=9C=E8=8A=B3 (xuite627=40=
gmail.com) wrote:

Hi =20

I am trying to switch from =46I=46O to =46AIR with standalone mode. =20

my environment: =20
hadoop 1.2.1 =20
spark 0.8.0 using stanalone mode =20

and i modified the code.......... =20

ClusterScheduler.scala -> System.getProperty(=22spark.scheduler.mode=22, =
=20
=22=46AIR=22)) =20
SchedulerBuilder.scala -> =20
val DE=46AULT=5FSCHEDULING=5FMODE =3D SchedulingMode.=46AIR =20

LocalScheduler.scala -> =20
System.getProperty(=22spark.scheduler.mode=22, =22=46AIR) =20

spark-env.sh -> =20
export SPARK=5FJAVA=5FOPTS=3D=22-Dspark.scheduler.mode=3D=46AIR=22 =20
export SPARK=5FJAVA=5FOPTS=3D=22 -Dspark.scheduler.mode=3D=46AIR=22 ./run=
-example =20
org.apache.spark.examples.SparkPi spark://streaming1:7077 =20


but it's not work =20
i want to switch from fifo to fair =20
how can i do=3F=3F =20

Regards =20
Crystal Lee =20

--53e7f6ec_625558ec_97--


From dev-return-8834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 09:00:46 2014
Return-Path: <dev-return-8834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5916D11F7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 09:00:46 +0000 (UTC)
Received: (qmail 21794 invoked by uid 500); 11 Aug 2014 09:00:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21736 invoked by uid 500); 11 Aug 2014 09:00:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21397 invoked by uid 99); 11 Aug 2014 09:00:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 09:00:44 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 09:00:40 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XGlSm-00014H-1h
	for dev@spark.incubator.apache.org; Mon, 11 Aug 2014 02:00:20 -0700
Date: Mon, 11 Aug 2014 02:00:20 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407747620040-7798.post@n3.nabble.com>
In-Reply-To: <1407531797021-7778.post@n3.nabble.com>
References: <1407531797021-7778.post@n3.nabble.com>
Subject: Re: spark-shell is broken! (bad option: '--master')
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

an issue 3 - 4 PR, spark dev community is really active :)

it seems currently spark-shell takes only some SUBMISSION_OPTS, but no
APPLICATION_OPTS

do you have plan to add some APPLICATION_OPTS or CLI_OPTS like
hive -e
hive -f
hive -hivevar

then we can use our scala code as scripts, run them direktly via
spark-shell, without compiling, building, packing and so on...

those APPLICATION_OPTS should be some augments for
org.apache.spark.repl.Main right?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-is-broken-bad-option-master-tp7778p7798.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 12:38:05 2014
Return-Path: <dev-return-8835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D512B11550
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 12:38:05 +0000 (UTC)
Received: (qmail 14516 invoked by uid 500); 11 Aug 2014 12:38:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14443 invoked by uid 500); 11 Aug 2014 12:38:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14431 invoked by uid 99); 11 Aug 2014 12:38:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 12:38:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of devl.development@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 12:37:37 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so8316623qgf.6
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 05:37:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=C59J6CvuDRBodqejfWGCedxZaogxpwYiVespCpXdwE4=;
        b=VKarJsuwUqj7JXU8qp2DtLNlFD3KoTv3cmS80/7fFjGsD8bdqqnsXbu8+bwyzsH9er
         8QTmm4iTUnacGaGx25MOOjXHJWCo4DT/Q4PlyQajoWP8yVXQXN+hgnlaGB/Hc9rpenqj
         0pSmvGP/JtjlhKLBxWwDd5e2TsvQ9zEART97ATzoA36uDDm/Fe37bLzaqL7UWgUYxD/G
         msZAsbAoU6CGztpur1FNhjeoBuN6FsG5Goay+kfmiKfGCYLuRgik9bf+ozHsoIliaxr+
         UomGu6HjPMmH206QNT3igxwcLuqFlxOCA5hN45AKgytX5HeYfhrCtWJntOt93foc9EXy
         0JaA==
MIME-Version: 1.0
X-Received: by 10.224.88.3 with SMTP id y3mr63929251qal.65.1407760656208; Mon,
 11 Aug 2014 05:37:36 -0700 (PDT)
Received: by 10.140.81.199 with HTTP; Mon, 11 Aug 2014 05:37:36 -0700 (PDT)
Date: Mon, 11 Aug 2014 13:37:36 +0100
Message-ID: <CAMQ+LQMbMGS8BiX+U8qn5++g+_26TLA939zZOAw==Y61YkDO7w@mail.gmail.com>
Subject: Spark Avro Generation
From: Devl Devel <devl.development@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3dbbcd79789050059d132
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3dbbcd79789050059d132
Content-Type: text/plain; charset=UTF-8

Hi

So far I've been managing to build Spark from source but since a change in
spark-streaming-flume I have no idea how to generate classes (e.g.
SparkFlumeProtocol) from the avro schema.

I have used sbt to run avro:generate (from the top level spark dir) but it
produces nothing - it just says:

> avro:generate
[success] Total time: 0 s, completed Aug 11, 2014 12:26:49 PM.

Please can someone send me their build.sbt or just tell me how to build
spark so that all avro files get generated as well?

Sorry for the noob question but I really have tried by best on this one!
Cheers

--001a11c3dbbcd79789050059d132--

From dev-return-8836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 14:52:39 2014
Return-Path: <dev-return-8836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A00271196E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 14:52:39 +0000 (UTC)
Received: (qmail 52803 invoked by uid 500); 11 Aug 2014 14:52:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52746 invoked by uid 500); 11 Aug 2014 14:52:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52728 invoked by uid 99); 11 Aug 2014 14:52:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 14:52:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 14:52:13 +0000
Received: by mail-wi0-f177.google.com with SMTP id ho1so4339877wib.4
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 07:52:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ayIdV+4ocoXs6UbY35OOMfNbGOY6WYLaNqxjfwrmEVM=;
        b=VqmcTBxXorSku17kx99MRPJnNBR/uEcV+aXL7/h98VdmpBSV2azrnufoVMXO+ta6dq
         QMTqfomjNsibvmoqcA7vd0Nbkm6IurtYG2kKwqC+zUYbE0MHlfj2soz+DFOWdmn9moBG
         lfOHUmT/0uu/fgmJuoktE4nH2j8lT86gRn9XXvn/+0GB9Vmh2SsocE5Vajr/Txex860E
         yIC7x7uOhnW00Nbc9eu9k9kkh1I/DQ+OCHtO+dDQC6aVFG11z5BvLpXuuzW0VnhZQTXb
         hE4QixyUKh6GVm2h6/xljOImFbdNV3J7DVKwhHiOrJf3+5do37fx9aWA0uW5VxXKPX05
         HbCQ==
X-Received: by 10.180.211.233 with SMTP id nf9mr25497598wic.33.1407768732884;
 Mon, 11 Aug 2014 07:52:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 11 Aug 2014 07:51:32 -0700 (PDT)
In-Reply-To: <CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
 <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
 <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com> <CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 11 Aug 2014 10:51:32 -0400
Message-ID: <CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c338d03fe38805005bb3a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c338d03fe38805005bb3a1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It looks like this script doesn't catch PRs that are opened and *then* have
the JIRA issue ID added to the name. Would it be easy to somehow have the
script trigger on PR name changes as well as PR creates?

Alternately, is there a reason we can't or don't want to use the plugin
mentioned below? (I'm assuming it covers cases like this, but I'm not sure.=
)

Nick



On Wed, Jul 23, 2014 at 12:52 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> By the way, it looks like there=E2=80=99s a JIRA plugin that integrates i=
t with
> GitHub:
>
>    -
>    https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.j=
ira-bitbucket-connector-plugin
>    -
>    https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+a=
nd+GitHub+accounts+to+JIRA
>
> It does the automatic linking and shows some additional information
> <https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.pl=
ugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.=
png>
> that might be nice to have for heavy JIRA users.
>
> Nick
> =E2=80=8B
>
>
> On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Yeah it needs to have SPARK-XXX in the title (this is the format we
>> request already). It just works with small synchronization script I
>> wrote that we run every five minutes on Jeknins that uses the Github
>> and Jenkins API:
>>
>>
>> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd13=
9232929
>>
>> - Patrick
>>
>> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
>> <nicholas.chammas@gmail.com> wrote:
>> > That's pretty neat.
>> >
>> > How does it work? Do we just need to put the issue ID (e.g. SPARK-1234=
)
>> > anywhere in the pull request?
>> >
>> > Nick
>> >
>> >
>> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >
>> >> Just a small note, today I committed a tool that will automatically
>> >> mirror pull requests to JIRA issues, so contributors will no longer
>> >> have to manually post a pull request on the JIRA when they make one.
>> >>
>> >> It will create a "link" on the JIRA and also make a comment to trigge=
r
>> >> an e-mail to people watching.
>> >>
>> >> This should make some things easier, such as avoiding accidental
>> >> duplicate effort on the same JIRA.
>> >>
>> >> - Patrick
>> >>
>>
>
>

--001a11c338d03fe38805005bb3a1--

From dev-return-8837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 16:32:38 2014
Return-Path: <dev-return-8837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CF5A11C7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 16:32:38 +0000 (UTC)
Received: (qmail 92039 invoked by uid 500); 11 Aug 2014 16:32:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91975 invoked by uid 500); 11 Aug 2014 16:32:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91964 invoked by uid 99); 11 Aug 2014 16:32:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 16:32:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 16:32:31 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so4534416igd.17
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 09:32:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=f/jnfkWUqq+hCnnjqS7vYcEswYVliJIFmt88EZ+fOqY=;
        b=Kekkvwa3OoTpk5Ux3M6n0UIlZU4Yn38jEFx5dtuBMrnM6DeNAGUPBUmiOUaAMUO3bV
         RxMpPn03MB4r++bGfhVwhQB9rHruRAtSi+VwZgVcVVlxUWzla6i3tSwWAEEJ0xjN1/1u
         xsxf+qEuxkhbRugBW0hDnxLNoUqIMcyw/4m88D7xEpia6ez5jsnjQ4JmGhy3tPOdk3qc
         pLe3nkwcmGk4dwMiZPWDdIjFh8ZG8vaOR3q4tAp4NVni3zFuziwcU4J31e1d4TLIQGCb
         UWxD9OZxd9Q9K7tj1ROhKtR1hYpU0uYx59RJ2zNG2/h3tTM0Y/Ljk89kwTOzwcVTwiEF
         LQSQ==
X-Gm-Message-State: ALoCoQlB+xj9oDdWHveS3gSlL+8zMStowNBa1cmuopeqas6m+y9o4vrS/x1Yms2kaBYPFT0phh89
MIME-Version: 1.0
X-Received: by 10.50.152.9 with SMTP id uu9mr31175885igb.32.1407774731356;
 Mon, 11 Aug 2014 09:32:11 -0700 (PDT)
Received: by 10.107.25.17 with HTTP; Mon, 11 Aug 2014 09:32:11 -0700 (PDT)
In-Reply-To: <CAMQ+LQMbMGS8BiX+U8qn5++g+_26TLA939zZOAw==Y61YkDO7w@mail.gmail.com>
References: <CAMQ+LQMbMGS8BiX+U8qn5++g+_26TLA939zZOAw==Y61YkDO7w@mail.gmail.com>
Date: Mon, 11 Aug 2014 09:32:11 -0700
Message-ID: <CAHbPYVbhbDn_peyAuXBBUVANnpB_XhuhK377rQOMjveAegH4hQ@mail.gmail.com>
Subject: Re: Spark Avro Generation
From: Hari Shreedharan <hshreedharan@cloudera.com>
To: Devl Devel <devl.development@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013c67c4c98f5905005d1899
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c67c4c98f5905005d1899
Content-Type: text/plain; charset=UTF-8

Jay running sbt compile or assembly should generate the sources.

On Monday, August 11, 2014, Devl Devel <devl.development@gmail.com> wrote:

> Hi
>
> So far I've been managing to build Spark from source but since a change in
> spark-streaming-flume I have no idea how to generate classes (e.g.
> SparkFlumeProtocol) from the avro schema.
>
> I have used sbt to run avro:generate (from the top level spark dir) but it
> produces nothing - it just says:
>
> > avro:generate
> [success] Total time: 0 s, completed Aug 11, 2014 12:26:49 PM.
>
> Please can someone send me their build.sbt or just tell me how to build
> spark so that all avro files get generated as well?
>
> Sorry for the noob question but I really have tried by best on this one!
> Cheers
>

--089e013c67c4c98f5905005d1899--

From dev-return-8838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 16:52:49 2014
Return-Path: <dev-return-8838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97EAE11D1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 16:52:49 +0000 (UTC)
Received: (qmail 50270 invoked by uid 500); 11 Aug 2014 16:52:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50222 invoked by uid 500); 11 Aug 2014 16:52:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50210 invoked by uid 99); 11 Aug 2014 16:52:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 16:52:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 16:52:44 +0000
Received: by mail-ob0-f171.google.com with SMTP id wm4so6295269obc.16
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 09:52:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=H87P7X00o8ekD6sM6qxkXlBLe+FGwBYZ7TTTkd2VZe4=;
        b=i1+dIefsgAvezq2M4ztVvRjABKLiFT3kGDkb7/88XjabrMSPHtVJACczE2uPFOYdco
         CIFqwfNVLqidbBn1RtraEk0mJI7Px65vMrgjk7kVOdvgDhBVEnk1l7hslriraT587E8w
         EoxFDiPTZERd3GORriJ0U9wZGEBo9RM6X6I/Os9aUzvLJciTcCXPYW3nkQ2B02oYBYwz
         XdDyqcEI/JZTzgn0gqs4+jquEnzyNAiwQs/DLDFgVtIVXLVpRUWriQRYiKMbiiE40EJ3
         LdRWLyTi3gnejxrRpP6JmrkNXOji+3oOCRWNk+b4uereTHBV7In5oOZzJpm5Li93fGXU
         qcKA==
MIME-Version: 1.0
X-Received: by 10.60.83.134 with SMTP id q6mr50877750oey.46.1407775944190;
 Mon, 11 Aug 2014 09:52:24 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 11 Aug 2014 09:52:24 -0700 (PDT)
In-Reply-To: <CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
	<CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
	<CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
	<CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
	<CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com>
Date: Mon, 11 Aug 2014 09:52:24 -0700
Message-ID: <CABPQxstBXJmwdPxawQEzTTGpqNfC7fTJpo2rp=9PSv3tp5Jc1g@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115fca213b1b005005d6156
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115fca213b1b005005d6156
Content-Type: text/plain; charset=ISO-8859-1

I spent some time on this and I'm not sure either of these is an option,
unfortunately.

We typically can't use custom JIRA plug-in's because this JIRA is
controlled by the ASF and we don't have rights to modify most things about
how it works (it's a large shared JIRA instance used by more than 50
projects). It's worth looking into whether they can do something. In
general we've tended to avoid going through ASF infra them whenever
possible, since they are generally overloaded and things move very slowly,
even if there are outages.

Here is the script we use to do the sync:
https://github.com/apache/spark/blob/master/dev/github_jira_sync.py

It might be possible to modify this to support post-hoc changes, but we'd
need to think about how to do so while minimizing function calls to the ASF
JIRA API, which I found are very slow.

- Patrick



On Mon, Aug 11, 2014 at 7:51 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> It looks like this script doesn't catch PRs that are opened and *then* have
> the JIRA issue ID added to the name. Would it be easy to somehow have the
> script trigger on PR name changes as well as PR creates?
>
> Alternately, is there a reason we can't or don't want to use the plugin
> mentioned below? (I'm assuming it covers cases like this, but I'm not
> sure.)
>
> Nick
>
>
>
> On Wed, Jul 23, 2014 at 12:52 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > By the way, it looks like there's a JIRA plugin that integrates it with
> > GitHub:
> >
> >    -
> >
> https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin
> >    -
> >
> https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+and+GitHub+accounts+to+JIRA
> >
> > It does the automatic linking and shows some additional information
> > <
> https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png
> >
> > that might be nice to have for heavy JIRA users.
> >
> > Nick
> > 
> >
> >
> > On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> >> Yeah it needs to have SPARK-XXX in the title (this is the format we
> >> request already). It just works with small synchronization script I
> >> wrote that we run every five minutes on Jeknins that uses the Github
> >> and Jenkins API:
> >>
> >>
> >>
> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139232929
> >>
> >> - Patrick
> >>
> >> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
> >> <nicholas.chammas@gmail.com> wrote:
> >> > That's pretty neat.
> >> >
> >> > How does it work? Do we just need to put the issue ID (e.g.
> SPARK-1234)
> >> > anywhere in the pull request?
> >> >
> >> > Nick
> >> >
> >> >
> >> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <pwendell@gmail.com
> >
> >> > wrote:
> >> >
> >> >> Just a small note, today I committed a tool that will automatically
> >> >> mirror pull requests to JIRA issues, so contributors will no longer
> >> >> have to manually post a pull request on the JIRA when they make one.
> >> >>
> >> >> It will create a "link" on the JIRA and also make a comment to
> trigger
> >> >> an e-mail to people watching.
> >> >>
> >> >> This should make some things easier, such as avoiding accidental
> >> >> duplicate effort on the same JIRA.
> >> >>
> >> >> - Patrick
> >> >>
> >>
> >
> >
>

--089e0115fca213b1b005005d6156--

From dev-return-8839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 17:09:40 2014
Return-Path: <dev-return-8839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 321B911D9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 17:09:40 +0000 (UTC)
Received: (qmail 88539 invoked by uid 500); 11 Aug 2014 17:09:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88486 invoked by uid 500); 11 Aug 2014 17:09:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87705 invoked by uid 99); 11 Aug 2014 17:09:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 17:09:38 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.212.165 as permitted sender)
Received: from [98.139.212.165] (HELO nm6.bullet.mail.bf1.yahoo.com) (98.139.212.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 17:09:11 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=rDiSCm4l4v/wvvcxMOBfZiZ0Y4EEIOAWtfh/HXkme6LdEcfEMwC01SxNYu1Cps3Vz7hQ7xpr8We7pbUn3B7+Lu3a3ly1Zi5UdsV8Ww+XZg6MxnyphmeOEVRWGhrpNmtiU5zpPiMTs+pwSVjA1Yuo6l7D9kUgob64UUXL6DAqhDHzvEry+eN3zGBxLOwM8j3kEDr7aQKQL29PRZB5o4T3ulxjyyAp4P/hsZD09tfqwKtvfWC8Kr80bkyjZRCKhYfFUQvCwdGMNGgfyzph9aTP0FHZAcUkpECfohKdVCjdt6lirijbnXknQqG7PgZRSfwclOV3nlc9xNt98BL8SSQxyQ==;
Received: from [66.196.81.174] by nm6.bullet.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 17:09:10 -0000
Received: from [98.139.213.15] by tm20.bullet.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 17:09:10 -0000
Received: from [127.0.0.1] by smtp115.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 17:09:10 -0000
X-Yahoo-Newman-Id: 6457.55170.bm@smtp115.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: ULipw9gVM1msjB0OUclnvOI_c4cNdJ.T9UbazAWOG_ptYEp
 YbF6i0J9yvJzxycyj0BJ3YrPK_1AdcgMCmn.9MjlRZW4DRSe.Sl8yCS16LDy
 pdHT_WUvbYKIYln6tdYOmVH_yRk3CLBRxahLCHTBjtIJsxYrUzoziHK0ILD9
 HWYG81ADd8AJzlH5K39Pjv9gFYXa2dv3T5QcfrYJLFlOc7xRKudEwpkPVDLT
 pe5EZavalb6.e9.3W5Sl50z6riYF1gcWM4zNoPNB7DV8.P7Sm23o002xHg2l
 TRQ2Q5Mm6_3mCRxUk6HzDm4z4WQKg7vbTGlFMPmUXv0fR1C_bmUUHDlKmeOM
 LU5IGxBNIkOG0im8.nkon377OPJ5TTie_SRZR.tbc2El_JsMS6lR40tvimRr
 ZfXBUWtisjJfJ9oSMiyH74wS57U2r4Ab_fAb_BVQWNtO78OPQyoFjF5wiPm8
 iQSHmgzPV_iA0huypp5DXqJBVCmvk9PPc4DGcZKHvQAFcirmgysUkJL55xJl
 GKLDb39b3VKRbYRAfGmvNja9SdTUNTYrY
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Spark Avro Generation
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
X-Mailer: iPhone Mail (11D257)
In-Reply-To: <CAHbPYVbhbDn_peyAuXBBUVANnpB_XhuhK377rQOMjveAegH4hQ@mail.gmail.com>
Date: Mon, 11 Aug 2014 09:09:07 -0800
Cc: Devl Devel <devl.development@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <13ABCF8A-7992-4997-88FD-00114B3A32CB@yahoo.com>
References: <CAMQ+LQMbMGS8BiX+U8qn5++g+_26TLA939zZOAw==Y61YkDO7w@mail.gmail.com> <CAHbPYVbhbDn_peyAuXBBUVANnpB_XhuhK377rQOMjveAegH4hQ@mail.gmail.com>
To: Hari Shreedharan <hshreedharan@cloudera.com>
X-Virus-Checked: Checked by ClamAV on apache.org

If you don't want to build the entire thing, you can also do

mvn generate-sources in externals/flume-sink

Thanks,
Ron

Sent from my iPhone

> On Aug 11, 2014, at 8:32 AM, Hari Shreedharan <hshreedharan@cloudera.com> w=
rote:
>=20
> Jay running sbt compile or assembly should generate the sources.
>=20
>> On Monday, August 11, 2014, Devl Devel <devl.development@gmail.com> wrote=
:
>>=20
>> Hi
>>=20
>> So far I've been managing to build Spark from source but since a change i=
n
>> spark-streaming-flume I have no idea how to generate classes (e.g.
>> SparkFlumeProtocol) from the avro schema.
>>=20
>> I have used sbt to run avro:generate (from the top level spark dir) but i=
t
>> produces nothing - it just says:
>>=20
>>> avro:generate
>> [success] Total time: 0 s, completed Aug 11, 2014 12:26:49 PM.
>>=20
>> Please can someone send me their build.sbt or just tell me how to build
>> spark so that all avro files get generated as well?
>>=20
>> Sorry for the noob question but I really have tried by best on this one!
>> Cheers
>>=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 19:57:54 2014
Return-Path: <dev-return-8840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B28CE11385
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 19:57:54 +0000 (UTC)
Received: (qmail 18467 invoked by uid 500); 11 Aug 2014 19:57:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18418 invoked by uid 500); 11 Aug 2014 19:57:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18009 invoked by uid 99); 11 Aug 2014 19:57:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 19:57:49 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.126 as permitted sender)
Received: from [98.139.213.126] (HELO nm30-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 19:57:41 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=mJjyH97GsVe2hqldAt9d9Dq8c71z5fBXHPXnOxRSnl1AsPeQwRrRtE9IssgjKdynVbyhtbJXBqDBfsEDMFjiKlRg4qcbxrggijo9Gq0phxPjMYJ8X9R8UU6Yh150KZX+HUKbfQbV+/b6fwSIG9HI6JZIdlduZs1rgykqNlFOMWvSVSei2pyJhCtzPPanPxoEl8ObeNVI3XR6GoqNPRD49h0fgDkG5D9XA3iCcBydM+WHpwDy0JOkL5f2EeFW+FJAkUcZ5MmKjJ3aH1mNof7fTSntUW8OGSKJXI8OVRnEDd7K4Ytwe14OjH/xmOG2Xwi6EwxoQ+wXMzDu3TTP8XjUXw==;
Received: from [98.139.215.143] by nm30.bullet.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 19:57:20 -0000
Received: from [98.139.213.12] by tm14.bullet.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 19:57:20 -0000
Received: from [127.0.0.1] by smtp112.mail.bf1.yahoo.com with NNFMP; 11 Aug 2014 19:57:20 -0000
X-Yahoo-Newman-Id: 803722.3825.bm@smtp112.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: kivOVvcVM1lGtDdQBzqNHHBZq_X086e._8SdKIdvw.J85X.
 YtD7B55TS1m1N5YZr2WXRA77YiDgsT5JoR7SkCjKy5QRJP.QBXnIIAgjdtOO
 4WAprSLScbiCZG55DBJouRmlUeY1UOEvwgUaU6cUli5enh5PTVdTMVWuWCcF
 l2dWMArDb.5n5hSgzs.zfipo7KukU15pJLMcBR.9WONAOzzUo.yyx04B3H4h
 H53P0eg5pVskjWB_d.xXhbZ4ApBlmUed.5M9KQzl.jKr1oTtJGg4RzexG_Tv
 On3QNxsciOp05vIB6ixMxJfKxfaRcA8LYMGLhyRtLt.Nz06Tq8cPd1.t2I5T
 JrMiuJtJe_gjrUb1M5BIgOYVXNZxBX_kWxXNE3GsYAkArr5tjfvn0vAMz5jw
 Uuw134m0SxKx1oxMqAGCAcQfNbX8dpzljwXpdvQtUAXZFXu5CbGmLEE9.p.m
 tAl6Ld2D6I1yw7knzLcRxvQuZKFXbA6u9.iuUO.2pgYAIf3kMMqZDgPFgTHU
 EWdxG1pF23at_pjm7GtlZN0haJOdjbg--
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
From: Ron's Yahoo! <zlgonzalez@yahoo.com.INVALID>
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
Subject: More productive compilation of spark code base
Message-Id: <86CCD1D1-0D20-40BF-9B8B-7DBA588236F2@yahoo.com>
Date: Mon, 11 Aug 2014 11:57:21 -0800
To: Dev <dev@spark.apache.org>
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,
  I=92ve been able to get things compiled on my environment, but I=92m =
noticing that it=92s been quite difficult in IntelliJ. It always =
recompiles everything when I try to run one test like BroadcastTest, for =
example, despite having compiled make-distribution previously. In =
eclipse, I have no such recompilation issues. IntelliJ unfortunately =
does not support auto compilation for Scala. It also doesn=92t seem as =
if IntelliJ knows that that there are classes that have already been =
compiled since it always opts to recompile everything. I=92m new to =
IntelliJ so it might really just be a lack of knowledge on my part.
  Can anyone share any tips on how they are productive compiling against =
the Spark code base using IntelliJ?

Thanks,
Ron=

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 20:11:25 2014
Return-Path: <dev-return-8841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8209E113EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 20:11:25 +0000 (UTC)
Received: (qmail 47524 invoked by uid 500); 11 Aug 2014 20:11:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47459 invoked by uid 500); 11 Aug 2014 20:11:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47444 invoked by uid 99); 11 Aug 2014 20:11:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 20:11:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 20:11:19 +0000
Received: by mail-ie0-f177.google.com with SMTP id at20so9905249iec.8
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 13:10:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qQLtRAF79m+piX/OTZ7CWNQLh0ykW3UPlCywJ/FWhCE=;
        b=kvq0JNqBGC/Xh2nw9CQFyMKGX4pQdJi6vGvhHn11GSBKK90gpDWi83N6GTJ00lxBc1
         t1kvXvJw0bTL+/KXkshfwlls55gJAnzb3aFq/LRJ4MZpBw8+YKunXuILhVkxpoIOpMN8
         IIlumNMv9XwmEHvHNgAjlL/a1V5RMfNzvvwG2JNUhAZ4UNIE3ZD9nOK3KQBBNyxJzBNa
         C+f6M8HNDk0bw5m5Jg5IJxBZRTnR50WwAe8v5EKx/PK7Hsu0zdrk7JbKk3hN0nMmmKSy
         uKwOdZxPwX5j5V/VSG6UYZ6FCwP3t5NRBPYlOdDy2ERCuAKZ209BldpcumKMdqyBKD5r
         rHoQ==
MIME-Version: 1.0
X-Received: by 10.50.1.17 with SMTP id 17mr32659516igi.3.1407787858612; Mon,
 11 Aug 2014 13:10:58 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Mon, 11 Aug 2014 13:10:58 -0700 (PDT)
In-Reply-To: <86CCD1D1-0D20-40BF-9B8B-7DBA588236F2@yahoo.com>
References: <86CCD1D1-0D20-40BF-9B8B-7DBA588236F2@yahoo.com>
Date: Mon, 11 Aug 2014 13:10:58 -0700
Message-ID: <CACkSZy3+PFC-ZcafyoJrd6ntnkZVfP0VuJeKQUzzxxwbhSrKjw@mail.gmail.com>
Subject: Re: More productive compilation of spark code base
From: Stephen Boesch <javadba@gmail.com>
To: "Ron's Yahoo!" <zlgonzalez@yahoo.com.invalid>
Cc: Dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f838ac53b559d0500602713
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f838ac53b559d0500602713
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Ron,
   A possible recommendation is to use maven for the entire process
(avoiding  the sbt artifacts/processing).  IJ is pretty solid in its maven
support.

a) mvn -DskipTests  -Pyarn -Phive -Phadoop-2.3 compile package
b)  Inside IJ:  Open the parent/root pom.xml as a new maven project
c)  Inside IJ: Build | Project
d) Enjoy running/debugging the individual scalatest classes
e) (No unnecessary recompilation pain after the initial build)

Also i found that when using maven there are no duplicate class directories
(whereas in sbt everything is doubled up under the project/target directory
for some reason).



2014-08-11 12:57 GMT-07:00 Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>:

> Hi,
>   I=E2=80=99ve been able to get things compiled on my environment, but I=
=E2=80=99m
> noticing that it=E2=80=99s been quite difficult in IntelliJ. It always re=
compiles
> everything when I try to run one test like BroadcastTest, for example,
> despite having compiled make-distribution previously. In eclipse, I have =
no
> such recompilation issues. IntelliJ unfortunately does not support auto
> compilation for Scala. It also doesn=E2=80=99t seem as if IntelliJ knows =
that that
> there are classes that have already been compiled since it always opts to
> recompile everything. I=E2=80=99m new to IntelliJ so it might really just=
 be a lack
> of knowledge on my part.
>   Can anyone share any tips on how they are productive compiling against
> the Spark code base using IntelliJ?
>
> Thanks,
> Ron
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--e89a8f838ac53b559d0500602713--

From dev-return-8842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 20:11:51 2014
Return-Path: <dev-return-8842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D29F113F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 20:11:51 +0000 (UTC)
Received: (qmail 51503 invoked by uid 500); 11 Aug 2014 20:11:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51446 invoked by uid 500); 11 Aug 2014 20:11:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51428 invoked by uid 99); 11 Aug 2014 20:11:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 20:11:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 20:11:23 +0000
Received: by mail-wi0-f169.google.com with SMTP id n3so6068994wiv.2
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 13:11:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ke86VZkUj9vIkD5SM57I7xZGrXbYQmsYSqriaYScRg8=;
        b=zJkZXkWJNTXTgR3NLKLKmibtASGr5Ucaur0HcY8pHMGmdm7kgGG4GMjjC1rOcoCE98
         LXTFNGyBWsa2TOjnHUtnAW8e6HYnYZWJgWipNL1j28MzDJcZsFKAWc9qazjs5jS+yGIX
         kNTHAq2AS3QEc2gi6jf6TCDn/W+vyqy19EK5x7bYZR5ZiPZi9PUBp7ngF08PAr4gOdLZ
         J9NhmzOu+q7UBVI4iZDzjDF3Z+byOR5wK4KorJjHYgwD6tVNzt6jdmmX6V1D1bW3zxno
         Ki8ZAGWyRlRnyduM57SEF2mrT6WiklseYtnLrL33LY7qHvZ+Y2PhJdqSrgtzkJuCNNCA
         MWZg==
X-Received: by 10.180.211.233 with SMTP id nf9mr27252726wic.33.1407787883354;
 Mon, 11 Aug 2014 13:11:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 11 Aug 2014 13:10:43 -0700 (PDT)
In-Reply-To: <CABPQxstBXJmwdPxawQEzTTGpqNfC7fTJpo2rp=9PSv3tp5Jc1g@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
 <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
 <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
 <CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
 <CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com> <CABPQxstBXJmwdPxawQEzTTGpqNfC7fTJpo2rp=9PSv3tp5Jc1g@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 11 Aug 2014 16:10:43 -0400
Message-ID: <CAOhmDzdAcd6W5jCmHKkut9ER8ZZ08yNZioNe=JWGbRLypvW6RQ@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c338d0b4e23305006028a2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c338d0b4e23305006028a2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for looking into this. I think little tools like this are super
helpful.

Would it hurt to open a request with INFRA to install/configure the
JIRA-GitHub plugin while we continue to use the Python script we have? I
wouldn't mind opening that JIRA issue with them.

Nick


On Mon, Aug 11, 2014 at 12:52 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> I spent some time on this and I'm not sure either of these is an option,
> unfortunately.
>
> We typically can't use custom JIRA plug-in's because this JIRA is
> controlled by the ASF and we don't have rights to modify most things abou=
t
> how it works (it's a large shared JIRA instance used by more than 50
> projects). It's worth looking into whether they can do something. In
> general we've tended to avoid going through ASF infra them whenever
> possible, since they are generally overloaded and things move very slowly=
,
> even if there are outages.
>
> Here is the script we use to do the sync:
> https://github.com/apache/spark/blob/master/dev/github_jira_sync.py
>
> It might be possible to modify this to support post-hoc changes, but we'd
> need to think about how to do so while minimizing function calls to the A=
SF
> JIRA API, which I found are very slow.
>
> - Patrick
>
>
>
> On Mon, Aug 11, 2014 at 7:51 AM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> It looks like this script doesn't catch PRs that are opened and *then*
>> have
>>
>> the JIRA issue ID added to the name. Would it be easy to somehow have th=
e
>> script trigger on PR name changes as well as PR creates?
>>
>> Alternately, is there a reason we can't or don't want to use the plugin
>> mentioned below? (I'm assuming it covers cases like this, but I'm not
>> sure.)
>>
>> Nick
>>
>>
>>
>> On Wed, Jul 23, 2014 at 12:52 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>> > By the way, it looks like there=E2=80=99s a JIRA plugin that integrate=
s it with
>> > GitHub:
>> >
>> >    -
>> >
>> https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.jir=
a-bitbucket-connector-plugin
>>
>> >    -
>> >
>> https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+and=
+GitHub+accounts+to+JIRA
>> >
>> > It does the automatic linking and shows some additional information
>> > <
>> https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.pl=
ugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.=
png
>> >
>>
>> > that might be nice to have for heavy JIRA users.
>> >
>> > Nick
>> >
>> >
>> >
>> > On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >
>> >> Yeah it needs to have SPARK-XXX in the title (this is the format we
>> >> request already). It just works with small synchronization script I
>> >> wrote that we run every five minutes on Jeknins that uses the Github
>> >> and Jenkins API:
>> >>
>> >>
>> >>
>> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd13=
9232929
>> >>
>> >> - Patrick
>> >>
>> >> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
>> >> <nicholas.chammas@gmail.com> wrote:
>> >> > That's pretty neat.
>> >> >
>> >> > How does it work? Do we just need to put the issue ID (e.g.
>> SPARK-1234)
>> >> > anywhere in the pull request?
>> >> >
>> >> > Nick
>> >> >
>> >> >
>> >> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <
>> pwendell@gmail.com>
>> >> > wrote:
>> >> >
>> >> >> Just a small note, today I committed a tool that will automaticall=
y
>> >> >> mirror pull requests to JIRA issues, so contributors will no longe=
r
>> >> >> have to manually post a pull request on the JIRA when they make on=
e.
>> >> >>
>> >> >> It will create a "link" on the JIRA and also make a comment to
>> trigger
>> >> >> an e-mail to people watching.
>> >> >>
>> >> >> This should make some things easier, such as avoiding accidental
>> >> >> duplicate effort on the same JIRA.
>> >> >>
>> >> >> - Patrick
>> >> >>
>> >>
>> >
>> >
>>
>
>

--001a11c338d0b4e23305006028a2--

From dev-return-8843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 23:17:46 2014
Return-Path: <dev-return-8843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3D7E11B0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 23:17:45 +0000 (UTC)
Received: (qmail 33711 invoked by uid 500); 11 Aug 2014 23:17:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33653 invoked by uid 500); 11 Aug 2014 23:17:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33640 invoked by uid 99); 11 Aug 2014 23:17:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:17:44 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:17:18 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XGyq5-0000T6-4j
	for dev@spark.incubator.apache.org; Mon, 11 Aug 2014 16:17:17 -0700
Date: Mon, 11 Aug 2014 16:17:17 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407799037095-7807.post@n3.nabble.com>
Subject: Spark testsuite error for hive 0.13.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am trying to change spark to support hive-0.13, but always met following
problem when running the test. My feeling is the test setup may need to
change, but don't know exactly. Who has the similar issue or is able to shed
light on it?

13:50:53.331 ERROR org.apache.hadoop.hive.ql.Driver: FAILED:
SemanticException [Error 10072]: Database does not exist: default
org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist:
default
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1302)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1291)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9944)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9180)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:391)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:291)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:944)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1009)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:880)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:870)
        at
org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
        at
org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:266)
        at
org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:83)
        at
org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
        at
org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
        at
org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:405)
        at
org.apache.spark.sql.hive.test.TestHiveContext$SqlCmd$$anonfun$cmd$1.apply$mcV$sp(TestHive.scala:164)
        at
org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
        at
org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
        at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at
org.apache.spark.sql.hive.test.TestHiveContext.loadTestTable(TestHive.scala:282)
        at
org.apache.spark.sql.hive.CachedTableSuite.<init>(CachedTableSuite.scala:28)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at java.lang.Class.newInstance(Class.java:374)
        at
org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:621)
        at sbt.ForkMain$Run$2.call(ForkMain.java:294)
        at sbt.ForkMain$Run$2.call(ForkMain.java:284)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Database does
not exist: default
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1298)
        ... 35 more



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-testsuite-error-for-hive-0-13-tp7807.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 23:49:35 2014
Return-Path: <dev-return-8844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AB2B11BF9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 23:49:35 +0000 (UTC)
Received: (qmail 7119 invoked by uid 500); 11 Aug 2014 23:49:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7055 invoked by uid 500); 11 Aug 2014 23:49:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7042 invoked by uid 99); 11 Aug 2014 23:49:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:49:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:49:28 +0000
Received: by mail-ie0-f178.google.com with SMTP id rd18so10404335iec.23
        for <dev@spark.apache.org>; Mon, 11 Aug 2014 16:49:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type:content-transfer-encoding;
        bh=3tOHfHY4uYKttF2nKCMBWGQ/5edvxlc4l/J+vJJrXZI=;
        b=ADYdS6KTAaAVE1GILS8iLjYdILj9Lfz8mrjq4mB6pbiAzPJBwn5zd7UL7r+TVnvfw4
         ehTsQHNzgQaqyDDsXQDBk5JpEHXAFlK7B3N47P9TLn0MAIsqJYYgHl/9lzUTaGB4KNHQ
         plRseXtBfQTQKBtZQQmcVn6DUmDj4nJbt3BWr1JA25qMpvNAkvYJSkbA09t6XEJrp0z2
         Pkzqf4XAs5UltRtdDN2tuOYn8X1EjOaPQCB3/a19Sa19K+gVp6RqohsE/vH0McKRuY2J
         iPXChP/KeoEU6W5ub9tMCkApIcNAQrRfZGtDtNwLi2WXuzv8MDBvJoZgnrQ9ry+s8tXi
         LjBg==
X-Gm-Message-State: ALoCoQmkdSBsuDHJS0Kge0T5V1oG+DtIjDk+QjnGnynrzErF42e9HuQV7jT+k1KD18mSIJyO35v7
X-Received: by 10.50.50.198 with SMTP id e6mr34157738igo.1.1407800948249; Mon,
 11 Aug 2014 16:49:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Mon, 11 Aug 2014 16:48:48 -0700 (PDT)
In-Reply-To: <86CCD1D1-0D20-40BF-9B8B-7DBA588236F2@yahoo.com>
References: <86CCD1D1-0D20-40BF-9B8B-7DBA588236F2@yahoo.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 12 Aug 2014 00:48:48 +0100
Message-ID: <CAMAsSdJManfRkft59Rc-83KSaQyFyobs6go=4DikxMkhKEzGhQ@mail.gmail.com>
Subject: Re: More productive compilation of spark code base
To: Dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Try setting it to handle incremental compilation of Scala by itself
(IntelliJ) and to run its own compile server. This is in global
settings, under the Scala settings. It seems to compile incrementally
for me when I change a file or two.

On Mon, Aug 11, 2014 at 8:57 PM, Ron's Yahoo!
<zlgonzalez@yahoo.com.invalid> wrote:
> Hi,
>   I=E2=80=99ve been able to get things compiled on my environment, but I=
=E2=80=99m noticing that it=E2=80=99s been quite difficult in IntelliJ. It =
always recompiles everything when I try to run one test like BroadcastTest,=
 for example, despite having compiled make-distribution previously. In ecli=
pse, I have no such recompilation issues. IntelliJ unfortunately does not s=
upport auto compilation for Scala. It also doesn=E2=80=99t seem as if Intel=
liJ knows that that there are classes that have already been compiled since=
 it always opts to recompile everything. I=E2=80=99m new to IntelliJ so it =
might really just be a lack of knowledge on my part.
>   Can anyone share any tips on how they are productive compiling against =
the Spark code base using IntelliJ?
>
> Thanks,
> Ron
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 11 23:50:33 2014
Return-Path: <dev-return-8845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58D3C11C04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Aug 2014 23:50:33 +0000 (UTC)
Received: (qmail 11713 invoked by uid 500); 11 Aug 2014 23:50:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11650 invoked by uid 500); 11 Aug 2014 23:50:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11639 invoked by uid 99); 11 Aug 2014 23:50:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:50:32 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Aug 2014 23:50:06 +0000
Received: by mail-ig0-f169.google.com with SMTP id r2so6440587igi.4
        for <dev@spark.incubator.apache.org>; Mon, 11 Aug 2014 16:50:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=qpewfX3mVouorZPLgJH1WOWMawDRTh0ORiBlS42KOfs=;
        b=TCbah+a9ALhZyWnnl/rznwkhCpdeoSHhurOwxPPVUBDDYPUNc2K+Yux6m6PndjSh0N
         nPmuAB4jNdqM+/Gvo+lApNaAhrLpGpQ+Ln3qDZH9oW8NId5N2ytB78vfNN0MnemoMTbG
         WL+lPE0UU0KmVMe6S6uUwqqofTACLmgUf1NR/52AbyTgjO8DihiNOGt5uwqSRMpbyZ40
         aWpCaeFbi1F1wnkuF7lTpyC1Fu1jrwut+3tVpcJ8UxEHE4b+j2OGfAmn5C3vsbomda4d
         IUZz6f6LuvcJHxPW1egij3wtiGyC5ShfnJ8ZYMaf//xEW46LuiYlvrS+WJteTKYw5qwD
         LKCw==
X-Gm-Message-State: ALoCoQmhYSGBPozdwE4LXN2vAgDmx7YH9t6YgGwHiWdBjifQkJxVqLJ4+AWLnu9tZScst1Q7S9i8
X-Received: by 10.50.126.100 with SMTP id mx4mr33726199igb.1.1407801005060;
 Mon, 11 Aug 2014 16:50:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Mon, 11 Aug 2014 16:49:44 -0700 (PDT)
In-Reply-To: <1407799037095-7807.post@n3.nabble.com>
References: <1407799037095-7807.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 12 Aug 2014 00:49:44 +0100
Message-ID: <CAMAsSdK1KSj+ah6Kvm=9+yUzjiAVT9kskUaqbh1rDYKrMSzcHQ@mail.gmail.com>
Subject: Re: Spark testsuite error for hive 0.13.
To: Zhan Zhang <zhazhan@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think this will work just by changing the version. Have a look
at: https://issues.apache.org/jira/browse/SPARK-2706

On Tue, Aug 12, 2014 at 12:17 AM, Zhan Zhang <zhazhan@gmail.com> wrote:
> I am trying to change spark to support hive-0.13, but always met following
> problem when running the test. My feeling is the test setup may need to
> change, but don't know exactly. Who has the similar issue or is able to shed
> light on it?
>
> 13:50:53.331 ERROR org.apache.hadoop.hive.ql.Driver: FAILED:
> SemanticException [Error 10072]: Database does not exist: default
> org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist:
> default
>         at
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1302)
>         at
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1291)
>         at
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9944)
>         at
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9180)
>         at
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
>         at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:391)
>         at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:291)
>         at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:944)
>         at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1009)
>         at org.apache.hadoop.hive.ql.Driver.run(Driver.java:880)
>         at org.apache.hadoop.hive.ql.Driver.run(Driver.java:870)
>         at
> org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
>         at
> org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:266)
>         at
> org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:83)
>         at
> org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
>         at
> org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
>         at
> org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:405)
>         at
> org.apache.spark.sql.hive.test.TestHiveContext$SqlCmd$$anonfun$cmd$1.apply$mcV$sp(TestHive.scala:164)
>         at
> org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
>         at
> org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
>         at
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>         at
> scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
>         at
> org.apache.spark.sql.hive.test.TestHiveContext.loadTestTable(TestHive.scala:282)
>         at
> org.apache.spark.sql.hive.CachedTableSuite.<init>(CachedTableSuite.scala:28)
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> Method)
>         at
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
>         at
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
>         at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
>         at java.lang.Class.newInstance(Class.java:374)
>         at
> org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:621)
>         at sbt.ForkMain$Run$2.call(ForkMain.java:294)
>         at sbt.ForkMain$Run$2.call(ForkMain.java:284)
>         at java.util.concurrent.FutureTask.run(FutureTask.java:262)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
> Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Database does
> not exist: default
>         at
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1298)
>         ... 35 more
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-testsuite-error-for-hive-0-13-tp7807.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 00:11:22 2014
Return-Path: <dev-return-8846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA2BA11CB7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 00:11:22 +0000 (UTC)
Received: (qmail 48088 invoked by uid 500); 12 Aug 2014 00:11:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48024 invoked by uid 500); 12 Aug 2014 00:11:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48013 invoked by uid 99); 12 Aug 2014 00:11:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 00:11:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 00:10:49 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XGzfo-0002Zo-Dj
	for dev@spark.incubator.apache.org; Mon, 11 Aug 2014 17:10:44 -0700
Date: Mon, 11 Aug 2014 17:10:44 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407802244418-7810.post@n3.nabble.com>
In-Reply-To: <CAMAsSdK1KSj+ah6Kvm=9+yUzjiAVT9kskUaqbh1rDYKrMSzcHQ@mail.gmail.com>
References: <1407799037095-7807.post@n3.nabble.com> <CAMAsSdK1KSj+ah6Kvm=9+yUzjiAVT9kskUaqbh1rDYKrMSzcHQ@mail.gmail.com>
Subject: Re: Spark testsuite error for hive 0.13.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Sean,

I change both the API and version because there are some incompatibility
with hive-0.13, and actually can do some basic operation with the real hive
environment. But the test suite always complain with no default database
message. No clue yet.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-testsuite-error-for-hive-0-13-tp7807p7810.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 01:17:02 2014
Return-Path: <dev-return-8847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 570FA11E95
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 01:17:02 +0000 (UTC)
Received: (qmail 98889 invoked by uid 500); 12 Aug 2014 01:17:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98825 invoked by uid 500); 12 Aug 2014 01:17:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98814 invoked by uid 99); 12 Aug 2014 01:17:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 01:17:00 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of saisai.shao@intel.com designates 192.55.52.115 as permitted sender)
Received: from [192.55.52.115] (HELO mga14.intel.com) (192.55.52.115)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 01:16:54 +0000
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
  by fmsmga103.fm.intel.com with ESMTP; 11 Aug 2014 18:09:11 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,862,1389772800"; 
   d="scan'208,217";a="371134844"
Received: from fmsmsx108.amr.corp.intel.com ([10.18.124.206])
  by FMSMGA003.fm.intel.com with ESMTP; 11 Aug 2014 18:13:19 -0700
Received: from fmsmsx119.amr.corp.intel.com (10.19.9.28) by
 FMSMSX108.amr.corp.intel.com (10.18.124.206) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Mon, 11 Aug 2014 18:16:33 -0700
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 FMSMSX119.amr.corp.intel.com (10.19.9.28) with Microsoft SMTP Server (TLS) id
 14.3.195.1; Mon, 11 Aug 2014 18:16:33 -0700
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.17]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.246]) with mapi id 14.03.0195.001;
 Tue, 12 Aug 2014 09:16:32 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Spark SQL unit test failed when sort-based shuffle is enabled
Thread-Topic: Spark SQL unit test failed when sort-based shuffle is enabled
Thread-Index: Ac+1yu+IcYwOC0AAQq6RzbDafPLrqQ==
Date: Tue, 12 Aug 2014 01:16:31 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F702707BA1@SHSMSX104.ccr.corp.intel.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_64474308D680D540A4D8151B0F7C03F702707BA1SHSMSX104ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_64474308D680D540A4D8151B0F7C03F702707BA1SHSMSX104ccrcor_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi folks,

I met several Spark SQL unit test failures when sort-based shuffle is enabl=
ed, seems Spark SQL uses GenericMutableRow which will make ExternalSorter's=
 internal buffer all referred to the same object, I guess GenericMutableRow=
 uses only one mutable object to represent different rows, this is OK for h=
ash-based shuffle because the row is directly written to file; but will be =
failed in sort-based shuffle because it will store the object to sort them.=
 I just opened a JIRA ticket for this, details can be seen in https://issue=
s.apache.org/jira/browse/SPARK-2967.

Any suggestion?

Thanks
Jerry

--_000_64474308D680D540A4D8151B0F7C03F702707BA1SHSMSX104ccrcor_--

From dev-return-8848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 05:36:25 2014
Return-Path: <dev-return-8848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B864C473
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 05:36:25 +0000 (UTC)
Received: (qmail 73284 invoked by uid 500); 12 Aug 2014 05:36:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73223 invoked by uid 500); 12 Aug 2014 05:36:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 8945 invoked by uid 99); 12 Aug 2014 04:57:01 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of chris.p.rigano@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Mon, 11 Aug 2014 21:56:34 -0700 (PDT)
From: crigano <chris.p.rigano@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407819394330-7812.post@n3.nabble.com>
Subject: New to Open Source and Sparc Would Like to Contribute
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am new at contributing, How is the best way to start out?

Thanks!

Chris



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/New-to-Open-Source-and-Sparc-Would-Like-to-Contribute-tp7812.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 05:37:37 2014
Return-Path: <dev-return-8849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 54616C47C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 05:37:37 +0000 (UTC)
Received: (qmail 76715 invoked by uid 500); 12 Aug 2014 05:37:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76657 invoked by uid 500); 12 Aug 2014 05:37:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76645 invoked by uid 99); 12 Aug 2014 05:37:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 05:37:36 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 05:37:31 +0000
Received: by mail-pa0-f49.google.com with SMTP id hz1so12206351pad.8
        for <dev@spark.incubator.apache.org>; Mon, 11 Aug 2014 22:37:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=bDNzHZenhDrLlFksp4L0+La+JlDzmM0qDUTDqRWw/GU=;
        b=mKI3CiqkzAI5ZH405RYcC5nvD+pEEdeRFXVNcTuWHSTi7CRPUoGOhCaGs79wJ5RjO2
         Dpux2fRY6VS9eSXPhQBNQntYO2tzy54Mzz14miuY4NYJWgOQMDRuWjo43bQ8CPbyfJ5E
         3MhvRSSd6PCnev1FEOc2/LxlmAoB7W9H1iXEZbsPPSTCJJjtN5mwg9QlVbS/pRXSRJAM
         Egbf3E0RMC5xUbRpPHS3yWMgZM64IACLJQ3qK+Nh0gFW2huCtGFEa9a+VUT4dNNHIsWi
         I0kGX2AmtPJL2P0N5Pb/pK7BAgELImIvrKW5ureSbn103YSiIXtK8a4o1frgvqeGJhTo
         SaTg==
X-Received: by 10.68.104.66 with SMTP id gc2mr2327192pbb.17.1407821830562;
        Mon, 11 Aug 2014 22:37:10 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id da14sm9149363pac.24.2014.08.11.22.37.07
        for <dev@spark.incubator.apache.org>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Mon, 11 Aug 2014 22:37:07 -0700 (PDT)
Date: Mon, 11 Aug 2014 22:37:06 -0700
From: Josh Rosen <rosenville@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <etPan.53e9a802.625558ec.8702@joshs-mbp>
In-Reply-To: <1407819394330-7812.post@n3.nabble.com>
References: <1407819394330-7812.post@n3.nabble.com>
Subject: Re: New to Open Source and Sparc Would Like to Contribute
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53e9a802_238e1f29_8702"
X-Virus-Checked: Checked by ClamAV on apache.org

--53e9a802_238e1f29_8702
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

The =E2=80=9CContributing to Spark=E2=80=9D guide is a good place to star=
t:=C2=A0https://cwiki.apache.org/confluence/display/SPARK/Contributing+to=
+Spark


On August 11, 2014 at 10:36:25 PM, crigano (chris.p.rigano=40gmail.com) w=
rote:

I am new at contributing, How is the best way to start out=3F =20

Thanks=21 =20

Chris =20



-- =20
View this message in context: http://apache-spark-developers-list.1001551=
.n3.nabble.com/New-to-Open-Source-and-Sparc-Would-Like-to-Contribute-tp78=
12.html =20
Sent from the Apache Spark Developers List mailing list archive at Nabble=
.com. =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--53e9a802_238e1f29_8702--


From dev-return-8850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 07:13:25 2014
Return-Path: <dev-return-8850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 667FAC78B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 07:13:25 +0000 (UTC)
Received: (qmail 69112 invoked by uid 500); 12 Aug 2014 07:13:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69053 invoked by uid 500); 12 Aug 2014 07:13:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69041 invoked by uid 99); 12 Aug 2014 07:13:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 07:13:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of devl.development@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 07:13:19 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so9391603qge.11
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 00:12:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=W5fP+YbC9xXGRroWT72Hx+TqY7xHpQ6satVAOENPm0s=;
        b=UaYCWJtL+e80r58oJUCtySgaCcy+qzjMIPW+bxmRq7NBPBkphySGPc6ncgov8wKgHx
         k3j6xRgyL/MSN0UDFWc/4oZMkSYlyJ3+jPUb4+ppajCgCWABMcGeQM3K5mbQXRMm/btL
         gdMcPO7/4qySr2i/bSK9Ku/7j7cLxckIrhTYrPcHw4f3NHkokGDD9pQXgRInwUOcsvrA
         peogro9FgnV5dQRJVgRQCyRNUXkyM4WLu1wFSfXTjwV2fzHK145GhG8ORs6Op0e67Zwy
         Mx92f+M93Go+k6uBXzryZij8DK+z6+wPkVCquqrY3CS4YVcnDp8CTQQBmKzCK8y0Angl
         O26g==
MIME-Version: 1.0
X-Received: by 10.229.178.138 with SMTP id bm10mr3770091qcb.16.1407827579066;
 Tue, 12 Aug 2014 00:12:59 -0700 (PDT)
Received: by 10.140.81.199 with HTTP; Tue, 12 Aug 2014 00:12:59 -0700 (PDT)
In-Reply-To: <13ABCF8A-7992-4997-88FD-00114B3A32CB@yahoo.com>
References: <CAMQ+LQMbMGS8BiX+U8qn5++g+_26TLA939zZOAw==Y61YkDO7w@mail.gmail.com>
	<CAHbPYVbhbDn_peyAuXBBUVANnpB_XhuhK377rQOMjveAegH4hQ@mail.gmail.com>
	<13ABCF8A-7992-4997-88FD-00114B3A32CB@yahoo.com>
Date: Tue, 12 Aug 2014 08:12:59 +0100
Message-ID: <CAMQ+LQNYxf3vn6YCRyT3vGCgRgczQnWC1RAeAHPJQNVsrFqYSg@mail.gmail.com>
Subject: Re: Spark Avro Generation
From: Devl Devel <devl.development@gmail.com>
To: Ron Gonzalez <zlgonzalez@yahoo.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2bb7ec1632105006966a0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2bb7ec1632105006966a0
Content-Type: text/plain; charset=UTF-8

Thanks very much that helps, not having to generate the entire build.


On Mon, Aug 11, 2014 at 6:09 PM, Ron Gonzalez <zlgonzalez@yahoo.com> wrote:

> If you don't want to build the entire thing, you can also do
>
> mvn generate-sources in externals/flume-sink
>
> Thanks,
> Ron
>
> Sent from my iPhone
>
> > On Aug 11, 2014, at 8:32 AM, Hari Shreedharan <hshreedharan@cloudera.com>
> wrote:
> >
> > Jay running sbt compile or assembly should generate the sources.
> >
> >> On Monday, August 11, 2014, Devl Devel <devl.development@gmail.com>
> wrote:
> >>
> >> Hi
> >>
> >> So far I've been managing to build Spark from source but since a change
> in
> >> spark-streaming-flume I have no idea how to generate classes (e.g.
> >> SparkFlumeProtocol) from the avro schema.
> >>
> >> I have used sbt to run avro:generate (from the top level spark dir) but
> it
> >> produces nothing - it just says:
> >>
> >>> avro:generate
> >> [success] Total time: 0 s, completed Aug 11, 2014 12:26:49 PM.
> >>
> >> Please can someone send me their build.sbt or just tell me how to build
> >> spark so that all avro files get generated as well?
> >>
> >> Sorry for the noob question but I really have tried by best on this one!
> >> Cheers
> >>
>

--001a11c2bb7ec1632105006966a0--

From dev-return-8851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 07:19:36 2014
Return-Path: <dev-return-8851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D1E3C7B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 07:19:36 +0000 (UTC)
Received: (qmail 90750 invoked by uid 500); 12 Aug 2014 07:19:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90696 invoked by uid 500); 12 Aug 2014 07:19:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90682 invoked by uid 99); 12 Aug 2014 07:19:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 07:19:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of devl.development@gmail.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 07:19:29 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so8828317qaq.29
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 00:19:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Kuxy4TrPpDOtyKVbAJEEuplbfHLzv47c/mu9GMOa8bo=;
        b=vux9XW4kHa6Nvb6A+qOL+x+W+jLKjrc9jBhPWUbMjPsqTSnTKQvPhRaGRZA1dLTcUR
         73wJdMiviqZHRxyEf3BtgDP27QyiSKt5Yyq1VRwcaKtOtNdMtV8mPWMtph/9qXwvAB/2
         DvOCZDx7Ic8Hr+jMho7Bc75gJ058/auTQXLQMbqkgFtje9rqGpVMCdeVdZCENglvBQrG
         /soVZjXa9J/tMtv7ahL/WonpvkoLWZXC7rO9l7k3mHJstoU8rqCCXMKQaVJVx/DlIuvZ
         9Z1u3fALZC7G94QLaf+9t5qnAfsMJ1T+mu6JYmy80Hoyy6Nly9mMQGLMWK9zP0B4y9yv
         nhuQ==
MIME-Version: 1.0
X-Received: by 10.140.103.75 with SMTP id x69mr3528301qge.17.1407827949322;
 Tue, 12 Aug 2014 00:19:09 -0700 (PDT)
Received: by 10.140.81.199 with HTTP; Tue, 12 Aug 2014 00:19:09 -0700 (PDT)
Date: Tue, 12 Aug 2014 08:19:09 +0100
Message-ID: <CAMQ+LQNhpAd_dP69bxphxv2Bq59tc4ePS6yz1yT_dQd8x6y19g@mail.gmail.com>
Subject: Compie error with XML elements
From: Devl Devel <devl.development@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c17ab6d30b1b0500697c49
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c17ab6d30b1b0500697c49
Content-Type: text/plain; charset=UTF-8

When compiling the master checkout of spark. The Intellij compile fails
with:

    Error:(45, 8) not found: value $scope
      <div class="row-fluid">
       ^
which is caused by HTML elements in classes like HistoryPage.scala:

    val content =
      <div class="row-fluid">
        <div class="span12">...

How can I compile these classes that have html node elements in them?

Thanks in advance.

--001a11c17ab6d30b1b0500697c49--

From dev-return-8852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 08:44:41 2014
Return-Path: <dev-return-8852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 582BACA4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 08:44:41 +0000 (UTC)
Received: (qmail 54973 invoked by uid 500); 12 Aug 2014 08:44:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54916 invoked by uid 500); 12 Aug 2014 08:44:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54904 invoked by uid 99); 12 Aug 2014 08:44:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 08:44:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 08:44:36 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so9581216wes.5
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 01:44:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=eeDya8qP//BudBPYXOVfDE9taNvLLWI9qutEDi5nG7k=;
        b=tSIfy0QJjzUKQI/aFVq7eZx6YfTKhvSazZlHtIkp/b15LjW9+48HCd0wht5YjhgV3s
         G4Xe35smRc83TTRAZVzdptRBRQSw7EtIVvCNPKPjeC/tp2kOLDSrUH4OtSUHMfdPR/Re
         mfs1btbdazLAch6VmwU74Jsd8dw3BmUbPXcYtY1cmFkZmhuznN3b96VcoVbUT0nXcw2n
         Dgi7lsW8FGSgIZqbL/n3Je0bSp8CcVQq40SxayXb5HLYsswDtXY9ZKdHuwZ+IrrFGKVm
         XQT+urRMvJawyNQcM/LAd8jgpa/+idbE5j1f4SgZJ0OxurbemMyD7q8hgBeKvRuzqxBU
         +9vg==
MIME-Version: 1.0
X-Received: by 10.180.79.72 with SMTP id h8mr30035805wix.55.1407833055024;
 Tue, 12 Aug 2014 01:44:15 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Tue, 12 Aug 2014 01:44:14 -0700 (PDT)
In-Reply-To: <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
Date: Tue, 12 Aug 2014 18:44:14 +1000
Message-ID: <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0442827a25de7c05006aadd5
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442827a25de7c05006aadd5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I've submitted a work-in-progress pull request for this issue that I'd like
feedback on.  See https://github.com/apache/spark/pull/1890 . I've also
submitted a pull request for the related issue that the exceptions hit when
trying to use a custom kryo registrator are being swallowed:
https://github.com/apache/spark/pull/1827

The approach in my pull request is to get the Worker processes to download
the application jars and add them to the Executor class path at launch
time. There are a couple of things that still need to be done before this
can be merged:
1. At the moment, the first time a task runs in the executor, the
application jars are downloaded again.  My solution here would be to make
the executor not download any jars that already exist.  Previously, the
driver & executor kept track of the timestamp of jar files and would
redownload 'updated' jars, however this never made sense as the previous
version of the updated jar may have already been loaded into the executor,
so the updated jar may have no effect.  As my current pull request removes
the timestamp for jars, just checking whether the jar exists will allow us
to avoid downloading the jars again.
2. Tests. :-)

A side-benefit of my pull request is that you will be able to use custom
serialisers that are distributed in a user jar.  Currently, the serialiser
instance is created in the Executor process before the first task is
received and therefore before any user jars are downloaded.  As this PR
adds user jars to the Executor process at launch time, this won't be an
issue.


On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com> wrote:

> See my comment on https://issues.apache.org/jira/browse/SPARK-2878 for
> the full stacktrace, but it's in the BlockManager/BlockManagerWorker wher=
e
> it's trying to fulfil a "getBlock" request for another node.  The objects
> that would be in the block haven't yet been serialised, and that then
> causes the deserialisation to happen on that thread.  See
> MemoryStore.scala:102.
>
>
> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>
>> I don't think it was a conscious design decision to not include the
>> application classes in the connection manager serializer. We should fix
>> that. Where is it deserializing data in that thread?
>>
>> 4 might make sense in the long run, but it adds a lot of complexity to
>> the code base (whole separate code base, task queue, blocking/non-blocki=
ng
>> logic within task threads) that can be error prone, so I think it is bes=
t
>> to stay away from that right now.
>>
>>
>>
>>
>>
>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <graham.dennis@gmail.com>
>> wrote:
>>
>>> Hi Spark devs,
>>>
>>> I=E2=80=99ve posted an issue on JIRA (
>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when
>>> using
>>> Kryo serialisation with a custom Kryo registrator to register custom
>>> classes with Kryo.  This is an insidious issue that non-deterministical=
ly
>>> causes Kryo to have different ID number =3D> class name maps on differe=
nt
>>> nodes, which then causes weird exceptions (ClassCastException,
>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>> deserialisation
>>> time.  I=E2=80=99ve created a reliable reproduction for the issue here:
>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>
>>> I=E2=80=99m happy to try and put a pull request together to try and add=
ress this,
>>> but it=E2=80=99s not obvious to me the right way to solve this and I=E2=
=80=99d like to
>>> get
>>> feedback / ideas on how to address this.
>>>
>>> The root cause of the problem is a "Failed to run spark.kryo.registrato=
r=E2=80=9D
>>> error which non-deterministically occurs in some executor processes
>>> during
>>> operation.  My custom Kryo registrator is in the application jar, and i=
t
>>> is
>>> accessible on the worker nodes.  This is demonstrated by the fact that
>>> most
>>> of the time the custom kryo registrator is successfully run.
>>>
>>> What=E2=80=99s happening is that Kryo serialisation/deserialisation is =
happening
>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D th=
read, which has
>>> the
>>> thread's class loader set to contain the application jar.  This happens
>>> in
>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what I ca=
n
>>> tell, it is only these threads that have access to the application jar
>>> (that contains the custom Kryo registrator).  However, the
>>> ConnectionManager threads sometimes need to serialise/deserialise objec=
ts
>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects haven=
=E2=80=99t previously been
>>> serialised.  As the ConnectionManager threads don=E2=80=99t have the ap=
plication
>>> jar available from their class loader, when it tries to look up the
>>> custom
>>> Kryo registrator, this fails.  Spark then swallows this exception, whic=
h
>>> results in a different ID number =E2=80=94> class mapping for this kryo=
 instance,
>>> and this then causes deserialisation errors later on a different node.
>>>
>>> A related issue to the issue reported in SPARK-2878 is that Spark
>>> probably
>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kryo
>>> registrators.
>>>  The user has explicitly specified this class, and if it
>>> deterministically
>>> can=E2=80=99t be found, then it may cause problems at serialisation /
>>> deserialisation time.  If only sometimes it can=E2=80=99t be found (as =
in this
>>> case), then it leads to a data corruption issue later on.  Either way,
>>> we=E2=80=99re better off dying due to the ClassNotFound exception earli=
er, than
>>> the
>>> weirder errors later on.
>>>
>>> I have some ideas on potential solutions to this issue, but I=E2=80=99m=
 keen for
>>> experienced eyes to critique these approaches:
>>>
>>> 1. The simplest approach to fixing this would be to just make the
>>> application jar available to the connection manager threads, but I=E2=
=80=99m
>>> guessing it=E2=80=99s a design decision to isolate the application jar =
to just
>>> the
>>> executor task runner threads.  Also, I don=E2=80=99t know if there are =
any other
>>> threads that might be interacting with kryo serialisation /
>>> deserialisation.
>>> 2. Before looking up the custom Kryo registrator, change the thread=E2=
=80=99s
>>> class
>>> loader to include the application jar, then restore the class loader
>>> after
>>> the kryo registrator has been run.  I don=E2=80=99t know if this would =
have any
>>> other side-effects.
>>> 3. Always serialise / deserialise on the existing TaskRunner threads,
>>> rather than delaying serialisation until later, when it can be done onl=
y
>>> if
>>> needed.  This approach would probably have negative performance
>>> consequences.
>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>> deserialisation that has the application jar on the class path.
>>>  Serialisation / deserialisation would be the only thing these threads
>>> do,
>>> and this would minimise conflicts / interactions between the applicatio=
n
>>> jar and other jars.
>>>
>>> #4 sounds like the best approach to me, but I think would require
>>> considerable knowledge of Spark internals, which is beyond me at presen=
t.
>>>  Does anyone have any better (and ideally simpler) ideas?
>>>
>>> Cheers,
>>>
>>> Graham
>>>
>>
>>
>

--f46d0442827a25de7c05006aadd5--

From dev-return-8853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 09:29:13 2014
Return-Path: <dev-return-8853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DCC9CBD4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 09:29:13 +0000 (UTC)
Received: (qmail 45429 invoked by uid 500); 12 Aug 2014 09:29:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45368 invoked by uid 500); 12 Aug 2014 09:29:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45350 invoked by uid 99); 12 Aug 2014 09:29:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 09:29:12 +0000
X-ASF-Spam-Status: No, hits=1.1 required=10.0
	tests=DATE_IN_PAST_06_12,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of fireflyc@163.com designates 220.181.12.15 as permitted sender)
Received: from [220.181.12.15] (HELO m12-15.163.com) (220.181.12.15)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 09:28:46 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Subject:From:Message-Id:Date:Mime-Version; bh=k1qZC
	1INg22Zqb1ptpknUrZRpMRS6bUlAFfUg0D51Os=; b=ZTMVMX6W3sNRaPiXpzgXl
	zE3+Hb9DQthoQNcAQS/C96sF6iNNdPpclA04FSqEU8Ug2W3dbHSE/FcYwIAPBL79
	idU8+9CaaSvUl5TfPJSHw7K+KTWiRsP0f87A4vBIpA6dAmjm5F36Kxmy2WApc9tH
	RBhRdyjzDz5JS5d0rHcC4Q=
Received: from [192.168.3.107] (unknown [116.236.234.102])
	by smtp11 (Coremail) with SMTP id D8CowED5un1C3ulTPs4FAA--.38S2;
	Tue, 12 Aug 2014 17:28:35 +0800 (CST)
X-Coremail-DSSMTP: 116.236.234.102
Content-Type: text/plain; charset=gb2312
Content-Transfer-Encoding: quoted-printable
Subject: Re: fair scheduler
References: <CADzqkaAkGqCG5nmWKmhLzC0yfSjvW5Dia37yzf_BS1ZC7y2gUg@mail.gmail.com> <etPan.53e7f6ec.2ae8944a.97@mbp-3>
From: fireflyc <fireflyc@163.com>
In-Reply-To: <etPan.53e7f6ec.2ae8944a.97@mbp-3>
Message-Id: <48FCE271-C081-47A0-AAA0-49523829CEAD@163.com>
Date: Tue, 12 Aug 2014 08:12:34 +0800
To: "dev@spark.apache.org" <dev@spark.apache.org>
Mime-Version: 1.0 (1.0)
X-Mailer: iPad Mail (11D257)
X-CM-TRANSID:D8CowED5un1C3ulTPs4FAA--.38S2
X-Coremail-Antispam: 1Uf129KBjvJXoW7tw45ur1xGw43Xr47Jry3XFb_yoW8GrW7pr
	ZYgrsFkw17GFy0yws293sYqFy5uF43Aw4Ut395JryDJ39YgFnFkw10yr15XFWxCrn7W3y3
	AFWUK3srWa4YvaUanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUUjbIjqfuFe4nvWSU5nxnvy2
	9KBjDUYxBIdaVFxhVjvjDU0xZFpf9x0zE1CJ9UUUUU=
X-Originating-IP: [116.236.234.102]
X-CM-SenderInfo: piluvwxo1fqiywtou0bp/xtbB0QnltVEAN4OwdAAAsp
X-Virus-Checked: Checked by ClamAV on apache.org

@Crystal
You can use spark on yarn. Yarn have fair scheduler,modified yarn-site.xml.

=B7=A2=D7=D4=CE=D2=B5=C4 iPad

> =D4=DA 2014=C4=EA8=D4=C211=C8=D5=A3=AC6:49=A3=ACMatei Zaharia <matei.zahar=
ia@gmail.com> =D0=B4=B5=C0=A3=BA
>=20
> Hi Crystal,
>=20
> The fair scheduler is only for jobs running concurrently within the same S=
parkContext (i.e. within an application), not for separate applications on t=
he standalone cluster manager. It has no effect there. To run more of those c=
oncurrently, you need to set a cap on how many cores they each grab with spa=
rk.cores.max.
>=20
> Matei
>=20
> On August 10, 2014 at 12:13:08 PM, =C0=EE=D2=CB=B7=BC (xuite627@gmail.com)=
 wrote:
>=20
> Hi =20
>=20
> I am trying to switch from FIFO to FAIR with standalone mode. =20
>=20
> my environment: =20
> hadoop 1.2.1 =20
> spark 0.8.0 using stanalone mode =20
>=20
> and i modified the code.......... =20
>=20
> ClusterScheduler.scala -> System.getProperty("spark.scheduler.mode", =20
> "FAIR")) =20
> SchedulerBuilder.scala -> =20
> val DEFAULT_SCHEDULING_MODE =3D SchedulingMode.FAIR =20
>=20
> LocalScheduler.scala -> =20
> System.getProperty("spark.scheduler.mode", "FAIR) =20
>=20
> spark-env.sh -> =20
> export SPARK_JAVA_OPTS=3D"-Dspark.scheduler.mode=3DFAIR" =20
> export SPARK_JAVA_OPTS=3D" -Dspark.scheduler.mode=3DFAIR" ./run-example =20=

> org.apache.spark.examples.SparkPi spark://streaming1:7077 =20
>=20
>=20
> but it's not work =20
> i want to switch from fifo to fair =20
> how can i do?? =20
>=20
> Regards =20
> Crystal Lee =20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 18:21:06 2014
Return-Path: <dev-return-8854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4F6B11CC4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 18:21:06 +0000 (UTC)
Received: (qmail 99044 invoked by uid 500); 12 Aug 2014 18:21:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98984 invoked by uid 500); 12 Aug 2014 18:21:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98972 invoked by uid 99); 12 Aug 2014 18:21:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 18:21:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 18:21:01 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so6292830wib.14
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mPQgKTf5sghP0pg8tIeTW9KghNqPaTw5iz4AwpGALek=;
        b=uJer2Q1Kvw9sjX1U20meoG5kAcWoYfp6NQbGP4uAf8GTEaFltBDizK1HPxpnyDQx0Q
         hhRd9Y9kM0phK+PQU1kr+H5xBKHqHzsEyo88ecdguWPy/j61Zu12q2LH/Tnv61LxFIwV
         7Nr0tFdTUzRU+zpyqB4bp/7lY4SGuSr5RpOG6Vf47PQs28kVg28bCPSjW1N9UwQKkyQz
         BK+2OkbvBHTN09TXMF+WejgWWfsaFhKj+WxKKbaTNFbILY8EDg4poGaajXdVq/vduG2l
         XZpwYbv17O9xkJeTFMHMcs0fuWxItFLGajsx4ASWRm0abSGNIzX6KUOw/otjfxUDdGCM
         H0BA==
MIME-Version: 1.0
X-Received: by 10.194.78.100 with SMTP id a4mr6669855wjx.106.1407867640255;
 Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
In-Reply-To: <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
	<CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
Date: Tue, 12 Aug 2014 14:20:40 -0400
Message-ID: <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd0582969c39050072ba6c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0582969c39050072ba6c
Content-Type: text/plain; charset=UTF-8

Hi all,

I wanted to follow up.

I have a prototype for an optimized version of hierarchical k-means.  I
wanted to get some feedback on my apporach.

Jeremy's implementation splits the largest cluster in each round.  Is it
better to do it that way or to split each cluster in half?

Are there are any open-source examples that are being widely used in
production?

Thanks!



On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> wrote:

> Nice to meet you, Jeremy!
>
> This is great!  Hierarchical clustering was next on my list --
> currently trying to get my PR for MiniBatch KMeans accepted.
>
> If it's cool with you, I'll try converting your code to fit in with
> the existing MLLib code as you suggest. I also need to review the
> Decision Tree code (as suggested above) to see how much of that can be
> reused.
>
> Maybe I can ask you to do a code review for me when I'm done?
>
>
>
>
>
> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
> <freeman.jeremy@gmail.com> wrote:
> > Hi all,
> >
> > Cool discussion! I agree that a more standardized API for clustering, and
> > easy access to underlying routines, would be useful (we've also been
> > discussing this when trying to develop streaming clustering algorithms,
> > similar to https://github.com/apache/spark/pull/1361)
> >
> > For divisive, hierarchical clustering I implemented something awhile
> back,
> > here's a gist.
> >
> > https://gist.github.com/freeman-lab/5947e7c53b368fe90371
> >
> > It does bisecting k-means clustering (with k=2), with a recursive class
> for
> > keeping track of the tree. I also found this much better than
> agglomerative
> > methods (for the reasons Hector points out).
> >
> > This needs to be cleaned up, and can surely be optimized (esp. by
> replacing
> > the core KMeans step with existing MLLib code), but I can say I was
> running
> > it successfully on quite large data sets.
> >
> > RJ, depending on where you are in your progress, I'd be happy to help
> work
> > on this piece and / or have you use this as a jumping off point, if
> useful.
> >
> > -- Jeremy
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>



-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bfd0582969c39050072ba6c--

From dev-return-8855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 18:21:07 2014
Return-Path: <dev-return-8855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B8C511CC5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 18:21:07 +0000 (UTC)
Received: (qmail 237 invoked by uid 500); 12 Aug 2014 18:21:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 183 invoked by uid 500); 12 Aug 2014 18:21:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 127 invoked by uid 99); 12 Aug 2014 18:21:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 18:21:06 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 18:20:40 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so10237047wgh.10
        for <dev@spark.incubator.apache.org>; Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mPQgKTf5sghP0pg8tIeTW9KghNqPaTw5iz4AwpGALek=;
        b=uJer2Q1Kvw9sjX1U20meoG5kAcWoYfp6NQbGP4uAf8GTEaFltBDizK1HPxpnyDQx0Q
         hhRd9Y9kM0phK+PQU1kr+H5xBKHqHzsEyo88ecdguWPy/j61Zu12q2LH/Tnv61LxFIwV
         7Nr0tFdTUzRU+zpyqB4bp/7lY4SGuSr5RpOG6Vf47PQs28kVg28bCPSjW1N9UwQKkyQz
         BK+2OkbvBHTN09TXMF+WejgWWfsaFhKj+WxKKbaTNFbILY8EDg4poGaajXdVq/vduG2l
         XZpwYbv17O9xkJeTFMHMcs0fuWxItFLGajsx4ASWRm0abSGNIzX6KUOw/otjfxUDdGCM
         H0BA==
MIME-Version: 1.0
X-Received: by 10.194.78.100 with SMTP id a4mr6669855wjx.106.1407867640255;
 Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Tue, 12 Aug 2014 11:20:40 -0700 (PDT)
In-Reply-To: <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
	<CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
Date: Tue, 12 Aug 2014 14:20:40 -0400
Message-ID: <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd0582969c39050072ba6c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0582969c39050072ba6c
Content-Type: text/plain; charset=UTF-8

Hi all,

I wanted to follow up.

I have a prototype for an optimized version of hierarchical k-means.  I
wanted to get some feedback on my apporach.

Jeremy's implementation splits the largest cluster in each round.  Is it
better to do it that way or to split each cluster in half?

Are there are any open-source examples that are being widely used in
production?

Thanks!



On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> wrote:

> Nice to meet you, Jeremy!
>
> This is great!  Hierarchical clustering was next on my list --
> currently trying to get my PR for MiniBatch KMeans accepted.
>
> If it's cool with you, I'll try converting your code to fit in with
> the existing MLLib code as you suggest. I also need to review the
> Decision Tree code (as suggested above) to see how much of that can be
> reused.
>
> Maybe I can ask you to do a code review for me when I'm done?
>
>
>
>
>
> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
> <freeman.jeremy@gmail.com> wrote:
> > Hi all,
> >
> > Cool discussion! I agree that a more standardized API for clustering, and
> > easy access to underlying routines, would be useful (we've also been
> > discussing this when trying to develop streaming clustering algorithms,
> > similar to https://github.com/apache/spark/pull/1361)
> >
> > For divisive, hierarchical clustering I implemented something awhile
> back,
> > here's a gist.
> >
> > https://gist.github.com/freeman-lab/5947e7c53b368fe90371
> >
> > It does bisecting k-means clustering (with k=2), with a recursive class
> for
> > keeping track of the tree. I also found this much better than
> agglomerative
> > methods (for the reasons Hector points out).
> >
> > This needs to be cleaned up, and can surely be optimized (esp. by
> replacing
> > the core KMeans step with existing MLLib code), but I can say I was
> running
> > it successfully on quite large data sets.
> >
> > RJ, depending on where you are in your progress, I'd be happy to help
> work
> > on this piece and / or have you use this as a jumping off point, if
> useful.
> >
> > -- Jeremy
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>



-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bfd0582969c39050072ba6c--

From dev-return-8856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 22:29:37 2014
Return-Path: <dev-return-8856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E111117CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 22:29:37 +0000 (UTC)
Received: (qmail 60379 invoked by uid 500); 12 Aug 2014 22:29:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60317 invoked by uid 500); 12 Aug 2014 22:29:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60294 invoked by uid 99); 12 Aug 2014 22:29:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 22:29:36 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 22:29:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XHKZ4-0003Tg-4k
	for dev@spark.incubator.apache.org; Tue, 12 Aug 2014 15:29:10 -0700
Date: Tue, 12 Aug 2014 15:29:10 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407882550139-7819.post@n3.nabble.com>
In-Reply-To: <1407802244418-7810.post@n3.nabble.com>
References: <1407799037095-7807.post@n3.nabble.com> <CAMAsSdK1KSj+ah6Kvm=9+yUzjiAVT9kskUaqbh1rDYKrMSzcHQ@mail.gmail.com> <1407802244418-7810.post@n3.nabble.com>
Subject: Re: Spark testsuite error for hive 0.13.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Problem solved by a walkaround with create database and use database.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-testsuite-error-for-hive-0-13-tp7807p7819.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 12 23:39:41 2014
Return-Path: <dev-return-8857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB1EE119EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Aug 2014 23:39:41 +0000 (UTC)
Received: (qmail 13859 invoked by uid 500); 12 Aug 2014 23:39:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13790 invoked by uid 500); 12 Aug 2014 23:39:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13777 invoked by uid 99); 12 Aug 2014 23:39:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 23:39:40 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Aug 2014 23:39:09 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so9908359qgf.20
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 16:39:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=K6GUFoJcmzAeAJIJSzrojxyUgT7NXWWVNNzrrS2GjUE=;
        b=DtoKga1j5IjtJKQi8Nwv9qz3dUeHslGsQNp/hr7DKd61KQRcaRl6lSQ4dnyuPRFTqv
         KyV+NxOpVgO2rQMtrafmNTXYQKvzby+KDXXgkAfAn91/no5qSOJ/m+wSEuszyXzgiBPr
         LDEqWosxdPBLyMzjQ/HgC/lmBix43Li9Y42stGvhpUsLK5j5VIQ4QZQ5cKQnMs1Msd6U
         KchmOT9gtaOX82LAP7VHlCSTD3iY50PHch3zkRH1zUesmPnQkPNo6+qkYCpFj58mDjVP
         KxhczYpgdMTbVLFsbCDv03FdzRezr++IDsMLb3scS3Ke+/y+Ls8D+bUjftnDPQ53tELG
         lVIA==
MIME-Version: 1.0
X-Received: by 10.140.86.6 with SMTP id o6mr1143938qgd.71.1407886747950; Tue,
 12 Aug 2014 16:39:07 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Tue, 12 Aug 2014 16:39:07 -0700 (PDT)
In-Reply-To: <CA+B-+fw0WP4arriG3Lgqpi4LEoeeERBiz1MUK_Q_vLssPqnE3g@mail.gmail.com>
References: <CA+B-+fzd7hZ1YbCs8PWLgSNbLeL-kyZbG3BhxJbh6QQzCX7NNQ@mail.gmail.com>
	<CAJgQjQ9bzmK8TG-OCKz=zD-V8EDaf05zy4K3rNgdGvNVRb-qSA@mail.gmail.com>
	<CA+B-+fzWF4ZjpyPWH8w4Bu2f-u7kaEbKTbKFUH0H=9JRV3N3-w@mail.gmail.com>
	<CAJgQjQ_aQwaNjDqew1OvfPOCZYoz-jU0iqk0g_zKC2pkqCS21g@mail.gmail.com>
	<CA+B-+fx4sBWg2AobpQm6ELEURg_rP4uTu-qGkymNvYSMioYMTw@mail.gmail.com>
	<CAJgQjQ97pkmAPNqHVRVB2ayE5i_sctM+y1S99kFs3c+dBr5snw@mail.gmail.com>
	<CA+B-+fz-6nS6-mPcEv=zm94ndiieAgniHjaWpro_PqONDoSYRw@mail.gmail.com>
	<CA+B-+fzLDkw=pPmCiCUcW+QEDJ12u8O870pAjz4p6OxNoG+OTQ@mail.gmail.com>
	<CAEYYnxb62V4=AEEM2k82kXfj5JKFJpgN9X38m+6LgmLTit=1ww@mail.gmail.com>
	<CA+B-+fxWRS7WvyN8veK_i0grqHVOwBX9i4s2a0Wfoii25zuzpg@mail.gmail.com>
	<CA+B-+fyrZXBRrQ9qHC-2RH2n0fpus5Cj65Cpwvnv_fmgXG+5Zg@mail.gmail.com>
	<CA+B-+fzQH8H+x8i6yrm+Ar8p+XjX6V2yEFvG4iMjC+=AqKjBFQ@mail.gmail.com>
	<CAFZt-ESs4+h7-qGT4PGHcaezmONp3PHYhTkhUyHqyqcV-uiz9g@mail.gmail.com>
	<CA+B-+fyQ0QXPk-nLQkgUdOaSYUSQp+-v9Mx2Rh+RSZxgQroMZQ@mail.gmail.com>
	<CA+B-+fw0WP4arriG3Lgqpi4LEoeeERBiz1MUK_Q_vLssPqnE3g@mail.gmail.com>
Date: Tue, 12 Aug 2014 16:39:07 -0700
Message-ID: <CA+B-+fxzxb5Gq5dSgK57GvoPsMtRYDVv+ZdBwSnTzYDR5hVtAQ@mail.gmail.com>
Subject: Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1
From: Debasish Das <debasish.das83@gmail.com>
To: Matt Forbes <matt@tellapart.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c131267f07e60500772dd7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c131267f07e60500772dd7
Content-Type: text/plain; charset=UTF-8

I figured out the issue....the driver memory was at 512 MB and for our
datasets, the following code needed more memory...

// Materialize usersOut and productsOut.

usersOut.count()

productsOut.count()

Thanks.

Deb


On Sat, Aug 9, 2014 at 6:12 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Actually nope it did not work fine...
>
> With multiple ALS iteration, I am getting the same error (with or without
> my mllib changes)....
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted
> due to stage failure: Task 206 in stage 52.0 failed 4 times, most recent
> failure: Lost task 206.3 in stage 52.0 (TID 9999,
> tblpmidn42adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
> scala.Tuple1 cannot be cast to scala.Product2
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)
>
>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>
>
> org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
>         scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)
>
>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>
>
> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>
>
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>
>         scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>
>
> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>
>         org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
>
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
>
>         org.apache.spark.scheduler.Task.run(Task.scala:54)
>
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
>
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>
>         java.lang.Thread.run(Thread.java:744)
>
> Driver stacktrace:
>
> at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>
> at scala.Option.foreach(Option.scala:236)
>
> at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
>
> at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
>
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>
> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>
> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
> at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
> The behavior is consistent in standalone and yarn mode...
>
> I am at the following checkin: commit
> ec79063fad44751a6689f5e58d47886babeaecff
>
> I also tested yarn deployment and I will use standalone mode to deploy
> stable spark release (1.0.1 right now) and all the mllib changes I can test
> on our datasets through yarn deployment...it works fine...
>
> By the way, let me try if I can reproduce this issue on MovieLensALS
> locally....Most likely it is a bug
>
> Thanks.
>
> Deb
>
>
> On Sat, Aug 9, 2014 at 11:12 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Including mllib inside assembly worked fine...If I deploy only the core
>> and send mllib as --jars then this problem shows up...
>>
>> Xiangrui could you please comment if it is a bug or expected behavior ? I
>> will create a JIRA if this needs to be tracked...
>>
>>
>> On Sat, Aug 9, 2014 at 11:01 AM, Matt Forbes <matt@tellapart.com> wrote:
>>
>>> I was having this same problem early this week and had to include my
>>> changes in the assembly.
>>>
>>>
>>> On Sat, Aug 9, 2014 at 9:59 AM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>>
>>>> I validated that I can reproduce this problem with master as well
>>>> (without
>>>> adding any of my mllib changes)...
>>>>
>>>> I separated mllib jar from assembly, deploy the assembly and then I
>>>> supply
>>>> the mllib jar as --jars option to spark-submit...
>>>>
>>>> I get this error:
>>>>
>>>> 14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at
>>>> ALS.scala:299
>>>>
>>>> Exception in thread "main" org.apache.spark.SparkException: Job aborted
>>>> due
>>>> to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
>>>> failure: Lost task 238.3 in stage 40.0 (TID 10002,
>>>> tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>>>> scala.Tuple1 cannot be cast to scala.Product2
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)
>>>>
>>>>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>>>
>>>>
>>>>
>>>> org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>>
>>>>
>>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>>
>>>>
>>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>>
>>>>
>>>>
>>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>>>
>>>>
>>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>>>
>>>>
>>>>
>>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>>
>>>>
>>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>>
>>>>
>>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>>>
>>>>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>>
>>>>
>>>> org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
>>>>
>>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>>>>
>>>>
>>>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>>>>
>>>>         org.apache.spark.scheduler.Task.run(Task.scala:54)
>>>>
>>>>
>>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
>>>>
>>>>
>>>>
>>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>>>
>>>>
>>>>
>>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>>>
>>>>         java.lang.Thread.run(Thread.java:744)
>>>>
>>>> Driver stacktrace:
>>>>
>>>> at org.apache.spark.scheduler.DAGScheduler.org
>>>>
>>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
>>>>
>>>> at
>>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>>
>>>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
>>>>
>>>> at scala.Option.foreach(Option.scala:236)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
>>>>
>>>> at
>>>>
>>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
>>>>
>>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>>>
>>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>>>
>>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>>>
>>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>>>
>>>> at
>>>>
>>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>>>
>>>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>
>>>> at
>>>>
>>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>>
>>>> at
>>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>>
>>>> at
>>>>
>>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>>
>>>> I will try now with mllib inside the assembly....If that works then
>>>> something is weird here !
>>>>
>>>>
>>>> On Sat, Aug 9, 2014 at 12:46 AM, Debasish Das <debasish.das83@gmail.com
>>>> >
>>>> wrote:
>>>>
>>>> > Hi Xiangrui,
>>>> >
>>>> > Based on your suggestion I moved core and mllib both to
>>>> 1.1.0-SNAPSHOT...I
>>>> > am still getting class cast exception:
>>>> >
>>>> > Exception in thread "main" org.apache.spark.SparkException: Job
>>>> aborted
>>>> > due to stage failure: Task 249 in stage 52.0 failed 4 times, most
>>>> recent
>>>> > failure: Lost task 249.3 in stage 52.0 (TID 10002,
>>>> > tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
>>>> > scala.Tuple1 cannot be cast to scala.Product2
>>>> >
>>>> > I am running ALS.scala merged with my changes. I will try the mllib
>>>> jar
>>>> > without my changes next...
>>>> >
>>>> > Can this be due to the fact that my jars are compiled with Java
>>>> 1.7_55 but
>>>> > the cluster JRE is at 1.7_45.
>>>> >
>>>> > Thanks.
>>>> >
>>>> > Deb
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > On Wed, Aug 6, 2014 at 12:01 PM, Debasish Das <
>>>> debasish.das83@gmail.com>
>>>> > wrote:
>>>> >
>>>> >> I did not play with Hadoop settings...everything is compiled with
>>>> >> 2.3.0CDH5.0.2 for me...
>>>> >>
>>>> >> I did try to bump the version number of HBase from 0.94 to 0.96 or
>>>> 0.98
>>>> >> but there was no profile for CDH in the pom...but that's unrelated
>>>> to this !
>>>> >>
>>>> >>
>>>> >> On Wed, Aug 6, 2014 at 9:45 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>>> >>
>>>> >>> One related question, is mllib jar independent from hadoop version
>>>> >>> (doesnt use hadoop api directly)? Can I use mllib jar compile for
>>>> one
>>>> >>> version of hadoop and use it in another version of hadoop?
>>>> >>>
>>>> >>> Sent from my Google Nexus 5
>>>> >>> On Aug 6, 2014 8:29 AM, "Debasish Das" <debasish.das83@gmail.com>
>>>> wrote:
>>>> >>>
>>>> >>>> Hi Xiangrui,
>>>> >>>>
>>>> >>>> Maintaining another file will be a pain later so I deployed spark
>>>> 1.0.1
>>>> >>>> without mllib and then my application jar bundles mllib
>>>> 1.1.0-SNAPSHOT
>>>> >>>> along with the code changes for quadratic optimization...
>>>> >>>>
>>>> >>>> Later the plan is to patch the snapshot mllib with the deployed
>>>> stable
>>>> >>>> mllib...
>>>> >>>>
>>>> >>>> There are 5 variants that I am experimenting with around 400M
>>>> ratings
>>>> >>>> (daily data, monthly data I will update in few days)...
>>>> >>>>
>>>> >>>> 1. LS
>>>> >>>> 2. NNLS
>>>> >>>> 3. Quadratic with bounds
>>>> >>>> 4. Quadratic with L1
>>>> >>>> 5. Quadratic with equality and positivity
>>>> >>>>
>>>> >>>> Now the ALS 1.1.0 snapshot runs fine but after completion on this
>>>> step
>>>> >>>> ALS.scala:311
>>>> >>>>
>>>> >>>> // Materialize usersOut and productsOut.
>>>> >>>> usersOut.count()
>>>> >>>>
>>>> >>>> I am getting from one of the executors:
>>>> java.lang.ClassCastException:
>>>> >>>> scala.Tuple1 cannot be cast to scala.Product2
>>>> >>>>
>>>> >>>> I am debugging it further but I was wondering if this is due to RDD
>>>> >>>> compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?
>>>> >>>>
>>>> >>>> I have built the jars on my Mac which has Java 1.7.0_55 but the
>>>> deployed
>>>> >>>> cluster has Java 1.7.0_45.
>>>> >>>>
>>>> >>>> The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can
>>>> that
>>>> >>>> Java
>>>> >>>> version mismatch cause this ?
>>>> >>>>
>>>> >>>> Stack traces are below
>>>> >>>>
>>>> >>>> Thanks.
>>>> >>>> Deb
>>>> >>>>
>>>> >>>>
>>>> >>>> Executor stacktrace:
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>> >>>>
>>>> >>>>
>>>> >>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>> >>>>
>>>> >>>>
>>>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
>>>> >>>>
>>>> >>>>
>>>> >>>> scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
>>>> >>>>
>>>> >>>>
>>>> >>>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>>>> >>>>
>>>> >>>>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>>> >>>>
>>>> >>>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>>> >>>>
>>>> >>>>         java.lang.Thread.run(Thread.java:744)
>>>> >>>>
>>>> >>>> Driver stacktrace:
>>>> >>>>
>>>> >>>> at org.apache.spark.scheduler.DAGScheduler.org
>>>> >>>>
>>>> >>>>
>>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>>> >>>>
>>>> >>>> at
>>>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
>>>> >>>>
>>>> >>>> at scala.Option.foreach(Option.scala:236)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
>>>> >>>>
>>>> >>>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>>> >>>>
>>>> >>>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>>> >>>>
>>>> >>>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>>> >>>>
>>>> >>>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>>> >>>>
>>>> >>>> at
>>>> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> >>>>
>>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>> >>>>
>>>> >>>> at
>>>> >>>>
>>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>> >>>>
>>>> >>>>  at
>>>> >>>>
>>>> >>>>
>>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>> >>>>
>>>> >>>>
>>>> >>>> On Tue, Aug 5, 2014 at 5:59 PM, Debasish Das <
>>>> debasish.das83@gmail.com>
>>>> >>>> wrote:
>>>> >>>>
>>>> >>>> > Hi Xiangrui,
>>>> >>>> >
>>>> >>>> > I used your idea and kept a cherry picked version of ALS.scala
>>>> in my
>>>> >>>> > application and call it ALSQp.scala...this is a OK workaround
>>>> for now
>>>> >>>> till
>>>> >>>> > a version adds up to master for example...
>>>> >>>> >
>>>> >>>> > For the bug with userClassPathFirst, looks like Koert already
>>>> found
>>>> >>>> this
>>>> >>>> > issue in the following JIRA:
>>>> >>>> >
>>>> >>>> > https://issues.apache.org/jira/browse/SPARK-1863
>>>> >>>> >
>>>> >>>> > By the way the userClassPathFirst feature is very useful since I
>>>> am
>>>> >>>> sure
>>>> >>>> > the deployed version of spark on a production cluster will
>>>> always be
>>>> >>>> the
>>>> >>>> > last stable (core at 1.0.1 in my case) and people would like to
>>>> deploy
>>>> >>>> > SNAPSHOT versions of libraries that build on top of spark core
>>>> (mllib,
>>>> >>>> > streaming etc)...
>>>> >>>> >
>>>> >>>> > Another way is to have a build option that deploys only the core
>>>> and
>>>> >>>> not
>>>> >>>> > the libraries built upon core...
>>>> >>>> >
>>>> >>>> > Do we have an option like that in make-distribution script ?
>>>> >>>> >
>>>> >>>> > Thanks.
>>>> >>>> > Deb
>>>> >>>> >
>>>> >>>> >
>>>> >>>> > On Tue, Aug 5, 2014 at 10:37 AM, Xiangrui Meng <mengxr@gmail.com
>>>> >
>>>> >>>> wrote:
>>>> >>>> >
>>>> >>>> >> If you cannot change the Spark jar deployed on the cluster, an
>>>> easy
>>>> >>>> >> solution would be renaming ALS in your jar. If
>>>> userClassPathFirst
>>>> >>>> >> doesn't work, could you create a JIRA and attach the log?
>>>> Thanks!
>>>> >>>> >> -Xiangrui
>>>> >>>> >>
>>>> >>>> >> On Tue, Aug 5, 2014 at 9:10 AM, Debasish Das <
>>>> >>>> debasish.das83@gmail.com>
>>>> >>>> >> wrote:
>>>> >>>> >> > I created the assembly file but still it wants to pick the
>>>> mllib
>>>> >>>> from
>>>> >>>> >> the
>>>> >>>> >> > cluster:
>>>> >>>> >> >
>>>> >>>> >> > jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar |
>>>> grep
>>>> >>>> >> > QuadraticMinimizer
>>>> >>>> >> >
>>>> >>>> >> >
>>>> >>>>
>>>> org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class
>>>> >>>> >> >
>>>> >>>> >> > /Users/v606014/dist-1.0.1/bin/spark-submit --master
>>>> >>>> >> > spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
>>>> >>>> >> > ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath
>>>> >>>> >> outputPath
>>>> >>>> >> >
>>>> >>>> >> > Exception in thread "main" java.lang.NoSuchMethodError:
>>>> >>>> >> >
>>>> >>>> >>
>>>> >>>>
>>>> org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;
>>>> >>>> >> >
>>>> >>>> >> > Now if I force it to use the jar that I gave using
>>>> >>>> >> > spark.files.userClassPathFirst, then it fails on some
>>>> serialization
>>>> >>>> >> > issues...
>>>> >>>> >> >
>>>> >>>> >> > A simple solution is to cherry pick the files I need from
>>>> spark
>>>> >>>> branch
>>>> >>>> >> to
>>>> >>>> >> > the application branch but I am not sure that's the right
>>>> thing to
>>>> >>>> do...
>>>> >>>> >> >
>>>> >>>> >> > The way userClassPathFirst is behaving, there might be bugs in
>>>> >>>> it...
>>>> >>>> >> >
>>>> >>>> >> > Any suggestions will be appreciated....
>>>> >>>> >> >
>>>> >>>> >> > Thanks.
>>>> >>>> >> > Deb
>>>> >>>> >> >
>>>> >>>> >> >
>>>> >>>> >> > On Sat, Aug 2, 2014 at 11:12 AM, Xiangrui Meng <
>>>> mengxr@gmail.com>
>>>> >>>> >> wrote:
>>>> >>>> >> >>
>>>> >>>> >> >> Yes, that should work. spark-mllib-1.1.0 should be
>>>> compatible with
>>>> >>>> >> >> spark-core-1.0.1.
>>>> >>>> >> >>
>>>> >>>> >> >> On Sat, Aug 2, 2014 at 10:54 AM, Debasish Das <
>>>> >>>> >> debasish.das83@gmail.com>
>>>> >>>> >> >> wrote:
>>>> >>>> >> >> > Let me try it...
>>>> >>>> >> >> >
>>>> >>>> >> >> > Will this be fixed if I generate a assembly file with
>>>> >>>> mllib-1.1.0
>>>> >>>> >> >> > SNAPSHOT
>>>> >>>> >> >> > jar and other dependencies with the rest of the application
>>>> >>>> code ?
>>>> >>>> >> >> >
>>>> >>>> >> >> >
>>>> >>>> >> >> >
>>>> >>>> >> >> > On Sat, Aug 2, 2014 at 10:46 AM, Xiangrui Meng <
>>>> >>>> mengxr@gmail.com>
>>>> >>>> >> wrote:
>>>> >>>> >> >> >>
>>>> >>>> >> >> >> You can try enabling "spark.files.userClassPathFirst".
>>>> But I'm
>>>> >>>> not
>>>> >>>> >> >> >> sure whether it could solve your problem. -Xiangrui
>>>> >>>> >> >> >>
>>>> >>>> >> >> >> On Sat, Aug 2, 2014 at 10:13 AM, Debasish Das
>>>> >>>> >> >> >> <debasish.das83@gmail.com>
>>>> >>>> >> >> >> wrote:
>>>> >>>> >> >> >> > Hi,
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > I have deployed spark stable 1.0.1 on the cluster but I
>>>> have
>>>> >>>> new
>>>> >>>> >> code
>>>> >>>> >> >> >> > that
>>>> >>>> >> >> >> > I added in mllib-1.1.0-SNAPSHOT.
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > I am trying to access the new code using spark-submit as
>>>> >>>> follows:
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > spark-job --class
>>>> >>>> com.verizon.bda.mllib.recommendation.ALSDriver
>>>> >>>> >> >> >> > --executor-memory 16g --total-executor-cores 16 --jars
>>>> >>>> >> >> >> > spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
>>>> >>>> >> >> >> > sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10
>>>> >>>> --lambda
>>>> >>>> >> 1.0
>>>> >>>> >> >> >> > --qpProblem 2 inputPath outputPath
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > I can see the jars are getting added to httpServer as
>>>> >>>> expected:
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >>>> >> >> >> >
>>>> >>>> >>
>>>> file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
>>>> >>>> >> >> >> >
>>>> >>>> >>
>>>> http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar
>>>> >>>> >> >> >> > with
>>>> >>>> >> >> >> > timestamp 1406998204236
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >>>> >> >> >> > file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
>>>> >>>> >> >> >> > http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar
>>>> with
>>>> >>>> >> timestamp
>>>> >>>> >> >> >> > 1406998204237
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > 14/08/02 12:50:04 INFO SparkContext: Added JAR
>>>> >>>> >> >> >> >
>>>> file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
>>>> >>>> >> >> >> >
>>>> http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar
>>>> >>>> with
>>>> >>>> >> >> >> > timestamp
>>>> >>>> >> >> >> > 1406998204238
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > But the job still can't access code form mllib-1.1.0
>>>> >>>> >> SNAPSHOT.jar...I
>>>> >>>> >> >> >> > think
>>>> >>>> >> >> >> > it's picking up the mllib from cluster which is at
>>>> 1.0.1...
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > Please help. I will ask for a PR tomorrow but
>>>> internally we
>>>> >>>> want
>>>> >>>> >> to
>>>> >>>> >> >> >> > generate results from the new code.
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > Thanks.
>>>> >>>> >> >> >> >
>>>> >>>> >> >> >> > Deb
>>>> >>>> >> >> >
>>>> >>>> >> >> >
>>>> >>>> >> >
>>>> >>>> >> >
>>>> >>>> >>
>>>> >>>> >
>>>> >>>> >
>>>> >>>>
>>>> >>>
>>>> >>
>>>> >
>>>>
>>>
>>>
>>
>

--001a11c131267f07e60500772dd7--

From dev-return-8858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 05:07:21 2014
Return-Path: <dev-return-8858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7836A11260
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 05:07:21 +0000 (UTC)
Received: (qmail 99191 invoked by uid 500); 13 Aug 2014 05:07:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99118 invoked by uid 500); 13 Aug 2014 05:07:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99102 invoked by uid 99); 13 Aug 2014 05:07:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 05:07:18 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 05:06:51 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so14613096vcb.1
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 22:06:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=Fhy3wR5EbX2q1L0bxjLqrdzbq6OXca+wtCr6fvBxsYM=;
        b=Ba/YYK/x/0Pwou4KODwCT8/4c5vyFy4heS68KGOfQj18kSeKSZ21Q6RqXZ07m+a5t4
         Do/0zp2QNXSN/8cRHKzy8I8vTpfTH/Mj98rqjSkbX6mRLX0P+XiCcaVl975DobndLROC
         ASHfaD0AR3YldAMsjtVWG7cG9DWb9YJIQmL874tXVRRaEOBssWqYpmnVA8JPfH2U33IL
         VNyrFQDmDtru67vVk1dqUaQsgPu1AnEdZONXgKmbPefHqJBmIjUXoyv579DAh/RXXkn6
         JQRoEtb9Dno9nAsvSvALtONPQrqeOCjsI3Yo8sFaHvw1Px9Ior+b0E7wEjenRAhYWyDk
         9iww==
X-Gm-Message-State: ALoCoQn8ZyJk7PL0Jk4UAccDhhNR1/GhhtVwr9IHF3t9+xaxiKpmZZg3u2OeNdK6BWwOxJhHQNB9
X-Received: by 10.220.49.10 with SMTP id t10mr1732979vcf.34.1407906409409;
        Tue, 12 Aug 2014 22:06:49 -0700 (PDT)
Received: from mail-vc0-f175.google.com (mail-vc0-f175.google.com [209.85.220.175])
        by mx.google.com with ESMTPSA id dr7sm2589509vdc.26.2014.08.12.22.06.48
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 12 Aug 2014 22:06:48 -0700 (PDT)
Received: by mail-vc0-f175.google.com with SMTP id ik5so14244869vcb.6
        for <dev@spark.apache.org>; Tue, 12 Aug 2014 22:06:48 -0700 (PDT)
X-Received: by 10.52.190.71 with SMTP id go7mr1458954vdc.28.1407906408057;
 Tue, 12 Aug 2014 22:06:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Tue, 12 Aug 2014 22:06:27 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 12 Aug 2014 22:06:27 -0700
Message-ID: <CA+-p3AEB-v3xmxbkw8Sq1ve0dSsTRxcz1_ppeuso26iKug1-2w@mail.gmail.com>
Subject: FileNotFoundException with _temporary in the name
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133ecec5453f905007bc11e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133ecec5453f905007bc11e
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

Several people on the mailing list have seen issues with
FileNotFoundExceptions related to _temporary in the name.  I've personally
observed this several times, as have a few of my coworkers on various Spark
clusters.

Any ideas what might be going on?

I've collected the various stack traces from various mailing list posts and
put their stacktraces and a link back to the original report at
https://issues.apache.org/jira/browse/SPARK-2984

Thanks!
Andrew

--001a1133ecec5453f905007bc11e--

From dev-return-8859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 13:20:31 2014
Return-Path: <dev-return-8859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B973311D62
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 13:20:31 +0000 (UTC)
Received: (qmail 1489 invoked by uid 500); 13 Aug 2014 13:20:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1425 invoked by uid 500); 13 Aug 2014 13:20:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1114 invoked by uid 99); 13 Aug 2014 13:20:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 13:20:27 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 13:20:02 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <yuu.ishikawa+spark@gmail.com>)
	id 1XHYTB-0006Ry-Jc
	for dev@spark.incubator.apache.org; Wed, 13 Aug 2014 06:20:01 -0700
Date: Wed, 13 Aug 2014 06:20:01 -0700 (PDT)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407936001588-7822.post@n3.nabble.com>
In-Reply-To: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I am also interested in specifying a common framework.
And I am trying to implement a hierarchical k-means and a hierarchical
clustering like single-link method with LSH.
https://issues.apache.org/jira/browse/SPARK-2966

If you have designed the standardized clustering algorithms API, please let
me know.


best,
Yu Ishikawa



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7822.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 18:10:02 2014
Return-Path: <dev-return-8860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 708EB1180B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 18:10:02 +0000 (UTC)
Received: (qmail 35554 invoked by uid 500); 13 Aug 2014 18:10:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35495 invoked by uid 500); 13 Aug 2014 18:10:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35473 invoked by uid 99); 13 Aug 2014 18:10:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 18:10:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 18:09:36 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so86408qaq.41
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 11:09:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=YT+0zNqrJb3Xk3sxajsUoC8OPhbcgOXsLE68Uj+vFLg=;
        b=PPIMQjBFKd6RQ5S+dqpT6+Y+8SDhLBgKLVMI+QNc6tWyKMUdc/d77wgb3WRmX5GVH1
         siRzGt8iyEOLqG5niKEVmRmJS2MHJ9hsAvdC+S9VxVTs4pVSOIRySgsGxYO/Fv/tGQDY
         CbRKBOy9uHrtFmextA/59U+jF0h1pejWq6Cmql7qcnDMLwSRA4jOlEnj9U2GEn4AtTzC
         5bWFaxW63VSFKqDmLl1oVQ6qJ6uDjFpGMvriqJtrGsPK4fJWqM501k9AJax1cavYlgqJ
         JyCIte8NiDhq6MgyhHRrJvv209Ozbvm/g2w8Y9G/KvRQrYOC5kSn3fUCT6kWsy7kJ97s
         GSzA==
MIME-Version: 1.0
X-Received: by 10.140.38.169 with SMTP id t38mr8861933qgt.3.1407953375042;
 Wed, 13 Aug 2014 11:09:35 -0700 (PDT)
Received: by 10.140.49.106 with HTTP; Wed, 13 Aug 2014 11:09:35 -0700 (PDT)
Date: Wed, 13 Aug 2014 11:09:35 -0700
Message-ID: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
Subject: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12ce6c7e0f1050086b0bc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12ce6c7e0f1050086b0bc
Content-Type: text/plain; charset=UTF-8

Has anyone had a chance to look at this paper (with title in subject)?
http://www.cs.rice.edu/~lp6/comparison.pdf

Interesting that they chose to use Python alone. Do we know how much faster
Scala is vs. Python in general, if at all?

As with any and all benchmarks, I'm sure there are caveats, but it'd be
nice to have a response to the question above for starters.

Thanks,
Ignacio

--001a11c12ce6c7e0f1050086b0bc--

From dev-return-8861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 19:30:06 2014
Return-Path: <dev-return-8861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A7CB11B8F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 19:30:06 +0000 (UTC)
Received: (qmail 3063 invoked by uid 500); 13 Aug 2014 19:30:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3001 invoked by uid 500); 13 Aug 2014 19:30:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2988 invoked by uid 99); 13 Aug 2014 19:30:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 19:30:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 19:29:38 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so197300qgd.13
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 12:29:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=78wmeke5byaVXKGrvCsTqwW0pc1LhnXDEWTNna6IxTE=;
        b=B0Py62RvKZ08Vrz79DiJjJ3mGvW2eaBttx2gbum0euuQAm0g6Bu1dHhnG/8wTR2eCY
         N8wfHZTX3pf+pWwtK17Elw0XZ9WS6Zka0BNDRhHmE4DDwQfFemQD2/mBGvIORRODzEzm
         M++Qo5yjoPtm8Pcl6xmNCrDDc/aSACjAhV4ifZ70b5rXpzqu+R6o0chG++VrbN5P1n3q
         nEy7O9WL/IuimyZB94UhRnxcehvHOitWQ1LfgQNuTxZUIMNDvPgbMz/VfD5qYo3YzVAW
         9jBo2PDrrtRmyMwxTnPFl+dTvp4kkt+fk5vURs5CoAZjMExwmowtLTHFupQ0VOJ3getb
         9HqA==
X-Gm-Message-State: ALoCoQll0KQ0Hq9fNv4ggxmM4tNdrt70fKL7yHSELvHhYuZyYrc9HPqYeyr0zHXxq0JRIdHRgIoI
X-Received: by 10.140.92.20 with SMTP id a20mr10018420qge.0.1407958176733;
 Wed, 13 Aug 2014 12:29:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 13 Aug 2014 12:29:16 -0700 (PDT)
In-Reply-To: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 13 Aug 2014 12:29:16 -0700
Message-ID: <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
To: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab95afbf08d050087ce43
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab95afbf08d050087ce43
Content-Type: text/plain; charset=UTF-8

They only compared their own implementations of couple algorithms on
different platforms rather than comparing the different platforms
themselves (in the case of Spark -- PySpark). I can write two variants of
an algorithm on Spark and make them perform drastically differently.

I have no doubt if you implement a ML algorithm in Python itself without
any native libraries, the performance will be sub-optimal.

What PySpark really provides is:

- Using Spark transformations in Python
- ML algorithms implemented in Scala (leveraging native numerical libraries
for high performance), and callable in Python

The paper claims "Python is now one of the most popular languages for
ML-oriented programming", and that's why they went ahead with Python.
However, as I understand, very few people actually implement algorithms in
Python directly because of the sub-optimal performance. Most people
implement algorithms in other languages (e.g. C / Java), and expose APIs in
Python for ease-of-use. This is what we are trying to do with PySpark as
well.


On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
ignacio.zendejas.cs@gmail.com> wrote:

> Has anyone had a chance to look at this paper (with title in subject)?
> http://www.cs.rice.edu/~lp6/comparison.pdf
>
> Interesting that they chose to use Python alone. Do we know how much faster
> Scala is vs. Python in general, if at all?
>
> As with any and all benchmarks, I'm sure there are caveats, but it'd be
> nice to have a response to the question above for starters.
>
> Thanks,
> Ignacio
>

--001a113ab95afbf08d050087ce43--

From dev-return-8862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 20:00:43 2014
Return-Path: <dev-return-8862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C1AEA11CCC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 20:00:43 +0000 (UTC)
Received: (qmail 76422 invoked by uid 500); 13 Aug 2014 20:00:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76356 invoked by uid 500); 13 Aug 2014 20:00:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76345 invoked by uid 99); 13 Aug 2014 20:00:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 20:00:41 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of freeman.jeremy@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 20:00:16 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <freeman.jeremy@gmail.com>)
	id 1XHeiV-0005Zu-G9
	for dev@spark.incubator.apache.org; Wed, 13 Aug 2014 13:00:15 -0700
Date: Wed, 13 Aug 2014 13:00:15 -0700 (PDT)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407960015482-7825.post@n3.nabble.com>
In-Reply-To: <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com> <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Our experience matches Reynold's comments; pure-Python implementations of
anything are generally sub-optimal compared to pure Scala implementations,
or Scala versions exposed to Python (which are faster, but still slower than
pure Scala). It also seems on first glance that some of the implementations
in the paper themselves might not have been optimal (regardless of Python vs
Scala).

All that said, we have found it useful to implement some workflows purely in
Python, mainly when we want to exploit libraries like NumPy, SciPy, or
Scikit Learn, or incorporate existing Python code bases, in which case the
flexibility is worth a drop in performance, at least for us! This might also
make more sense for specialized routines as opposed to core, low-level
algorithms.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/A-Comparison-of-Platforms-for-Implementing-and-Running-Very-Large-Scale-Machine-Learning-Algorithms-tp7823p7825.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 20:05:47 2014
Return-Path: <dev-return-8863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A42911D0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 20:05:47 +0000 (UTC)
Received: (qmail 91542 invoked by uid 500); 13 Aug 2014 20:05:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91477 invoked by uid 500); 13 Aug 2014 20:05:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91464 invoked by uid 99); 13 Aug 2014 20:05:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 20:05:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 20:05:20 +0000
Received: by mail-wi0-f174.google.com with SMTP id d1so7963682wiv.7
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 13:05:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=QCCOZK9n7eYLTPPzaUVdjYHHfdJn8a+NkH0fV5DzbjM=;
        b=ijM8kZZLIkisg7Ncp2i+qqK7MJot+updLN8jJ9pjojHfEyXiF1qGBvOLCLBWBDhszA
         Nmjmg28FCQU0dQOM1h2dllPfFuAy1kGQFQmoh1e4Qir92Ehe7MfIGGye1Yqkjt7i6LAt
         yEt7fXEPtaUlk8HBDLhJ1uFnVQ3cBr/TE7r/iiaNNegdf6CRUfdDngYO8rUp8+l+im/Q
         Gq1jeRnSvBh1SIMV1dw7x8A5EqbFSPulybHVYDnPTSzftSrHh0z3G5sVYjYnuCRJtsn/
         7OWHL4psMS1p1Amrnb0nEw2iwpddhrSe6wXSvkemQOn3z1BQtqYLHuzxGmZUHYFLyNoj
         ajjA==
X-Received: by 10.180.77.193 with SMTP id u1mr6864357wiw.45.1407960320015;
 Wed, 13 Aug 2014 13:05:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Wed, 13 Aug 2014 13:04:39 -0700 (PDT)
In-Reply-To: <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
 <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 13 Aug 2014 16:04:39 -0400
Message-ID: <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
To: Reynold Xin <rxin@databricks.com>
Cc: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043d6759bb86f10500884ea6
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043d6759bb86f10500884ea6
Content-Type: text/plain; charset=UTF-8

On a related note, I recently heard about Distributed R
<https://github.com/vertica/DistributedR>, which is coming out of
HP/Vertica and seems to be their proposition for machine learning at scale.

It would be interesting to see some kind of comparison between that and
MLlib (and perhaps also SparkR <https://github.com/amplab-extras/SparkR-pkg>?),
especially since Distributed R has a concept of distributed arrays and
works on data in-memory. Docs are here.
<https://github.com/vertica/DistributedR/tree/master/doc/platform>

Nick


On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com> wrote:

> They only compared their own implementations of couple algorithms on
> different platforms rather than comparing the different platforms
> themselves (in the case of Spark -- PySpark). I can write two variants of
> an algorithm on Spark and make them perform drastically differently.
>
> I have no doubt if you implement a ML algorithm in Python itself without
> any native libraries, the performance will be sub-optimal.
>
> What PySpark really provides is:
>
> - Using Spark transformations in Python
> - ML algorithms implemented in Scala (leveraging native numerical libraries
> for high performance), and callable in Python
>
> The paper claims "Python is now one of the most popular languages for
> ML-oriented programming", and that's why they went ahead with Python.
> However, as I understand, very few people actually implement algorithms in
> Python directly because of the sub-optimal performance. Most people
> implement algorithms in other languages (e.g. C / Java), and expose APIs in
> Python for ease-of-use. This is what we are trying to do with PySpark as
> well.
>
>
> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
> ignacio.zendejas.cs@gmail.com> wrote:
>
> > Has anyone had a chance to look at this paper (with title in subject)?
> > http://www.cs.rice.edu/~lp6/comparison.pdf
> >
> > Interesting that they chose to use Python alone. Do we know how much
> faster
> > Scala is vs. Python in general, if at all?
> >
> > As with any and all benchmarks, I'm sure there are caveats, but it'd be
> > nice to have a response to the question above for starters.
> >
> > Thanks,
> > Ignacio
> >
>

--f46d043d6759bb86f10500884ea6--

From dev-return-8864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:16:41 2014
Return-Path: <dev-return-8864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DB5911034
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:16:41 +0000 (UTC)
Received: (qmail 30629 invoked by uid 500); 13 Aug 2014 21:16:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30563 invoked by uid 500); 13 Aug 2014 21:16:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30017 invoked by uid 99); 13 Aug 2014 21:16:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:16:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:16:35 +0000
Received: by mail-qg0-f46.google.com with SMTP id z60so307567qgd.19
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:16:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Ey+Y4Th+z8QX6y2XBK3qSA64Y+xE4Ad1o6fJR5mYu/I=;
        b=cBbuq9AdaA5UwP9laZmlv9ULwztnQrzFmyifkER/yLU60jii7ve1lC7K5jHzgAgV90
         0iH1iUJT9w4T6103vCyiYqbBzLtpmmVzwWPJsZf3JIl76LOlVlskIBSkB27Ra9xDMQRG
         tLbCJtLlxrIFOR7D5lBprQTY1hn+v2vTcQ0D52KfeIkGwfpGBmTJhP6pQH+QZxK8IP+g
         c83Xqq4gkauSVO0jWkScV4hcJwO8hiKR8tK1B/HFn8UxNKCaOsU5Y6Iy/Eb+ge9Eimur
         9YRiecYRC0agpINYp508kmyF1jSLVKl0JxichRqD3lTk2rmQZEwiPerqG3fCdD4lLgsV
         cvZQ==
MIME-Version: 1.0
X-Received: by 10.229.74.74 with SMTP id t10mr11163979qcj.12.1407964574205;
 Wed, 13 Aug 2014 14:16:14 -0700 (PDT)
Received: by 10.140.49.106 with HTTP; Wed, 13 Aug 2014 14:16:14 -0700 (PDT)
In-Reply-To: <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
	<CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
	<CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
Date: Wed, 13 Aug 2014 14:16:14 -0700
Message-ID: <CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133b5c24d6c580500894c3f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133b5c24d6c580500894c3f
Content-Type: text/plain; charset=UTF-8

Yep, I thought it was a bogus comparison.

I should rephrase my question as it was poorly phrased: on average, how
much faster is Spark v. PySpark (I didn't really mean Scala v. Python)?
I've only used Spark and don't have a chance to test this at the moment so
if anybody has these numbers or general estimates (10x, etc), that'd be
great.

@Jeremy, if you can discuss this, what's an example of a project you
implemented using these libraries + PySpark?

Thanks everyone!




On Wed, Aug 13, 2014 at 1:04 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On a related note, I recently heard about Distributed R
> <https://github.com/vertica/DistributedR>, which is coming out of
> HP/Vertica and seems to be their proposition for machine learning at scale.
>
> It would be interesting to see some kind of comparison between that and
> MLlib (and perhaps also SparkR
> <https://github.com/amplab-extras/SparkR-pkg>?), especially since
> Distributed R has a concept of distributed arrays and works on data
> in-memory. Docs are here.
> <https://github.com/vertica/DistributedR/tree/master/doc/platform>
>
> Nick
>
>
> On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> They only compared their own implementations of couple algorithms on
>> different platforms rather than comparing the different platforms
>> themselves (in the case of Spark -- PySpark). I can write two variants of
>> an algorithm on Spark and make them perform drastically differently.
>>
>> I have no doubt if you implement a ML algorithm in Python itself without
>> any native libraries, the performance will be sub-optimal.
>>
>> What PySpark really provides is:
>>
>> - Using Spark transformations in Python
>> - ML algorithms implemented in Scala (leveraging native numerical
>> libraries
>> for high performance), and callable in Python
>>
>> The paper claims "Python is now one of the most popular languages for
>> ML-oriented programming", and that's why they went ahead with Python.
>> However, as I understand, very few people actually implement algorithms in
>> Python directly because of the sub-optimal performance. Most people
>> implement algorithms in other languages (e.g. C / Java), and expose APIs
>> in
>> Python for ease-of-use. This is what we are trying to do with PySpark as
>> well.
>>
>>
>> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
>> ignacio.zendejas.cs@gmail.com> wrote:
>>
>> > Has anyone had a chance to look at this paper (with title in subject)?
>> > http://www.cs.rice.edu/~lp6/comparison.pdf
>> >
>> > Interesting that they chose to use Python alone. Do we know how much
>> faster
>> > Scala is vs. Python in general, if at all?
>> >
>> > As with any and all benchmarks, I'm sure there are caveats, but it'd be
>> > nice to have a response to the question above for starters.
>> >
>> > Thanks,
>> > Ignacio
>> >
>>
>
>

--001a1133b5c24d6c580500894c3f--

From dev-return-8865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:17:15 2014
Return-Path: <dev-return-8865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5A7C1103D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:17:15 +0000 (UTC)
Received: (qmail 32684 invoked by uid 500); 13 Aug 2014 21:17:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32613 invoked by uid 500); 13 Aug 2014 21:17:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32601 invoked by uid 99); 13 Aug 2014 21:17:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:17:14 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:17:10 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so287709qaq.0
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:16:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZhoS3CP7FMgpqN94VfvnHN3iQfgNEdlaNkYQRqc7O0I=;
        b=h3bdF0PD9Xot533H+12A8LQtOZ44eXI1N3HgjIE3bQKhx/B1lzaUTP3I2yU9ZNmd+l
         +AC0pgnBp9CDqDWSskcN4sfgw6l1iXxO/IzwpiyOjD30QsxRVu5/lMVMTgVV0iJmtK3j
         zaLtEmSUCh5WwWlwbHewWMWCjZqIkQL9P9qWywWL0fjx9aqKkdmwGqTyrZsR+sxAOvAd
         i2HZnFYqKQyzErboCnH+kjcHNSJZnsIQvqA+V7/2+Slf1+Js93HQZmSU/EeYG/eEsgNF
         nnviS1MIjNQqW/IP/bo21KaxUnrP7humamEycqEer4ICwtcjMi5OA8c2kxoUgjKIyN3S
         +fEw==
X-Gm-Message-State: ALoCoQnm/uKR+Zb91OHPw5SB8AMeMt203wN4PZNbhw6Swv3JNZhrhEljXv7hhKSpBaJMOsNUOU4C
X-Received: by 10.224.129.68 with SMTP id n4mr10765543qas.86.1407964608825;
 Wed, 13 Aug 2014 14:16:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 13 Aug 2014 14:16:28 -0700 (PDT)
In-Reply-To: <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
 <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com> <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 13 Aug 2014 14:16:28 -0700
Message-ID: <CAPh_B=Z2w2rkAUnk=HhGx_1To0uarPx7MErvAXaMhctz3r4Bzg@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2bac45da9b20500894e85
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2bac45da9b20500894e85
Content-Type: text/plain; charset=UTF-8

Actually I believe the same person started both projects.

The Distributed R project from HP was started by Shivaram Venkataraman when
he was there. He since moved to Berkeley AMPLab to pursue a PhD and SparkR
was his latest project.



On Wed, Aug 13, 2014 at 1:04 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On a related note, I recently heard about Distributed R
> <https://github.com/vertica/DistributedR>, which is coming out of
> HP/Vertica and seems to be their proposition for machine learning at scale.
>
> It would be interesting to see some kind of comparison between that and
> MLlib (and perhaps also SparkR
> <https://github.com/amplab-extras/SparkR-pkg>?), especially since
> Distributed R has a concept of distributed arrays and works on data
> in-memory. Docs are here.
> <https://github.com/vertica/DistributedR/tree/master/doc/platform>
>
> Nick
>
>
> On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> They only compared their own implementations of couple algorithms on
>> different platforms rather than comparing the different platforms
>> themselves (in the case of Spark -- PySpark). I can write two variants of
>> an algorithm on Spark and make them perform drastically differently.
>>
>> I have no doubt if you implement a ML algorithm in Python itself without
>> any native libraries, the performance will be sub-optimal.
>>
>> What PySpark really provides is:
>>
>> - Using Spark transformations in Python
>> - ML algorithms implemented in Scala (leveraging native numerical
>> libraries
>> for high performance), and callable in Python
>>
>> The paper claims "Python is now one of the most popular languages for
>> ML-oriented programming", and that's why they went ahead with Python.
>> However, as I understand, very few people actually implement algorithms in
>> Python directly because of the sub-optimal performance. Most people
>> implement algorithms in other languages (e.g. C / Java), and expose APIs
>> in
>> Python for ease-of-use. This is what we are trying to do with PySpark as
>> well.
>>
>>
>> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
>> ignacio.zendejas.cs@gmail.com> wrote:
>>
>> > Has anyone had a chance to look at this paper (with title in subject)?
>> > http://www.cs.rice.edu/~lp6/comparison.pdf
>> >
>> > Interesting that they chose to use Python alone. Do we know how much
>> faster
>> > Scala is vs. Python in general, if at all?
>> >
>> > As with any and all benchmarks, I'm sure there are caveats, but it'd be
>> > nice to have a response to the question above for starters.
>> >
>> > Thanks,
>> > Ignacio
>> >
>>
>
>

--001a11c2bac45da9b20500894e85--

From dev-return-8866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:21:14 2014
Return-Path: <dev-return-8866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2708911051
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:21:14 +0000 (UTC)
Received: (qmail 42397 invoked by uid 500); 13 Aug 2014 21:21:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42330 invoked by uid 500); 13 Aug 2014 21:21:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42311 invoked by uid 99); 13 Aug 2014 21:21:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:12 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:08 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so280761qae.20
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:20:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=aQBmm7TI7gMeZzrKkHK+mhmfOFL544tXsRZTJ8FdKfU=;
        b=eNI8ic7oBr085LAqb05SRFHM5r7nY4XoEurIH4zQ8vav41bauR4MuhG22w/pDdc/nP
         hGtRUi3iR+YBlwhWf5+NROYofYnuqFUERITowdAcYQN4L/IDdbyFwEWDjVBiXqSBd0FL
         1yXQgVf/R+su1din/9urB3DBT6Jsq8ivXJj4FW1yoFJuRMqZof322mPMe9aO75x5deJH
         +Zr+DiD62SL6LczeZwVp5CkqLeKDBXwP0eSSwoCCRgKa94uaTJl6vOuEjzpcZoPBj+lk
         eNayHb/0V+CL+rh7Yblovhts6ItDM+P4nmx5UWnVAOeF1PHx26ZS5MVJL318pZSbeUDb
         0e4g==
X-Gm-Message-State: ALoCoQmhvgZS4YkMNBUewZMLWA/h3OFIfkT6ARlxSwozMrcmdbYn+CpC6/kdYulyHP7eqZeRG51G
X-Received: by 10.224.30.77 with SMTP id t13mr8240973qac.63.1407964845325;
 Wed, 13 Aug 2014 14:20:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 13 Aug 2014 14:20:24 -0700 (PDT)
In-Reply-To: <CAPh_B=Z2w2rkAUnk=HhGx_1To0uarPx7MErvAXaMhctz3r4Bzg@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
 <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
 <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com> <CAPh_B=Z2w2rkAUnk=HhGx_1To0uarPx7MErvAXaMhctz3r4Bzg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 13 Aug 2014 14:20:24 -0700
Message-ID: <CAPh_B=Y0-4_VU-QMc3pg6H345w4Xt4Fg5=gN_b2g1gv95QdbzQ@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bea4300765ec00500895c5b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bea4300765ec00500895c5b
Content-Type: text/plain; charset=UTF-8

BTW you can find the original Presto (rebranded as Distributed R) paper
here:
http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Venkataraman.pdf


On Wed, Aug 13, 2014 at 2:16 PM, Reynold Xin <rxin@databricks.com> wrote:

> Actually I believe the same person started both projects.
>
> The Distributed R project from HP was started by Shivaram Venkataraman
> when he was there. He since moved to Berkeley AMPLab to pursue a PhD and
> SparkR was his latest project.
>
>
>
> On Wed, Aug 13, 2014 at 1:04 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> On a related note, I recently heard about Distributed R
>> <https://github.com/vertica/DistributedR>, which is coming out of
>> HP/Vertica and seems to be their proposition for machine learning at scale.
>>
>> It would be interesting to see some kind of comparison between that and
>> MLlib (and perhaps also SparkR
>> <https://github.com/amplab-extras/SparkR-pkg>?), especially since
>> Distributed R has a concept of distributed arrays and works on data
>> in-memory. Docs are here.
>> <https://github.com/vertica/DistributedR/tree/master/doc/platform>
>>
>> Nick
>>
>>
>> On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> They only compared their own implementations of couple algorithms on
>>> different platforms rather than comparing the different platforms
>>> themselves (in the case of Spark -- PySpark). I can write two variants of
>>> an algorithm on Spark and make them perform drastically differently.
>>>
>>> I have no doubt if you implement a ML algorithm in Python itself without
>>> any native libraries, the performance will be sub-optimal.
>>>
>>> What PySpark really provides is:
>>>
>>> - Using Spark transformations in Python
>>> - ML algorithms implemented in Scala (leveraging native numerical
>>> libraries
>>> for high performance), and callable in Python
>>>
>>> The paper claims "Python is now one of the most popular languages for
>>> ML-oriented programming", and that's why they went ahead with Python.
>>> However, as I understand, very few people actually implement algorithms
>>> in
>>> Python directly because of the sub-optimal performance. Most people
>>> implement algorithms in other languages (e.g. C / Java), and expose APIs
>>> in
>>> Python for ease-of-use. This is what we are trying to do with PySpark as
>>> well.
>>>
>>>
>>> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
>>> ignacio.zendejas.cs@gmail.com> wrote:
>>>
>>> > Has anyone had a chance to look at this paper (with title in subject)?
>>> > http://www.cs.rice.edu/~lp6/comparison.pdf
>>> >
>>> > Interesting that they chose to use Python alone. Do we know how much
>>> faster
>>> > Scala is vs. Python in general, if at all?
>>> >
>>> > As with any and all benchmarks, I'm sure there are caveats, but it'd be
>>> > nice to have a response to the question above for starters.
>>> >
>>> > Thanks,
>>> > Ignacio
>>> >
>>>
>>
>>
>

--047d7bea4300765ec00500895c5b--

From dev-return-8867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:21:43 2014
Return-Path: <dev-return-8867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00D7F11053
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:21:43 +0000 (UTC)
Received: (qmail 43913 invoked by uid 500); 13 Aug 2014 21:21:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43843 invoked by uid 500); 13 Aug 2014 21:21:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43830 invoked by uid 99); 13 Aug 2014 21:21:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:42 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rcsenkbe@us.ibm.com designates 32.97.110.159 as permitted sender)
Received: from [32.97.110.159] (HELO e38.co.us.ibm.com) (32.97.110.159)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:36 +0000
Received: from /spool/local
	by e38.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <rcsenkbe@us.ibm.com>;
	Wed, 13 Aug 2014 15:21:15 -0600
Received: from d03dlp01.boulder.ibm.com (9.17.202.177)
	by e38.co.us.ibm.com (192.168.1.138) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Wed, 13 Aug 2014 15:21:13 -0600
Received: from b03cxnp07028.gho.boulder.ibm.com (b03cxnp07028.gho.boulder.ibm.com [9.17.130.15])
	by d03dlp01.boulder.ibm.com (Postfix) with ESMTPS id 0B8F01FF0045
	for <dev@spark.apache.org>; Wed, 13 Aug 2014 15:21:12 -0600 (MDT)
Received: from d03av06.boulder.ibm.com (d03av06.boulder.ibm.com [9.17.195.245])
	by b03cxnp07028.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id s7DLJTLf7471460
	for <dev@spark.apache.org>; Wed, 13 Aug 2014 23:19:29 +0200
Received: from d03av06.boulder.ibm.com (loopback [127.0.0.1])
	by d03av06.boulder.ibm.com (8.14.4/8.13.1/NCO v10.0 AVout) with ESMTP id s7DLPTwX011754
	for <dev@spark.apache.org>; Wed, 13 Aug 2014 15:25:29 -0600
Received: from d03nm127.boulder.ibm.com (d03nm127.boulder.ibm.com [9.63.33.48])
	by d03av06.boulder.ibm.com (8.14.4/8.13.1/NCO v10.0 AVin) with ESMTP id s7DLPTjx011746
	for <dev@spark.apache.org>; Wed, 13 Aug 2014 15:25:29 -0600
Subject: Added support for :cp <jar> to the Spark Shell
X-KeepSent: D6FADA5C:C1CB7363-87257D33:0073AA10;
 type=4; name=$KeepSent
To: "dev@spark.apache.org"@us.ibm.com
X-Mailer: IBM Notes Release 9.0.1 October 14, 2013
Message-ID: <OFD6FADA5C.C1CB7363-ON87257D33.0073AA10-86257D33.00754BC0@us.ibm.com>
From: Robert C Senkbeil <rcsenkbe@us.ibm.com>
Date: Wed, 13 Aug 2014 16:21:09 -0500
X-MIMETrack: Serialize by Router on D03NM127/03/M/IBM(Release 9.0.1FP1|April  03, 2014) at
 08/13/2014 15:21:09
MIME-Version: 1.0
Content-type: multipart/alternative; 
	Boundary="0__=08BBF7A0DFE02C808f9e8a93df938690918c08BBF7A0DFE02C80"
Content-Disposition: inline
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14081321-1344-0000-0000-000003726920
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=08BBF7A0DFE02C808f9e8a93df938690918c08BBF7A0DFE02C80
Content-type: text/plain; charset=US-ASCII
Content-transfer-encoding: quoted-printable



I've created a new pull request, which can be found at
https://github.com/apache/spark/pull/1929. Since Spark is using Scala
2.10.3 and there is a known issue with Scala 2.10.x not supporting the =
:cp
command (https://issues.scala-lang.org/browse/SI-6502), the Spark shell=

does not have the ability to add jars to the classpath after it has bee=
n
started.

The advantage of dynamically adding the jars versus restarting the
shell/interpreter (global) is that you can keep your shell's current st=
ate
(you don't lose your RDDs or anything). The previously-supported Scala
2.9.x implementation wiped the interpreter and replayed all of the
commands. This isn't ideal for Spark since some operations can still be=

quite heavy. Furthermore, if some operations involved loading external
data, there is the potential for said data to have changed if you repla=
y
the commands.

Signed,
Chip Senkbeil=

--0__=08BBF7A0DFE02C808f9e8a93df938690918c08BBF7A0DFE02C80--


From dev-return-8868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:21:45 2014
Return-Path: <dev-return-8868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3C0311054
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:21:45 +0000 (UTC)
Received: (qmail 44924 invoked by uid 500); 13 Aug 2014 21:21:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44859 invoked by uid 500); 13 Aug 2014 21:21:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44847 invoked by uid 99); 13 Aug 2014 21:21:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:21:41 +0000
Received: by mail-wg0-f48.google.com with SMTP id x13so287271wgg.19
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:21:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=P8jDBu1IwuVrwc3BHKtVAPSCV+cZ07CxJ7P5M6UunXE=;
        b=OrrXHWl9HpYjikBZp7K+tE2kC1bsO3T/TUIy9WWYf9RfOllftWPhqS6c09U20btdxt
         CBvoTUlqn58b5WB4rxD1f65cbyMN6KRkJrOoYVTIjWt9LYPSE1vPsWPTGZSz5PMSNVtz
         XuA+fIc9vY8ZBOCyaWlj3Olj8Dt1yqcH2YgFcYSZ06nqXQ6Y17UqzJCyL/Q6UaxXDRaI
         gZUMIs1UxtShD/gQPHbI/DBJ2ivNqqkyt/BcInYJKFKbth2rsoV30IrYYi8hniFgh/xe
         GhOhXKgEFtPIthAVo6Jz0jgYnDemC43JfuTOvg3d1fEUSH3A6DmMSAmEUGOyc194qEsA
         bMUg==
X-Gm-Message-State: ALoCoQkCKf0bkXsLj9+WTupj/IuolvRMAnySoIF8TSA/XqEvABPifB5zij5M0nq5qC2aTw67ZVXf
MIME-Version: 1.0
X-Received: by 10.180.20.40 with SMTP id k8mr7356600wie.54.1407964879609; Wed,
 13 Aug 2014 14:21:19 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.108.198 with HTTP; Wed, 13 Aug 2014 14:21:19 -0700 (PDT)
In-Reply-To: <CAPh_B=Z2w2rkAUnk=HhGx_1To0uarPx7MErvAXaMhctz3r4Bzg@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
	<CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
	<CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
	<CAPh_B=Z2w2rkAUnk=HhGx_1To0uarPx7MErvAXaMhctz3r4Bzg@mail.gmail.com>
Date: Wed, 13 Aug 2014 14:21:19 -0700
Message-ID: <CAKx7Bf-5GBS6A++8tSnobZcdRVak+E2M1P69stDZP=sQ9C_KSA@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Reynold Xin <rxin@databricks.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec53d57af8172c30500895e69
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d57af8172c30500895e69
Content-Type: text/plain; charset=UTF-8

Yeah I worked on DistributedR while I was an intern at HP Labs, but it has
evolved a lot since then. I don't think its a direct comparison as
DistributedR is a pure R implementation in a distributed setting while
SparkR is a wrapper around the Scala / Java implementations in Spark.

That said, it would be an interesting exercise to compare them and I hope
to do it at some point.

Shivaram


On Wed, Aug 13, 2014 at 2:16 PM, Reynold Xin <rxin@databricks.com> wrote:

> Actually I believe the same person started both projects.
>
> The Distributed R project from HP was started by Shivaram Venkataraman when
> he was there. He since moved to Berkeley AMPLab to pursue a PhD and SparkR
> was his latest project.
>
>
>
> On Wed, Aug 13, 2014 at 1:04 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > On a related note, I recently heard about Distributed R
> > <https://github.com/vertica/DistributedR>, which is coming out of
> > HP/Vertica and seems to be their proposition for machine learning at
> scale.
> >
> > It would be interesting to see some kind of comparison between that and
> > MLlib (and perhaps also SparkR
> > <https://github.com/amplab-extras/SparkR-pkg>?), especially since
> > Distributed R has a concept of distributed arrays and works on data
> > in-memory. Docs are here.
> > <https://github.com/vertica/DistributedR/tree/master/doc/platform>
> >
> > Nick
> >
> >
> > On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> >> They only compared their own implementations of couple algorithms on
> >> different platforms rather than comparing the different platforms
> >> themselves (in the case of Spark -- PySpark). I can write two variants
> of
> >> an algorithm on Spark and make them perform drastically differently.
> >>
> >> I have no doubt if you implement a ML algorithm in Python itself without
> >> any native libraries, the performance will be sub-optimal.
> >>
> >> What PySpark really provides is:
> >>
> >> - Using Spark transformations in Python
> >> - ML algorithms implemented in Scala (leveraging native numerical
> >> libraries
> >> for high performance), and callable in Python
> >>
> >> The paper claims "Python is now one of the most popular languages for
> >> ML-oriented programming", and that's why they went ahead with Python.
> >> However, as I understand, very few people actually implement algorithms
> in
> >> Python directly because of the sub-optimal performance. Most people
> >> implement algorithms in other languages (e.g. C / Java), and expose APIs
> >> in
> >> Python for ease-of-use. This is what we are trying to do with PySpark as
> >> well.
> >>
> >>
> >> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
> >> ignacio.zendejas.cs@gmail.com> wrote:
> >>
> >> > Has anyone had a chance to look at this paper (with title in subject)?
> >> > http://www.cs.rice.edu/~lp6/comparison.pdf
> >> >
> >> > Interesting that they chose to use Python alone. Do we know how much
> >> faster
> >> > Scala is vs. Python in general, if at all?
> >> >
> >> > As with any and all benchmarks, I'm sure there are caveats, but it'd
> be
> >> > nice to have a response to the question above for starters.
> >> >
> >> > Thanks,
> >> > Ignacio
> >> >
> >>
> >
> >
>

--bcaec53d57af8172c30500895e69--

From dev-return-8869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:29:34 2014
Return-Path: <dev-return-8869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 284A1110A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:29:34 +0000 (UTC)
Received: (qmail 62063 invoked by uid 500); 13 Aug 2014 21:29:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61997 invoked by uid 500); 13 Aug 2014 21:29:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61984 invoked by uid 99); 13 Aug 2014 21:29:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:29:32 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:29:07 +0000
Received: by mail-qc0-f178.google.com with SMTP id x3so342055qcv.37
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:29:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=hv4AW1oRwXoHCja6Z8upFfAIMplWfXgJD9hyqBpgTrM=;
        b=BXg1XA7qGjlQKsBlW+fFZhzZ2UdaOTfvpOyi6aPZZ6fyhIBEr0YWPXpY5z0cCMUPoE
         cjus9TwOwVHq0OOgUHQ/fuPp8khH1GVR9UJbw+GDumVgrt4FDqWPzDOcNpByXp3QN+Gr
         pSMg7q9A6tWyr6o9m60ZSyxN1dW+1uTCWsqQFpWGSDCZJOs5k7MkXM61vjkpB2gllxvv
         vLX0z/091/7+TGqiUNfXEE1CxM+jyzOIBwDKbG+L2weThypWfxPkm6cl7q9ntoKbni+u
         TdXymHwd5EUqdzmIFPHcqRksARrnGnrYb33jJ/xNsb7IbPqpAKWUSzNrqJh4D3kZiVwx
         f7ow==
X-Gm-Message-State: ALoCoQkFzXH444aCRrACMDyMI1D6NVE9rD0q0j+u1KirGAJ5x732W3A/k1pNQ/keGJyaKGMsSDfr
X-Received: by 10.140.92.20 with SMTP id a20mr10868024qge.0.1407965345640;
 Wed, 13 Aug 2014 14:29:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 13 Aug 2014 14:28:45 -0700 (PDT)
In-Reply-To: <OFD6FADA5C.C1CB7363-ON87257D33.0073AA10-86257D33.00754BC0@us.ibm.com>
References: <OFD6FADA5C.C1CB7363-ON87257D33.0073AA10-86257D33.00754BC0@us.ibm.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 13 Aug 2014 14:28:45 -0700
Message-ID: <CAPh_B=ZVnWmjfKcObJu-keKXwR_o_2Ta_=fVws7zQwLGuwNuww@mail.gmail.com>
Subject: Re: Added support for :cp <jar> to the Spark Shell
To: Robert C Senkbeil <rcsenkbe@us.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab95a488fa90500897a7e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab95a488fa90500897a7e
Content-Type: text/plain; charset=UTF-8

I haven't read the code yet, but if it is what I think it is, this is
SUPER, UBER, HUGELY useful.

On a related note, I asked about this on the Scala dev list but never got a
satisfactory answer ....
https://groups.google.com/forum/#!msg/scala-internals/_cZ1pK7q6cU/xyBQA0DdcYwJ


On Wed, Aug 13, 2014 at 2:21 PM, Robert C Senkbeil <rcsenkbe@us.ibm.com>
wrote:

>
>
> I've created a new pull request, which can be found at
> https://github.com/apache/spark/pull/1929. Since Spark is using Scala
> 2.10.3 and there is a known issue with Scala 2.10.x not supporting the :cp
> command (https://issues.scala-lang.org/browse/SI-6502), the Spark shell
> does not have the ability to add jars to the classpath after it has been
> started.
>
> The advantage of dynamically adding the jars versus restarting the
> shell/interpreter (global) is that you can keep your shell's current state
> (you don't lose your RDDs or anything). The previously-supported Scala
> 2.9.x implementation wiped the interpreter and replayed all of the
> commands. This isn't ideal for Spark since some operations can still be
> quite heavy. Furthermore, if some operations involved loading external
> data, there is the potential for said data to have changed if you replay
> the commands.
>
> Signed,
> Chip Senkbeil

--001a113ab95a488fa90500897a7e--

From dev-return-8870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:32:17 2014
Return-Path: <dev-return-8870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5690A110BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:32:17 +0000 (UTC)
Received: (qmail 69593 invoked by uid 500); 13 Aug 2014 21:32:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69525 invoked by uid 500); 13 Aug 2014 21:32:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69514 invoked by uid 99); 13 Aug 2014 21:32:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:32:16 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:32:11 +0000
Received: by mail-wg0-f46.google.com with SMTP id m15so294907wgh.17
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:31:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=FUlclqXdetrIETBEHkdEhRY4Q7yIBTNxHRcgH3n6lxc=;
        b=Erg5SJ7fqujU0ZU6I2BMVJnlflZCVjZ9JpY66eJfifJsp8xR6W2buxsO44I9EoP5l6
         wGI0RWQysxmRrYwW4kYii098zDLwQGBbpLbhBo1IYpdg0lLJEiCQCk+brF65ckV4lSaQ
         hzMPvHfFdHl+Q25TbbCX9TX6vMOB5HL+o5IgOBgRq47zDPppuK9r7+xmZiDzh400iCW5
         Ew2O9H73vypsHjdifev9WYFLqgZZ7DpTZRKsGLvtzQMWfiwJToUEKpX1JBokta7Qb71n
         aK5QWg0VTkLtD5t/1kONLrXC7jQqekbOqd/yYwFaS5L0N1wzATMSlMQjy5yrEHjuSFKA
         jRJw==
X-Gm-Message-State: ALoCoQlasu1MjefRWy3/i7rTsCAuOFMD3RqgsicBiTmbU4e+k6C4s7oBl/Xne17bq6AI6MAzXreZ
MIME-Version: 1.0
X-Received: by 10.180.94.234 with SMTP id df10mr41736523wib.76.1407965509152;
 Wed, 13 Aug 2014 14:31:49 -0700 (PDT)
Received: by 10.180.101.225 with HTTP; Wed, 13 Aug 2014 14:31:49 -0700 (PDT)
In-Reply-To: <CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
	<CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
	<CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
	<CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
Date: Wed, 13 Aug 2014 14:31:49 -0700
Message-ID: <CA+2Pv=gkfdFBtPFGUmswvi-EDwTgFwZOqOJYXbKPT+Efan5Hkw@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Davies Liu <davies@databricks.com>
To: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Aug 13, 2014 at 2:16 PM, Ignacio Zendejas
<ignacio.zendejas.cs@gmail.com> wrote:
> Yep, I thought it was a bogus comparison.
>
> I should rephrase my question as it was poorly phrased: on average, how
> much faster is Spark v. PySpark (I didn't really mean Scala v. Python)?
> I've only used Spark and don't have a chance to test this at the moment so
> if anybody has these numbers or general estimates (10x, etc), that'd be
> great.

A quick comparison by word count on 4.3G text file (local mode),

Spark:  40 seconds
PySpark: 2 minutes and 16 seconds

So PySpark is 3.4x slower than Spark.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:48:16 2014
Return-Path: <dev-return-8871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB52B11142
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:48:15 +0000 (UTC)
Received: (qmail 12731 invoked by uid 500); 13 Aug 2014 21:48:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12665 invoked by uid 500); 13 Aug 2014 21:48:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12643 invoked by uid 99); 13 Aug 2014 21:48:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:48:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:47:49 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so312786wgh.22
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:47:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ISH1oAi9hVWG1w5cyJxcH5ccuT96OUy/SolPKDkevuA=;
        b=JNd8Qa5AqYbzZmEetEFICSbl67mtrinxuzT6Kn1rYvVIgKQ7FqkMV2vtbRY0qXwo3N
         looFMYtgo+hIZAUuK8YhnCAtftENvoFflRvJQRWT3staXw/dpq2gwQNFIhLDxVj0+vJu
         qIybjVb1xVJa6SR1Dr1LlIyd7ipaAM7sPTYdLf9rN8VXpYV9usYmlj87CmImdR0uPOw8
         Tlhc5Z4T1lQBUdPaucyq6L9WHDMlfi0kTJ9rqoR6E/JmW/H/R7/p2mLCwFSMbG+7u9Td
         vyvsfSFLnebimz5TI99k1DJvxGD09I+mem0XaBQC1Els4Oya0tSRPsU4MFpD1uin+LEQ
         10Yw==
MIME-Version: 1.0
X-Received: by 10.180.92.104 with SMTP id cl8mr40706212wib.43.1407966468496;
 Wed, 13 Aug 2014 14:47:48 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Wed, 13 Aug 2014 14:47:48 -0700 (PDT)
In-Reply-To: <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
Date: Thu, 14 Aug 2014 07:47:48 +1000
Message-ID: <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0434bf1635e350050089bd02
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0434bf1635e350050089bd02
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I now have a complete pull request for this issue that I'd like to get
reviewed and committed.  The PR is available here:
https://github.com/apache/spark/pull/1890 and includes a testcase for the
issue I described.  I've also submitted a related PR (
https://github.com/apache/spark/pull/1827) that causes exceptions raised
while attempting to run the custom kryo registrator not to be swallowed.

Thanks,
Graham


On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com> wrote:

> I've submitted a work-in-progress pull request for this issue that I'd
> like feedback on.  See https://github.com/apache/spark/pull/1890 . I've
> also submitted a pull request for the related issue that the exceptions h=
it
> when trying to use a custom kryo registrator are being swallowed:
> https://github.com/apache/spark/pull/1827
>
> The approach in my pull request is to get the Worker processes to downloa=
d
> the application jars and add them to the Executor class path at launch
> time. There are a couple of things that still need to be done before this
> can be merged:
> 1. At the moment, the first time a task runs in the executor, the
> application jars are downloaded again.  My solution here would be to make
> the executor not download any jars that already exist.  Previously, the
> driver & executor kept track of the timestamp of jar files and would
> redownload 'updated' jars, however this never made sense as the previous
> version of the updated jar may have already been loaded into the executor=
,
> so the updated jar may have no effect.  As my current pull request remove=
s
> the timestamp for jars, just checking whether the jar exists will allow u=
s
> to avoid downloading the jars again.
> 2. Tests. :-)
>
> A side-benefit of my pull request is that you will be able to use custom
> serialisers that are distributed in a user jar.  Currently, the serialise=
r
> instance is created in the Executor process before the first task is
> received and therefore before any user jars are downloaded.  As this PR
> adds user jars to the Executor process at launch time, this won't be an
> issue.
>
>
> On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com> wrote:
>
>> See my comment on https://issues.apache.org/jira/browse/SPARK-2878 for
>> the full stacktrace, but it's in the BlockManager/BlockManagerWorker whe=
re
>> it's trying to fulfil a "getBlock" request for another node.  The object=
s
>> that would be in the block haven't yet been serialised, and that then
>> causes the deserialisation to happen on that thread.  See
>> MemoryStore.scala:102.
>>
>>
>> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> I don't think it was a conscious design decision to not include the
>>> application classes in the connection manager serializer. We should fix
>>> that. Where is it deserializing data in that thread?
>>>
>>>  4 might make sense in the long run, but it adds a lot of complexity to
>>> the code base (whole separate code base, task queue, blocking/non-block=
ing
>>> logic within task threads) that can be error prone, so I think it is be=
st
>>> to stay away from that right now.
>>>
>>>
>>>
>>>
>>>
>>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <graham.dennis@gmail.com>
>>> wrote:
>>>
>>>> Hi Spark devs,
>>>>
>>>> I=E2=80=99ve posted an issue on JIRA (
>>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when
>>>> using
>>>> Kryo serialisation with a custom Kryo registrator to register custom
>>>> classes with Kryo.  This is an insidious issue that
>>>> non-deterministically
>>>> causes Kryo to have different ID number =3D> class name maps on differ=
ent
>>>> nodes, which then causes weird exceptions (ClassCastException,
>>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>> deserialisation
>>>> time.  I=E2=80=99ve created a reliable reproduction for the issue here=
:
>>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>
>>>> I=E2=80=99m happy to try and put a pull request together to try and ad=
dress
>>>> this,
>>>> but it=E2=80=99s not obvious to me the right way to solve this and I=
=E2=80=99d like to
>>>> get
>>>> feedback / ideas on how to address this.
>>>>
>>>> The root cause of the problem is a "Failed to run
>>>> spark.kryo.registrator=E2=80=9D
>>>> error which non-deterministically occurs in some executor processes
>>>> during
>>>> operation.  My custom Kryo registrator is in the application jar, and
>>>> it is
>>>> accessible on the worker nodes.  This is demonstrated by the fact that
>>>> most
>>>> of the time the custom kryo registrator is successfully run.
>>>>
>>>> What=E2=80=99s happening is that Kryo serialisation/deserialisation is=
 happening
>>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D t=
hread, which has
>>>> the
>>>> thread's class loader set to contain the application jar.  This happen=
s
>>>> in
>>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what I c=
an
>>>> tell, it is only these threads that have access to the application jar
>>>> (that contains the custom Kryo registrator).  However, the
>>>> ConnectionManager threads sometimes need to serialise/deserialise
>>>> objects
>>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects haven=
=E2=80=99t previously been
>>>> serialised.  As the ConnectionManager threads don=E2=80=99t have the a=
pplication
>>>> jar available from their class loader, when it tries to look up the
>>>> custom
>>>> Kryo registrator, this fails.  Spark then swallows this exception, whi=
ch
>>>> results in a different ID number =E2=80=94> class mapping for this kry=
o
>>>> instance,
>>>> and this then causes deserialisation errors later on a different node.
>>>>
>>>> A related issue to the issue reported in SPARK-2878 is that Spark
>>>> probably
>>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kryo
>>>> registrators.
>>>>  The user has explicitly specified this class, and if it
>>>> deterministically
>>>> can=E2=80=99t be found, then it may cause problems at serialisation /
>>>> deserialisation time.  If only sometimes it can=E2=80=99t be found (as=
 in this
>>>> case), then it leads to a data corruption issue later on.  Either way,
>>>> we=E2=80=99re better off dying due to the ClassNotFound exception earl=
ier, than
>>>> the
>>>> weirder errors later on.
>>>>
>>>> I have some ideas on potential solutions to this issue, but I=E2=80=99=
m keen for
>>>> experienced eyes to critique these approaches:
>>>>
>>>> 1. The simplest approach to fixing this would be to just make the
>>>> application jar available to the connection manager threads, but I=E2=
=80=99m
>>>> guessing it=E2=80=99s a design decision to isolate the application jar=
 to just
>>>> the
>>>> executor task runner threads.  Also, I don=E2=80=99t know if there are=
 any other
>>>> threads that might be interacting with kryo serialisation /
>>>> deserialisation.
>>>> 2. Before looking up the custom Kryo registrator, change the thread=E2=
=80=99s
>>>> class
>>>> loader to include the application jar, then restore the class loader
>>>> after
>>>> the kryo registrator has been run.  I don=E2=80=99t know if this would=
 have any
>>>> other side-effects.
>>>> 3. Always serialise / deserialise on the existing TaskRunner threads,
>>>> rather than delaying serialisation until later, when it can be done
>>>> only if
>>>> needed.  This approach would probably have negative performance
>>>> consequences.
>>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>> deserialisation that has the application jar on the class path.
>>>>  Serialisation / deserialisation would be the only thing these threads
>>>> do,
>>>> and this would minimise conflicts / interactions between the applicati=
on
>>>> jar and other jars.
>>>>
>>>> #4 sounds like the best approach to me, but I think would require
>>>> considerable knowledge of Spark internals, which is beyond me at
>>>> present.
>>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>
>>>> Cheers,
>>>>
>>>> Graham
>>>>
>>>
>>>
>>
>

--f46d0434bf1635e350050089bd02--

From dev-return-8872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 21:52:31 2014
Return-Path: <dev-return-8872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E367711165
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 21:52:30 +0000 (UTC)
Received: (qmail 24524 invoked by uid 500); 13 Aug 2014 21:52:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24465 invoked by uid 500); 13 Aug 2014 21:52:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24454 invoked by uid 99); 13 Aug 2014 21:52:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:52:29 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 21:52:04 +0000
Received: by mail-wg0-f49.google.com with SMTP id k14so315867wgh.8
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 14:52:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=agjctuBc6UNQYBh5w79lxYzt9jxb/KNGObxqG2cDQJU=;
        b=HbIr4H+bHzLxWObUc5YBrHWl+GcaZ7y/w7rtFv0IGpSemV19Q4Zqcdwot1/4/BevrD
         QB81Jpnfnf2azgZJwCgo6Wvcz8biMJFiLV4TCUMk48VZjpwTQvw7nJRPIfQm8WPLs2pj
         gwdgDgpnIX9skm/psAZp0VbaSDi3qtqpFGIGjYnp849a3XOhnEBlD/U9sUXlVs8uYDL0
         JR0elNtTA0rujutyexp72lcjeV5Zoh/IidNAMTTqUPsAlbvkhDPDqss/RLAmoEmb4yz1
         AW12qaZD9kP2TjzrWySPfty19ffqGViT7OHHKzBgW6Z8IHywCNPzaKwYO0Q3YSEpho47
         UH/g==
X-Gm-Message-State: ALoCoQkb2aNssw0rs7KVgaoCjQ1SfOpaOqnJ4InosUTQ5I0c6mH62qDpgpuwzHddRFKohfysSaaC
MIME-Version: 1.0
X-Received: by 10.180.104.163 with SMTP id gf3mr40858585wib.24.1407966723318;
 Wed, 13 Aug 2014 14:52:03 -0700 (PDT)
Received: by 10.180.101.225 with HTTP; Wed, 13 Aug 2014 14:52:03 -0700 (PDT)
In-Reply-To: <CA+2Pv=gkfdFBtPFGUmswvi-EDwTgFwZOqOJYXbKPT+Efan5Hkw@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
	<CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com>
	<CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com>
	<CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
	<CA+2Pv=gkfdFBtPFGUmswvi-EDwTgFwZOqOJYXbKPT+Efan5Hkw@mail.gmail.com>
Date: Wed, 13 Aug 2014 14:52:03 -0700
Message-ID: <CA+2Pv=jiZEkBG7K=BBGgK=UMfADx7bjjT3ypvgwW53GBqV7bXw@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Davies Liu <davies@databricks.com>
To: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Aug 13, 2014 at 2:31 PM, Davies Liu <davies@databricks.com> wrote:
> On Wed, Aug 13, 2014 at 2:16 PM, Ignacio Zendejas
> <ignacio.zendejas.cs@gmail.com> wrote:
>> Yep, I thought it was a bogus comparison.
>>
>> I should rephrase my question as it was poorly phrased: on average, how
>> much faster is Spark v. PySpark (I didn't really mean Scala v. Python)?
>> I've only used Spark and don't have a chance to test this at the moment so
>> if anybody has these numbers or general estimates (10x, etc), that'd be
>> great.
>
> A quick comparison by word count on 4.3G text file (local mode),
>
> Spark:  40 seconds
> PySpark: 2 minutes and 16 seconds
>
> So PySpark is 3.4x slower than Spark.

I also tried DPark, which is a pure Python clone of Spark:

DPark: 53 seconds

so it's 2 times faster than PySpark, because of it does not have
the over head of passing data between JVM and Python.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 23:37:16 2014
Return-Path: <dev-return-8873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A7E71150E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 23:37:16 +0000 (UTC)
Received: (qmail 82875 invoked by uid 500); 13 Aug 2014 23:37:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82805 invoked by uid 500); 13 Aug 2014 23:37:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82786 invoked by uid 99); 13 Aug 2014 23:37:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 23:37:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of aniket.adnaik@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 23:37:10 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <aniket.adnaik@gmail.com>)
	id 1XHi66-0005mV-5v
	for dev@spark.incubator.apache.org; Wed, 13 Aug 2014 16:36:50 -0700
Date: Wed, 13 Aug 2014 16:36:50 -0700 (PDT)
From: aniketadnaik <aniket.adnaik@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1407973009489-7836.post@n3.nabble.com>
Subject: Need info on Spark's Communication/Networking layer...
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,
I am new to Spark and want to explore more on Spark's master-worker/Cluster
manager communication architecture.
Any documents ? or code pointers will be helpful to start with.
Thanks!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Need-info-on-Spark-s-Communication-Networking-layer-tp7836.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 13 23:58:08 2014
Return-Path: <dev-return-8874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0ACA1115AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Aug 2014 23:58:08 +0000 (UTC)
Received: (qmail 41317 invoked by uid 500); 13 Aug 2014 23:58:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41239 invoked by uid 500); 13 Aug 2014 23:58:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41227 invoked by uid 99); 13 Aug 2014 23:58:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 23:58:07 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rajiv.abraham@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Aug 2014 23:58:02 +0000
Received: by mail-lb0-f177.google.com with SMTP id s7so379541lbd.36
        for <dev@spark.incubator.apache.org>; Wed, 13 Aug 2014 16:57:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=riS0E0Z+iCFoydN3+uy1nR4tUYUrEfszYcEogEz+17M=;
        b=0QuE65FEpuUmLhDTc+koKpEGtNYuVq6ZzAyzYxMfeaUBoVLzdjl3jS04zXtp6/CTu7
         SpPUIEeyLr5WG8yxNYpYCABO19I/4wky4MKrIQRTd7Ii6xVus8vS22k42u5xfl7zBCZ4
         MQyPnPFo4CzTvzPz20TXMiP52P/tXthPcCDrvBP9gF+H88tKEuB/vD5PPRHx5vn3UTKU
         LJVcj3fXhg9Q8AoiZToDGn5U5dx0V+GzwAq6IkW5w7HFM0h/OWLafCLlNAmc/ZC1iBjh
         fpZ7zhKFRV5PwG4wr/8eixqr2AZYKjKN8h062MRt/witsOODm/tevUYuqRE2t3oUd244
         c5Pw==
X-Received: by 10.152.26.100 with SMTP id k4mr1127310lag.30.1407974261160;
 Wed, 13 Aug 2014 16:57:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.176.35 with HTTP; Wed, 13 Aug 2014 16:57:11 -0700 (PDT)
In-Reply-To: <1407973009489-7836.post@n3.nabble.com>
References: <1407973009489-7836.post@n3.nabble.com>
From: Rajiv Abraham <rajiv.abraham@gmail.com>
Date: Wed, 13 Aug 2014 19:57:11 -0400
Message-ID: <CADnDY-UYPd3YUHkpgJhjEL1x3fJM=t5K2-7idDVqVkcHaL7jVw@mail.gmail.com>
Subject: Re: Need info on Spark's Communication/Networking layer...
To: aniketadnaik <aniket.adnaik@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0160b9b2b0854205008b8da0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b9b2b0854205008b8da0
Content-Type: text/plain; charset=UTF-8

Hi Aniket,
Perhaps this video will help:
https://www.youtube.com/watch?v=HG2Yd-3r4-M&list=PLTPXxbhUt-YWGNTaDj6HSjnHMxiTD1HCR&index=1

You can see other upto date videos and slides here at :
http://spark-summit.org/2014/training

Best regards,
Rajiv


2014-08-13 19:36 GMT-04:00 aniketadnaik <aniket.adnaik@gmail.com>:

> Hi,
> I am new to Spark and want to explore more on Spark's master-worker/Cluster
> manager communication architecture.
> Any documents ? or code pointers will be helpful to start with.
> Thanks!
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Need-info-on-Spark-s-Communication-Networking-layer-tp7836.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


-- 
Take care,
Rajiv

--089e0160b9b2b0854205008b8da0--

From dev-return-8875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 03:13:22 2014
Return-Path: <dev-return-8875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D797111B6C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 03:13:22 +0000 (UTC)
Received: (qmail 57208 invoked by uid 500); 14 Aug 2014 03:13:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57157 invoked by uid 500); 14 Aug 2014 03:13:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57145 invoked by uid 99); 14 Aug 2014 03:13:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 03:13:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nzjemail@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 03:12:55 +0000
Received: by mail-wi0-f169.google.com with SMTP id n3so9203755wiv.0
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 20:12:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=u75zBJlblhySXwFDJRdwpthJmU6XmcR2UdvSmQQmwt8=;
        b=ARcyZli0oJIPTnLz4MNKQFhWf7yK4y7vtmSQNVBuKVBXDRDo3N+pOp0YH5/LtyswMV
         CkZ/7/hZmgUqX3/kXA+TCDuWC5o6Mvj20F+Q6+F9Pv257zkFzedFTaElr8fzFEolBvKx
         uNg7yv4OAS06bpHeHxs4yLGfl0xgd4/C7lgAOR20ektyPHXZC3q2qKOxFHrLQZbAw417
         vTagFbnVu6fQYac16Jjiyj9r5cJtb2ysYAa3XzwT2ZU1K5DFCflhXcCXCpPSeauZx3h9
         QhOGBLsg18++zBzytJMs3G3tzUTvfW95Rlc2D2OSXTQV32scQLDNuAt1j5FXOHcSzyz+
         6kqg==
MIME-Version: 1.0
X-Received: by 10.180.88.167 with SMTP id bh7mr43531970wib.12.1407985975119;
 Wed, 13 Aug 2014 20:12:55 -0700 (PDT)
Received: by 10.194.174.231 with HTTP; Wed, 13 Aug 2014 20:12:55 -0700 (PDT)
Date: Thu, 14 Aug 2014 11:12:55 +0800
Message-ID: <CAHc8ag0a1PWbK8aFxoEPoiKV_+Yr0p5nUdi9OOLOn0ioeq0uJQ@mail.gmail.com>
Subject: acquire and give back resources dynamically
From: =?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04430610e55a1d05008e4735
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04430610e55a1d05008e4735
Content-Type: text/plain; charset=UTF-8

Dear all:

Does spark can acquire resources from and give back resources to
YARN dynamically ?


-- 
*Regards,*
*Zhaojie*

--f46d04430610e55a1d05008e4735--

From dev-return-8876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 04:52:30 2014
Return-Path: <dev-return-8876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC4E411D66
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 04:52:29 +0000 (UTC)
Received: (qmail 78708 invoked by uid 500); 14 Aug 2014 04:52:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78646 invoked by uid 500); 14 Aug 2014 04:52:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78635 invoked by uid 99); 14 Aug 2014 04:52:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 04:52:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 04:52:23 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so611363qgd.23
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 21:52:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=J8gPvpplYjq5rK7Cvhm5QG9DfJpX1LeGYY2PH5tT1YE=;
        b=Cwf2lLTKxoaKygulM0yRd1www2e1rh2zPDnoLVvQo9peIoo4NhiM3gLzb/Qh5b7aut
         V71mglRDhWK2EMoAu8M+gw/7Ek/jmsNgJCPqatKPxHO+nYQ2ZQWJRfZmeITdLKea1Kg7
         31SMwtUfj8xbbkOzbMHsl9bdahgRMLv1DdDKenlz99n0Q9Vt8wH1TXwv5F9wh/wsO7tG
         Lsgc/5/vmraQuDg7i6R3UYQIR2rXaS3HBQEISpdBJMkkdp5HfOk9aZe18SwyggSBps/L
         LQqin1qvbG+x7L1JLkxHjo85mP0y0VXm6qDpoTnyBrrMAMBrgArGvTKQ1JDdp5lizTRk
         iWfQ==
X-Gm-Message-State: ALoCoQkIL6YTojf7eZPK8t0jrlTHfajc4wtcGf4KlyoNELOYw8slpxPVfXVnLVanBQpAkGpbJ5RA
X-Received: by 10.140.16.132 with SMTP id 4mr6319829qgb.78.1407991922561; Wed,
 13 Aug 2014 21:52:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Wed, 13 Aug 2014 21:51:42 -0700 (PDT)
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 13 Aug 2014 21:51:42 -0700
Message-ID: <CAPh_B=ZbCJh7BGvFZT2mjDehJeXeG0X6ObL-QwN-qgdEe91tYA@mail.gmail.com>
Subject: proposal for pluggable block transfer interface
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c0aed864319205008faab7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c0aed864319205008faab7
Content-Type: text/plain; charset=UTF-8

Hi devs,

I posted a design doc proposing an interface for pluggable block transfer
(used in shuffle, broadcast, block replication, etc). This is expected to
be done in 1.2 time frame.

It should make our code base cleaner, and enable us to provide alternative
implementations of block transfers (e.g. via the new Netty module that I'm
working on, or possibly via MapR file system).

https://issues.apache.org/jira/browse/SPARK-3019

Please take a look and comment on the JIRA ticket. Thanks.

--001a11c0aed864319205008faab7--

From dev-return-8877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 06:37:01 2014
Return-Path: <dev-return-8877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 042C511FC8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 06:37:01 +0000 (UTC)
Received: (qmail 32778 invoked by uid 500); 14 Aug 2014 06:37:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32710 invoked by uid 500); 14 Aug 2014 06:37:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32696 invoked by uid 99); 14 Aug 2014 06:37:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:37:00 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:36:35 +0000
Received: by mail-qc0-f174.google.com with SMTP id l6so715333qcy.19
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 23:36:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=G16Jr6OfqCZpf4ojzjoFL3urwIqr9/fppIDJgOce8eM=;
        b=BBJ54aO8T7E9dNobngsE3OrNbJc6kIgzK5ba1tISuQDrSQWD/BKdprwDQecnvU3td8
         pfXlI/b5jlDEzp4mQZZFZTsgvkQJM2xH/heMGcDs3yf0ZBSKJt0JgQnZ1gNPrwFBm5Zh
         YGii/gUnQBA1EvFAHys1clgG/3SjhHouELd2uuhN/EsCuuepNsAnawV0Hs7G2LpYySbZ
         PfwwMrrMlF14xu/K/jKI8ZOyuu9goVLrY0xziHpIpGqupGlTpw8hGr1f+rrnev0vuiKi
         H/weRqHQWIJ7wCvt0R8+G+9qSKkN3Hqa55vHOr7YF71nT2Cg8XzlmtjU9I4ki6PTmMvS
         4wJA==
MIME-Version: 1.0
X-Received: by 10.140.44.67 with SMTP id f61mr666130qga.44.1407998193796; Wed,
 13 Aug 2014 23:36:33 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 13 Aug 2014 23:36:33 -0700 (PDT)
Date: Wed, 13 Aug 2014 23:36:33 -0700
Message-ID: <CA+B-+fxbBEFoc1DED3D11LHs9y-go7JNCmwfP+ohQozPDp9c0w@mail.gmail.com>
Subject: Kryo serialization issues
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113943d82f8de40500912014
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113943d82f8de40500912014
Content-Type: text/plain; charset=UTF-8

Hi,

Is there a JIRA for this bug ?

I have seen it multiple times during our ALS runs now...some runs don't
show while some runs fail due to the error msg

https://github.com/GrahamDennis/spark-kryo-serialisation/blob/master/README.md

One way to circumvent this is to not use kryo but then I am not sure how
performance will get impacted...

Thanks.
Deb

--001a113943d82f8de40500912014--

From dev-return-8878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 06:38:48 2014
Return-Path: <dev-return-8878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8ADB211FCF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 06:38:48 +0000 (UTC)
Received: (qmail 35058 invoked by uid 500); 14 Aug 2014 06:38:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34994 invoked by uid 500); 14 Aug 2014 06:38:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34978 invoked by uid 99); 14 Aug 2014 06:38:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:38:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:38:22 +0000
Received: by mail-qc0-f171.google.com with SMTP id r5so717825qcx.16
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 23:38:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=H55AH7DVMsdq+jdQ3pue06/EUgU/f99j3jcuxZ8lYbw=;
        b=Qy67ad/64QiV/U3zN8pSKXJT0i0YfeeiZZ6S0654noFNXdKNmEodPl5h3qGg2xBQoF
         pxf1aBZP5GGqs4p08qJkfgnV7KDXEAy35n9RPkudhMKsFk2t9rLAaYZN2gIBbU+ctdQx
         mS1aakXH7PYbcc1ugQXG50DUcDupc/GsC42w3Nq4W8RPMB03E7KhmyVzA3ZxntTn2taY
         n6aARgal7ROyk/25LTEt+5TMyFy6zsMTNK9I0mXScAe4ZyfD3RnmbMuhZhw5ui/zkFK1
         pB9iuRYVwoQ3+XGdDWZQUWiftXW3CcA6C1IKaMP91aRz0A4W/cWC8sSrqEM0IzMXTANb
         ukmw==
X-Received: by 10.140.23.37 with SMTP id 34mr13471814qgo.2.1407998300850;
        Wed, 13 Aug 2014 23:38:20 -0700 (PDT)
Received: from [10.60.1.87] (simcoe.janelia.org. [206.241.0.254])
        by mx.google.com with ESMTPSA id u5sm7252812qae.18.2014.08.13.23.38.20
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 13 Aug 2014 23:38:20 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_AFA512DE-29FA-49D1-80FB-035265AE0876"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
Date: Thu, 14 Aug 2014 02:38:18 -0400
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>,
 Reynold Xin <rxin@databricks.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <0388F2F0-F7F1-45E3-9930-FCFDD0FFE825@gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com> <CAPh_B=bUnkRFF7R0iAsGw8y_F0NcA8aMRF3KJbby+kMnnQE=Yw@mail.gmail.com> <CAOhmDzcDk5RQ0FCfbJzydPx6p5aExNdMwBrtbCJYy+sk5udfHg@mail.gmail.com> <CANJrAvAkFNWQ7XjFGX=H-MEiJcpFTxC=z_CCycCYPpttGTm-0w@mail.gmail.com>
To: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_AFA512DE-29FA-49D1-80FB-035265AE0876
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

@Ignacio, happy to share, here's a link to a library we've been =
developing (https://github.com/freeman-lab/thunder). As just a couple =
examples, we have pipelines that use fourier transforms and other signal =
processing from scipy, and others that do massively parallel model =
fitting via Scikit learn functions, etc. That should give you some idea =
of how such libraries could be usefully integrated into a PySpark =
project. Btw, a couple things we do overlap with functionality now =
available in MLLib via the Python API, which we're working on =
integrating.

On Aug 13, 2014, at 5:16 PM, Ignacio Zendejas =
<ignacio.zendejas.cs@gmail.com> wrote:

> Yep, I thought it was a bogus comparison.
>=20
> I should rephrase my question as it was poorly phrased: on average, =
how
> much faster is Spark v. PySpark (I didn't really mean Scala v. =
Python)?
> I've only used Spark and don't have a chance to test this at the =
moment so
> if anybody has these numbers or general estimates (10x, etc), that'd =
be
> great.
>=20
> @Jeremy, if you can discuss this, what's an example of a project you
> implemented using these libraries + PySpark?
>=20
> Thanks everyone!
>=20
>=20
>=20
>=20
> On Wed, Aug 13, 2014 at 1:04 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>=20
>> On a related note, I recently heard about Distributed R
>> <https://github.com/vertica/DistributedR>, which is coming out of
>> HP/Vertica and seems to be their proposition for machine learning at =
scale.
>>=20
>> It would be interesting to see some kind of comparison between that =
and
>> MLlib (and perhaps also SparkR
>> <https://github.com/amplab-extras/SparkR-pkg>?), especially since
>> Distributed R has a concept of distributed arrays and works on data
>> in-memory. Docs are here.
>> <https://github.com/vertica/DistributedR/tree/master/doc/platform>
>>=20
>> Nick
>>=20
>>=20
>> On Wed, Aug 13, 2014 at 3:29 PM, Reynold Xin <rxin@databricks.com> =
wrote:
>>=20
>>> They only compared their own implementations of couple algorithms on
>>> different platforms rather than comparing the different platforms
>>> themselves (in the case of Spark -- PySpark). I can write two =
variants of
>>> an algorithm on Spark and make them perform drastically differently.
>>>=20
>>> I have no doubt if you implement a ML algorithm in Python itself =
without
>>> any native libraries, the performance will be sub-optimal.
>>>=20
>>> What PySpark really provides is:
>>>=20
>>> - Using Spark transformations in Python
>>> - ML algorithms implemented in Scala (leveraging native numerical
>>> libraries
>>> for high performance), and callable in Python
>>>=20
>>> The paper claims "Python is now one of the most popular languages =
for
>>> ML-oriented programming", and that's why they went ahead with =
Python.
>>> However, as I understand, very few people actually implement =
algorithms in
>>> Python directly because of the sub-optimal performance. Most people
>>> implement algorithms in other languages (e.g. C / Java), and expose =
APIs
>>> in
>>> Python for ease-of-use. This is what we are trying to do with =
PySpark as
>>> well.
>>>=20
>>>=20
>>> On Wed, Aug 13, 2014 at 11:09 AM, Ignacio Zendejas <
>>> ignacio.zendejas.cs@gmail.com> wrote:
>>>=20
>>>> Has anyone had a chance to look at this paper (with title in =
subject)?
>>>> http://www.cs.rice.edu/~lp6/comparison.pdf
>>>>=20
>>>> Interesting that they chose to use Python alone. Do we know how =
much
>>> faster
>>>> Scala is vs. Python in general, if at all?
>>>>=20
>>>> As with any and all benchmarks, I'm sure there are caveats, but =
it'd be
>>>> nice to have a response to the question above for starters.
>>>>=20
>>>> Thanks,
>>>> Ignacio
>>>>=20
>>>=20
>>=20
>>=20


--Apple-Mail=_AFA512DE-29FA-49D1-80FB-035265AE0876--

From dev-return-8879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 06:56:29 2014
Return-Path: <dev-return-8879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 237E111048
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 06:56:29 +0000 (UTC)
Received: (qmail 63143 invoked by uid 500); 14 Aug 2014 06:56:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63072 invoked by uid 500); 14 Aug 2014 06:56:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63060 invoked by uid 99); 14 Aug 2014 06:56:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:56:28 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:56:24 +0000
Received: by mail-qg0-f49.google.com with SMTP id j107so686959qga.8
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 23:56:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=sFfDdB8bSdSH7Xntex5VUi1oZ1DNqSWtuL1YANwpZOY=;
        b=VNG9T4EJYkb3eTRemRWVCunbr5j5j15lMyRn5rRyeC3RXl4lQE0UNUqmcUhglfQI+4
         o1tHL0VEqZ2UP7sHPNwJ6uBTvTHrBK83WITSRpAw7m6JPkmfrIw0CHjrJyL4Ve+HDNV8
         qKJ6l3MbQH4wd79rZQRxqpmdZG+xBaTDHsY7IatX+WCvOHNJDGk9Mwrz0WMuO3M4j9Fs
         S8DY4NdiVg2STKBUemIJcDRO4N1p7/bBZaFrV4rLNRFFm+GqVCTSuaEe7h2/jBk6xYZs
         5cDiOZc+u5plSEEy30aj4zik4zU3pjjn7AxHTJHGl31+onDD7tUSYZdxuWTGCOA/s31y
         Zqzg==
MIME-Version: 1.0
X-Received: by 10.229.73.70 with SMTP id p6mr14139966qcj.13.1407999363482;
 Wed, 13 Aug 2014 23:56:03 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 13 Aug 2014 23:56:03 -0700 (PDT)
In-Reply-To: <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
Date: Wed, 13 Aug 2014 23:56:03 -0700
Message-ID: <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Debasish Das <debasish.das83@gmail.com>
To: Graham Dennis <graham.dennis@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2bd34e789cd0500916567
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2bd34e789cd0500916567
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sorry I just saw Graham's email after sending my previous email about this
bug...

I have been seeing this same issue on our ALS runs last week but I thought
it was due my hacky way to run mllib 1.1 snapshot on core 1.0...

What's the status of this PR ? Will this fix be back-ported to 1.0.1 as we
are running 1.0.1 stable standalone cluster ?

Till the PR merges does it make sense to not use Kryo ? What are the other
recommended efficient serializers ?

Thanks.
Deb


On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> I now have a complete pull request for this issue that I'd like to get
> reviewed and committed.  The PR is available here:
> https://github.com/apache/spark/pull/1890 and includes a testcase for the
> issue I described.  I've also submitted a related PR (
> https://github.com/apache/spark/pull/1827) that causes exceptions raised
> while attempting to run the custom kryo registrator not to be swallowed.
>
> Thanks,
> Graham
>
>
> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com> wrote:
>
> > I've submitted a work-in-progress pull request for this issue that I'd
> > like feedback on.  See https://github.com/apache/spark/pull/1890 . I've
> > also submitted a pull request for the related issue that the exceptions
> hit
> > when trying to use a custom kryo registrator are being swallowed:
> > https://github.com/apache/spark/pull/1827
> >
> > The approach in my pull request is to get the Worker processes to
> download
> > the application jars and add them to the Executor class path at launch
> > time. There are a couple of things that still need to be done before th=
is
> > can be merged:
> > 1. At the moment, the first time a task runs in the executor, the
> > application jars are downloaded again.  My solution here would be to ma=
ke
> > the executor not download any jars that already exist.  Previously, the
> > driver & executor kept track of the timestamp of jar files and would
> > redownload 'updated' jars, however this never made sense as the previou=
s
> > version of the updated jar may have already been loaded into the
> executor,
> > so the updated jar may have no effect.  As my current pull request
> removes
> > the timestamp for jars, just checking whether the jar exists will allow
> us
> > to avoid downloading the jars again.
> > 2. Tests. :-)
> >
> > A side-benefit of my pull request is that you will be able to use custo=
m
> > serialisers that are distributed in a user jar.  Currently, the
> serialiser
> > instance is created in the Executor process before the first task is
> > received and therefore before any user jars are downloaded.  As this PR
> > adds user jars to the Executor process at launch time, this won't be an
> > issue.
> >
> >
> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com> wrote:
> >
> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878 for
> >> the full stacktrace, but it's in the BlockManager/BlockManagerWorker
> where
> >> it's trying to fulfil a "getBlock" request for another node.  The
> objects
> >> that would be in the block haven't yet been serialised, and that then
> >> causes the deserialisation to happen on that thread.  See
> >> MemoryStore.scala:102.
> >>
> >>
> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
> >>
> >>> I don't think it was a conscious design decision to not include the
> >>> application classes in the connection manager serializer. We should f=
ix
> >>> that. Where is it deserializing data in that thread?
> >>>
> >>>  4 might make sense in the long run, but it adds a lot of complexity =
to
> >>> the code base (whole separate code base, task queue,
> blocking/non-blocking
> >>> logic within task threads) that can be error prone, so I think it is
> best
> >>> to stay away from that right now.
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <graham.dennis@gmail.co=
m
> >
> >>> wrote:
> >>>
> >>>> Hi Spark devs,
> >>>>
> >>>> I=E2=80=99ve posted an issue on JIRA (
> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when
> >>>> using
> >>>> Kryo serialisation with a custom Kryo registrator to register custom
> >>>> classes with Kryo.  This is an insidious issue that
> >>>> non-deterministically
> >>>> causes Kryo to have different ID number =3D> class name maps on
> different
> >>>> nodes, which then causes weird exceptions (ClassCastException,
> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
> >>>> deserialisation
> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issue he=
re:
> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
> >>>>
> >>>> I=E2=80=99m happy to try and put a pull request together to try and =
address
> >>>> this,
> >>>> but it=E2=80=99s not obvious to me the right way to solve this and I=
=E2=80=99d like to
> >>>> get
> >>>> feedback / ideas on how to address this.
> >>>>
> >>>> The root cause of the problem is a "Failed to run
> >>>> spark.kryo.registrator=E2=80=9D
> >>>> error which non-deterministically occurs in some executor processes
> >>>> during
> >>>> operation.  My custom Kryo registrator is in the application jar, an=
d
> >>>> it is
> >>>> accessible on the worker nodes.  This is demonstrated by the fact th=
at
> >>>> most
> >>>> of the time the custom kryo registrator is successfully run.
> >>>>
> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisation =
is
> happening
> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=9D=
 thread, which has
> >>>> the
> >>>> thread's class loader set to contain the application jar.  This
> happens
> >>>> in
> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what I
> can
> >>>> tell, it is only these threads that have access to the application j=
ar
> >>>> (that contains the custom Kryo registrator).  However, the
> >>>> ConnectionManager threads sometimes need to serialise/deserialise
> >>>> objects
> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects have=
n=E2=80=99t previously
> been
> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have the
> application
> >>>> jar available from their class loader, when it tries to look up the
> >>>> custom
> >>>> Kryo registrator, this fails.  Spark then swallows this exception,
> which
> >>>> results in a different ID number =E2=80=94> class mapping for this k=
ryo
> >>>> instance,
> >>>> and this then causes deserialisation errors later on a different nod=
e.
> >>>>
> >>>> A related issue to the issue reported in SPARK-2878 is that Spark
> >>>> probably
> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kry=
o
> >>>> registrators.
> >>>>  The user has explicitly specified this class, and if it
> >>>> deterministically
> >>>> can=E2=80=99t be found, then it may cause problems at serialisation =
/
> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be found (=
as in this
> >>>> case), then it leads to a data corruption issue later on.  Either wa=
y,
> >>>> we=E2=80=99re better off dying due to the ClassNotFound exception ea=
rlier,
> than
> >>>> the
> >>>> weirder errors later on.
> >>>>
> >>>> I have some ideas on potential solutions to this issue, but I=E2=80=
=99m keen
> for
> >>>> experienced eyes to critique these approaches:
> >>>>
> >>>> 1. The simplest approach to fixing this would be to just make the
> >>>> application jar available to the connection manager threads, but I=
=E2=80=99m
> >>>> guessing it=E2=80=99s a design decision to isolate the application j=
ar to just
> >>>> the
> >>>> executor task runner threads.  Also, I don=E2=80=99t know if there a=
re any
> other
> >>>> threads that might be interacting with kryo serialisation /
> >>>> deserialisation.
> >>>> 2. Before looking up the custom Kryo registrator, change the thread=
=E2=80=99s
> >>>> class
> >>>> loader to include the application jar, then restore the class loader
> >>>> after
> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this wou=
ld have
> any
> >>>> other side-effects.
> >>>> 3. Always serialise / deserialise on the existing TaskRunner threads=
,
> >>>> rather than delaying serialisation until later, when it can be done
> >>>> only if
> >>>> needed.  This approach would probably have negative performance
> >>>> consequences.
> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
> >>>> deserialisation that has the application jar on the class path.
> >>>>  Serialisation / deserialisation would be the only thing these threa=
ds
> >>>> do,
> >>>> and this would minimise conflicts / interactions between the
> application
> >>>> jar and other jars.
> >>>>
> >>>> #4 sounds like the best approach to me, but I think would require
> >>>> considerable knowledge of Spark internals, which is beyond me at
> >>>> present.
> >>>>  Does anyone have any better (and ideally simpler) ideas?
> >>>>
> >>>> Cheers,
> >>>>
> >>>> Graham
> >>>>
> >>>
> >>>
> >>
> >
>

--001a11c2bd34e789cd0500916567--

From dev-return-8880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 06:59:36 2014
Return-Path: <dev-return-8880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4554311061
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 06:59:36 +0000 (UTC)
Received: (qmail 69698 invoked by uid 500); 14 Aug 2014 06:59:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69631 invoked by uid 500); 14 Aug 2014 06:59:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69619 invoked by uid 99); 14 Aug 2014 06:59:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:59:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of graham.dennis@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 06:59:08 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so1810356wiv.13
        for <dev@spark.apache.org>; Wed, 13 Aug 2014 23:59:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KJxj8jcO7LDAYqwhWwXkdGeONXwrxssCHhu3YaWFT9s=;
        b=KMeJiqSnmDfVXjCDm3ZhXlFj4vIGh4u3foYn0Z6+BIYgyp+Zw2wG89/vmKBsgjACb6
         he3aG2PEavkoUXtuIWVZR/omnfC/Kkgs94+W42h8S7+0sLhO4WB+HKyrI9fTvj/HInIh
         11//haZfGNw22D0Pq0EFXorCCmPIz2tF6JdARLYmxIVr88bf1cgo/bYFg4kg1nK3CMce
         YuUl7ytrBH9y0B7SBz7aqXd8r/MNGbjdwwfYNUpjs0c/IR4oTtZ7/FMONrdjTY5OxRTx
         TQO4SGmStDQ3lNnCB3XRsMtz0Qa/q4Hw1Sm0Dyk0BkYVdU/7dCN7v/AmZDbOTRlXVwuu
         zeRA==
MIME-Version: 1.0
X-Received: by 10.194.216.163 with SMTP id or3mr8978910wjc.31.1407999548163;
 Wed, 13 Aug 2014 23:59:08 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Wed, 13 Aug 2014 23:59:08 -0700 (PDT)
In-Reply-To: <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
Date: Thu, 14 Aug 2014 16:59:08 +1000
Message-ID: <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c27eb4e99cc2050091701b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c27eb4e99cc2050091701b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Deb,

The only alternative serialiser is the JavaSerialiser (the default).
 Theoretically Spark supports custom serialisers, but due to a related
issue, custom serialisers currently can't live in application jars and must
be available to all executors at launch.  My PR fixes this issue as well,
allowing custom serialisers to be shipped in application jars.

Graham


On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote:

> Sorry I just saw Graham's email after sending my previous email about thi=
s
> bug...
>
> I have been seeing this same issue on our ALS runs last week but I though=
t
> it was due my hacky way to run mllib 1.1 snapshot on core 1.0...
>
> What's the status of this PR ? Will this fix be back-ported to 1.0.1 as w=
e
> are running 1.0.1 stable standalone cluster ?
>
> Till the PR merges does it make sense to not use Kryo ? What are the othe=
r
> recommended efficient serializers ?
>
> Thanks.
> Deb
>
>
> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> I now have a complete pull request for this issue that I'd like to get
>> reviewed and committed.  The PR is available here:
>> https://github.com/apache/spark/pull/1890 and includes a testcase for th=
e
>> issue I described.  I've also submitted a related PR (
>> https://github.com/apache/spark/pull/1827) that causes exceptions raised
>> while attempting to run the custom kryo registrator not to be swallowed.
>>
>> Thanks,
>> Graham
>>
>>
>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com> wrote:
>>
>> > I've submitted a work-in-progress pull request for this issue that I'd
>> > like feedback on.  See https://github.com/apache/spark/pull/1890 . I'v=
e
>> > also submitted a pull request for the related issue that the exception=
s
>> hit
>> > when trying to use a custom kryo registrator are being swallowed:
>> > https://github.com/apache/spark/pull/1827
>> >
>> > The approach in my pull request is to get the Worker processes to
>> download
>> > the application jars and add them to the Executor class path at launch
>> > time. There are a couple of things that still need to be done before
>> this
>> > can be merged:
>> > 1. At the moment, the first time a task runs in the executor, the
>> > application jars are downloaded again.  My solution here would be to
>> make
>> > the executor not download any jars that already exist.  Previously, th=
e
>> > driver & executor kept track of the timestamp of jar files and would
>> > redownload 'updated' jars, however this never made sense as the previo=
us
>> > version of the updated jar may have already been loaded into the
>> executor,
>> > so the updated jar may have no effect.  As my current pull request
>> removes
>> > the timestamp for jars, just checking whether the jar exists will allo=
w
>> us
>> > to avoid downloading the jars again.
>> > 2. Tests. :-)
>> >
>> > A side-benefit of my pull request is that you will be able to use cust=
om
>> > serialisers that are distributed in a user jar.  Currently, the
>> serialiser
>> > instance is created in the Executor process before the first task is
>> > received and therefore before any user jars are downloaded.  As this P=
R
>> > adds user jars to the Executor process at launch time, this won't be a=
n
>> > issue.
>> >
>> >
>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com> wrote:
>> >
>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878 fo=
r
>> >> the full stacktrace, but it's in the BlockManager/BlockManagerWorker
>> where
>> >> it's trying to fulfil a "getBlock" request for another node.  The
>> objects
>> >> that would be in the block haven't yet been serialised, and that then
>> >> causes the deserialisation to happen on that thread.  See
>> >> MemoryStore.scala:102.
>> >>
>> >>
>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>> >>
>> >>> I don't think it was a conscious design decision to not include the
>> >>> application classes in the connection manager serializer. We should
>> fix
>> >>> that. Where is it deserializing data in that thread?
>> >>>
>> >>>  4 might make sense in the long run, but it adds a lot of complexity
>> to
>> >>> the code base (whole separate code base, task queue,
>> blocking/non-blocking
>> >>> logic within task threads) that can be error prone, so I think it is
>> best
>> >>> to stay away from that right now.
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>> graham.dennis@gmail.com>
>> >>> wrote:
>> >>>
>> >>>> Hi Spark devs,
>> >>>>
>> >>>> I=E2=80=99ve posted an issue on JIRA (
>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs when
>> >>>> using
>> >>>> Kryo serialisation with a custom Kryo registrator to register custo=
m
>> >>>> classes with Kryo.  This is an insidious issue that
>> >>>> non-deterministically
>> >>>> causes Kryo to have different ID number =3D> class name maps on
>> different
>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>> >>>> deserialisation
>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issue h=
ere:
>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>> >>>>
>> >>>> I=E2=80=99m happy to try and put a pull request together to try and=
 address
>> >>>> this,
>> >>>> but it=E2=80=99s not obvious to me the right way to solve this and =
I=E2=80=99d like
>> to
>> >>>> get
>> >>>> feedback / ideas on how to address this.
>> >>>>
>> >>>> The root cause of the problem is a "Failed to run
>> >>>> spark.kryo.registrator=E2=80=9D
>> >>>> error which non-deterministically occurs in some executor processes
>> >>>> during
>> >>>> operation.  My custom Kryo registrator is in the application jar, a=
nd
>> >>>> it is
>> >>>> accessible on the worker nodes.  This is demonstrated by the fact
>> that
>> >>>> most
>> >>>> of the time the custom kryo registrator is successfully run.
>> >>>>
>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisation=
 is
>> happening
>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=
=9D thread, which
>> has
>> >>>> the
>> >>>> thread's class loader set to contain the application jar.  This
>> happens
>> >>>> in
>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what =
I
>> can
>> >>>> tell, it is only these threads that have access to the application
>> jar
>> >>>> (that contains the custom Kryo registrator).  However, the
>> >>>> ConnectionManager threads sometimes need to serialise/deserialise
>> >>>> objects
>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects hav=
en=E2=80=99t previously
>> been
>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have th=
e
>> application
>> >>>> jar available from their class loader, when it tries to look up the
>> >>>> custom
>> >>>> Kryo registrator, this fails.  Spark then swallows this exception,
>> which
>> >>>> results in a different ID number =E2=80=94> class mapping for this =
kryo
>> >>>> instance,
>> >>>> and this then causes deserialisation errors later on a different
>> node.
>> >>>>
>> >>>> A related issue to the issue reported in SPARK-2878 is that Spark
>> >>>> probably
>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom Kr=
yo
>> >>>> registrators.
>> >>>>  The user has explicitly specified this class, and if it
>> >>>> deterministically
>> >>>> can=E2=80=99t be found, then it may cause problems at serialisation=
 /
>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be found =
(as in
>> this
>> >>>> case), then it leads to a data corruption issue later on.  Either
>> way,
>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exception e=
arlier,
>> than
>> >>>> the
>> >>>> weirder errors later on.
>> >>>>
>> >>>> I have some ideas on potential solutions to this issue, but I=E2=80=
=99m keen
>> for
>> >>>> experienced eyes to critique these approaches:
>> >>>>
>> >>>> 1. The simplest approach to fixing this would be to just make the
>> >>>> application jar available to the connection manager threads, but I=
=E2=80=99m
>> >>>> guessing it=E2=80=99s a design decision to isolate the application =
jar to
>> just
>> >>>> the
>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if there =
are any
>> other
>> >>>> threads that might be interacting with kryo serialisation /
>> >>>> deserialisation.
>> >>>> 2. Before looking up the custom Kryo registrator, change the thread=
=E2=80=99s
>> >>>> class
>> >>>> loader to include the application jar, then restore the class loade=
r
>> >>>> after
>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this wo=
uld have
>> any
>> >>>> other side-effects.
>> >>>> 3. Always serialise / deserialise on the existing TaskRunner thread=
s,
>> >>>> rather than delaying serialisation until later, when it can be done
>> >>>> only if
>> >>>> needed.  This approach would probably have negative performance
>> >>>> consequences.
>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>> >>>> deserialisation that has the application jar on the class path.
>> >>>>  Serialisation / deserialisation would be the only thing these
>> threads
>> >>>> do,
>> >>>> and this would minimise conflicts / interactions between the
>> application
>> >>>> jar and other jars.
>> >>>>
>> >>>> #4 sounds like the best approach to me, but I think would require
>> >>>> considerable knowledge of Spark internals, which is beyond me at
>> >>>> present.
>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>> >>>>
>> >>>> Cheers,
>> >>>>
>> >>>> Graham
>> >>>>
>> >>>
>> >>>
>> >>
>> >
>>
>
>

--001a11c27eb4e99cc2050091701b--

From dev-return-8881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 07:37:59 2014
Return-Path: <dev-return-8881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F6A811173
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 07:37:59 +0000 (UTC)
Received: (qmail 34520 invoked by uid 500); 14 Aug 2014 07:37:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34455 invoked by uid 500); 14 Aug 2014 07:37:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34444 invoked by uid 99); 14 Aug 2014 07:37:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:37:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:37:32 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so709122qge.39
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 00:37:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1K8nXwjPsoIPHlTWy4RNGFeowkO1IpWOJRrXc7TnjSg=;
        b=AYp7HKUqF8/vk1gR2KWod6mKGo3k6mWNirPVzziK+n8nec6qtOet6Iv2F3rDkZVqG/
         k570Vqe4BrpdWLTgGw2Z7IQuZaD1BMzCCbVNpKFG+82HJ7YW50szAc8pGAIhWE5FpGkG
         70INa0jItlpG+FdFUknQOASWZYQMFchjjHR2ZgisZdSm1exsBmDVtg5KqEVVnn2DIhTz
         PTTyhbD1tEx5VWV3fA/oS/XOR61maxBOaDgu6INA03X+zOPdQPKISr0AYwbAyW5BLXJu
         Vn+t8LfaRuYMvRXNw2r0Cu+7suc7MD/5gSYlymiAwOXVO25y6woZ0xjgjEyOW8okSnYX
         GwRw==
X-Gm-Message-State: ALoCoQnlCD3JJX4MDIFwRw5YC/R56EVaOJoedv0Dcupe+dVdGaZ+Tc2/3ZDyivAd2qabvNFYPJDt
X-Received: by 10.140.16.132 with SMTP id 4mr7247682qgb.78.1408001850592; Thu,
 14 Aug 2014 00:37:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Thu, 14 Aug 2014 00:37:10 -0700 (PDT)
In-Reply-To: <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
 <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
 <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
 <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
 <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
 <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com> <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 14 Aug 2014 00:37:10 -0700
Message-ID: <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
To: Graham Dennis <graham.dennis@gmail.com>
Cc: Debasish Das <debasish.das83@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c0aed825ea7d050091fa44
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c0aed825ea7d050091fa44
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Graham,

Thanks for working on this. This is an important bug to fix.

I don't have the whole context and obviously I haven't spent nearly as much
time on this as you have, but I'm wondering what if we always pass the
executor's ClassLoader to the Kryo serializer? Will that solve this problem=
?




On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> Hi Deb,
>
> The only alternative serialiser is the JavaSerialiser (the default).
>  Theoretically Spark supports custom serialisers, but due to a related
> issue, custom serialisers currently can't live in application jars and mu=
st
> be available to all executors at launch.  My PR fixes this issue as well,
> allowing custom serialisers to be shipped in application jars.
>
> Graham
>
>
> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote:
>
>> Sorry I just saw Graham's email after sending my previous email about
>> this bug...
>>
>> I have been seeing this same issue on our ALS runs last week but I
>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0...
>>
>> What's the status of this PR ? Will this fix be back-ported to 1.0.1 as
>> we are running 1.0.1 stable standalone cluster ?
>>
>> Till the PR merges does it make sense to not use Kryo ? What are the
>> other recommended efficient serializers ?
>>
>> Thanks.
>> Deb
>>
>>
>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.com>
>> wrote:
>>
>>> I now have a complete pull request for this issue that I'd like to get
>>> reviewed and committed.  The PR is available here:
>>> https://github.com/apache/spark/pull/1890 and includes a testcase for
>>> the
>>> issue I described.  I've also submitted a related PR (
>>> https://github.com/apache/spark/pull/1827) that causes exceptions raise=
d
>>> while attempting to run the custom kryo registrator not to be swallowed=
.
>>>
>>> Thanks,
>>> Graham
>>>
>>>
>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com> wrote:
>>>
>>> > I've submitted a work-in-progress pull request for this issue that I'=
d
>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>> I've
>>> > also submitted a pull request for the related issue that the
>>> exceptions hit
>>> > when trying to use a custom kryo registrator are being swallowed:
>>> > https://github.com/apache/spark/pull/1827
>>> >
>>> > The approach in my pull request is to get the Worker processes to
>>> download
>>> > the application jars and add them to the Executor class path at launc=
h
>>> > time. There are a couple of things that still need to be done before
>>> this
>>> > can be merged:
>>> > 1. At the moment, the first time a task runs in the executor, the
>>> > application jars are downloaded again.  My solution here would be to
>>> make
>>> > the executor not download any jars that already exist.  Previously, t=
he
>>> > driver & executor kept track of the timestamp of jar files and would
>>> > redownload 'updated' jars, however this never made sense as the
>>> previous
>>> > version of the updated jar may have already been loaded into the
>>> executor,
>>> > so the updated jar may have no effect.  As my current pull request
>>> removes
>>> > the timestamp for jars, just checking whether the jar exists will
>>> allow us
>>> > to avoid downloading the jars again.
>>> > 2. Tests. :-)
>>> >
>>> > A side-benefit of my pull request is that you will be able to use
>>> custom
>>> > serialisers that are distributed in a user jar.  Currently, the
>>> serialiser
>>> > instance is created in the Executor process before the first task is
>>> > received and therefore before any user jars are downloaded.  As this =
PR
>>> > adds user jars to the Executor process at launch time, this won't be =
an
>>> > issue.
>>> >
>>> >
>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com> wrote=
:
>>> >
>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878
>>> for
>>> >> the full stacktrace, but it's in the BlockManager/BlockManagerWorker
>>> where
>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>> objects
>>> >> that would be in the block haven't yet been serialised, and that the=
n
>>> >> causes the deserialisation to happen on that thread.  See
>>> >> MemoryStore.scala:102.
>>> >>
>>> >>
>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>> >>
>>> >>> I don't think it was a conscious design decision to not include the
>>> >>> application classes in the connection manager serializer. We should
>>> fix
>>> >>> that. Where is it deserializing data in that thread?
>>> >>>
>>> >>>  4 might make sense in the long run, but it adds a lot of complexit=
y
>>> to
>>> >>> the code base (whole separate code base, task queue,
>>> blocking/non-blocking
>>> >>> logic within task threads) that can be error prone, so I think it i=
s
>>> best
>>> >>> to stay away from that right now.
>>> >>>
>>> >>>
>>> >>>
>>> >>>
>>> >>>
>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>> graham.dennis@gmail.com>
>>> >>> wrote:
>>> >>>
>>> >>>> Hi Spark devs,
>>> >>>>
>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs whe=
n
>>> >>>> using
>>> >>>> Kryo serialisation with a custom Kryo registrator to register cust=
om
>>> >>>> classes with Kryo.  This is an insidious issue that
>>> >>>> non-deterministically
>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>> different
>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>> >>>> deserialisation
>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issue =
here:
>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>> >>>>
>>> >>>> I=E2=80=99m happy to try and put a pull request together to try an=
d address
>>> >>>> this,
>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this and=
 I=E2=80=99d like
>>> to
>>> >>>> get
>>> >>>> feedback / ideas on how to address this.
>>> >>>>
>>> >>>> The root cause of the problem is a "Failed to run
>>> >>>> spark.kryo.registrator=E2=80=9D
>>> >>>> error which non-deterministically occurs in some executor processe=
s
>>> >>>> during
>>> >>>> operation.  My custom Kryo registrator is in the application jar,
>>> and
>>> >>>> it is
>>> >>>> accessible on the worker nodes.  This is demonstrated by the fact
>>> that
>>> >>>> most
>>> >>>> of the time the custom kryo registrator is successfully run.
>>> >>>>
>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisatio=
n is
>>> happening
>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=
=9D thread, which
>>> has
>>> >>>> the
>>> >>>> thread's class loader set to contain the application jar.  This
>>> happens
>>> >>>> in
>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from what
>>> I can
>>> >>>> tell, it is only these threads that have access to the application
>>> jar
>>> >>>> (that contains the custom Kryo registrator).  However, the
>>> >>>> ConnectionManager threads sometimes need to serialise/deserialise
>>> >>>> objects
>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects ha=
ven=E2=80=99t previously
>>> been
>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have t=
he
>>> application
>>> >>>> jar available from their class loader, when it tries to look up th=
e
>>> >>>> custom
>>> >>>> Kryo registrator, this fails.  Spark then swallows this exception,
>>> which
>>> >>>> results in a different ID number =E2=80=94> class mapping for this=
 kryo
>>> >>>> instance,
>>> >>>> and this then causes deserialisation errors later on a different
>>> node.
>>> >>>>
>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spark
>>> >>>> probably
>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom K=
ryo
>>> >>>> registrators.
>>> >>>>  The user has explicitly specified this class, and if it
>>> >>>> deterministically
>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisatio=
n /
>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be found=
 (as in
>>> this
>>> >>>> case), then it leads to a data corruption issue later on.  Either
>>> way,
>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exception =
earlier,
>>> than
>>> >>>> the
>>> >>>> weirder errors later on.
>>> >>>>
>>> >>>> I have some ideas on potential solutions to this issue, but I=E2=
=80=99m
>>> keen for
>>> >>>> experienced eyes to critique these approaches:
>>> >>>>
>>> >>>> 1. The simplest approach to fixing this would be to just make the
>>> >>>> application jar available to the connection manager threads, but I=
=E2=80=99m
>>> >>>> guessing it=E2=80=99s a design decision to isolate the application=
 jar to
>>> just
>>> >>>> the
>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if there=
 are any
>>> other
>>> >>>> threads that might be interacting with kryo serialisation /
>>> >>>> deserialisation.
>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>> thread=E2=80=99s
>>> >>>> class
>>> >>>> loader to include the application jar, then restore the class load=
er
>>> >>>> after
>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this w=
ould have
>>> any
>>> >>>> other side-effects.
>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>> threads,
>>> >>>> rather than delaying serialisation until later, when it can be don=
e
>>> >>>> only if
>>> >>>> needed.  This approach would probably have negative performance
>>> >>>> consequences.
>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>> >>>> deserialisation that has the application jar on the class path.
>>> >>>>  Serialisation / deserialisation would be the only thing these
>>> threads
>>> >>>> do,
>>> >>>> and this would minimise conflicts / interactions between the
>>> application
>>> >>>> jar and other jars.
>>> >>>>
>>> >>>> #4 sounds like the best approach to me, but I think would require
>>> >>>> considerable knowledge of Spark internals, which is beyond me at
>>> >>>> present.
>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>> >>>>
>>> >>>> Cheers,
>>> >>>>
>>> >>>> Graham
>>> >>>>
>>> >>>
>>> >>>
>>> >>
>>> >
>>>
>>
>>
>

--001a11c0aed825ea7d050091fa44--

From dev-return-8882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 07:54:15 2014
Return-Path: <dev-return-8882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61AA2111D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 07:54:15 +0000 (UTC)
Received: (qmail 60021 invoked by uid 500); 14 Aug 2014 07:54:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59949 invoked by uid 500); 14 Aug 2014 07:54:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59926 invoked by uid 99); 14 Aug 2014 07:54:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:54:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of graham.dennis@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:53:47 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so8499722wib.14
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 00:53:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=WYm4b8WlezDKS1n/N+//4d2M7/cG1np1yQt0hgF+AgQ=;
        b=gtJSjfEiI6PhScPaE4ux7j8Wp8fkugpsio0MGpzAgJVCNnGHE7XvvQzi0+ZJjZp5dK
         p3u20H8l3SC/yweNjYPXWdBfyicCCnqX+DCTnXoCtF5Mu411Nm5kD5Lp1d/tRXmAYz6P
         Sfln0UqtQYcaxBSVFtzven/4IsdaXqd/z8VOmJQmuQ5xPA672EmNGw2RaEVYyr5d1Ija
         +8YrDUVrwSqOyEJhb/QIhOJKpjckZuZM9W1Ah9VBr8Y9YUJidz8YuBKZd+/ZlOdjYqi6
         h9/YpwfE67kXOvzAUv2wh8FLifm5rflfj7K0GenJgdXx0KXz3dfvZN7Uib4zlAveXjPJ
         O2aA==
MIME-Version: 1.0
X-Received: by 10.181.13.44 with SMTP id ev12mr44960198wid.57.1408002826947;
 Thu, 14 Aug 2014 00:53:46 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Thu, 14 Aug 2014 00:53:46 -0700 (PDT)
In-Reply-To: <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
Date: Thu, 14 Aug 2014 17:53:46 +1000
Message-ID: <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Debasish Das <debasish.das83@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0438954957d4c7050092348d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438954957d4c7050092348d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Reynold,

That would solve this specific issue, but you'd need to be careful that you
never created a serialiser instance before the first task is received.
 Currently in Executor.TaskRunner.run a closure serialiser instance is
created before any application jars are downloaded, but that could be
moved.  To me, this seems a little fragile.

However there is a related issue where you can't ship a custom serialiser
in an application jar because the serialiser is instantiated when the
SparkEnv object is created, which is before any tasks are received by the
executor.  The above approach wouldn't help with this problem.
 Additionally, the YARN scheduler currently uses this approach of adding
the application jar to the Executor classpath, so it would make things a
bit more uniform.

Cheers,
Graham


On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:

> Graham,
>
> Thanks for working on this. This is an important bug to fix.
>
> I don't have the whole context and obviously I haven't spent nearly as
> much time on this as you have, but I'm wondering what if we always pass t=
he
> executor's ClassLoader to the Kryo serializer? Will that solve this probl=
em?
>
>
>
>
> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> Hi Deb,
>>
>> The only alternative serialiser is the JavaSerialiser (the default).
>>  Theoretically Spark supports custom serialisers, but due to a related
>> issue, custom serialisers currently can't live in application jars and m=
ust
>> be available to all executors at launch.  My PR fixes this issue as well=
,
>> allowing custom serialisers to be shipped in application jars.
>>
>> Graham
>>
>>
>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote:
>>
>>> Sorry I just saw Graham's email after sending my previous email about
>>> this bug...
>>>
>>> I have been seeing this same issue on our ALS runs last week but I
>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0..=
.
>>>
>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1 as
>>> we are running 1.0.1 stable standalone cluster ?
>>>
>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>> other recommended efficient serializers ?
>>>
>>> Thanks.
>>> Deb
>>>
>>>
>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.com=
>
>>> wrote:
>>>
>>>> I now have a complete pull request for this issue that I'd like to get
>>>> reviewed and committed.  The PR is available here:
>>>> https://github.com/apache/spark/pull/1890 and includes a testcase for
>>>> the
>>>> issue I described.  I've also submitted a related PR (
>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>> raised
>>>> while attempting to run the custom kryo registrator not to be swallowe=
d.
>>>>
>>>> Thanks,
>>>> Graham
>>>>
>>>>
>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com> wrote=
:
>>>>
>>>> > I've submitted a work-in-progress pull request for this issue that I=
'd
>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>>> I've
>>>> > also submitted a pull request for the related issue that the
>>>> exceptions hit
>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>> > https://github.com/apache/spark/pull/1827
>>>> >
>>>> > The approach in my pull request is to get the Worker processes to
>>>> download
>>>> > the application jars and add them to the Executor class path at laun=
ch
>>>> > time. There are a couple of things that still need to be done before
>>>> this
>>>> > can be merged:
>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>> > application jars are downloaded again.  My solution here would be to
>>>> make
>>>> > the executor not download any jars that already exist.  Previously,
>>>> the
>>>> > driver & executor kept track of the timestamp of jar files and would
>>>> > redownload 'updated' jars, however this never made sense as the
>>>> previous
>>>> > version of the updated jar may have already been loaded into the
>>>> executor,
>>>> > so the updated jar may have no effect.  As my current pull request
>>>> removes
>>>> > the timestamp for jars, just checking whether the jar exists will
>>>> allow us
>>>> > to avoid downloading the jars again.
>>>> > 2. Tests. :-)
>>>> >
>>>> > A side-benefit of my pull request is that you will be able to use
>>>> custom
>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>> serialiser
>>>> > instance is created in the Executor process before the first task is
>>>> > received and therefore before any user jars are downloaded.  As this
>>>> PR
>>>> > adds user jars to the Executor process at launch time, this won't be
>>>> an
>>>> > issue.
>>>> >
>>>> >
>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>> wrote:
>>>> >
>>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878
>>>> for
>>>> >> the full stacktrace, but it's in the BlockManager/BlockManagerWorke=
r
>>>> where
>>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>>> objects
>>>> >> that would be in the block haven't yet been serialised, and that th=
en
>>>> >> causes the deserialisation to happen on that thread.  See
>>>> >> MemoryStore.scala:102.
>>>> >>
>>>> >>
>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>> >>
>>>> >>> I don't think it was a conscious design decision to not include th=
e
>>>> >>> application classes in the connection manager serializer. We shoul=
d
>>>> fix
>>>> >>> that. Where is it deserializing data in that thread?
>>>> >>>
>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>> complexity to
>>>> >>> the code base (whole separate code base, task queue,
>>>> blocking/non-blocking
>>>> >>> logic within task threads) that can be error prone, so I think it
>>>> is best
>>>> >>> to stay away from that right now.
>>>> >>>
>>>> >>>
>>>> >>>
>>>> >>>
>>>> >>>
>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>> graham.dennis@gmail.com>
>>>> >>> wrote:
>>>> >>>
>>>> >>>> Hi Spark devs,
>>>> >>>>
>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>> when
>>>> >>>> using
>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>> custom
>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>> >>>> non-deterministically
>>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>>> different
>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>> >>>> deserialisation
>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issue=
 here:
>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>> >>>>
>>>> >>>> I=E2=80=99m happy to try and put a pull request together to try a=
nd address
>>>> >>>> this,
>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this an=
d I=E2=80=99d
>>>> like to
>>>> >>>> get
>>>> >>>> feedback / ideas on how to address this.
>>>> >>>>
>>>> >>>> The root cause of the problem is a "Failed to run
>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>> >>>> error which non-deterministically occurs in some executor process=
es
>>>> >>>> during
>>>> >>>> operation.  My custom Kryo registrator is in the application jar,
>>>> and
>>>> >>>> it is
>>>> >>>> accessible on the worker nodes.  This is demonstrated by the fact
>>>> that
>>>> >>>> most
>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>> >>>>
>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisati=
on is
>>>> happening
>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=80=
=9D thread, which
>>>> has
>>>> >>>> the
>>>> >>>> thread's class loader set to contain the application jar.  This
>>>> happens
>>>> >>>> in
>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from wha=
t
>>>> I can
>>>> >>>> tell, it is only these threads that have access to the applicatio=
n
>>>> jar
>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>> >>>> ConnectionManager threads sometimes need to serialise/deserialise
>>>> >>>> objects
>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects h=
aven=E2=80=99t previously
>>>> been
>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have =
the
>>>> application
>>>> >>>> jar available from their class loader, when it tries to look up t=
he
>>>> >>>> custom
>>>> >>>> Kryo registrator, this fails.  Spark then swallows this exception=
,
>>>> which
>>>> >>>> results in a different ID number =E2=80=94> class mapping for thi=
s kryo
>>>> >>>> instance,
>>>> >>>> and this then causes deserialisation errors later on a different
>>>> node.
>>>> >>>>
>>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spark
>>>> >>>> probably
>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom =
Kryo
>>>> >>>> registrators.
>>>> >>>>  The user has explicitly specified this class, and if it
>>>> >>>> deterministically
>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisati=
on /
>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be foun=
d (as in
>>>> this
>>>> >>>> case), then it leads to a data corruption issue later on.  Either
>>>> way,
>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exception=
 earlier,
>>>> than
>>>> >>>> the
>>>> >>>> weirder errors later on.
>>>> >>>>
>>>> >>>> I have some ideas on potential solutions to this issue, but I=E2=
=80=99m
>>>> keen for
>>>> >>>> experienced eyes to critique these approaches:
>>>> >>>>
>>>> >>>> 1. The simplest approach to fixing this would be to just make the
>>>> >>>> application jar available to the connection manager threads, but
>>>> I=E2=80=99m
>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applicatio=
n jar to
>>>> just
>>>> >>>> the
>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if ther=
e are any
>>>> other
>>>> >>>> threads that might be interacting with kryo serialisation /
>>>> >>>> deserialisation.
>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>> thread=E2=80=99s
>>>> >>>> class
>>>> >>>> loader to include the application jar, then restore the class
>>>> loader
>>>> >>>> after
>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this =
would
>>>> have any
>>>> >>>> other side-effects.
>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>> threads,
>>>> >>>> rather than delaying serialisation until later, when it can be do=
ne
>>>> >>>> only if
>>>> >>>> needed.  This approach would probably have negative performance
>>>> >>>> consequences.
>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>> >>>> deserialisation that has the application jar on the class path.
>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>> threads
>>>> >>>> do,
>>>> >>>> and this would minimise conflicts / interactions between the
>>>> application
>>>> >>>> jar and other jars.
>>>> >>>>
>>>> >>>> #4 sounds like the best approach to me, but I think would require
>>>> >>>> considerable knowledge of Spark internals, which is beyond me at
>>>> >>>> present.
>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>> >>>>
>>>> >>>> Cheers,
>>>> >>>>
>>>> >>>> Graham
>>>> >>>>
>>>> >>>
>>>> >>>
>>>> >>
>>>> >
>>>>
>>>
>>>
>>
>

--f46d0438954957d4c7050092348d--

From dev-return-8883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 07:57:44 2014
Return-Path: <dev-return-8883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4236111E5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 07:57:43 +0000 (UTC)
Received: (qmail 68942 invoked by uid 500); 14 Aug 2014 07:57:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68894 invoked by uid 500); 14 Aug 2014 07:57:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68881 invoked by uid 99); 14 Aug 2014 07:57:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:57:42 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 07:57:38 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so756408qcr.20
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 00:57:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=XQCaPF0p6QeipjxhhfZymroPbfrJGgr2WH7403YMQ5c=;
        b=jJrxWVS+w4Nearl6Jv5FLYYGdN1NViNJk2gkPmBVXHttoiCGCMSN2S1v1SZFTf66Ek
         1ma7nQh6Dy+ipSr9n0gS0WDZjP79nx9NkxS54mkp1TOb4EIJSd9BLLPGu9R6ShyP5h58
         tblc7siusTXK43lpHEEjnLYYf8abLsWbW13jQPk51rUwoRwv3avmwUClm6n2HgSS4Lq1
         ViS7paaafkiS6UjV/xFc2YrhJJbHr4jjS7pt1vjQxClse6m3BUDqBZfL/Wkn2POrot6U
         RsRoUk5OQnobpD+b8YNs/yUOlpf0RE+jqNL2sxM6cuKJHTTfUGeblYX4wvwDpcdcI0dn
         RevQ==
MIME-Version: 1.0
X-Received: by 10.229.231.68 with SMTP id jp4mr14681060qcb.4.1408003038033;
 Thu, 14 Aug 2014 00:57:18 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Thu, 14 Aug 2014 00:57:17 -0700 (PDT)
In-Reply-To: <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
Date: Thu, 14 Aug 2014 00:57:17 -0700
Message-ID: <CA+B-+fz7pvGHmNSQspTpOtnp0aznRWXptpKcGK2_78N6beyGUA@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Debasish Das <debasish.das83@gmail.com>
To: Graham Dennis <graham.dennis@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134497aeccaad05009240fc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134497aeccaad05009240fc
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

By the way I have seen this same problem while deploying 1.1.0-SNAPSHOT on
YARN as well...

So it is a common problem in both standalone and YARN mode deployment...


On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> Hi Reynold,
>
> That would solve this specific issue, but you'd need to be careful that
> you never created a serialiser instance before the first task is received=
.
>  Currently in Executor.TaskRunner.run a closure serialiser instance is
> created before any application jars are downloaded, but that could be
> moved.  To me, this seems a little fragile.
>
> However there is a related issue where you can't ship a custom serialiser
> in an application jar because the serialiser is instantiated when the
> SparkEnv object is created, which is before any tasks are received by the
> executor.  The above approach wouldn't help with this problem.
>  Additionally, the YARN scheduler currently uses this approach of adding
> the application jar to the Executor classpath, so it would make things a
> bit more uniform.
>
> Cheers,
> Graham
>
>
> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>
>> Graham,
>>
>> Thanks for working on this. This is an important bug to fix.
>>
>> I don't have the whole context and obviously I haven't spent nearly as
>> much time on this as you have, but I'm wondering what if we always pass =
the
>> executor's ClassLoader to the Kryo serializer? Will that solve this prob=
lem?
>>
>>
>>
>>
>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.com=
>
>> wrote:
>>
>>> Hi Deb,
>>>
>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>  Theoretically Spark supports custom serialisers, but due to a related
>>> issue, custom serialisers currently can't live in application jars and =
must
>>> be available to all executors at launch.  My PR fixes this issue as wel=
l,
>>> allowing custom serialisers to be shipped in application jars.
>>>
>>> Graham
>>>
>>>
>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote:
>>>
>>>> Sorry I just saw Graham's email after sending my previous email about
>>>> this bug...
>>>>
>>>> I have been seeing this same issue on our ALS runs last week but I
>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0.=
..
>>>>
>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1 a=
s
>>>> we are running 1.0.1 stable standalone cluster ?
>>>>
>>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>>> other recommended efficient serializers ?
>>>>
>>>> Thanks.
>>>> Deb
>>>>
>>>>
>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.co=
m
>>>> > wrote:
>>>>
>>>>> I now have a complete pull request for this issue that I'd like to ge=
t
>>>>> reviewed and committed.  The PR is available here:
>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase for
>>>>> the
>>>>> issue I described.  I've also submitted a related PR (
>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>> raised
>>>>> while attempting to run the custom kryo registrator not to be
>>>>> swallowed.
>>>>>
>>>>> Thanks,
>>>>> Graham
>>>>>
>>>>>
>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>> wrote:
>>>>>
>>>>> > I've submitted a work-in-progress pull request for this issue that
>>>>> I'd
>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>>>> I've
>>>>> > also submitted a pull request for the related issue that the
>>>>> exceptions hit
>>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>>> > https://github.com/apache/spark/pull/1827
>>>>> >
>>>>> > The approach in my pull request is to get the Worker processes to
>>>>> download
>>>>> > the application jars and add them to the Executor class path at
>>>>> launch
>>>>> > time. There are a couple of things that still need to be done befor=
e
>>>>> this
>>>>> > can be merged:
>>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>>> > application jars are downloaded again.  My solution here would be t=
o
>>>>> make
>>>>> > the executor not download any jars that already exist.  Previously,
>>>>> the
>>>>> > driver & executor kept track of the timestamp of jar files and woul=
d
>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>> previous
>>>>> > version of the updated jar may have already been loaded into the
>>>>> executor,
>>>>> > so the updated jar may have no effect.  As my current pull request
>>>>> removes
>>>>> > the timestamp for jars, just checking whether the jar exists will
>>>>> allow us
>>>>> > to avoid downloading the jars again.
>>>>> > 2. Tests. :-)
>>>>> >
>>>>> > A side-benefit of my pull request is that you will be able to use
>>>>> custom
>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>> serialiser
>>>>> > instance is created in the Executor process before the first task i=
s
>>>>> > received and therefore before any user jars are downloaded.  As thi=
s
>>>>> PR
>>>>> > adds user jars to the Executor process at launch time, this won't b=
e
>>>>> an
>>>>> > issue.
>>>>> >
>>>>> >
>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>> wrote:
>>>>> >
>>>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878
>>>>> for
>>>>> >> the full stacktrace, but it's in the
>>>>> BlockManager/BlockManagerWorker where
>>>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>>>> objects
>>>>> >> that would be in the block haven't yet been serialised, and that
>>>>> then
>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>> >> MemoryStore.scala:102.
>>>>> >>
>>>>> >>
>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>>> >>
>>>>> >>> I don't think it was a conscious design decision to not include t=
he
>>>>> >>> application classes in the connection manager serializer. We
>>>>> should fix
>>>>> >>> that. Where is it deserializing data in that thread?
>>>>> >>>
>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>> complexity to
>>>>> >>> the code base (whole separate code base, task queue,
>>>>> blocking/non-blocking
>>>>> >>> logic within task threads) that can be error prone, so I think it
>>>>> is best
>>>>> >>> to stay away from that right now.
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>> graham.dennis@gmail.com>
>>>>> >>> wrote:
>>>>> >>>
>>>>> >>>> Hi Spark devs,
>>>>> >>>>
>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>>> when
>>>>> >>>> using
>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>> custom
>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>> >>>> non-deterministically
>>>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>>>> different
>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>> >>>> deserialisation
>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issu=
e here:
>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>> >>>>
>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to try =
and
>>>>> address
>>>>> >>>> this,
>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this a=
nd I=E2=80=99d
>>>>> like to
>>>>> >>>> get
>>>>> >>>> feedback / ideas on how to address this.
>>>>> >>>>
>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>> >>>> error which non-deterministically occurs in some executor
>>>>> processes
>>>>> >>>> during
>>>>> >>>> operation.  My custom Kryo registrator is in the application jar=
,
>>>>> and
>>>>> >>>> it is
>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the fac=
t
>>>>> that
>>>>> >>>> most
>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>> >>>>
>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisat=
ion is
>>>>> happening
>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=
=80=9D thread,
>>>>> which has
>>>>> >>>> the
>>>>> >>>> thread's class loader set to contain the application jar.  This
>>>>> happens
>>>>> >>>> in
>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>> what I can
>>>>> >>>> tell, it is only these threads that have access to the
>>>>> application jar
>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>> >>>> ConnectionManager threads sometimes need to serialise/deserialis=
e
>>>>> >>>> objects
>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects =
haven=E2=80=99t
>>>>> previously been
>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have=
 the
>>>>> application
>>>>> >>>> jar available from their class loader, when it tries to look up
>>>>> the
>>>>> >>>> custom
>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>> exception, which
>>>>> >>>> results in a different ID number =E2=80=94> class mapping for th=
is kryo
>>>>> >>>> instance,
>>>>> >>>> and this then causes deserialisation errors later on a different
>>>>> node.
>>>>> >>>>
>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spar=
k
>>>>> >>>> probably
>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom=
 Kryo
>>>>> >>>> registrators.
>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>> >>>> deterministically
>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisat=
ion /
>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be fou=
nd (as in
>>>>> this
>>>>> >>>> case), then it leads to a data corruption issue later on.  Eithe=
r
>>>>> way,
>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exceptio=
n
>>>>> earlier, than
>>>>> >>>> the
>>>>> >>>> weirder errors later on.
>>>>> >>>>
>>>>> >>>> I have some ideas on potential solutions to this issue, but I=E2=
=80=99m
>>>>> keen for
>>>>> >>>> experienced eyes to critique these approaches:
>>>>> >>>>
>>>>> >>>> 1. The simplest approach to fixing this would be to just make th=
e
>>>>> >>>> application jar available to the connection manager threads, but
>>>>> I=E2=80=99m
>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applicati=
on jar to
>>>>> just
>>>>> >>>> the
>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if the=
re are
>>>>> any other
>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>> >>>> deserialisation.
>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>> thread=E2=80=99s
>>>>> >>>> class
>>>>> >>>> loader to include the application jar, then restore the class
>>>>> loader
>>>>> >>>> after
>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this=
 would
>>>>> have any
>>>>> >>>> other side-effects.
>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>> threads,
>>>>> >>>> rather than delaying serialisation until later, when it can be
>>>>> done
>>>>> >>>> only if
>>>>> >>>> needed.  This approach would probably have negative performance
>>>>> >>>> consequences.
>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>>> >>>> deserialisation that has the application jar on the class path.
>>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>>> threads
>>>>> >>>> do,
>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>> application
>>>>> >>>> jar and other jars.
>>>>> >>>>
>>>>> >>>> #4 sounds like the best approach to me, but I think would requir=
e
>>>>> >>>> considerable knowledge of Spark internals, which is beyond me at
>>>>> >>>> present.
>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>> >>>>
>>>>> >>>> Cheers,
>>>>> >>>>
>>>>> >>>> Graham
>>>>> >>>>
>>>>> >>>
>>>>> >>>
>>>>> >>
>>>>> >
>>>>>
>>>>
>>>>
>>>
>>
>

--001a1134497aeccaad05009240fc--

From dev-return-8884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 08:11:18 2014
Return-Path: <dev-return-8884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B28A211234
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 08:11:18 +0000 (UTC)
Received: (qmail 96304 invoked by uid 500); 14 Aug 2014 08:11:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96234 invoked by uid 500); 14 Aug 2014 08:11:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96222 invoked by uid 99); 14 Aug 2014 08:11:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 08:11:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 08:10:51 +0000
Received: by mail-wg0-f47.google.com with SMTP id b13so725726wgh.18
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 01:10:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1/gbtnV1+ge8s3DmOuKTQ48meyHC0fGoq31lwJDN4Gc=;
        b=CHLgvm2A9ZWGV4qLTbqfLbsBPgILKIY9zMRgGG4QFv4B+dsBN8eoUuS2gfu6O7gziY
         1Q+ILhOrqPmmB62UHQx8M+7KU2FNE6KjZuF3VtsnMDtEKS8p+B0QRvf5N0PElRenhVd/
         6c9TJoFsDyTloJb8PCw3wPcjju9GB+K5j9mrml4NL0N/EG9sVTDqU4yrR+9j6CawvqXF
         Ui20AeNVFQKM6urISm8IpXH4s7/i8JqOuQUHx74/bNal4tB5fI3SZIjZi+BE67+KtzZa
         faHBq/dyj+Llx8FbSWgIyPZqi9tNPyUwmL59YshxG0acdKBTYIBYTIDiwlnafxViH8/k
         qi4w==
MIME-Version: 1.0
X-Received: by 10.180.92.104 with SMTP id cl8mr43446693wib.43.1408003850791;
 Thu, 14 Aug 2014 01:10:50 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Thu, 14 Aug 2014 01:10:50 -0700 (PDT)
In-Reply-To: <CA+B-+fz7pvGHmNSQspTpOtnp0aznRWXptpKcGK2_78N6beyGUA@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
	<CA+B-+fz7pvGHmNSQspTpOtnp0aznRWXptpKcGK2_78N6beyGUA@mail.gmail.com>
Date: Thu, 14 Aug 2014 18:10:50 +1000
Message-ID: <CABpRO2djjS-bO=hebw4q9wF=tf9xkuGewpEVBo5mUjn_v-qEyA@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0434bf165e70c80500927111
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0434bf165e70c80500927111
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

In part, my assertion was based on a comment by sryza on my PR (
https://github.com/apache/spark/pull/1890#issuecomment-51805750), however I
thought I had also seen it in the YARN code base.  However, now that I look
for it, I can't find where this happens, so perhaps I was imagining the
YARN behaviour.


On 14 August 2014 17:57, Debasish Das <debasish.das83@gmail.com> wrote:

> By the way I have seen this same problem while deploying 1.1.0-SNAPSHOT o=
n
> YARN as well...
>
> So it is a common problem in both standalone and YARN mode deployment...
>
>
> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> Hi Reynold,
>>
>> That would solve this specific issue, but you'd need to be careful that
>> you never created a serialiser instance before the first task is receive=
d.
>>  Currently in Executor.TaskRunner.run a closure serialiser instance is
>> created before any application jars are downloaded, but that could be
>> moved.  To me, this seems a little fragile.
>>
>> However there is a related issue where you can't ship a custom serialise=
r
>> in an application jar because the serialiser is instantiated when the
>> SparkEnv object is created, which is before any tasks are received by th=
e
>> executor.  The above approach wouldn't help with this problem.
>>  Additionally, the YARN scheduler currently uses this approach of adding
>> the application jar to the Executor classpath, so it would make things a
>> bit more uniform.
>>
>> Cheers,
>> Graham
>>
>>
>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Graham,
>>>
>>> Thanks for working on this. This is an important bug to fix.
>>>
>>>  I don't have the whole context and obviously I haven't spent nearly as
>>> much time on this as you have, but I'm wondering what if we always pass=
 the
>>> executor's ClassLoader to the Kryo serializer? Will that solve this pro=
blem?
>>>
>>>
>>>
>>>
>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.co=
m
>>> > wrote:
>>>
>>>> Hi Deb,
>>>>
>>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>>  Theoretically Spark supports custom serialisers, but due to a related
>>>> issue, custom serialisers currently can't live in application jars and=
 must
>>>> be available to all executors at launch.  My PR fixes this issue as we=
ll,
>>>> allowing custom serialisers to be shipped in application jars.
>>>>
>>>> Graham
>>>>
>>>>
>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote=
:
>>>>
>>>>> Sorry I just saw Graham's email after sending my previous email about
>>>>> this bug...
>>>>>
>>>>> I have been seeing this same issue on our ALS runs last week but I
>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0=
...
>>>>>
>>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1
>>>>> as we are running 1.0.1 stable standalone cluster ?
>>>>>
>>>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>>>> other recommended efficient serializers ?
>>>>>
>>>>> Thanks.
>>>>> Deb
>>>>>
>>>>>
>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>> graham.dennis@gmail.com> wrote:
>>>>>
>>>>>> I now have a complete pull request for this issue that I'd like to g=
et
>>>>>> reviewed and committed.  The PR is available here:
>>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase
>>>>>> for the
>>>>>> issue I described.  I've also submitted a related PR (
>>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>>> raised
>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>> swallowed.
>>>>>>
>>>>>> Thanks,
>>>>>> Graham
>>>>>>
>>>>>>
>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>> > I've submitted a work-in-progress pull request for this issue that
>>>>>> I'd
>>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>>>>> I've
>>>>>> > also submitted a pull request for the related issue that the
>>>>>> exceptions hit
>>>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>> >
>>>>>> > The approach in my pull request is to get the Worker processes to
>>>>>> download
>>>>>> > the application jars and add them to the Executor class path at
>>>>>> launch
>>>>>> > time. There are a couple of things that still need to be done
>>>>>> before this
>>>>>> > can be merged:
>>>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>>>> > application jars are downloaded again.  My solution here would be
>>>>>> to make
>>>>>> > the executor not download any jars that already exist.  Previously=
,
>>>>>> the
>>>>>> > driver & executor kept track of the timestamp of jar files and wou=
ld
>>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>>> previous
>>>>>> > version of the updated jar may have already been loaded into the
>>>>>> executor,
>>>>>> > so the updated jar may have no effect.  As my current pull request
>>>>>> removes
>>>>>> > the timestamp for jars, just checking whether the jar exists will
>>>>>> allow us
>>>>>> > to avoid downloading the jars again.
>>>>>> > 2. Tests. :-)
>>>>>> >
>>>>>> > A side-benefit of my pull request is that you will be able to use
>>>>>> custom
>>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>>> serialiser
>>>>>> > instance is created in the Executor process before the first task =
is
>>>>>> > received and therefore before any user jars are downloaded.  As
>>>>>> this PR
>>>>>> > adds user jars to the Executor process at launch time, this won't
>>>>>> be an
>>>>>> > issue.
>>>>>> >
>>>>>> >
>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>>> wrote:
>>>>>> >
>>>>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-287=
8
>>>>>> for
>>>>>> >> the full stacktrace, but it's in the
>>>>>> BlockManager/BlockManagerWorker where
>>>>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>>>>> objects
>>>>>> >> that would be in the block haven't yet been serialised, and that
>>>>>> then
>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>> >> MemoryStore.scala:102.
>>>>>> >>
>>>>>> >>
>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>>>> >>
>>>>>> >>> I don't think it was a conscious design decision to not include
>>>>>> the
>>>>>> >>> application classes in the connection manager serializer. We
>>>>>> should fix
>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>> >>>
>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>> complexity to
>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>> blocking/non-blocking
>>>>>> >>> logic within task threads) that can be error prone, so I think i=
t
>>>>>> is best
>>>>>> >>> to stay away from that right now.
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>> graham.dennis@gmail.com>
>>>>>> >>> wrote:
>>>>>> >>>
>>>>>> >>>> Hi Spark devs,
>>>>>> >>>>
>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>>>> when
>>>>>> >>>> using
>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>>> custom
>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>> >>>> non-deterministically
>>>>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>>>>> different
>>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>> >>>> deserialisation
>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the iss=
ue here:
>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>> >>>>
>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to try=
 and
>>>>>> address
>>>>>> >>>> this,
>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this =
and I=E2=80=99d
>>>>>> like to
>>>>>> >>>> get
>>>>>> >>>> feedback / ideas on how to address this.
>>>>>> >>>>
>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>> processes
>>>>>> >>>> during
>>>>>> >>>> operation.  My custom Kryo registrator is in the application
>>>>>> jar, and
>>>>>> >>>> it is
>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the
>>>>>> fact that
>>>>>> >>>> most
>>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>>> >>>>
>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisa=
tion is
>>>>>> happening
>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=
=80=9D thread,
>>>>>> which has
>>>>>> >>>> the
>>>>>> >>>> thread's class loader set to contain the application jar.  This
>>>>>> happens
>>>>>> >>>> in
>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>>> what I can
>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>> application jar
>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>> >>>> ConnectionManager threads sometimes need to serialise/deseriali=
se
>>>>>> >>>> objects
>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects=
 haven=E2=80=99t
>>>>>> previously been
>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t hav=
e the
>>>>>> application
>>>>>> >>>> jar available from their class loader, when it tries to look up
>>>>>> the
>>>>>> >>>> custom
>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>> exception, which
>>>>>> >>>> results in a different ID number =E2=80=94> class mapping for t=
his kryo
>>>>>> >>>> instance,
>>>>>> >>>> and this then causes deserialisation errors later on a differen=
t
>>>>>> node.
>>>>>> >>>>
>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spa=
rk
>>>>>> >>>> probably
>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custo=
m Kryo
>>>>>> >>>> registrators.
>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>> >>>> deterministically
>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisa=
tion /
>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be fo=
und (as
>>>>>> in this
>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>  Either way,
>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound excepti=
on
>>>>>> earlier, than
>>>>>> >>>> the
>>>>>> >>>> weirder errors later on.
>>>>>> >>>>
>>>>>> >>>> I have some ideas on potential solutions to this issue, but I=
=E2=80=99m
>>>>>> keen for
>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>> >>>>
>>>>>> >>>> 1. The simplest approach to fixing this would be to just make t=
he
>>>>>> >>>> application jar available to the connection manager threads, bu=
t
>>>>>> I=E2=80=99m
>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applicat=
ion jar
>>>>>> to just
>>>>>> >>>> the
>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if th=
ere are
>>>>>> any other
>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>> >>>> deserialisation.
>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>>> thread=E2=80=99s
>>>>>> >>>> class
>>>>>> >>>> loader to include the application jar, then restore the class
>>>>>> loader
>>>>>> >>>> after
>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if thi=
s would
>>>>>> have any
>>>>>> >>>> other side-effects.
>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>>> threads,
>>>>>> >>>> rather than delaying serialisation until later, when it can be
>>>>>> done
>>>>>> >>>> only if
>>>>>> >>>> needed.  This approach would probably have negative performance
>>>>>> >>>> consequences.
>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>>>> >>>> deserialisation that has the application jar on the class path.
>>>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>>>> threads
>>>>>> >>>> do,
>>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>>> application
>>>>>> >>>> jar and other jars.
>>>>>> >>>>
>>>>>> >>>> #4 sounds like the best approach to me, but I think would requi=
re
>>>>>> >>>> considerable knowledge of Spark internals, which is beyond me a=
t
>>>>>> >>>> present.
>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>> >>>>
>>>>>> >>>> Cheers,
>>>>>> >>>>
>>>>>> >>>> Graham
>>>>>> >>>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>
>>>>>> >
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--f46d0434bf165e70c80500927111--

From dev-return-8885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 09:23:51 2014
Return-Path: <dev-return-8885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0B0711499
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 09:23:51 +0000 (UTC)
Received: (qmail 52617 invoked by uid 500); 14 Aug 2014 09:23:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52572 invoked by uid 500); 14 Aug 2014 09:23:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52560 invoked by uid 99); 14 Aug 2014 09:23:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 09:23:50 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 09:23:46 +0000
Received: by mail-qa0-f44.google.com with SMTP id f12so747333qad.31
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 02:23:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=QyBo3io//eepxjyetNg9vh9N7HF9ac5Ar6t72eL2fBY=;
        b=MzXsH1XkdvehkvwNepX8IrMvB8CaH2EWwhEOClcWshZe3JwyWnownKpym3QVQ51yML
         aI43eP8oyRgrwZrlOkNcmsrMH8vAPmjqw624O7A3Lf240EZUqsm9QqQjb2Lt6Iy7OuGf
         6ugpTcYQOANC82WA3NK+q7qQRjQj2cL5SYhmgK5PrE7Ru4tX46RENuCeBuZ6g4eWhoqZ
         e2Mp2+qezIZfx3ycBzhTYF9EZ5+k7SYoaHRSn5JIFhjFKVlO7+0Df23Opv4gPHJB4Fqn
         sfuDXwF0JTVMCzt+I9GuGB2Ee951m7oijQqRazHDvqcF2fpmsJEYRXXC/8bmianpFEBh
         nhzA==
X-Gm-Message-State: ALoCoQkYEu3/uQpJ9yHjt+e17BzYRubOhXvnYC9gCy78fRdS4bQkFIPp0+arK4k5HebidPsqvFs4
X-Received: by 10.224.166.195 with SMTP id n3mr12784958qay.22.1408008205093;
 Thu, 14 Aug 2014 02:23:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Thu, 14 Aug 2014 02:23:04 -0700 (PDT)
In-Reply-To: <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
 <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
 <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
 <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
 <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
 <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
 <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
 <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com> <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 14 Aug 2014 02:23:04 -0700
Message-ID: <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
To: Graham Dennis <graham.dennis@gmail.com>
Cc: Debasish Das <debasish.das83@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01538110e7fb7f05009374b9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01538110e7fb7f05009374b9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Graham,

SparkEnv only creates a KryoSerializer, but as I understand that serializer
doesn't actually initializes the registrator since that is only called when
newKryo() is called when KryoSerializerInstance is initialized.

Basically I'm thinking a quick fix for 1.2:

1. Add a classLoader field to KryoSerializer; initialize new
KryoSerializerInstance with that class loader

2. Set that classLoader to the executor's class loader when Executor is
initialized.

Then all deser calls should be using the executor's class loader.




On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> Hi Reynold,
>
> That would solve this specific issue, but you'd need to be careful that
> you never created a serialiser instance before the first task is received=
.
>  Currently in Executor.TaskRunner.run a closure serialiser instance is
> created before any application jars are downloaded, but that could be
> moved.  To me, this seems a little fragile.
>
> However there is a related issue where you can't ship a custom serialiser
> in an application jar because the serialiser is instantiated when the
> SparkEnv object is created, which is before any tasks are received by the
> executor.  The above approach wouldn't help with this problem.
>  Additionally, the YARN scheduler currently uses this approach of adding
> the application jar to the Executor classpath, so it would make things a
> bit more uniform.
>
> Cheers,
> Graham
>
>
> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>
>> Graham,
>>
>> Thanks for working on this. This is an important bug to fix.
>>
>>  I don't have the whole context and obviously I haven't spent nearly as
>> much time on this as you have, but I'm wondering what if we always pass =
the
>> executor's ClassLoader to the Kryo serializer? Will that solve this prob=
lem?
>>
>>
>>
>>
>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.com=
>
>> wrote:
>>
>>> Hi Deb,
>>>
>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>  Theoretically Spark supports custom serialisers, but due to a related
>>> issue, custom serialisers currently can't live in application jars and =
must
>>> be available to all executors at launch.  My PR fixes this issue as wel=
l,
>>> allowing custom serialisers to be shipped in application jars.
>>>
>>> Graham
>>>
>>>
>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote:
>>>
>>>> Sorry I just saw Graham's email after sending my previous email about
>>>> this bug...
>>>>
>>>> I have been seeing this same issue on our ALS runs last week but I
>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0.=
..
>>>>
>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1 a=
s
>>>> we are running 1.0.1 stable standalone cluster ?
>>>>
>>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>>> other recommended efficient serializers ?
>>>>
>>>> Thanks.
>>>> Deb
>>>>
>>>>
>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <graham.dennis@gmail.co=
m
>>>> > wrote:
>>>>
>>>>> I now have a complete pull request for this issue that I'd like to ge=
t
>>>>> reviewed and committed.  The PR is available here:
>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase for
>>>>> the
>>>>> issue I described.  I've also submitted a related PR (
>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>> raised
>>>>> while attempting to run the custom kryo registrator not to be
>>>>> swallowed.
>>>>>
>>>>> Thanks,
>>>>> Graham
>>>>>
>>>>>
>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>> wrote:
>>>>>
>>>>> > I've submitted a work-in-progress pull request for this issue that
>>>>> I'd
>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>>>> I've
>>>>> > also submitted a pull request for the related issue that the
>>>>> exceptions hit
>>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>>> > https://github.com/apache/spark/pull/1827
>>>>> >
>>>>> > The approach in my pull request is to get the Worker processes to
>>>>> download
>>>>> > the application jars and add them to the Executor class path at
>>>>> launch
>>>>> > time. There are a couple of things that still need to be done befor=
e
>>>>> this
>>>>> > can be merged:
>>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>>> > application jars are downloaded again.  My solution here would be t=
o
>>>>> make
>>>>> > the executor not download any jars that already exist.  Previously,
>>>>> the
>>>>> > driver & executor kept track of the timestamp of jar files and woul=
d
>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>> previous
>>>>> > version of the updated jar may have already been loaded into the
>>>>> executor,
>>>>> > so the updated jar may have no effect.  As my current pull request
>>>>> removes
>>>>> > the timestamp for jars, just checking whether the jar exists will
>>>>> allow us
>>>>> > to avoid downloading the jars again.
>>>>> > 2. Tests. :-)
>>>>> >
>>>>> > A side-benefit of my pull request is that you will be able to use
>>>>> custom
>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>> serialiser
>>>>> > instance is created in the Executor process before the first task i=
s
>>>>> > received and therefore before any user jars are downloaded.  As thi=
s
>>>>> PR
>>>>> > adds user jars to the Executor process at launch time, this won't b=
e
>>>>> an
>>>>> > issue.
>>>>> >
>>>>> >
>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>> wrote:
>>>>> >
>>>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-2878
>>>>> for
>>>>> >> the full stacktrace, but it's in the
>>>>> BlockManager/BlockManagerWorker where
>>>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>>>> objects
>>>>> >> that would be in the block haven't yet been serialised, and that
>>>>> then
>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>> >> MemoryStore.scala:102.
>>>>> >>
>>>>> >>
>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>>> >>
>>>>> >>> I don't think it was a conscious design decision to not include t=
he
>>>>> >>> application classes in the connection manager serializer. We
>>>>> should fix
>>>>> >>> that. Where is it deserializing data in that thread?
>>>>> >>>
>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>> complexity to
>>>>> >>> the code base (whole separate code base, task queue,
>>>>> blocking/non-blocking
>>>>> >>> logic within task threads) that can be error prone, so I think it
>>>>> is best
>>>>> >>> to stay away from that right now.
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>>
>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>> graham.dennis@gmail.com>
>>>>> >>> wrote:
>>>>> >>>
>>>>> >>>> Hi Spark devs,
>>>>> >>>>
>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>>> when
>>>>> >>>> using
>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>> custom
>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>> >>>> non-deterministically
>>>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>>>> different
>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>> >>>> deserialisation
>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the issu=
e here:
>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>> >>>>
>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to try =
and
>>>>> address
>>>>> >>>> this,
>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this a=
nd I=E2=80=99d
>>>>> like to
>>>>> >>>> get
>>>>> >>>> feedback / ideas on how to address this.
>>>>> >>>>
>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>> >>>> error which non-deterministically occurs in some executor
>>>>> processes
>>>>> >>>> during
>>>>> >>>> operation.  My custom Kryo registrator is in the application jar=
,
>>>>> and
>>>>> >>>> it is
>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the fac=
t
>>>>> that
>>>>> >>>> most
>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>> >>>>
>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisat=
ion is
>>>>> happening
>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=
=80=9D thread,
>>>>> which has
>>>>> >>>> the
>>>>> >>>> thread's class loader set to contain the application jar.  This
>>>>> happens
>>>>> >>>> in
>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>> what I can
>>>>> >>>> tell, it is only these threads that have access to the
>>>>> application jar
>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>> >>>> ConnectionManager threads sometimes need to serialise/deserialis=
e
>>>>> >>>> objects
>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects =
haven=E2=80=99t
>>>>> previously been
>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t have=
 the
>>>>> application
>>>>> >>>> jar available from their class loader, when it tries to look up
>>>>> the
>>>>> >>>> custom
>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>> exception, which
>>>>> >>>> results in a different ID number =E2=80=94> class mapping for th=
is kryo
>>>>> >>>> instance,
>>>>> >>>> and this then causes deserialisation errors later on a different
>>>>> node.
>>>>> >>>>
>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spar=
k
>>>>> >>>> probably
>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custom=
 Kryo
>>>>> >>>> registrators.
>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>> >>>> deterministically
>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisat=
ion /
>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be fou=
nd (as in
>>>>> this
>>>>> >>>> case), then it leads to a data corruption issue later on.  Eithe=
r
>>>>> way,
>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exceptio=
n
>>>>> earlier, than
>>>>> >>>> the
>>>>> >>>> weirder errors later on.
>>>>> >>>>
>>>>> >>>> I have some ideas on potential solutions to this issue, but I=E2=
=80=99m
>>>>> keen for
>>>>> >>>> experienced eyes to critique these approaches:
>>>>> >>>>
>>>>> >>>> 1. The simplest approach to fixing this would be to just make th=
e
>>>>> >>>> application jar available to the connection manager threads, but
>>>>> I=E2=80=99m
>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applicati=
on jar to
>>>>> just
>>>>> >>>> the
>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if the=
re are
>>>>> any other
>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>> >>>> deserialisation.
>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>> thread=E2=80=99s
>>>>> >>>> class
>>>>> >>>> loader to include the application jar, then restore the class
>>>>> loader
>>>>> >>>> after
>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if this=
 would
>>>>> have any
>>>>> >>>> other side-effects.
>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>> threads,
>>>>> >>>> rather than delaying serialisation until later, when it can be
>>>>> done
>>>>> >>>> only if
>>>>> >>>> needed.  This approach would probably have negative performance
>>>>> >>>> consequences.
>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>>> >>>> deserialisation that has the application jar on the class path.
>>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>>> threads
>>>>> >>>> do,
>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>> application
>>>>> >>>> jar and other jars.
>>>>> >>>>
>>>>> >>>> #4 sounds like the best approach to me, but I think would requir=
e
>>>>> >>>> considerable knowledge of Spark internals, which is beyond me at
>>>>> >>>> present.
>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>> >>>>
>>>>> >>>> Cheers,
>>>>> >>>>
>>>>> >>>> Graham
>>>>> >>>>
>>>>> >>>
>>>>> >>>
>>>>> >>
>>>>> >
>>>>>
>>>>
>>>>
>>>
>>
>

--089e01538110e7fb7f05009374b9--

From dev-return-8886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 09:36:17 2014
Return-Path: <dev-return-8886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A548114FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 09:36:17 +0000 (UTC)
Received: (qmail 76997 invoked by uid 500); 14 Aug 2014 09:36:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76940 invoked by uid 500); 14 Aug 2014 09:36:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76626 invoked by uid 99); 14 Aug 2014 09:36:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 09:36:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of graham.dennis@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 09:36:12 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so1993076wiv.13
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 02:35:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0+QsP6c/TD/NRryYZeWpEyd1+lhzqPIUqK/aaT2ehqM=;
        b=Wm6NXBNXafeKTSmiBl0Nq0ZDuhG0UJnjtFhs6uP2eJ4L/prsDwZiTApb6jwWxCD2IX
         UmeVwTWojF1kf/Fq+nlBgBgZ51F5+4ryL+62R/U0re7Bl1iZNWY96Xa4wtBtxKfXrFvm
         hZoWLdxhgaZLhjICQY76hP6yL/+qCF+UrexGKFlULtwQ7SUD10OWFB1F81MCKoh4vIdm
         E+zH4Z5kO5XiyZiY+9+pO0u3VEfHOoCYTIoU9sh7Du5WCqLRpUckvS6I+8tQnXfTZFse
         4saMP3ztpHkKqINKTqmS1iZ5pn+QMDicZ5SlgPVTNKmevhNB0SbYC4QVIpASF0yoYMmi
         5MrQ==
MIME-Version: 1.0
X-Received: by 10.180.92.104 with SMTP id cl8mr43990411wib.43.1408008950485;
 Thu, 14 Aug 2014 02:35:50 -0700 (PDT)
Received: by 10.194.119.104 with HTTP; Thu, 14 Aug 2014 02:35:50 -0700 (PDT)
In-Reply-To: <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
	<CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
Date: Thu, 14 Aug 2014 19:35:50 +1000
Message-ID: <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Debasish Das <debasish.das83@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0434bf16559a43050093a17d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0434bf16559a43050093a17d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That should work, but would you also make these changes to the
JavaSerializer?  The API of these is the same so that you can select one or
the other (or in theory a custom serializer)?  This also wouldn't address
the problem of shipping custom *serializers* (not kryo registrators) in
user jars.

On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:

> Graham,
>
> SparkEnv only creates a KryoSerializer, but as I understand that
> serializer doesn't actually initializes the registrator since that is onl=
y
> called when newKryo() is called when KryoSerializerInstance is initialize=
d.
>
> Basically I'm thinking a quick fix for 1.2:
>
> 1. Add a classLoader field to KryoSerializer; initialize new
> KryoSerializerInstance with that class loader
>
>  2. Set that classLoader to the executor's class loader when Executor is
> initialized.
>
> Then all deser calls should be using the executor's class loader.
>
>
>
>
> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> Hi Reynold,
>>
>> That would solve this specific issue, but you'd need to be careful that
>> you never created a serialiser instance before the first task is receive=
d.
>>  Currently in Executor.TaskRunner.run a closure serialiser instance is
>> created before any application jars are downloaded, but that could be
>> moved.  To me, this seems a little fragile.
>>
>> However there is a related issue where you can't ship a custom serialise=
r
>> in an application jar because the serialiser is instantiated when the
>> SparkEnv object is created, which is before any tasks are received by th=
e
>> executor.  The above approach wouldn't help with this problem.
>>  Additionally, the YARN scheduler currently uses this approach of adding
>> the application jar to the Executor classpath, so it would make things a
>> bit more uniform.
>>
>> Cheers,
>> Graham
>>
>>
>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Graham,
>>>
>>> Thanks for working on this. This is an important bug to fix.
>>>
>>>  I don't have the whole context and obviously I haven't spent nearly as
>>> much time on this as you have, but I'm wondering what if we always pass=
 the
>>> executor's ClassLoader to the Kryo serializer? Will that solve this pro=
blem?
>>>
>>>
>>>
>>>
>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <graham.dennis@gmail.co=
m
>>> > wrote:
>>>
>>>> Hi Deb,
>>>>
>>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>>  Theoretically Spark supports custom serialisers, but due to a related
>>>> issue, custom serialisers currently can't live in application jars and=
 must
>>>> be available to all executors at launch.  My PR fixes this issue as we=
ll,
>>>> allowing custom serialisers to be shipped in application jars.
>>>>
>>>> Graham
>>>>
>>>>
>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com> wrote=
:
>>>>
>>>>> Sorry I just saw Graham's email after sending my previous email about
>>>>> this bug...
>>>>>
>>>>> I have been seeing this same issue on our ALS runs last week but I
>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.0=
...
>>>>>
>>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1
>>>>> as we are running 1.0.1 stable standalone cluster ?
>>>>>
>>>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>>>> other recommended efficient serializers ?
>>>>>
>>>>> Thanks.
>>>>> Deb
>>>>>
>>>>>
>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>> graham.dennis@gmail.com> wrote:
>>>>>
>>>>>> I now have a complete pull request for this issue that I'd like to g=
et
>>>>>> reviewed and committed.  The PR is available here:
>>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase
>>>>>> for the
>>>>>> issue I described.  I've also submitted a related PR (
>>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>>> raised
>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>> swallowed.
>>>>>>
>>>>>> Thanks,
>>>>>> Graham
>>>>>>
>>>>>>
>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>> > I've submitted a work-in-progress pull request for this issue that
>>>>>> I'd
>>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890 .
>>>>>> I've
>>>>>> > also submitted a pull request for the related issue that the
>>>>>> exceptions hit
>>>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>> >
>>>>>> > The approach in my pull request is to get the Worker processes to
>>>>>> download
>>>>>> > the application jars and add them to the Executor class path at
>>>>>> launch
>>>>>> > time. There are a couple of things that still need to be done
>>>>>> before this
>>>>>> > can be merged:
>>>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>>>> > application jars are downloaded again.  My solution here would be
>>>>>> to make
>>>>>> > the executor not download any jars that already exist.  Previously=
,
>>>>>> the
>>>>>> > driver & executor kept track of the timestamp of jar files and wou=
ld
>>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>>> previous
>>>>>> > version of the updated jar may have already been loaded into the
>>>>>> executor,
>>>>>> > so the updated jar may have no effect.  As my current pull request
>>>>>> removes
>>>>>> > the timestamp for jars, just checking whether the jar exists will
>>>>>> allow us
>>>>>> > to avoid downloading the jars again.
>>>>>> > 2. Tests. :-)
>>>>>> >
>>>>>> > A side-benefit of my pull request is that you will be able to use
>>>>>> custom
>>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>>> serialiser
>>>>>> > instance is created in the Executor process before the first task =
is
>>>>>> > received and therefore before any user jars are downloaded.  As
>>>>>> this PR
>>>>>> > adds user jars to the Executor process at launch time, this won't
>>>>>> be an
>>>>>> > issue.
>>>>>> >
>>>>>> >
>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>>> wrote:
>>>>>> >
>>>>>> >> See my comment on https://issues.apache.org/jira/browse/SPARK-287=
8
>>>>>> for
>>>>>> >> the full stacktrace, but it's in the
>>>>>> BlockManager/BlockManagerWorker where
>>>>>> >> it's trying to fulfil a "getBlock" request for another node.  The
>>>>>> objects
>>>>>> >> that would be in the block haven't yet been serialised, and that
>>>>>> then
>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>> >> MemoryStore.scala:102.
>>>>>> >>
>>>>>> >>
>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>>>> >>
>>>>>> >>> I don't think it was a conscious design decision to not include
>>>>>> the
>>>>>> >>> application classes in the connection manager serializer. We
>>>>>> should fix
>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>> >>>
>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>> complexity to
>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>> blocking/non-blocking
>>>>>> >>> logic within task threads) that can be error prone, so I think i=
t
>>>>>> is best
>>>>>> >>> to stay away from that right now.
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>> graham.dennis@gmail.com>
>>>>>> >>> wrote:
>>>>>> >>>
>>>>>> >>>> Hi Spark devs,
>>>>>> >>>>
>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>>>> when
>>>>>> >>>> using
>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>>> custom
>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>> >>>> non-deterministically
>>>>>> >>>> causes Kryo to have different ID number =3D> class name maps on
>>>>>> different
>>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>> >>>> deserialisation
>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the iss=
ue here:
>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>> >>>>
>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to try=
 and
>>>>>> address
>>>>>> >>>> this,
>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this =
and I=E2=80=99d
>>>>>> like to
>>>>>> >>>> get
>>>>>> >>>> feedback / ideas on how to address this.
>>>>>> >>>>
>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>> processes
>>>>>> >>>> during
>>>>>> >>>> operation.  My custom Kryo registrator is in the application
>>>>>> jar, and
>>>>>> >>>> it is
>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the
>>>>>> fact that
>>>>>> >>>> most
>>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>>> >>>>
>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialisa=
tion is
>>>>>> happening
>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=
=80=9D thread,
>>>>>> which has
>>>>>> >>>> the
>>>>>> >>>> thread's class loader set to contain the application jar.  This
>>>>>> happens
>>>>>> >>>> in
>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>>> what I can
>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>> application jar
>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>> >>>> ConnectionManager threads sometimes need to serialise/deseriali=
se
>>>>>> >>>> objects
>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objects=
 haven=E2=80=99t
>>>>>> previously been
>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t hav=
e the
>>>>>> application
>>>>>> >>>> jar available from their class loader, when it tries to look up
>>>>>> the
>>>>>> >>>> custom
>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>> exception, which
>>>>>> >>>> results in a different ID number =E2=80=94> class mapping for t=
his kryo
>>>>>> >>>> instance,
>>>>>> >>>> and this then causes deserialisation errors later on a differen=
t
>>>>>> node.
>>>>>> >>>>
>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that Spa=
rk
>>>>>> >>>> probably
>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for custo=
m Kryo
>>>>>> >>>> registrators.
>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>> >>>> deterministically
>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialisa=
tion /
>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be fo=
und (as
>>>>>> in this
>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>  Either way,
>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound excepti=
on
>>>>>> earlier, than
>>>>>> >>>> the
>>>>>> >>>> weirder errors later on.
>>>>>> >>>>
>>>>>> >>>> I have some ideas on potential solutions to this issue, but I=
=E2=80=99m
>>>>>> keen for
>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>> >>>>
>>>>>> >>>> 1. The simplest approach to fixing this would be to just make t=
he
>>>>>> >>>> application jar available to the connection manager threads, bu=
t
>>>>>> I=E2=80=99m
>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applicat=
ion jar
>>>>>> to just
>>>>>> >>>> the
>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if th=
ere are
>>>>>> any other
>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>> >>>> deserialisation.
>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>>> thread=E2=80=99s
>>>>>> >>>> class
>>>>>> >>>> loader to include the application jar, then restore the class
>>>>>> loader
>>>>>> >>>> after
>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if thi=
s would
>>>>>> have any
>>>>>> >>>> other side-effects.
>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>>> threads,
>>>>>> >>>> rather than delaying serialisation until later, when it can be
>>>>>> done
>>>>>> >>>> only if
>>>>>> >>>> needed.  This approach would probably have negative performance
>>>>>> >>>> consequences.
>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>>>> >>>> deserialisation that has the application jar on the class path.
>>>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>>>> threads
>>>>>> >>>> do,
>>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>>> application
>>>>>> >>>> jar and other jars.
>>>>>> >>>>
>>>>>> >>>> #4 sounds like the best approach to me, but I think would requi=
re
>>>>>> >>>> considerable knowledge of Spark internals, which is beyond me a=
t
>>>>>> >>>> present.
>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>> >>>>
>>>>>> >>>> Cheers,
>>>>>> >>>>
>>>>>> >>>> Graham
>>>>>> >>>>
>>>>>> >>>
>>>>>> >>>
>>>>>> >>
>>>>>> >
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--f46d0434bf16559a43050093a17d--

From dev-return-8887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 18:00:27 2014
Return-Path: <dev-return-8887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3BE9311684
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 18:00:27 +0000 (UTC)
Received: (qmail 27733 invoked by uid 500); 14 Aug 2014 18:00:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27665 invoked by uid 500); 14 Aug 2014 18:00:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27654 invoked by uid 99); 14 Aug 2014 18:00:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 18:00:26 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 18:00:22 +0000
Received: by mail-qc0-f169.google.com with SMTP id c9so1443274qcz.28
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 11:00:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=uSYlmMNyu1wrZ6Hp6dKT4S4rbCAzZB/mHB4GWOS/shw=;
        b=hXfBHeiYy9zVE/0hJZu/64qvqVni0wNLpyqzjh166cAbA7kyWDebmZ8UfJnkI88jNI
         o4lmNvJgSxfoklg4qxeSG2CO39HvQSw36wZzAkE7E1TUr/p8EzFoV+YQU4CAJeaTT9gj
         mZU4XcbtwVoZXsLgsCR4UrObcyO2Nq6hFjIegxGnT1nMtxRryvgSEWZWc5z547l5+UFo
         dOXTSwbcb8O+D2H2gH7vkg6HjjQTG9rKdXJ43+NC6+J1J1eCiCG8ABCpml8Ev/rLtniJ
         zgHxIzXXZL2E8/RoNEuuOtzE3lndR06i6oReUTRLVs1grn18J8NKFaRdt+MCqpjMaADO
         czsg==
X-Gm-Message-State: ALoCoQnauUSRNQrDk0Y8FVrbro56qpqtJOqxCmh64KPS6EP8tSZFs6z2xh9W+L4xNRd3g0d0OPP0
X-Received: by 10.224.79.13 with SMTP id n13mr19326432qak.79.1408039200804;
 Thu, 14 Aug 2014 11:00:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Thu, 14 Aug 2014 10:59:39 -0700 (PDT)
In-Reply-To: <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
 <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
 <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
 <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
 <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
 <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
 <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
 <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
 <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
 <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com> <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 14 Aug 2014 10:59:39 -0700
Message-ID: <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
To: Graham Dennis <graham.dennis@gmail.com>
Cc: Debasish Das <debasish.das83@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc8016650e1505009aac7c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc8016650e1505009aac7c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes, I understand it might not work for custom serializer, but that is a
much less common path.

Basically I want a quick fix for 1.1 release (which is coming up soon). I
would not be comfortable making big changes to class path late into the
release cycle. We can do that for 1.2.





On Thu, Aug 14, 2014 at 2:35 AM, Graham Dennis <graham.dennis@gmail.com>
wrote:

> That should work, but would you also make these changes to the
> JavaSerializer?  The API of these is the same so that you can select one =
or
> the other (or in theory a custom serializer)?  This also wouldn't address
> the problem of shipping custom *serializers* (not kryo registrators) in
> user jars.
>
> On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:
>
>> Graham,
>>
>> SparkEnv only creates a KryoSerializer, but as I understand that
>> serializer doesn't actually initializes the registrator since that is on=
ly
>> called when newKryo() is called when KryoSerializerInstance is initializ=
ed.
>>
>> Basically I'm thinking a quick fix for 1.2:
>>
>> 1. Add a classLoader field to KryoSerializer; initialize new
>> KryoSerializerInstance with that class loader
>>
>>  2. Set that classLoader to the executor's class loader when Executor is
>> initialized.
>>
>> Then all deser calls should be using the executor's class loader.
>>
>>
>>
>>
>> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.com=
>
>> wrote:
>>
>>> Hi Reynold,
>>>
>>> That would solve this specific issue, but you'd need to be careful that
>>> you never created a serialiser instance before the first task is receiv=
ed.
>>>  Currently in Executor.TaskRunner.run a closure serialiser instance is
>>> created before any application jars are downloaded, but that could be
>>> moved.  To me, this seems a little fragile.
>>>
>>> However there is a related issue where you can't ship a custom
>>> serialiser in an application jar because the serialiser is instantiated
>>> when the SparkEnv object is created, which is before any tasks are rece=
ived
>>> by the executor.  The above approach wouldn't help with this problem.
>>>  Additionally, the YARN scheduler currently uses this approach of addin=
g
>>> the application jar to the Executor classpath, so it would make things =
a
>>> bit more uniform.
>>>
>>> Cheers,
>>> Graham
>>>
>>>
>>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>>
>>>> Graham,
>>>>
>>>> Thanks for working on this. This is an important bug to fix.
>>>>
>>>>  I don't have the whole context and obviously I haven't spent nearly
>>>> as much time on this as you have, but I'm wondering what if we always =
pass
>>>> the executor's ClassLoader to the Kryo serializer? Will that solve thi=
s
>>>> problem?
>>>>
>>>>
>>>>
>>>>
>>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <
>>>> graham.dennis@gmail.com> wrote:
>>>>
>>>>> Hi Deb,
>>>>>
>>>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>>>  Theoretically Spark supports custom serialisers, but due to a relate=
d
>>>>> issue, custom serialisers currently can't live in application jars an=
d must
>>>>> be available to all executors at launch.  My PR fixes this issue as w=
ell,
>>>>> allowing custom serialisers to be shipped in application jars.
>>>>>
>>>>> Graham
>>>>>
>>>>>
>>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Sorry I just saw Graham's email after sending my previous email abou=
t
>>>>>> this bug...
>>>>>>
>>>>>> I have been seeing this same issue on our ALS runs last week but I
>>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1.=
0...
>>>>>>
>>>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.1
>>>>>> as we are running 1.0.1 stable standalone cluster ?
>>>>>>
>>>>>> Till the PR merges does it make sense to not use Kryo ? What are the
>>>>>> other recommended efficient serializers ?
>>>>>>
>>>>>> Thanks.
>>>>>> Deb
>>>>>>
>>>>>>
>>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>
>>>>>>> I now have a complete pull request for this issue that I'd like to
>>>>>>> get
>>>>>>> reviewed and committed.  The PR is available here:
>>>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase
>>>>>>> for the
>>>>>>> issue I described.  I've also submitted a related PR (
>>>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>>>> raised
>>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>>> swallowed.
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Graham
>>>>>>>
>>>>>>>
>>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>> > I've submitted a work-in-progress pull request for this issue tha=
t
>>>>>>> I'd
>>>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890
>>>>>>> . I've
>>>>>>> > also submitted a pull request for the related issue that the
>>>>>>> exceptions hit
>>>>>>> > when trying to use a custom kryo registrator are being swallowed:
>>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>>> >
>>>>>>> > The approach in my pull request is to get the Worker processes to
>>>>>>> download
>>>>>>> > the application jars and add them to the Executor class path at
>>>>>>> launch
>>>>>>> > time. There are a couple of things that still need to be done
>>>>>>> before this
>>>>>>> > can be merged:
>>>>>>> > 1. At the moment, the first time a task runs in the executor, the
>>>>>>> > application jars are downloaded again.  My solution here would be
>>>>>>> to make
>>>>>>> > the executor not download any jars that already exist.
>>>>>>>  Previously, the
>>>>>>> > driver & executor kept track of the timestamp of jar files and
>>>>>>> would
>>>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>>>> previous
>>>>>>> > version of the updated jar may have already been loaded into the
>>>>>>> executor,
>>>>>>> > so the updated jar may have no effect.  As my current pull reques=
t
>>>>>>> removes
>>>>>>> > the timestamp for jars, just checking whether the jar exists will
>>>>>>> allow us
>>>>>>> > to avoid downloading the jars again.
>>>>>>> > 2. Tests. :-)
>>>>>>> >
>>>>>>> > A side-benefit of my pull request is that you will be able to use
>>>>>>> custom
>>>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>>>> serialiser
>>>>>>> > instance is created in the Executor process before the first task
>>>>>>> is
>>>>>>> > received and therefore before any user jars are downloaded.  As
>>>>>>> this PR
>>>>>>> > adds user jars to the Executor process at launch time, this won't
>>>>>>> be an
>>>>>>> > issue.
>>>>>>> >
>>>>>>> >
>>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>>>> wrote:
>>>>>>> >
>>>>>>> >> See my comment on
>>>>>>> https://issues.apache.org/jira/browse/SPARK-2878 for
>>>>>>> >> the full stacktrace, but it's in the
>>>>>>> BlockManager/BlockManagerWorker where
>>>>>>> >> it's trying to fulfil a "getBlock" request for another node.  Th=
e
>>>>>>> objects
>>>>>>> >> that would be in the block haven't yet been serialised, and that
>>>>>>> then
>>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>>> >> MemoryStore.scala:102.
>>>>>>> >>
>>>>>>> >>
>>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote:
>>>>>>> >>
>>>>>>> >>> I don't think it was a conscious design decision to not include
>>>>>>> the
>>>>>>> >>> application classes in the connection manager serializer. We
>>>>>>> should fix
>>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>>> >>>
>>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>>> complexity to
>>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>>> blocking/non-blocking
>>>>>>> >>> logic within task threads) that can be error prone, so I think
>>>>>>> it is best
>>>>>>> >>> to stay away from that right now.
>>>>>>> >>>
>>>>>>> >>>
>>>>>>> >>>
>>>>>>> >>>
>>>>>>> >>>
>>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>>> graham.dennis@gmail.com>
>>>>>>> >>> wrote:
>>>>>>> >>>
>>>>>>> >>>> Hi Spark devs,
>>>>>>> >>>>
>>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which occurs
>>>>>>> when
>>>>>>> >>>> using
>>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>>>> custom
>>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>>> >>>> non-deterministically
>>>>>>> >>>> causes Kryo to have different ID number =3D> class name maps o=
n
>>>>>>> different
>>>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException,
>>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>>> >>>> deserialisation
>>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the is=
sue here:
>>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>>> >>>>
>>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to tr=
y and
>>>>>>> address
>>>>>>> >>>> this,
>>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve this=
 and I=E2=80=99d
>>>>>>> like to
>>>>>>> >>>> get
>>>>>>> >>>> feedback / ideas on how to address this.
>>>>>>> >>>>
>>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>>> processes
>>>>>>> >>>> during
>>>>>>> >>>> operation.  My custom Kryo registrator is in the application
>>>>>>> jar, and
>>>>>>> >>>> it is
>>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the
>>>>>>> fact that
>>>>>>> >>>> most
>>>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>>>> >>>>
>>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserialis=
ation is
>>>>>>> happening
>>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=E2=
=80=9D thread,
>>>>>>> which has
>>>>>>> >>>> the
>>>>>>> >>>> thread's class loader set to contain the application jar.  Thi=
s
>>>>>>> happens
>>>>>>> >>>> in
>>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>>>> what I can
>>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>>> application jar
>>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>>> >>>> ConnectionManager threads sometimes need to
>>>>>>> serialise/deserialise
>>>>>>> >>>> objects
>>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the object=
s haven=E2=80=99t
>>>>>>> previously been
>>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t ha=
ve the
>>>>>>> application
>>>>>>> >>>> jar available from their class loader, when it tries to look u=
p
>>>>>>> the
>>>>>>> >>>> custom
>>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>>> exception, which
>>>>>>> >>>> results in a different ID number =E2=80=94> class mapping for =
this kryo
>>>>>>> >>>> instance,
>>>>>>> >>>> and this then causes deserialisation errors later on a
>>>>>>> different node.
>>>>>>> >>>>
>>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that
>>>>>>> Spark
>>>>>>> >>>> probably
>>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for cust=
om Kryo
>>>>>>> >>>> registrators.
>>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>>> >>>> deterministically
>>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serialis=
ation /
>>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be f=
ound (as
>>>>>>> in this
>>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>>  Either way,
>>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound except=
ion
>>>>>>> earlier, than
>>>>>>> >>>> the
>>>>>>> >>>> weirder errors later on.
>>>>>>> >>>>
>>>>>>> >>>> I have some ideas on potential solutions to this issue, but I=
=E2=80=99m
>>>>>>> keen for
>>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>>> >>>>
>>>>>>> >>>> 1. The simplest approach to fixing this would be to just make
>>>>>>> the
>>>>>>> >>>> application jar available to the connection manager threads,
>>>>>>> but I=E2=80=99m
>>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applica=
tion jar
>>>>>>> to just
>>>>>>> >>>> the
>>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if t=
here are
>>>>>>> any other
>>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>>> >>>> deserialisation.
>>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>>>> thread=E2=80=99s
>>>>>>> >>>> class
>>>>>>> >>>> loader to include the application jar, then restore the class
>>>>>>> loader
>>>>>>> >>>> after
>>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if th=
is would
>>>>>>> have any
>>>>>>> >>>> other side-effects.
>>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>>>> threads,
>>>>>>> >>>> rather than delaying serialisation until later, when it can be
>>>>>>> done
>>>>>>> >>>> only if
>>>>>>> >>>> needed.  This approach would probably have negative performanc=
e
>>>>>>> >>>> consequences.
>>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation /
>>>>>>> >>>> deserialisation that has the application jar on the class path=
.
>>>>>>> >>>>  Serialisation / deserialisation would be the only thing these
>>>>>>> threads
>>>>>>> >>>> do,
>>>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>>>> application
>>>>>>> >>>> jar and other jars.
>>>>>>> >>>>
>>>>>>> >>>> #4 sounds like the best approach to me, but I think would
>>>>>>> require
>>>>>>> >>>> considerable knowledge of Spark internals, which is beyond me =
at
>>>>>>> >>>> present.
>>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>>> >>>>
>>>>>>> >>>> Cheers,
>>>>>>> >>>>
>>>>>>> >>>> Graham
>>>>>>> >>>>
>>>>>>> >>>
>>>>>>> >>>
>>>>>>> >>
>>>>>>> >
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7bdc8016650e1505009aac7c--

From dev-return-8888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 19:22:19 2014
Return-Path: <dev-return-8888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8FE1119E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 19:22:19 +0000 (UTC)
Received: (qmail 22307 invoked by uid 500); 14 Aug 2014 19:22:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22244 invoked by uid 500); 14 Aug 2014 19:22:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22228 invoked by uid 99); 14 Aug 2014 19:22:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 19:22:18 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 19:21:52 +0000
Received: by mail-pa0-f49.google.com with SMTP id hz1so2160037pad.36
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 12:21:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=nQz8AU/r7qRTYGwtf1cFqQZElpyIj5rtTqUpOtlEYPo=;
        b=JiKYo/66AU2n983EJ0h2sigiMO8xapDw5ZWjHCCfRuor+KudfDmxu4QGCRFZ6muM6K
         reLIAd+T2t0MtZT9T8ui0YC9/GIi/62DRjnkGdD/2b4xTrnFeBQlJ3ZE6UQFfeaB50NZ
         ToA79Rj9LYvwBdlY8b+umLpMrjkjIAmTL+dDufEq5EyhGGvnJMEfwkT1pxCI3XFi+s0N
         q56DR3KBqKi17asmb/rcUegGvii3GaHjQuH+zworfct6OS0P0oecVk2xVMadqBwAjCSt
         q6lY7IBYhfyawImifrKkwkUhxXWgFqLWmFrEP9T+LF782ttVdrqF3RMF9G3QRVhkw+iq
         1dGA==
X-Received: by 10.66.155.105 with SMTP id vv9mr6229833pab.61.1408044111025;
        Thu, 14 Aug 2014 12:21:51 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id fw3sm6070809pbb.2.2014.08.14.12.21.47
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 14 Aug 2014 12:21:48 -0700 (PDT)
Date: Thu, 14 Aug 2014 12:21:46 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: dev@spark.apache.org, Ignacio Zendejas
 <ignacio.zendejas.cs@gmail.com>
Message-ID: <etPan.53ed0c4a.836c40e.3d37@mbp-3>
In-Reply-To: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53ed0c4a_2901d82_3d37"
X-Virus-Checked: Checked by ClamAV on apache.org

--53ed0c4a_2901d82_3d37
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Just as a note on this paper, apart from implementing the algorithms in naive Python, they also run it in a fairly inefficient way. In particular their implementations send the model out with every task closure, which is really expensive for a large model, and bring it back with collectAsMap(). It would be much more efficient to send it e.g. with SparkContext.broadcast() or keep it distributed on the cluster throughout the computation, instead of making the drive node a bottleneck for communication.

Implementing ML algorithms well by hand is unfortunately difficult, and this is why we have MLlib. The hope is that you either get your desired algorithm out of the box or get a higher-level primitive (e.g. stochastic gradient descent) that you can plug some functions into, without worrying about the communication.

Matei

On August 13, 2014 at 11:10:02 AM, Ignacio Zendejas (ignacio.zendejas.cs@gmail.com) wrote:

Has anyone had a chance to look at this paper (with title in subject)? 
http://www.cs.rice.edu/~lp6/comparison.pdf 

Interesting that they chose to use Python alone. Do we know how much faster 
Scala is vs. Python in general, if at all? 

As with any and all benchmarks, I'm sure there are caveats, but it'd be 
nice to have a response to the question above for starters. 

Thanks, 
Ignacio 

--53ed0c4a_2901d82_3d37--


From dev-return-8889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 20:25:34 2014
Return-Path: <dev-return-8889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7095A11FB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 20:25:34 +0000 (UTC)
Received: (qmail 95399 invoked by uid 500); 14 Aug 2014 20:25:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95335 invoked by uid 500); 14 Aug 2014 20:25:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95320 invoked by uid 99); 14 Aug 2014 20:25:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 20:25:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 20:25:29 +0000
Received: by mail-qc0-f176.google.com with SMTP id m20so1586909qcx.7
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 13:25:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+gb0tnMC4065Ri8r4DcbDXWLttyAsX3hMQae8PoMJ3M=;
        b=A66cC1I71PCFdsMiSCd68sCaRsCOFzPNbk5nRAcxtHzagJCX0WP39Ec0/bAGtl2DCZ
         CN0N3EMJj3jc4s4t47rCOiHM4hEfV8wscDk/WtgrLHhxo4Q9g9e/lVWlyfkvdlmvuy7Z
         thXduk0rwjO0Jwtp6FCHO6/QSq1frqW+NvB3yXq25fQPiMpC9/GleSKlZHTjUXVjLSmZ
         9jh1Ea5RpOzCrRpWUGNU12dE7nVRKSGlxommk/dV4vG01bXhxLKoPJEbh/LjIts4jGXz
         F4VeHFnSmeSJri3AoWS+t626vijUp6Ej+gvLAILW38RqgT3kFFMFJ8Sj4ReR9QdHk9jp
         LDQw==
MIME-Version: 1.0
X-Received: by 10.224.75.130 with SMTP id y2mr20786360qaj.72.1408047907951;
 Thu, 14 Aug 2014 13:25:07 -0700 (PDT)
Received: by 10.140.102.1 with HTTP; Thu, 14 Aug 2014 13:25:07 -0700 (PDT)
In-Reply-To: <etPan.53ed0c4a.836c40e.3d37@mbp-3>
References: <CANJrAvBfzo_9d3TjKv9bQ59A82nau6m_cPrZ3cM9n2XT3YqeZw@mail.gmail.com>
	<etPan.53ed0c4a.836c40e.3d37@mbp-3>
Date: Thu, 14 Aug 2014 13:25:07 -0700
Message-ID: <CANJrAvDOAegcaV=L3VgCjt2f-jH2Mmtrtd9QJA1xgb6Efq2rYA@mail.gmail.com>
Subject: Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ef7e615e0605009cb3ec
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef7e615e0605009cb3ec
Content-Type: text/plain; charset=UTF-8

Thanks, Jeremy! That's awesome. There's a group at Facebook that is
considering using Spark, so to have more projects to refer to is great.

And Matei, I completely agree. MLlib is very exciting. I respect how well
you guys are managing the project for quality. This will set the Spark
ecosystem apart beyond the already impressive gains in performance and
productivity.

cheers,
Ignacio



On Thu, Aug 14, 2014 at 12:21 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Just as a note on this paper, apart from implementing the algorithms in
> naive Python, they also run it in a fairly inefficient way. In particular
> their implementations send the model out with every task closure, which is
> really expensive for a large model, and bring it back with collectAsMap().
> It would be much more efficient to send it e.g. with
> SparkContext.broadcast() or keep it distributed on the cluster throughout
> the computation, instead of making the drive node a bottleneck for
> communication.
>
> Implementing ML algorithms well by hand is unfortunately difficult, and
> this is why we have MLlib. The hope is that you either get your desired
> algorithm out of the box or get a higher-level primitive (e.g. stochastic
> gradient descent) that you can plug some functions into, without worrying
> about the communication.
>
> Matei
>
> On August 13, 2014 at 11:10:02 AM, Ignacio Zendejas (
> ignacio.zendejas.cs@gmail.com) wrote:
>
> Has anyone had a chance to look at this paper (with title in subject)?
> http://www.cs.rice.edu/~lp6/comparison.pdf
>
> Interesting that they chose to use Python alone. Do we know how much
> faster
> Scala is vs. Python in general, if at all?
>
> As with any and all benchmarks, I'm sure there are caveats, but it'd be
> nice to have a response to the question above for starters.
>
> Thanks,
> Ignacio
>
>

--001a11c2ef7e615e0605009cb3ec--

From dev-return-8890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 22:39:56 2014
Return-Path: <dev-return-8890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72E471156A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 22:39:56 +0000 (UTC)
Received: (qmail 71542 invoked by uid 500); 14 Aug 2014 22:39:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71474 invoked by uid 500); 14 Aug 2014 22:39:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71463 invoked by uid 99); 14 Aug 2014 22:39:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 22:39:55 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 22:39:28 +0000
Received: from EXDR01-WEST.YOJOE.local (10.160.10.135) by
 ex02-west.YOJOE.local (10.160.10.131) with Microsoft SMTP Server (TLS) id
 14.3.195.1; Thu, 14 Aug 2014 15:39:25 -0700
Received: from EX02-WEST.YOJOE.local ([169.254.1.40]) by
 EXDR01-WEST.YOJOE.local ([169.254.3.37]) with mapi id 14.03.0195.001; Thu, 14
 Aug 2014 15:39:25 -0700
From: Mingyu Kim <mkim@palantir.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Topic: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Index: AQHPuBCZ/+f7rUz2C0O3higuAjnLYQ==
Date: Thu, 14 Aug 2014 22:39:24 +0000
Message-ID: <D01288A8.12442%mkim@palantir.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: yes
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.100.91.90]
Content-Type: multipart/signed; protocol="application/pkcs7-signature";
	micalg=sha1; boundary="B_3490875560_8539693"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3490875560_8539693
Content-type: multipart/alternative;
	boundary="B_3490875560_8577719"


--B_3490875560_8577719
Content-type: text/plain;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

I ran a really simple code that runs with Spark 1.0.2 jar and connects to a
Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException. I
filed the bug at https://issues.apache.org/jira/browse/SPARK-3050.

I assumed the minor and patch releases shouldn=B9t break compatibility. Is
that correct?

Thanks,
Mingyu



--B_3490875560_8577719
Content-type: text/html;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

<html><head></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size:=
 14px; font-family: Calibri, sans-serif;"><div>I ran a really simple code th=
at runs with Spark 1.0.2 jar and connects to a Spark 1.0.1 cluster, but it f=
ails with java.io.InvalidClassException. I filed the bug at&nbsp;<a href=3D"ht=
tps://issues.apache.org/jira/browse/SPARK-3050">https://issues.apache.org/ji=
ra/browse/SPARK-3050</a>.</div><div><br></div><div>I assumed the minor and p=
atch releases shouldn&#8217;t break compatibility. Is that correct?</div><di=
v><div><br></div><div>Thanks,</div><div>Mingyu</div></div></body></html>

--B_3490875560_8577719--

--B_3490875560_8539693
Content-Type: application/pkcs7-signature; name="smime.p7s"
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename="smime.p7s"
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=

--B_3490875560_8539693--

From dev-return-8891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 14 22:43:38 2014
Return-Path: <dev-return-8891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A48DC1158E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Aug 2014 22:43:38 +0000 (UTC)
Received: (qmail 84219 invoked by uid 500); 14 Aug 2014 22:43:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84150 invoked by uid 500); 14 Aug 2014 22:43:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84129 invoked by uid 99); 14 Aug 2014 22:43:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 22:43:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Aug 2014 22:43:33 +0000
Received: by mail-qc0-f181.google.com with SMTP id x13so1713716qcv.26
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 15:43:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=C4Py9vPlXuRGzXxpDOJkClN9PgboBCto2qLDZcPfHlY=;
        b=Wnsh0v8xN9d+zyV2RrBFrecpjBkg+eLWDXG87+qfWdXmG/JPPFqxzYCVkPIK2mRhNo
         Wna2YkDJZuQtbJUmdBuOZi7teI1H163BonRzTaBIqGkUCP1ckjSV6763hDR3zlj0TwuT
         30aY1TKrFV122C05+dC25yUaTwTkraW992beCd43KvEhA4Nzw7BQnxWOYtkm4/tIJwGv
         1ywoG20eohodXXH+vILbJaPt7Dw6MIj+6+O2cBhM8PSOjnH35ybFzTlqfEIppM60C+W7
         tkOuWHVO3nmzoz42zq7CcusTEHw9tTzKY5wo4gAph+T6cyiwkS3DFHQ0JQic3M76BHqw
         6R0w==
MIME-Version: 1.0
X-Received: by 10.224.22.9 with SMTP id l9mr20302349qab.73.1408056192858; Thu,
 14 Aug 2014 15:43:12 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Thu, 14 Aug 2014 15:43:12 -0700 (PDT)
In-Reply-To: <D01288A8.12442%mkim@palantir.com>
References: <D01288A8.12442%mkim@palantir.com>
Date: Thu, 14 Aug 2014 18:43:12 -0400
Message-ID: <CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
From: Gary Malouf <malouf.gary@gmail.com>
To: Mingyu Kim <mkim@palantir.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdcac383334ca05009ea111
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdcac383334ca05009ea111
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

To be clear, is it 'compiled' against 1.0.2 or it packaged with it?


On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim <mkim@palantir.com> wrote:

> I ran a really simple code that runs with Spark 1.0.2 jar and connects to
> a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException. I
> filed the bug at https://issues.apache.org/jira/browse/SPARK-3050.
>
> I assumed the minor and patch releases shouldn=E2=80=99t break compatibil=
ity. Is
> that correct?
>
> Thanks,
> Mingyu
>

--047d7bdcac383334ca05009ea111--

From dev-return-8892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 00:45:39 2014
Return-Path: <dev-return-8892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4555E11930
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 00:45:39 +0000 (UTC)
Received: (qmail 24089 invoked by uid 500); 15 Aug 2014 00:45:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24018 invoked by uid 500); 15 Aug 2014 00:45:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24004 invoked by uid 99); 15 Aug 2014 00:45:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 00:45:38 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 00:45:34 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so1596465qac.19
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 17:45:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tv99laQTPBDbXVnfItvm5AjexI4FivWkzw2UmHKy3oQ=;
        b=dfY04xsWDU/TmtXUGd/iXAX1k+Y9ZVB5NuKh2E8JWvw3nryjpWE7YyHuuoh75Lm9oN
         iwgKcbhc1TxHVTxoXbh6e8qb2FL35hPFQV9sK4+50aYtdPhJaRNopoKgAoDXO/UiUjCq
         B+DLhr270xq0EDm222B4WurqtLVYOn6hTIR8YrmMXwutA4wnNvRk+KCFLtJZ1dFpAaad
         pkcfiMqLPg6/y4QPL4w3hgNVAaW0A7N98M6N5v4R81VTYdRAuEb7c0x4qGTI3XlXIy7x
         AmYZDkW3zXpWmfxv+l8eH9BA213Rt+J5mH1tUdYlz4J0de/JubWQL4/Et11Wvwn3u8Kg
         wPdA==
MIME-Version: 1.0
X-Received: by 10.224.79.139 with SMTP id p11mr22459474qak.93.1408063513280;
 Thu, 14 Aug 2014 17:45:13 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Thu, 14 Aug 2014 17:45:13 -0700 (PDT)
In-Reply-To: <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
	<CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
	<CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
	<CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com>
Date: Thu, 14 Aug 2014 17:45:13 -0700
Message-ID: <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Debasish Das <debasish.das83@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Graham Dennis <graham.dennis@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc801a87c7f50500a05578
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc801a87c7f50500a05578
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Is there a fix that I can test ? I have the flows setup for both standalone
and YARN runs...

Thanks.
Deb



On Thu, Aug 14, 2014 at 10:59 AM, Reynold Xin <rxin@databricks.com> wrote:

> Yes, I understand it might not work for custom serializer, but that is a
> much less common path.
>
> Basically I want a quick fix for 1.1 release (which is coming up soon). I
> would not be comfortable making big changes to class path late into the
> release cycle. We can do that for 1.2.
>
>
>
>
>
> On Thu, Aug 14, 2014 at 2:35 AM, Graham Dennis <graham.dennis@gmail.com>
> wrote:
>
>> That should work, but would you also make these changes to the
>> JavaSerializer?  The API of these is the same so that you can select one=
 or
>> the other (or in theory a custom serializer)?  This also wouldn't addres=
s
>> the problem of shipping custom *serializers* (not kryo registrators) in
>> user jars.
>>
>> On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Graham,
>>>
>>> SparkEnv only creates a KryoSerializer, but as I understand that
>>> serializer doesn't actually initializes the registrator since that is o=
nly
>>> called when newKryo() is called when KryoSerializerInstance is initiali=
zed.
>>>
>>> Basically I'm thinking a quick fix for 1.2:
>>>
>>> 1. Add a classLoader field to KryoSerializer; initialize new
>>> KryoSerializerInstance with that class loader
>>>
>>>  2. Set that classLoader to the executor's class loader when Executor i=
s
>>> initialized.
>>>
>>> Then all deser calls should be using the executor's class loader.
>>>
>>>
>>>
>>>
>>> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <graham.dennis@gmail.co=
m
>>> > wrote:
>>>
>>>> Hi Reynold,
>>>>
>>>> That would solve this specific issue, but you'd need to be careful tha=
t
>>>> you never created a serialiser instance before the first task is recei=
ved.
>>>>  Currently in Executor.TaskRunner.run a closure serialiser instance is
>>>> created before any application jars are downloaded, but that could be
>>>> moved.  To me, this seems a little fragile.
>>>>
>>>> However there is a related issue where you can't ship a custom
>>>> serialiser in an application jar because the serialiser is instantiate=
d
>>>> when the SparkEnv object is created, which is before any tasks are rec=
eived
>>>> by the executor.  The above approach wouldn't help with this problem.
>>>>  Additionally, the YARN scheduler currently uses this approach of addi=
ng
>>>> the application jar to the Executor classpath, so it would make things=
 a
>>>> bit more uniform.
>>>>
>>>> Cheers,
>>>> Graham
>>>>
>>>>
>>>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>>>
>>>>> Graham,
>>>>>
>>>>> Thanks for working on this. This is an important bug to fix.
>>>>>
>>>>>  I don't have the whole context and obviously I haven't spent nearly
>>>>> as much time on this as you have, but I'm wondering what if we always=
 pass
>>>>> the executor's ClassLoader to the Kryo serializer? Will that solve th=
is
>>>>> problem?
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <
>>>>> graham.dennis@gmail.com> wrote:
>>>>>
>>>>>> Hi Deb,
>>>>>>
>>>>>> The only alternative serialiser is the JavaSerialiser (the default).
>>>>>>  Theoretically Spark supports custom serialisers, but due to a relat=
ed
>>>>>> issue, custom serialisers currently can't live in application jars a=
nd must
>>>>>> be available to all executors at launch.  My PR fixes this issue as =
well,
>>>>>> allowing custom serialisers to be shipped in application jars.
>>>>>>
>>>>>> Graham
>>>>>>
>>>>>>
>>>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Sorry I just saw Graham's email after sending my previous email
>>>>>>> about this bug...
>>>>>>>
>>>>>>> I have been seeing this same issue on our ALS runs last week but I
>>>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core 1=
.0...
>>>>>>>
>>>>>>> What's the status of this PR ? Will this fix be back-ported to 1.0.=
1
>>>>>>> as we are running 1.0.1 stable standalone cluster ?
>>>>>>>
>>>>>>> Till the PR merges does it make sense to not use Kryo ? What are th=
e
>>>>>>> other recommended efficient serializers ?
>>>>>>>
>>>>>>> Thanks.
>>>>>>> Deb
>>>>>>>
>>>>>>>
>>>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>
>>>>>>>> I now have a complete pull request for this issue that I'd like to
>>>>>>>> get
>>>>>>>> reviewed and committed.  The PR is available here:
>>>>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase
>>>>>>>> for the
>>>>>>>> issue I described.  I've also submitted a related PR (
>>>>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>>>>> raised
>>>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>>>> swallowed.
>>>>>>>>
>>>>>>>> Thanks,
>>>>>>>> Graham
>>>>>>>>
>>>>>>>>
>>>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>> > I've submitted a work-in-progress pull request for this issue
>>>>>>>> that I'd
>>>>>>>> > like feedback on.  See https://github.com/apache/spark/pull/1890
>>>>>>>> . I've
>>>>>>>> > also submitted a pull request for the related issue that the
>>>>>>>> exceptions hit
>>>>>>>> > when trying to use a custom kryo registrator are being swallowed=
:
>>>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>>>> >
>>>>>>>> > The approach in my pull request is to get the Worker processes t=
o
>>>>>>>> download
>>>>>>>> > the application jars and add them to the Executor class path at
>>>>>>>> launch
>>>>>>>> > time. There are a couple of things that still need to be done
>>>>>>>> before this
>>>>>>>> > can be merged:
>>>>>>>> > 1. At the moment, the first time a task runs in the executor, th=
e
>>>>>>>> > application jars are downloaded again.  My solution here would b=
e
>>>>>>>> to make
>>>>>>>> > the executor not download any jars that already exist.
>>>>>>>>  Previously, the
>>>>>>>> > driver & executor kept track of the timestamp of jar files and
>>>>>>>> would
>>>>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>>>>> previous
>>>>>>>> > version of the updated jar may have already been loaded into the
>>>>>>>> executor,
>>>>>>>> > so the updated jar may have no effect.  As my current pull
>>>>>>>> request removes
>>>>>>>> > the timestamp for jars, just checking whether the jar exists wil=
l
>>>>>>>> allow us
>>>>>>>> > to avoid downloading the jars again.
>>>>>>>> > 2. Tests. :-)
>>>>>>>> >
>>>>>>>> > A side-benefit of my pull request is that you will be able to us=
e
>>>>>>>> custom
>>>>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>>>>> serialiser
>>>>>>>> > instance is created in the Executor process before the first tas=
k
>>>>>>>> is
>>>>>>>> > received and therefore before any user jars are downloaded.  As
>>>>>>>> this PR
>>>>>>>> > adds user jars to the Executor process at launch time, this won'=
t
>>>>>>>> be an
>>>>>>>> > issue.
>>>>>>>> >
>>>>>>>> >
>>>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>> wrote:
>>>>>>>> >
>>>>>>>> >> See my comment on
>>>>>>>> https://issues.apache.org/jira/browse/SPARK-2878 for
>>>>>>>> >> the full stacktrace, but it's in the
>>>>>>>> BlockManager/BlockManagerWorker where
>>>>>>>> >> it's trying to fulfil a "getBlock" request for another node.
>>>>>>>>  The objects
>>>>>>>> >> that would be in the block haven't yet been serialised, and tha=
t
>>>>>>>> then
>>>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>>>> >> MemoryStore.scala:102.
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com> wrote=
:
>>>>>>>> >>
>>>>>>>> >>> I don't think it was a conscious design decision to not includ=
e
>>>>>>>> the
>>>>>>>> >>> application classes in the connection manager serializer. We
>>>>>>>> should fix
>>>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>>>> >>>
>>>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>>>> complexity to
>>>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>>>> blocking/non-blocking
>>>>>>>> >>> logic within task threads) that can be error prone, so I think
>>>>>>>> it is best
>>>>>>>> >>> to stay away from that right now.
>>>>>>>> >>>
>>>>>>>> >>>
>>>>>>>> >>>
>>>>>>>> >>>
>>>>>>>> >>>
>>>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>>>> graham.dennis@gmail.com>
>>>>>>>> >>> wrote:
>>>>>>>> >>>
>>>>>>>> >>>> Hi Spark devs,
>>>>>>>> >>>>
>>>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which
>>>>>>>> occurs when
>>>>>>>> >>>> using
>>>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to register
>>>>>>>> custom
>>>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>>>> >>>> non-deterministically
>>>>>>>> >>>> causes Kryo to have different ID number =3D> class name maps =
on
>>>>>>>> different
>>>>>>>> >>>> nodes, which then causes weird exceptions (ClassCastException=
,
>>>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>>>> >>>> deserialisation
>>>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the i=
ssue here:
>>>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>>>> >>>>
>>>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to t=
ry and
>>>>>>>> address
>>>>>>>> >>>> this,
>>>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve thi=
s and I=E2=80=99d
>>>>>>>> like to
>>>>>>>> >>>> get
>>>>>>>> >>>> feedback / ideas on how to address this.
>>>>>>>> >>>>
>>>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>>>> processes
>>>>>>>> >>>> during
>>>>>>>> >>>> operation.  My custom Kryo registrator is in the application
>>>>>>>> jar, and
>>>>>>>> >>>> it is
>>>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the
>>>>>>>> fact that
>>>>>>>> >>>> most
>>>>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>>>>> >>>>
>>>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deseriali=
sation is
>>>>>>>> happening
>>>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=
=E2=80=9D thread,
>>>>>>>> which has
>>>>>>>> >>>> the
>>>>>>>> >>>> thread's class loader set to contain the application jar.
>>>>>>>>  This happens
>>>>>>>> >>>> in
>>>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and from
>>>>>>>> what I can
>>>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>>>> application jar
>>>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>>>> >>>> ConnectionManager threads sometimes need to
>>>>>>>> serialise/deserialise
>>>>>>>> >>>> objects
>>>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the objec=
ts haven=E2=80=99t
>>>>>>>> previously been
>>>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t h=
ave the
>>>>>>>> application
>>>>>>>> >>>> jar available from their class loader, when it tries to look
>>>>>>>> up the
>>>>>>>> >>>> custom
>>>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>>>> exception, which
>>>>>>>> >>>> results in a different ID number =E2=80=94> class mapping for=
 this kryo
>>>>>>>> >>>> instance,
>>>>>>>> >>>> and this then causes deserialisation errors later on a
>>>>>>>> different node.
>>>>>>>> >>>>
>>>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that
>>>>>>>> Spark
>>>>>>>> >>>> probably
>>>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for cus=
tom Kryo
>>>>>>>> >>>> registrators.
>>>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>>>> >>>> deterministically
>>>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at seriali=
sation /
>>>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be =
found (as
>>>>>>>> in this
>>>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>>>  Either way,
>>>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound excep=
tion
>>>>>>>> earlier, than
>>>>>>>> >>>> the
>>>>>>>> >>>> weirder errors later on.
>>>>>>>> >>>>
>>>>>>>> >>>> I have some ideas on potential solutions to this issue, but
>>>>>>>> I=E2=80=99m keen for
>>>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>>>> >>>>
>>>>>>>> >>>> 1. The simplest approach to fixing this would be to just make
>>>>>>>> the
>>>>>>>> >>>> application jar available to the connection manager threads,
>>>>>>>> but I=E2=80=99m
>>>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the applic=
ation jar
>>>>>>>> to just
>>>>>>>> >>>> the
>>>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if =
there are
>>>>>>>> any other
>>>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>>>> >>>> deserialisation.
>>>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>>>>> thread=E2=80=99s
>>>>>>>> >>>> class
>>>>>>>> >>>> loader to include the application jar, then restore the class
>>>>>>>> loader
>>>>>>>> >>>> after
>>>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if t=
his would
>>>>>>>> have any
>>>>>>>> >>>> other side-effects.
>>>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>>>>> threads,
>>>>>>>> >>>> rather than delaying serialisation until later, when it can b=
e
>>>>>>>> done
>>>>>>>> >>>> only if
>>>>>>>> >>>> needed.  This approach would probably have negative performan=
ce
>>>>>>>> >>>> consequences.
>>>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation =
/
>>>>>>>> >>>> deserialisation that has the application jar on the class pat=
h.
>>>>>>>> >>>>  Serialisation / deserialisation would be the only thing thes=
e
>>>>>>>> threads
>>>>>>>> >>>> do,
>>>>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>>>>> application
>>>>>>>> >>>> jar and other jars.
>>>>>>>> >>>>
>>>>>>>> >>>> #4 sounds like the best approach to me, but I think would
>>>>>>>> require
>>>>>>>> >>>> considerable knowledge of Spark internals, which is beyond me
>>>>>>>> at
>>>>>>>> >>>> present.
>>>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>>>> >>>>
>>>>>>>> >>>> Cheers,
>>>>>>>> >>>>
>>>>>>>> >>>> Graham
>>>>>>>> >>>>
>>>>>>>> >>>
>>>>>>>> >>>
>>>>>>>> >>
>>>>>>>> >
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7bdc801a87c7f50500a05578--

From dev-return-8893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 00:49:03 2014
Return-Path: <dev-return-8893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 463471194C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 00:49:03 +0000 (UTC)
Received: (qmail 30971 invoked by uid 500); 15 Aug 2014 00:49:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30907 invoked by uid 500); 15 Aug 2014 00:49:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30889 invoked by uid 99); 15 Aug 2014 00:49:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 00:49:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 00:48:58 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so1579653qap.23
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 17:48:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=nwQJ0BoJflojlVi7q0wiGdkvQToghPgZCajo5IkswR8=;
        b=T0raJ+vIkFA7He8Wvfw76bZOhcp1x+HlwG6IVqheEIFvQk3I9BylKDriTqoqz8NdcK
         Mz1+dZEEqkf2H9/duGudEUP3bcj+t1GxF4gNlKRq2np2zQgM3QLnbBJyI6Kh7pmQGaFT
         i+nXDsO1zQyl6D3CEXTwQxTWtiLyDM5HeGph+CaRHudquCI9fDt/yO6pVXm46DoIHJ+j
         CInyu1+CALKEWUWA1f42kUjB7JWGysYOPU7MZCTxjM0fl6LRz17E4mAQcCEhMxpdC3Xq
         K7PqYAC3zdAuJCcx+2Hy5TJLKdTLbvB7p/iw3IFiMqXdTnIoZXWRmGnmObzlDb9x6XBp
         7fIg==
X-Gm-Message-State: ALoCoQlVh5xOINq9tqdoxoEaQeczFDqYyGgR7mpMEwrvPafsPQJGQgJtGMhj+NB3nz663NtSYyWz
X-Received: by 10.224.137.65 with SMTP id v1mr22465855qat.53.1408063716558;
 Thu, 14 Aug 2014 17:48:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Thu, 14 Aug 2014 17:48:16 -0700 (PDT)
In-Reply-To: <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
 <CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
 <CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
 <CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
 <CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
 <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
 <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
 <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
 <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
 <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
 <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
 <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com> <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 14 Aug 2014 17:48:16 -0700
Message-ID: <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
To: Debasish Das <debasish.das83@gmail.com>
Cc: Graham Dennis <graham.dennis@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2cc30a5ae1f0500a061f2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2cc30a5ae1f0500a061f2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here: https://github.com/apache/spark/pull/1948



On Thu, Aug 14, 2014 at 5:45 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Is there a fix that I can test ? I have the flows setup for both
> standalone and YARN runs...
>
> Thanks.
> Deb
>
>
>
> On Thu, Aug 14, 2014 at 10:59 AM, Reynold Xin <rxin@databricks.com> wrote=
:
>
>> Yes, I understand it might not work for custom serializer, but that is a
>> much less common path.
>>
>> Basically I want a quick fix for 1.1 release (which is coming up soon). =
I
>> would not be comfortable making big changes to class path late into the
>> release cycle. We can do that for 1.2.
>>
>>
>>
>>
>>
>> On Thu, Aug 14, 2014 at 2:35 AM, Graham Dennis <graham.dennis@gmail.com>
>> wrote:
>>
>>> That should work, but would you also make these changes to the
>>> JavaSerializer?  The API of these is the same so that you can select on=
e or
>>> the other (or in theory a custom serializer)?  This also wouldn't addre=
ss
>>> the problem of shipping custom *serializers* (not kryo registrators) in
>>> user jars.
>>>
>>> On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:
>>>
>>>> Graham,
>>>>
>>>> SparkEnv only creates a KryoSerializer, but as I understand that
>>>> serializer doesn't actually initializes the registrator since that is =
only
>>>> called when newKryo() is called when KryoSerializerInstance is initial=
ized.
>>>>
>>>> Basically I'm thinking a quick fix for 1.2:
>>>>
>>>> 1. Add a classLoader field to KryoSerializer; initialize new
>>>> KryoSerializerInstance with that class loader
>>>>
>>>>  2. Set that classLoader to the executor's class loader when Executor
>>>> is initialized.
>>>>
>>>> Then all deser calls should be using the executor's class loader.
>>>>
>>>>
>>>>
>>>>
>>>> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <
>>>> graham.dennis@gmail.com> wrote:
>>>>
>>>>> Hi Reynold,
>>>>>
>>>>> That would solve this specific issue, but you'd need to be careful
>>>>> that you never created a serialiser instance before the first task is
>>>>> received.  Currently in Executor.TaskRunner.run a closure serialiser
>>>>> instance is created before any application jars are downloaded, but t=
hat
>>>>> could be moved.  To me, this seems a little fragile.
>>>>>
>>>>> However there is a related issue where you can't ship a custom
>>>>> serialiser in an application jar because the serialiser is instantiat=
ed
>>>>> when the SparkEnv object is created, which is before any tasks are re=
ceived
>>>>> by the executor.  The above approach wouldn't help with this problem.
>>>>>  Additionally, the YARN scheduler currently uses this approach of add=
ing
>>>>> the application jar to the Executor classpath, so it would make thing=
s a
>>>>> bit more uniform.
>>>>>
>>>>> Cheers,
>>>>> Graham
>>>>>
>>>>>
>>>>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>>>>
>>>>>> Graham,
>>>>>>
>>>>>> Thanks for working on this. This is an important bug to fix.
>>>>>>
>>>>>>  I don't have the whole context and obviously I haven't spent nearly
>>>>>> as much time on this as you have, but I'm wondering what if we alway=
s pass
>>>>>> the executor's ClassLoader to the Kryo serializer? Will that solve t=
his
>>>>>> problem?
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <
>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>
>>>>>>> Hi Deb,
>>>>>>>
>>>>>>> The only alternative serialiser is the JavaSerialiser (the default)=
.
>>>>>>>  Theoretically Spark supports custom serialisers, but due to a rela=
ted
>>>>>>> issue, custom serialisers currently can't live in application jars =
and must
>>>>>>> be available to all executors at launch.  My PR fixes this issue as=
 well,
>>>>>>> allowing custom serialisers to be shipped in application jars.
>>>>>>>
>>>>>>> Graham
>>>>>>>
>>>>>>>
>>>>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Sorry I just saw Graham's email after sending my previous email
>>>>>>>> about this bug...
>>>>>>>>
>>>>>>>> I have been seeing this same issue on our ALS runs last week but I
>>>>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core =
1.0...
>>>>>>>>
>>>>>>>> What's the status of this PR ? Will this fix be back-ported to
>>>>>>>> 1.0.1 as we are running 1.0.1 stable standalone cluster ?
>>>>>>>>
>>>>>>>> Till the PR merges does it make sense to not use Kryo ? What are
>>>>>>>> the other recommended efficient serializers ?
>>>>>>>>
>>>>>>>> Thanks.
>>>>>>>> Deb
>>>>>>>>
>>>>>>>>
>>>>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>>
>>>>>>>>> I now have a complete pull request for this issue that I'd like t=
o
>>>>>>>>> get
>>>>>>>>> reviewed and committed.  The PR is available here:
>>>>>>>>> https://github.com/apache/spark/pull/1890 and includes a testcase
>>>>>>>>> for the
>>>>>>>>> issue I described.  I've also submitted a related PR (
>>>>>>>>> https://github.com/apache/spark/pull/1827) that causes exceptions
>>>>>>>>> raised
>>>>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>>>>> swallowed.
>>>>>>>>>
>>>>>>>>> Thanks,
>>>>>>>>> Graham
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>> > I've submitted a work-in-progress pull request for this issue
>>>>>>>>> that I'd
>>>>>>>>> > like feedback on.  See https://github.com/apache/spark/pull/189=
0
>>>>>>>>> . I've
>>>>>>>>> > also submitted a pull request for the related issue that the
>>>>>>>>> exceptions hit
>>>>>>>>> > when trying to use a custom kryo registrator are being swallowe=
d:
>>>>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>>>>> >
>>>>>>>>> > The approach in my pull request is to get the Worker processes
>>>>>>>>> to download
>>>>>>>>> > the application jars and add them to the Executor class path at
>>>>>>>>> launch
>>>>>>>>> > time. There are a couple of things that still need to be done
>>>>>>>>> before this
>>>>>>>>> > can be merged:
>>>>>>>>> > 1. At the moment, the first time a task runs in the executor, t=
he
>>>>>>>>> > application jars are downloaded again.  My solution here would
>>>>>>>>> be to make
>>>>>>>>> > the executor not download any jars that already exist.
>>>>>>>>>  Previously, the
>>>>>>>>> > driver & executor kept track of the timestamp of jar files and
>>>>>>>>> would
>>>>>>>>> > redownload 'updated' jars, however this never made sense as the
>>>>>>>>> previous
>>>>>>>>> > version of the updated jar may have already been loaded into th=
e
>>>>>>>>> executor,
>>>>>>>>> > so the updated jar may have no effect.  As my current pull
>>>>>>>>> request removes
>>>>>>>>> > the timestamp for jars, just checking whether the jar exists
>>>>>>>>> will allow us
>>>>>>>>> > to avoid downloading the jars again.
>>>>>>>>> > 2. Tests. :-)
>>>>>>>>> >
>>>>>>>>> > A side-benefit of my pull request is that you will be able to
>>>>>>>>> use custom
>>>>>>>>> > serialisers that are distributed in a user jar.  Currently, the
>>>>>>>>> serialiser
>>>>>>>>> > instance is created in the Executor process before the first
>>>>>>>>> task is
>>>>>>>>> > received and therefore before any user jars are downloaded.  As
>>>>>>>>> this PR
>>>>>>>>> > adds user jars to the Executor process at launch time, this
>>>>>>>>> won't be an
>>>>>>>>> > issue.
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>> >
>>>>>>>>> >> See my comment on
>>>>>>>>> https://issues.apache.org/jira/browse/SPARK-2878 for
>>>>>>>>> >> the full stacktrace, but it's in the
>>>>>>>>> BlockManager/BlockManagerWorker where
>>>>>>>>> >> it's trying to fulfil a "getBlock" request for another node.
>>>>>>>>>  The objects
>>>>>>>>> >> that would be in the block haven't yet been serialised, and
>>>>>>>>> that then
>>>>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>>>>> >> MemoryStore.scala:102.
>>>>>>>>> >>
>>>>>>>>> >>
>>>>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com>
>>>>>>>>> wrote:
>>>>>>>>> >>
>>>>>>>>> >>> I don't think it was a conscious design decision to not
>>>>>>>>> include the
>>>>>>>>> >>> application classes in the connection manager serializer. We
>>>>>>>>> should fix
>>>>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>>>>> >>>
>>>>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>>>>> complexity to
>>>>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>>>>> blocking/non-blocking
>>>>>>>>> >>> logic within task threads) that can be error prone, so I thin=
k
>>>>>>>>> it is best
>>>>>>>>> >>> to stay away from that right now.
>>>>>>>>> >>>
>>>>>>>>> >>>
>>>>>>>>> >>>
>>>>>>>>> >>>
>>>>>>>>> >>>
>>>>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>>>>> graham.dennis@gmail.com>
>>>>>>>>> >>> wrote:
>>>>>>>>> >>>
>>>>>>>>> >>>> Hi Spark devs,
>>>>>>>>> >>>>
>>>>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which
>>>>>>>>> occurs when
>>>>>>>>> >>>> using
>>>>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to registe=
r
>>>>>>>>> custom
>>>>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>>>>> >>>> non-deterministically
>>>>>>>>> >>>> causes Kryo to have different ID number =3D> class name maps=
 on
>>>>>>>>> different
>>>>>>>>> >>>> nodes, which then causes weird exceptions (ClassCastExceptio=
n,
>>>>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>>>>> >>>> deserialisation
>>>>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the =
issue
>>>>>>>>> here:
>>>>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>>>>> >>>>
>>>>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to =
try and
>>>>>>>>> address
>>>>>>>>> >>>> this,
>>>>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve th=
is and
>>>>>>>>> I=E2=80=99d like to
>>>>>>>>> >>>> get
>>>>>>>>> >>>> feedback / ideas on how to address this.
>>>>>>>>> >>>>
>>>>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>>>>> processes
>>>>>>>>> >>>> during
>>>>>>>>> >>>> operation.  My custom Kryo registrator is in the application
>>>>>>>>> jar, and
>>>>>>>>> >>>> it is
>>>>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by the
>>>>>>>>> fact that
>>>>>>>>> >>>> most
>>>>>>>>> >>>> of the time the custom kryo registrator is successfully run.
>>>>>>>>> >>>>
>>>>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deserial=
isation
>>>>>>>>> is happening
>>>>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=
=E2=80=9D thread,
>>>>>>>>> which has
>>>>>>>>> >>>> the
>>>>>>>>> >>>> thread's class loader set to contain the application jar.
>>>>>>>>>  This happens
>>>>>>>>> >>>> in
>>>>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and fro=
m
>>>>>>>>> what I can
>>>>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>>>>> application jar
>>>>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>>>>> >>>> ConnectionManager threads sometimes need to
>>>>>>>>> serialise/deserialise
>>>>>>>>> >>>> objects
>>>>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the obje=
cts haven=E2=80=99t
>>>>>>>>> previously been
>>>>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t =
have the
>>>>>>>>> application
>>>>>>>>> >>>> jar available from their class loader, when it tries to look
>>>>>>>>> up the
>>>>>>>>> >>>> custom
>>>>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>>>>> exception, which
>>>>>>>>> >>>> results in a different ID number =E2=80=94> class mapping fo=
r this
>>>>>>>>> kryo
>>>>>>>>> >>>> instance,
>>>>>>>>> >>>> and this then causes deserialisation errors later on a
>>>>>>>>> different node.
>>>>>>>>> >>>>
>>>>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that
>>>>>>>>> Spark
>>>>>>>>> >>>> probably
>>>>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for cu=
stom Kryo
>>>>>>>>> >>>> registrators.
>>>>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>>>>> >>>> deterministically
>>>>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at serial=
isation /
>>>>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t be=
 found
>>>>>>>>> (as in this
>>>>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>>>>  Either way,
>>>>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exce=
ption
>>>>>>>>> earlier, than
>>>>>>>>> >>>> the
>>>>>>>>> >>>> weirder errors later on.
>>>>>>>>> >>>>
>>>>>>>>> >>>> I have some ideas on potential solutions to this issue, but
>>>>>>>>> I=E2=80=99m keen for
>>>>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>>>>> >>>>
>>>>>>>>> >>>> 1. The simplest approach to fixing this would be to just mak=
e
>>>>>>>>> the
>>>>>>>>> >>>> application jar available to the connection manager threads,
>>>>>>>>> but I=E2=80=99m
>>>>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the appli=
cation
>>>>>>>>> jar to just
>>>>>>>>> >>>> the
>>>>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know if=
 there
>>>>>>>>> are any other
>>>>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>>>>> >>>> deserialisation.
>>>>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change the
>>>>>>>>> thread=E2=80=99s
>>>>>>>>> >>>> class
>>>>>>>>> >>>> loader to include the application jar, then restore the clas=
s
>>>>>>>>> loader
>>>>>>>>> >>>> after
>>>>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if =
this
>>>>>>>>> would have any
>>>>>>>>> >>>> other side-effects.
>>>>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunner
>>>>>>>>> threads,
>>>>>>>>> >>>> rather than delaying serialisation until later, when it can
>>>>>>>>> be done
>>>>>>>>> >>>> only if
>>>>>>>>> >>>> needed.  This approach would probably have negative
>>>>>>>>> performance
>>>>>>>>> >>>> consequences.
>>>>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisation=
 /
>>>>>>>>> >>>> deserialisation that has the application jar on the class
>>>>>>>>> path.
>>>>>>>>> >>>>  Serialisation / deserialisation would be the only thing
>>>>>>>>> these threads
>>>>>>>>> >>>> do,
>>>>>>>>> >>>> and this would minimise conflicts / interactions between the
>>>>>>>>> application
>>>>>>>>> >>>> jar and other jars.
>>>>>>>>> >>>>
>>>>>>>>> >>>> #4 sounds like the best approach to me, but I think would
>>>>>>>>> require
>>>>>>>>> >>>> considerable knowledge of Spark internals, which is beyond m=
e
>>>>>>>>> at
>>>>>>>>> >>>> present.
>>>>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>>>>> >>>>
>>>>>>>>> >>>> Cheers,
>>>>>>>>> >>>>
>>>>>>>>> >>>> Graham
>>>>>>>>> >>>>
>>>>>>>>> >>>
>>>>>>>>> >>>
>>>>>>>>> >>
>>>>>>>>> >
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c2cc30a5ae1f0500a061f2--

From dev-return-8894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 01:32:41 2014
Return-Path: <dev-return-8894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9D2D211ABA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 01:32:41 +0000 (UTC)
Received: (qmail 8394 invoked by uid 500); 15 Aug 2014 01:32:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8327 invoked by uid 500); 15 Aug 2014 01:32:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8315 invoked by uid 99); 15 Aug 2014 01:32:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 01:32:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 01:32:15 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so1619508oag.5
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 18:32:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=UhEtayOqnmcJRVkXUUr9lK2xefqnutLry06LMz5zu6s=;
        b=wYdg/JyOcCdA+mJCIaRZ2z2UFBU4wom8Cb11rDbA7rwfdk7UG686OiSbBbkBOzAWgL
         FiOYjp1IqUuTbivOLBovxb5rMweH9J1pW+YTTMKfZESVOsvw5v6JQaWlFKLPR4LdQr7S
         bVnzrMAi+PGLZxx/eBj++JLxugsoI7DfMF3meB+coV2F1AOSg0qXF4hYpbdID4h8XrrB
         fkSuX+1Xr1zxgMuZ+D+u9nJz2MlD6hzmnCvCCTp2QgA5feL4MGyKQjYWaJmSx6PmUs6P
         MnwFxyjNujmM5OxvoonBut2ScQpF3hTqCn7FhGnLRuNSd5b4DGTJrLJATYS9DHGvDpjm
         fH8w==
MIME-Version: 1.0
X-Received: by 10.60.159.164 with SMTP id xd4mr17240806oeb.19.1408066333877;
 Thu, 14 Aug 2014 18:32:13 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 14 Aug 2014 18:32:13 -0700 (PDT)
In-Reply-To: <CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
References: <D01288A8.12442%mkim@palantir.com>
	<CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
Date: Thu, 14 Aug 2014 18:32:13 -0700
Message-ID: <CABPQxstdGkEv3jQoU4xmdGhJqdM0dyjKyxk70L-Mjt6oL8J67Q@mail.gmail.com>
Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
From: Patrick Wendell <pwendell@gmail.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: Mingyu Kim <mkim@palantir.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd6b7e8a6adab0500a0fd32
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6b7e8a6adab0500a0fd32
Content-Type: text/plain; charset=ISO-8859-1

I commented on the bug. For driver mode, you'll need to get the
corresponding version of spark-submit for Spark 1.0.2.


On Thu, Aug 14, 2014 at 3:43 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> To be clear, is it 'compiled' against 1.0.2 or it packaged with it?
>
>
> On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim <mkim@palantir.com> wrote:
>
> > I ran a really simple code that runs with Spark 1.0.2 jar and connects to
> > a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException. I
> > filed the bug at https://issues.apache.org/jira/browse/SPARK-3050.
> >
> > I assumed the minor and patch releases shouldn't break compatibility. Is
> > that correct?
> >
> > Thanks,
> > Mingyu
> >
>

--047d7bd6b7e8a6adab0500a0fd32--

From dev-return-8895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 03:02:13 2014
Return-Path: <dev-return-8895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5290211D34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 03:02:13 +0000 (UTC)
Received: (qmail 49403 invoked by uid 500); 15 Aug 2014 03:02:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49356 invoked by uid 500); 15 Aug 2014 03:02:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49345 invoked by uid 99); 15 Aug 2014 03:02:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 03:02:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wangfei1@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 03:02:06 +0000
Received: from 172.24.2.119 (EHLO szxeml449-hub.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ATC32722;
	Fri, 15 Aug 2014 11:01:42 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml449-hub.china.huawei.com
 (10.82.67.192) with Microsoft SMTP Server id 14.3.158.1; Fri, 15 Aug 2014
 11:01:39 +0800
Message-ID: <53ED77FD.4030007@huawei.com>
Date: Fri, 15 Aug 2014 11:01:17 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: <dev@spark.apache.org>
Subject: mvn test error
Content-Type: text/plain; charset="ISO-8859-1"; format=flowed
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020201.53ED7817.011C,ss=1,re=0.000,fgs=0,
	ip=0.0.0.0,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: b09d926cea0a2b955a3c51c972947155
X-Virus-Checked: Checked by ClamAV on apache.org

env: ubuntu 14.04 + spark master buranch

mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package

mvn -Pyarn -Phadoop-2.4 -Phive test

test error:

DriverSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- driver should exit after finishing *** FAILED ***
   SparkException was thrown during property evaluation. (DriverSuite.scala:40)
     Message: Process List(./bin/spark-class, org.apache.spark.DriverWithoutCleanup, local) exited with code 1
     Occurred at table row 0 (zero based, not counting headings), which had values (
       master = local
     )

SparkSubmitSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- launch simple application with spark-submit *** FAILED ***
   org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.SimpleApplicationTest, --name, testApp, --master, local, file:/tmp/1408015655220-0/testJar-1408015655220.jar) exited with code 1

   at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
   at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply$mcV$sp(SparkSubmitSuite.scala:291)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   at org.scalatest.Transformer.apply(Transformer.scala:22)
   ...
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- spark submit includes jars passed in through --jar *** FAILED ***
   org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.JarCreationTest, --name, testApp, --master, local-cluster[2,1,512], --jars, file:/tmp/1408015659416-0/testJar-1408015659471.jar,fi
le:/tmp/1408015659472-0/testJar-1408015659513.jar, file:/tmp/1408015659415-0/testJar-1408015659416.jar) exited with code 1
   at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
   at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply$mcV$sp(SparkSubmitSuite.scala:305)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   at org.scalatest.Transformer.apply(Transformer.scala:22)
   ...


but only test the specific suite as follows will be ok:
mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=org.apache.spark.DriverSuite test

it seems when run with "mvn -Pyarn -Phadoop-2.4 -Phive test",the process with Utils.executeAndGetOutput started can not exited successfully (exitcode is not zero)

anyone has idea for this?




-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 03:22:58 2014
Return-Path: <dev-return-8896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C959911DC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 03:22:58 +0000 (UTC)
Received: (qmail 80522 invoked by uid 500); 15 Aug 2014 03:22:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80467 invoked by uid 500); 15 Aug 2014 03:22:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80455 invoked by uid 99); 15 Aug 2014 03:22:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 03:22:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wangfei1@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 03:22:31 +0000
Received: from 172.24.2.119 (EHLO szxeml403-hub.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BYF03399;
	Fri, 15 Aug 2014 11:22:15 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml403-hub.china.huawei.com
 (10.82.67.35) with Microsoft SMTP Server id 14.3.158.1; Fri, 15 Aug 2014
 11:22:14 +0800
Message-ID: <53ED7CD7.9000307@huawei.com>
Date: Fri, 15 Aug 2014 11:21:59 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: <dev@spark.apache.org>
Subject: [sql]enable spark sql cli support spark sql
Content-Type: text/plain; charset="ISO-8859-1"; format=flowed
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

hi all,
   now spark sql cli only support spark hql, i think we can enable this cli to support spark sql, do you think it's necessary?

-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 05:14:05 2014
Return-Path: <dev-return-8897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5CEB11FD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 05:14:05 +0000 (UTC)
Received: (qmail 33592 invoked by uid 500); 15 Aug 2014 05:14:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33523 invoked by uid 500); 15 Aug 2014 05:14:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33509 invoked by uid 99); 15 Aug 2014 05:14:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 05:14:04 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hao.cheng@intel.com designates 192.55.52.88 as permitted sender)
Received: from [192.55.52.88] (HELO mga01.intel.com) (192.55.52.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 05:13:38 +0000
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
  by fmsmga101.fm.intel.com with ESMTP; 14 Aug 2014 22:13:36 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,862,1389772800"; 
   d="scan'208";a="372691942"
Received: from fmsmsx108.amr.corp.intel.com ([10.18.124.206])
  by FMSMGA003.fm.intel.com with ESMTP; 14 Aug 2014 22:10:09 -0700
Received: from fmsmsx116.amr.corp.intel.com (10.18.116.20) by
 FMSMSX108.amr.corp.intel.com (10.18.124.206) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 14 Aug 2014 22:13:36 -0700
Received: from shsmsx152.ccr.corp.intel.com (10.239.6.52) by
 fmsmsx116.amr.corp.intel.com (10.18.116.20) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 14 Aug 2014 22:13:35 -0700
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.246]) by
 SHSMSX152.ccr.corp.intel.com ([169.254.6.147]) with mapi id 14.03.0195.001;
 Fri, 15 Aug 2014 13:13:33 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: scwf <wangfei1@huawei.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: [sql]enable spark sql cli support spark sql
Thread-Topic: [sql]enable spark sql cli support spark sql
Thread-Index: AQHPuDhC5mCp3XVuk0abB5vlkuMdXpvRHcYA
Date: Fri, 15 Aug 2014 05:13:33 +0000
Message-ID: <80833ADD533E324CA05C160E41B6366102726A26@shsmsx102.ccr.corp.intel.com>
References: <53ED7CD7.9000307@huawei.com>
In-Reply-To: <53ED7CD7.9000307@huawei.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Actually the SQL Parser (another SQL dialect in SparkSQL) is quite weak, an=
d only support some basic queries, not sure what's the plan for its enhance=
ment.

-----Original Message-----
From: scwf [mailto:wangfei1@huawei.com]=20
Sent: Friday, August 15, 2014 11:22 AM
To: dev@spark.apache.org
Subject: [sql]enable spark sql cli support spark sql

hi all,
   now spark sql cli only support spark hql, i think we can enable this cli=
 to support spark sql, do you think it's necessary?

--=20

Best Regards
Fei Wang

---------------------------------------------------------------------------=
-----



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional com=
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 05:55:50 2014
Return-Path: <dev-return-8898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C1ED1117D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 05:55:50 +0000 (UTC)
Received: (qmail 34995 invoked by uid 500); 15 Aug 2014 05:55:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34914 invoked by uid 500); 15 Aug 2014 05:55:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34898 invoked by uid 99); 15 Aug 2014 05:55:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 05:55:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 05:55:44 +0000
Received: by mail-pa0-f43.google.com with SMTP id lf10so2956476pab.16
        for <dev@spark.apache.org>; Thu, 14 Aug 2014 22:55:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=NT0uoo+L9cvIEWrfVaYpki7D0uwOxQeifKz54U4UMws=;
        b=v/j0t5o5TkHftYUUNHPx+49spc3EkMWRdl0PQf/l9bZsEFDK3QNuzG7tnyFNhVSXF3
         /s/YliEd8ajHcAyg43nj+SdU2oXzCsN8JYNz7d4yBAcHW8EuP1pt+BiI4CiRbnmb7YR6
         7KfPo4T94+BK6RxHIqOqELaVmoJMgwwFIEzVF2rilU3V+Rvo7JV82Vev+M5Y+c6xsHkq
         CE/uV/KNpfdYNaa6dWq8Oxd0UqmYV1UJlUXVqWXW69FC3KQD7t8yKagEplRrdINbXNXu
         uo761VxLDA0nUTF7NaZtnl3Pb8rx6V67SoWk3+k9lJuvzVZ3x6tJSHIhoaDljXLkbK25
         UXuQ==
X-Received: by 10.68.104.98 with SMTP id gd2mr5725143pbb.13.1408082123796;
        Thu, 14 Aug 2014 22:55:23 -0700 (PDT)
Received: from [127.0.0.1] (fmdmzpr01-ext.fm.intel.com. [192.55.54.36])
        by mx.google.com with ESMTPSA id p8sm10591158pdm.36.2014.08.14.22.55.21
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 14 Aug 2014 22:55:23 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [sql]enable spark sql cli support spark sql
From: Cheng Lian <lian.cs.zju@gmail.com>
In-Reply-To: <80833ADD533E324CA05C160E41B6366102726A26@shsmsx102.ccr.corp.intel.com>
Date: Fri, 15 Aug 2014 13:56:31 +0800
Cc: scwf <wangfei1@huawei.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <C036B265-BD46-4AB8-961D-A7D26EE4A9F1@gmail.com>
References: <53ED7CD7.9000307@huawei.com> <80833ADD533E324CA05C160E41B6366102726A26@shsmsx102.ccr.corp.intel.com>
To: "Cheng, Hao" <hao.cheng@intel.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

In the long run, as Michael suggested in his Spark Summit 14 talk, we=92d =
like to implement SQL-92, maybe with the help of Optiq.

On Aug 15, 2014, at 1:13 PM, Cheng, Hao <hao.cheng@intel.com> wrote:

> Actually the SQL Parser (another SQL dialect in SparkSQL) is quite =
weak, and only support some basic queries, not sure what's the plan for =
its enhancement.
>=20
> -----Original Message-----
> From: scwf [mailto:wangfei1@huawei.com]=20
> Sent: Friday, August 15, 2014 11:22 AM
> To: dev@spark.apache.org
> Subject: [sql]enable spark sql cli support spark sql
>=20
> hi all,
>   now spark sql cli only support spark hql, i think we can enable this =
cli to support spark sql, do you think it's necessary?
>=20
> --=20
>=20
> Best Regards
> Fei Wang
>=20
> =
--------------------------------------------------------------------------=
------
>=20
>=20
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For =
additional commands, e-mail: dev-help@spark.apache.org
>=20
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 08:17:04 2014
Return-Path: <dev-return-8899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 15FA1114E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 08:17:04 +0000 (UTC)
Received: (qmail 47105 invoked by uid 500); 15 Aug 2014 08:17:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47040 invoked by uid 500); 15 Aug 2014 08:17:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47029 invoked by uid 99); 15 Aug 2014 08:17:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 08:17:03 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hao.cheng@intel.com designates 192.55.52.88 as permitted sender)
Received: from [192.55.52.88] (HELO mga01.intel.com) (192.55.52.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 08:16:58 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga101.fm.intel.com with ESMTP; 15 Aug 2014 01:16:38 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.01,868,1400050800"; 
   d="scan'208";a="585229134"
Received: from fmsmsx107.amr.corp.intel.com ([10.18.124.205])
  by fmsmga002.fm.intel.com with ESMTP; 15 Aug 2014 01:16:38 -0700
Received: from fmsmsx154.amr.corp.intel.com (10.18.116.70) by
 fmsmsx107.amr.corp.intel.com (10.18.124.205) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Fri, 15 Aug 2014 01:16:37 -0700
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 FMSMSX154.amr.corp.intel.com (10.18.116.70) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Fri, 15 Aug 2014 01:16:37 -0700
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.246]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.17]) with mapi id 14.03.0195.001;
 Fri, 15 Aug 2014 16:16:35 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
CC: scwf <wangfei1@huawei.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: [sql]enable spark sql cli support spark sql
Thread-Topic: [sql]enable spark sql cli support spark sql
Thread-Index: AQHPuDhC5mCp3XVuk0abB5vlkuMdXpvRHcYA//+G/4CAAKvuMA==
Date: Fri, 15 Aug 2014 08:16:35 +0000
Message-ID: <80833ADD533E324CA05C160E41B6366102726C17@shsmsx102.ccr.corp.intel.com>
References: <53ED7CD7.9000307@huawei.com>
 <80833ADD533E324CA05C160E41B6366102726A26@shsmsx102.ccr.corp.intel.com>
 <C036B265-BD46-4AB8-961D-A7D26EE4A9F1@gmail.com>
In-Reply-To: <C036B265-BD46-4AB8-961D-A7D26EE4A9F1@gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

If so, probably we need to add the SQL dialects switching support for Spark=
SQLCLI, as Fei suggested. What do you think the priority for this?

-----Original Message-----
From: Cheng Lian [mailto:lian.cs.zju@gmail.com]=20
Sent: Friday, August 15, 2014 1:57 PM
To: Cheng, Hao
Cc: scwf; dev@spark.apache.org
Subject: Re: [sql]enable spark sql cli support spark sql

In the long run, as Michael suggested in his Spark Summit 14 talk, we'd lik=
e to implement SQL-92, maybe with the help of Optiq.

On Aug 15, 2014, at 1:13 PM, Cheng, Hao <hao.cheng@intel.com> wrote:

> Actually the SQL Parser (another SQL dialect in SparkSQL) is quite weak, =
and only support some basic queries, not sure what's the plan for its enhan=
cement.
>=20
> -----Original Message-----
> From: scwf [mailto:wangfei1@huawei.com]
> Sent: Friday, August 15, 2014 11:22 AM
> To: dev@spark.apache.org
> Subject: [sql]enable spark sql cli support spark sql
>=20
> hi all,
>   now spark sql cli only support spark hql, i think we can enable this cl=
i to support spark sql, do you think it's necessary?
>=20
> --
>=20
> Best Regards
> Fei Wang
>=20
> ----------------------------------------------------------------------
> ----------
>=20
>=20
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For=20
> additional commands, e-mail: dev-help@spark.apache.org
>=20
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For=20
> additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 09:20:01 2014
Return-Path: <dev-return-8900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9282911611
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 09:20:01 +0000 (UTC)
Received: (qmail 39052 invoked by uid 500); 15 Aug 2014 09:20:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38988 invoked by uid 500); 15 Aug 2014 09:20:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38972 invoked by uid 99); 15 Aug 2014 09:20:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 09:20:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 09:19:33 +0000
Received: by mail-pa0-f47.google.com with SMTP id kx10so3195266pab.20
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 02:19:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=dC2kfFyCgRtoqZ/f9T83fAwzXAzqAr2TIpD+qr+QxRA=;
        b=UIEuGTYXgaYRmBulcCvIhcMNb3+R9pwH4j/rRnkcgbXlbyA4sAR7oZWlUf+M9nFrXt
         dERuCshMdsw87GjOARzTXqza5WCdeLSBuWq64MCNMMHU7V9xg6+Pxbl0S5mjN4qP3gqP
         3T84v2jlETr0T6YIHWo+aqUIsVfAzXOzEicAgv4LwfNXFCeQ+2Rv9qW7R6EhisOVw37Y
         WehZUMUsZYJPZAsL6Yyouzv+NBd3Rc+huebr1dO9HmO6cxPbFBcI1f/JjrleGqdFrgmf
         t8qC7BntWB3EPpFGLQ0Mw6uu4lxYC9wwBU3a6SVBWPmPq/33VMwAoSFMojVO1sfuws7e
         PHKg==
X-Received: by 10.68.250.131 with SMTP id zc3mr10748355pbc.1.1408094370702;
        Fri, 15 Aug 2014 02:19:30 -0700 (PDT)
Received: from [127.0.0.1] (fmdmzpr01-ext.fm.intel.com. [192.55.54.36])
        by mx.google.com with ESMTPSA id qm11sm11245101pdb.85.2014.08.15.02.19.28
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 15 Aug 2014 02:19:30 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [sql]enable spark sql cli support spark sql
From: Cheng Lian <lian.cs.zju@gmail.com>
In-Reply-To: <80833ADD533E324CA05C160E41B6366102726C17@shsmsx102.ccr.corp.intel.com>
Date: Fri, 15 Aug 2014 17:20:53 +0800
Cc: scwf <wangfei1@huawei.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <5554B3C2-E37C-4930-9F24-0853E468B43C@gmail.com>
References: <53ED7CD7.9000307@huawei.com> <80833ADD533E324CA05C160E41B6366102726A26@shsmsx102.ccr.corp.intel.com> <C036B265-BD46-4AB8-961D-A7D26EE4A9F1@gmail.com> <80833ADD533E324CA05C160E41B6366102726C17@shsmsx102.ccr.corp.intel.com>
To: "Cheng, Hao" <hao.cheng@intel.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

It would be good to have, but I=92m afraid it=92s not super useful for =
users as the current SqlParser is very limited, especially it doesn=92t =
support either DDL statements or any DML statements other then INSERT.

On Aug 15, 2014, at 4:16 PM, Cheng, Hao <hao.cheng@intel.com> wrote:

> If so, probably we need to add the SQL dialects switching support for =
SparkSQLCLI, as Fei suggested. What do you think the priority for this?
>=20
> -----Original Message-----
> From: Cheng Lian [mailto:lian.cs.zju@gmail.com]=20
> Sent: Friday, August 15, 2014 1:57 PM
> To: Cheng, Hao
> Cc: scwf; dev@spark.apache.org
> Subject: Re: [sql]enable spark sql cli support spark sql
>=20
> In the long run, as Michael suggested in his Spark Summit 14 talk, =
we'd like to implement SQL-92, maybe with the help of Optiq.
>=20
> On Aug 15, 2014, at 1:13 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
>=20
>> Actually the SQL Parser (another SQL dialect in SparkSQL) is quite =
weak, and only support some basic queries, not sure what's the plan for =
its enhancement.
>>=20
>> -----Original Message-----
>> From: scwf [mailto:wangfei1@huawei.com]
>> Sent: Friday, August 15, 2014 11:22 AM
>> To: dev@spark.apache.org
>> Subject: [sql]enable spark sql cli support spark sql
>>=20
>> hi all,
>>  now spark sql cli only support spark hql, i think we can enable this =
cli to support spark sql, do you think it's necessary?
>>=20
>> --
>>=20
>> Best Regards
>> Fei Wang
>>=20
>> =
----------------------------------------------------------------------
>> ----------
>>=20
>>=20
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For=20
>> additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For=20
>> additional commands, e-mail: dev-help@spark.apache.org
>>=20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 14:38:09 2014
Return-Path: <dev-return-8901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D603E11DF5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 14:38:09 +0000 (UTC)
Received: (qmail 30702 invoked by uid 500); 15 Aug 2014 14:38:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30639 invoked by uid 500); 15 Aug 2014 14:38:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30627 invoked by uid 99); 15 Aug 2014 14:38:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 14:38:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 14:37:43 +0000
Received: by mail-we0-f171.google.com with SMTP id p10so2465465wes.16
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 07:37:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=himzRZ31xoLixqxomW7En/MqTjsn4921RksxllwVUYc=;
        b=OHAYKISHmXgsFtOVUctZYi1K41303msBdyRAXowl73Ccd7yVJFXhVD14gTyqbThbfO
         2rq5Bm7e02dwOzPfbC2bd2PSO1B3WBjBkkBvAGkqHck9TqkG/ESDfh90pZfDOTCzokw/
         3tMfTIlL+NR2GDzYdwVAuXXD5ld1s8ZMObkrhSPRva/wjPVvbt9GqktZhLiJJRsJ1yex
         gEEj+NOm2nH2Q6S8o3aOfBJNnfTXqRAId4+XCLRm3PBPlRKahOv48Rvn4ZEz9RYURD/T
         7I8rFTLils5V8XrRzaClKEmWLTRyC6C0GTMx+JaOumDl3uECodRlJi8h2QKfb1l5aNZJ
         VcAw==
X-Received: by 10.180.12.239 with SMTP id b15mr53809663wic.75.1408113462659;
 Fri, 15 Aug 2014 07:37:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 15 Aug 2014 07:37:02 -0700 (PDT)
In-Reply-To: <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
 <CAOhmDzfUoAAbtzC7MKRbpTzyYo9ZdwkQ_bwMGbY9ATEJpXYjTg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 15 Aug 2014 10:37:02 -0400
Message-ID: <CAOhmDzc_M_JxqVkpq6mPui90Y78CYTu7S8v8fVupW13-KVnntw@mail.gmail.com>
Subject: Re: -1s on pull requests?
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c35284bed1710500abf699
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c35284bed1710500abf699
Content-Type: text/plain; charset=UTF-8

On Sun, Aug 3, 2014 at 4:35 PM, Nicholas Chammas <nicholas.chammas@gmail.com
> wrote:

> Include the commit hash in the "tests have started/completed" messages, so
> that it's clear what code exactly is/has been tested for each test cycle.


This is now captured in this JIRA issue
<https://issues.apache.org/jira/browse/SPARK-2912> and completed in this PR
<https://github.com/apache/spark/pull/1816> which has been merged in to
master.

Example of old style: tests starting
<https://github.com/apache/spark/pull/1819#issuecomment-51416510> / tests
finished <https://github.com/apache/spark/pull/1819#issuecomment-51417477>
(with
new classes)

Example of new style: tests starting
<https://github.com/apache/spark/pull/1816#issuecomment-51855254> / tests
finished <https://github.com/apache/spark/pull/1816#issuecomment-51855255>
(with
new classes)

Nick

--001a11c35284bed1710500abf699--

From dev-return-8902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 15:19:01 2014
Return-Path: <dev-return-8902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 29D6D11F5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 15:19:01 +0000 (UTC)
Received: (qmail 29903 invoked by uid 500); 15 Aug 2014 15:18:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29844 invoked by uid 500); 15 Aug 2014 15:18:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29833 invoked by uid 99); 15 Aug 2014 15:18:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:18:59 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:18:55 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XIJH0-0004YM-Lt
	for dev@spark.incubator.apache.org; Fri, 15 Aug 2014 08:18:34 -0700
Date: Fri, 15 Aug 2014 08:18:34 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408115914518-7865.post@n3.nabble.com>
Subject: spark.akka.frameSize stalls job in 1.1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,
I'm not sure if I should file a JIRA or if I'm missing something obvious
since the test code I'm trying is so simple. I've isolated the problem I'm
seeing to a memory issue but I don't know what parameter I need to tweak, it
does seem related to spark.akka.frameSize. If I sample my RDD with 35% of
the data, everything runs to completion, with more than 35%, it fails. In
standalone mode, I can run on the full RDD without any problems. 

// works 
val samples = sc.textFile("s3n://geonames").sample(false,0.35) // 64MB,
2849439 Lines 

// fails 
val samples = sc.textFile("s3n://geonames").sample(false,0.4) // 64MB,
2849439 Lines 

Any ideas? 

1) RDD size is causing the problem. The code below as is fails but if I swap
smallSample for samples, the code runs end to end on both cluster and
standalone. 
2) The error I get is: 
rg.apache.spark.SparkException: Job aborted due to stage failure: Task 3.0:1
failed 4 times, most recent failure: TID 12 on host
ip-10-251-14-74.us-west-2.compute.internal failed for unknown reason 
Driver stacktrace: 
        at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028) 
3) Using the 1.1.0 branch the driver freezes instead of aborting with the
previous error in #2.
4) In 1.1.0, changing spark.akka.frameSize also has the effect of no
progress in the driver.

Code: 
val smallSample = sc.parallelize(Array("foo word", "bar word", "baz word")) 

val samples = sc.textFile("s3n://geonames") // 64MB, 2849439 Lines of short
strings 

val counts = new collection.mutable.HashMap[String, Int].withDefaultValue(0) 

samples.toArray.foreach(counts(_) += 1) 

val result = samples.map( 
  l => (l, counts.get(l)) 
) 

result.count 

Settings (with or without Kryo doesn't matter): 
export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g" 
export SPARK_MEM=10g 
spark.akka.frameSize 40 
#spark.serializer org.apache.spark.serializer.KryoSerializer 
#spark.kryoserializer.buffer.mb 1000 
spark.executor.memory 58315m 
spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/ 
spark.executor.extraClassPath /root/ephemeral-hdfs/conf



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 15:36:21 2014
Return-Path: <dev-return-8903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0068211FC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 15:36:21 +0000 (UTC)
Received: (qmail 83300 invoked by uid 500); 15 Aug 2014 15:36:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83230 invoked by uid 500); 15 Aug 2014 15:36:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83218 invoked by uid 99); 15 Aug 2014 15:36:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:36:19 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:36:15 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so2244149igd.5
        for <dev@spark.incubator.apache.org>; Fri, 15 Aug 2014 08:35:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ny4fBiic/c4V75XZXPwt1kGu4SUEsoaWLpFwqxF1tYk=;
        b=ftrgjWnh2tQe5F8fBPtcvFEzyBWyspcM7YwS/EhzV1XLqiOzdeF95NMSfi7vUowLOi
         b48lDDKBGPNB6tuGDnJ4hRjt6EYsJ00YseOOjR351zd1sWGcx0CiJQAYdn74HqwNyUmB
         7avqGxA+z372Lt5avd1WnoJYzYBba5reEEQZ7oWG+fpgP0xRWt6EsbL2RimDPTIGTjc7
         BgOIlBEX7WK+9gcBKD4/DjGRvV6vobY0WCjgNS/a8xplLVdgRLAxryzwXTX+MwFOtn+S
         o5xkHRfXP24uDtzLM2Z6I0MpZ/m61hpis9bUcwc5nDNgAUff/qBcw7Rt2ikbFdtrZ6hy
         6PcA==
MIME-Version: 1.0
X-Received: by 10.50.88.37 with SMTP id bd5mr6761519igb.1.1408116954676; Fri,
 15 Aug 2014 08:35:54 -0700 (PDT)
Received: by 10.107.152.196 with HTTP; Fri, 15 Aug 2014 08:35:54 -0700 (PDT)
In-Reply-To: <1408115914518-7865.post@n3.nabble.com>
References: <1408115914518-7865.post@n3.nabble.com>
Date: Fri, 15 Aug 2014 08:35:54 -0700
Message-ID: <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Xiangrui Meng <mengxr@gmail.com>
To: jerryye <jerryye@gmail.com>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Did you set driver memory? You can confirm it in the Executors tab of
the WebUI. Btw, the code may only work in local mode. In a cluster
mode, counts will be serialized to remote workers and the result is
not fetched by the driver after foreach. You can use RDD.countByValue
instead. -Xiangrui

On Fri, Aug 15, 2014 at 8:18 AM, jerryye <jerryye@gmail.com> wrote:
> Hi All,
> I'm not sure if I should file a JIRA or if I'm missing something obvious
> since the test code I'm trying is so simple. I've isolated the problem I'm
> seeing to a memory issue but I don't know what parameter I need to tweak, it
> does seem related to spark.akka.frameSize. If I sample my RDD with 35% of
> the data, everything runs to completion, with more than 35%, it fails. In
> standalone mode, I can run on the full RDD without any problems.
>
> // works
> val samples = sc.textFile("s3n://geonames").sample(false,0.35) // 64MB,
> 2849439 Lines
>
> // fails
> val samples = sc.textFile("s3n://geonames").sample(false,0.4) // 64MB,
> 2849439 Lines
>
> Any ideas?
>
> 1) RDD size is causing the problem. The code below as is fails but if I swap
> smallSample for samples, the code runs end to end on both cluster and
> standalone.
> 2) The error I get is:
> rg.apache.spark.SparkException: Job aborted due to stage failure: Task 3.0:1
> failed 4 times, most recent failure: TID 12 on host
> ip-10-251-14-74.us-west-2.compute.internal failed for unknown reason
> Driver stacktrace:
>         at
> org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
> 3) Using the 1.1.0 branch the driver freezes instead of aborting with the
> previous error in #2.
> 4) In 1.1.0, changing spark.akka.frameSize also has the effect of no
> progress in the driver.
>
> Code:
> val smallSample = sc.parallelize(Array("foo word", "bar word", "baz word"))
>
> val samples = sc.textFile("s3n://geonames") // 64MB, 2849439 Lines of short
> strings
>
> val counts = new collection.mutable.HashMap[String, Int].withDefaultValue(0)
>
> samples.toArray.foreach(counts(_) += 1)
>
> val result = samples.map(
>   l => (l, counts.get(l))
> )
>
> result.count
>
> Settings (with or without Kryo doesn't matter):
> export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g"
> export SPARK_MEM=10g
> spark.akka.frameSize 40
> #spark.serializer org.apache.spark.serializer.KryoSerializer
> #spark.kryoserializer.buffer.mb 1000
> spark.executor.memory 58315m
> spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
> spark.executor.extraClassPath /root/ephemeral-hdfs/conf
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 15:58:30 2014
Return-Path: <dev-return-8904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 107FE110A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 15:58:30 +0000 (UTC)
Received: (qmail 48610 invoked by uid 500); 15 Aug 2014 15:58:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48549 invoked by uid 500); 15 Aug 2014 15:58:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48532 invoked by uid 99); 15 Aug 2014 15:58:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:58:28 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 15:58:02 +0000
Received: by mail-qg0-f43.google.com with SMTP id a108so2388209qge.30
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 08:58:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=RG0r0oyVXLQjjA2NVuDqMz+JNytEoFG8/I2cgGiUv2I=;
        b=YKCFQqb5hBASzmIiNbGTLLHF0lG8lbreBK1+4340DGRdkAIy4RmxRIIUCM6Tvu5mXm
         3tnfOHB8NAHJ8XY/HQWe/U+XenK+V2hMbO6b8aKlPmDOBFnmuqj0HPpZeMZhQj21wzJv
         MNWX6ViC82lposcPqFXwju1vZvDAZUDPpYEkijVt9ozD+WwCG9S/ARey45fe8gCu/R+R
         rPQZUedFKhoQnvA49xNlL4bkIOV+YkC5uuTyobIf6Jw4drk5MSlf0jXYIa2BEUl3yccB
         nVsA2/kmZxGH8R7tRO+lIPcUPwFaFlcFpEeMKWlOQsaYUVGUbK0rleo3ahD5ecClnmV/
         PUbA==
MIME-Version: 1.0
X-Received: by 10.224.80.10 with SMTP id r10mr29634214qak.24.1408118280873;
 Fri, 15 Aug 2014 08:58:00 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Fri, 15 Aug 2014 08:58:00 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Fri, 15 Aug 2014 08:58:00 -0700 (PDT)
In-Reply-To: <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
	<CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
	<CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
	<CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com>
	<CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com>
	<CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com>
Date: Fri, 15 Aug 2014 08:58:00 -0700
Message-ID: <CA+B-+fz6NbUoYFtNgF-r1Sye25URqnnrA1z1h+XFqX+08tQ+Tw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Debasish Das <debasish.das83@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Graham Dennis <graham.dennis@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2dba8eeefce0500ad152b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2dba8eeefce0500ad152b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I am still a bit confused that why this issue did not show up in 0.9...at
that time there was no spark-submit and the context was constructed with
low level calls...

Kryo register for ALS was always in my application code..

Was this bug introduced in 1.0 or it was always there ?
 On Aug 14, 2014 5:48 PM, "Reynold Xin" <rxin@databricks.com> wrote:

> Here: https://github.com/apache/spark/pull/1948
>
>
>
> On Thu, Aug 14, 2014 at 5:45 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Is there a fix that I can test ? I have the flows setup for both
>> standalone and YARN runs...
>>
>> Thanks.
>> Deb
>>
>>
>>
>> On Thu, Aug 14, 2014 at 10:59 AM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>
>>> Yes, I understand it might not work for custom serializer, but that is =
a
>>> much less common path.
>>>
>>> Basically I want a quick fix for 1.1 release (which is coming up soon).
>>> I would not be comfortable making big changes to class path late into t=
he
>>> release cycle. We can do that for 1.2.
>>>
>>>
>>>
>>>
>>>
>>> On Thu, Aug 14, 2014 at 2:35 AM, Graham Dennis <graham.dennis@gmail.com=
>
>>> wrote:
>>>
>>>> That should work, but would you also make these changes to the
>>>> JavaSerializer?  The API of these is the same so that you can select o=
ne or
>>>> the other (or in theory a custom serializer)?  This also wouldn't addr=
ess
>>>> the problem of shipping custom *serializers* (not kryo registrators) i=
n
>>>> user jars.
>>>>
>>>> On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:
>>>>
>>>>> Graham,
>>>>>
>>>>> SparkEnv only creates a KryoSerializer, but as I understand that
>>>>> serializer doesn't actually initializes the registrator since that is=
 only
>>>>> called when newKryo() is called when KryoSerializerInstance is initia=
lized.
>>>>>
>>>>> Basically I'm thinking a quick fix for 1.2:
>>>>>
>>>>> 1. Add a classLoader field to KryoSerializer; initialize new
>>>>> KryoSerializerInstance with that class loader
>>>>>
>>>>>  2. Set that classLoader to the executor's class loader when Executor
>>>>> is initialized.
>>>>>
>>>>> Then all deser calls should be using the executor's class loader.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <
>>>>> graham.dennis@gmail.com> wrote:
>>>>>
>>>>>> Hi Reynold,
>>>>>>
>>>>>> That would solve this specific issue, but you'd need to be careful
>>>>>> that you never created a serialiser instance before the first task i=
s
>>>>>> received.  Currently in Executor.TaskRunner.run a closure serialiser
>>>>>> instance is created before any application jars are downloaded, but =
that
>>>>>> could be moved.  To me, this seems a little fragile.
>>>>>>
>>>>>> However there is a related issue where you can't ship a custom
>>>>>> serialiser in an application jar because the serialiser is instantia=
ted
>>>>>> when the SparkEnv object is created, which is before any tasks are r=
eceived
>>>>>> by the executor.  The above approach wouldn't help with this problem=
.
>>>>>>  Additionally, the YARN scheduler currently uses this approach of ad=
ding
>>>>>> the application jar to the Executor classpath, so it would make thin=
gs a
>>>>>> bit more uniform.
>>>>>>
>>>>>> Cheers,
>>>>>> Graham
>>>>>>
>>>>>>
>>>>>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>>>>>
>>>>>>> Graham,
>>>>>>>
>>>>>>> Thanks for working on this. This is an important bug to fix.
>>>>>>>
>>>>>>>  I don't have the whole context and obviously I haven't spent
>>>>>>> nearly as much time on this as you have, but I'm wondering what if =
we
>>>>>>> always pass the executor's ClassLoader to the Kryo serializer? Will=
 that
>>>>>>> solve this problem?
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <
>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi Deb,
>>>>>>>>
>>>>>>>> The only alternative serialiser is the JavaSerialiser (the
>>>>>>>> default).  Theoretically Spark supports custom serialisers, but du=
e to a
>>>>>>>> related issue, custom serialisers currently can't live in applicat=
ion jars
>>>>>>>> and must be available to all executors at launch.  My PR fixes thi=
s issue
>>>>>>>> as well, allowing custom serialisers to be shipped in application =
jars.
>>>>>>>>
>>>>>>>> Graham
>>>>>>>>
>>>>>>>>
>>>>>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> Sorry I just saw Graham's email after sending my previous email
>>>>>>>>> about this bug...
>>>>>>>>>
>>>>>>>>> I have been seeing this same issue on our ALS runs last week but =
I
>>>>>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core=
 1.0...
>>>>>>>>>
>>>>>>>>> What's the status of this PR ? Will this fix be back-ported to
>>>>>>>>> 1.0.1 as we are running 1.0.1 stable standalone cluster ?
>>>>>>>>>
>>>>>>>>> Till the PR merges does it make sense to not use Kryo ? What are
>>>>>>>>> the other recommended efficient serializers ?
>>>>>>>>>
>>>>>>>>> Thanks.
>>>>>>>>> Deb
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> I now have a complete pull request for this issue that I'd like
>>>>>>>>>> to get
>>>>>>>>>> reviewed and committed.  The PR is available here:
>>>>>>>>>> https://github.com/apache/spark/pull/1890 and includes a
>>>>>>>>>> testcase for the
>>>>>>>>>> issue I described.  I've also submitted a related PR (
>>>>>>>>>> https://github.com/apache/spark/pull/1827) that causes
>>>>>>>>>> exceptions raised
>>>>>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>>>>>> swallowed.
>>>>>>>>>>
>>>>>>>>>> Thanks,
>>>>>>>>>> Graham
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>> > I've submitted a work-in-progress pull request for this issue
>>>>>>>>>> that I'd
>>>>>>>>>> > like feedback on.  See
>>>>>>>>>> https://github.com/apache/spark/pull/1890 . I've
>>>>>>>>>> > also submitted a pull request for the related issue that the
>>>>>>>>>> exceptions hit
>>>>>>>>>> > when trying to use a custom kryo registrator are being
>>>>>>>>>> swallowed:
>>>>>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>>>>>> >
>>>>>>>>>> > The approach in my pull request is to get the Worker processes
>>>>>>>>>> to download
>>>>>>>>>> > the application jars and add them to the Executor class path a=
t
>>>>>>>>>> launch
>>>>>>>>>> > time. There are a couple of things that still need to be done
>>>>>>>>>> before this
>>>>>>>>>> > can be merged:
>>>>>>>>>> > 1. At the moment, the first time a task runs in the executor,
>>>>>>>>>> the
>>>>>>>>>> > application jars are downloaded again.  My solution here would
>>>>>>>>>> be to make
>>>>>>>>>> > the executor not download any jars that already exist.
>>>>>>>>>>  Previously, the
>>>>>>>>>> > driver & executor kept track of the timestamp of jar files and
>>>>>>>>>> would
>>>>>>>>>> > redownload 'updated' jars, however this never made sense as th=
e
>>>>>>>>>> previous
>>>>>>>>>> > version of the updated jar may have already been loaded into
>>>>>>>>>> the executor,
>>>>>>>>>> > so the updated jar may have no effect.  As my current pull
>>>>>>>>>> request removes
>>>>>>>>>> > the timestamp for jars, just checking whether the jar exists
>>>>>>>>>> will allow us
>>>>>>>>>> > to avoid downloading the jars again.
>>>>>>>>>> > 2. Tests. :-)
>>>>>>>>>> >
>>>>>>>>>> > A side-benefit of my pull request is that you will be able to
>>>>>>>>>> use custom
>>>>>>>>>> > serialisers that are distributed in a user jar.  Currently, th=
e
>>>>>>>>>> serialiser
>>>>>>>>>> > instance is created in the Executor process before the first
>>>>>>>>>> task is
>>>>>>>>>> > received and therefore before any user jars are downloaded.  A=
s
>>>>>>>>>> this PR
>>>>>>>>>> > adds user jars to the Executor process at launch time, this
>>>>>>>>>> won't be an
>>>>>>>>>> > issue.
>>>>>>>>>> >
>>>>>>>>>> >
>>>>>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com=
>
>>>>>>>>>> wrote:
>>>>>>>>>> >
>>>>>>>>>> >> See my comment on
>>>>>>>>>> https://issues.apache.org/jira/browse/SPARK-2878 for
>>>>>>>>>> >> the full stacktrace, but it's in the
>>>>>>>>>> BlockManager/BlockManagerWorker where
>>>>>>>>>> >> it's trying to fulfil a "getBlock" request for another node.
>>>>>>>>>>  The objects
>>>>>>>>>> >> that would be in the block haven't yet been serialised, and
>>>>>>>>>> that then
>>>>>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>>>>>> >> MemoryStore.scala:102.
>>>>>>>>>> >>
>>>>>>>>>> >>
>>>>>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com>
>>>>>>>>>> wrote:
>>>>>>>>>> >>
>>>>>>>>>> >>> I don't think it was a conscious design decision to not
>>>>>>>>>> include the
>>>>>>>>>> >>> application classes in the connection manager serializer. We
>>>>>>>>>> should fix
>>>>>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>>>>>> >>>
>>>>>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>>>>>> complexity to
>>>>>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>>>>>> blocking/non-blocking
>>>>>>>>>> >>> logic within task threads) that can be error prone, so I
>>>>>>>>>> think it is best
>>>>>>>>>> >>> to stay away from that right now.
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>>>>>> graham.dennis@gmail.com>
>>>>>>>>>> >>> wrote:
>>>>>>>>>> >>>
>>>>>>>>>> >>>> Hi Spark devs,
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which
>>>>>>>>>> occurs when
>>>>>>>>>> >>>> using
>>>>>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to
>>>>>>>>>> register custom
>>>>>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>>>>>> >>>> non-deterministically
>>>>>>>>>> >>>> causes Kryo to have different ID number =3D> class name map=
s
>>>>>>>>>> on different
>>>>>>>>>> >>>> nodes, which then causes weird exceptions
>>>>>>>>>> (ClassCastException,
>>>>>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>>>>>> >>>> deserialisation
>>>>>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the=
 issue
>>>>>>>>>> here:
>>>>>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to=
 try and
>>>>>>>>>> address
>>>>>>>>>> >>>> this,
>>>>>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve t=
his and
>>>>>>>>>> I=E2=80=99d like to
>>>>>>>>>> >>>> get
>>>>>>>>>> >>>> feedback / ideas on how to address this.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>>>>>> processes
>>>>>>>>>> >>>> during
>>>>>>>>>> >>>> operation.  My custom Kryo registrator is in the applicatio=
n
>>>>>>>>>> jar, and
>>>>>>>>>> >>>> it is
>>>>>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by th=
e
>>>>>>>>>> fact that
>>>>>>>>>> >>>> most
>>>>>>>>>> >>>> of the time the custom kryo registrator is successfully run=
.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deseria=
lisation
>>>>>>>>>> is happening
>>>>>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=
=E2=80=9D thread,
>>>>>>>>>> which has
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> thread's class loader set to contain the application jar.
>>>>>>>>>>  This happens
>>>>>>>>>> >>>> in
>>>>>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and
>>>>>>>>>> from what I can
>>>>>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>>>>>> application jar
>>>>>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>>>>>> >>>> ConnectionManager threads sometimes need to
>>>>>>>>>> serialise/deserialise
>>>>>>>>>> >>>> objects
>>>>>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the obj=
ects haven=E2=80=99t
>>>>>>>>>> previously been
>>>>>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t=
 have the
>>>>>>>>>> application
>>>>>>>>>> >>>> jar available from their class loader, when it tries to loo=
k
>>>>>>>>>> up the
>>>>>>>>>> >>>> custom
>>>>>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>>>>>> exception, which
>>>>>>>>>> >>>> results in a different ID number =E2=80=94> class mapping f=
or this
>>>>>>>>>> kryo
>>>>>>>>>> >>>> instance,
>>>>>>>>>> >>>> and this then causes deserialisation errors later on a
>>>>>>>>>> different node.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that
>>>>>>>>>> Spark
>>>>>>>>>> >>>> probably
>>>>>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for c=
ustom Kryo
>>>>>>>>>> >>>> registrators.
>>>>>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>>>>>> >>>> deterministically
>>>>>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at seria=
lisation /
>>>>>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t b=
e found
>>>>>>>>>> (as in this
>>>>>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>>>>>  Either way,
>>>>>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exc=
eption
>>>>>>>>>> earlier, than
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> weirder errors later on.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I have some ideas on potential solutions to this issue, but
>>>>>>>>>> I=E2=80=99m keen for
>>>>>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> 1. The simplest approach to fixing this would be to just
>>>>>>>>>> make the
>>>>>>>>>> >>>> application jar available to the connection manager threads=
,
>>>>>>>>>> but I=E2=80=99m
>>>>>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the appl=
ication
>>>>>>>>>> jar to just
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know i=
f there
>>>>>>>>>> are any other
>>>>>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>>>>>> >>>> deserialisation.
>>>>>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change th=
e
>>>>>>>>>> thread=E2=80=99s
>>>>>>>>>> >>>> class
>>>>>>>>>> >>>> loader to include the application jar, then restore the
>>>>>>>>>> class loader
>>>>>>>>>> >>>> after
>>>>>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if=
 this
>>>>>>>>>> would have any
>>>>>>>>>> >>>> other side-effects.
>>>>>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunne=
r
>>>>>>>>>> threads,
>>>>>>>>>> >>>> rather than delaying serialisation until later, when it can
>>>>>>>>>> be done
>>>>>>>>>> >>>> only if
>>>>>>>>>> >>>> needed.  This approach would probably have negative
>>>>>>>>>> performance
>>>>>>>>>> >>>> consequences.
>>>>>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisatio=
n
>>>>>>>>>> /
>>>>>>>>>> >>>> deserialisation that has the application jar on the class
>>>>>>>>>> path.
>>>>>>>>>> >>>>  Serialisation / deserialisation would be the only thing
>>>>>>>>>> these threads
>>>>>>>>>> >>>> do,
>>>>>>>>>> >>>> and this would minimise conflicts / interactions between th=
e
>>>>>>>>>> application
>>>>>>>>>> >>>> jar and other jars.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> #4 sounds like the best approach to me, but I think would
>>>>>>>>>> require
>>>>>>>>>> >>>> considerable knowledge of Spark internals, which is beyond
>>>>>>>>>> me at
>>>>>>>>>> >>>> present.
>>>>>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> Cheers,
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> Graham
>>>>>>>>>> >>>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>
>>>>>>>>>> >
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c2dba8eeefce0500ad152b--

From dev-return-8905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 16:04:55 2014
Return-Path: <dev-return-8905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CDD5110CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 16:04:55 +0000 (UTC)
Received: (qmail 63037 invoked by uid 500); 15 Aug 2014 16:04:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62969 invoked by uid 500); 15 Aug 2014 16:04:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62954 invoked by uid 99); 15 Aug 2014 16:04:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 16:04:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 16:04:27 +0000
Received: by mail-oa0-f49.google.com with SMTP id eb12so2166729oac.22
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 09:04:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=GxIDOIXsAO+ckZ8ptt1z+sLpRKVJ6wQ8GdEVmfD+a+8=;
        b=BVjX426Nqmydqc6V36VnEnTLMXZPBFY3CR4WUE9TzUOfd2Ze/w9wikkFD7izy99ID9
         i8b+iFUsM3CzSG7css7du8zLu6DmiSrnfNxXmQxsOYtPJia9jaRkT8zjuqTb2wqzRirr
         WhHb6dU98FidLAlVbNK6suUstMszXALGG1sRSunsfh0QUjzyFWXBOlWju71hd9W4fcY2
         vEDadhMQsZZqC9a+K0gfsQm8y+XY2a4nWDT1ML1weKNw7V8krXWhqPLan8dyoy0fMskd
         XsrnrXyPVjeTB04rIy3hm2J2rjW6Ook0qzrTZxBwWYddcgFdnUWiMAe1XcxdzzXa3x7b
         3qzA==
MIME-Version: 1.0
X-Received: by 10.60.220.169 with SMTP id px9mr20643549oec.67.1408118666309;
 Fri, 15 Aug 2014 09:04:26 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 15 Aug 2014 09:04:26 -0700 (PDT)
Date: Fri, 15 Aug 2014 09:04:26 -0700
Message-ID: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
Subject: Tests failing
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133e0e4e8abd00500ad2c2e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133e0e4e8abd00500ad2c2e
Content-Type: text/plain; charset=ISO-8859-1

Hi All,

I noticed that all PR tests run overnight had failed due to timeouts. The
patch that updates the netty shuffle I believe somehow inflated to the
build time significantly. That patch had been tested, but one change was
made before it was merged that was not tested.

I've reverted the patch for now to see if it brings the build times back
down.

- Patrick

--001a1133e0e4e8abd00500ad2c2e--

From dev-return-8906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 18:08:46 2014
Return-Path: <dev-return-8906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87F76115CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 18:08:46 +0000 (UTC)
Received: (qmail 95670 invoked by uid 500); 15 Aug 2014 18:08:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95607 invoked by uid 500); 15 Aug 2014 18:08:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95578 invoked by uid 99); 15 Aug 2014 18:08:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:08:45 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nellaivijay@live.com designates 65.55.90.141 as permitted sender)
Received: from [65.55.90.141] (HELO SNT004-OMC3S2.hotmail.com) (65.55.90.141)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:08:20 +0000
Received: from SNT406-EAS106 ([65.55.90.137]) by SNT004-OMC3S2.hotmail.com with Microsoft SMTPSVC(7.5.7601.22701);
	 Fri, 15 Aug 2014 11:08:19 -0700
X-TMN: [ULpsZ3xakLcxkUtecpaNGA+iwuKlNzjk]
X-Originating-Email: [nellaivijay@live.com]
Message-ID: <SNT406-EAS10697CA34307BA0DB3B520ECDE90@phx.gbl>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
Subject: Re: Need info on Spark's Communication/Networking layer...
References: <1407973009489-7836.post@n3.nabble.com> <CADnDY-UYPd3YUHkpgJhjEL1x3fJM=t5K2-7idDVqVkcHaL7jVw@mail.gmail.com>
From: Vijayakumar Ramdoss <nellaivijay@live.com>
MIME-Version: 1.0 (1.0)
In-Reply-To: <CADnDY-UYPd3YUHkpgJhjEL1x3fJM=t5K2-7idDVqVkcHaL7jVw@mail.gmail.com>
Date: Fri, 15 Aug 2014 14:08:17 -0400
To: Rajiv Abraham <rajiv.abraham@gmail.com>
CC: aniketadnaik <aniket.adnaik@gmail.com>, "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>
X-OriginalArrivalTime: 15 Aug 2014 18:08:19.0061 (UTC) FILETIME=[E4943A50:01CFB8B3]
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Rajiv

Sent from my iPhone

> On Aug 13, 2014, at 7:58 PM, "Rajiv Abraham" <rajiv.abraham@gmail.com> wro=
te:
>=20
> Hi Aniket,
> Perhaps this video will help:
> https://www.youtube.com/watch?v=3DHG2Yd-3r4-M&list=3DPLTPXxbhUt-YWGNTaDj6H=
SjnHMxiTD1HCR&index=3D1
>=20
> You can see other upto date videos and slides here at :
> http://spark-summit.org/2014/training
>=20
> Best regards,
> Rajiv
>=20
>=20
> 2014-08-13 19:36 GMT-04:00 aniketadnaik <aniket.adnaik@gmail.com>:
>=20
>> Hi,
>> I am new to Spark and want to explore more on Spark's master-worker/Clust=
er
>> manager communication architecture.
>> Any documents ? or code pointers will be helpful to start with.
>> Thanks!
>>=20
>>=20
>>=20
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Need-info-on-Sp=
ark-s-Communication-Networking-layer-tp7836.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>=20
>=20
> --=20
> Take care,
> Rajiv

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 18:14:06 2014
Return-Path: <dev-return-8907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 666AF1160F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 18:14:06 +0000 (UTC)
Received: (qmail 12888 invoked by uid 500); 15 Aug 2014 18:14:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12823 invoked by uid 500); 15 Aug 2014 18:14:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12812 invoked by uid 99); 15 Aug 2014 18:14:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:14:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:13:39 +0000
Received: from EX02-WEST.YOJOE.local ([169.254.1.40]) by EX03-WEST.YOJOE.local
 ([169.254.2.151]) with mapi id 14.03.0195.001; Fri, 15 Aug 2014 11:13:37
 -0700
From: Mingyu Kim <mkim@palantir.com>
To: Patrick Wendell <pwendell@gmail.com>, Gary Malouf <malouf.gary@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Topic: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Index: AQHPuBCZ/+f7rUz2C0O3higuAjnLYZvRJ3gAgAAvOoCAAKJtAA==
Date: Fri, 15 Aug 2014 18:13:36 +0000
Message-ID: <D01396DA.1257B%mkim@palantir.com>
References: <D01288A8.12442%mkim@palantir.com>
 <CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
 <CABPQxstdGkEv3jQoU4xmdGhJqdM0dyjKyxk70L-Mjt6oL8J67Q@mail.gmail.com>
In-Reply-To: <CABPQxstdGkEv3jQoU4xmdGhJqdM0dyjKyxk70L-Mjt6oL8J67Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: yes
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.100.91.90]
Content-Type: multipart/signed; protocol="application/pkcs7-signature";
	micalg=sha1; boundary="B_3490946014_10859048"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3490946014_10859048
Content-type: multipart/alternative;
	boundary="B_3490946014_10829568"


--B_3490946014_10829568
Content-type: text/plain;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

Thanks for your response. I think I misinterpreted the
stability/compatibility guarantee with 1.0 release. It seems like the
compatibility is only at the API level.

This is interesting because it means any system/product that is built on to=
p
of Spark and uses Spark with a long-running SparkContext connecting to the
cluster over network, will need to make sure it has the exact same version
of Spark jar as the cluster, even to the patch version. This would be
analogous to having to compile Spark against a very specific version of
Hadoop, as opposed to currently being able to use the Spark package with
CDH4 against most of the CDH4 Hadoop clusters.

Is it correct that Spark is focusing and prioritizing around the
spark-submit use cases than the aforementioned use cases? I just wanted to
better understand the future direction/prioritization of spark.

Thanks,
Mingyu

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Thursday, August 14, 2014 at 6:32 PM
To:  Gary Malouf <malouf.gary@gmail.com>
Cc:  Mingyu Kim <mkim@palantir.com>, "dev@spark.apache.org"
<dev@spark.apache.org>
Subject:  Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
against a 1.0.1 cluster

I commented on the bug. For driver mode, you'll need to get the
corresponding version of spark-submit for Spark 1.0.2.


On Thu, Aug 14, 2014 at 3:43 PM, Gary Malouf <malouf.gary@gmail.com> wrote:
> To be clear, is it 'compiled' against 1.0.2 or it packaged with it?
>=20
>=20
> On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim <mkim@palantir.com> wrote:
>=20
>> > I ran a really simple code that runs with Spark 1.0.2 jar and connects=
 to
>> > a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException=
. I
>> > filed the bug at https://issues.apache.org/jira/browse/SPARK-3050
>> <https://urldefense.proofpoint.com/v1/url?u=3Dhttps://issues.apache.org/ji=
ra/br
>> owse/SPARK-3050&k=3DfDZpZZQMmYwf27OU23GmAQ%3D%3D%0A&r=3DUKDOcu6qL3KsoZhpOohN=
BR1uc
>> PNmWnbd3eEJ9hVUdMk%3D%0A&m=3DqvQ59wZwD7EuezjTuLzmNTRUamDRDnI7%2F0%2BnULtXk=
4k%3D
>> %0A&s=3Db7abf7638a3e6fac2ddac9d8f0ca52f1a92945465abfb2e2d996a96d2301fec5> =
.
>> >
>> > I assumed the minor and patch releases shouldn=B9t break compatibility. =
Is
>> > that correct?
>> >
>> > Thanks,
>> > Mingyu
>> >




--B_3490946014_10829568
Content-type: text/html;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

<html><head></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size:=
 14px; font-family: Calibri, sans-serif;"><div><div>Thanks for your response=
. I think I misinterpreted the stability/compatibility guarantee with 1.0 re=
lease. It seems like the compatibility is only at the API level.</div><div><=
br></div><div>This is interesting because it means any system/product that i=
s built on top of Spark and uses Spark with a long-running SparkContext conn=
ecting to the cluster over network, will need to make sure it has the exact =
same version of Spark jar as the cluster, even to the patch version. This wo=
uld be analogous to having to compile Spark against a very specific version =
of Hadoop, as opposed to currently being able to use the Spark package with =
CDH4 against most of the CDH4 Hadoop clusters.</div><div><br></div><div>Is i=
t correct that Spark is focusing and prioritizing around the spark-submit us=
e cases than the aforementioned use cases? I just wanted to better understan=
d the future direction/prioritization of spark.</div><div><div><br></div><di=
v>Thanks,</div><div>Mingyu</div></div></div><div><br></div><span id=3D"OLK_SRC=
_BODY_SECTION"><div style=3D"font-family:Calibri; font-size:11pt; text-align:l=
eft; color:black; BORDER-BOTTOM: medium none; BORDER-LEFT: medium none; PADD=
ING-BOTTOM: 0in; PADDING-LEFT: 0in; PADDING-RIGHT: 0in; BORDER-TOP: #b5c4df =
1pt solid; BORDER-RIGHT: medium none; PADDING-TOP: 3pt"><span style=3D"font-we=
ight:bold">From: </span> Patrick Wendell &lt;<a href=3D"mailto:pwendell@gmail.=
com">pwendell@gmail.com</a>&gt;<br><span style=3D"font-weight:bold">Date: </sp=
an> Thursday, August 14, 2014 at 6:32 PM<br><span style=3D"font-weight:bold">T=
o: </span> Gary Malouf &lt;<a href=3D"mailto:malouf.gary@gmail.com">malouf.gar=
y@gmail.com</a>&gt;<br><span style=3D"font-weight:bold">Cc: </span> Mingyu Kim=
 &lt;<a href=3D"mailto:mkim@palantir.com">mkim@palantir.com</a>&gt;, "<a href=3D=
"mailto:dev@spark.apache.org">dev@spark.apache.org</a>" &lt;<a href=3D"mailto:=
dev@spark.apache.org">dev@spark.apache.org</a>&gt;<br><span style=3D"font-weig=
ht:bold">Subject: </span> Re: [SPARK-3050] Spark program running with 1.0.2 =
jar cannot run against a 1.0.1 cluster<br></div><div><br></div><div><meta ht=
tp-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><div><div dir=3D"lt=
r">I commented on the bug. For driver mode, you'll need to get the correspon=
ding version of spark-submit for Spark 1.0.2.</div><div class=3D"gmail_extra">=
<br><br><div class=3D"gmail_quote">On Thu, Aug 14, 2014 at 3:43 PM, Gary Malou=
f <span dir=3D"ltr">
&lt;<a href=3D"mailto:malouf.gary@gmail.com" target=3D"_blank">malouf.gary@gmai=
l.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin=
:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
To be clear, is it 'compiled' against 1.0.2 or it packaged with it?<br><div=
 class=3D"HOEnZb"><div class=3D"h5"><br><br>
On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim &lt;<a href=3D"mailto:mkim@palant=
ir.com">mkim@palantir.com</a>&gt; wrote:<br><br>
&gt; I ran a really simple code that runs with Spark 1.0.2 jar and connects=
 to<br>
&gt; a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException=
. I<br>
&gt; filed the bug at <a href=3D"https://urldefense.proofpoint.com/v1/url?u=3Dh=
ttps://issues.apache.org/jira/browse/SPARK-3050&amp;k=3DfDZpZZQMmYwf27OU23GmAQ=
%3D%3D%0A&amp;r=3DUKDOcu6qL3KsoZhpOohNBR1ucPNmWnbd3eEJ9hVUdMk%3D%0A&amp;m=3DqvQ5=
9wZwD7EuezjTuLzmNTRUamDRDnI7%2F0%2BnULtXk4k%3D%0A&amp;s=3Db7abf7638a3e6fac2dda=
c9d8f0ca52f1a92945465abfb2e2d996a96d2301fec5" target=3D"_blank">
https://issues.apache.org/jira/browse/SPARK-3050</a>.<br>
&gt;<br>
&gt; I assumed the minor and patch releases shouldn&#8217;t break compatibi=
lity. Is<br>
&gt; that correct?<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Mingyu<br>
&gt;<br></div></div></blockquote></div><br></div></div></div></span></body>=
</html>

--B_3490946014_10829568--

--B_3490946014_10859048
Content-Type: application/pkcs7-signature; name="smime.p7s"
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename="smime.p7s"

MIIWigYJKoZIhvcNAQcCoIIWezCCFncCAQExCzAJBgUrDgMCGgUAMAsGCSqGSIb3DQEHAaCC
FCIwggYlMIIFDaADAgECAhEAxVE8RoF7tBb2e1bnZ+EQfTANBgkqhkiG9w0BAQUFADCBkzEL
MAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2Fs
Zm9yZDEaMBgGA1UEChMRQ09NT0RPIENBIExpbWl0ZWQxOTA3BgNVBAMTMENPTU9ETyBDbGll
bnQgQXV0aGVudGljYXRpb24gYW5kIFNlY3VyZSBFbWFpbCBDQTAeFw0xMjA0MTEwMDAwMDBa
Fw0xNTA0MTEyMzU5NTlaMIIBNzELMAkGA1UEBhMCVVMxDjAMBgNVBBETBTk0MzAxMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRIwEAYDVQQHEwlQYWxvIEFsdG8xEjAQBgNVBAkTCVN1aXRlIDMw
MDEZMBcGA1UECRMQMTAwIEhhbWlsdG9uIEF2ZTEeMBwGA1UEChMVUGFsYW50aXIgVGVjaG5v
bG9naWVzMQswCQYDVQQLEwJJVDE7MDkGA1UECxMySXNzdWVkIHRocm91Z2ggUGFsYW50aXIg
VGVjaG5vbG9naWVzIEUtUEtJIE1hbmFnZXIxHzAdBgNVBAsTFkNvcnBvcmF0ZSBTZWN1cmUg
RW1haWwxEzARBgNVBAMTCk1pbmd5dSBLaW0xIDAeBgkqhkiG9w0BCQEWEW1raW1AcGFsYW50
aXIuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0PgTFke9p8CRQHjeLwtS
W1TiAP8AhDoANwRdBYwB+ovQJrVAfAxsZXay3gdG6NbSE+khN58VdadLAIdb3xhofgm2shfW
sYs/kEVVr8VBj8l18/8pFBz0BTQXzfHatQQpCqdQi9zfqMFEu0aeFD6VNyuR1RStR+8Q1Qcd
IUdhO+2HRx28Q0VAyAWEO7LwJ5JnqIxGlD9Qgfhq7WDQqHMDxbHyt9Qnm70Sw+IKPTN5mDGA
vObmfIx9iCNDj5Ams41z2xH6mpWGXVaxrZbfxbVepNJoo7Q0sH0MK3PlkhEQI5w0Rg7VdPzW
g8oTBEj2jQXfy2K99YGfcrE5XuQJkxe/0wIDAQABo4IByzCCAccwHwYDVR0jBBgwFoAUehNO
AHRbxnhjZCfBL+KgW7x5xXswHQYDVR0OBBYEFFrF5FjUZNuNSlZOsNReZmaKI5aVMA4GA1Ud
DwEB/wQEAwIFoDAMBgNVHRMBAf8EAjAAMB0GA1UdJQQWMBQGCCsGAQUFBwMEBggrBgEFBQcD
AjBGBgNVHSAEPzA9MDsGDCsGAQQBsjEBAgEDBTArMCkGCCsGAQUFBwIBFh1odHRwczovL3Nl
Y3VyZS5jb21vZG8ubmV0L0NQUzBXBgNVHR8EUDBOMEygSqBIhkZodHRwOi8vY3JsLmNvbW9k
b2NhLmNvbS9DT01PRE9DbGllbnRBdXRoZW50aWNhdGlvbmFuZFNlY3VyZUVtYWlsQ0EuY3Js
MIGIBggrBgEFBQcBAQR8MHowUgYIKwYBBQUHMAKGRmh0dHA6Ly9jcnQuY29tb2RvY2EuY29t
L0NPTU9ET0NsaWVudEF1dGhlbnRpY2F0aW9uYW5kU2VjdXJlRW1haWxDQS5jcnQwJAYIKwYB
BQUHMAGGGGh0dHA6Ly9vY3NwLmNvbW9kb2NhLmNvbTAcBgNVHREEFTATgRFta2ltQHBhbGFu
dGlyLmNvbTANBgkqhkiG9w0BAQUFAAOCAQEAOQe8Mp7I3VL3zWfMmxEPV0f7WeDNwUYo90y6
KDM9wMci1GodWgqdPLmVc0LVakLZGDVtJxLE4RgFNXgt8u0L3BudOr9Nd3x3oYb1cnNh4kCh
3yZucsTia4JEJ9uAI3fhrvnHZBz2GIwQMoS05m8a4dcztDKzQLHH7vRxj0aAwAoo5pz2ZPHI
9+EjtfBOXd2UfAlW3bC0o9fuScMSENLOA9TfdZW5OIgSxC2byrnOccn6zPylGzzmRluITTcd
W1DDG17bLq/F6evfQE15oR2WxapRP3v47wF7LXP7wljZaFi+HL2ki5XH5s2xEtLnZz/5+a18
1IgamlQ+MxM+TIYyYTCCBRowggQCoAMCAQICEG0Z6qcZT2ozIuYiMnqqcd4wDQYJKoZIhvcN
AQEFBQAwga4xCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJVVDEXMBUGA1UEBxMOU2FsdCBMYWtl
IENpdHkxHjAcBgNVBAoTFVRoZSBVU0VSVFJVU1QgTmV0d29yazEhMB8GA1UECxMYaHR0cDov
L3d3dy51c2VydHJ1c3QuY29tMTYwNAYDVQQDEy1VVE4tVVNFUkZpcnN0LUNsaWVudCBBdXRo
ZW50aWNhdGlvbiBhbmQgRW1haWwwHhcNMTEwNDI4MDAwMDAwWhcNMjAwNTMwMTA0ODM4WjCB
kzELMAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMH
U2FsZm9yZDEaMBgGA1UEChMRQ09NT0RPIENBIExpbWl0ZWQxOTA3BgNVBAMTMENPTU9ETyBD
bGllbnQgQXV0aGVudGljYXRpb24gYW5kIFNlY3VyZSBFbWFpbCBDQTCCASIwDQYJKoZIhvcN
AQEBBQADggEPADCCAQoCggEBAJKEhFtLV5jUXi+LpOFAyKNTWF9mZfEyTvefMn1V0HhMVbdC
lOD5J3EHxcZppLkyxPFAGpDMJ1Zifxe1cWmu5SAb5MtjXmDKokH2auGj/7jfH0htZUOMKi4r
Yzh337EXrMLaggLW1DJq1GdvIBOPXDX65VSAr9hxCh03CgJQU2yVHakQFLSZlVkSMf8JotJM
3FLb3uJAAVtIaN3FSrTg7SQfOq9xXwfjrL8UO7AlcWg99A/WF1hGFYE8aIuLgw9teiFX5jSw
2zJ+40rhpVJyZCaRTqWSD//gsWD9Gm9oUZljjRqLpcxCm5t9ImPTqaD8zp6Q30QZ9FxbNboW
86eb/8ECAwEAAaOCAUswggFHMB8GA1UdIwQYMBaAFImCZ33EnSZwAEu0UEh83j2uBG59MB0G
A1UdDgQWBBR6E04AdFvGeGNkJ8Ev4qBbvHnFezAOBgNVHQ8BAf8EBAMCAQYwEgYDVR0TAQH/
BAgwBgEB/wIBADARBgNVHSAECjAIMAYGBFUdIAAwWAYDVR0fBFEwTzBNoEugSYZHaHR0cDov
L2NybC51c2VydHJ1c3QuY29tL1VUTi1VU0VSRmlyc3QtQ2xpZW50QXV0aGVudGljYXRpb25h
bmRFbWFpbC5jcmwwdAYIKwYBBQUHAQEEaDBmMD0GCCsGAQUFBzAChjFodHRwOi8vY3J0LnVz
ZXJ0cnVzdC5jb20vVVROQWRkVHJ1c3RDbGllbnRfQ0EuY3J0MCUGCCsGAQUFBzABhhlodHRw
Oi8vb2NzcC51c2VydHJ1c3QuY29tMA0GCSqGSIb3DQEBBQUAA4IBAQCF1r54V1VtM39EUv5C
1QaoAQOAivsNsv1Kv/avQUn1G1rF0q0bc24+6SZ85kyYwTAo38v7QjyhJT4KddbQPTmGZtGh
m7VNm2+vKGwdr+XqdFqo2rHA8XV6L566k3nK/uKRHlZ0sviN0+BDchvtj/1gOSBH+4uvOmVI
PJg9pSW/ve9g4EnlFsjrP0OD8ODuDcHTzTNfm9C9YGqzO/761Mk6PB/tm/+bSTO+Qik5g+4z
aS6CnUVNqGnagBsePdIaXXxHmaWbCG0SmYbWXVcHG6cwvktJRLiQfsrReTjrtDP6oDpdJlie
YVUYtCHVmdXgQ0BCML7qpeeU0rD+83X5f27nMIIEnTCCA4WgAwIBAgIQND3pK6wnNP+PyzSU
+8xwVDANBgkqhkiG9w0BAQUFADBvMQswCQYDVQQGEwJTRTEUMBIGA1UEChMLQWRkVHJ1c3Qg
QUIxJjAkBgNVBAsTHUFkZFRydXN0IEV4dGVybmFsIFRUUCBOZXR3b3JrMSIwIAYDVQQDExlB
ZGRUcnVzdCBFeHRlcm5hbCBDQSBSb290MB4XDTA1MDYwNzA4MDkxMFoXDTIwMDUzMDEwNDgz
OFowga4xCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJVVDEXMBUGA1UEBxMOU2FsdCBMYWtlIENp
dHkxHjAcBgNVBAoTFVRoZSBVU0VSVFJVU1QgTmV0d29yazEhMB8GA1UECxMYaHR0cDovL3d3
dy51c2VydHJ1c3QuY29tMTYwNAYDVQQDEy1VVE4tVVNFUkZpcnN0LUNsaWVudCBBdXRoZW50
aWNhdGlvbiBhbmQgRW1haWwwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCyOYWk
8n2rQTtiRjeuzcFgdbw5ZflKGkeiucxIzGqY1U01GbmkQuXOSeKKLx580jEHx060g2SdLinV
omTEhb2FUTV5pE5okHsceqSSqBfymBXyk8zJpDKVuwxPML2YoAuL5W4bokb6eLyib6tZXqUv
z8rabaov66yhs2qqty5nNYt54R5piOLmRs2gpeq+C852OnoOm+r82idbPXMfIuZIYcZM82mx
qC4bttQxICy8goqOpA6l14lD/BZarx1x1xFZ2rqHDa/68+HC8KTFZ4zW1lQ63gqkugN3s2XI
/R7TdGKqGMpokx6hhX71R2XL+E1XKHTSNP8wtu72YjAUjCzrAgMBAAGjgfQwgfEwHwYDVR0j
BBgwFoAUrb2YejS0Jvf6xCZU7wO94CTLVBowHQYDVR0OBBYEFImCZ33EnSZwAEu0UEh83j2u
BG59MA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MBEGA1UdIAQKMAgwBgYEVR0g
ADBEBgNVHR8EPTA7MDmgN6A1hjNodHRwOi8vY3JsLnVzZXJ0cnVzdC5jb20vQWRkVHJ1c3RF
eHRlcm5hbENBUm9vdC5jcmwwNQYIKwYBBQUHAQEEKTAnMCUGCCsGAQUFBzABhhlodHRwOi8v
b2NzcC51c2VydHJ1c3QuY29tMA0GCSqGSIb3DQEBBQUAA4IBAQABvJzjYyiw8zEBwt973WKg
AZ0jMQ+cknNTUeofTPrWn8TKL2d+eDMPdBa5kYeR9Yom+mRwANge+QsEYlCHk4HU2vUj2zS7
hVa0cDRueIM3HoUcxREVkl+HF72sav3xwtHMiV+xfPA+UfI183zsYJhrOivg79+zfYbrtRv1
W+yifJgT1wBQudEtc94DeHThBYUxXsuauZ2UxrmUN3Vy3ET7Z+jw+iUeUqfaJelH4KDHPKBO
sQo2+3dIn++Xivu0/uOUFKiDvFwtP9JgcWDuwnGCDOmINuPaILSjoGyqlku4gI51ykkH9jsU
ut/cBdmf2+Cy5k2geCbn5y1uf1/GHogVMIIENjCCAx6gAwIBAgIBATANBgkqhkiG9w0BAQUF
ADBvMQswCQYDVQQGEwJTRTEUMBIGA1UEChMLQWRkVHJ1c3QgQUIxJjAkBgNVBAsTHUFkZFRy
dXN0IEV4dGVybmFsIFRUUCBOZXR3b3JrMSIwIAYDVQQDExlBZGRUcnVzdCBFeHRlcm5hbCBD
QSBSb290MB4XDTAwMDUzMDEwNDgzOFoXDTIwMDUzMDEwNDgzOFowbzELMAkGA1UEBhMCU0Ux
FDASBgNVBAoTC0FkZFRydXN0IEFCMSYwJAYDVQQLEx1BZGRUcnVzdCBFeHRlcm5hbCBUVFAg
TmV0d29yazEiMCAGA1UEAxMZQWRkVHJ1c3QgRXh0ZXJuYWwgQ0EgUm9vdDCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBALf3GjPm8gAELTngTlvtH7xsD821+iO2zt6bETOXpClM
fZOfvUq8k+0DGuOPz+VtUFrWlymUWoCwSXrbLpX9uMq/NzgtHj6RQa1wVsfwTz/oMp50ysiQ
VOnGXw94nZpAPA6sYapeFI+eh6FqUNzXmk6vBbOmcZSccbNQYArHE504B4YCqOmoaSYYkKtM
sE8jqzpPhNjfzp/haW+710LXa0Tkx63ubUFfclpxCDezeWWkWaCUN/cALw3CknLa0Dhy2xSo
RcRdKn23tNbE7qzNE0S3ySvdQwAl+mG5aWpYIxG3pzOPVnVZ9c0p10a3CitlttNCbxWyuHv7
7+ldU9U0WicCAwEAAaOB3DCB2TAdBgNVHQ4EFgQUrb2YejS0Jvf6xCZU7wO94CTLVBowCwYD
VR0PBAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wgZkGA1UdIwSBkTCBjoAUrb2YejS0Jvf6xCZU
7wO94CTLVBqhc6RxMG8xCzAJBgNVBAYTAlNFMRQwEgYDVQQKEwtBZGRUcnVzdCBBQjEmMCQG
A1UECxMdQWRkVHJ1c3QgRXh0ZXJuYWwgVFRQIE5ldHdvcmsxIjAgBgNVBAMTGUFkZFRydXN0
IEV4dGVybmFsIENBIFJvb3SCAQEwDQYJKoZIhvcNAQEFBQADggEBALCb4IUlwtYj4g+WBpKd
QZic2YR5gdkeWxQHIzZlj7DYd7usQWxHYINRsPkyPef89iYTx4AWpb9a/IfPeHmJIZriTAcK
hjW88t5RxNKWt9x+Tu5w/Rw56wwCURQtjr0W4MHfRnXnJK3s9EK0hZNwEGe6nQY1ShjTK3rM
UUKhemPR5ruhxSvCNr4TDea9Y355e6cJDUCrat2PisP29owaQgVR1EX1n6diIWgVIEM8med8
vSTYqZEXc4g/VhsxOBi0cQ+azcgOno4uG+GMmIPLHzHxREzGBHNJdmAPx/i9F4BrLunMTA5a
mnkPIAou1Z5jJh5VkpTYghdae9C8x49OhgQxggIwMIICLAIBATCBqTCBkzELMAkGA1UEBhMC
R0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2FsZm9yZDEaMBgG
A1UEChMRQ09NT0RPIENBIExpbWl0ZWQxOTA3BgNVBAMTMENPTU9ETyBDbGllbnQgQXV0aGVu
dGljYXRpb24gYW5kIFNlY3VyZSBFbWFpbCBDQQIRAMVRPEaBe7QW9ntW52fhEH0wCQYFKw4D
AhoFAKBdMCMGCSqGSIb3DQEJBDEWBBRGQDdyW1Z0yC4r10P5BenuVT8CxzAYBgkqhkiG9w0B
CQMxCwYJKoZIhvcNAQcBMBwGCSqGSIb3DQEJBTEPFw0xNDA4MTUxODEzMzRaMA0GCSqGSIb3
DQEBAQUABIIBABCt7MHIKhAuqoF4QOXxbjrvf14R3by/hVCa6axot7/n0tI8q4I9mtG+vwMj
o8rIM0+qB0XHeyA9GDqEVCyzgMiiqD/KAUCLnmbRLIkImh8iCOhdZmlAhWgXZSQ5UGJVXEDe
OuNVrx5SDisDz+X5+noqelMZBIun1T0TgBpqjkhUEBkhtNE6bxXIjMbmnpa2D1HmtbQPBw34
9GAmxYiV3141d6wTPk02NBKcBLbX2jvT2xCQ6wQcORIevOO5QSSHiWetSOSpskW2OylSJBXu
863PNZxXVOAra4uEeYp9uLN8Epkxa2uQ65idBXo8Hhl244pgleT+MzBD9O/hDK6Z1ik=

--B_3490946014_10859048--

From dev-return-8908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 18:25:29 2014
Return-Path: <dev-return-8908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E88D116B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 18:25:29 +0000 (UTC)
Received: (qmail 52629 invoked by uid 500); 15 Aug 2014 18:25:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52564 invoked by uid 500); 15 Aug 2014 18:25:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52548 invoked by uid 99); 15 Aug 2014 18:25:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:25:25 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:24:59 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XIMBO-0003FX-MD
	for dev@spark.incubator.apache.org; Fri, 15 Aug 2014 11:24:58 -0700
Date: Fri, 15 Aug 2014 11:24:58 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
In-Reply-To: <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
References: <1408115914518-7865.post@n3.nabble.com> <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_180052_20111482.1408127098671"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_180052_20111482.1408127098671
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Xiangrui,
I wasn't setting spark.driver.memory. I'll try that and report back.

In terms of this running on the cluster, my assumption was that calling foreach on an array(I converted samples using toArray) would mean counts is propagated locally. The object would then be serialized to executors fully propagated. Is this correct?

I'm actually trying to load a trie and used the hashmap as an example of loading data into an object that needs to be serialized. Is there a better way of doing this?

- jerry


> On Aug 15, 2014, at 8:36 AM, "Xiangrui Meng [via Apache Spark Developers List]" <ml-node+s1001551n7866h85@n3.nabble.com> wrote:
> 
> Did you set driver memory? You can confirm it in the Executors tab of 
> the WebUI. Btw, the code may only work in local mode. In a cluster 
> mode, counts will be serialized to remote workers and the result is 
> not fetched by the driver after foreach. You can use RDD.countByValue 
> instead. -Xiangrui 
> 
> On Fri, Aug 15, 2014 at 8:18 AM, jerryye <[hidden email]> wrote:
> 
> > Hi All, 
> > I'm not sure if I should file a JIRA or if I'm missing something obvious 
> > since the test code I'm trying is so simple. I've isolated the problem I'm 
> > seeing to a memory issue but I don't know what parameter I need to tweak, it 
> > does seem related to spark.akka.frameSize. If I sample my RDD with 35% of 
> > the data, everything runs to completion, with more than 35%, it fails. In 
> > standalone mode, I can run on the full RDD without any problems. 
> > 
> > // works 
> > val samples = sc.textFile("s3n://geonames").sample(false,0.35) // 64MB, 
> > 2849439 Lines 
> > 
> > // fails 
> > val samples = sc.textFile("s3n://geonames").sample(false,0.4) // 64MB, 
> > 2849439 Lines 
> > 
> > Any ideas? 
> > 
> > 1) RDD size is causing the problem. The code below as is fails but if I swap 
> > smallSample for samples, the code runs end to end on both cluster and 
> > standalone. 
> > 2) The error I get is: 
> > rg.apache.spark.SparkException: Job aborted due to stage failure: Task 3.0:1 
> > failed 4 times, most recent failure: TID 12 on host 
> > ip-10-251-14-74.us-west-2.compute.internal failed for unknown reason 
> > Driver stacktrace: 
> >         at 
> > org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044) 
> >         at 
> > org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028) 
> > 3) Using the 1.1.0 branch the driver freezes instead of aborting with the 
> > previous error in #2. 
> > 4) In 1.1.0, changing spark.akka.frameSize also has the effect of no 
> > progress in the driver. 
> > 
> > Code: 
> > val smallSample = sc.parallelize(Array("foo word", "bar word", "baz word")) 
> > 
> > val samples = sc.textFile("s3n://geonames") // 64MB, 2849439 Lines of short 
> > strings 
> > 
> > val counts = new collection.mutable.HashMap[String, Int].withDefaultValue(0) 
> > 
> > samples.toArray.foreach(counts(_) += 1) 
> > 
> > val result = samples.map( 
> >   l => (l, counts.get(l)) 
> > ) 
> > 
> > result.count 
> > 
> > Settings (with or without Kryo doesn't matter): 
> > export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g" 
> > export SPARK_MEM=10g 
> > spark.akka.frameSize 40 
> > #spark.serializer org.apache.spark.serializer.KryoSerializer 
> > #spark.kryoserializer.buffer.mb 1000 
> > spark.executor.memory 58315m 
> > spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/ 
> > spark.executor.extraClassPath /root/ephemeral-hdfs/conf 
> > 
> > 
> > 
> > -- 
> > View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865.html
> > Sent from the Apache Spark Developers List mailing list archive at Nabble.com. 
> > 
> > --------------------------------------------------------------------- 
> > To unsubscribe, e-mail: [hidden email] 
> > For additional commands, e-mail: [hidden email] 
> >
> 
> --------------------------------------------------------------------- 
> To unsubscribe, e-mail: [hidden email] 
> For additional commands, e-mail: [hidden email] 
> 
> 
> 
> If you reply to this email, your message will be added to the discussion below:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7866.html
> To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h70@n3.nabble.com 
> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click here.
> NAML




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7871.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_180052_20111482.1408127098671--

From dev-return-8909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 18:28:32 2014
Return-Path: <dev-return-8909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59913116E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 18:28:32 +0000 (UTC)
Received: (qmail 64277 invoked by uid 500); 15 Aug 2014 18:28:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64207 invoked by uid 500); 15 Aug 2014 18:28:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64196 invoked by uid 99); 15 Aug 2014 18:28:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:28:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:28:26 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so2610156wgh.35
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 11:28:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=+oZL7qagzNmuNUh+QFYJxFnQDaNJgVPP0KRWgKevOyg=;
        b=cxYEdyuVPPWnZBc3kUQrCTUYtTwE/2yWk9iAOuAN4o5MQCfLjYoPp/Wpqg5Pyg8/vf
         TMKzc4/4tlC9u8A0JmjSR0zlQ5AO7d5rtyPH7PG1FOtA8OqpvbzEJar22D1aZKn4fgFi
         sL1C+D5ZzLRwqB8J6M+zvzsyIbYKqzEgUh1uECyI2wWAR6r38E/TbZz0OsxaOhCOFO/h
         tiHXrFA5DWsJb9ft6pNrXcQQf38XA8B5q7OUVSaEoVCFGc51MYNTLSsPojuI0/jIM8Nh
         frW9xpOZqjB88AGmHWF1ih07cFWMxK0K0LzMTlWxjJBPqY9M76eHpB4p5wGyFyxAqW1o
         84Ug==
X-Gm-Message-State: ALoCoQnjNFoWEAGNnN0NMaKEl22QuXudqeq2H1nNORGVKPJ2sfulRRdSeQ9zhZ4ZsnuYuuGP9JkY
MIME-Version: 1.0
X-Received: by 10.180.82.166 with SMTP id j6mr506423wiy.83.1408127284817; Fri,
 15 Aug 2014 11:28:04 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.108.198 with HTTP; Fri, 15 Aug 2014 11:28:04 -0700 (PDT)
Received: by 10.216.108.198 with HTTP; Fri, 15 Aug 2014 11:28:04 -0700 (PDT)
In-Reply-To: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
Date: Fri, 15 Aug 2014 11:28:04 -0700
Message-ID: <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
Subject: Re: Tests failing
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0442810a9c431f0500af2e65
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442810a9c431f0500af2e65
Content-Type: text/plain; charset=UTF-8

Also I think Jenkins doesn't post build timeouts to github. Is there anyway
we can fix that ?
On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Hi All,
>
> I noticed that all PR tests run overnight had failed due to timeouts. The
> patch that updates the netty shuffle I believe somehow inflated to the
> build time significantly. That patch had been tested, but one change was
> made before it was merged that was not tested.
>
> I've reverted the patch for now to see if it brings the build times back
> down.
>
> - Patrick
>

--f46d0442810a9c431f0500af2e65--

From dev-return-8910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 18:46:01 2014
Return-Path: <dev-return-8910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2FBD2117B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 18:46:01 +0000 (UTC)
Received: (qmail 16863 invoked by uid 500); 15 Aug 2014 18:46:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16796 invoked by uid 500); 15 Aug 2014 18:46:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16778 invoked by uid 99); 15 Aug 2014 18:45:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:45:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 18:45:55 +0000
Received: by mail-wg0-f47.google.com with SMTP id b13so2668788wgh.30
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 11:45:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=FgXxcS028HEm+A5TY2IHYKEMD8/trAvH237t7oc36Rg=;
        b=b52e0hYKPjY0MrXxrPZDd3VYVQekD/a2EQq8e/T9T8GnqD+0nKfuaIGkmCN8i3IONe
         IKZrN7RmTpSRWXWwkgdxvQiRMOPT3dVkzmVP/bwM4Ge1E0HIEv8BEj5OEQUE+sCwic4o
         hEuPmvpDEIrmsevTwfVv3fYgtBen2mK0iTW0KZhF2JKeVNgmU3KohNmdpBoXC4KBRiFu
         iWT/StUCs8n22slzsLJs4GqfIlKasaZQ01AHGv6otAY0VSSAa1pC2roQgb/WiIF245SP
         QHr2tvTVhicL/1Q8SKYiaqcr2dAbVDBRTM9t79szkV+tJR8aXquLVHLkyk1okrjsLWIM
         yBWQ==
X-Received: by 10.180.210.172 with SMTP id mv12mr50213668wic.45.1408128334669;
 Fri, 15 Aug 2014 11:45:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 15 Aug 2014 11:44:54 -0700 (PDT)
In-Reply-To: <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
 <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 15 Aug 2014 14:44:54 -0400
Message-ID: <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
Subject: Re: Tests failing
To: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37ede2faff90500af6db8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37ede2faff90500af6db8
Content-Type: text/plain; charset=UTF-8

Shivaram,

Can you point us to an example of that happening? The Jenkins console
output, that is.

Nick


On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
shivaram@eecs.berkeley.edu> wrote:

> Also I think Jenkins doesn't post build timeouts to github. Is there anyway
> we can fix that ?
> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
> > Hi All,
> >
> > I noticed that all PR tests run overnight had failed due to timeouts. The
> > patch that updates the netty shuffle I believe somehow inflated to the
> > build time significantly. That patch had been tested, but one change was
> > made before it was merged that was not tested.
> >
> > I've reverted the patch for now to see if it brings the build times back
> > down.
> >
> > - Patrick
> >
>

--001a11c37ede2faff90500af6db8--

From dev-return-8911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 19:19:11 2014
Return-Path: <dev-return-8911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0128711924
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 19:19:11 +0000 (UTC)
Received: (qmail 26429 invoked by uid 500); 15 Aug 2014 19:19:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26358 invoked by uid 500); 15 Aug 2014 19:19:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26346 invoked by uid 99); 15 Aug 2014 19:19:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:19:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:18:44 +0000
Received: by mail-oa0-f54.google.com with SMTP id n16so2345245oag.41
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 12:18:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SmERNEffdYG9HKyF8dBgleY6xhTj7FMf1dEFWk/XNVs=;
        b=UKfj4IPzeWbxK73/hJ+wMQ7tDmhJB6CpRaMDPzzDV+O9glmQ9IV5gN8x5uHxWwOSLk
         2phEc2S9GBDWB0+bQAC7qDCaCJoIy9foUKqFMsD6Tbwa6X/NVYLlMRMISDk0AtH6TE+g
         50I8ApeY5770WPpPnrs1MwlfD0ZxKxuEsQY8D33kkAfK1BrPY7li3ODQxPs+QwN9ukR9
         e1NsEZ1OJn7zX3bj1kbh1T9MSUBq0gZ3+gnlUpWpTL9SwLZgWimmIl3YKJN/2Lcm21NJ
         QPZxC2rtI6+SvTT1RqgJQpCevNNrHhS2Lew6vUqTWCQRTpHN2L9KwYEOFkz/hAUNTmvL
         xX/w==
MIME-Version: 1.0
X-Received: by 10.182.249.52 with SMTP id yr20mr21885702obc.10.1408130322763;
 Fri, 15 Aug 2014 12:18:42 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 15 Aug 2014 12:18:42 -0700 (PDT)
In-Reply-To: <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
	<CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
	<CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
Date: Fri, 15 Aug 2014 12:18:42 -0700
Message-ID: <CABPQxstPsN=2qxUOqhgSdJhJ3aABcEXmohGKnuZ73BNfyqk2zQ@mail.gmail.com>
Subject: Re: Tests failing
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f92410eaf95610500afe313
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f92410eaf95610500afe313
Content-Type: text/plain; charset=ISO-8859-1

We'll need to build timeouts into our own reporting infrastructure - it
shouldn't be too bad but we just need to script it. Unfortunately the
Jenkins plug-in is either "all or nothing" in what it reports, so we can't
have it report timeouts unless we want all the other fairly noisy messages
from it.


On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Shivaram,
>
> Can you point us to an example of that happening? The Jenkins console
> output, that is.
>
> Nick
>
>
> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
> shivaram@eecs.berkeley.edu> wrote:
>
>> Also I think Jenkins doesn't post build timeouts to github. Is there
>> anyway
>> we can fix that ?
>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>
>> > Hi All,
>> >
>> > I noticed that all PR tests run overnight had failed due to timeouts.
>> The
>> > patch that updates the netty shuffle I believe somehow inflated to the
>> > build time significantly. That patch had been tested, but one change was
>> > made before it was merged that was not tested.
>> >
>> > I've reverted the patch for now to see if it brings the build times back
>> > down.
>> >
>> > - Patrick
>> >
>>
>
>

--e89a8f92410eaf95610500afe313--

From dev-return-8912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 19:28:49 2014
Return-Path: <dev-return-8912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42C7511957
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 19:28:49 +0000 (UTC)
Received: (qmail 41018 invoked by uid 500); 15 Aug 2014 19:28:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40949 invoked by uid 500); 15 Aug 2014 19:28:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40931 invoked by uid 99); 15 Aug 2014 19:28:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:28:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:28:44 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so2306379obc.19
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 12:28:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=xxWumRwJE/SzDq+C5x2s1RGmhvCE/gXnFjgPHUYPCBI=;
        b=HiJXpWwRrtqp2496RgdnazXus9Sc9ssMsT/lAOuuFGy7GCA9uBawO9vfocg570v7wo
         S5T+Ja/mL/ehcbk+CFJtCWpLt+l6KEDaq5eRhWDB0yxitc8CYmb+HiHbTxX4WXFI1q2f
         0+8z9/0nWOkHj/Xv3W8WIhtuit0XCVm9Hm0QOanGHirQaVEmQhPf+DnVwByMXPkEQNzY
         BDlAzI2DvtgYvFxMWwDmpktew49GquvfZChM1Cp/jRE54DK0G15AZF9QOI9O1WwZQUm2
         V0094E8aBdkDeP+RYNNdHSAXVyFoYZqd565GXSNuK1Nx7RZpZyPTLg7B6uN0V/Qyeb3j
         4uKw==
MIME-Version: 1.0
X-Received: by 10.182.27.5 with SMTP id p5mr22121503obg.42.1408130903669; Fri,
 15 Aug 2014 12:28:23 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 15 Aug 2014 12:28:23 -0700 (PDT)
In-Reply-To: <D01396DA.1257B%mkim@palantir.com>
References: <D01288A8.12442%mkim@palantir.com>
	<CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
	<CABPQxstdGkEv3jQoU4xmdGhJqdM0dyjKyxk70L-Mjt6oL8J67Q@mail.gmail.com>
	<D01396DA.1257B%mkim@palantir.com>
Date: Fri, 15 Aug 2014 12:28:23 -0700
Message-ID: <CABPQxstjpmZHRd_p_mPMi4G_qcqN1Rwkc1=z7sTaXrFAN8WbNw@mail.gmail.com>
Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
From: Patrick Wendell <pwendell@gmail.com>
To: Mingyu Kim <mkim@palantir.com>
Cc: Gary Malouf <malouf.gary@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115f6cc4f81af0500b0066d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f6cc4f81af0500b0066d
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Hey Mingyu,

For this reason we are encouraging all users to run spark-submit. In Spark
we capture closures and send them over the network from the driver to the
executors. These are then deserialized on the executor. So if your driver
program has different versions of certain classes than exist on the
executor, it doesn't work well. We've even run into stranger issues, where
the exact same version of Spark was used at the driver and the executor,
but they were compiled at different times. Since Scala doesn't guarantee
stable naming for certain types of anonymous classes, the class names
didn't match up and it caused errors at runtime.

The most straightforward way to deal with this is to inject, at run-time,
the exact version of Spark that the cluster expects if you are running the
standalone mode.

I think we'd be totally open to improving this to provide "API stability"
for the case you are working with, i.e. the case where you have spark 1.0.X
at the driver and 1.0.Y on the executors. But it will require looking at
what exactly causes incompatibility and seeing if there is a solution. In
this case I think we changed a publicly exposed class (the RDD class) in
some way that caused compatibility issues... even though we didn't change
any binary signatures.

BTW - this is not relevant to YARN mode where you ship Spark with your job
so there is no "cluster version of Spark".

- Patrick


On Fri, Aug 15, 2014 at 11:13 AM, Mingyu Kim <mkim@palantir.com> wrote:

> Thanks for your response. I think I misinterpreted the
> stability/compatibility guarantee with 1.0 release. It seems like the
> compatibility is only at the API level.
>
> This is interesting because it means any system/product that is built on
> top of Spark and uses Spark with a long-running SparkContext connecting t=
o
> the cluster over network, will need to make sure it has the exact same
> version of Spark jar as the cluster, even to the patch version. This woul=
d
> be analogous to having to compile Spark against a very specific version o=
f
> Hadoop, as opposed to currently being able to use the Spark package with
> CDH4 against most of the CDH4 Hadoop clusters.
>
> Is it correct that Spark is focusing and prioritizing around the
> spark-submit use cases than the aforementioned use cases? I just wanted t=
o
> better understand the future direction/prioritization of spark.
>
> Thanks,
> Mingyu
>
> From: Patrick Wendell <pwendell@gmail.com>
> Date: Thursday, August 14, 2014 at 6:32 PM
> To: Gary Malouf <malouf.gary@gmail.com>
> Cc: Mingyu Kim <mkim@palantir.com>, "dev@spark.apache.org" <
> dev@spark.apache.org>
> Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
> against a 1.0.1 cluster
>
> I commented on the bug. For driver mode, you'll need to get the
> corresponding version of spark-submit for Spark 1.0.2.
>
>
> On Thu, Aug 14, 2014 at 3:43 PM, Gary Malouf <malouf.gary@gmail.com>
> wrote:
>
>> To be clear, is it 'compiled' against 1.0.2 or it packaged with it?
>>
>>
>> On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim <mkim@palantir.com> wrote:
>>
>> > I ran a really simple code that runs with Spark 1.0.2 jar and connects
>> to
>> > a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException=
.
>> I
>> > filed the bug at https://issues.apache.org/jira/browse/SPARK-3050
>> <https://urldefense.proofpoint.com/v1/url?u=3Dhttps://issues.apache.org/=
jira/browse/SPARK-3050&k=3DfDZpZZQMmYwf27OU23GmAQ%3D%3D%0A&r=3DUKDOcu6qL3Ks=
oZhpOohNBR1ucPNmWnbd3eEJ9hVUdMk%3D%0A&m=3DqvQ59wZwD7EuezjTuLzmNTRUamDRDnI7%=
2F0%2BnULtXk4k%3D%0A&s=3Db7abf7638a3e6fac2ddac9d8f0ca52f1a92945465abfb2e2d9=
96a96d2301fec5>
>> .
>> >
>> > I assumed the minor and patch releases shouldn't break compatibility. =
Is
>> > that correct?
>> >
>> > Thanks,
>> > Mingyu
>> >
>>
>
>

--089e0115f6cc4f81af0500b0066d--

From dev-return-8913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 19:34:01 2014
Return-Path: <dev-return-8913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B475A11978
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 19:34:01 +0000 (UTC)
Received: (qmail 53808 invoked by uid 500); 15 Aug 2014 19:34:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53737 invoked by uid 500); 15 Aug 2014 19:34:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53726 invoked by uid 99); 15 Aug 2014 19:33:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:33:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:33:34 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so2717003wes.11
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 12:33:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=LE12Nwl3isd5zkWlFqyB3Y+Jv+RJ5l7NTKByGxIG778=;
        b=eUWTp5dnrlgEBpHjGSW7D1TmnJpgi2FVkGaJMzWcSRDyPDMaRALZGVWfDsSHiuJqzr
         giG3uhD62w1aZsfWMjaGbRZtuHA94akUwBcaK6f5CMvsJk1liV4Km2ONXIomijdrFqqT
         998L5YYjXzv3rzw83oTvhDabt5qAOvfXvICE5xDMTc97dy4Oq/JHLOlrUnsOcvW+z5q4
         tIADkNqEchOWrIEswkHcVlgZRqce0HmUoOdBax3u3qVot6t+yGRY9JDPuGnd/AfuUO6A
         1KqLFEW+mwtNUyqWXhcFt8Da1CMY9yBXMG2DLXz988B/IKnnAtxiKYaYDUyP6cKLFGDg
         zY7Q==
X-Gm-Message-State: ALoCoQkVjS4OybDD9chPrxv+GAOX2kiHTLnYeceZEL61T6HXcHZQcs0szS1e24iMdGVsoetL9NHR
MIME-Version: 1.0
X-Received: by 10.180.20.40 with SMTP id k8mr23505917wie.54.1408131213551;
 Fri, 15 Aug 2014 12:33:33 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.108.198 with HTTP; Fri, 15 Aug 2014 12:33:33 -0700 (PDT)
In-Reply-To: <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
	<CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
	<CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
Date: Fri, 15 Aug 2014 12:33:33 -0700
Message-ID: <CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
Subject: Re: Tests failing
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, Patrick Wendell <pwendell@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec53d57afc801ac0500b018d9
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d57afc801ac0500b018d9
Content-Type: text/plain; charset=UTF-8

Jenkins runs for this PR https://github.com/apache/spark/pull/1960 timed
out without notification. The relevant Jenkins logs are at

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull


On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Shivaram,
>
> Can you point us to an example of that happening? The Jenkins console
> output, that is.
>
> Nick
>
>
> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
> shivaram@eecs.berkeley.edu> wrote:
>
>> Also I think Jenkins doesn't post build timeouts to github. Is there
>> anyway
>> we can fix that ?
>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>
>> > Hi All,
>> >
>> > I noticed that all PR tests run overnight had failed due to timeouts.
>> The
>> > patch that updates the netty shuffle I believe somehow inflated to the
>> > build time significantly. That patch had been tested, but one change was
>> > made before it was merged that was not tested.
>> >
>> > I've reverted the patch for now to see if it brings the build times back
>> > down.
>> >
>> > - Patrick
>> >
>>
>
>

--bcaec53d57afc801ac0500b018d9--

From dev-return-8914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 19:42:15 2014
Return-Path: <dev-return-8914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20C28119B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 19:42:15 +0000 (UTC)
Received: (qmail 72315 invoked by uid 500); 15 Aug 2014 19:42:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72251 invoked by uid 500); 15 Aug 2014 19:42:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72239 invoked by uid 99); 15 Aug 2014 19:42:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:42:14 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 19:41:48 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XINNj-0006By-JP
	for dev@spark.incubator.apache.org; Fri, 15 Aug 2014 12:41:47 -0700
Date: Fri, 15 Aug 2014 12:41:47 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408131707593-7877.post@n3.nabble.com>
In-Reply-To: <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
References: <1408115914518-7865.post@n3.nabble.com> <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com> <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Setting spark.driver.memory has no effect. It's still hanging trying to
compute result.count when I'm sampling greater than 35% regardless of what
value of spark.driver.memory I'm setting.

Here's my settings:
export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g"
export SPARK_MEM=10g

in conf/spark-defaults:
spark.driver.memory 1500
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.mb 500
spark.executor.memory 58315m
spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
spark.executor.extraClassPath /root/ephemeral-hdfs/conf



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7877.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 20:32:12 2014
Return-Path: <dev-return-8915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4593011BC5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 20:32:12 +0000 (UTC)
Received: (qmail 871 invoked by uid 500); 15 Aug 2014 20:32:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 806 invoked by uid 500); 15 Aug 2014 20:32:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 793 invoked by uid 99); 15 Aug 2014 20:32:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:32:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:32:07 +0000
Received: by mail-we0-f182.google.com with SMTP id k48so2798264wev.27
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 13:31:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=xjbMJa8iUKEnayaDioxbKTMtqweyXAyzbWquDwjpotM=;
        b=RAkFGQ1wz9VoozL9C0ThJCV/uVP5KvX6TkuSn1Vhv7PIK9Yyx8b1yJkPwntTOSAtTr
         YOP5J8t3FzL5e0bDCnzk0kh0tXUwm5uKeGtcCETv8oDhUVpWahKcDz8y57DAsdCPiGTj
         RhjDm9VwTZPzMXQyYy0wAEHQ443RTzwZ7RRUB63Z3p7usYIXBGMjW5bFy84YjWcv8RtY
         AOKPwNnjiCbJZxODW0YWTtZKOIodSJxrti8sBl3PIEOPiaW/p67FtFEt8HsjqEW3M8nO
         1ALR97x3EM49NGcwcdW/KCoBfA1R1gIzeDsNDaTuBeJZPX7jtrH5Qkq+5pdT64S2FrQ2
         J+rg==
X-Received: by 10.180.210.172 with SMTP id mv12mr169515wic.45.1408134706001;
 Fri, 15 Aug 2014 13:31:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 15 Aug 2014 13:31:05 -0700 (PDT)
In-Reply-To: <CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
 <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
 <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com> <CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 15 Aug 2014 16:31:05 -0400
Message-ID: <CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com>
Subject: Re: Tests failing
To: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37edef27c1b0500b0e831
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37edef27c1b0500b0e831
Content-Type: text/plain; charset=UTF-8

OK, I've captured this in SPARK-3076
<https://issues.apache.org/jira/browse/SPARK-3076>.

Patrick,

Is the problem that this run-tests
<https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151>
step
times out, and that is currently not handled gracefully? To be more
specific, it hangs for 120 minutes, times out, but the parent script for
some reason is also terminated. Does that sound right?

Nick


On Fri, Aug 15, 2014 at 3:33 PM, Shivaram Venkataraman <
shivaram@eecs.berkeley.edu> wrote:

> Jenkins runs for this PR https://github.com/apache/spark/pull/1960 timed
> out without notification. The relevant Jenkins logs are at
>
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull
>
>
> On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> Shivaram,
>>
>> Can you point us to an example of that happening? The Jenkins console
>> output, that is.
>>
>> Nick
>>
>>
>> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
>> shivaram@eecs.berkeley.edu> wrote:
>>
>>> Also I think Jenkins doesn't post build timeouts to github. Is there
>>> anyway
>>> we can fix that ?
>>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>>
>>> > Hi All,
>>> >
>>> > I noticed that all PR tests run overnight had failed due to timeouts.
>>> The
>>> > patch that updates the netty shuffle I believe somehow inflated to the
>>> > build time significantly. That patch had been tested, but one change
>>> was
>>> > made before it was merged that was not tested.
>>> >
>>> > I've reverted the patch for now to see if it brings the build times
>>> back
>>> > down.
>>> >
>>> > - Patrick
>>> >
>>>
>>
>>
>

--001a11c37edef27c1b0500b0e831--

From dev-return-8916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 20:43:51 2014
Return-Path: <dev-return-8916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59B8311C54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 20:43:51 +0000 (UTC)
Received: (qmail 42334 invoked by uid 500); 15 Aug 2014 20:43:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42272 invoked by uid 500); 15 Aug 2014 20:43:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42260 invoked by uid 99); 15 Aug 2014 20:43:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:43:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:43:46 +0000
Received: by mail-ob0-f178.google.com with SMTP id va2so412072obc.37
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 13:43:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mA6z9YOT4xnaTgCFXxKbPaUbDCg97PUCrqup7rOtdbc=;
        b=shtSEcpl31Eox6kZhh+LJj/qsCvMFSTJ88IehFcgA8jP/WkWhqyTFNbn4hA4p12pTg
         GR6uhB/IxpFbNJXHrBagzEAkCGC9p0PC/JEzewkOxrEANB/QPxctOi4FjeuO0P7A1ybi
         LC97BjWJnWt8nuLcEJH0KcTmnKt8GhcMzAN5CV0lxcrUL6ERg39sH2EEDuCB6Hv0mCUx
         1zFUFFU4+26NRMe8e7JqDfUCZo/IeTLea1E/xv2IwOQur+uISXugDkMaQ3WJ8+cUSVY0
         Qb2vEZMHmIDwl93lnSpjqGgrPEwkvN1U/TKbmZ45hA9I0n+8A7Eid10okygmuCM2SpX4
         HH0Q==
MIME-Version: 1.0
X-Received: by 10.60.83.134 with SMTP id q6mr22508042oey.46.1408135405652;
 Fri, 15 Aug 2014 13:43:25 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 15 Aug 2014 13:43:25 -0700 (PDT)
In-Reply-To: <CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
	<CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
	<CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
	<CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
	<CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com>
Date: Fri, 15 Aug 2014 13:43:25 -0700
Message-ID: <CABPQxsv=hq32RorJ_dqAruMECMHGjerpzqEPy0s64JKpESbNWQ@mail.gmail.com>
Subject: Re: Tests failing
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115fca2a655df0500b1124f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115fca2a655df0500b1124f
Content-Type: text/plain; charset=ISO-8859-1

Hey Nicholas,

Yeah so Jenkins has it's own timeout mechanism and it will just kill the
entire build after 120 minutes. But since run-tests is sitting in the
middle of the tests, it can't actually post a failure message.

I think run-tests-jenkins should just wrap the call to run-tests in a call
in its own timeout. It might be possible to just use this:

http://linux.die.net/man/1/timeout

- Patrick


On Fri, Aug 15, 2014 at 1:31 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> OK, I've captured this in SPARK-3076
> <https://issues.apache.org/jira/browse/SPARK-3076>.
>
> Patrick,
>
> Is the problem that this run-tests
> <https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151> step
> times out, and that is currently not handled gracefully? To be more
> specific, it hangs for 120 minutes, times out, but the parent script for
> some reason is also terminated. Does that sound right?
>
> Nick
>
>
> On Fri, Aug 15, 2014 at 3:33 PM, Shivaram Venkataraman <
> shivaram@eecs.berkeley.edu> wrote:
>
>> Jenkins runs for this PR https://github.com/apache/spark/pull/1960 timed
>> out without notification. The relevant Jenkins logs are at
>>
>>
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
>>
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
>>
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull
>>
>>
>> On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> Shivaram,
>>>
>>> Can you point us to an example of that happening? The Jenkins console
>>> output, that is.
>>>
>>> Nick
>>>
>>>
>>> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
>>> shivaram@eecs.berkeley.edu> wrote:
>>>
>>>> Also I think Jenkins doesn't post build timeouts to github. Is there
>>>> anyway
>>>> we can fix that ?
>>>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>>>
>>>> > Hi All,
>>>> >
>>>> > I noticed that all PR tests run overnight had failed due to timeouts.
>>>> The
>>>> > patch that updates the netty shuffle I believe somehow inflated to the
>>>> > build time significantly. That patch had been tested, but one change
>>>> was
>>>> > made before it was merged that was not tested.
>>>> >
>>>> > I've reverted the patch for now to see if it brings the build times
>>>> back
>>>> > down.
>>>> >
>>>> > - Patrick
>>>> >
>>>>
>>>
>>>
>>
>

--089e0115fca2a655df0500b1124f--

From dev-return-8917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 20:56:53 2014
Return-Path: <dev-return-8917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E010711CDB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 20:56:53 +0000 (UTC)
Received: (qmail 75574 invoked by uid 500); 15 Aug 2014 20:56:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75506 invoked by uid 500); 15 Aug 2014 20:56:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75469 invoked by uid 99); 15 Aug 2014 20:56:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:56:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 20:56:49 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so2772355wes.39
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 13:56:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=lTuigwkeHX7rA/h1ZWWH4Jf3MsrcCz3e5MBQFnLamwU=;
        b=BzOJFMwXlE1EkQqDvpPj2xl24g2B+t6M+CS86iAWIxt3Z41easTur+mMu0oKNT4IYw
         WaStUz3ADQD/+IILWsZl0ohZtHG3H5ib9BBoXYFZAPjZR8vstwTDLU1kzECqwKZKsJUA
         cerFOYzXrhPfB1xj0USi4XI9UiEa7ARrSSqSxHQobzyl2TIhOP8MODZ8ZcSZawY9YFcy
         nWop2KmSkirnYGbN8CD9epOwTMU+rRPnEFZTBTMtqJBE0FbubN/q/2N6va9r/JDLqODn
         IzZM4pGw9XdUYULBVHTxw9JjNdadCHj8aUTsv5pK7M53KWeBX5s5J886sF1IPkPqcbbo
         aQmQ==
X-Received: by 10.194.71.210 with SMTP id x18mr23026689wju.6.1408136187921;
 Fri, 15 Aug 2014 13:56:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 15 Aug 2014 13:55:46 -0700 (PDT)
In-Reply-To: <CABPQxsv=hq32RorJ_dqAruMECMHGjerpzqEPy0s64JKpESbNWQ@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
 <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
 <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
 <CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
 <CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com> <CABPQxsv=hq32RorJ_dqAruMECMHGjerpzqEPy0s64JKpESbNWQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 15 Aug 2014 16:55:46 -0400
Message-ID: <CAOhmDzdSj5iBfVi8bb++n88UPog-ZAc0p5kZ=bZCuVvmtQCcuQ@mail.gmail.com>
Subject: Re: Tests failing
To: Patrick Wendell <pwendell@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd077646cb280500b1415d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd077646cb280500b1415d
Content-Type: text/plain; charset=UTF-8

So 2 hours is a hard cap on how long a build can run. Okie doke.

Perhaps then I'll wrap the run-tests step as you suggest and limit it to
100 minutes or something, and cleanly report if it times out.

Sound good?


On Fri, Aug 15, 2014 at 4:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Nicholas,
>
> Yeah so Jenkins has it's own timeout mechanism and it will just kill the
> entire build after 120 minutes. But since run-tests is sitting in the
> middle of the tests, it can't actually post a failure message.
>
> I think run-tests-jenkins should just wrap the call to run-tests in a call
> in its own timeout. It might be possible to just use this:
>
> http://linux.die.net/man/1/timeout
>
> - Patrick
>
>
> On Fri, Aug 15, 2014 at 1:31 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> OK, I've captured this in SPARK-3076
>> <https://issues.apache.org/jira/browse/SPARK-3076>.
>>
>> Patrick,
>>
>> Is the problem that this run-tests
>> <https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151> step
>> times out, and that is currently not handled gracefully? To be more
>> specific, it hangs for 120 minutes, times out, but the parent script for
>> some reason is also terminated. Does that sound right?
>>
>> Nick
>>
>>
>> On Fri, Aug 15, 2014 at 3:33 PM, Shivaram Venkataraman <
>> shivaram@eecs.berkeley.edu> wrote:
>>
>>> Jenkins runs for this PR https://github.com/apache/spark/pull/1960
>>> timed out without notification. The relevant Jenkins logs are at
>>>
>>>
>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
>>>
>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
>>>
>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull
>>>
>>>
>>> On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> Shivaram,
>>>>
>>>> Can you point us to an example of that happening? The Jenkins console
>>>> output, that is.
>>>>
>>>> Nick
>>>>
>>>>
>>>> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
>>>> shivaram@eecs.berkeley.edu> wrote:
>>>>
>>>>> Also I think Jenkins doesn't post build timeouts to github. Is there
>>>>> anyway
>>>>> we can fix that ?
>>>>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>>>>
>>>>> > Hi All,
>>>>> >
>>>>> > I noticed that all PR tests run overnight had failed due to
>>>>> timeouts. The
>>>>> > patch that updates the netty shuffle I believe somehow inflated to
>>>>> the
>>>>> > build time significantly. That patch had been tested, but one change
>>>>> was
>>>>> > made before it was merged that was not tested.
>>>>> >
>>>>> > I've reverted the patch for now to see if it brings the build times
>>>>> back
>>>>> > down.
>>>>> >
>>>>> > - Patrick
>>>>> >
>>>>>
>>>>
>>>>
>>>
>>
>

--047d7bfd077646cb280500b1415d--

From dev-return-8918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 21:05:17 2014
Return-Path: <dev-return-8918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7666311D2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 21:05:17 +0000 (UTC)
Received: (qmail 93256 invoked by uid 500); 15 Aug 2014 21:05:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93188 invoked by uid 500); 15 Aug 2014 21:05:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93175 invoked by uid 99); 15 Aug 2014 21:05:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 21:05:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 21:04:45 +0000
Received: by mail-oi0-f43.google.com with SMTP id u20so2053965oif.2
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 14:04:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rFvYAXTqwUU7hLz3z6+GjwGdsSesIl3qSCEroKTLO1Y=;
        b=lc8syrKmuQnJZ1e75MoZEn1ulGdAPfpc0AJ7QMr7wVnvf81pUN7jbovJzJW34Jit+C
         /mAyYa+cYDOny6qDS9RRKTdAIKjWTyneCu6R33xKwY2gwuJeyE48cmGI1NrPrl6QAVMJ
         PAbBHg5BFGwQfqIvb+kBuoHwvV2DOAyIF+UWXcxglN54EELL5dr70z6GvsaaPkT8t+bO
         4xOPqABblbg4n7H5IO2yfHyXmYUUZ5bEd7l1iXmst7Ge5YPawKo4xh5K5XspdTvqsOx0
         G7WEN50ByobIZFAN5BVEqRTEhAB7j9eRm14x7jeEt274znu3Tayh1Qkd7ba14ozkQtGD
         wI+w==
MIME-Version: 1.0
X-Received: by 10.182.249.52 with SMTP id yr20mr22494766obc.10.1408136684078;
 Fri, 15 Aug 2014 14:04:44 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 15 Aug 2014 14:04:44 -0700 (PDT)
In-Reply-To: <CAOhmDzdSj5iBfVi8bb++n88UPog-ZAc0p5kZ=bZCuVvmtQCcuQ@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
	<CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
	<CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
	<CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
	<CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com>
	<CABPQxsv=hq32RorJ_dqAruMECMHGjerpzqEPy0s64JKpESbNWQ@mail.gmail.com>
	<CAOhmDzdSj5iBfVi8bb++n88UPog-ZAc0p5kZ=bZCuVvmtQCcuQ@mail.gmail.com>
Date: Fri, 15 Aug 2014 14:04:44 -0700
Message-ID: <CABPQxst8oO1muKA6Vif+svZZngo+fjPr8An0xQ9OBM3zV=PaCw@mail.gmail.com>
Subject: Re: Tests failing
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f92410ed98e4c0500b15e74
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f92410ed98e4c0500b15e74
Content-Type: text/plain; charset=ISO-8859-1

Yeah I was thinking something like that. Basically we should just have a
variable for the timeout and I can make sure it's under the configured
Jenkins time.


On Fri, Aug 15, 2014 at 1:55 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> So 2 hours is a hard cap on how long a build can run. Okie doke.
>
> Perhaps then I'll wrap the run-tests step as you suggest and limit it to
> 100 minutes or something, and cleanly report if it times out.
>
> Sound good?
>
>
> On Fri, Aug 15, 2014 at 4:43 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey Nicholas,
>>
>> Yeah so Jenkins has it's own timeout mechanism and it will just kill the
>> entire build after 120 minutes. But since run-tests is sitting in the
>> middle of the tests, it can't actually post a failure message.
>>
>> I think run-tests-jenkins should just wrap the call to run-tests in a
>> call in its own timeout. It might be possible to just use this:
>>
>> http://linux.die.net/man/1/timeout
>>
>> - Patrick
>>
>>
>> On Fri, Aug 15, 2014 at 1:31 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> OK, I've captured this in SPARK-3076
>>> <https://issues.apache.org/jira/browse/SPARK-3076>.
>>>
>>> Patrick,
>>>
>>> Is the problem that this run-tests
>>> <https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151> step
>>> times out, and that is currently not handled gracefully? To be more
>>> specific, it hangs for 120 minutes, times out, but the parent script for
>>> some reason is also terminated. Does that sound right?
>>>
>>> Nick
>>>
>>>
>>> On Fri, Aug 15, 2014 at 3:33 PM, Shivaram Venkataraman <
>>> shivaram@eecs.berkeley.edu> wrote:
>>>
>>>> Jenkins runs for this PR https://github.com/apache/spark/pull/1960
>>>> timed out without notification. The relevant Jenkins logs are at
>>>>
>>>>
>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
>>>>
>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
>>>>
>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull
>>>>
>>>>
>>>> On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
>>>> nicholas.chammas@gmail.com> wrote:
>>>>
>>>>> Shivaram,
>>>>>
>>>>> Can you point us to an example of that happening? The Jenkins console
>>>>> output, that is.
>>>>>
>>>>> Nick
>>>>>
>>>>>
>>>>> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
>>>>> shivaram@eecs.berkeley.edu> wrote:
>>>>>
>>>>>> Also I think Jenkins doesn't post build timeouts to github. Is there
>>>>>> anyway
>>>>>> we can fix that ?
>>>>>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>> > Hi All,
>>>>>> >
>>>>>> > I noticed that all PR tests run overnight had failed due to
>>>>>> timeouts. The
>>>>>> > patch that updates the netty shuffle I believe somehow inflated to
>>>>>> the
>>>>>> > build time significantly. That patch had been tested, but one
>>>>>> change was
>>>>>> > made before it was merged that was not tested.
>>>>>> >
>>>>>> > I've reverted the patch for now to see if it brings the build times
>>>>>> back
>>>>>> > down.
>>>>>> >
>>>>>> > - Patrick
>>>>>> >
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--e89a8f92410ed98e4c0500b15e74--

From dev-return-8919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 22:06:11 2014
Return-Path: <dev-return-8919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8047411F48
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 22:06:11 +0000 (UTC)
Received: (qmail 42077 invoked by uid 500); 15 Aug 2014 22:06:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42007 invoked by uid 500); 15 Aug 2014 22:06:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41994 invoked by uid 99); 15 Aug 2014 22:06:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:06:10 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:05:43 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so2732251qgf.35
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 15:05:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=DEUmLw3+VE6eE7BgmqjBPtlzWOOMzg1SCGiBRT3oFJ4=;
        b=zLx7liXWSuz25/DndHmwJPKDaMhysd/iVs1dUmGrhX2INfOVRepDa/66JoFg1WmdBs
         HKyanFajaOSep23ZRiI51XRUSREr7Abs0CEElANL5MW3Ka+tEXfrFi3siSR5M5CJZfYh
         nN1e2SMRC7wCvTa9hKGUpjiKZcn91ySctKW+YpswHEhba4+v+SdVxvGBamYw1QKtq/Fx
         I3WBeMGcrC7gw9+GVTr/0VUt4Sac6Pxjtws8+LdgXxxWxSh3dsifuP56omesy+lbE/+z
         xKu9Zd6w2qFYF+cnQ7SgOFeaQNo3X/0mdzp4nKINIot0HcOv6I36+SRqazjz37slKzwo
         K7xw==
MIME-Version: 1.0
X-Received: by 10.229.231.68 with SMTP id jp4mr32443342qcb.4.1408140341888;
 Fri, 15 Aug 2014 15:05:41 -0700 (PDT)
Received: by 10.140.83.177 with HTTP; Fri, 15 Aug 2014 15:05:41 -0700 (PDT)
In-Reply-To: <1406237435217.8dcdb73e@Nodemailer>
References: <CABPQxsth1osuj5A-mwV8wSSOfi8xv5hyKkNBBE1cPjazxiMZDw@mail.gmail.com>
	<1406237435217.8dcdb73e@Nodemailer>
Date: Fri, 15 Aug 2014 23:05:41 +0100
Message-ID: <CAKqT-W0heMdHjdGmu+wb8K_mafKsPrR0Z0hKTD9WS5zg5fy2jg@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Neil Ferguson <nferguson@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134497adf4d280500b238f5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134497adf4d280500b238f5
Content-Type: text/plain; charset=UTF-8

I've opened SPARK-3051 (https://issues.apache.org/jira/browse/SPARK-3051)
based on this thread.

Neil


On Thu, Jul 24, 2014 at 10:30 PM, Neil Ferguson <nferguson@gmail.com> wrote:

> That would work well for me! Do you think it would be necessary to specify
> which accumulators should be available in the registry, or would we just
> broadcast all named accumulators registered in SparkContext and make them
> available in the registry?
>
> Anyway, I'm happy to make the necessary changes (unless someone else wants
> to).
>
>
> On Thu, Jul 24, 2014 at 10:17 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> What if we have a registry for accumulators, where you can access them
>> statically by name?
>>
>> - Patrick
>>
>> On Thu, Jul 24, 2014 at 1:51 PM, Neil Ferguson <nferguson@gmail.com>
>> wrote:
>> > I realised that my last reply wasn't very clear -- let me try and
>> clarify.
>> >
>> > The patch for named accumulators looks very useful, however in
>> Shivaram's
>> > example he was able to retrieve the named task metrics (statically)
>> from a
>> > TaskMetrics object, as follows:
>> >
>> > TaskMetrics.get("f1-time")
>> >
>> > However, I don't think this would be possible with the named
>> accumulators
>> > -- I believe they'd need to be passed to every function that needs
>> them,
>> > which I think would be cumbersome in any application of reasonable
>> > complexity.
>> >
>> > This is what I was trying to solve with my proposal for dynamic
>> variables
>> > in Spark. However, the ability to retrieve named accumulators from a
>> > thread-local would work just as well for my use case. I'd be happy to
>> > implement either solution if there's interest.
>> >
>> > Alternatively, if I'm missing some other way to accomplish this please
>> let
>> > me know.
>> >
>> > On a (slight) aside, I now think it would be possible to implement
>> dynamic
>> > variables by broadcasting them. I was looking at Reynold's PR [1] to
>> > broadcast the RDD object, and I think it would be possible to take a
>> > similar approach -- that is, broadcast the serialized form, and
>> deserialize
>> > when executing each task.
>> >
>> > [1] https://github.com/apache/spark/pull/1498
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > On Wed, Jul 23, 2014 at 8:30 AM, Neil Ferguson <nferguson@gmail.com>
>> wrote:
>> >
>> >> Hi Patrick.
>> >>
>> >> That looks very useful. The thing that seems to be missing from
>> Shivaram's
>> >> example is the ability to access TaskMetrics statically (this is the
>> same
>> >> problem that I am trying to solve with dynamic variables).
>> >>
>> >>
>> >>
>> >> You mention defining an accumulator on the RDD. Perhaps I am missing
>> >> something here, but my understanding was that accumulators are defined
>> in
>> >> SparkContext and are not part of the RDD. Is that correct?
>> >>
>> >> Neil
>> >>
>> >> On Tue, Jul 22, 2014 at 22:21, Patrick Wendell <pwendell@gmail.com
>> >> ="mailto:pwendell@gmail.com">> wrote:
>> >>
>> >>> Shivaram,
>> >>>
>> >>> You should take a look at this patch which adds support for naming
>> >>> accumulators - this is likely to get merged in soon. I actually
>> >>> started this patch by supporting named TaskMetrics similar to what
>> you
>> >>> have there, but then I realized there is too much semantic overlap
>> >>> with accumulators, so I just went that route.
>> >>>
>> >>> For instance, it would be nice if any user-defined metrics are
>> >>> accessible at the driver program.
>> >>>
>> >>> https://github.com/apache/spark/pull/1309
>> >>>
>> >>> In your example, you could just define an accumulator here on the RDD
>> >>> and you'd see the incremental update in the web UI automatically.
>> >>>
>> >>> - Patrick
>> >>>
>> >>> On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman
>> >>> <shivaram@eecs.berkeley.edu> wrote:
>> >>> > From reading Neil's first e-mail, I think the motivation is to get
>> some
>> >>> > metrics in ADAM ? -- I've run into a similar use-case with having
>> >>> > user-defined metrics in long-running tasks and I think a nice way
>> to
>> >>> solve
>> >>> > this would be to have user-defined TaskMetrics.
>> >>> >
>> >>> > To state my problem more clearly, lets say you have two functions
>> you
>> >>> use
>> >>> > in a map call and want to measure how much time each of them takes.
>> For
>> >>> > example, if you have a code block like the one below and you want
>> to
>> >>> > measure how much time f1 takes as a fraction of the task.
>> >>> >
>> >>> > a.map { l =>
>> >>> > val f = f1(l)
>> >>> > ... some work here ...
>> >>> > }
>> >>> >
>> >>> > It would be really cool if we could do something like
>> >>> >
>> >>> > a.map { l =>
>> >>> > val start = System.nanoTime
>> >>> > val f = f1(l)
>> >>> > TaskMetrics.get("f1-time").add(System.nanoTime - start)
>> >>> > }
>> >>> >
>> >>> > These task metrics have a different purpose from Accumulators in
>> the
>> >>> sense
>> >>> > that we don't need to track lineage, perform commutative operations
>> >>> etc.
>> >>> > Further we also have a bunch of code in place to aggregate task
>> metrics
>> >>> > across a stage etc. So it would be great if we could also populate
>> >>> these in
>> >>> > the UI and show median/max etc.
>> >>> > I think counters [1] in Hadoop served a similar purpose.
>> >>> >
>> >>> > Thanks
>> >>> > Shivaram
>> >>> >
>> >>> > [1]
>> >>> >
>> >>>
>> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters
>> >>> >
>> >>> >
>> >>> >
>> >>> > On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com>
>>
>> >>> wrote:
>> >>> >
>> >>> >> Hi Reynold
>> >>> >>
>> >>> >> Thanks for your reply.
>> >>> >>
>> >>> >> Accumulators are, of course, stored in the Accumulators object as
>> >>> >> thread-local variables. However, the Accumulators object isn't
>> public,
>> >>> so
>> >>> >> when a Task is executing there's no way to get the set of
>> accumulators
>> >>> for
>> >>> >> the current thread -- accumulators still have to be passed to
>> every
>> >>> method
>> >>> >> that needs them.
>> >>> >>
>> >>> >> Additionally, unless an accumulator is explicitly referenced it
>> won't
>> >>> be
>> >>> >> serialized as part of a Task, and won't make it into the
>> Accumulators
>> >>> >> object in the first place.
>> >>> >>
>> >>> >> I should also note that what I'm proposing is not specific to
>> >>> Accumulators
>> >>> >> -- I am proposing that any data can be stored in a thread-local
>> >>> variable. I
>> >>> >> think there are probably many other use cases other than my one.
>> >>> >>
>> >>> >> Neil
>> >>> >>
>> >>> >>
>> >>> >> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com>
>>
>> >>> wrote:
>> >>> >>
>> >>> >> > Thanks for the thoughtful email, Neil and Christopher.
>> >>> >> >
>> >>> >> > If I understand this correctly, it seems like the dynamic
>> variable
>> >>> is
>> >>> >> just
>> >>> >> > a variant of the accumulator (a static one since it is a global
>> >>> object).
>> >>> >> > Accumulators are already implemented using thread-local
>> variables
>> >>> under
>> >>> >> the
>> >>> >> > hood. Am I misunderstanding something?
>> >>> >> >
>> >>> >> >
>> >>> >> >
>> >>> >> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <
>> ctn@adatao.com>
>> >>>
>> >>> >> > wrote:
>> >>> >> >
>> >>> >> > > Hi Neil, first off, I'm generally a sympathetic advocate for
>> >>> making
>> >>> >> > changes
>> >>> >> > > to Spark internals to make it easier/better/faster/more
>> awesome.
>> >>> >> > >
>> >>> >> > > In this case, I'm (a) not clear about what you're trying to
>> >>> accomplish,
>> >>> >> > and
>> >>> >> > > (b) a bit worried about the proposed solution.
>> >>> >> > >
>> >>> >> > > On (a): it is stated that you want to pass some Accumulators
>> >>> around.
>> >>> >> Yet
>> >>> >> > > the proposed solution is for some "shared" variable that may
>> be
>> >>> set and
>> >>> >> > > "mapped out" and possibly "reduced back", but without any
>> >>> accompanying
>> >>> >> > > accumulation semantics. And yet it doesn't seem like you only
>> want
>> >>> just
>> >>> >> > the
>> >>> >> > > broadcast property. Can you clarify the problem statement with
>> >>> some
>> >>> >> > > before/after client code examples?
>> >>> >> > >
>> >>> >> > > On (b): you're right that adding variables to SparkContext
>> should
>> >>> be
>> >>> >> done
>> >>> >> > > with caution, as it may have unintended consequences beyond
>> just
>> >>> serdes
>> >>> >> > > payload size. For example, there is a stated intention of
>> >>> supporting
>> >>> >> > > multiple SparkContexts in the future, and this proposed
>> solution
>> >>> can
>> >>> >> make
>> >>> >> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching
>> >>> call to
>> >>> >> > make
>> >>> >> > > a while back on a subject related to this (see
>> >>> >> > > https://github.com/mesos/spark/pull/779). Furthermore, even
>> in a
>> >>> >> single
>> >>> >> > > SparkContext application, there may be multiple "clients" (of
>> that
>> >>> >> > > application) whose intent to use the proposed "SparkDynamic"
>> would
>> >>> not
>> >>> >> > > necessarily be coordinated.
>> >>> >> > >
>> >>> >> > > So, considering a ratio of a/b (benefit/cost), it's not clear
>> to
>> >>> me
>> >>> >> that
>> >>> >> > > the benefits are significant enough to warrant the costs. Do I
>> >>> >> > > misunderstand that the benefit is to save one explicit
>> parameter
>> >>> (the
>> >>> >> > > "context") in the signature/closure code?
>> >>> >> > >
>> >>> >> > > --
>> >>> >> > > Christopher T. Nguyen
>> >>> >> > > Co-founder & CEO, Adatao <http://adatao.com>
>> >>> >> > > linkedin.com/in/ctnguyen
>> >>> >> > >
>> >>> >> > >
>> >>> >> > >
>> >>> >> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <
>> >>> nferguson@gmail.com>
>> >>> >> > > wrote:
>> >>> >> > >
>> >>> >> > > > Hi all
>> >>> >> > > >
>> >>> >> > > > I have been adding some metrics to the ADAM project
>> >>> >> > > > https://github.com/bigdatagenomics/adam, which runs on
>> Spark,
>> >>> and
>> >>> >> > have a
>> >>> >> > > > proposal for an enhancement to Spark that would make this
>> work
>> >>> >> cleaner
>> >>> >> > > and
>> >>> >> > > > easier.
>> >>> >> > > >
>> >>> >> > > > I need to pass some Accumulators around, which will
>> aggregate
>> >>> metrics
>> >>> >> > > > (timing stats and other metrics) across the cluster.
>> However, it
>> >>> is
>> >>> >> > > > cumbersome to have to explicitly pass some "context"
>> containing
>> >>> these
>> >>> >> > > > accumulators around everywhere that might need them. I can
>> use
>> >>> Scala
>> >>> >> > > > implicits, which help slightly, but I'd still need to modify
>> >>> every
>> >>> >> > method
>> >>> >> > > > in the call stack to take an implicit variable.
>> >>> >> > > >
>> >>> >> > > > So, I'd like to propose that we add the ability to have
>> "dynamic
>> >>> >> > > variables"
>> >>> >> > > > (basically thread-local variables) to Spark. This would
>> avoid
>> >>> having
>> >>> >> to
>> >>> >> > > > pass the Accumulators around explicitly.
>> >>> >> > > >
>> >>> >> > > > My proposed approach is to add a method to the SparkContext
>> >>> class as
>> >>> >> > > > follows:
>> >>> >> > > >
>> >>> >> > > > /**
>> >>> >> > > > * Sets the value of a "dynamic variable". This value is made
>> >>> >> available
>> >>> >> > > to
>> >>> >> > > > jobs
>> >>> >> > > > * without having to be passed around explicitly. During
>> >>> execution
>> >>> >> of a
>> >>> >> > > > Spark job
>> >>> >> > > > * this value can be obtained from the [[SparkDynamic]]
>> object.
>> >>> >> > > > */
>> >>> >> > > > def setDynamicVariableValue(value: Any)
>> >>> >> > > >
>> >>> >> > > > Then, when a job is executing the SparkDynamic can be
>> accessed
>> >>> to
>> >>> >> > obtain
>> >>> >> > > > the value of the dynamic variable. The implementation of
>> this
>> >>> object
>> >>> >> is
>> >>> >> > > as
>> >>> >> > > > follows:
>> >>> >> > > >
>> >>> >> > > > object SparkDynamic {
>> >>> >> > > > private val dynamicVariable = new DynamicVariable[Any]()
>> >>> >> > > > /**
>> >>> >> > > > * Gets the value of the "dynamic variable" that has been set
>> in
>> >>> >> the
>> >>> >> > > > [[SparkContext]]
>> >>> >> > > > */
>> >>> >> > > > def getValue: Option[Any] = {
>> >>> >> > > > Option(dynamicVariable.value)
>> >>> >> > > > }
>> >>> >> > > > private[spark] def withValue[S](threadValue:
>> Option[Any])(thunk:
>> >>> =>
>> >>> >> > > S): S
>> >>> >> > > > = {
>> >>> >> > > > dynamicVariable.withValue(threadValue.orNull)(thunk)
>> >>> >> > > > }
>> >>> >> > > > }
>> >>> >> > > >
>> >>> >> > > > The change involves modifying the Task object to serialize
>> the
>> >>> value
>> >>> >> of
>> >>> >> > > the
>> >>> >> > > > dynamic variable, and modifying the TaskRunner class to
>> >>> deserialize
>> >>> >> the
>> >>> >> > > > value and make it available in the thread that is running
>> the
>> >>> task
>> >>> >> > (using
>> >>> >> > > > the SparkDynamic.withValue method).
>> >>> >> > > >
>> >>> >> > > > I have done a quick prototype of this in this commit:
>> >>> >> > > >
>> >>> >> > > >
>> >>> >> > >
>> >>> >> >
>> >>> >>
>> >>>
>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
>> >>> >> > > > and it seems to work fine in my (limited) testing. It needs
>> more
>> >>> >> > testing,
>> >>> >> > > > tidy-up and documentation though.
>> >>> >> > > >
>> >>> >> > > > One drawback is that the dynamic variable will be serialized
>> for
>> >>> >> every
>> >>> >> > > Task
>> >>> >> > > > whether it needs it or not. For my use case this might not
>> be
>> >>> too
>> >>> >> much
>> >>> >> > > of a
>> >>> >> > > > problem, as serializing and deserializing Accumulators looks
>> >>> fairly
>> >>> >> > > > lightweight -- however we should certainly warn users
>> against
>> >>> >> setting a
>> >>> >> > > > dynamic variable containing lots of data. I thought about
>> using
>> >>> >> > broadcast
>> >>> >> > > > tables here, but I don't think it's possible to put
>> Accumulators
>> >>> in a
>> >>> >> > > > broadcast table (as I understand it, they're intended for
>> purely
>> >>> >> > > read-only
>> >>> >> > > > data).
>> >>> >> > > >
>> >>> >> > > > What do people think about this proposal? My use case aside,
>> it
>> >>> seems
>> >>> >> > > like
>> >>> >> > > > it would be a generally useful enhancment to be able to pass
>> >>> certain
>> >>> >> > data
>> >>> >> > > > around without having to explicitly pass it everywhere.
>> >>> >> > > >
>> >>> >> > > > Neil
>> >>> >> > > >
>> >>> >> > >
>> >>> >> >
>> >>> >>
>> >>>
>> >>
>>
>
>

--001a1134497adf4d280500b238f5--

From dev-return-8920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 22:29:17 2014
Return-Path: <dev-return-8920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70CB011FF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 22:29:17 +0000 (UTC)
Received: (qmail 95701 invoked by uid 500); 15 Aug 2014 22:29:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95633 invoked by uid 500); 15 Aug 2014 22:29:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95614 invoked by uid 99); 15 Aug 2014 22:29:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:29:16 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:28:50 +0000
Received: by mail-ig0-f171.google.com with SMTP id l13so3260515iga.16
        for <dev@spark.incubator.apache.org>; Fri, 15 Aug 2014 15:28:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=76RjKyqM5bnFFaQtQ3ObYAyGQZe+YB1bAZxY4veFaYo=;
        b=IPpiPF0Mf6b6I2ftRAyIqS0oejccGVPQ/CDXhxKEXsJ26Yc/7qBatGSpkPE1/MfX0G
         Dw+HL8cbxCTQ34NuYERIMOK1hnA9mpiSi1WXwCCwb3uBQraAv2Gay3TPgxHUYLh0mN0P
         edxtHD/YVoJsU/vSJXEZY6IbD49UW2sBkAtZlBYizTYW7jSkT3LzpltCmSO3e5k9t/9g
         yLIDrt10LjE1f/fm87PdDh0BYWZUpvtL5WrIgl9QQwUgExQXSffJO/P9ygqgYaO7M5kf
         rs1OdwqR6KpJvquBT9N9vGSeTKdNmzk6NWKeOIK8DttSBjAfJjQSJY3w9TBagMAUS/Kj
         m4jw==
MIME-Version: 1.0
X-Received: by 10.50.111.112 with SMTP id ih16mr68651843igb.30.1408141729405;
 Fri, 15 Aug 2014 15:28:49 -0700 (PDT)
Received: by 10.107.152.196 with HTTP; Fri, 15 Aug 2014 15:28:49 -0700 (PDT)
In-Reply-To: <1408131707593-7877.post@n3.nabble.com>
References: <1408115914518-7865.post@n3.nabble.com>
	<CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
	<E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
	<1408131707593-7877.post@n3.nabble.com>
Date: Fri, 15 Aug 2014 15:28:49 -0700
Message-ID: <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Xiangrui Meng <mengxr@gmail.com>
To: jerryye <jerryye@gmail.com>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Did you verify the driver memory in the Executor tab of the WebUI? I
think you need `--driver-memory 8g` with spark-shell or spark-submit
instead of setting it in spark-defaults.conf.

On Fri, Aug 15, 2014 at 12:41 PM, jerryye <jerryye@gmail.com> wrote:
> Setting spark.driver.memory has no effect. It's still hanging trying to
> compute result.count when I'm sampling greater than 35% regardless of what
> value of spark.driver.memory I'm setting.
>
> Here's my settings:
> export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g"
> export SPARK_MEM=10g
>
> in conf/spark-defaults:
> spark.driver.memory 1500
> spark.serializer org.apache.spark.serializer.KryoSerializer
> spark.kryoserializer.buffer.mb 500
> spark.executor.memory 58315m
> spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
> spark.executor.extraClassPath /root/ephemeral-hdfs/conf
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7877.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 22:48:49 2014
Return-Path: <dev-return-8921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83772110A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 22:48:49 +0000 (UTC)
Received: (qmail 38393 invoked by uid 500); 15 Aug 2014 22:48:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38345 invoked by uid 500); 15 Aug 2014 22:48:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38329 invoked by uid 99); 15 Aug 2014 22:48:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:48:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 22:48:44 +0000
Received: by mail-wg0-f48.google.com with SMTP id x13so2816797wgg.31
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 15:48:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=J3GGhxfoz0q57/rgyvMZtRrpFDddHTBWeeOvBtUNg+o=;
        b=t23iqfEUC7CbjAzMhx2dKUhcqvAsGQ7Utz41X57aZNshcKK0CHcwXoilMRo4Q1K6hZ
         hLaK7QrjjdVDIsYd7NJaMz+mneCARcZJvuVPKHUSW+3MmS7bcV7iqizNK2UmCitltlmc
         UNF2niXPf8vGAXXzX1WdYljdxJ4LRnG1B6MaKxq7+yAdvQ4MwI5tBBImQzdFgu7++gq/
         MuDh/HTKDz81nw40pYsdJL/fQugSoveRv3hWsIocdWHMXExfBsyOXiAy4+YpXGwYT1WU
         GbvIW/RV2ZAIWDYRUrsr2cACEjI016YggM7tZTQOpCGqpYxbHXOw8iYfcwM2IA4BJ9Lw
         MnTQ==
X-Received: by 10.195.11.200 with SMTP id ek8mr24592182wjd.85.1408142903032;
 Fri, 15 Aug 2014 15:48:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 15 Aug 2014 15:47:42 -0700 (PDT)
In-Reply-To: <CABPQxst8oO1muKA6Vif+svZZngo+fjPr8An0xQ9OBM3zV=PaCw@mail.gmail.com>
References: <CABPQxsveD8bD_zx=MoQDECp85PTVLTePsdjzWZyU9KPR+ohENA@mail.gmail.com>
 <CAKx7Bf_SPTKqOAgvs4o+0BJ22jRAeGgMkTDM7WJNL7XiGumkdA@mail.gmail.com>
 <CAOhmDzeM8EeFujkAJrhnp0tRexu1AZ_0o7zVyw__xSLQnxZdSA@mail.gmail.com>
 <CAKx7Bf_9t23npYny+fhwHsu_BrupFn32Vn31ALj90rrFTEX4EQ@mail.gmail.com>
 <CAOhmDzfsjPPR=6M=5zUfO4CGQ-nf+yf8NdUXL9uJrnmRnZYPRw@mail.gmail.com>
 <CABPQxsv=hq32RorJ_dqAruMECMHGjerpzqEPy0s64JKpESbNWQ@mail.gmail.com>
 <CAOhmDzdSj5iBfVi8bb++n88UPog-ZAc0p5kZ=bZCuVvmtQCcuQ@mail.gmail.com> <CABPQxst8oO1muKA6Vif+svZZngo+fjPr8An0xQ9OBM3zV=PaCw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 15 Aug 2014 18:47:42 -0400
Message-ID: <CAOhmDzeCtfSJwShrWn61ZagvrPiZ1LP_wfm70wdef5vEj+a8fg@mail.gmail.com>
Subject: Re: Tests failing
To: Patrick Wendell <pwendell@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b8737628742830500b2d189
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b8737628742830500b2d189
Content-Type: text/plain; charset=UTF-8

*Bam. <https://github.com/apache/spark/pull/1974#issuecomment-52368527>*


On Fri, Aug 15, 2014 at 5:04 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Yeah I was thinking something like that. Basically we should just have a
> variable for the timeout and I can make sure it's under the configured
> Jenkins time.
>
>
> On Fri, Aug 15, 2014 at 1:55 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> So 2 hours is a hard cap on how long a build can run. Okie doke.
>>
>> Perhaps then I'll wrap the run-tests step as you suggest and limit it to
>> 100 minutes or something, and cleanly report if it times out.
>>
>> Sound good?
>>
>>
>> On Fri, Aug 15, 2014 at 4:43 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Hey Nicholas,
>>>
>>> Yeah so Jenkins has it's own timeout mechanism and it will just kill the
>>> entire build after 120 minutes. But since run-tests is sitting in the
>>> middle of the tests, it can't actually post a failure message.
>>>
>>> I think run-tests-jenkins should just wrap the call to run-tests in a
>>> call in its own timeout. It might be possible to just use this:
>>>
>>> http://linux.die.net/man/1/timeout
>>>
>>> - Patrick
>>>
>>>
>>> On Fri, Aug 15, 2014 at 1:31 PM, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> OK, I've captured this in SPARK-3076
>>>> <https://issues.apache.org/jira/browse/SPARK-3076>.
>>>>
>>>> Patrick,
>>>>
>>>> Is the problem that this run-tests
>>>> <https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151> step
>>>> times out, and that is currently not handled gracefully? To be more
>>>> specific, it hangs for 120 minutes, times out, but the parent script for
>>>> some reason is also terminated. Does that sound right?
>>>>
>>>> Nick
>>>>
>>>>
>>>> On Fri, Aug 15, 2014 at 3:33 PM, Shivaram Venkataraman <
>>>> shivaram@eecs.berkeley.edu> wrote:
>>>>
>>>>> Jenkins runs for this PR https://github.com/apache/spark/pull/1960
>>>>> timed out without notification. The relevant Jenkins logs are at
>>>>>
>>>>>
>>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
>>>>>
>>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
>>>>>
>>>>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull
>>>>>
>>>>>
>>>>> On Fri, Aug 15, 2014 at 11:44 AM, Nicholas Chammas <
>>>>> nicholas.chammas@gmail.com> wrote:
>>>>>
>>>>>> Shivaram,
>>>>>>
>>>>>> Can you point us to an example of that happening? The Jenkins console
>>>>>> output, that is.
>>>>>>
>>>>>> Nick
>>>>>>
>>>>>>
>>>>>> On Fri, Aug 15, 2014 at 2:28 PM, Shivaram Venkataraman <
>>>>>> shivaram@eecs.berkeley.edu> wrote:
>>>>>>
>>>>>>> Also I think Jenkins doesn't post build timeouts to github. Is there
>>>>>>> anyway
>>>>>>> we can fix that ?
>>>>>>> On Aug 15, 2014 9:04 AM, "Patrick Wendell" <pwendell@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>> > Hi All,
>>>>>>> >
>>>>>>> > I noticed that all PR tests run overnight had failed due to
>>>>>>> timeouts. The
>>>>>>> > patch that updates the netty shuffle I believe somehow inflated to
>>>>>>> the
>>>>>>> > build time significantly. That patch had been tested, but one
>>>>>>> change was
>>>>>>> > made before it was merged that was not tested.
>>>>>>> >
>>>>>>> > I've reverted the patch for now to see if it brings the build
>>>>>>> times back
>>>>>>> > down.
>>>>>>> >
>>>>>>> > - Patrick
>>>>>>> >
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7b8737628742830500b2d189--

From dev-return-8922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 15 23:07:52 2014
Return-Path: <dev-return-8922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 511FA11101
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Aug 2014 23:07:52 +0000 (UTC)
Received: (qmail 68985 invoked by uid 500); 15 Aug 2014 23:07:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68913 invoked by uid 500); 15 Aug 2014 23:07:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68901 invoked by uid 99); 15 Aug 2014 23:07:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 23:07:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Aug 2014 23:07:25 +0000
Received: by mail-ig0-f179.google.com with SMTP id h18so3180576igc.0
        for <dev@spark.apache.org>; Fri, 15 Aug 2014 16:07:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=llPXAAetN+2wiEQRMmyiQ2x043XEVtQJuY4IidRFU+0=;
        b=K+V8ehcJK0bowXxhfRo9FE1kpuwo3q4IW7kF4nBIoI5JhOAuqI8rR9yqTNlYh+sPSk
         7D+vNxLmz+YcvEGseq7dDmexgOre0DxMCgFbkr9g1fLameHlmXvvAoj3xC9KAAlp9/TG
         AWmw/X7inkq99FFPCJoA9UDHZs5HXYaV/93maRmPyxXxNC3wNjH15o+PIaHdghEUze7Z
         Xy7p1YpdNPbLbIwfoccHGkmT9Tbcr5wiN2E8UM3+oDaUmItngKtrQNlXcu5PaIfDQRN0
         8VQIx5BWdTghvm0tRc9URPySVcRqiMIzZwgmttU9qs43849R/qd5w76/CiXBPy+ZjjE/
         01FQ==
MIME-Version: 1.0
X-Received: by 10.42.82.6 with SMTP id b6mr23189121icl.51.1408144044137; Fri,
 15 Aug 2014 16:07:24 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Fri, 15 Aug 2014 16:07:24 -0700 (PDT)
Date: Fri, 15 Aug 2014 16:07:24 -0700
Message-ID: <CACkSZy2U=6MPv+Gu3d6nd+ATXZ6dcaT+w3iLaA1vNM_br4xq1A@mail.gmail.com>
Subject: Extra libs for bin/spark-shell - specifically for hbase
From: Stephen Boesch <javadba@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=485b397dd7018b53380500b315b3
X-Virus-Checked: Checked by ClamAV on apache.org

--485b397dd7018b53380500b315b3
Content-Type: text/plain; charset=UTF-8

Although this has been discussed a number of times here, I am still unclear
how to add user jars to the spark-shell:

a) for importing classes for use directly within the shell interpreter

b) for  invoking SparkContext commands with closures referencing user
supplied classes contained within jar's.

Similarly to other posts, I have gone through:

 updating bin/spark-env.sh
 SPARK_CLASSPATH
 SPARK_SUBMIT_OPTS
  creating conf/spark-defaults.conf  and adding
 spark.executor.extraClassPath
--driver-class-path
  etc

Hopefully there would be something along the lines of  a single entry added
to some claspath somewhere like this

   SPARK_CLASSPATH/driver-class-path/spark.executor.extraClassPath (or
whatever is the correct option..)  =
$HBASE_HOME/*:$HBASE_HOME/lib/*:$SPARK_CLASSPATH

Any ideas here?

thanks

--485b397dd7018b53380500b315b3--

From dev-return-8923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 16 00:11:33 2014
Return-Path: <dev-return-8923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A69011282
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 16 Aug 2014 00:11:33 +0000 (UTC)
Received: (qmail 75881 invoked by uid 500); 16 Aug 2014 00:11:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75819 invoked by uid 500); 16 Aug 2014 00:11:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75806 invoked by uid 99); 16 Aug 2014 00:11:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 00:11:32 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 00:11:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XIRaN-0006Wk-BP
	for dev@spark.incubator.apache.org; Fri, 15 Aug 2014 17:11:07 -0700
Date: Fri, 15 Aug 2014 17:11:07 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
In-Reply-To: <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
References: <1408115914518-7865.post@n3.nabble.com> <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com> <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com> <1408131707593-7877.post@n3.nabble.com> <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_189539_15391724.1408147867346"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_189539_15391724.1408147867346
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Xiangrui,
You were right, I had to use --driver_memory instead of setting it in
spark-defaults.conf.

However, now my just hangs with the following message:
4/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
29433434 bytes in 202 ms
14/08/15 23:54:46 INFO scheduler.TaskSetManager: Starting task 1.0:1 as TID
3 on executor 1: ip-10-226-198-31.us-west-2.compute.internal (PROCESS_LOCAL)
14/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
29433434 bytes in 203 ms

Any ideas on where else to look?


On Fri, Aug 15, 2014 at 3:29 PM, Xiangrui Meng [via Apache Spark Developers
List] <ml-node+s1001551n7883h12@n3.nabble.com> wrote:

> Did you verify the driver memory in the Executor tab of the WebUI? I
> think you need `--driver-memory 8g` with spark-shell or spark-submit
> instead of setting it in spark-defaults.conf.
>
> On Fri, Aug 15, 2014 at 12:41 PM, jerryye <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=7883&i=0>> wrote:
>
> > Setting spark.driver.memory has no effect. It's still hanging trying to
> > compute result.count when I'm sampling greater than 35% regardless of
> what
> > value of spark.driver.memory I'm setting.
> >
> > Here's my settings:
> > export SPARK_JAVA_OPTS="-Xms5g -Xmx10g -XX:MaxPermSize=10g"
> > export SPARK_MEM=10g
> >
> > in conf/spark-defaults:
> > spark.driver.memory 1500
> > spark.serializer org.apache.spark.serializer.KryoSerializer
> > spark.kryoserializer.buffer.mb 500
> > spark.executor.memory 58315m
> > spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
> > spark.executor.extraClassPath /root/ephemeral-hdfs/conf
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7877.html
>
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=7883&i=1>
> > For additional commands, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=7883&i=2>
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=7883&i=3>
> For additional commands, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=7883&i=4>
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7883.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h70@n3.nabble.com
> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=7865&code=amVycnl5ZUBnbWFpbC5jb218Nzg2NXwtNTI4OTc1MTAz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7886.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_189539_15391724.1408147867346--

From dev-return-8924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 16 05:02:34 2014
Return-Path: <dev-return-8924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 987F311753
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 16 Aug 2014 05:02:34 +0000 (UTC)
Received: (qmail 66081 invoked by uid 500); 16 Aug 2014 05:02:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66006 invoked by uid 500); 16 Aug 2014 05:02:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65994 invoked by uid 99); 16 Aug 2014 05:02:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 05:02:33 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 05:02:29 +0000
Received: by mail-ig0-f180.google.com with SMTP id l13so3580615iga.13
        for <dev@spark.incubator.apache.org>; Fri, 15 Aug 2014 22:02:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=CmOIs9HOewp2tQNEZ+e/TDWBHk2irSiIzS9Isxscr44=;
        b=0+spKKEITzqdYajpXFcdDrmCwklQgamCNUApOpDYARQYG9xDGvX9cG6IjXDr4w7VWT
         nqqtw7oBRdcKhATcmqfx1uUVnlNUsZDNTCxzQ8BXoYHSGHu/P5O2CoCXMscMYUSJLBpY
         54P8XMT8Q1QY1FNVNNmA3W5dEw3flsZGRtFPyoLPt4U5QSMAAc6Xs7T9S6+WKNiM16Df
         w4pMjGFjfdB8mUY3AoEwq+lFprVr8u8OGY90efiE8WW+RwnqeEp6Yx12fuJ7IF1SBzKl
         +gsjw7yk4GGLh1kabvlxU9e+QC/Geg13gLEkblYVkYMrLU3d2NrFcqbQuVh4PhejXj1a
         huhw==
MIME-Version: 1.0
X-Received: by 10.50.111.112 with SMTP id ih16mr70077943igb.30.1408165328586;
 Fri, 15 Aug 2014 22:02:08 -0700 (PDT)
Received: by 10.107.152.196 with HTTP; Fri, 15 Aug 2014 22:02:08 -0700 (PDT)
In-Reply-To: <CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
References: <1408115914518-7865.post@n3.nabble.com>
	<CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
	<E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
	<1408131707593-7877.post@n3.nabble.com>
	<CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
	<CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
Date: Fri, 15 Aug 2014 22:02:08 -0700
Message-ID: <CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Xiangrui Meng <mengxr@gmail.com>
To: jerryye <jerryye@gmail.com>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Just saw you used toArray on an RDD. That copies all data to the
driver and it is deprecated. countByValue is what you need:

val samples =3D sc.textFile("s3n://geonames")
val counts =3D samples.countByValue()
val result =3D samples.map(l =3D> (l, counts.getOrElse(l, 0L))

Could you also try to use the latest branch-1.1 or master with the
default akka.frameSize setting? The serialized task size should be
small because we now use broadcast RDD objects.

-Xiangrui

On Fri, Aug 15, 2014 at 5:11 PM, jerryye <jerryye@gmail.com> wrote:
> Hi Xiangrui,
> You were right, I had to use --driver_memory instead of setting it in
> spark-defaults.conf.
>
> However, now my just hangs with the following message:
> 4/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
> 29433434 bytes in 202 ms
> 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Starting task 1.0:1 as T=
ID
> 3 on executor 1: ip-10-226-198-31.us-west-2.compute.internal (PROCESS_LOC=
AL)
> 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
> 29433434 bytes in 203 ms
>
> Any ideas on where else to look?
>
>
> On Fri, Aug 15, 2014 at 3:29 PM, Xiangrui Meng [via Apache Spark Develope=
rs
> List] <ml-node+s1001551n7883h12@n3.nabble.com> wrote:
>
>> Did you verify the driver memory in the Executor tab of the WebUI? I
>> think you need `--driver-memory 8g` with spark-shell or spark-submit
>> instead of setting it in spark-defaults.conf.
>>
>> On Fri, Aug 15, 2014 at 12:41 PM, jerryye <[hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D0>> wrote:
>>
>> > Setting spark.driver.memory has no effect. It's still hanging trying t=
o
>> > compute result.count when I'm sampling greater than 35% regardless of
>> what
>> > value of spark.driver.memory I'm setting.
>> >
>> > Here's my settings:
>> > export SPARK_JAVA_OPTS=3D"-Xms5g -Xmx10g -XX:MaxPermSize=3D10g"
>> > export SPARK_MEM=3D10g
>> >
>> > in conf/spark-defaults:
>> > spark.driver.memory 1500
>> > spark.serializer org.apache.spark.serializer.KryoSerializer
>> > spark.kryoserializer.buffer.mb 500
>> > spark.executor.memory 58315m
>> > spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
>> > spark.executor.extraClassPath /root/ephemeral-hdfs/conf
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fra=
meSize-stalls-job-in-1-1-0-tp7865p7877.html
>>
>> > Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: [hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D1>
>> > For additional commands, e-mail: [hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D2>
>> >
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: [hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D3>
>> For additional commands, e-mail: [hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D4>
>>
>>
>>
>> ------------------------------
>>  If you reply to this email, your message will be added to the discussio=
n
>> below:
>>
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fra=
meSize-stalls-job-in-1-1-0-tp7865p7883.html
>>  To start a new topic under Apache Spark Developers List, email
>> ml-node+s1001551n1h70@n3.nabble.com
>> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click here
>> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/Naml=
Servlet.jtp?macro=3Dunsubscribe_by_code&node=3D7865&code=3DamVycnl5ZUBnbWFp=
bC5jb218Nzg2NXwtNTI4OTc1MTAz>
>> .
>> NAML
>> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/Naml=
Servlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&ba=
se=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleN=
amespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nab=
bleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_su=
bscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_i=
nstant_email%21nabble%3Aemail.naml>
>>
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551=
.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7886.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble=
.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 16 07:11:50 2014
Return-Path: <dev-return-8925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5BA3311900
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 16 Aug 2014 07:11:50 +0000 (UTC)
Received: (qmail 81621 invoked by uid 500); 16 Aug 2014 07:11:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81554 invoked by uid 500); 16 Aug 2014 07:11:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81537 invoked by uid 99); 16 Aug 2014 07:11:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 07:11:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 07:11:22 +0000
Received: by mail-qc0-f174.google.com with SMTP id l6so3097344qcy.5
        for <dev@spark.apache.org>; Sat, 16 Aug 2014 00:11:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=1r/Lo6mVmY4cwZNClgC9wHLBnMi2Mxy9KnILPYh5oUA=;
        b=XjOwAKGlkc2GIjLf42pNGA3Lk1Pjt+vn0Nkl/Q9dSIDevI0v28nLXxBYgBdsIbECUg
         SW+wt8isvL8F6jg4jbYZUeYZUnLQtsgDhp6Pbfge/M0FZwZOEyYtqfKldT1Xip9Uw9IL
         Wp914bagfuLkTiMUnA/zl14BLfkv8cDxwAGwzOQY7+lvKyBig3IxuVi8uvuT4x9tVkLf
         eI8QxdxXPwA/RBfWUU76IZp8yomZWv1QLZzKmVsJCP2EdinvC7cyeVy+RYvcMrFvGu/Q
         C0iUd4llzMeHNCKr6aL/DSPEJZttZVYzor4u96UT4MXwdLp4n58PKVEATR4wzTIV7fdI
         W2zg==
X-Gm-Message-State: ALoCoQn4wZJK6c/fCZL4f6OFjd+LvOqfmNK/PVfOzkIkZdGV6PNNIWuyFFOVIqbE+xFnwDqV32Gk
MIME-Version: 1.0
X-Received: by 10.140.96.85 with SMTP id j79mr2827057qge.5.1408173081291; Sat,
 16 Aug 2014 00:11:21 -0700 (PDT)
Received: by 10.140.42.37 with HTTP; Sat, 16 Aug 2014 00:11:21 -0700 (PDT)
In-Reply-To: <CACkSZy2U=6MPv+Gu3d6nd+ATXZ6dcaT+w3iLaA1vNM_br4xq1A@mail.gmail.com>
References: <CACkSZy2U=6MPv+Gu3d6nd+ATXZ6dcaT+w3iLaA1vNM_br4xq1A@mail.gmail.com>
Date: Sat, 16 Aug 2014 00:11:21 -0700
Message-ID: <CACBYxKKR=WeCJ6B=64O2r8VwpZtEXwSVnZovOkjnuH2GL-9P9A@mail.gmail.com>
Subject: Re: Extra libs for bin/spark-shell - specifically for hbase
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ac4684af7fe0500b9d8d7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ac4684af7fe0500b9d8d7
Content-Type: text/plain; charset=UTF-8

Hi Stephen,

Have you tried the --jars option (with jars separated by commas)?  It
should make the given jars available both to the driver and the executors.
 I believe one caveat currently is that if you give it a folder it won't
pick up all the jars inside.

-Sandy


On Fri, Aug 15, 2014 at 4:07 PM, Stephen Boesch <javadba@gmail.com> wrote:

> Although this has been discussed a number of times here, I am still unclear
> how to add user jars to the spark-shell:
>
> a) for importing classes for use directly within the shell interpreter
>
> b) for  invoking SparkContext commands with closures referencing user
> supplied classes contained within jar's.
>
> Similarly to other posts, I have gone through:
>
>  updating bin/spark-env.sh
>  SPARK_CLASSPATH
>  SPARK_SUBMIT_OPTS
>   creating conf/spark-defaults.conf  and adding
>  spark.executor.extraClassPath
> --driver-class-path
>   etc
>
> Hopefully there would be something along the lines of  a single entry added
> to some claspath somewhere like this
>
>    SPARK_CLASSPATH/driver-class-path/spark.executor.extraClassPath (or
> whatever is the correct option..)  =
> $HBASE_HOME/*:$HBASE_HOME/lib/*:$SPARK_CLASSPATH
>
> Any ideas here?
>
> thanks
>

--001a113ac4684af7fe0500b9d8d7--

From dev-return-8926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 16 07:18:49 2014
Return-Path: <dev-return-8926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3ACDC11905
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 16 Aug 2014 07:18:49 +0000 (UTC)
Received: (qmail 84328 invoked by uid 500); 16 Aug 2014 07:18:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84257 invoked by uid 500); 16 Aug 2014 07:18:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 83786 invoked by uid 99); 16 Aug 2014 07:16:46 -0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jerry.ye@gmail.com designates 209.85.213.179 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=NEtDdM46qIUC1ivOi8ENRXmHvvxOUEbbgyzb+9Q5BdQ=;
        b=Yr8A7emcnJzNvKJMFtnMhDmkqVYSPktsRpnM2ZcgyZdkHeuXH79U0m784rNsyQjF2P
         WSxTud/cCGPhyin3C0WGMtSTl4s+Lt89KxWLZKSLe7Ax4Z1mcnLsS5ChJU+E1ssfW5Dv
         Zxp4plpHdEwmAbQZoPDyVy74cX0y8FBpEtwYp0Z7zeKSebbW8TcpwkhVeKPPUL1yEVxv
         KL0BlhvA59rdPwroCdYVlg+GTyPHBMS8XtuQW++lni7yN7T9hmSj1BiEDO6tsxpVEf0Y
         TyUBsBQuMKPfLa6YYE8YTOGPbH/1VR5jvrLvKkTncQz52ZsHBfwTn7St2KEYBJ2iyJVZ
         67Mg==
MIME-Version: 1.0
X-Received: by 10.42.68.1 with SMTP id v1mr25347998ici.49.1408173377951; Sat,
 16 Aug 2014 00:16:17 -0700 (PDT)
Sender: jerry.ye@gmail.com
In-Reply-To: <CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com>
References: <1408115914518-7865.post@n3.nabble.com>
	<CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
	<E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
	<1408131707593-7877.post@n3.nabble.com>
	<CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
	<CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
	<CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com>
Date: Sat, 16 Aug 2014 00:16:17 -0700
X-Google-Sender-Auth: NCEU5qPsfLrBMQ0fh1OVVnmQLBI
Message-ID: <CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Jerry Ye <jerryye@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=20cf30334b15f99df50500b9e9ab
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30334b15f99df50500b9e9ab
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Xiangrui,
I actually tried branch-1.1 and master and it resulted in the job being
stuck at the TaskSetManager:
14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
with 2 tasks
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 as TID
2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
(PROCESS_LOCAL)
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
28055875 bytes in 162 ms
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 as TID
3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal (PROCESS_LOCAL)
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
28055875 bytes in 178 ms

It's been 10 minutes with no progress on relatively small data. I'll let it
run overnight and update in the morning. Is there some place that I should
look to see what is happening? I tried to ssh into the executor and look at
/root/spark/logs but there wasn't anything informative there.

I'm sure using CountByValue works fine but my use of a HashMap is only an
example. In my actual task, I'm loading a Trie data structure to perform
efficient string matching between a dataset of locations and strings
possibly containing mentions of locations.

This seems like a common thing, to process input with a relatively memory
intensive object like a Trie. I hope I'm not missing something obvious. Do
you know of any example code like my use case?

Thanks!

- jerry




On Fri, Aug 15, 2014 at 10:02 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Just saw you used toArray on an RDD. That copies all data to the
> driver and it is deprecated. countByValue is what you need:
>
> val samples =3D sc.textFile("s3n://geonames")
> val counts =3D samples.countByValue()
> val result =3D samples.map(l =3D> (l, counts.getOrElse(l, 0L))
>
> Could you also try to use the latest branch-1.1 or master with the
> default akka.frameSize setting? The serialized task size should be
> small because we now use broadcast RDD objects.
>
> -Xiangrui
>
> On Fri, Aug 15, 2014 at 5:11 PM, jerryye <jerryye@gmail.com> wrote:
> > Hi Xiangrui,
> > You were right, I had to use --driver_memory instead of setting it in
> > spark-defaults.conf.
> >
> > However, now my just hangs with the following message:
> > 4/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:0 a=
s
> > 29433434 bytes in 202 ms
> > 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Starting task 1.0:1 as
> TID
> > 3 on executor 1: ip-10-226-198-31.us-west-2.compute.internal
> (PROCESS_LOCAL)
> > 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:1 =
as
> > 29433434 bytes in 203 ms
> >
> > Any ideas on where else to look?
> >
> >
> > On Fri, Aug 15, 2014 at 3:29 PM, Xiangrui Meng [via Apache Spark
> Developers
> > List] <ml-node+s1001551n7883h12@n3.nabble.com> wrote:
> >
> >> Did you verify the driver memory in the Executor tab of the WebUI? I
> >> think you need `--driver-memory 8g` with spark-shell or spark-submit
> >> instead of setting it in spark-defaults.conf.
> >>
> >> On Fri, Aug 15, 2014 at 12:41 PM, jerryye <[hidden email]
> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D0>> wrote:
> >>
> >> > Setting spark.driver.memory has no effect. It's still hanging trying
> to
> >> > compute result.count when I'm sampling greater than 35% regardless o=
f
> >> what
> >> > value of spark.driver.memory I'm setting.
> >> >
> >> > Here's my settings:
> >> > export SPARK_JAVA_OPTS=3D"-Xms5g -Xmx10g -XX:MaxPermSize=3D10g"
> >> > export SPARK_MEM=3D10g
> >> >
> >> > in conf/spark-defaults:
> >> > spark.driver.memory 1500
> >> > spark.serializer org.apache.spark.serializer.KryoSerializer
> >> > spark.kryoserializer.buffer.mb 500
> >> > spark.executor.memory 58315m
> >> > spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
> >> > spark.executor.extraClassPath /root/ephemeral-hdfs/conf
> >> >
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fram=
eSize-stalls-job-in-1-1-0-tp7865p7877.html
> >>
> >> > Sent from the Apache Spark Developers List mailing list archive at
> >> Nabble.com.
> >> >
> >> > --------------------------------------------------------------------=
-
> >> > To unsubscribe, e-mail: [hidden email]
> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D1>
> >> > For additional commands, e-mail: [hidden email]
> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D2>
> >> >
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: [hidden email]
> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D3>
> >> For additional commands, e-mail: [hidden email]
> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D4>
> >>
> >>
> >>
> >> ------------------------------
> >>  If you reply to this email, your message will be added to the
> discussion
> >> below:
> >>
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fram=
eSize-stalls-job-in-1-1-0-tp7865p7883.html
> >>  To start a new topic under Apache Spark Developers List, email
> >> ml-node+s1001551n1h70@n3.nabble.com
> >> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click he=
re
> >> <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D7865&code=3DamVycnl5ZUBnbWFpbC=
5jb218Nzg2NXwtNTI4OTc1MTAz
> >
> >> .
> >> NAML
> >> <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=
=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNam=
espace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabbl=
eNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subs=
cribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_ins=
tant_email%21nabble%3Aemail.naml
> >
> >>
> >
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fram=
eSize-stalls-job-in-1-1-0-tp7865p7886.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--20cf30334b15f99df50500b9e9ab--

From dev-return-8927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 16 08:46:20 2014
Return-Path: <dev-return-8927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D459C11A80
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 16 Aug 2014 08:46:20 +0000 (UTC)
Received: (qmail 84253 invoked by uid 500); 16 Aug 2014 08:46:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84171 invoked by uid 500); 16 Aug 2014 08:46:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84148 invoked by uid 99); 16 Aug 2014 08:46:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 08:46:19 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of fireflyc@163.com designates 220.181.12.11 as permitted sender)
Received: from [220.181.12.11] (HELO m12-11.163.com) (220.181.12.11)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 16 Aug 2014 08:45:52 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Mime-Version:Message-Id:From:Subject:Date; bh=9yLPC
	FUnkttWQ4I4pq5s+2YAXuaRXYZWRi2VpMRvWMU=; b=CU+h189JpPKs/AF/ekxl8
	aiDtlnSO+Iku7hIa/37yrsek6zApw6FQvcEGgaaZB0YfQ8zE9hLPtqeQvO+rZKtZ
	197mhEhfgRETdgzw2jZIyB2r7Inrb8YhaAYhV+6ddbmRmQZjkIXAvh9lbooTqo2S
	ERtJOPGS8DtdzBSj6ii2m4=
Received: from [192.168.1.127] (unknown [223.249.163.153])
	by smtp7 (Coremail) with SMTP id C8CowEDZ67E5Gu9TXlj6AA--.1156S2;
	Sat, 16 Aug 2014 16:45:48 +0800 (CST)
References: <CAHc8ag0a1PWbK8aFxoEPoiKV_+Yr0p5nUdi9OOLOn0ioeq0uJQ@mail.gmail.com>
Mime-Version: 1.0 (1.0)
In-Reply-To: <CAHc8ag0a1PWbK8aFxoEPoiKV_+Yr0p5nUdi9OOLOn0ioeq0uJQ@mail.gmail.com>
Content-Type: text/plain;
	charset=gb2312
Content-Transfer-Encoding: quoted-printable
Message-Id: <D284378A-2E81-4DF3-AD8B-92AE069C1CAE@163.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: iPad Mail (11D257)
From: fireflyc <fireflyc@163.com>
Subject: Re: acquire and give back resources dynamically
Date: Sat, 16 Aug 2014 16:45:46 +0800
To: =?GB2312?B?xaPV173d?= <nzjemail@gmail.com>
X-CM-TRANSID:C8CowEDZ67E5Gu9TXlj6AA--.1156S2
X-Coremail-Antispam: 1Uf129KBjDUn29KB7ZKAUJUUUUU529EdanIXcx71UUUUU7v73
	VFW2AGmfu7bjvjm3AaLaJ3UbIYCTnIWIevJa73UjIFyTuYvjxU5rcfUUUUU
X-Originating-IP: [223.249.163.153]
X-CM-SenderInfo: piluvwxo1fqiywtou0bp/1tbiMh3ptVD-u4eM6AAAsE
X-Virus-Checked: Checked by ClamAV on apache.org

http://spark.apache.org/docs/latest/running-on-yarn.html
Spark just a Yarn application


> =D4=DA 2014=C4=EA8=D4=C214=C8=D5=A3=AC11:12=A3=AC=C5=A3=D5=D7=BD=DD <nzjem=
ail@gmail.com> =D0=B4=B5=C0=A3=BA
>=20
> Dear all:
>=20
> Does spark can acquire resources from and give back resources to
> YARN dynamically ?
>=20
>=20
> --=20
> *Regards,*
> *Zhaojie*


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 17 06:05:45 2014
Return-Path: <dev-return-8928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C1C9811AFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 17 Aug 2014 06:05:45 +0000 (UTC)
Received: (qmail 55779 invoked by uid 500); 17 Aug 2014 06:05:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55729 invoked by uid 500); 17 Aug 2014 06:05:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54632 invoked by uid 99); 17 Aug 2014 06:05:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 06:05:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sujee@sujee.net designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 06:05:16 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so3355927qac.19
        for <dev@spark.apache.org>; Sat, 16 Aug 2014 23:05:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=ceQj+E4vvbQ8IO7j/OEdoCYkkfFXfmM6CtAIUvXjOLc=;
        b=H30XkSBnYWGhIO+3jJ3nSgDP7WnOq/hzz4rYRC2a+7LHLcWShSDfW5E/iUBJl9zncw
         NMf2LInfjWskOk7TjjoiStQtv/lqT0mQa7W9XgKEDCzYa2o/Nx9QxWpKo6sjOVHFXSSc
         KvqVFwwhU0mSU+eAd+iLL/FxPxhH9bgnlfg+9VEuqrBBSNolreUhD9dzMyP0DgZdYdvi
         x0qV/uOAbsnMs3TRuK23FqeLTPqv5rSU/WPAEZkYZk5q7b8O/wtmXsFwWhzmjreH2VHC
         TQ5+rTnDlow9qFTfY/ae7r4O2N9s3ajnRWwQ+UYuHFQAw8oL7FL2HWK+MpjaUBFyjkD5
         VjkA==
X-Gm-Message-State: ALoCoQn/Dm3H29kLe29iN3GNcG+4xYiF1IlLZPjPgkHiC7AseW25UWN8AxEXGDykpXqEYhs2xA24
X-Received: by 10.229.53.134 with SMTP id m6mr23818328qcg.19.1408255515445;
 Sat, 16 Aug 2014 23:05:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.102.205 with HTTP; Sat, 16 Aug 2014 23:04:55 -0700 (PDT)
From: Sujee Maniyam <sujee@sujee.net>
Date: Sat, 16 Aug 2014 23:04:55 -0700
Message-ID: <CAC7UcBZ4_QRk33LSr=EFo9FYPYWv=07Bp-Q-THOgNdjQhQJnqQ@mail.gmail.com>
Subject: akka error : play framework (2.3.3) and spark (1.0.2)
To: Spark User Group <user@spark.apache.org>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134a8c6c061000500cd0932
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134a8c6c061000500cd0932
Content-Type: text/plain; charset=UTF-8

Hi

I am trying to connect to Spark from Play framework. Getting the following
Akka error...

[ERROR] [08/16/2014 17:12:05.249]
[spark-akka.actor.default-dispatcher-3] [ActorSystem(spark)] Uncaught
fatal error from thread [spark-akka.actor.default-dispatcher-3]
shutting down ActorSystem [spark]
java.lang.AbstractMethodError
  at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
  at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
  at akka.actor.ActorCell.terminate(ActorCell.scala:369)
  at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
  at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
  at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
  at akka.dispatch.Mailbox.run(Mailbox.scala:219)
  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


full stack trace : https://gist.github.com/sujee/ff14fd602b76314e693d

source code here : https://github.com/sujee/play-spark-test

I have also found this thread mentioning Akka in-compatibility How to run
Play 2.2.x with Akka 2.3.x?
<http://stackoverflow.com/questions/22779882/how-to-run-play-2-2-x-with-akka-2-3-x>

Stack overflow thread :
http://stackoverflow.com/questions/25346657/akka-error-play-framework-2-3-3-and-spark-1-0-2

any suggestions?

thanks!

Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam )

--001a1134a8c6c061000500cd0932--

From dev-return-8929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 17 06:15:54 2014
Return-Path: <dev-return-8929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E50B811B18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 17 Aug 2014 06:15:54 +0000 (UTC)
Received: (qmail 65776 invoked by uid 500); 17 Aug 2014 06:15:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65700 invoked by uid 500); 17 Aug 2014 06:15:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64606 invoked by uid 99); 17 Aug 2014 06:15:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 06:15:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suryavanshi.manu@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 06:15:47 +0000
Received: by mail-lb0-f177.google.com with SMTP id s7so3054304lbd.8
        for <multiple recipients>; Sat, 16 Aug 2014 23:15:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1Udch5nVv37MxEZzk5dqCb+QakEQWg/tkKWapCErXHY=;
        b=hgRiA0UlnBGIxpf6O4aebedNeT7rg5cihZ+fXcVx50FDI+WBD1f8XKvFsV2H99X1De
         nqhH4K8cnHK+C25ddSpWtsiNuVRW3RGr1NWx2wA25c8ldAwspzvXhxnvuWnE/HZBTVfV
         J3za10TawZbvePPRX5Z94g9chqSLrRLLgylFQk+VKi7BZP5XPcartAssVgzKFBOboLfp
         zVeraZTnRfTBtkNf7Uj9hUCRqxdDP9lO1FSiEPBXNDDFbQU7hIvxkScFwBxxTqGi5enJ
         H2u9s/TWV0aLxiuJoWKvsneGD1RZ07UbVrlAlnMieKkr4pqW4YMHAkaI2CA26PqhnUKz
         VQuA==
MIME-Version: 1.0
X-Received: by 10.152.27.66 with SMTP id r2mr21752872lag.34.1408256126089;
 Sat, 16 Aug 2014 23:15:26 -0700 (PDT)
Received: by 10.112.33.110 with HTTP; Sat, 16 Aug 2014 23:15:26 -0700 (PDT)
In-Reply-To: <CAC7UcBZ4_QRk33LSr=EFo9FYPYWv=07Bp-Q-THOgNdjQhQJnqQ@mail.gmail.com>
References: <CAC7UcBZ4_QRk33LSr=EFo9FYPYWv=07Bp-Q-THOgNdjQhQJnqQ@mail.gmail.com>
Date: Sat, 16 Aug 2014 23:15:26 -0700
Message-ID: <CADJJraJF-2TS20f8=+n+XgnqOaHNuYrqq1Xrxp0Ut5n_P4+OAQ@mail.gmail.com>
Subject: Re: akka error : play framework (2.3.3) and spark (1.0.2)
From: Manu Suryavansh <suryavanshi.manu@gmail.com>
To: Sujee Maniyam <sujee@sujee.net>
Cc: Spark User Group <user@spark.apache.org>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0158c0702600ee0500cd2e61
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c0702600ee0500cd2e61
Content-Type: text/plain; charset=UTF-8

Hi,

I tried the Spark(1.0.0)+Play(2.3.3) example from the Knoldus blog -
http://blog.knoldus.com/2014/06/18/play-with-spark-building-apache-spark-with-play-framework/
and
it worked for me. The project is here -
https://github.com/knoldus/Play-Spark-Scala

Regards,
Manu


On Sat, Aug 16, 2014 at 11:04 PM, Sujee Maniyam <sujee@sujee.net> wrote:

> Hi
>
> I am trying to connect to Spark from Play framework. Getting the following
> Akka error...
>
> [ERROR] [08/16/2014 17:12:05.249] [spark-akka.actor.default-dispatcher-3] [ActorSystem(spark)] Uncaught fatal error from thread [spark-akka.actor.default-dispatcher-3] shutting down ActorSystem [spark]
>
> java.lang.AbstractMethodError
>   at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
>
>   at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
>   at akka.actor.ActorCell.terminate(ActorCell.scala:369)
>
>   at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
>   at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
>
>   at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
>   at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>
>   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
>   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
>   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
>   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> full stack trace : https://gist.github.com/sujee/ff14fd602b76314e693d
>
> source code here : https://github.com/sujee/play-spark-test
>
> I have also found this thread mentioning Akka in-compatibility How to run
> Play 2.2.x with Akka 2.3.x?
> <http://stackoverflow.com/questions/22779882/how-to-run-play-2-2-x-with-akka-2-3-x>
>
> Stack overflow thread :
> http://stackoverflow.com/questions/25346657/akka-error-play-framework-2-3-3-and-spark-1-0-2
>
> any suggestions?
>
> thanks!
>
> Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam
> )
>



-- 
Manu Suryavansh

--089e0158c0702600ee0500cd2e61--

From dev-return-8930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 17 07:59:02 2014
Return-Path: <dev-return-8930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C772111CC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 17 Aug 2014 07:59:02 +0000 (UTC)
Received: (qmail 50244 invoked by uid 500); 17 Aug 2014 07:59:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50189 invoked by uid 500); 17 Aug 2014 07:59:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50174 invoked by uid 99); 17 Aug 2014 07:59:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 07:59:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sujee@sujee.net designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 07:58:36 +0000
Received: by mail-qa0-f44.google.com with SMTP id f12so3342716qad.17
        for <dev@spark.apache.org>; Sun, 17 Aug 2014 00:58:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=IjdTyMLk7+oWioFROO1kiIGjzmQN+UKbg5yIo4SPGZU=;
        b=kQpR62+hrq8wUQkKpckLvUCUz0woa2wDw1WAYV5hDqWbZg5H7yE5CsjX6LV6AhhuvL
         DIZbP1r+KVksB1t9PBvClJfnDsqNHSnP+U5608P7ZDKRXLdWxKBGslMg+vysiCQVHYNy
         45oXb8+JeMQJRlzhFvqcCIgvocniG7gDc7e6ovnymzxPi55u/ojdnM0wiudl9nJGmees
         POpxszx5+ioXxvu1gKd9FnDnw0sMhaXFxfrSlXYg1eHx08t9O3yityYXK7s7IkFZuvVs
         BmTBxkqyI3u6Q0RX+MVk58bUnt0RQfpFSPmWflBgvBnnniH6n6kvu5S7VE5O4qXstsK8
         R51w==
X-Gm-Message-State: ALoCoQk6UrYE4yLiZ/7JOcgMy8i7Gj7qcOEk2onLjKbFq7zFcDjJ3tJyaWypM4jTi/7wT4LTxknr
X-Received: by 10.224.62.8 with SMTP id v8mr43974488qah.9.1408262314714; Sun,
 17 Aug 2014 00:58:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.102.205 with HTTP; Sun, 17 Aug 2014 00:58:14 -0700 (PDT)
In-Reply-To: <CADJJraJF-2TS20f8=+n+XgnqOaHNuYrqq1Xrxp0Ut5n_P4+OAQ@mail.gmail.com>
References: <CAC7UcBZ4_QRk33LSr=EFo9FYPYWv=07Bp-Q-THOgNdjQhQJnqQ@mail.gmail.com>
 <CADJJraJF-2TS20f8=+n+XgnqOaHNuYrqq1Xrxp0Ut5n_P4+OAQ@mail.gmail.com>
From: Sujee Maniyam <sujee@sujee.net>
Date: Sun, 17 Aug 2014 00:58:14 -0700
Message-ID: <CAC7UcBbn3XZy4+sVLs0ZGuN0SA1nCWtZREw67FoUQcGqpawhAg@mail.gmail.com>
Subject: Re: akka error : play framework (2.3.3) and spark (1.0.2)
To: Manu Suryavansh <suryavanshi.manu@gmail.com>
Cc: Spark User Group <user@spark.apache.org>, Spark Dev List <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cb7005036f0500ce9f61
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cb7005036f0500ce9f61
Content-Type: text/plain; charset=UTF-8

thanks Manu..

For me, the sample app works only in 'local' mode.
If I tried to connect a spark cluster (even one running locally :
spark://localhost:7077)  I get the following error

spark.master=spark://localhost:7077
[error] o.a.s.s.c.SparkDeploySchedulerBackend - Application has been
killed. Reason: Master removed our application: FAILED
[error] application -

! @6j8im8dfj - Internal server error, for (GET) [/] ->

play.api.Application$$anon$1: Execution exception[[SparkException: Job
aborted due to stage failure: Master removed our application: FAILED]]



Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam )


On Sat, Aug 16, 2014 at 11:15 PM, Manu Suryavansh <
suryavanshi.manu@gmail.com> wrote:

> Hi,
>
> I tried the Spark(1.0.0)+Play(2.3.3) example from the Knoldus blog -
> http://blog.knoldus.com/2014/06/18/play-with-spark-building-apache-spark-with-play-framework/ and
> it worked for me. The project is here -
> https://github.com/knoldus/Play-Spark-Scala
>
> Regards,
> Manu
>
>
> On Sat, Aug 16, 2014 at 11:04 PM, Sujee Maniyam <sujee@sujee.net> wrote:
>
>> Hi
>>
>> I am trying to connect to Spark from Play framework. Getting the
>> following Akka error...
>>
>>
>> [ERROR] [08/16/2014 17:12:05.249] [spark-akka.actor.default-dispatcher-3] [ActorSystem(spark)] Uncaught fatal error from thread [spark-akka.actor.default-dispatcher-3] shutting down ActorSystem [spark]
>>
>> java.lang.AbstractMethodError
>>   at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
>>
>>   at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
>>   at akka.actor.ActorCell.terminate(ActorCell.scala:369)
>>
>>   at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
>>   at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
>>
>>   at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
>>   at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>
>>   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
>>   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>
>>   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>
>>   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>
>>
>> full stack trace : https://gist.github.com/sujee/ff14fd602b76314e693d
>>
>> source code here : https://github.com/sujee/play-spark-test
>>
>> I have also found this thread mentioning Akka in-compatibility How to
>> run Play 2.2.x with Akka 2.3.x?
>> <http://stackoverflow.com/questions/22779882/how-to-run-play-2-2-x-with-akka-2-3-x>
>>
>> Stack overflow thread :
>> http://stackoverflow.com/questions/25346657/akka-error-play-framework-2-3-3-and-spark-1-0-2
>>
>> any suggestions?
>>
>> thanks!
>>
>> Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam
>> )
>>
>
>
>
> --
> Manu Suryavansh
>

--089e0149cb7005036f0500ce9f61--

From dev-return-8931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 17 16:47:59 2014
Return-Path: <dev-return-8931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0ABBD11344
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 17 Aug 2014 16:47:59 +0000 (UTC)
Received: (qmail 18133 invoked by uid 500); 17 Aug 2014 16:47:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18069 invoked by uid 500); 17 Aug 2014 16:47:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18057 invoked by uid 99); 17 Aug 2014 16:47:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 16:47:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 17 Aug 2014 16:47:54 +0000
Received: from EXDR01-WEST.YOJOE.local (10.160.10.135) by
 EX03-WEST.YOJOE.local (10.160.10.136) with Microsoft SMTP Server (TLS) id
 14.3.195.1; Sun, 17 Aug 2014 09:47:32 -0700
Received: from EX02-WEST.YOJOE.local ([169.254.1.40]) by
 EXDR01-WEST.YOJOE.local ([169.254.3.37]) with mapi id 14.03.0195.001; Sun, 17
 Aug 2014 09:47:32 -0700
From: Mingyu Kim <mkim@palantir.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: Gary Malouf <malouf.gary@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Topic: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster
Thread-Index: AQHPuBCZ/+f7rUz2C0O3higuAjnLYZvRJ3gAgAAvOoCAAKJtAIAAikCAgAKCXIA=
Date: Sun, 17 Aug 2014 16:47:31 +0000
Message-ID: <D0162839.12722%mkim@palantir.com>
References: <D01288A8.12442%mkim@palantir.com>
 <CAGOvqipYz0WdRKXF01HWusfZ9R7fRLQYPffTtYH1p+3o-xQh9Q@mail.gmail.com>
 <CABPQxstdGkEv3jQoU4xmdGhJqdM0dyjKyxk70L-Mjt6oL8J67Q@mail.gmail.com>
 <D01396DA.1257B%mkim@palantir.com>
 <CABPQxstjpmZHRd_p_mPMi4G_qcqN1Rwkc1=z7sTaXrFAN8WbNw@mail.gmail.com>
In-Reply-To: <CABPQxstjpmZHRd_p_mPMi4G_qcqN1Rwkc1=z7sTaXrFAN8WbNw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: yes
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.160.25.148]
Content-Type: multipart/signed; protocol="application/pkcs7-signature";
	micalg=sha1; boundary="B_3491113649_13558732"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3491113649_13558732
Content-type: multipart/alternative;
	boundary="B_3491113649_13528964"


--B_3491113649_13528964
Content-type: text/plain;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

Thanks for the clarification. I don=B9t have a deep knowledge of Scala, but I
thought this was going to be reasonable to support since Java serialization
framework provides relatively easy ways to support these kinds of backwards
compatibility. I can see how this could be harder with closures.

Supporting at least the stability between different patch versions would
help a lot.

Mingyu

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Friday, August 15, 2014 at 12:28 PM
To:  Mingyu Kim <mkim@palantir.com>
Cc:  Gary Malouf <malouf.gary@gmail.com>, "dev@spark.apache.org"
<dev@spark.apache.org>
Subject:  Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
against a 1.0.1 cluster

Hey Mingyu,=20

For this reason we are encouraging all users to run spark-submit. In Spark
we capture closures and send them over the network from the driver to the
executors. These are then deserialized on the executor. So if your driver
program has different versions of certain classes than exist on the
executor, it doesn't work well. We've even run into stranger issues, where
the exact same version of Spark was used at the driver and the executor, bu=
t
they were compiled at different times. Since Scala doesn't guarantee stable
naming for certain types of anonymous classes, the class names didn't match
up and it caused errors at runtime.

The most straightforward way to deal with this is to inject, at run-time,
the exact version of Spark that the cluster expects if you are running the
standalone mode.

I think we'd be totally open to improving this to provide "API stability"
for the case you are working with, i.e. the case where you have spark 1.0.X
at the driver and 1.0.Y on the executors. But it will require looking at
what exactly causes incompatibility and seeing if there is a solution. In
this case I think we changed a publicly exposed class (the RDD class) in
some way that caused compatibility issues... even though we didn't change
any binary signatures.

BTW - this is not relevant to YARN mode where you ship Spark with your job
so there is no "cluster version of Spark".

- Patrick


On Fri, Aug 15, 2014 at 11:13 AM, Mingyu Kim <mkim@palantir.com> wrote:
> Thanks for your response. I think I misinterpreted the stability/compatib=
ility
> guarantee with 1.0 release. It seems like the compatibility is only at th=
e API
> level.
>=20
> This is interesting because it means any system/product that is built on =
top
> of Spark and uses Spark with a long-running SparkContext connecting to th=
e
> cluster over network, will need to make sure it has the exact same versio=
n of
> Spark jar as the cluster, even to the patch version. This would be analog=
ous
> to having to compile Spark against a very specific version of Hadoop, as
> opposed to currently being able to use the Spark package with CDH4 agains=
t
> most of the CDH4 Hadoop clusters.
>=20
> Is it correct that Spark is focusing and prioritizing around the spark-su=
bmit
> use cases than the aforementioned use cases? I just wanted to better
> understand the future direction/prioritization of spark.
>=20
> Thanks,
> Mingyu
>=20
> From: Patrick Wendell <pwendell@gmail.com>
> Date: Thursday, August 14, 2014 at 6:32 PM
> To: Gary Malouf <malouf.gary@gmail.com>
> Cc: Mingyu Kim <mkim@palantir.com>, "dev@spark.apache.org"
> <dev@spark.apache.org>
> Subject: Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
> against a 1.0.1 cluster
>=20
> I commented on the bug. For driver mode, you'll need to get the correspon=
ding
> version of spark-submit for Spark 1.0.2.
>=20
>=20
> On Thu, Aug 14, 2014 at 3:43 PM, Gary Malouf <malouf.gary@gmail.com> wrot=
e:
>> To be clear, is it 'compiled' against 1.0.2 or it packaged with it?
>>=20
>>=20
>> On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim <mkim@palantir.com> wrote:
>>=20
>>> > I ran a really simple code that runs with Spark 1.0.2 jar and connect=
s to
>>> > a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassExceptio=
n. I
>>> > filed the bug at https://issues.apache.org/jira/browse/SPARK-3050
>>> <https://urldefense.proofpoint.com/v1/url?u=3Dhttps://issues.apache.org/j=
ira/b
>>> rowse/SPARK-3050&k=3DfDZpZZQMmYwf27OU23GmAQ%3D%3D%0A&r=3DUKDOcu6qL3KsoZhpOo=
hNBR1
>>> ucPNmWnbd3eEJ9hVUdMk%3D%0A&m=3DqvQ59wZwD7EuezjTuLzmNTRUamDRDnI7%2F0%2BnUL=
tXk4k
>>> %3D%0A&s=3Db7abf7638a3e6fac2ddac9d8f0ca52f1a92945465abfb2e2d996a96d2301fe=
c5> .
>>> >
>>> > I assumed the minor and patch releases shouldn=B9t break compatibility.=
 Is
>>> > that correct?
>>> >
>>> > Thanks,
>>> > Mingyu
>>> >
>=20




--B_3491113649_13528964
Content-type: text/html;
	charset="ISO-8859-1"
Content-transfer-encoding: quoted-printable

<html><head></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size:=
 14px; font-family: Calibri, sans-serif;"><div><div>Thanks for the clarifica=
tion. I don&#8217;t have a deep knowledge of Scala, but I thought this was g=
oing to be reasonable to support since Java serialization framework provides=
 relatively easy ways to support these kinds of backwards compatibility. I c=
an see how this could be harder with closures.</div><div><br></div><div>Supp=
orting at least the stability between different patch versions would help a =
lot.</div><div><div><br></div><div>Mingyu</div></div></div><div><br></div><s=
pan id=3D"OLK_SRC_BODY_SECTION"><div style=3D"font-family:Calibri; font-size:11p=
t; text-align:left; color:black; BORDER-BOTTOM: medium none; BORDER-LEFT: me=
dium none; PADDING-BOTTOM: 0in; PADDING-LEFT: 0in; PADDING-RIGHT: 0in; BORDE=
R-TOP: #b5c4df 1pt solid; BORDER-RIGHT: medium none; PADDING-TOP: 3pt"><span=
 style=3D"font-weight:bold">From: </span> Patrick Wendell &lt;<a href=3D"mailto:=
pwendell@gmail.com">pwendell@gmail.com</a>&gt;<br><span style=3D"font-weight:b=
old">Date: </span> Friday, August 15, 2014 at 12:28 PM<br><span style=3D"font-=
weight:bold">To: </span> Mingyu Kim &lt;<a href=3D"mailto:mkim@palantir.com">m=
kim@palantir.com</a>&gt;<br><span style=3D"font-weight:bold">Cc: </span> Gary =
Malouf &lt;<a href=3D"mailto:malouf.gary@gmail.com">malouf.gary@gmail.com</a>&=
gt;, "<a href=3D"mailto:dev@spark.apache.org">dev@spark.apache.org</a>" &lt;<a=
 href=3D"mailto:dev@spark.apache.org">dev@spark.apache.org</a>&gt;<br><span st=
yle=3D"font-weight:bold">Subject: </span> Re: [SPARK-3050] Spark program runni=
ng with 1.0.2 jar cannot run against a 1.0.1 cluster<br></div><div><br></div=
><div><meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><di=
v><div dir=3D"ltr">Hey Mingyu,
<div><br></div><div>For this reason we are encouraging all users to run spa=
rk-submit. In Spark we capture closures and send them over the network from =
the driver to the executors. These are then deserialized on the executor. So=
 if your driver program has different versions
 of certain classes than exist on the executor, it doesn't work well. We've=
 even run into stranger issues, where the exact same version of Spark was us=
ed at the driver and the executor, but they were compiled at different times=
. Since Scala doesn't guarantee
 stable naming for certain types of anonymous classes, the class names didn=
't match up and it caused errors at runtime.</div><div><br></div><div>The mo=
st straightforward way to deal with this is to inject, at run-time, the exac=
t version of Spark that the cluster expects if you are running the standalon=
e mode.</div><div><br></div><div>I think we'd be totally open to improving t=
his to provide "API stability" for the case you are working with, i.e. the c=
ase where you have spark 1.0.X at the driver and 1.0.Y on the executors. But=
 it will require looking at what exactly causes incompatibility
 and seeing if there is a solution. In this case I think we changed a publi=
cly exposed class (the RDD class) in some way that caused compatibility issu=
es... even though we didn't change any binary signatures.</div><div><br></di=
v><div>BTW - this is not relevant to YARN mode where you ship Spark with you=
r job so there is no "cluster version of Spark".</div><div><br></div><div>- =
Patrick</div></div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote"=
>On Fri, Aug 15, 2014 at 11:13 AM, Mingyu Kim <span dir=3D"ltr">
&lt;<a href=3D"mailto:mkim@palantir.com" target=3D"_blank">mkim@palantir.com</a=
>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break=
-word;color:rgb(0,0,0);font-size:14px;font-family:Calibri,sans-serif"><div><=
div>Thanks for your response. I think I misinterpreted the stability/compati=
bility guarantee with 1.0 release. It seems like the compatibility is only a=
t the API level.</div><div><br></div><div>This is interesting because it mea=
ns any system/product that is built on top of Spark and uses Spark with a lo=
ng-running SparkContext connecting to the cluster over network, will need to=
 make sure it has the exact same version of Spark jar as the cluster,
 even to the patch version. This would be analogous to having to compile Sp=
ark against a very specific version of Hadoop, as opposed to currently being=
 able to use the Spark package with CDH4 against most of the CDH4 Hadoop clu=
sters.</div><div><br></div><div>Is it correct that Spark is focusing and pri=
oritizing around the spark-submit use cases than the aforementioned use case=
s? I just wanted to better understand the future direction/prioritization of=
 spark.</div><div><div><br></div><div>Thanks,</div><div>Mingyu</div></div></=
div><div><br></div><span><div style=3D"font-family:Calibri;font-size:11pt;text=
-align:left;color:black;BORDER-BOTTOM:medium none;BORDER-LEFT:medium none;PA=
DDING-BOTTOM:0in;PADDING-LEFT:0in;PADDING-RIGHT:0in;BORDER-TOP:#b5c4df 1pt s=
olid;BORDER-RIGHT:medium none;PADDING-TOP:3pt"><span style=3D"font-weight:bold=
">From: </span>Patrick Wendell &lt;<a href=3D"mailto:pwendell@gmail.com" targe=
t=3D"_blank">pwendell@gmail.com</a>&gt;<br><span style=3D"font-weight:bold">Date=
: </span>Thursday, August 14, 2014 at 6:32 PM<br><span style=3D"font-weight:bo=
ld">To: </span>Gary Malouf &lt;<a href=3D"mailto:malouf.gary@gmail.com" target=
=3D"_blank">malouf.gary@gmail.com</a>&gt;<br><span style=3D"font-weight:bold">Cc=
: </span>Mingyu Kim &lt;<a href=3D"mailto:mkim@palantir.com" target=3D"_blank">m=
kim@palantir.com</a>&gt;, "<a href=3D"mailto:dev@spark.apache.org" target=3D"_bl=
ank">dev@spark.apache.org</a>" &lt;<a href=3D"mailto:dev@spark.apache.org" tar=
get=3D"_blank">dev@spark.apache.org</a>&gt;<br><span style=3D"font-weight:bold">=
Subject: </span>Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot=
 run against a 1.0.1 cluster<br></div><div><div class=3D"h5"><div><br></div><d=
iv><div><div dir=3D"ltr">I commented on the bug. For driver mode, you'll need =
to get the corresponding version of spark-submit for Spark 1.0.2.</div><div =
class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">On Thu, Aug 14, 2014 at=
 3:43 PM, Gary Malouf <span dir=3D"ltr">
&lt;<a href=3D"mailto:malouf.gary@gmail.com" target=3D"_blank">malouf.gary@gmai=
l.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin=
:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
To be clear, is it 'compiled' against 1.0.2 or it packaged with it?<br><div=
><div><br><br>
On Thu, Aug 14, 2014 at 6:39 PM, Mingyu Kim &lt;<a href=3D"mailto:mkim@palant=
ir.com" target=3D"_blank">mkim@palantir.com</a>&gt; wrote:<br><br>
&gt; I ran a really simple code that runs with Spark 1.0.2 jar and connects=
 to<br>
&gt; a Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException=
. I<br>
&gt; filed the bug at <a href=3D"https://urldefense.proofpoint.com/v1/url?u=3Dh=
ttps://issues.apache.org/jira/browse/SPARK-3050&amp;k=3DfDZpZZQMmYwf27OU23GmAQ=
%3D%3D%0A&amp;r=3DUKDOcu6qL3KsoZhpOohNBR1ucPNmWnbd3eEJ9hVUdMk%3D%0A&amp;m=3DqvQ5=
9wZwD7EuezjTuLzmNTRUamDRDnI7%2F0%2BnULtXk4k%3D%0A&amp;s=3Db7abf7638a3e6fac2dda=
c9d8f0ca52f1a92945465abfb2e2d996a96d2301fec5" target=3D"_blank">
https://issues.apache.org/jira/browse/SPARK-3050</a>.<br>
&gt;<br>
&gt; I assumed the minor and patch releases shouldn&#8217;t break compatibi=
lity. Is<br>
&gt; that correct?<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Mingyu<br>
&gt;<br></div></div></blockquote></div><br></div></div></div></div></div></=
span></div></blockquote></div><br></div></div></div></span></body></html>

--B_3491113649_13528964--

--B_3491113649_13558732
Content-Type: application/pkcs7-signature; name="smime.p7s"
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename="smime.p7s"
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=

--B_3491113649_13558732--

From dev-return-8932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 04:45:36 2014
Return-Path: <dev-return-8932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4991711EA5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 04:45:36 +0000 (UTC)
Received: (qmail 98584 invoked by uid 500); 18 Aug 2014 04:45:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98515 invoked by uid 500); 18 Aug 2014 04:45:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 4522 invoked by uid 99); 16 Aug 2014 16:19:02 -0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jerry.ye@gmail.com designates 209.85.213.180 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=/zkGNSdK/NMKlvlhbzG1+mwBHDIkl7+ocZChKOyZgLU=;
        b=Jv3r37HekwRssCM3mO2XsHlUzfalIY9XyNsgQMLOjqqR/W/flgnP+M2i7glQ7iuLH4
         j+ot8J5xIiLTS36e46IV/kd7bDrCzvDzSuGtpwmmXmiJk2uyY0ddZS7CNDP8dCldo5AQ
         RaBwBWNrQxdQrfoHbGzjNqo1tfTGEjtl83qnfSLkVwIz1UjbMsqMfbKD8PN5fO16G+C/
         ELhngUC+y3XWcKUv3+2uBzfFmwgXg39TtvCEhCwLa2Q52C3MxZg6UivDR1p7qeS/rSY/
         9kr8NWqmJsgeOkI0QisUB1D9Wwbzj9d3CaKc2dYHQlF2I61X8NnuiOU6TYlwz0fN0lw/
         Hvow==
MIME-Version: 1.0
X-Received: by 10.50.28.75 with SMTP id z11mr74203347igg.11.1408205917105;
 Sat, 16 Aug 2014 09:18:37 -0700 (PDT)
Sender: jerry.ye@gmail.com
In-Reply-To: <CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com>
References: <1408115914518-7865.post@n3.nabble.com>
	<CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
	<E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
	<1408131707593-7877.post@n3.nabble.com>
	<CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
	<CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
	<CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com>
	<CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com>
Date: Sat, 16 Aug 2014 09:18:37 -0700
X-Google-Sender-Auth: CCpyqebW5yU7zVrpqBzJ3TJMjys
Message-ID: <CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Jerry Ye <jerryye@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b14475efbf0500c17ddb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b14475efbf0500c17ddb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The job ended up running overnight with no progress. :-(


On Sat, Aug 16, 2014 at 12:16 AM, Jerry Ye <jerryye@gmail.com> wrote:

> Hi Xiangrui,
> I actually tried branch-1.1 and master and it resulted in the job being
> stuck at the TaskSetManager:
> 14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
> with 2 tasks
> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 as
> TID 2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
> (PROCESS_LOCAL)
> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
> 28055875 bytes in 162 ms
> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 as
> TID 3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal
> (PROCESS_LOCAL)
> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
> 28055875 bytes in 178 ms
>
> It's been 10 minutes with no progress on relatively small data. I'll let
> it run overnight and update in the morning. Is there some place that I
> should look to see what is happening? I tried to ssh into the executor an=
d
> look at /root/spark/logs but there wasn't anything informative there.
>
> I'm sure using CountByValue works fine but my use of a HashMap is only an
> example. In my actual task, I'm loading a Trie data structure to perform
> efficient string matching between a dataset of locations and strings
> possibly containing mentions of locations.
>
> This seems like a common thing, to process input with a relatively memory
> intensive object like a Trie. I hope I'm not missing something obvious. D=
o
> you know of any example code like my use case?
>
> Thanks!
>
> - jerry
>
>
>
>
> On Fri, Aug 15, 2014 at 10:02 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> Just saw you used toArray on an RDD. That copies all data to the
>> driver and it is deprecated. countByValue is what you need:
>>
>> val samples =3D sc.textFile("s3n://geonames")
>> val counts =3D samples.countByValue()
>> val result =3D samples.map(l =3D> (l, counts.getOrElse(l, 0L))
>>
>> Could you also try to use the latest branch-1.1 or master with the
>> default akka.frameSize setting? The serialized task size should be
>> small because we now use broadcast RDD objects.
>>
>> -Xiangrui
>>
>> On Fri, Aug 15, 2014 at 5:11 PM, jerryye <jerryye@gmail.com> wrote:
>> > Hi Xiangrui,
>> > You were right, I had to use --driver_memory instead of setting it in
>> > spark-defaults.conf.
>> >
>> > However, now my just hangs with the following message:
>> > 4/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:0 =
as
>> > 29433434 bytes in 202 ms
>> > 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Starting task 1.0:1 a=
s
>> TID
>> > 3 on executor 1: ip-10-226-198-31.us-west-2.compute.internal
>> (PROCESS_LOCAL)
>> > 14/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:1
>> as
>> > 29433434 bytes in 203 ms
>> >
>> > Any ideas on where else to look?
>> >
>> >
>> > On Fri, Aug 15, 2014 at 3:29 PM, Xiangrui Meng [via Apache Spark
>> Developers
>> > List] <ml-node+s1001551n7883h12@n3.nabble.com> wrote:
>> >
>> >> Did you verify the driver memory in the Executor tab of the WebUI? I
>> >> think you need `--driver-memory 8g` with spark-shell or spark-submit
>> >> instead of setting it in spark-defaults.conf.
>> >>
>> >> On Fri, Aug 15, 2014 at 12:41 PM, jerryye <[hidden email]
>> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D0>> wrote:
>> >>
>> >> > Setting spark.driver.memory has no effect. It's still hanging tryin=
g
>> to
>> >> > compute result.count when I'm sampling greater than 35% regardless =
of
>> >> what
>> >> > value of spark.driver.memory I'm setting.
>> >> >
>> >> > Here's my settings:
>> >> > export SPARK_JAVA_OPTS=3D"-Xms5g -Xmx10g -XX:MaxPermSize=3D10g"
>> >> > export SPARK_MEM=3D10g
>> >> >
>> >> > in conf/spark-defaults:
>> >> > spark.driver.memory 1500
>> >> > spark.serializer org.apache.spark.serializer.KryoSerializer
>> >> > spark.kryoserializer.buffer.mb 500
>> >> > spark.executor.memory 58315m
>> >> > spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
>> >> > spark.executor.extraClassPath /root/ephemeral-hdfs/conf
>> >> >
>> >> >
>> >> >
>> >> > --
>> >> > View this message in context:
>> >>
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fra=
meSize-stalls-job-in-1-1-0-tp7865p7877.html
>> >>
>> >> > Sent from the Apache Spark Developers List mailing list archive at
>> >> Nabble.com.
>> >> >
>> >> > -------------------------------------------------------------------=
--
>> >> > To unsubscribe, e-mail: [hidden email]
>> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D1>
>> >> > For additional commands, e-mail: [hidden email]
>> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D2>
>> >> >
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: [hidden email]
>> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D3>
>> >> For additional commands, e-mail: [hidden email]
>> >> <http://user/SendEmail.jtp?type=3Dnode&node=3D7883&i=3D4>
>> >>
>> >>
>> >>
>> >> ------------------------------
>> >>  If you reply to this email, your message will be added to the
>> discussion
>> >> below:
>> >>
>> >>
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fra=
meSize-stalls-job-in-1-1-0-tp7865p7883.html
>> >>  To start a new topic under Apache Spark Developers List, email
>> >> ml-node+s1001551n1h70@n3.nabble.com
>> >> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click
>> here
>> >> <
>> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D7865&code=3DamVycnl5ZUBnbWFpb=
C5jb218Nzg2NXwtNTI4OTc1MTAz
>> >
>> >> .
>> >> NAML
>> >> <
>> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabb=
leNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_sub=
scribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_in=
stant_email%21nabble%3Aemail.naml
>> >
>> >>
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fra=
meSize-stalls-job-in-1-1-0-tp7865p7886.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>
>

--089e0158b14475efbf0500c17ddb--

From dev-return-8933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 05:28:31 2014
Return-Path: <dev-return-8933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DEE4411F4D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 05:28:31 +0000 (UTC)
Received: (qmail 35050 invoked by uid 500); 18 Aug 2014 05:28:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34969 invoked by uid 500); 18 Aug 2014 05:28:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34948 invoked by uid 99); 18 Aug 2014 05:28:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 05:28:30 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of wenjuandou@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 05:28:26 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <wenjuandou@gmail.com>)
	id 1XJFUE-0003To-1e
	for dev@spark.incubator.apache.org; Sun, 17 Aug 2014 22:28:06 -0700
Date: Sun, 17 Aug 2014 22:28:06 -0700 (PDT)
From: DouWenjuan <wenjuandou@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408339685986-7896.post@n3.nabble.com>
Subject: [Graphx] some problem about using SVDPlusPlus
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The implementation of SVDPlusPlus shows that it produces two new graph in
each iteration which will also be cached to memory. However, as the
iteration goes on, more and more graph will be cached and out of memory
happens. So I think it maybe need to unpersist old graph which will not be
used any more and add a few lines of code, the details are showed as
follows:
def run(edges: RDD[Edge[Double]], conf: Conf)
    : (Graph[(DoubleMatrix, DoubleMatrix, Double, Double), Double], Double)
=
  {

    // Generate default vertex attribute
    def defaultF(rank: Int): (DoubleMatrix, DoubleMatrix, Double, Double) =
{
      val v1 = new DoubleMatrix(rank)
      val v2 = new DoubleMatrix(rank)
      for (i <- 0 until rank) {
        v1.put(i, Random.nextDouble())
        v2.put(i, Random.nextDouble())
      }
      (v1, v2, 0.0, 0.0)
    }

    // calculate global rating mean
    edges.cache()

    val (rs, rc) = edges.map(e => (e.attr, 1L)).reduce((a, b) => (a._1 +
b._1, a._2 + b._2))
    val u = rs / rc

    *var preG: Graph[(DoubleMatrix, DoubleMatrix, Double, Double), Double] =
null*

    // construct graph
    var g = Graph.fromEdges(edges, defaultF(conf.rank)).cache()
   * preG = g*

    // Calculate initial bias and norm
    val t0 = g.mapReduceTriplets(
      et => Iterator((et.srcId, (1L, et.attr)), (et.dstId, (1L, et.attr))),
        (g1: (Long, Double), g2: (Long, Double)) => (g1._1 + g2._1, g1._2 +
g2._2))

    g = g.outerJoinVertices(t0) {
      (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double),
       msg: Option[(Long, Double)]) =>
        (vd._1, vd._2, msg.get._2 / msg.get._1, 1.0 /
scala.math.sqrt(msg.get._1))
    }

    def mapTrainF(conf: Conf, u: Double)
        (et: EdgeTriplet[(DoubleMatrix, DoubleMatrix, Double, Double),
Double])
      : Iterator[(VertexId, (DoubleMatrix, DoubleMatrix, Double))] = {
      val (usr, itm) = (et.srcAttr, et.dstAttr)
      val (p, q) = (usr._1, itm._1)
      var pred = u + usr._3 + itm._3 + q.dot(usr._2)
      pred = math.max(pred, conf.minVal)
      pred = math.min(pred, conf.maxVal)
      val err = et.attr - pred
      val updateP = q.mul(err)
        .subColumnVector(p.mul(conf.gamma7))
        .mul(conf.gamma2)
      val updateQ = usr._2.mul(err)
        .subColumnVector(q.mul(conf.gamma7))
        .mul(conf.gamma2)
      val updateY = q.mul(err * usr._4)
        .subColumnVector(itm._2.mul(conf.gamma7))
        .mul(conf.gamma2)
      Iterator((et.srcId, (updateP, updateY, (err - conf.gamma6 * usr._3) *
conf.gamma1)),
        (et.dstId, (updateQ, updateY, (err - conf.gamma6 * itm._3) *
conf.gamma1)))
    }

    for (i <- 0 until conf.maxIters) {

      // Phase 1, calculate pu + |N(u)|^(-0.5)*sum(y) for user nodes
      g.cache()
      *preG.unpersistVertices(blocking = false)
      preG.edges.unpersist(blocking = false)
      preG = g*
      val t1 = g.mapReduceTriplets(
        et => Iterator((et.srcId, et.dstAttr._2)),
        (g1: DoubleMatrix, g2: DoubleMatrix) => g1.addColumnVector(g2))

      g = g.outerJoinVertices(t1) {
        (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double),
         msg: Option[DoubleMatrix]) =>
          if (msg.isDefined) (vd._1, vd._1
            .addColumnVector(msg.get.mul(vd._4)), vd._3, vd._4) else vd
      }

      // Phase 2, update p for user nodes and q, y for item nodes
      g.cache()
      *preG.unpersistVertices(blocking = false)
      preG.edges.unpersist(blocking = false)
      preG = g*
      val t2 = g.mapReduceTriplets(
        mapTrainF(conf, u),
        (g1: (DoubleMatrix, DoubleMatrix, Double), g2: (DoubleMatrix,
DoubleMatrix, Double)) =>
          (g1._1.addColumnVector(g2._1), g1._2.addColumnVector(g2._2), g1._3
+ g2._3))

      g = g.outerJoinVertices(t2) {
        (vid: VertexId,
         vd: (DoubleMatrix, DoubleMatrix, Double, Double),
         msg: Option[(DoubleMatrix, DoubleMatrix, Double)]) =>
          (vd._1.addColumnVector(msg.get._1),
vd._2.addColumnVector(msg.get._2),
            vd._3 + msg.get._3, vd._4)
      }

    }

    // calculate error on training set
    def mapTestF(conf: Conf, u: Double)
        (et: EdgeTriplet[(DoubleMatrix, DoubleMatrix, Double, Double),
Double])
      : Iterator[(VertexId, Double)] =
    {
      val (usr, itm) = (et.srcAttr, et.dstAttr)
      val (p, q) = (usr._1, itm._1)
      var pred = u + usr._3 + itm._3 + q.dot(usr._2)
      pred = math.max(pred, conf.minVal)
      pred = math.min(pred, conf.maxVal)
      val err = (et.attr - pred) * (et.attr - pred)
      Iterator((et.dstId, err))
    }
    g.cache()
    val t3 = g.mapReduceTriplets(mapTestF(conf, u), (g1: Double, g2: Double)
=> g1 + g2)
    g = g.outerJoinVertices(t3) {
      (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double), msg:
Option[Double]) =>
        if (msg.isDefined) (vd._1, vd._2, vd._3, msg.get) else vd
    }

    (g, u)
  }

Bold black lines are the code I added. I hoped that in each iteration when
new graph was cached, old graph would be unpersist. However, the fact seems
to be that both new graph and old graph are unpersist because the time used
for outerJoiinVertices and mapReduceTriples become longer and longer as the
iteration goes on. So I guess that no graph is stored and should be
recomputed in each iteration. In addition, in logs I find "WARN
impl.ShippableVertexPartitionOps: Joining two VertexPartitions with
different indexes is slow  ".
So how can I correctly unpersist old graph?
Thanks



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-some-problem-about-using-SVDPlusPlus-tp7896.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 06:35:51 2014
Return-Path: <dev-return-8934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 209AF1110D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 06:35:51 +0000 (UTC)
Received: (qmail 35666 invoked by uid 500); 18 Aug 2014 06:35:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35599 invoked by uid 500); 18 Aug 2014 06:35:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35588 invoked by uid 99); 18 Aug 2014 06:35:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 06:35:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zzhang@hortonworks.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 06:35:22 +0000
Received: by mail-pa0-f48.google.com with SMTP id et14so7048814pad.21
        for <dev@spark.incubator.apache.org>; Sun, 17 Aug 2014 23:35:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to:content-type:content-transfer-encoding;
        bh=uKDKBnKLhe90wg2BmoN5K6Uyy2/G9yqM3X+Er5mZnQI=;
        b=XKCwLDl+e077nhexRf/U/nuHy7FQq5yHnNo0k1pUjvmU53UTiOgF3kO8di9lw0+W4g
         5kdNaMXGX/Rkz4VX9WMztc/Gp5PCqB5rTo6DLvvynlIXXddMW46TypWTADUv1pUvEb3V
         heLO6gYH7KVtmJY8ZuduhkSy6D65qFe0z8WzWlKdB5qSpoyYyDab8YBZif3aG0d28lPz
         ZbB5/P9PdimaltZI/KDNJTNTbjo74HShTEg2Zu9pqtq+deN9ponOZfAOB19Kk9mbf6lk
         XbKhoMRU7lBtK1oUpKBtvDRyF96njaQ4cOADxIltaRUe4Rf10BOmOIuVy0GNumJJjO5v
         5GGA==
X-Gm-Message-State: ALoCoQlzjDncLWPAP4fZEpfT+OFWNLFOf/OC6kbOApfWMIljGHaW7Fv+NYcQWF6D26K9xLOwhqDZkk/Ail7i/2WulPH8LyRtih2imgvOiSeOdIytvtX6vrQ=
X-Received: by 10.70.96.74 with SMTP id dq10mr27740776pdb.112.1408343714646;
        Sun, 17 Aug 2014 23:35:14 -0700 (PDT)
Received: from [192.168.0.11] (c-24-6-100-89.hsd1.ca.comcast.net. [24.6.100.89])
        by mx.google.com with ESMTPSA id kl1sm15166225pbd.31.2014.08.17.23.35.11
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 17 Aug 2014 23:35:13 -0700 (PDT)
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Zhan Zhang <zzhang@hortonworks.com>
In-Reply-To: <CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com>
Date: Sun, 17 Aug 2014 23:35:08 -0700
Cc: Xiangrui Meng <mengxr@gmail.com>,
 dev <dev@spark.incubator.apache.org>
Message-Id: <4F6FE01E-C9C7-4A80-A23A-9D64F5CDB87C@hortonworks.com>
References: <1408115914518-7865.post@n3.nabble.com> <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com> <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com> <1408131707593-7877.post@n3.nabble.com> <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com> <CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com> <CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com> <CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com> <CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com>
To: Jerry Ye <jerryye@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Is it because countByValue or toArray put too much stress on the driver, if=
 there are many unique words=20
To me it is a typical word count problem, then you can solve it as follows =
(correct me if I am wrong)

val textFile =3D sc.textFile(=93file")
val counts =3D textFile.flatMap(line =3D> line.split(" ")).map(word =3D> (w=
ord, 1)).reduceByKey((a, b) =3D> a + b)
counts.saveAsTextFile(=93file=94)//any way you don=92t want to collect resu=
lts to master, and instead putting them in file.

Thanks.

Zhan Zhang

On Aug 16, 2014, at 9:18 AM, Jerry Ye <jerryye@gmail.com> wrote:

> The job ended up running overnight with no progress. :-(
>=20
>=20
> On Sat, Aug 16, 2014 at 12:16 AM, Jerry Ye <jerryye@gmail.com> wrote:
>=20
>> Hi Xiangrui,
>> I actually tried branch-1.1 and master and it resulted in the job being
>> stuck at the TaskSetManager:
>> 14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
>> with 2 tasks
>> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 as
>> TID 2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
>> (PROCESS_LOCAL)
>> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0 a=
s
>> 28055875 bytes in 162 ms
>> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 as
>> TID 3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal
>> (PROCESS_LOCAL)
>> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1 a=
s
>> 28055875 bytes in 178 ms
>>=20
>> It's been 10 minutes with no progress on relatively small data. I'll let
>> it run overnight and update in the morning. Is there some place that I
>> should look to see what is happening? I tried to ssh into the executor a=
nd
>> look at /root/spark/logs but there wasn't anything informative there.
>>=20
>> I'm sure using CountByValue works fine but my use of a HashMap is only a=
n
>> example. In my actual task, I'm loading a Trie data structure to perform
>> efficient string matching between a dataset of locations and strings
>> possibly containing mentions of locations.
>>=20
>> This seems like a common thing, to process input with a relatively memor=
y
>> intensive object like a Trie. I hope I'm not missing something obvious. =
Do
>> you know of any example code like my use case?
>>=20
>> Thanks!
>>=20
>> - jerry
>>=20


--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:00:43 2014
Return-Path: <dev-return-8935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7036112CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:00:43 +0000 (UTC)
Received: (qmail 86805 invoked by uid 500); 18 Aug 2014 16:00:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86743 invoked by uid 500); 18 Aug 2014 16:00:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86731 invoked by uid 99); 18 Aug 2014 16:00:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:00:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:00:37 +0000
Received: by mail-vc0-f173.google.com with SMTP id hy10so6001249vcb.32
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 09:00:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Y6sEqIKHpOY4A6Zn4nSfq9bL9uTp4t7ukYP1Umqpg2w=;
        b=VI+fsfyIvuCV630QluCKHU9cVETsjZytLtLUy6Py5MPZiM8uYUgL0Bekn5RBEo+zeL
         HCd96IS+igCoVQ42UdbRDHXWWeMc9URtkAa5keuuFtrQ01D/i/En+rtEu6pkcukN6eL9
         6jsfUHvRGb6TScuGy5ObKwhjqNFKzmFgA4HnByLVBE9fiduCJT9UUSAnU6TWARbOXbah
         Xk51ozE+1nFzyFsHXDvBhwnwhFIXPawWjw1Oq0NA7WMONLTOpw0uIQhoPW63afXs2hmQ
         O6OaCmQUdjczphrZMa/HXoFji8yZ7DOcjHZoYv+JFM40mkyMbhUddFPKb7apeu3T6ycq
         kZDQ==
MIME-Version: 1.0
X-Received: by 10.220.250.142 with SMTP id mo14mr17609713vcb.26.1408377617057;
 Mon, 18 Aug 2014 09:00:17 -0700 (PDT)
Received: by 10.52.114.194 with HTTP; Mon, 18 Aug 2014 09:00:17 -0700 (PDT)
Date: Mon, 18 Aug 2014 09:00:17 -0700
Message-ID: <CACkSZy0iAA19+B2HwUT3-OTJCNPcOnBBkjZ0+eNA=wsTy61hYg@mail.gmail.com>
Subject: Markdown viewer for the docs
From: Stephen Boesch <javadba@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013cba22932a090500e977a5
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cba22932a090500e977a5
Content-Type: text/plain; charset=UTF-8

Which viewer is capable of seeing all of the content in the spark docs
-including the (apparent) extensions?

An example page:
https://github.com/apache/spark/blob/master/docs/mllib-linear-methods.md


Local MD viewers/editors that I have  tried include:   mdcharm,  retext and
haroopad: one of these handle the TOC,  the math symbols, or proper
formatting of the scala code

 Even directly opening the md file from  github.com with the browser those
same issues appear: no TOC, math, or proper code formatting.   I am tried
both FF and chrome (on ubuntu 12.0.4)


Any tips from  the creators/maintainers of these pages  Thanks!

--089e013cba22932a090500e977a5--

From dev-return-8936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:10:52 2014
Return-Path: <dev-return-8936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9AE411323
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:10:52 +0000 (UTC)
Received: (qmail 15218 invoked by uid 500); 18 Aug 2014 16:10:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15173 invoked by uid 500); 18 Aug 2014 16:10:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 69902 invoked by uid 99); 18 Aug 2014 15:54:40 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jerry.ye@gmail.com designates 209.85.217.170 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=OBkZurNX6uZIRX40uOI2GOtZF8nHgt8ny8vVkfsEubI=;
        b=Pr1DvLAzF448lFO+fYQicn8RlD+LWkUA0TDmX0Wjj2f2FzUuBRPv3v1G74EPGsAfgj
         RjOgjP5Cwxl4FdUu9v8mutM/HJ4tA098Pop8K1ObywO/3PL9F9ikGqg3pmqtRRya8siq
         vUExzKArpwKjpR4rj1ff86ibGUVB65PdZ8K0jZPV95ECFFm0T7UtPdEVv3ttNdbdXCuD
         NNUkwBRLahfrST0RFQ4sDETVpMYA9iQ852EaZAwIIMATJbq1C7as+Z7DDcnKXySX0Bp6
         g5Pvba72BH8pW2hl6sADkk5zCCkdN9EzWwTj6/qE7MvUTOPHg4XzaMNEBTo9xQoB9Nw8
         wu3g==
MIME-Version: 1.0
X-Received: by 10.152.43.6 with SMTP id s6mr31092484lal.65.1408377252478; Mon,
 18 Aug 2014 08:54:12 -0700 (PDT)
Sender: jerry.ye@gmail.com
In-Reply-To: <4F6FE01E-C9C7-4A80-A23A-9D64F5CDB87C@hortonworks.com>
References: <1408115914518-7865.post@n3.nabble.com>
	<CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com>
	<E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com>
	<1408131707593-7877.post@n3.nabble.com>
	<CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com>
	<CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com>
	<CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com>
	<CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com>
	<CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com>
	<4F6FE01E-C9C7-4A80-A23A-9D64F5CDB87C@hortonworks.com>
Date: Mon, 18 Aug 2014 08:54:12 -0700
X-Google-Sender-Auth: qAxiwmv-nzyrqAk9EGMYSWW7TjE
Message-ID: <CAAziPCmXSH2uO4EFR_QxCdd9zXE0KCh44Kpqz=vewPB0SJZqfA@mail.gmail.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Jerry Ye <jerryye@gmail.com>
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c289f8d806240500e9615f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c289f8d806240500e9615f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Zhan,
Thanks for looking into this. I'm actually using the hash map as an example
of the simplest snippet of code that is failing for me. I know that this is
just the word count. In my actual problem I'm using a Trie data structure
to find substring matches.


On Sun, Aug 17, 2014 at 11:35 PM, Zhan Zhang <zzhang@hortonworks.com> wrote=
:

> Is it because countByValue or toArray put too much stress on the driver,
> if there are many unique words
> To me it is a typical word count problem, then you can solve it as follow=
s
> (correct me if I am wrong)
>
> val textFile =3D sc.textFile(=E2=80=9Cfile")
> val counts =3D textFile.flatMap(line =3D> line.split(" ")).map(word =3D> =
(word,
> 1)).reduceByKey((a, b) =3D> a + b)
> counts.saveAsTextFile(=E2=80=9Cfile=E2=80=9D)//any way you don=E2=80=99t =
want to collect results
> to master, and instead putting them in file.
>
> Thanks.
>
> Zhan Zhang
>
> On Aug 16, 2014, at 9:18 AM, Jerry Ye <jerryye@gmail.com> wrote:
>
> > The job ended up running overnight with no progress. :-(
> >
> >
> > On Sat, Aug 16, 2014 at 12:16 AM, Jerry Ye <jerryye@gmail.com> wrote:
> >
> >> Hi Xiangrui,
> >> I actually tried branch-1.1 and master and it resulted in the job bein=
g
> >> stuck at the TaskSetManager:
> >> 14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.=
0
> >> with 2 tasks
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 a=
s
> >> TID 2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
> >> (PROCESS_LOCAL)
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0
> as
> >> 28055875 bytes in 162 ms
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 a=
s
> >> TID 3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal
> >> (PROCESS_LOCAL)
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1
> as
> >> 28055875 bytes in 178 ms
> >>
> >> It's been 10 minutes with no progress on relatively small data. I'll l=
et
> >> it run overnight and update in the morning. Is there some place that I
> >> should look to see what is happening? I tried to ssh into the executor
> and
> >> look at /root/spark/logs but there wasn't anything informative there.
> >>
> >> I'm sure using CountByValue works fine but my use of a HashMap is only
> an
> >> example. In my actual task, I'm loading a Trie data structure to perfo=
rm
> >> efficient string matching between a dataset of locations and strings
> >> possibly containing mentions of locations.
> >>
> >> This seems like a common thing, to process input with a relatively
> memory
> >> intensive object like a Trie. I hope I'm not missing something obvious=
.
> Do
> >> you know of any example code like my use case?
> >>
> >> Thanks!
> >>
> >> - jerry
> >>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>

--001a11c289f8d806240500e9615f--

From dev-return-8937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:20:39 2014
Return-Path: <dev-return-8937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B47181138E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:20:39 +0000 (UTC)
Received: (qmail 48961 invoked by uid 500); 18 Aug 2014 16:20:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48889 invoked by uid 500); 18 Aug 2014 16:20:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48867 invoked by uid 99); 18 Aug 2014 16:20:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:20:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kanzhangemail@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:20:34 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so8559951igd.5
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 09:20:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:sender:in-reply-to:references:date:message-id
         :subject:from:to:cc:content-type;
        bh=y9bYQgApedd/Lc5l8qpsutQcFNhCIHgviwEbiYLkMn0=;
        b=A5AZKtQFex1VHIAcVWGvyzGQ77eMjATDWfclm8fRCrOa/wek+FHufW/xHGRKvPcrng
         i231O2RPU56DPRJldfgxyTGu35u9wMG1xfMIlukG02Ih6V9S3ebxilsoCzriIe8cALG/
         5rH3hxaPKGyIcOp/qKmykX0IRXVTPVoPh3to1K2aaAaKrXjcz11ce/rS9h9zAT1Ux9wJ
         qTJTv4a98nuHIlYw/QnktVEM0pZ7opklqAbN0xw4+ybKWhb5uQKWqXVJ3mFhqYGc7gGu
         ykgymTNe+5mJB+jt4+1hLjZ2KmA5YCqCyFOC80hD5H9ux+FgT348WHoTKvqcGcJL0gia
         slUw==
MIME-Version: 1.0
X-Received: by 10.42.50.212 with SMTP id b20mr6887536icg.57.1408378813914;
 Mon, 18 Aug 2014 09:20:13 -0700 (PDT)
Reply-To: kzhang@apache.org
Sender: kanzhangemail@gmail.com
Received: by 10.64.135.8 with HTTP; Mon, 18 Aug 2014 09:20:13 -0700 (PDT)
In-Reply-To: <CACkSZy0iAA19+B2HwUT3-OTJCNPcOnBBkjZ0+eNA=wsTy61hYg@mail.gmail.com>
References: <CACkSZy0iAA19+B2HwUT3-OTJCNPcOnBBkjZ0+eNA=wsTy61hYg@mail.gmail.com>
Date: Mon, 18 Aug 2014 09:20:13 -0700
X-Google-Sender-Auth: zKutWS-30w-5k4xeaHhmdh9WfHY
Message-ID: <CALRHqP_rAwxXyuby5V7Xr4UzVnhwbawNdPn3gkKRBLXo2NHuyQ@mail.gmail.com>
Subject: Re: Markdown viewer for the docs
From: Kan Zhang <kzhang@apache.org>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba61458ce9a33c0500e9be64
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba61458ce9a33c0500e9be64
Content-Type: text/plain; charset=UTF-8

If you are willing to compile it, "The markdown code can be compiled to
HTML using the [Jekyll tool](http://jekyllrb.com)." More in docs/README.md.


On Mon, Aug 18, 2014 at 9:00 AM, Stephen Boesch <javadba@gmail.com> wrote:

> Which viewer is capable of seeing all of the content in the spark docs
> -including the (apparent) extensions?
>
> An example page:
> https://github.com/apache/spark/blob/master/docs/mllib-linear-methods.md
>
>
> Local MD viewers/editors that I have  tried include:   mdcharm,  retext and
> haroopad: one of these handle the TOC,  the math symbols, or proper
> formatting of the scala code
>
>  Even directly opening the md file from  github.com with the browser those
> same issues appear: no TOC, math, or proper code formatting.   I am tried
> both FF and chrome (on ubuntu 12.0.4)
>
>
> Any tips from  the creators/maintainers of these pages  Thanks!
>

--90e6ba61458ce9a33c0500e9be64--

From dev-return-8938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:20:45 2014
Return-Path: <dev-return-8938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01C131138F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:20:45 +0000 (UTC)
Received: (qmail 50085 invoked by uid 500); 18 Aug 2014 16:20:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50022 invoked by uid 500); 18 Aug 2014 16:20:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50009 invoked by uid 99); 18 Aug 2014 16:20:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:20:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:20:39 +0000
Received: by mail-pd0-f177.google.com with SMTP id p10so7744903pdj.22
        for <dev@spark.incubator.apache.org>; Mon, 18 Aug 2014 09:20:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to:content-type;
        bh=upPxDd1uM5JkHYhrQXYfTGeIAKIav3OFp7jb2CjVUdY=;
        b=EZ270nBqOi+EJBpIObFIXy6RCQqvfKPgZOJ5rG0bWUG21i2NEeCV180YfmGq5uUmkr
         w6kkdfS1X3vgJvXRoH9yu9/eAeJlSiB60xbuffnqirjxOclRdfJM9umEWeFYjTtjWDnS
         N43RrWy8kedYYWeVey25moa6mz0i/dX717AAzj2nKVyh0+OvJACdcdIGvl0V3voYFbtf
         n9C3cfaz45Qfnw7N8AksJaElWdmaeGqwYC+wfVZb5wqtiRVz17Eg/6HlmYwpf+Ga1zD2
         bTI2dLPTKSVS/GP+uX6SzQCV+1lDW2HkvP+zkes4tvvqv+1h4jJBD+cQN1BNift19b6X
         pHIw==
X-Gm-Message-State: ALoCoQkSo8EfqeKcyq+u6A/kPvyMdyvLpbd0SNgagLyFZHEQFYtDqe7BdM/F2tdxNZH+UgrPBWZScThEnEVWy7h20Gd3dU2UaQt1XoTdmBhP+Fpi344EGKQ=
X-Received: by 10.66.178.205 with SMTP id da13mr11859589pac.146.1408378814866;
        Mon, 18 Aug 2014 09:20:14 -0700 (PDT)
Received: from [10.11.3.111] ([192.175.27.2])
        by mx.google.com with ESMTPSA id gs2sm16624597pbc.20.2014.08.18.09.20.12
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 18 Aug 2014 09:20:13 -0700 (PDT)
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
From: Zhan Zhang <zzhang@hortonworks.com>
In-Reply-To: <CAAziPCmXSH2uO4EFR_QxCdd9zXE0KCh44Kpqz=vewPB0SJZqfA@mail.gmail.com>
Date: Mon, 18 Aug 2014 09:20:08 -0700
Cc: Xiangrui Meng <mengxr@gmail.com>,
 dev <dev@spark.incubator.apache.org>
Message-Id: <04A800D9-74FC-460F-8A94-F923D10D5181@hortonworks.com>
References: <1408115914518-7865.post@n3.nabble.com> <CAJgQjQ8wCB8h1Tt93B3-dMk1=UE8Cw1OGYoNpkih5xAgQkPvEg@mail.gmail.com> <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com> <1408131707593-7877.post@n3.nabble.com> <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com> <CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com> <CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com> <CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com> <CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com> <4F6FE01E-C9C7-4A80-A23A-9D64F5CDB87C@hortonworks.com> <CAAziPCmXSH2uO4EFR_QxCdd9zXE0KCh44Kpqz=vewPB0SJZqfA@mail.gmail.com>
To: Jerry Ye <jerryye@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
Content-Type: multipart/alternative; boundary="Apple-Mail=_4A12336F-271D-4FD0-B688-BC739417B54B"
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_4A12336F-271D-4FD0-B688-BC739417B54B
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain; charset=windows-1252

Not sure exactly how you use it. My understanding is that in spark it would=
 be better to keep the overhead of driver as less as possible. Is it possib=
le to broadcast trie to executors, do computation there and then aggregate =
the counters (??) in reduct phase?

Thanks.

Zhan Zhang

On Aug 18, 2014, at 8:54 AM, Jerry Ye <jerryye@gmail.com> wrote:

> Hi Zhan,
> Thanks for looking into this. I'm actually using the hash map as an examp=
le of the simplest snippet of code that is failing for me. I know that this=
 is just the word count. In my actual problem I'm using a Trie data structu=
re to find substring matches.
>=20
>=20
> On Sun, Aug 17, 2014 at 11:35 PM, Zhan Zhang <zzhang@hortonworks.com> wro=
te:
> Is it because countByValue or toArray put too much stress on the driver, =
if there are many unique words
> To me it is a typical word count problem, then you can solve it as follow=
s (correct me if I am wrong)
>=20
> val textFile =3D sc.textFile(=93file")
> val counts =3D textFile.flatMap(line =3D> line.split(" ")).map(word =3D> =
(word, 1)).reduceByKey((a, b) =3D> a + b)
> counts.saveAsTextFile(=93file=94)//any way you don=92t want to collect re=
sults to master, and instead putting them in file.
>=20
> Thanks.
>=20
> Zhan Zhang
>=20
> On Aug 16, 2014, at 9:18 AM, Jerry Ye <jerryye@gmail.com> wrote:
>=20
> > The job ended up running overnight with no progress. :-(
> >
> >
> > On Sat, Aug 16, 2014 at 12:16 AM, Jerry Ye <jerryye@gmail.com> wrote:
> >
> >> Hi Xiangrui,
> >> I actually tried branch-1.1 and master and it resulted in the job bein=
g
> >> stuck at the TaskSetManager:
> >> 14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.=
0
> >> with 2 tasks
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 a=
s
> >> TID 2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
> >> (PROCESS_LOCAL)
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0=
 as
> >> 28055875 bytes in 162 ms
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 a=
s
> >> TID 3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal
> >> (PROCESS_LOCAL)
> >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1=
 as
> >> 28055875 bytes in 178 ms
> >>
> >> It's been 10 minutes with no progress on relatively small data. I'll l=
et
> >> it run overnight and update in the morning. Is there some place that I
> >> should look to see what is happening? I tried to ssh into the executor=
 and
> >> look at /root/spark/logs but there wasn't anything informative there.
> >>
> >> I'm sure using CountByValue works fine but my use of a HashMap is only=
 an
> >> example. In my actual task, I'm loading a Trie data structure to perfo=
rm
> >> efficient string matching between a dataset of locations and strings
> >> possibly containing mentions of locations.
> >>
> >> This seems like a common thing, to process input with a relatively mem=
ory
> >> intensive object like a Trie. I hope I'm not missing something obvious=
. Do
> >> you know of any example code like my use case?
> >>
> >> Thanks!
> >>
> >> - jerry
> >>
>=20
>=20
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>=20


--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

--Apple-Mail=_4A12336F-271D-4FD0-B688-BC739417B54B--

From dev-return-8939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:47:17 2014
Return-Path: <dev-return-8939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00121114C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:47:16 +0000 (UTC)
Received: (qmail 41537 invoked by uid 500); 18 Aug 2014 16:46:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41490 invoked by uid 500); 18 Aug 2014 16:46:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41388 invoked by uid 99); 18 Aug 2014 16:46:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:46:58 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of opensourcecodefish@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:46:53 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <opensourcecodefish@gmail.com>)
	id 1XJQ4m-0007QB-Uo
	for dev@spark.incubator.apache.org; Mon, 18 Aug 2014 09:46:32 -0700
Date: Mon, 18 Aug 2014 09:46:32 -0700 (PDT)
From: zycodefish <opensourcecodefish@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408380392938-7902.post@n3.nabble.com>
Subject: Shuffle overlapping
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I'm reading the implementation of the shuffle in Spark. 
My understanding is that it's not overlapping with upstream stage.

Is it helpful to overlap the computation of upstream stage w/ the shuffle (I
mean the network copy, like in Hadoop)? If it is, is there any plan to
implement it in the any version?

--Z



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Shuffle-overlapping-tp7902.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:51:49 2014
Return-Path: <dev-return-8940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CEEC114F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:51:49 +0000 (UTC)
Received: (qmail 60235 invoked by uid 500); 18 Aug 2014 16:51:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60171 invoked by uid 500); 18 Aug 2014 16:51:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60154 invoked by uid 99); 18 Aug 2014 16:51:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:51:47 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:51:43 +0000
Received: by mail-pd0-f178.google.com with SMTP id w10so7919898pde.9
        for <dev@spark.incubator.apache.org>; Mon, 18 Aug 2014 09:51:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=hbT7k/kwb1W4qfNZcEQc1dnlcBFRzcXRk4CBaGNy4pk=;
        b=wBijZETu70lsgTcjjVYZjOIXg8Kz6mGTLZKqy6UJCWvwzHRdUDpwU38seydUZnMHG/
         JsjCm49v+dJ3650/t37fzz0uRAULnRR2OqRhqjd7hV2gO2tz4KN/AQuSHvGwcwi8KbBX
         ovLHYBxboRpgxLNplu/PVhFyqmIymF9grJ68jZjxN4Ys6W3vqUoEGCQXa0uLzS4Mum5H
         pRnY5ZifYHnxxzd36qtihDpf9yGGS0sJFT9GDJrGwHuItBA7YkD6SdBuVeiUBKgaXErc
         rH0CrPTMJeVmOKS7mzWu/zvzRV3Rpdzm9GasF6EeshzkVuALC0nXgM1MOQComa0hVyv/
         mmfQ==
MIME-Version: 1.0
X-Received: by 10.68.163.100 with SMTP id yh4mr36532119pbb.122.1408380678073;
 Mon, 18 Aug 2014 09:51:18 -0700 (PDT)
Received: by 10.70.48.101 with HTTP; Mon, 18 Aug 2014 09:51:17 -0700 (PDT)
In-Reply-To: <1408380392938-7902.post@n3.nabble.com>
References: <1408380392938-7902.post@n3.nabble.com>
Date: Mon, 18 Aug 2014 09:51:17 -0700
Message-ID: <CAOEPXP5-yzGjmuP3aQw2xn7VTQHMTKBqOgdPR==ahDh+4gLR4Q@mail.gmail.com>
Subject: Re: Shuffle overlapping
From: Josh Rosen <rosenville@gmail.com>
To: zycodefish <opensourcecodefish@gmail.com>
Cc: "Spark Dev (Apache Incubator)" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba96d940673190500ea2e74
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba96d940673190500ea2e74
Content-Type: text/plain; charset=UTF-8

I think there's some discussion of this at
https://issues.apache.org/jira/browse/SPARK-2387 and
https://github.com/apache/spark/pull/1328.

- Josh


On Mon, Aug 18, 2014 at 9:46 AM, zycodefish <opensourcecodefish@gmail.com>
wrote:

> Hi all,
>
> I'm reading the implementation of the shuffle in Spark.
> My understanding is that it's not overlapping with upstream stage.
>
> Is it helpful to overlap the computation of upstream stage w/ the shuffle
> (I
> mean the network copy, like in Hadoop)? If it is, is there any plan to
> implement it in the any version?
>
> --Z
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Shuffle-overlapping-tp7902.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7ba96d940673190500ea2e74--

From dev-return-8941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 16:54:45 2014
Return-Path: <dev-return-8941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44FA411536
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 16:54:45 +0000 (UTC)
Received: (qmail 88477 invoked by uid 500); 18 Aug 2014 16:54:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88416 invoked by uid 500); 18 Aug 2014 16:54:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88151 invoked by uid 99); 18 Aug 2014 16:54:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:54:37 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 16:54:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XJQC9-0007nH-Oc
	for dev@spark.incubator.apache.org; Mon, 18 Aug 2014 09:54:09 -0700
Date: Mon, 18 Aug 2014 09:54:09 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <E56D2275-B1AF-491F-A913-384348C3BC94@gmail.com>
In-Reply-To: <04A800D9-74FC-460F-8A94-F923D10D5181@hortonworks.com>
References: <E99224FE-E92F-476D-885F-22BE51A8DC9E@gmail.com> <1408131707593-7877.post@n3.nabble.com> <CAJgQjQ8srtYFnzw_XFThxD1uSvnjLo-+6eLE=a3RZxkvhV-+sA@mail.gmail.com> <CAAziPCm0zmEPGpWeBTRNjKzxDBEu4CFABio27--dirAhRbAJsQ@mail.gmail.com> <CAJgQjQ-sA0LotjXCRUf3cpFu8H8PnPafQzUnxbku8NneMd3z7w@mail.gmail.com> <CAAziPC=sKW99Us6FbzunM6udHbpqcSgqubbw8YfjOWGb7a7tJA@mail.gmail.com> <CAAziPCk71iw-9eYGKRPJ43ji++e62eP3dqDW+HcuBB5yLAYjAg@mail.gmail.com> <4F6FE01E-C9C7-4A80-A23A-9D64F5CDB87C@hortonworks.com> <CAAziPCmXSH2uO4EFR_QxCdd9zXE0KCh44Kpqz=vewPB0SJZqfA@mail.gmail.com> <04A800D9-74FC-460F-8A94-F923D10D5181@hortonworks.com>
Subject: Re: spark.akka.frameSize stalls job in 1.1.0
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_34611_5822565.1408380849755"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_34611_5822565.1408380849755
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I've been trying different approaches of this: populating the trie on the d=
river and serializing the instance to executors, broadcasting the strings i=
n an array and populating the trie on the executors, and variants of what I=
'm broadcasting or serializing. All approaches seem to have a memory issue.

Xiangrui has been able to run this snippet on his cluster without problems =
and we're trying to identify the difference.

- jerry


> On Aug 18, 2014, at 9:21 AM, "zhazhan [via Apache Spark Developers List]"=
 <ml-node+s1001551n7901h70@n3.nabble.com> wrote:
>=20
> Not sure exactly how you use it. My understanding is that in spark it wou=
ld be better to keep the overhead of driver as less as possible. Is it poss=
ible to broadcast trie to executors, do computation there and then aggregat=
e the counters (??) in reduct phase?=20
>=20
> Thanks.=20
>=20
> Zhan Zhang=20
>=20
> On Aug 18, 2014, at 8:54 AM, Jerry Ye <[hidden email]> wrote:=20
>=20
> > Hi Zhan,=20
> > Thanks for looking into this. I'm actually using the hash map as an exa=
mple of the simplest snippet of code that is failing for me. I know that th=
is is just the word count. In my actual problem I'm using a Trie data struc=
ture to find substring matches.=20
> >=20
> >=20
> > On Sun, Aug 17, 2014 at 11:35 PM, Zhan Zhang <[hidden email]> wrote:=20
> > Is it because countByValue or toArray put too much stress on the driver=
, if there are many unique words=20
> > To me it is a typical word count problem, then you can solve it as foll=
ows (correct me if I am wrong)=20
> >=20
> > val textFile =3D sc.textFile(=E2=80=9Cfile")=20
> > val counts =3D textFile.flatMap(line =3D> line.split(" ")).map(word =3D=
> (word, 1)).reduceByKey((a, b) =3D> a + b)=20
> > counts.saveAsTextFile(=E2=80=9Cfile=E2=80=9D)//any way you don=E2=80=99=
t want to collect results to master, and instead putting them in file.=20
> >=20
> > Thanks.=20
> >=20
> > Zhan Zhang=20
> >=20
> > On Aug 16, 2014, at 9:18 AM, Jerry Ye <[hidden email]> wrote:=20
> >=20
> > > The job ended up running overnight with no progress. :-(=20
> > >=20
> > >=20
> > > On Sat, Aug 16, 2014 at 12:16 AM, Jerry Ye <[hidden email]> wrote:=20
> > >=20
> > >> Hi Xiangrui,=20
> > >> I actually tried branch-1.1 and master and it resulted in the job be=
ing=20
> > >> stuck at the TaskSetManager:=20
> > >> 14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set =
1.0=20
> > >> with 2 tasks=20
> > >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0=
 as=20
> > >> TID 2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal=20
> > >> (PROCESS_LOCAL)=20
> > >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0=
:0 as=20
> > >> 28055875 bytes in 162 ms=20
> > >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1=
 as=20
> > >> TID 3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal=20
> > >> (PROCESS_LOCAL)=20
> > >> 14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0=
:1 as=20
> > >> 28055875 bytes in 178 ms=20
> > >>=20
> > >> It's been 10 minutes with no progress on relatively small data. I'll=
 let=20
> > >> it run overnight and update in the morning. Is there some place that=
 I=20
> > >> should look to see what is happening? I tried to ssh into the execut=
or and=20
> > >> look at /root/spark/logs but there wasn't anything informative there=
.=20
> > >>=20
> > >> I'm sure using CountByValue works fine but my use of a HashMap is on=
ly an=20
> > >> example. In my actual task, I'm loading a Trie data structure to per=
form=20
> > >> efficient string matching between a dataset of locations and strings=
=20
> > >> possibly containing mentions of locations.=20
> > >>=20
> > >> This seems like a common thing, to process input with a relatively m=
emory=20
> > >> intensive object like a Trie. I hope I'm not missing something obvio=
us. Do=20
> > >> you know of any example code like my use case?=20
> > >>=20
> > >> Thanks!=20
> > >>=20
> > >> - jerry=20
> > >>=20
> >=20
> >=20
> > --=20
> > CONFIDENTIALITY NOTICE=20
> > NOTICE: This message is intended for the use of the individual or entit=
y to=20
> > which it is addressed and may contain information that is confidential,=
=20
> > privileged and exempt from disclosure under applicable law. If the read=
er=20
> > of this message is not the intended recipient, you are hereby notified =
that=20
> > any printing, copying, dissemination, distribution, disclosure or=20
> > forwarding of this communication is strictly prohibited. If you have=20
> > received this communication in error, please contact the sender immedia=
tely=20
> > and delete it from your system. Thank You.=20
> >
>=20
>=20
> --=20
> CONFIDENTIALITY NOTICE=20
> NOTICE: This message is intended for the use of the individual or entity =
to=20
> which it is addressed and may contain information that is confidential,=
=20
> privileged and exempt from disclosure under applicable law. If the reader=
=20
> of this message is not the intended recipient, you are hereby notified th=
at=20
> any printing, copying, dissemination, distribution, disclosure or=20
> forwarding of this communication is strictly prohibited. If you have=20
> received this communication in error, please contact the sender immediate=
ly=20
> and delete it from your system. Thank You.=20
>=20
>=20
> If you reply to this email, your message will be added to the discussion =
below:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-akka-fram=
eSize-stalls-job-in-1-1-0-tp7865p7901.html
> To start a new topic under Apache Spark Developers List, email ml-node+s1=
001551n1h70@n3.nabble.com=20
> To unsubscribe from spark.akka.frameSize stalls job in 1.1.0, click here.
> NAML




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7904.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_34611_5822565.1408380849755--

From dev-return-8942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 21:48:53 2014
Return-Path: <dev-return-8942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD00C113FF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 21:48:53 +0000 (UTC)
Received: (qmail 51558 invoked by uid 500); 18 Aug 2014 21:48:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51492 invoked by uid 500); 18 Aug 2014 21:48:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51465 invoked by uid 99); 18 Aug 2014 21:48:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 21:48:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 21:48:47 +0000
Received: by mail-qg0-f53.google.com with SMTP id z60so1688668qgd.26
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 14:48:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=H6pN5/QXV0FqS/Y1Fnn4MzUpD/sfbiIOiz9sqI0+0Ns=;
        b=qDh6wv+xg8lYZVyS5jNu8uenDFp1TtXv7YNNHiXYw7h8/hW1bjHHKyLl2Mr0uD3OI0
         +Cc+/bs9MDu5FYohzKndA8JJi05S82hjwf4pXxinTa9+KKcUtNur5Qo2MkTZdAtjoNpq
         8KWse+LLyVQaQKDJl7i9hkYomM9LKfP8sBra4NsTvjad94AkxwzzWen4cMOlOSuyVuDS
         Vr/0KWqClqqR4bJOpTC8qRcX1LPU0k8uxOgaQELvtaR30T808G0wdF/I3cFgkpMcbt5T
         6/5sW0de9T1S8ptoVytIS6PW6u/WTCq1vGEpR3iLewpNQAqg1GWbMEWKR9ZL6ITHhtUr
         KHnA==
MIME-Version: 1.0
X-Received: by 10.224.69.136 with SMTP id z8mr59549116qai.60.1408398506508;
 Mon, 18 Aug 2014 14:48:26 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Mon, 18 Aug 2014 14:48:26 -0700 (PDT)
Date: Mon, 18 Aug 2014 17:48:26 -0400
Message-ID: <CAGOvqippqSNnp-SdBg6w5Tno_yqGu5ymTVXJXJhx2ZkKB59AXw@mail.gmail.com>
Subject: Spark 1.1.0 Progress
From: Gary Malouf <malouf.gary@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c28f22af34a00500ee54a7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28f22af34a00500ee54a7
Content-Type: text/plain; charset=UTF-8

I understand there must still being work done preventing the cutting of an
RC, is the specific remaining items tracked just through Jira?

--001a11c28f22af34a00500ee54a7--

From dev-return-8943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 18 22:18:21 2014
Return-Path: <dev-return-8943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20D9F1154E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Aug 2014 22:18:21 +0000 (UTC)
Received: (qmail 31342 invoked by uid 500); 18 Aug 2014 22:18:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31280 invoked by uid 500); 18 Aug 2014 22:18:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31247 invoked by uid 99); 18 Aug 2014 22:18:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 22:18:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URIBL_DBL_ABUSE_REDIR,URIBL_DBL_REDIR
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Aug 2014 22:18:15 +0000
Received: by mail-oa0-f43.google.com with SMTP id i7so4595203oag.2
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 15:17:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=VZgatA5CU0ZVlqWr+YtyYsDwByXF7qS5PHDHX8s31dw=;
        b=e/eS/ciZBjP5hhXUQ+OsWB3jnzG+jGVK7wRZg0K1zTS5/tJfd5D4tNv587fJF3GgOi
         om8yIdZCyoU9wzZVU3nEgAkWxSKz4PpGdNTE3MP1euz2/I6l9n3u9eR70c9vTp5ddWa1
         idW+yqd65jKKlLfQd0OwXr5OeJgVQYO6AAHbCt+tE10L2ZCZmMBY2H6QSVQg1uGUThB8
         pNbrwOGOB39ilU0G7QVCy1hndHH8DlSFdrLDPUs6O9G1VCWvADLK1nqbnNbl7kpoENPu
         cEwN7VL/s9XZa67KXPpnE2KxPHhRbk8/s0B/TuBoW0UYDROVcUQ/rT2yutnC+uqNKwDn
         psKQ==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr5404742oes.78.1408400275268; Mon,
 18 Aug 2014 15:17:55 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Mon, 18 Aug 2014 15:17:55 -0700 (PDT)
In-Reply-To: <CAGOvqippqSNnp-SdBg6w5Tno_yqGu5ymTVXJXJhx2ZkKB59AXw@mail.gmail.com>
References: <CAGOvqippqSNnp-SdBg6w5Tno_yqGu5ymTVXJXJhx2ZkKB59AXw@mail.gmail.com>
Date: Mon, 18 Aug 2014 15:17:55 -0700
Message-ID: <CABPQxssgbMXvSzyfe-NEYyigWOkbcHk8_PatgC==zcfNxnEpaA@mail.gmail.com>
Subject: Re: Spark 1.1.0 Progress
From: Patrick Wendell <pwendell@gmail.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c249ae1be9ef0500eebe6e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c249ae1be9ef0500eebe6e
Content-Type: text/plain; charset=ISO-8859-1

Hey Gary,

There are couple of blockers in Spark core and SQL - but we're quite close.
The goal was to have rc1 on Friday (ish) of last week... I think by tonight
I will be able to cut one. If not, I'll cut a preview release tonight that
does a full package but doesn't trigger an official vote yet so that people
can test it.

bit.ly/1tgfZrQ

- Patrick


On Mon, Aug 18, 2014 at 2:48 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> I understand there must still being work done preventing the cutting of an
> RC, is the specific remaining items tracked just through Jira?
>

--001a11c249ae1be9ef0500eebe6e--

From dev-return-8944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 05:28:02 2014
Return-Path: <dev-return-8944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8AF4E112BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 05:28:02 +0000 (UTC)
Received: (qmail 95016 invoked by uid 500); 19 Aug 2014 05:28:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94952 invoked by uid 500); 19 Aug 2014 05:28:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94911 invoked by uid 99); 19 Aug 2014 05:28:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:28:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wangfei1@huawei.com designates 119.145.14.64 as permitted sender)
Received: from [119.145.14.64] (HELO szxga01-in.huawei.com) (119.145.14.64)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:27:56 +0000
Received: from 172.24.2.119 (EHLO szxeml417-hub.china.huawei.com) ([172.24.2.119])
	by szxrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id CAM16932;
	Tue, 19 Aug 2014 13:27:33 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml417-hub.china.huawei.com
 (10.82.67.156) with Microsoft SMTP Server id 14.3.158.1; Tue, 19 Aug 2014
 13:27:31 +0800
Message-ID: <53F2E029.4080908@huawei.com>
Date: Tue, 19 Aug 2014 13:27:05 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: <dev@spark.apache.org>
Subject: Re: mvn test error
References: <53ED77FD.4030007@huawei.com>
In-Reply-To: <53ED77FD.4030007@huawei.com>
Content-Type: text/plain; charset="ISO-8859-1"; format=flowed
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

hi, all
   I notice that jenkins may also throw this error when running tests(https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18688/consoleFull).


This is because in Utils.executeAndGetOutput our progress exitCode is not 0, may be we should logWarning here rather than throw a exception?

Utils.executeAndGetOutput {
     val exitCode = process.waitFor()
     stdoutThread.join()   // Wait for it to finish reading output
     if (exitCode != 0) {
       throw new SparkException("Process " + command + " exited with code " + exitCode)
     }
}

any idea?


On 2014/8/15 11:01, scwf wrote:
> env: ubuntu 14.04 + spark master buranch
>
> mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
>
> mvn -Pyarn -Phadoop-2.4 -Phive test
>
> test error:
>
> DriverSuite:
> Spark assembly has been built with Hive, including Datanucleus jars on classpath
> - driver should exit after finishing *** FAILED ***
>    SparkException was thrown during property evaluation. (DriverSuite.scala:40)
>      Message: Process List(./bin/spark-class, org.apache.spark.DriverWithoutCleanup, local) exited with code 1
>      Occurred at table row 0 (zero based, not counting headings), which had values (
>        master = local
>      )
>
> SparkSubmitSuite:
> Spark assembly has been built with Hive, including Datanucleus jars on classpath
> - launch simple application with spark-submit *** FAILED ***
>    org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.SimpleApplicationTest, --name, testApp, --master, local, file:/tmp/1408015655220-0/testJar-1408015655220.jar) exited with code 1
>
>    at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
>    at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply$mcV$sp(SparkSubmitSuite.scala:291)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
>    at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>    at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>    at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>    at org.scalatest.Transformer.apply(Transformer.scala:22)
>    ...
> Spark assembly has been built with Hive, including Datanucleus jars on classpath
> - spark submit includes jars passed in through --jar *** FAILED ***
>    org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.JarCreationTest, --name, testApp, --master, local-cluster[2,1,512], --jars, file:/tmp/1408015659416-0/testJar-1408015659471.jar,fi
> le:/tmp/1408015659472-0/testJar-1408015659513.jar, file:/tmp/1408015659415-0/testJar-1408015659416.jar) exited with code 1
>    at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
>    at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply$mcV$sp(SparkSubmitSuite.scala:305)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
>    at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>    at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>    at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>    at org.scalatest.Transformer.apply(Transformer.scala:22)
>    ...
>
>
> but only test the specific suite as follows will be ok:
> mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=org.apache.spark.DriverSuite test
>
> it seems when run with "mvn -Pyarn -Phadoop-2.4 -Phive test",the process with Utils.executeAndGetOutput started can not exited successfully (exitcode is not zero)
>
> anyone has idea for this?
>
>
>
>


-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 05:36:22 2014
Return-Path: <dev-return-8945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 88A0C11313
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 05:36:22 +0000 (UTC)
Received: (qmail 12907 invoked by uid 500); 19 Aug 2014 05:36:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12837 invoked by uid 500); 19 Aug 2014 05:36:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12820 invoked by uid 99); 19 Aug 2014 05:36:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:36:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:35:55 +0000
Received: by mail-qg0-f43.google.com with SMTP id a108so5499843qge.16
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 22:35:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=QCOfLCpCusufPXFBrE4aVAtseh4COpp8VO1PGAOqCe8=;
        b=owTgRyN7a5s4BwqvApvfx4eZqRe8KLYCvnlve2K+Fsosvi6xe4eIIu2Y7LQ6tpo9wZ
         e35ly5ntqwHlAl+8IjWFJ+eaB9/7EkZiXVFSEve4Ju/v2k7xnDAzedmXbo+/aUFMvwk/
         nz7d4eB47mXTUfv70RBPmt/EMU4aC+i5c0G5sSUAe2ai06aZcgi/Qxe7hl765uyW544b
         ij8JhzkR3M1b4jIyvzGgJAdHE00wBveZA+d4IpayV9EueAkdVY9kgWMMd0Pn2VDEBJ0j
         LnYOywDTYcoW7SUjgyaChzkxGeg3WswAK/UmcndyxoT0VKQmY+iW4tkdGoobkJrQawop
         QQKg==
X-Received: by 10.140.38.17 with SMTP id s17mr59022338qgs.40.1408426554685;
 Mon, 18 Aug 2014 22:35:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Mon, 18 Aug 2014 22:35:34 -0700 (PDT)
In-Reply-To: <53F2E029.4080908@huawei.com>
References: <53ED77FD.4030007@huawei.com> <53F2E029.4080908@huawei.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Tue, 19 Aug 2014 13:35:34 +0800
Message-ID: <CAA_qdLr=aCiUUkC2R_ihYukZGX_dU2iCwYfKd3iLUQXhyvXDng@mail.gmail.com>
Subject: Re: mvn test error
To: scwf <wangfei1@huawei.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c132a87c03990500f4dc8d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c132a87c03990500f4dc8d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The exception indicates that the forked process doesn=E2=80=99t executed as
expected, thus the test case *should* fail.

Instead of replacing the exception with a logWarning, capturing and
printing stdout/stderr of the forked process can be helpful for diagnosis.
Currently the only information we have at hand is the process exit code,
it=E2=80=99s hard to determine the reason why the forked process fails.
=E2=80=8B


On Tue, Aug 19, 2014 at 1:27 PM, scwf <wangfei1@huawei.com> wrote:

> hi, all
>   I notice that jenkins may also throw this error when running tests(
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18688/
> consoleFull).
>
>
> This is because in Utils.executeAndGetOutput our progress exitCode is not
> 0, may be we should logWarning here rather than throw a exception?
>
> Utils.executeAndGetOutput {
>     val exitCode =3D process.waitFor()
>     stdoutThread.join()   // Wait for it to finish reading output
>     if (exitCode !=3D 0) {
>       throw new SparkException("Process " + command + " exited with code =
"
> + exitCode)
>     }
> }
>
> any idea?
>
>
>
> On 2014/8/15 11:01, scwf wrote:
>
>> env: ubuntu 14.04 + spark master buranch
>>
>> mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0 -DskipTests clea=
n
>> package
>>
>> mvn -Pyarn -Phadoop-2.4 -Phive test
>>
>> test error:
>>
>> DriverSuite:
>> Spark assembly has been built with Hive, including Datanucleus jars on
>> classpath
>> - driver should exit after finishing *** FAILED ***
>>    SparkException was thrown during property evaluation.
>> (DriverSuite.scala:40)
>>      Message: Process List(./bin/spark-class, org.apache.spark.DriverWit=
houtCleanup,
>> local) exited with code 1
>>      Occurred at table row 0 (zero based, not counting headings), which
>> had values (
>>        master =3D local
>>      )
>>
>> SparkSubmitSuite:
>> Spark assembly has been built with Hive, including Datanucleus jars on
>> classpath
>> - launch simple application with spark-submit *** FAILED ***
>>    org.apache.spark.SparkException: Process List(./bin/spark-submit,
>> --class, org.apache.spark.deploy.SimpleApplicationTest, --name, testApp,
>> --master, local, file:/tmp/1408015655220-0/testJar-1408015655220.jar)
>> exited with code 1
>>
>>    at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
>>    at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(
>> SparkSubmitSuite.scala:311)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.
>> apply$mcV$sp(SparkSubmitSuite.scala:291)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.
>> apply(SparkSubmitSuite.scala:284)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.
>> apply(SparkSubmitSuite.scala:284)
>>    at org.scalatest.Transformer$$anonfun$apply$1.apply(
>> Transformer.scala:22)
>>    at org.scalatest.Transformer$$anonfun$apply$1.apply(
>> Transformer.scala:22)
>>    at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>>    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>>    at org.scalatest.Transformer.apply(Transformer.scala:22)
>>    ...
>> Spark assembly has been built with Hive, including Datanucleus jars on
>> classpath
>> - spark submit includes jars passed in through --jar *** FAILED ***
>>    org.apache.spark.SparkException: Process List(./bin/spark-submit,
>> --class, org.apache.spark.deploy.JarCreationTest, --name, testApp,
>> --master, local-cluster[2,1,512], --jars, file:/tmp/1408015659416-0/
>> testJar-1408015659471.jar,fi
>> le:/tmp/1408015659472-0/testJar-1408015659513.jar,
>> file:/tmp/1408015659415-0/testJar-1408015659416.jar) exited with code 1
>>    at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
>>    at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(
>> SparkSubmitSuite.scala:311)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.
>> apply$mcV$sp(SparkSubmitSuite.scala:305)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.
>> apply(SparkSubmitSuite.scala:294)
>>    at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.
>> apply(SparkSubmitSuite.scala:294)
>>    at org.scalatest.Transformer$$anonfun$apply$1.apply(
>> Transformer.scala:22)
>>    at org.scalatest.Transformer$$anonfun$apply$1.apply(
>> Transformer.scala:22)
>>    at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>>    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>>    at org.scalatest.Transformer.apply(Transformer.scala:22)
>>    ...
>>
>>
>> but only test the specific suite as follows will be ok:
>> mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=3Dorg.apache.spark.Drive=
rSuite
>> test
>>
>> it seems when run with "mvn -Pyarn -Phadoop-2.4 -Phive test",the process
>> with Utils.executeAndGetOutput started can not exited successfully
>> (exitcode is not zero)
>>
>> anyone has idea for this?
>>
>>
>>
>>
>>
>
> --
>
> Best Regards
> Fei Wang
>
> ------------------------------------------------------------
> --------------------
>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c132a87c03990500f4dc8d--

From dev-return-8946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 05:49:36 2014
Return-Path: <dev-return-8946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 27BFD11356
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 05:49:36 +0000 (UTC)
Received: (qmail 39955 invoked by uid 500); 19 Aug 2014 05:49:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39889 invoked by uid 500); 19 Aug 2014 05:49:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39877 invoked by uid 99); 19 Aug 2014 05:49:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:49:35 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 05:49:08 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so5389678qgf.20
        for <dev@spark.apache.org>; Mon, 18 Aug 2014 22:49:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JoNELlpA68mlbuTOqf2e6h0COMjmSoROb6NZ/TVXuUc=;
        b=aEfFX0NXUi7j3GfFTXUx1IuMMfNNMvsCWaaMZWvhnjBM/BeRY15K30pSjVnHq+irS4
         P3nLiLLQmAka36W8iU4FJeWodJgUBbgZxErIWjBcA+dEuWLgjKvWxEnBy+5SX4Q8tAs9
         SX+7vMWM547WyS5XTbHiKmacjAW4OB0LVYBA2GIJSljbfOHx8L0veUuKX6PnKg76FKwm
         KSe/Ml7zC8PkKNNVLZM/+rIPfLBVX4CoWFC3NSY4aq2G1WP/T4P5KOebigjJY2pUDosC
         yfryIzqnGImnx6LSGA2F3D0k7cKd+kqR1fMYtpdJbQhZfUaxqBWLve9IpnonaBLl28rk
         0yzg==
MIME-Version: 1.0
X-Received: by 10.229.229.135 with SMTP id ji7mr63564717qcb.15.1408427347640;
 Mon, 18 Aug 2014 22:49:07 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Mon, 18 Aug 2014 22:49:07 -0700 (PDT)
Date: Mon, 18 Aug 2014 22:49:07 -0700
Message-ID: <CA+B-+fzg_m1NG_y3rvobytP-TxdaSSqw1c0M3PhRTSJT7OafGQ@mail.gmail.com>
Subject: Spark on YARN webui
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11345e44bfa4f90500f50bb0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11345e44bfa4f90500f50bb0
Content-Type: text/plain; charset=UTF-8

Hi,

We are running the snapshots (new spark features) on YARN and I was
wondering if the webui is available on YARN mode...

The deployment document does not mention webui on YARN mode...

Is it available ?

Thanks.
Deb

--001a11345e44bfa4f90500f50bb0--

From dev-return-8947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 06:56:51 2014
Return-Path: <dev-return-8947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EEB141156E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 06:56:51 +0000 (UTC)
Received: (qmail 79853 invoked by uid 500); 19 Aug 2014 06:56:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79770 invoked by uid 500); 19 Aug 2014 06:56:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79758 invoked by uid 99); 19 Aug 2014 06:56:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 06:56:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wangfei1@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 06:56:46 +0000
Received: from 172.24.2.119 (EHLO szxeml419-hub.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BYJ03803;
	Tue, 19 Aug 2014 14:56:19 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml419-hub.china.huawei.com
 (10.82.67.158) with Microsoft SMTP Server id 14.3.158.1; Tue, 19 Aug 2014
 14:56:13 +0800
Message-ID: <53F2F4F6.8000803@huawei.com>
Date: Tue, 19 Aug 2014 14:55:50 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: Cheng Lian <lian.cs.zju@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: mvn test error
References: <53ED77FD.4030007@huawei.com> <53F2E029.4080908@huawei.com> <CAA_qdLr=aCiUUkC2R_ihYukZGX_dU2iCwYfKd3iLUQXhyvXDng@mail.gmail.com>
In-Reply-To: <CAA_qdLr=aCiUUkC2R_ihYukZGX_dU2iCwYfKd3iLUQXhyvXDng@mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"; format=flowed
Content-Transfer-Encoding: 8bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

hi,Cheng Lian
   thanks, printing stdout/stderr of the forked process is more reasonable.

On 2014/8/19 13:35, Cheng Lian wrote:
> The exception indicates that the forked process doesnt executed as expected, thus the test case /should/ fail.
>
> Instead of replacing the exception with a |logWarning|, capturing and printing stdout/stderr of the forked process can be helpful for diagnosis. Currently the only information we have at hand is the process exit code, its hard to determine the reason why the forked process fails.
>
> 
>
>
> On Tue, Aug 19, 2014 at 1:27 PM, scwf <wangfei1@huawei.com <mailto:wangfei1@huawei.com>> wrote:
>
>     hi, all
>        I notice that jenkins may also throw this error when running tests(https://amplab.cs.__berkeley.edu/jenkins/job/__SparkPullRequestBuilder/18688/__consoleFull <https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18688/consoleFull>).
>
>
>     This is because in Utils.executeAndGetOutput our progress exitCode is not 0, may be we should logWarning here rather than throw a exception?
>
>     Utils.executeAndGetOutput {
>          val exitCode = process.waitFor()
>          stdoutThread.join()   // Wait for it to finish reading output
>          if (exitCode != 0) {
>            throw new SparkException("Process " + command + " exited with code " + exitCode)
>          }
>     }
>
>     any idea?
>
>
>
>     On 2014/8/15 11:01, scwf wrote:
>
>         env: ubuntu 14.04 + spark master buranch
>
>         mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
>
>         mvn -Pyarn -Phadoop-2.4 -Phive test
>
>         test error:
>
>         DriverSuite:
>         Spark assembly has been built with Hive, including Datanucleus jars on classpath
>         - driver should exit after finishing *** FAILED ***
>             SparkException was thrown during property evaluation. (DriverSuite.scala:40)
>               Message: Process List(./bin/spark-class, org.apache.spark.__DriverWithoutCleanup, local) exited with code 1
>               Occurred at table row 0 (zero based, not counting headings), which had values (
>                 master = local
>               )
>
>         SparkSubmitSuite:
>         Spark assembly has been built with Hive, including Datanucleus jars on classpath
>         - launch simple application with spark-submit *** FAILED ***
>             org.apache.spark.__SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.__SimpleApplicationTest, --name, testApp, --master, local, file:/tmp/1408015655220-0/__testJar-1408015655220.jar) exited with code 1
>
>             at org.apache.spark.util.Utils$.__executeAndGetOutput(Utils.__scala:810)
>             at org.apache.spark.deploy.__SparkSubmitSuite.__runSparkSubmit(__SparkSubmitSuite.scala:311)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14.__apply$mcV$sp(SparkSubmitSuite.__scala:291)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14.__apply(SparkSubmitSuite.scala:__284)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14.__apply(SparkSubmitSuite.scala:__284)
>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__Transformer.scala:22)
>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__Transformer.scala:22)
>             at org.scalatest.OutcomeOf$class.__outcomeOf(OutcomeOf.scala:85)
>             at org.scalatest.OutcomeOf$.__outcomeOf(OutcomeOf.scala:104)
>             at org.scalatest.Transformer.__apply(Transformer.scala:22)
>             ...
>         Spark assembly has been built with Hive, including Datanucleus jars on classpath
>         - spark submit includes jars passed in through --jar *** FAILED ***
>             org.apache.spark.__SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.__JarCreationTest, --name, testApp, --master, local-cluster[2,1,512], --jars, file:/tmp/1408015659416-0/__testJar-1408015659471.jar,fi
>         le:/tmp/1408015659472-0/__testJar-1408015659513.jar, file:/tmp/1408015659415-0/__testJar-1408015659416.jar) exited with code 1
>             at org.apache.spark.util.Utils$.__executeAndGetOutput(Utils.__scala:810)
>             at org.apache.spark.deploy.__SparkSubmitSuite.__runSparkSubmit(__SparkSubmitSuite.scala:311)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15.__apply$mcV$sp(SparkSubmitSuite.__scala:305)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15.__apply(SparkSubmitSuite.scala:__294)
>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15.__apply(SparkSubmitSuite.scala:__294)
>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__Transformer.scala:22)
>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__Transformer.scala:22)
>             at org.scalatest.OutcomeOf$class.__outcomeOf(OutcomeOf.scala:85)
>             at org.scalatest.OutcomeOf$.__outcomeOf(OutcomeOf.scala:104)
>             at org.scalatest.Transformer.__apply(Transformer.scala:22)
>             ...
>
>
>         but only test the specific suite as follows will be ok:
>         mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=org.apache.__spark.DriverSuite test
>
>         it seems when run with "mvn -Pyarn -Phadoop-2.4 -Phive test",the process with Utils.executeAndGetOutput started can not exited successfully (exitcode is not zero)
>
>         anyone has idea for this?
>
>
>
>
>
>
>     --
>
>     Best Regards
>     Fei Wang
>
>     ------------------------------__------------------------------__--------------------
>
>
>
>     ------------------------------__------------------------------__---------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.__org <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>
>
>


-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 07:20:26 2014
Return-Path: <dev-return-8948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC20E11603
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 07:20:26 +0000 (UTC)
Received: (qmail 15205 invoked by uid 500); 19 Aug 2014 07:20:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15133 invoked by uid 500); 19 Aug 2014 07:20:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15119 invoked by uid 99); 19 Aug 2014 07:20:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 07:20:25 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 07:19:59 +0000
Received: by mail-qc0-f181.google.com with SMTP id x13so5998515qcv.12
        for <dev@spark.apache.org>; Tue, 19 Aug 2014 00:19:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8UDtDMvAH4GEp3SFtaonrDcaQhFdmHyoCmSMOPLMhxo=;
        b=i5WBH/JBiLFaDLr4IeD/oWm8Hg+fOwvr1rXiEmPx4BuJTsy+n9o9NcMZnX6oSQREJ7
         C5Mzj1i7ZY4xVOB2WyK9qx7edgyeEAEe2h3o43jHahekLHvuMTEJW7M6h9W/eARO4sDl
         Bnbm3uc0Y8eH1Fg6JMuWsSgD7sXTEygAphdnCIzUVBzBa8na2w6j07UE473TKRGONyMp
         VJNG4Mfw6VQf7XP66E/otiqzq+xx2EUExGy2guiqcQwMKxkke6puEecqCAaAdch4jQBj
         orbrkSeC6EljNHRrAXseDlqmSn5ArmaaHIosq8MJPo7PKD1pm7I+IZXkfPseKIwaF244
         ZK6A==
MIME-Version: 1.0
X-Received: by 10.140.44.67 with SMTP id f61mr48151288qga.44.1408432798386;
 Tue, 19 Aug 2014 00:19:58 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Tue, 19 Aug 2014 00:19:58 -0700 (PDT)
In-Reply-To: <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com>
References: <CABpRO2cPWMXpuVoRp9xV8Q0ZpMNcBTOF+veHsX7ZoK06vVeN6w@mail.gmail.com>
	<CAPh_B=a5b=ASjP19zLH=ax5By_N5n3zgLu_GDY1xJFt3YSkd8g@mail.gmail.com>
	<CABpRO2fXuhtXvGuQNT=gYykQA5Ytb4DMar3xyW9BmttiJzV5Pg@mail.gmail.com>
	<CABpRO2ds3CFY_hnP7kfOd+gQbR5y4sc4k-_Q=zSZWkO_kQV-tw@mail.gmail.com>
	<CABpRO2erYOMVsvG1HADy1xeQ_OnRVWPVQzHNA3PWPyyuVj7Sjw@mail.gmail.com>
	<CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com>
	<CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com>
	<CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com>
	<CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com>
	<CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com>
	<CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com>
	<CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com>
	<CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com>
	<CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com>
Date: Tue, 19 Aug 2014 00:19:58 -0700
Message-ID: <CA+B-+fzkk=RMbCT7m6veQwCWGq1Jn+-zegXD23sQwa91eSyVvw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Debasish Das <debasish.das83@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Graham Dennis <graham.dennis@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113943d8a355640500f650b9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113943d8a355640500f650b9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

@rxin With the fixes, I could run it fine on top of branch-1.0

On master when running using YARN I am getting another KryoException:

Exception in thread "main" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 247 in stage 52.0 failed 4 times, most recent
failure: Lost task 247.3 in stage 52.0 (TID 10010,
tblpmidn05adv-hdp.tdc.vzwcorp.com):
com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException

Serialization trace:

shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)


com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(Fiel=
dSerializer.java:626)


com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.=
java:221)

        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)

OutLinkBlock does not extend Serializable...Also I did not see this failure
before..

Was the fix tested on YARN ?

@dbtsai did your assembly on YARN ran fine or you are still noticing these
exceptions ?

Thanks.

Deb


On Thu, Aug 14, 2014 at 5:48 PM, Reynold Xin <rxin@databricks.com> wrote:

> Here: https://github.com/apache/spark/pull/1948
>
>
>
> On Thu, Aug 14, 2014 at 5:45 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Is there a fix that I can test ? I have the flows setup for both
>> standalone and YARN runs...
>>
>> Thanks.
>> Deb
>>
>>
>>
>> On Thu, Aug 14, 2014 at 10:59 AM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>
>>> Yes, I understand it might not work for custom serializer, but that is =
a
>>> much less common path.
>>>
>>> Basically I want a quick fix for 1.1 release (which is coming up soon).
>>> I would not be comfortable making big changes to class path late into t=
he
>>> release cycle. We can do that for 1.2.
>>>
>>>
>>>
>>>
>>>
>>> On Thu, Aug 14, 2014 at 2:35 AM, Graham Dennis <graham.dennis@gmail.com=
>
>>> wrote:
>>>
>>>> That should work, but would you also make these changes to the
>>>> JavaSerializer?  The API of these is the same so that you can select o=
ne or
>>>> the other (or in theory a custom serializer)?  This also wouldn't addr=
ess
>>>> the problem of shipping custom *serializers* (not kryo registrators) i=
n
>>>> user jars.
>>>>
>>>> On 14 August 2014 19:23, Reynold Xin <rxin@databricks.com> wrote:
>>>>
>>>>> Graham,
>>>>>
>>>>> SparkEnv only creates a KryoSerializer, but as I understand that
>>>>> serializer doesn't actually initializes the registrator since that is=
 only
>>>>> called when newKryo() is called when KryoSerializerInstance is initia=
lized.
>>>>>
>>>>> Basically I'm thinking a quick fix for 1.2:
>>>>>
>>>>> 1. Add a classLoader field to KryoSerializer; initialize new
>>>>> KryoSerializerInstance with that class loader
>>>>>
>>>>>  2. Set that classLoader to the executor's class loader when Executor
>>>>> is initialized.
>>>>>
>>>>> Then all deser calls should be using the executor's class loader.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Thu, Aug 14, 2014 at 12:53 AM, Graham Dennis <
>>>>> graham.dennis@gmail.com> wrote:
>>>>>
>>>>>> Hi Reynold,
>>>>>>
>>>>>> That would solve this specific issue, but you'd need to be careful
>>>>>> that you never created a serialiser instance before the first task i=
s
>>>>>> received.  Currently in Executor.TaskRunner.run a closure serialiser
>>>>>> instance is created before any application jars are downloaded, but =
that
>>>>>> could be moved.  To me, this seems a little fragile.
>>>>>>
>>>>>> However there is a related issue where you can't ship a custom
>>>>>> serialiser in an application jar because the serialiser is instantia=
ted
>>>>>> when the SparkEnv object is created, which is before any tasks are r=
eceived
>>>>>> by the executor.  The above approach wouldn't help with this problem=
.
>>>>>>  Additionally, the YARN scheduler currently uses this approach of ad=
ding
>>>>>> the application jar to the Executor classpath, so it would make thin=
gs a
>>>>>> bit more uniform.
>>>>>>
>>>>>> Cheers,
>>>>>> Graham
>>>>>>
>>>>>>
>>>>>> On 14 August 2014 17:37, Reynold Xin <rxin@databricks.com> wrote:
>>>>>>
>>>>>>> Graham,
>>>>>>>
>>>>>>> Thanks for working on this. This is an important bug to fix.
>>>>>>>
>>>>>>>  I don't have the whole context and obviously I haven't spent
>>>>>>> nearly as much time on this as you have, but I'm wondering what if =
we
>>>>>>> always pass the executor's ClassLoader to the Kryo serializer? Will=
 that
>>>>>>> solve this problem?
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Wed, Aug 13, 2014 at 11:59 PM, Graham Dennis <
>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi Deb,
>>>>>>>>
>>>>>>>> The only alternative serialiser is the JavaSerialiser (the
>>>>>>>> default).  Theoretically Spark supports custom serialisers, but du=
e to a
>>>>>>>> related issue, custom serialisers currently can't live in applicat=
ion jars
>>>>>>>> and must be available to all executors at launch.  My PR fixes thi=
s issue
>>>>>>>> as well, allowing custom serialisers to be shipped in application =
jars.
>>>>>>>>
>>>>>>>> Graham
>>>>>>>>
>>>>>>>>
>>>>>>>> On 14 August 2014 16:56, Debasish Das <debasish.das83@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> Sorry I just saw Graham's email after sending my previous email
>>>>>>>>> about this bug...
>>>>>>>>>
>>>>>>>>> I have been seeing this same issue on our ALS runs last week but =
I
>>>>>>>>> thought it was due my hacky way to run mllib 1.1 snapshot on core=
 1.0...
>>>>>>>>>
>>>>>>>>> What's the status of this PR ? Will this fix be back-ported to
>>>>>>>>> 1.0.1 as we are running 1.0.1 stable standalone cluster ?
>>>>>>>>>
>>>>>>>>> Till the PR merges does it make sense to not use Kryo ? What are
>>>>>>>>> the other recommended efficient serializers ?
>>>>>>>>>
>>>>>>>>> Thanks.
>>>>>>>>> Deb
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Wed, Aug 13, 2014 at 2:47 PM, Graham Dennis <
>>>>>>>>> graham.dennis@gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> I now have a complete pull request for this issue that I'd like
>>>>>>>>>> to get
>>>>>>>>>> reviewed and committed.  The PR is available here:
>>>>>>>>>> https://github.com/apache/spark/pull/1890 and includes a
>>>>>>>>>> testcase for the
>>>>>>>>>> issue I described.  I've also submitted a related PR (
>>>>>>>>>> https://github.com/apache/spark/pull/1827) that causes
>>>>>>>>>> exceptions raised
>>>>>>>>>> while attempting to run the custom kryo registrator not to be
>>>>>>>>>> swallowed.
>>>>>>>>>>
>>>>>>>>>> Thanks,
>>>>>>>>>> Graham
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 12 August 2014 18:44, Graham Dennis <graham.dennis@gmail.com>
>>>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>> > I've submitted a work-in-progress pull request for this issue
>>>>>>>>>> that I'd
>>>>>>>>>> > like feedback on.  See
>>>>>>>>>> https://github.com/apache/spark/pull/1890 . I've
>>>>>>>>>> > also submitted a pull request for the related issue that the
>>>>>>>>>> exceptions hit
>>>>>>>>>> > when trying to use a custom kryo registrator are being
>>>>>>>>>> swallowed:
>>>>>>>>>> > https://github.com/apache/spark/pull/1827
>>>>>>>>>> >
>>>>>>>>>> > The approach in my pull request is to get the Worker processes
>>>>>>>>>> to download
>>>>>>>>>> > the application jars and add them to the Executor class path a=
t
>>>>>>>>>> launch
>>>>>>>>>> > time. There are a couple of things that still need to be done
>>>>>>>>>> before this
>>>>>>>>>> > can be merged:
>>>>>>>>>> > 1. At the moment, the first time a task runs in the executor,
>>>>>>>>>> the
>>>>>>>>>> > application jars are downloaded again.  My solution here would
>>>>>>>>>> be to make
>>>>>>>>>> > the executor not download any jars that already exist.
>>>>>>>>>>  Previously, the
>>>>>>>>>> > driver & executor kept track of the timestamp of jar files and
>>>>>>>>>> would
>>>>>>>>>> > redownload 'updated' jars, however this never made sense as th=
e
>>>>>>>>>> previous
>>>>>>>>>> > version of the updated jar may have already been loaded into
>>>>>>>>>> the executor,
>>>>>>>>>> > so the updated jar may have no effect.  As my current pull
>>>>>>>>>> request removes
>>>>>>>>>> > the timestamp for jars, just checking whether the jar exists
>>>>>>>>>> will allow us
>>>>>>>>>> > to avoid downloading the jars again.
>>>>>>>>>> > 2. Tests. :-)
>>>>>>>>>> >
>>>>>>>>>> > A side-benefit of my pull request is that you will be able to
>>>>>>>>>> use custom
>>>>>>>>>> > serialisers that are distributed in a user jar.  Currently, th=
e
>>>>>>>>>> serialiser
>>>>>>>>>> > instance is created in the Executor process before the first
>>>>>>>>>> task is
>>>>>>>>>> > received and therefore before any user jars are downloaded.  A=
s
>>>>>>>>>> this PR
>>>>>>>>>> > adds user jars to the Executor process at launch time, this
>>>>>>>>>> won't be an
>>>>>>>>>> > issue.
>>>>>>>>>> >
>>>>>>>>>> >
>>>>>>>>>> > On 7 August 2014 12:01, Graham Dennis <graham.dennis@gmail.com=
>
>>>>>>>>>> wrote:
>>>>>>>>>> >
>>>>>>>>>> >> See my comment on
>>>>>>>>>> https://issues.apache.org/jira/browse/SPARK-2878 for
>>>>>>>>>> >> the full stacktrace, but it's in the
>>>>>>>>>> BlockManager/BlockManagerWorker where
>>>>>>>>>> >> it's trying to fulfil a "getBlock" request for another node.
>>>>>>>>>>  The objects
>>>>>>>>>> >> that would be in the block haven't yet been serialised, and
>>>>>>>>>> that then
>>>>>>>>>> >> causes the deserialisation to happen on that thread.  See
>>>>>>>>>> >> MemoryStore.scala:102.
>>>>>>>>>> >>
>>>>>>>>>> >>
>>>>>>>>>> >> On 7 August 2014 11:53, Reynold Xin <rxin@databricks.com>
>>>>>>>>>> wrote:
>>>>>>>>>> >>
>>>>>>>>>> >>> I don't think it was a conscious design decision to not
>>>>>>>>>> include the
>>>>>>>>>> >>> application classes in the connection manager serializer. We
>>>>>>>>>> should fix
>>>>>>>>>> >>> that. Where is it deserializing data in that thread?
>>>>>>>>>> >>>
>>>>>>>>>> >>>  4 might make sense in the long run, but it adds a lot of
>>>>>>>>>> complexity to
>>>>>>>>>> >>> the code base (whole separate code base, task queue,
>>>>>>>>>> blocking/non-blocking
>>>>>>>>>> >>> logic within task threads) that can be error prone, so I
>>>>>>>>>> think it is best
>>>>>>>>>> >>> to stay away from that right now.
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>> On Wed, Aug 6, 2014 at 6:47 PM, Graham Dennis <
>>>>>>>>>> graham.dennis@gmail.com>
>>>>>>>>>> >>> wrote:
>>>>>>>>>> >>>
>>>>>>>>>> >>>> Hi Spark devs,
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I=E2=80=99ve posted an issue on JIRA (
>>>>>>>>>> >>>> https://issues.apache.org/jira/browse/SPARK-2878) which
>>>>>>>>>> occurs when
>>>>>>>>>> >>>> using
>>>>>>>>>> >>>> Kryo serialisation with a custom Kryo registrator to
>>>>>>>>>> register custom
>>>>>>>>>> >>>> classes with Kryo.  This is an insidious issue that
>>>>>>>>>> >>>> non-deterministically
>>>>>>>>>> >>>> causes Kryo to have different ID number =3D> class name map=
s
>>>>>>>>>> on different
>>>>>>>>>> >>>> nodes, which then causes weird exceptions
>>>>>>>>>> (ClassCastException,
>>>>>>>>>> >>>> ClassNotFoundException, ArrayIndexOutOfBoundsException) at
>>>>>>>>>> >>>> deserialisation
>>>>>>>>>> >>>> time.  I=E2=80=99ve created a reliable reproduction for the=
 issue
>>>>>>>>>> here:
>>>>>>>>>> >>>> https://github.com/GrahamDennis/spark-kryo-serialisation
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I=E2=80=99m happy to try and put a pull request together to=
 try and
>>>>>>>>>> address
>>>>>>>>>> >>>> this,
>>>>>>>>>> >>>> but it=E2=80=99s not obvious to me the right way to solve t=
his and
>>>>>>>>>> I=E2=80=99d like to
>>>>>>>>>> >>>> get
>>>>>>>>>> >>>> feedback / ideas on how to address this.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> The root cause of the problem is a "Failed to run
>>>>>>>>>> >>>> spark.kryo.registrator=E2=80=9D
>>>>>>>>>> >>>> error which non-deterministically occurs in some executor
>>>>>>>>>> processes
>>>>>>>>>> >>>> during
>>>>>>>>>> >>>> operation.  My custom Kryo registrator is in the applicatio=
n
>>>>>>>>>> jar, and
>>>>>>>>>> >>>> it is
>>>>>>>>>> >>>> accessible on the worker nodes.  This is demonstrated by th=
e
>>>>>>>>>> fact that
>>>>>>>>>> >>>> most
>>>>>>>>>> >>>> of the time the custom kryo registrator is successfully run=
.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> What=E2=80=99s happening is that Kryo serialisation/deseria=
lisation
>>>>>>>>>> is happening
>>>>>>>>>> >>>> most of the time on an =E2=80=9CExecutor task launch worker=
=E2=80=9D thread,
>>>>>>>>>> which has
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> thread's class loader set to contain the application jar.
>>>>>>>>>>  This happens
>>>>>>>>>> >>>> in
>>>>>>>>>> >>>> `org.apache.spark.executor.Executor.TaskRunner.run`, and
>>>>>>>>>> from what I can
>>>>>>>>>> >>>> tell, it is only these threads that have access to the
>>>>>>>>>> application jar
>>>>>>>>>> >>>> (that contains the custom Kryo registrator).  However, the
>>>>>>>>>> >>>> ConnectionManager threads sometimes need to
>>>>>>>>>> serialise/deserialise
>>>>>>>>>> >>>> objects
>>>>>>>>>> >>>> to satisfy =E2=80=9CgetBlock=E2=80=9D requests when the obj=
ects haven=E2=80=99t
>>>>>>>>>> previously been
>>>>>>>>>> >>>> serialised.  As the ConnectionManager threads don=E2=80=99t=
 have the
>>>>>>>>>> application
>>>>>>>>>> >>>> jar available from their class loader, when it tries to loo=
k
>>>>>>>>>> up the
>>>>>>>>>> >>>> custom
>>>>>>>>>> >>>> Kryo registrator, this fails.  Spark then swallows this
>>>>>>>>>> exception, which
>>>>>>>>>> >>>> results in a different ID number =E2=80=94> class mapping f=
or this
>>>>>>>>>> kryo
>>>>>>>>>> >>>> instance,
>>>>>>>>>> >>>> and this then causes deserialisation errors later on a
>>>>>>>>>> different node.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> A related issue to the issue reported in SPARK-2878 is that
>>>>>>>>>> Spark
>>>>>>>>>> >>>> probably
>>>>>>>>>> >>>> shouldn=E2=80=99t swallow the ClassNotFound exception for c=
ustom Kryo
>>>>>>>>>> >>>> registrators.
>>>>>>>>>> >>>>  The user has explicitly specified this class, and if it
>>>>>>>>>> >>>> deterministically
>>>>>>>>>> >>>> can=E2=80=99t be found, then it may cause problems at seria=
lisation /
>>>>>>>>>> >>>> deserialisation time.  If only sometimes it can=E2=80=99t b=
e found
>>>>>>>>>> (as in this
>>>>>>>>>> >>>> case), then it leads to a data corruption issue later on.
>>>>>>>>>>  Either way,
>>>>>>>>>> >>>> we=E2=80=99re better off dying due to the ClassNotFound exc=
eption
>>>>>>>>>> earlier, than
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> weirder errors later on.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> I have some ideas on potential solutions to this issue, but
>>>>>>>>>> I=E2=80=99m keen for
>>>>>>>>>> >>>> experienced eyes to critique these approaches:
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> 1. The simplest approach to fixing this would be to just
>>>>>>>>>> make the
>>>>>>>>>> >>>> application jar available to the connection manager threads=
,
>>>>>>>>>> but I=E2=80=99m
>>>>>>>>>> >>>> guessing it=E2=80=99s a design decision to isolate the appl=
ication
>>>>>>>>>> jar to just
>>>>>>>>>> >>>> the
>>>>>>>>>> >>>> executor task runner threads.  Also, I don=E2=80=99t know i=
f there
>>>>>>>>>> are any other
>>>>>>>>>> >>>> threads that might be interacting with kryo serialisation /
>>>>>>>>>> >>>> deserialisation.
>>>>>>>>>> >>>> 2. Before looking up the custom Kryo registrator, change th=
e
>>>>>>>>>> thread=E2=80=99s
>>>>>>>>>> >>>> class
>>>>>>>>>> >>>> loader to include the application jar, then restore the
>>>>>>>>>> class loader
>>>>>>>>>> >>>> after
>>>>>>>>>> >>>> the kryo registrator has been run.  I don=E2=80=99t know if=
 this
>>>>>>>>>> would have any
>>>>>>>>>> >>>> other side-effects.
>>>>>>>>>> >>>> 3. Always serialise / deserialise on the existing TaskRunne=
r
>>>>>>>>>> threads,
>>>>>>>>>> >>>> rather than delaying serialisation until later, when it can
>>>>>>>>>> be done
>>>>>>>>>> >>>> only if
>>>>>>>>>> >>>> needed.  This approach would probably have negative
>>>>>>>>>> performance
>>>>>>>>>> >>>> consequences.
>>>>>>>>>> >>>> 4. Create a new dedicated thread pool for lazy serialisatio=
n
>>>>>>>>>> /
>>>>>>>>>> >>>> deserialisation that has the application jar on the class
>>>>>>>>>> path.
>>>>>>>>>> >>>>  Serialisation / deserialisation would be the only thing
>>>>>>>>>> these threads
>>>>>>>>>> >>>> do,
>>>>>>>>>> >>>> and this would minimise conflicts / interactions between th=
e
>>>>>>>>>> application
>>>>>>>>>> >>>> jar and other jars.
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> #4 sounds like the best approach to me, but I think would
>>>>>>>>>> require
>>>>>>>>>> >>>> considerable knowledge of Spark internals, which is beyond
>>>>>>>>>> me at
>>>>>>>>>> >>>> present.
>>>>>>>>>> >>>>  Does anyone have any better (and ideally simpler) ideas?
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> Cheers,
>>>>>>>>>> >>>>
>>>>>>>>>> >>>> Graham
>>>>>>>>>> >>>>
>>>>>>>>>> >>>
>>>>>>>>>> >>>
>>>>>>>>>> >>
>>>>>>>>>> >
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a113943d8a355640500f650b9--

From dev-return-8949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 07:42:59 2014
Return-Path: <dev-return-8949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C0AA116B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 07:42:59 +0000 (UTC)
Received: (qmail 55216 invoked by uid 500); 19 Aug 2014 07:42:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55170 invoked by uid 500); 19 Aug 2014 07:42:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55086 invoked by uid 99); 19 Aug 2014 07:42:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 07:42:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 07:42:41 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so5521191qgf.7
        for <dev@spark.apache.org>; Tue, 19 Aug 2014 00:42:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=xvZvM6hmggHH3aJ8agqP7nccEfARMtszRjAppEPI/VA=;
        b=Ki2FsRSFeictZyBUcXoXwuABBhrtu7R/pMc2jagF7sTM5N1WDSX2Jl7o7arX/GM1Nf
         4kp17SepM/Zs9NfDUjV4kq3IYFamIf8b0wmkzuY9nGqqa9Uj3DO+BsQTC6jGy7DTUEf/
         9iNioD5gnmnokzHGQTB2oY5Uk8dtVi9vKrFxwGyYExibkLhxt1f3BXZ8KoVZWwxkX4OW
         fJUS7cLZ6h2jRwS8pWASB0pPn5ZTeQWJe/0h2mYLheVeza2o8X90QfJmCCvI2++J6S0X
         Ux5P4ynqOQjgiUX8ACXtHdcz42XXLWCLJKq40Ncy8XzVURxQ/DXqkaGBxC7eWXhcHPCR
         ikZg==
X-Received: by 10.224.37.134 with SMTP id x6mr65449308qad.39.1408434140907;
 Tue, 19 Aug 2014 00:42:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Tue, 19 Aug 2014 00:42:00 -0700 (PDT)
In-Reply-To: <53F2F4F6.8000803@huawei.com>
References: <53ED77FD.4030007@huawei.com> <53F2E029.4080908@huawei.com>
 <CAA_qdLr=aCiUUkC2R_ihYukZGX_dU2iCwYfKd3iLUQXhyvXDng@mail.gmail.com> <53F2F4F6.8000803@huawei.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Tue, 19 Aug 2014 15:42:00 +0800
Message-ID: <CAA_qdLrf_wmwGhFCUG4SeQbDA_yLEWzPdm1FtLEG9DbqRNawNg@mail.gmail.com>
Subject: Re: mvn test error
To: scwf <wangfei1@huawei.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1f4a2a8932d0500f6a08f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f4a2a8932d0500f6a08f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Just FYI, thought this might be helpful, I'm refactoring Hive Thrift server
test suites. These suites also fork new processes and suffer similar
issues. Stdout and stderr of forked processes are logged in the new version
of test suites with utilities under scala.sys.process package
https://github.com/apache/spark/pull/1856/files


On Tue, Aug 19, 2014 at 2:55 PM, scwf <wangfei1@huawei.com> wrote:

> hi,Cheng Lian
>   thanks, printing stdout/stderr of the forked process is more reasonable=
.
>
> On 2014/8/19 13:35, Cheng Lian wrote:
>
>> The exception indicates that the forked process doesn=E2=80=99t executed=
 as
>> expected, thus the test case /should/ fail.
>>
>> Instead of replacing the exception with a |logWarning|, capturing and
>> printing stdout/stderr of the forked process can be helpful for diagnosi=
s.
>> Currently the only information we have at hand is the process exit code,
>> it=E2=80=99s hard to determine the reason why the forked process fails.
>>
>>
>> =E2=80=8B
>>
>>
>> On Tue, Aug 19, 2014 at 1:27 PM, scwf <wangfei1@huawei.com <mailto:
>> wangfei1@huawei.com>> wrote:
>>
>>     hi, all
>>        I notice that jenkins may also throw this error when running test=
s(
>> https://amplab.cs.__berkeley.edu/jenkins/job/__
>> SparkPullRequestBuilder/18688/__consoleFull <https://amplab.cs.berkeley.
>> edu/jenkins/job/SparkPullRequestBuilder/18688/consoleFull>).
>>
>>
>>
>>     This is because in Utils.executeAndGetOutput our progress exitCode i=
s
>> not 0, may be we should logWarning here rather than throw a exception?
>>
>>     Utils.executeAndGetOutput {
>>          val exitCode =3D process.waitFor()
>>          stdoutThread.join()   // Wait for it to finish reading output
>>          if (exitCode !=3D 0) {
>>            throw new SparkException("Process " + command + " exited with
>> code " + exitCode)
>>          }
>>     }
>>
>>     any idea?
>>
>>
>>
>>     On 2014/8/15 11:01, scwf wrote:
>>
>>         env: ubuntu 14.04 + spark master buranch
>>
>>         mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=3D2.4.0 -DskipTe=
sts
>> clean package
>>
>>         mvn -Pyarn -Phadoop-2.4 -Phive test
>>
>>         test error:
>>
>>         DriverSuite:
>>         Spark assembly has been built with Hive, including Datanucleus
>> jars on classpath
>>         - driver should exit after finishing *** FAILED ***
>>             SparkException was thrown during property evaluation.
>> (DriverSuite.scala:40)
>>               Message: Process List(./bin/spark-class, org.apache.spark.=
__DriverWithoutCleanup,
>> local) exited with code 1
>>
>>               Occurred at table row 0 (zero based, not counting
>> headings), which had values (
>>                 master =3D local
>>               )
>>
>>         SparkSubmitSuite:
>>         Spark assembly has been built with Hive, including Datanucleus
>> jars on classpath
>>         - launch simple application with spark-submit *** FAILED ***
>>             org.apache.spark.__SparkException: Process
>> List(./bin/spark-submit, --class, org.apache.spark.deploy.__SimpleApplic=
ationTest,
>> --name, testApp, --master, local, file:/tmp/1408015655220-0/__testJar-14=
08015655220.jar)
>> exited with code 1
>>
>>             at org.apache.spark.util.Utils$._
>> _executeAndGetOutput(Utils.__scala:810)
>>             at org.apache.spark.deploy.__SparkSubmitSuite.__
>> runSparkSubmit(__SparkSubmitSuite.scala:311)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14._
>> _apply$mcV$sp(SparkSubmitSuite.__scala:291)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14._
>> _apply(SparkSubmitSuite.scala:__284)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$14._
>> _apply(SparkSubmitSuite.scala:__284)
>>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__
>> Transformer.scala:22)
>>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__
>> Transformer.scala:22)
>>             at org.scalatest.OutcomeOf$class.__outcomeOf(OutcomeOf.scala=
:
>> 85)
>>             at org.scalatest.OutcomeOf$.__outcomeOf(OutcomeOf.scala:104)
>>             at org.scalatest.Transformer.__apply(Transformer.scala:22)
>>
>>             ...
>>         Spark assembly has been built with Hive, including Datanucleus
>> jars on classpath
>>         - spark submit includes jars passed in through --jar *** FAILED
>> ***
>>             org.apache.spark.__SparkException: Process
>> List(./bin/spark-submit, --class, org.apache.spark.deploy.__JarCreationT=
est,
>> --name, testApp, --master, local-cluster[2,1,512], --jars,
>> file:/tmp/1408015659416-0/__testJar-1408015659471.jar,fi
>>         le:/tmp/1408015659472-0/__testJar-1408015659513.jar,
>> file:/tmp/1408015659415-0/__testJar-1408015659416.jar) exited with code =
1
>>             at org.apache.spark.util.Utils$._
>> _executeAndGetOutput(Utils.__scala:810)
>>             at org.apache.spark.deploy.__SparkSubmitSuite.__
>> runSparkSubmit(__SparkSubmitSuite.scala:311)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15._
>> _apply$mcV$sp(SparkSubmitSuite.__scala:305)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15._
>> _apply(SparkSubmitSuite.scala:__294)
>>             at org.apache.spark.deploy.__SparkSubmitSuite$$anonfun$15._
>> _apply(SparkSubmitSuite.scala:__294)
>>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__
>> Transformer.scala:22)
>>             at org.scalatest.Transformer$$__anonfun$apply$1.apply(__
>> Transformer.scala:22)
>>             at org.scalatest.OutcomeOf$class.__outcomeOf(OutcomeOf.scala=
:
>> 85)
>>             at org.scalatest.OutcomeOf$.__outcomeOf(OutcomeOf.scala:104)
>>             at org.scalatest.Transformer.__apply(Transformer.scala:22)
>>
>>             ...
>>
>>
>>         but only test the specific suite as follows will be ok:
>>         mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=3Dorg.apache.__s=
park.DriverSuite
>> test
>>
>>
>>         it seems when run with "mvn -Pyarn -Phadoop-2.4 -Phive test",the
>> process with Utils.executeAndGetOutput started can not exited successful=
ly
>> (exitcode is not zero)
>>
>>         anyone has idea for this?
>>
>>
>>
>>
>>
>>
>>     --
>>
>>     Best Regards
>>     Fei Wang
>>
>>     ------------------------------__----------------------------
>> --__--------------------
>>
>>
>>
>>     ------------------------------__----------------------------
>> --__---------
>>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.__org <mailto:
>> dev-unsubscribe@spark.apache.org>
>>     For additional commands, e-mail: dev-help@spark.apache.org <mailto:
>> dev-help@spark.apache.org>
>>
>>
>>
>
> --
>
> Best Regards
> Fei Wang
>
> ------------------------------------------------------------
> --------------------
>
>
>

--001a11c1f4a2a8932d0500f6a08f--

From dev-return-8950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 14:17:59 2014
Return-Path: <dev-return-8950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5B601136E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 14:17:59 +0000 (UTC)
Received: (qmail 29899 invoked by uid 500); 19 Aug 2014 14:17:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29839 invoked by uid 500); 19 Aug 2014 14:17:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29811 invoked by uid 99); 19 Aug 2014 14:17:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 14:17:57 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 98.139.213.147 as permitted sender)
Received: from [98.139.213.147] (HELO nm10-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.147)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 14:17:50 +0000
Received: from [98.139.212.150] by nm10.bullet.mail.bf1.yahoo.com with NNFMP; 19 Aug 2014 14:17:28 -0000
Received: from [98.139.212.239] by tm7.bullet.mail.bf1.yahoo.com with NNFMP; 19 Aug 2014 14:17:28 -0000
Received: from [127.0.0.1] by omp1048.mail.bf1.yahoo.com with NNFMP; 19 Aug 2014 14:17:28 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 832813.78942.bm@omp1048.mail.bf1.yahoo.com
Received: (qmail 55111 invoked by uid 60001); 19 Aug 2014 14:17:28 -0000
X-YMail-OSG: ev4aZKoVM1n6JbOgBwZ8WLABeWlOQ.ibeiuxLNjwunvBKIr
 yZfTVfytQMmufQvlREJRS5_bdLvaI.XpP3fzXneGpkNX4oo0qq69qnrRS9rR
 VTe0TURwHEKLQHHusJqtsuOtJAUbh6ow5jdxhvrJVVngml6F.H.zGW85aL8O
 tjKQIsc6rYX6PCn7UDVZe2tZJVdIMXMdNbJcNDKAepESKfNBIyUYPcF_uwZy
 A2LOqvGjqwKdkQcvzg587857f9u_HTbqy_NK50ePfSuo_Dy_ry788ONTKPO5
 _wS6T7ll4E_EeLH01_VMbZLQGftOw2F08wwOe2Tgzkaha2jXvqDS.e_3vxYD
 CthOSDQ6WMg_HbeZ.uyMuyjmXkiDnEnh6celgTBfywItdk1jVRcOSLlemZsz
 oyIwLkfcc5rbyrRsEQT0C_Yvx48lDTKVokVtNs_OWOGCFlkfnTdzpPuM61eq
 4ULMu6ADzQd.DOYO0oufuH9TkmSdN0D2ikdu84OnhsLOnAxRpUCPGZ4eUqeH
 9Ys1rJCf79IrIzzXlDn0xGUplvw3yYJiFYqxBuiEt1XETlxmCmJNfN159cal
 7uynBBEBCiBcvXotz7yWv9uHsLlSV4RZK8VkjVTdHeF8cxmqMq5hUn8Dsl80
 jUA--
Received: from [209.131.52.50] by web140101.mail.bf1.yahoo.com via HTTP; Tue, 19 Aug 2014 07:17:28 PDT
X-Rocket-MIMEInfo: 002.001,eWVzIHRoZSB3ZWJ1aSB3b3JrcyBvbiB5YXJuLiBZb3Ugc2hvdWxkIGJlIGFibGUgdG8gZ28gdG8gdGhlIFlhcm4gUmVzb3VyY2VNYW5hZ2VyIFVJIGFuZCBpdCB3aWxsIGhhdmUgYSBsaW5rIHRvIHRoZSB3ZWIgVUkgZm9yIGEgcnVubmluZyBzcGFyayBhcHBsaWNhdGlvbi4gwqBZb3UgY2FuIGFsc28gc2V0IGl0IHVwIHRvIHNhdmUgdGhlIGhpc3RvcnkgYW5kIHZpZXcgaXQgYWZ0ZXIgaXQgaGFzIGZpbmlzaGVkLiDCoEhpc3RvcnkgaW5mbyBjYW4gYmUgZm91bmQgaGVyZTrCoE1vbml0b3JpbmcgYW5kIEluc3QBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.201.700
References: <CA+B-+fzg_m1NG_y3rvobytP-TxdaSSqw1c0M3PhRTSJT7OafGQ@mail.gmail.com> 
Message-ID: <1408457848.24349.YahooMailNeo@web140101.mail.bf1.yahoo.com>
Date: Tue, 19 Aug 2014 07:17:28 -0700
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: Spark on YARN webui
To: Debasish Das <debasish.das83@gmail.com>, dev <dev@spark.apache.org>
In-Reply-To: <CA+B-+fzg_m1NG_y3rvobytP-TxdaSSqw1c0M3PhRTSJT7OafGQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="584511794-129737358-1408457848=:24349"
X-Virus-Checked: Checked by ClamAV on apache.org

--584511794-129737358-1408457848=:24349
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

yes the webui works on yarn. You should be able to go to the Yarn ResourceM=
anager UI and it will have a link to the web UI for a running spark applica=
tion. =A0You can also set it up to save the history and view it after it ha=
s finished. =A0History info can be found here:=A0Monitoring and Instrumenta=
tion - Spark 1.0.2 Documentation=0A=A0 =0A=A0 =A0 =A0 =A0 =A0 =0AMonitoring=
 and Instrumentation - Spark 1.0.2 Documentation=0AMonitoring and Instrumen=
tation There are several ways to monitor Spark applications: web UIs, metri=
cs, and external instrumentation. Web Interfaces   =0AView on spark.apache.=
org Preview by Yahoo  =0A=A0 =0A=0A=0ATom=A0=0A=0A=0AOn Tuesday, August 19,=
 2014 12:49 AM, Debasish Das <debasish.das83@gmail.com> wrote:=0A =0A=0A=0A=
Hi,=0A=0AWe are running the snapshots (new spark features) on YARN and I wa=
s=0Awondering if the webui is available on YARN mode...=0A=0AThe deployment=
 document does not mention webui on YARN mode...=0A=0AIs it available ?=0A=
=0AThanks.=0ADeb
--584511794-129737358-1408457848=:24349--

From dev-return-8951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 18:48:03 2014
Return-Path: <dev-return-8951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3704F11FF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 18:48:03 +0000 (UTC)
Received: (qmail 81149 invoked by uid 500); 19 Aug 2014 18:48:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81090 invoked by uid 500); 19 Aug 2014 18:48:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 2242 invoked by uid 99); 19 Aug 2014 16:54:01 -0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,FREEMAIL_ENVFROM_END_DIGIT,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: 216.139.236.26 is neither permitted nor denied by domain of alex_liu68@yahoo.com)
Date: Tue, 19 Aug 2014 09:53:35 -0700 (PDT)
From: alexliu68 <alex_liu68@yahoo.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408467214979-7914.post@n3.nabble.com>
Subject: Spark SQL Query and join different data sources.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Is there anyone make the query join different data sources work? especially
Join hive table with other data sources.

For example, hql uses HiveContext, and it needs first call "use
<database_name>" and other datasources use SqlContext, how can SqlContext
know Hive tables? I follow https://spark.apache.org/sql/ example to join
hive table with Json table, but I can't make it work. Am I missing anything
here?

Alex



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 19 23:09:42 2014
Return-Path: <dev-return-8952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E58711A61
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Aug 2014 23:09:42 +0000 (UTC)
Received: (qmail 36089 invoked by uid 500); 19 Aug 2014 23:09:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36023 invoked by uid 500); 19 Aug 2014 23:09:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36010 invoked by uid 99); 19 Aug 2014 23:09:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 23:09:41 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Aug 2014 23:09:11 +0000
Received: by mail-wi0-f177.google.com with SMTP id ho1so6053662wib.10
        for <dev@spark.apache.org>; Tue, 19 Aug 2014 16:09:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=HBELg+1PbsSV4bkmYQKB/alrOFqFPe+HIBPCCZSkd9Y=;
        b=D2QdJy649MQ25RJ9Liz4JjQkBBZN7vSFU59pxBz5Vn+NP5NP+uXvST6f6XmDwm72dQ
         uii/tsynWgBndKnc15Vhs5d34QrFgaREe8UBpoSgSCTjfUsmcmRrfq2Rrso0XbiCp8CJ
         HSt80JmWOvhxikhYBTfH/gQgGQeEr9hxUKGVL9jY654iPh1+7OIjenyzxcqiV4ta5+m1
         +ayffP6zjIqB4Z5dFZeNCBOsHVt7PcTnJp+ndbG+dyqFO4CKD4Pl4zlj8yQW2IiZWex7
         /+MiRSWQJTBWIcX35IS2meLjTd2oPDUXjcb0ZJiIDGp+cFp1xdvDdfGzPGvClyNstZo5
         k0ug==
X-Gm-Message-State: ALoCoQnfh3dFe8SVA4RaPXWI2P27TdcTRZwD+MhXNTd0AX7ejIwICz4PW85E5Lugm5MSiILAbJIa
MIME-Version: 1.0
X-Received: by 10.194.95.66 with SMTP id di2mr52844128wjb.47.1408489750977;
 Tue, 19 Aug 2014 16:09:10 -0700 (PDT)
Received: by 10.194.201.164 with HTTP; Tue, 19 Aug 2014 16:09:10 -0700 (PDT)
In-Reply-To: <CACBYxKJcLfAKiifWHpgJP1qQ0t=nZfCASnoh2kCn-k3wsO8zKQ@mail.gmail.com>
References: <1404847632.48111.YahooMailAndroidMobile@web194605.mail.sg3.yahoo.com>
	<CACBYxKJcLfAKiifWHpgJP1qQ0t=nZfCASnoh2kCn-k3wsO8zKQ@mail.gmail.com>
Date: Tue, 19 Aug 2014 16:09:10 -0700
Message-ID: <CAJ7gjxwg7n9+0=s=yYaDz=+0Z4en8zkh3toXQFVGB2pv6Gj+ag@mail.gmail.com>
Subject: Re: Data Locality In Spark
From: Chris Fregly <chris@fregly.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb0498e46f2890501039363
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb0498e46f2890501039363
Content-Type: text/plain; charset=UTF-8

and even the same process where the data might be cached.


these are the different locality levels:

PROCESS_LOCAL
NODE_LOCAL
RACK_LOCAL
ANY

relevant code:
https://github.com/apache/spark/blob/7712e724ad69dd0b83754e938e9799d13a4d43b9/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala#L150

https://github.com/apache/spark/blob/63bdb1f41b4895e3a9444f7938094438a94d3007/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L250

relevant docs:
see the spark.locality configuration attributes here:
https://spark.apache.org/docs/latest/configuration.html


On Tue, Jul 8, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> Hi Anish,
>
> Spark, like MapReduce, makes an effort to schedule tasks on the same nodes
> and racks that the input blocks reside on.
>
> -Sandy
>
>
> On Tue, Jul 8, 2014 at 12:27 PM, anishsneh@yahoo.co.in <
> anishsneh@yahoo.co.in> wrote:
>
> > Hi All
> >
> > My apologies for very basic question, do we have full support of data
> > locality in Spark MapReduce.
> >
> > Please suggest.
> >
> > --
> > Anish Sneh
> > "Experience is the best teacher."
> > http://in.linkedin.com/in/anishsneh
> >
> >
>

--047d7bb0498e46f2890501039363--

From dev-return-8953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 03:52:05 2014
Return-Path: <dev-return-8953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C3D1112D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 03:52:05 +0000 (UTC)
Received: (qmail 97196 invoked by uid 500); 20 Aug 2014 03:52:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96910 invoked by uid 500); 20 Aug 2014 03:52:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96894 invoked by uid 99); 20 Aug 2014 03:52:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 03:52:04 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 03:52:00 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so6734952qgf.7
        for <dev@spark.apache.org>; Tue, 19 Aug 2014 20:51:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=eBk08mu/Ml8EMkqQ/mZkeqGWwF31MsGx/YNjtt0bnwY=;
        b=OFxHOARbm5Jz/FKnj7swyTIfQrVkjQ26HR4X7ScI/I68mC1aPx+NiP3TztvoAEnKNF
         ciUPtHlXFDnoWl3DV7IAvMhUyoZudXIGVEG8vQpJxtlHWGeERyFSHuAyM+z9oMPDROIF
         emTCMMhWo/alXb9skYdjE2f4lBHrNT2dhh0cC84GJ1Y2oB0pad43qp3qlrcvoYpM1oY2
         3P2vHbkhJAuAgrmXkf1SJ+JjtCVd70L0GAH9Y5Xtdi2ajS2DJkAIkS05OMPesQm9Cwdh
         Skuj1VquK0L9C71sMMxCKQa8HritAnV+Kd9h554lPPaYKc1jH8D3Q4DJ0e2hkwL3jWon
         p2DQ==
MIME-Version: 1.0
X-Received: by 10.224.80.10 with SMTP id r10mr75186212qak.24.1408506699491;
 Tue, 19 Aug 2014 20:51:39 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Tue, 19 Aug 2014 20:51:39 -0700 (PDT)
Date: Tue, 19 Aug 2014 20:51:39 -0700
Message-ID: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
Subject: Lost executor on YARN ALS iterations
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2dba87cb31c05010785f1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2dba87cb31c05010785f1
Content-Type: text/plain; charset=UTF-8

Hi,

During the 4th ALS iteration, I am noticing that one of the executor gets
disconnected:

14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
SendingConnectionManagerId not found

14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
disconnected, so removing it

14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost executor 5
on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated

14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 12)
Any idea if this is a bug related to akka on YARN ?

I am using master

Thanks.
Deb

--001a11c2dba87cb31c05010785f1--

From dev-return-8954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 06:30:00 2014
Return-Path: <dev-return-8954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13090116DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 06:30:00 +0000 (UTC)
Received: (qmail 57521 invoked by uid 500); 20 Aug 2014 06:29:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57474 invoked by uid 500); 20 Aug 2014 06:29:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57462 invoked by uid 99); 20 Aug 2014 06:29:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 06:29:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 06:29:32 +0000
Received: by mail-qa0-f49.google.com with SMTP id dc16so6621365qab.36
        for <dev@spark.apache.org>; Tue, 19 Aug 2014 23:29:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4SNlH59krs5/t9YXdPi4sDNZUuOpfHWEJErriSmb7O0=;
        b=Y6rJbX2prqgxmNfNwDjqnp+rNQQ+NAtgJAIl+vZM9Y6cWHqjwIRFeqxo9b1byKadsC
         b43+/vsL2ShZ4S/afvJ6brAfFOqJB6x83ti3uHf+XXrqnAoJ2XGaNZHJYZdWGSgCn+Xp
         VX1sOysbdoPHht0kPxC1zEVgrI/M0106FY6jW+BXqhb9JrbiqlcCUVNHUo2SX2mIARaZ
         JbHoqXYw4FzJf68USz2HeR6E2/yZWGDbtFVo25xfjQI4f1IcB6fy/8Qpx6xXy6GAo8a8
         HcWkH08DHmpU/Pffw3mksYqVMYZ6BQHg1AwQrHCT14hqhY/aN0denwfN2h/F+ZCISWwf
         R3yA==
MIME-Version: 1.0
X-Received: by 10.224.151.69 with SMTP id b5mr7073878qaw.37.1408516171290;
 Tue, 19 Aug 2014 23:29:31 -0700 (PDT)
Received: by 10.140.180.197 with HTTP; Tue, 19 Aug 2014 23:29:31 -0700 (PDT)
In-Reply-To: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
References: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
Date: Tue, 19 Aug 2014 23:29:31 -0700
Message-ID: <CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
Subject: Re: Lost executor on YARN ALS iterations
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>, sandy.ryza@cloudera.com
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

I think this may be the same issue as described in
https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
container got killed by YARN because it used much more memory that it
requested. But we haven't figured out the root cause yet.

+Sandy

Best,
Xiangrui

On Tue, Aug 19, 2014 at 8:51 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> During the 4th ALS iteration, I am noticing that one of the executor gets
> disconnected:
>
> 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
>
> 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
> disconnected, so removing it
>
> 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost executor 5
> on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated
>
> 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 12)
> Any idea if this is a bug related to akka on YARN ?
>
> I am using master
>
> Thanks.
> Deb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 07:28:07 2014
Return-Path: <dev-return-8955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 463C611908
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 07:28:07 +0000 (UTC)
Received: (qmail 81821 invoked by uid 500); 20 Aug 2014 07:28:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81750 invoked by uid 500); 20 Aug 2014 07:28:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81738 invoked by uid 99); 20 Aug 2014 07:28:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 07:28:06 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 07:27:40 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so7385628qcv.1
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 00:27:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ZDmOXs55GPEMtYc8YA49k81Y1RTFXRnkNAx+mmsQvSw=;
        b=C2FEdzpErRZ2cXZVnA/biN8aRUqY8k3AwrSespMoVYasFnTjf2qcN81rcTk9Z/MrU6
         /SVM7NTG1R0pbxSbGC2MT43UkkFWQvCmIPgBt9Gu44coOY3IU8qyz1fjD8+EdD7U22cy
         8XO7ogBN6uugti9t9N5lEZW9GKo8Cfnw6hnQLVZBohEM+RBeX1QSjxnzEn9lSFQbT24Y
         h1dy2TNWWkEZzvUXlehfYa89XeO9UWZ4igpRDwQdCQ6JFl5ifJvlILx44NsVf1/Pdogl
         zgSj0qc7R6b1ENF3PUNSAbb6ZegMy+q8gc4XtVV5xv+t+Ep7k/tNZ77rbeW53G1sAqF/
         qxJw==
MIME-Version: 1.0
X-Received: by 10.224.3.67 with SMTP id 3mr78069304qam.26.1408519659476; Wed,
 20 Aug 2014 00:27:39 -0700 (PDT)
Received: by 10.140.33.247 with HTTP; Wed, 20 Aug 2014 00:27:39 -0700 (PDT)
In-Reply-To: <CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
References: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
	<CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
Date: Wed, 20 Aug 2014 00:27:39 -0700
Message-ID: <CA+B-+fx=pQtL59VsaO_moc3HsY9+sW1WhHs+JMg0iuNtrnddPA@mail.gmail.com>
Subject: Re: Lost executor on YARN ALS iterations
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>, sandy.ryza@cloudera.com
Content-Type: multipart/alternative; boundary=047d7b670975f65c8d05010a8967
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b670975f65c8d05010a8967
Content-Type: text/plain; charset=UTF-8

I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is
definitely a YARN related problem...

At least for me right now only deployment option possible is standalone...



On Tue, Aug 19, 2014 at 11:29 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Deb,
>
> I think this may be the same issue as described in
> https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
> container got killed by YARN because it used much more memory that it
> requested. But we haven't figured out the root cause yet.
>
> +Sandy
>
> Best,
> Xiangrui
>
> On Tue, Aug 19, 2014 at 8:51 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi,
> >
> > During the 4th ALS iteration, I am noticing that one of the executor gets
> > disconnected:
> >
> > 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
> > SendingConnectionManagerId not found
> >
> > 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
> > disconnected, so removing it
> >
> > 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost
> executor 5
> > on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated
> >
> > 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch
> 12)
> > Any idea if this is a bug related to akka on YARN ?
> >
> > I am using master
> >
> > Thanks.
> > Deb
>

--047d7b670975f65c8d05010a8967--

From dev-return-8956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 07:40:33 2014
Return-Path: <dev-return-8956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABAFF11972
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 07:40:33 +0000 (UTC)
Received: (qmail 8303 invoked by uid 500); 20 Aug 2014 07:40:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8235 invoked by uid 500); 20 Aug 2014 07:40:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8224 invoked by uid 99); 20 Aug 2014 07:40:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 07:40:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 07:39:57 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so6524388qae.20
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 00:39:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=looVb2qL2UZXnvz5GEHLOt1nAOZ3R2A43kpmHJwL5OQ=;
        b=XLy38c4UkvOLekv/kTEVQYvOGZuZX7RzttMWQwG2aXdxHiyI+6vsqwjPX3gn8gtTao
         MoKhf22tnJDMjbN7lKV0jp2GwAzkZhSJdAj8gKhZmWilt/hDulMOIbTTUzx2A5rpeZ7g
         y3f/M/yfQWw2EL2DKEShSf2aybWLDtV+bhdQBZaPeKZY8gakBlARYab+oMuW/z9k3sO7
         9o78S3JbXMaSjedVh6TRj/pbr8Kq3Y8YRfJnOQqb60qxEqd0QdLMxuQVrbzq5MdH7NuY
         BPejp85Mu98f757673j0xyTPOlvzNJImmVWX7TMt6cl38SEFqqTjrIK3jgMM+m16k+8q
         onkA==
X-Gm-Message-State: ALoCoQm2kyMLnGUcsCiYyyt/rNMmPJeoL+mZlXbldFnYJBMnJsBoqhgjztpDemm37gJqq9OUfh+G
MIME-Version: 1.0
X-Received: by 10.140.29.138 with SMTP id b10mr71424193qgb.15.1408520396598;
 Wed, 20 Aug 2014 00:39:56 -0700 (PDT)
Received: by 10.140.42.37 with HTTP; Wed, 20 Aug 2014 00:39:56 -0700 (PDT)
In-Reply-To: <CA+B-+fx=pQtL59VsaO_moc3HsY9+sW1WhHs+JMg0iuNtrnddPA@mail.gmail.com>
References: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
	<CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
	<CA+B-+fx=pQtL59VsaO_moc3HsY9+sW1WhHs+JMg0iuNtrnddPA@mail.gmail.com>
Date: Wed, 20 Aug 2014 00:39:56 -0700
Message-ID: <CACBYxKLn_jFqdNXytkrE=MhgB+9RfOgMEHHmyCQn7tH+J6iU3g@mail.gmail.com>
Subject: Re: Lost executor on YARN ALS iterations
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a75a2e604ae05010ab57e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a75a2e604ae05010ab57e
Content-Type: text/plain; charset=UTF-8

Hi Debasish,

The fix is to raise spark.yarn.executor.memoryOverhead until this goes
away.  This controls the buffer between the JVM heap size and the amount of
memory requested from YARN (JVMs can take up memory beyond their heap
size). You should also make sure that, in the YARN NodeManager
configuration, yarn.nodemanager.vmem-check-enabled is set to false.

-Sandy


On Wed, Aug 20, 2014 at 12:27 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is
> definitely a YARN related problem...
>
> At least for me right now only deployment option possible is standalone...
>
>
>
> On Tue, Aug 19, 2014 at 11:29 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> Hi Deb,
>>
>> I think this may be the same issue as described in
>> https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
>> container got killed by YARN because it used much more memory that it
>> requested. But we haven't figured out the root cause yet.
>>
>> +Sandy
>>
>> Best,
>> Xiangrui
>>
>> On Tue, Aug 19, 2014 at 8:51 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi,
>> >
>> > During the 4th ALS iteration, I am noticing that one of the executor
>> gets
>> > disconnected:
>> >
>> > 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
>> > SendingConnectionManagerId not found
>> >
>> > 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
>> > disconnected, so removing it
>> >
>> > 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost
>> executor 5
>> > on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated
>> >
>> > 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch
>> 12)
>> > Any idea if this is a bug related to akka on YARN ?
>> >
>> > I am using master
>> >
>> > Thanks.
>> > Deb
>>
>
>

--001a113a75a2e604ae05010ab57e--

From dev-return-8957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 16:03:23 2014
Return-Path: <dev-return-8957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5FEA511A3D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 16:03:23 +0000 (UTC)
Received: (qmail 93226 invoked by uid 500); 20 Aug 2014 16:03:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93162 invoked by uid 500); 20 Aug 2014 16:03:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93150 invoked by uid 99); 20 Aug 2014 16:03:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 16:03:22 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.169 as permitted sender)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 16:03:16 +0000
Received: by mail-lb0-f169.google.com with SMTP id s7so7029922lbd.14
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 09:02:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=SozHjfmUCEM7+WpQ/wcyuUb5g3hrbqOa84bZTrgnFMs=;
        b=a4vQ/ivRecCnmu0SEGH18gqEnIXauLTe18Lx+72lcUe0+n/qtiQzLqfH9vx1JFpUN5
         NjICy2ZJCQ9P5tYIh9ZPf00XwD3Bl38fWIViVwU2E0H5/zzaNVH3YaC3GhATCJbV2Atb
         2KpNZGGgpeBQJU0Ep7sqzmbujkfdKmptGc4rPI9982m6McvsjNX1tMT2FbluH12Q5WHS
         E6K4YWq3rbqYHP3vuw1K5Dxr2CoKAMNykESZu/KXbVTS0YojP+cu1RY1uxZpktREz/JA
         eTo3jirhp/HyteWAV0MO5KU+ey9FT7DXOq5TIyImTV+8D+TnUHdrwt35QwIaWcpKuIv/
         SIAw==
MIME-Version: 1.0
X-Received: by 10.112.52.130 with SMTP id t2mr26206606lbo.61.1408550575390;
 Wed, 20 Aug 2014 09:02:55 -0700 (PDT)
Received: by 10.25.148.4 with HTTP; Wed, 20 Aug 2014 09:02:55 -0700 (PDT)
Date: Wed, 20 Aug 2014 09:02:55 -0700
Message-ID: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
Subject: Akka usage in Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3f458b1c4a5050111bc5c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3f458b1c4a5050111bc5c
Content-Type: text/plain; charset=UTF-8

Hi,

There have been some recent changes in the way akka is used in spark and I
feel they are major changes...

Is there a design document / JIRA / experiment on large datasets that
highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great
to understand where akka is used in the code base...

If I don't have to broadcast big variables but use akka's programming model
(use actors directly) on Spark's actorsystem is that allowed ? I understand
that it might look hacky :-)

Thanks.
Deb

--001a11c3f458b1c4a5050111bc5c--

From dev-return-8958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 19:40:06 2014
Return-Path: <dev-return-8958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED60A11462
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 19:40:06 +0000 (UTC)
Received: (qmail 58926 invoked by uid 500); 20 Aug 2014 19:40:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58858 invoked by uid 500); 20 Aug 2014 19:40:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58489 invoked by uid 99); 20 Aug 2014 19:40:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 19:40:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 19:39:38 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so6032973oig.11
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 12:39:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=XV1O0BmyWjGaVCUOLIVTygG1rjzoaFJ4R/ohL+yJRdU=;
        b=ZdECPFCQmvTzuclvYDXbxOna9e97L2jJSx4BBpTl6FLYh7iQAWgLFgwRfMU5TQReLB
         qW7OB83rDrMZYKSIvhBS3UwkhY7Mbdn1rGxhdLnq2lX5Ubb5LZUWuzeGMldLrckuPwqw
         XhSeMh4MIGSfvR0TKQdk/HRvVMuvA9KDQnHdDRNUMwoRdHLTffDISINMZeohtFhC8HtZ
         /2jiWehe4Cncs7xp/wCu+TG9ZsrsxAf+tM5XE5IrCSfUhmEk1NYcUGjX6/uKirE2X9bl
         kmN5GSPTv/K6O757JlBWbDYubQIscFNzPOcxqu6C0FjlXvu+3xLAKoiVm6DCyd70Rorg
         Hi9g==
X-Gm-Message-State: ALoCoQlnlEOlr5SAA3QMUZ27tZVY5q43tw/mEqFKh/MBWHENF5pXskpaB/eQStkPpYfisLq3QCGy
MIME-Version: 1.0
X-Received: by 10.60.161.136 with SMTP id xs8mr46014846oeb.42.1408563577278;
 Wed, 20 Aug 2014 12:39:37 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Wed, 20 Aug 2014 12:39:37 -0700 (PDT)
Date: Wed, 20 Aug 2014 14:39:37 -0500
Message-ID: <CAKWX9VXfVSdot9q3j81wpRvBsbMyOS_ry-z=SYf2ngV+Kxh=mA@mail.gmail.com>
Subject: Limit on number of simultaneous Spark frameworks on Mesos?
From: Cody Koeninger <cody@koeninger.org>
To: user@mesos.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01183a18ab498a050114c38d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01183a18ab498a050114c38d
Content-Type: text/plain; charset=UTF-8

I'm seeing situations where starting e.g. a 4th spark job on Mesos results
in none of the jobs making progress.  This happens even with
--executor-memory set to values that should not come close to exceeding the
availability per node, and even if the 4th job is doing something
completely trivial (e.g. parallelize 1 to 10000 and sum).  Killing one of
the jobs typically allows the others to start proceeding.

While jobs are hung, I see the following in mesos master logs:

I0820 19:28:02.651296 24666 master.cpp:2282] Sending 7 offers to framework
20140820-170154-1315739402-5050-24660-0020
I0820 19:28:02.654502 24668 master.cpp:1578] Processing reply for offers: [
20140820-170154-1315739402-5050-24660-96624 ] on slave
20140724-150750-1315739402-5050-25405-6 (dn-04) for framework
20140820-170154-1315739402-5050-24660-0020
I0820 19:28:02.654722 24668 hierarchical_allocator_process.hpp:590]
Framework 20140820-170154-1315739402-5050-24660-0020 filtered slave
20140724-150750-1315739402-5050-25405-6 for 1secs

Am I correctly interpreting that to mean that spark is being offered
resources, but is rejecting them?  Is there a way (short of patching spark
to add more logging) to figure out why resources are being rejected?

This is on the default fine-grained mode.

--089e01183a18ab498a050114c38d--

From dev-return-8959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 20:43:19 2014
Return-Path: <dev-return-8959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F82F117F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 20:43:19 +0000 (UTC)
Received: (qmail 75353 invoked by uid 500); 20 Aug 2014 20:43:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75291 invoked by uid 500); 20 Aug 2014 20:43:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75198 invoked by uid 99); 20 Aug 2014 20:43:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 20:43:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 20:43:12 +0000
Received: by mail-ob0-f173.google.com with SMTP id vb8so6780960obc.32
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 13:42:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tkkyNkMFkulC1Rh73SrFan6UMoatAANKtSOD8Vruq6k=;
        b=ksX1VokCzzSxkv2Lrj50H+QFx9+G6yBG5BKMDlr1IXoTq/ZmnjA5MABr3fqg4bk/MP
         +YK7FvzMmFeOJX6Bn0HBA1ERk8jFhmGVNmr19hHjjhFEmoS3kC6lIdOYqergGP9jtgYY
         xN7o9XX21tVTeWJ2XsdxoISdXmTsA1uUFhG+WGY+D8Ewolvhw0gyZgNJELZmwUDs3OG8
         JpPjNSmK+YCT+KuuPp29cm7GYtXjXak14Q71sfo0gF3EhBRP8aH2OT9hfmHxPzBNSUk+
         Hl2C+QmvA+MvmKQ/V4p0DvJXALvfj+nOxHV/TqSY6C73E6ktyB8co1gQcDECSk603Gwo
         Aoug==
MIME-Version: 1.0
X-Received: by 10.182.27.5 with SMTP id p5mr51095027obg.42.1408567372224; Wed,
 20 Aug 2014 13:42:52 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Wed, 20 Aug 2014 13:42:52 -0700 (PDT)
In-Reply-To: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
References: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
Date: Wed, 20 Aug 2014 13:42:52 -0700
Message-ID: <CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
Subject: Re: Akka usage in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115f6ccdd6a87050115a543
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f6ccdd6a87050115a543
Content-Type: text/plain; charset=ISO-8859-1

Hey Deb,

Can you be specific what changes you are mentioning? We have not, to my
knowledge, made major architectural changes around akka use.

I think in general we don't want people to be using Spark's actor system
directly - it is an internal communication component in Spark and could
e.g. be re-factored later to not use akka at all. Could you elaborate a bit
more on your use case?

- Patrick


On Wed, Aug 20, 2014 at 9:02 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> There have been some recent changes in the way akka is used in spark and I
> feel they are major changes...
>
> Is there a design document / JIRA / experiment on large datasets that
> highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great
> to understand where akka is used in the code base...
>
> If I don't have to broadcast big variables but use akka's programming model
> (use actors directly) on Spark's actorsystem is that allowed ? I understand
> that it might look hacky :-)
>
> Thanks.
> Deb
>

--089e0115f6ccdd6a87050115a543--

From dev-return-8960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 20:52:55 2014
Return-Path: <dev-return-8960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A004611859
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 20:52:55 +0000 (UTC)
Received: (qmail 385 invoked by uid 500); 20 Aug 2014 20:52:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 327 invoked by uid 500); 20 Aug 2014 20:52:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 315 invoked by uid 99); 20 Aug 2014 20:52:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 20:52:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 20:52:27 +0000
Received: by mail-ob0-f170.google.com with SMTP id wp4so6818694obc.1
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 13:52:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=lJT9gIg6MiDazawrvZyjgsCzC83aXTt/INkODQbOBko=;
        b=J9Rmz8q68afYdjgW8P1pwLNaOGcP7IHrXHEtLPJeS7aRZw4hJJC+NLGxD0ys6zlowK
         lcj9467Ud4xFnnu4lfg1OFmwZbqhFRWKbENAmjgzfvO0HH3PPmagOAJ+lwv/jpbejeWZ
         XQsJwYgz6mbwTuAH3C2q/SY4BYvYYlRmD2TU2LxITB5ocI5jCj4bouSNk1wG45FRleQ6
         e6VcI0llMjLHPwFr5G6s+x18C6WgifBKh8inXqvErY5I+8AfvQGSzYlt6aAp84qBF4b2
         pi6t6MKXOPORVL4XxHQato8aA4n+07s27JcwkFBS4GNmi0L/CkPBJ1IlKXCjm2Qeyx23
         3PaA==
X-Gm-Message-State: ALoCoQk0TPwlqS8nbHCM5i6gPk77hKT4dZttEei9kIsZmri1zKK9Y6zm1rc9py5h8tmcnoj/czRk
MIME-Version: 1.0
X-Received: by 10.60.43.38 with SMTP id t6mr50874681oel.14.1408567945515; Wed,
 20 Aug 2014 13:52:25 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Wed, 20 Aug 2014 13:52:25 -0700 (PDT)
In-Reply-To: <635D630D-46CE-4D79-ABD0-C9D202E78CAB@mesosphere.io>
References: <CAKWX9VXfVSdot9q3j81wpRvBsbMyOS_ry-z=SYf2ngV+Kxh=mA@mail.gmail.com>
	<635D630D-46CE-4D79-ABD0-C9D202E78CAB@mesosphere.io>
Date: Wed, 20 Aug 2014 15:52:25 -0500
Message-ID: <CAKWX9VV8_KMvRutFHWbKQyzcCsgvpCnx-HhBxnxwOCk=EfBpRQ@mail.gmail.com>
Subject: Re: Limit on number of simultaneous Spark frameworks on Mesos?
From: Cody Koeninger <cody@koeninger.org>
To: user@mesos.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ced40914cc050115c8f3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ced40914cc050115c8f3
Content-Type: text/plain; charset=UTF-8

At least some of the jobs are typically doing work that would make it
difficult to share, e.g. accessing hdfs.  I'll see if I can get a smaller
reproducible case.


On Wed, Aug 20, 2014 at 3:23 PM, Timothy Chen <tim@mesosphere.io> wrote:

> Can you share your spark / mesos configurations and the spark job? I'd
> like to repro it.
>
> Tim
>
> > On Aug 20, 2014, at 12:39 PM, Cody Koeninger <cody@koeninger.org> wrote:
> >
> > I'm seeing situations where starting e.g. a 4th spark job on Mesos
> results in none of the jobs making progress.  This happens even with
> --executor-memory set to values that should not come close to exceeding the
> availability per node, and even if the 4th job is doing something
> completely trivial (e.g. parallelize 1 to 10000 and sum).  Killing one of
> the jobs typically allows the others to start proceeding.
> >
> > While jobs are hung, I see the following in mesos master logs:
> >
> > I0820 19:28:02.651296 24666 master.cpp:2282] Sending 7 offers to
> framework 20140820-170154-1315739402-5050-24660-0020
> > I0820 19:28:02.654502 24668 master.cpp:1578] Processing reply for
> offers: [ 20140820-170154-1315739402-5050-24660-96624 ] on slave
> 20140724-150750-1315739402-5050-25405-6 (dn-04) for framework
> 20140820-170154-1315739402-5050-24660-0020
> > I0820 19:28:02.654722 24668 hierarchical_allocator_process.hpp:590]
> Framework 20140820-170154-1315739402-5050-24660-0020 filtered slave
> 20140724-150750-1315739402-5050-25405-6 for 1secs
> >
> > Am I correctly interpreting that to mean that spark is being offered
> resources, but is rejecting them?  Is there a way (short of patching spark
> to add more logging) to figure out why resources are being rejected?
> >
> > This is on the default fine-grained mode.
> >
>

--001a11c2ced40914cc050115c8f3--

From dev-return-8961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 22:19:51 2014
Return-Path: <dev-return-8961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C5CF11B99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 22:19:51 +0000 (UTC)
Received: (qmail 52047 invoked by uid 500); 20 Aug 2014 22:19:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51977 invoked by uid 500); 20 Aug 2014 22:19:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51965 invoked by uid 99); 20 Aug 2014 22:19:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:19:49 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:19:45 +0000
Received: by mail-la0-f45.google.com with SMTP id ty20so7884386lab.18
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 15:19:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=x3QQUhF2ysGAv4hB9iTq7BqhYHMAoEbUlcdCIRfFjqc=;
        b=uuEcGjpC5Kr8vclG77JG0yqjDDLfqVSoyLxVi0B0+4vj1jowNQ2DASaN9R5HzHM6EF
         eDyVRyd2gJA1K0C2rNG3zDGJVXQmFgrfgEy3z+p7qZC5zrAQaPbLL2AUMde1KkENiyQK
         kC2ELITsG0aqRCBH1AyiXsy93895dx9jBLtvLoIQbch5PLRGoe75PQWLds073HzIwiYA
         aa76IqPIHvZOz+IXdxfGHmoD9gSbd5IvDQgQ0HrzNEFoIQBn93F4wcyPe39tBrYBfEKM
         t+nCMj0wCT45f92gIKTSMdpuebN0swF/dDrlt5Kbm939bfW1fVmzhfsoezQU84kx1TUW
         8tTg==
MIME-Version: 1.0
X-Received: by 10.152.4.97 with SMTP id j1mr45882771laj.10.1408573163587; Wed,
 20 Aug 2014 15:19:23 -0700 (PDT)
Received: by 10.25.148.4 with HTTP; Wed, 20 Aug 2014 15:19:23 -0700 (PDT)
In-Reply-To: <CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
References: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
	<CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
Date: Wed, 20 Aug 2014 15:19:23 -0700
Message-ID: <CA+B-+fwA7=ZYtAbU8r_m1BxRzfgeyN-zGWKvjfetEKhfqCpRPw@mail.gmail.com>
Subject: Re: Akka usage in Spark
From: Debasish Das <debasish.das83@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149338c0e4b27050116ffe3
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149338c0e4b27050116ffe3
Content-Type: text/plain; charset=UTF-8

Hi Patrick,

Last few days I came across some bugs which got exposed due to ALS runs on
large scale data...although it was not related to the akka changes but
during the debug I found across some akka related changes that might have
an impact of overall performance...one example is the following:

https://github.com/apache/spark/pull/1907

@dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no longer
sent through akka msgs but over http-channels...If there is a document
detailing the architecture that is currently in-place (like how the core
changed from 1.0 to 1.1) it will help a lot in debugging the jobs which are
built upon the libraries like mllib and optimize them further for
efficiency...

For using the Spark actor system directly:

I spent few weeks December 2013 to make the Scalafish code (
https://github.com/azymnis/scalafish) operational on 10 nodes...It uses
scalding for matrix partitioning and actorSystem to coordinate the
updates...It is a cool use of akka but getting an actor system operational
is difficult...

Since Spark already has tested version of actor system running on both
standalone and yarn modes, I am planning to port scalafish to spark using
actor model...That's one of the use-cases I am looking for...

Another use-case that I am considering is to send msgs directly from kafka
queues to spark actorSystem for processing to get Storm like
latency...basically window sizes of 1-2 ms and no overhead of using an RDD
if possible...

Thanks.
Deb


On Wed, Aug 20, 2014 at 1:42 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Deb,
>
> Can you be specific what changes you are mentioning? We have not, to my
> knowledge, made major architectural changes around akka use.
>
> I think in general we don't want people to be using Spark's actor system
> directly - it is an internal communication component in Spark and could
> e.g. be re-factored later to not use akka at all. Could you elaborate a bit
> more on your use case?
>
> - Patrick
>
>
> On Wed, Aug 20, 2014 at 9:02 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Hi,
>>
>> There have been some recent changes in the way akka is used in spark and I
>> feel they are major changes...
>>
>> Is there a design document / JIRA / experiment on large datasets that
>> highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great
>> to understand where akka is used in the code base...
>>
>> If I don't have to broadcast big variables but use akka's programming
>> model
>> (use actors directly) on Spark's actorsystem is that allowed ? I
>> understand
>> that it might look hacky :-)
>>
>> Thanks.
>> Deb
>>
>
>

--089e0149338c0e4b27050116ffe3--

From dev-return-8962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 22:28:43 2014
Return-Path: <dev-return-8962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3D5211BFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 22:28:43 +0000 (UTC)
Received: (qmail 72768 invoked by uid 500); 20 Aug 2014 22:28:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72693 invoked by uid 500); 20 Aug 2014 22:28:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72682 invoked by uid 99); 20 Aug 2014 22:28:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:28:37 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:28:33 +0000
Received: by mail-qg0-f51.google.com with SMTP id a108so8110991qge.10
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 15:28:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=QQ1KdyI0iT5Eeqj108hmJL6fAzjUvjZ4Hirn+n+LW8Q=;
        b=TWRdayIlSUVGQam/zyMzVvQcNOwtCz2RNvTbpMfNx4jGF/GVIg7gEDWeGaXgzOVIZU
         fFa4K7gdg6MtO4tgoJwnWoojTYFdbYEg+mts9ddKMPjFHvlKvnS+t8BL+lZ8oXZFpnA+
         C2zvUELqEdG0j90NzxhCX8noBq7y0WdXMwyvBLOCxU8VEwPf33Ah15xPkQwTa361vE+I
         0KLgWBJUypZCFG+BFukguWdsXkcjD+OLVwodq+dxXqsibHB+76Qj0vZstKK3LkLnejen
         lO/phn6TYfSsOpAM7UkcS8CPXCDwdfzAAP1A6mrz9ttkVtzR0PvBobMRIbxKgK8PHkXu
         eprA==
X-Gm-Message-State: ALoCoQnSrhg7wBhlKPvNzgD923SrPl9sUfS2Yyt5jxVzmmxv0S0PW31/eA7gmhN4Kehu7FaxQ2IJ
MIME-Version: 1.0
X-Received: by 10.140.51.166 with SMTP id u35mr78202383qga.68.1408573691957;
 Wed, 20 Aug 2014 15:28:11 -0700 (PDT)
Received: by 10.229.44.73 with HTTP; Wed, 20 Aug 2014 15:28:11 -0700 (PDT)
In-Reply-To: <CA+B-+fwA7=ZYtAbU8r_m1BxRzfgeyN-zGWKvjfetEKhfqCpRPw@mail.gmail.com>
References: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
	<CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
	<CA+B-+fwA7=ZYtAbU8r_m1BxRzfgeyN-zGWKvjfetEKhfqCpRPw@mail.gmail.com>
Date: Wed, 20 Aug 2014 15:28:11 -0700
Message-ID: <CAEYYnxbaY-g5yBa3=gT+VHu9LD2ooCrkPM_TQCCU1CLKBcr_0w@mail.gmail.com>
Subject: Re: Akka usage in Spark
From: DB Tsai <dbtsai@dbtsai.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

To be specific, I was discussing this PR with Debasish which reduces
lots of issues when sending big objects to executors without using
broadcast explicitly.

Broadcast RDD object once per TaskSet (instead of sending it for every task)
https://issues.apache.org/jira/browse/SPARK-2521

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Aug 20, 2014 at 3:19 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Patrick,
>
> Last few days I came across some bugs which got exposed due to ALS runs on
> large scale data...although it was not related to the akka changes but
> during the debug I found across some akka related changes that might have
> an impact of overall performance...one example is the following:
>
> https://github.com/apache/spark/pull/1907
>
> @dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no longer
> sent through akka msgs but over http-channels...If there is a document
> detailing the architecture that is currently in-place (like how the core
> changed from 1.0 to 1.1) it will help a lot in debugging the jobs which are
> built upon the libraries like mllib and optimize them further for
> efficiency...
>
> For using the Spark actor system directly:
>
> I spent few weeks December 2013 to make the Scalafish code (
> https://github.com/azymnis/scalafish) operational on 10 nodes...It uses
> scalding for matrix partitioning and actorSystem to coordinate the
> updates...It is a cool use of akka but getting an actor system operational
> is difficult...
>
> Since Spark already has tested version of actor system running on both
> standalone and yarn modes, I am planning to port scalafish to spark using
> actor model...That's one of the use-cases I am looking for...
>
> Another use-case that I am considering is to send msgs directly from kafka
> queues to spark actorSystem for processing to get Storm like
> latency...basically window sizes of 1-2 ms and no overhead of using an RDD
> if possible...
>
> Thanks.
> Deb
>
>
> On Wed, Aug 20, 2014 at 1:42 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hey Deb,
>>
>> Can you be specific what changes you are mentioning? We have not, to my
>> knowledge, made major architectural changes around akka use.
>>
>> I think in general we don't want people to be using Spark's actor system
>> directly - it is an internal communication component in Spark and could
>> e.g. be re-factored later to not use akka at all. Could you elaborate a bit
>> more on your use case?
>>
>> - Patrick
>>
>>
>> On Wed, Aug 20, 2014 at 9:02 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> There have been some recent changes in the way akka is used in spark and I
>>> feel they are major changes...
>>>
>>> Is there a design document / JIRA / experiment on large datasets that
>>> highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great
>>> to understand where akka is used in the code base...
>>>
>>> If I don't have to broadcast big variables but use akka's programming
>>> model
>>> (use actors directly) on Spark's actorsystem is that allowed ? I
>>> understand
>>> that it might look hacky :-)
>>>
>>> Thanks.
>>> Deb
>>>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 22:35:02 2014
Return-Path: <dev-return-8963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81D0B11C29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 22:35:02 +0000 (UTC)
Received: (qmail 86051 invoked by uid 500); 20 Aug 2014 22:35:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85985 invoked by uid 500); 20 Aug 2014 22:35:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85972 invoked by uid 99); 20 Aug 2014 22:35:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:35:01 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.178 as permitted sender)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 22:34:57 +0000
Received: by mail-lb0-f178.google.com with SMTP id c11so7340324lbj.23
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 15:34:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=hQo9f8Yv1GPzAwXnXDM5OpnZiTZNGZLfgMGjWE53L0g=;
        b=kesyZ6Bgjgjfx4woW8CV5gknv7/gKFg+zNA2rpaMuMIVcmqmpqdDZN/rdDX0ZLqIyh
         Ux9Njr+pTb5mQ9e1D7vXcVh91j8VTZMaV90T1jwCzimvyq9lcVoTHz58vYTrM1mt/iZM
         B4447IGFnSfXRfF7bPIoeYzt5ygUEhaI/+xZefGoFJ4mjHbpGvtBvGDWaZ5ebAAVI7Ee
         lxidceNqS5hPoXOajM7eJyJunAQncWav4NvW7w38JTtHlOV/wJCEyqq1w4KP+roAzBEd
         fuIVU3W3F215r6MxHkwrh1qEShsKHL6eO0eHsVt4cGDM5iP0y1TWBzLYvtXlP+ze+P1M
         evzA==
MIME-Version: 1.0
X-Received: by 10.112.118.141 with SMTP id km13mr40569266lbb.37.1408574076344;
 Wed, 20 Aug 2014 15:34:36 -0700 (PDT)
Received: by 10.25.148.4 with HTTP; Wed, 20 Aug 2014 15:34:36 -0700 (PDT)
In-Reply-To: <CAEYYnxbaY-g5yBa3=gT+VHu9LD2ooCrkPM_TQCCU1CLKBcr_0w@mail.gmail.com>
References: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
	<CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
	<CA+B-+fwA7=ZYtAbU8r_m1BxRzfgeyN-zGWKvjfetEKhfqCpRPw@mail.gmail.com>
	<CAEYYnxbaY-g5yBa3=gT+VHu9LD2ooCrkPM_TQCCU1CLKBcr_0w@mail.gmail.com>
Date: Wed, 20 Aug 2014 15:34:36 -0700
Message-ID: <CA+B-+fzCPA=G3+hFZqpfveEZ1oodNeYpmN08wQs+4mGDN+q0wQ@mail.gmail.com>
Subject: Re: Akka usage in Spark
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcfd7e75df49050117353f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfd7e75df49050117353f
Content-Type: text/plain; charset=UTF-8

Yeah that's the one we discussed...sorry I pointed to a different one that
I was reading...


On Wed, Aug 20, 2014 at 3:28 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> To be specific, I was discussing this PR with Debasish which reduces
> lots of issues when sending big objects to executors without using
> broadcast explicitly.
>
> Broadcast RDD object once per TaskSet (instead of sending it for every
> task)
> https://issues.apache.org/jira/browse/SPARK-2521
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, Aug 20, 2014 at 3:19 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Patrick,
> >
> > Last few days I came across some bugs which got exposed due to ALS runs
> on
> > large scale data...although it was not related to the akka changes but
> > during the debug I found across some akka related changes that might have
> > an impact of overall performance...one example is the following:
> >
> > https://github.com/apache/spark/pull/1907
> >
> > @dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no longer
> > sent through akka msgs but over http-channels...If there is a document
> > detailing the architecture that is currently in-place (like how the core
> > changed from 1.0 to 1.1) it will help a lot in debugging the jobs which
> are
> > built upon the libraries like mllib and optimize them further for
> > efficiency...
> >
> > For using the Spark actor system directly:
> >
> > I spent few weeks December 2013 to make the Scalafish code (
> > https://github.com/azymnis/scalafish) operational on 10 nodes...It uses
> > scalding for matrix partitioning and actorSystem to coordinate the
> > updates...It is a cool use of akka but getting an actor system
> operational
> > is difficult...
> >
> > Since Spark already has tested version of actor system running on both
> > standalone and yarn modes, I am planning to port scalafish to spark using
> > actor model...That's one of the use-cases I am looking for...
> >
> > Another use-case that I am considering is to send msgs directly from
> kafka
> > queues to spark actorSystem for processing to get Storm like
> > latency...basically window sizes of 1-2 ms and no overhead of using an
> RDD
> > if possible...
> >
> > Thanks.
> > Deb
> >
> >
> > On Wed, Aug 20, 2014 at 1:42 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Hey Deb,
> >>
> >> Can you be specific what changes you are mentioning? We have not, to my
> >> knowledge, made major architectural changes around akka use.
> >>
> >> I think in general we don't want people to be using Spark's actor system
> >> directly - it is an internal communication component in Spark and could
> >> e.g. be re-factored later to not use akka at all. Could you elaborate a
> bit
> >> more on your use case?
> >>
> >> - Patrick
> >>
> >>
> >> On Wed, Aug 20, 2014 at 9:02 AM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >>
> >>> Hi,
> >>>
> >>> There have been some recent changes in the way akka is used in spark
> and I
> >>> feel they are major changes...
> >>>
> >>> Is there a design document / JIRA / experiment on large datasets that
> >>> highlight the impact of changes (1.0 vs 1.1) ? Basically it will be
> great
> >>> to understand where akka is used in the code base...
> >>>
> >>> If I don't have to broadcast big variables but use akka's programming
> >>> model
> >>> (use actors directly) on Spark's actorsystem is that allowed ? I
> >>> understand
> >>> that it might look hacky :-)
> >>>
> >>> Thanks.
> >>> Deb
> >>>
> >>
> >>
>

--047d7bfcfd7e75df49050117353f--

From dev-return-8964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 23:39:35 2014
Return-Path: <dev-return-8964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E1F111E4A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 23:39:35 +0000 (UTC)
Received: (qmail 46246 invoked by uid 500); 20 Aug 2014 23:39:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46180 invoked by uid 500); 20 Aug 2014 23:39:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46169 invoked by uid 99); 20 Aug 2014 23:39:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 23:39:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 23:39:28 +0000
Received: by mail-la0-f44.google.com with SMTP id el20so7936502lab.31
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 16:39:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=sv2L8fOfDKvXjWWCKroKZcZ5tv7Zcog+ItEdPojJV+c=;
        b=YWDfH1q30fWRV5qKhmmomtR8IyfBVby5jSEy81bgQ5VQjdBMQQLfcOpz+S8dYhy/jV
         dl/a58i7I+b/VDv1HCAHO04K3Rm5bvl4uI/KXmhQVvlBvF8jtBQZsNHxbaMYzMhSDvaq
         9sNioaK8OsSUq9C9Qa16ahR+tWB7St9VS9P4aA5QyA945WUxzITslRKmDEb4EGyMm336
         lztzTBeQXTpQZ2ffdRtBZyRyvAurGaebAkYGQ8Dwt4eO89g8v3ovB3vpIiG13SEzYwjf
         Z8ZvuDsVfd3A1qunYe2bWsNfAC1KOhsIxnFNaWBsGScYiBsnis0HZQh7ZkLcqwTdPhSK
         B8FQ==
X-Gm-Message-State: ALoCoQlFxIdjTQEGgwMVFNppLjRPSvJebNrcZQPmE+h8mcgD6FOkz2qIyOgQxJTW0jRtryGSINnr
MIME-Version: 1.0
X-Received: by 10.152.19.167 with SMTP id g7mr13386069lae.46.1408577946734;
 Wed, 20 Aug 2014 16:39:06 -0700 (PDT)
Received: by 10.25.17.151 with HTTP; Wed, 20 Aug 2014 16:39:06 -0700 (PDT)
Date: Wed, 20 Aug 2014 16:39:06 -0700
Message-ID: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
Subject: is Branch-1.1 SBT build broken for yarn-alpha ?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01493a0c2767ea0501181cd7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01493a0c2767ea0501181cd7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I just updated today's build and tried branch-1.1 for both yarn and
yarn-alpha.

For yarn build, this command seem to work fine.

sbt/sbt -Pyarn -Dhadoop.version=3D2.3.0-cdh5.0.1 projects

for yarn-alpha

sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alpha projects

I got the following

Any ideas


Chester

=E1=9A=9B |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alp=
ha
projects*

Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Home
as default JAVA_HOME.

Note, this will be overridden by -java-home if it is set.

[info] Loading project definition from
/Users/chester/projects/spark/project/project

[info] Loading project definition from
/Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/projec=
t

[warn] Multiple resolvers having different access mechanism configured with
same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
project resolvers (`resolvers`) or rename publishing resolver (`publishTo`)=
.

[info] Loading project definition from /Users/chester/projects/spark/projec=
t

org.apache.maven.model.building.ModelBuildingException: 1 problem was
encountered while building the effective model for
org.apache.spark:spark-yarn-alpha_2.10:1.1.0

*[FATAL] Non-resolvable parent POM: Could not find artifact
org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>)
and 'parent.relativePath' points at wrong local POM @ line 20, column 11*


 at
org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildi=
ngException(DefaultModelProblemCollector.java:195)

at
org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(De=
faultModelBuilder.java:841)

at
org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModel=
Builder.java:664)

at
org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuild=
er.java:310)

at
org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuild=
er.java:232)

at
com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.scala=
:61)

at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper=
.scala:128)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelpe=
r.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelpe=
r.scala:129)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala=
:244)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala=
:244)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:=
59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

at scala.collection.AbstractTraversable.map(Traversable.scala:105)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper=
.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelpe=
r.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelpe=
r.scala:129)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala=
:244)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala=
:244)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:=
59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

at scala.collection.AbstractTraversable.map(Traversable.scala:105)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper=
.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectHel=
per.scala:49)

at com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:28=
)

at SparkBuild$.projectDefinitions(SparkBuild.scala:165)

at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)

at sbt.Load$$anonfun$24.apply(Load.scala:415)

at sbt.Load$$anonfun$24.apply(Load.scala:415)

at scala.collection.immutable.Stream.flatMap(Stream.scala:442)

at sbt.Load$.loadUnit(Load.scala:415)

at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)

at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)

at
sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$=
5$$anonfun$apply$6.apply(BuildLoader.scala:93)

at
sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$=
5$$anonfun$apply$6.apply(BuildLoader.scala:92)

at sbt.BuildLoader.apply(BuildLoader.scala:143)

at sbt.Load$.loadAll(Load.scala:312)

at sbt.Load$.loadURI(Load.scala:264)

at sbt.Load$.load(Load.scala:260)

at sbt.Load$.load(Load.scala:251)

at sbt.Load$.apply(Load.scala:134)

at sbt.Load$.defaultLoad(Load.scala:37)

at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)

at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)

at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)

at
sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)

at
sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)

at
sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)

at
sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)

at sbt.Command$.process(Command.scala:95)

at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)

at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)

at sbt.State$$anon$1.process(State.scala:179)

at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)

at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)

at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)

at sbt.MainLoop$.next(MainLoop.scala:100)

at sbt.MainLoop$.run(MainLoop.scala:93)

at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)

at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)

at sbt.Using.apply(Using.scala:25)

at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)

at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)

at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)

at sbt.MainLoop$.runLogged(MainLoop.scala:25)

at sbt.StandardMain$.runManaged(Main.scala:57)

at sbt.xMain.run(Main.scala:29)

at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)

at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)

at xsbt.boot.Launch$.run(Launch.scala:109)

at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)

at xsbt.boot.Launch$.launch(Launch.scala:117)

at xsbt.boot.Launch$.apply(Launch.scala:19)

at xsbt.boot.Boot$.runImpl(Boot.scala:44)

at xsbt.boot.Boot$.main(Boot.scala:20)

at xsbt.boot.Boot.main(Boot.scala)

[error] org.apache.maven.model.building.ModelBuildingException: 1 problem
was encountered while building the effective model for
org.apache.spark:spark-yarn-alpha_2.10:1.1.0

[error] [FATAL] Non-resolvable parent POM: Could not find artifact
org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
wrong local POM @ line 20, column 11

[error] Use 'last' for the full log.

Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q

--089e01493a0c2767ea0501181cd7--

From dev-return-8965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 20 23:42:30 2014
Return-Path: <dev-return-8965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7FE111E52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Aug 2014 23:42:30 +0000 (UTC)
Received: (qmail 50379 invoked by uid 500); 20 Aug 2014 23:42:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50324 invoked by uid 500); 20 Aug 2014 23:42:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50308 invoked by uid 99); 20 Aug 2014 23:42:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 23:42:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.215.51 as permitted sender)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Aug 2014 23:42:23 +0000
Received: by mail-la0-f51.google.com with SMTP id pn19so7805380lab.38
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 16:42:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=o63rj2OyUz/ng1mDxVl4izH5BJO0igt4iDLm7SEG6vQ=;
        b=DUkaf1mEfIMujcz4CFFMdQ/Dku702mvIVtwA1aVtUnw0tEGtf52O0iy+g7n4n0h6aV
         mHZx5oUCly7Iu4KIemlAjONTDRNlR13EO/Qdlqc8CQap/WN3fNag/D6QshTfYIAd6Mpk
         Nn1fSn9btgbeaQd37wx3rOnwW7/O5J3I1EOeL0XkrAFC5Bz6Mhdfnej5b4i5jsgR+l8W
         3Vc9lF14roXSDKQIxXbR5d6c4inAnnGTWnZvflPw4jMRZV1gBjzxMK+AZAJjz1yKaCd6
         rekuYpDuGmXexpfDKbwHmX4dtvMVQUmFx3t2D188Z/CNL2pMOvAcEqPuOZMHAdeimCE3
         KLvQ==
X-Gm-Message-State: ALoCoQl8AnooSK/kEVYFvmvYStLH+4DM8QZ80khE0mhUIA3O6ja7JtteiLF6GhMUQqlcutLZ+iaf
MIME-Version: 1.0
X-Received: by 10.152.36.135 with SMTP id q7mr46215666laj.42.1408578121666;
 Wed, 20 Aug 2014 16:42:01 -0700 (PDT)
Received: by 10.25.17.151 with HTTP; Wed, 20 Aug 2014 16:42:01 -0700 (PDT)
In-Reply-To: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
Date: Wed, 20 Aug 2014 16:42:01 -0700
Message-ID: <CAPYnQ0Uf3qYR8w_CzpKtvp4G-ecNv=77ZfvQHuQvKhNGLNn8eQ@mail.gmail.com>
Subject: Re: is Branch-1.1 SBT build broken for yarn-alpha ?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b8a094a77505011826f6
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b8a094a77505011826f6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Just tried on master branch, and the master branch works fine for yarn-alph=
a


On Wed, Aug 20, 2014 at 4:39 PM, Chester Chen <chester@alpinenow.com> wrote=
:

> I just updated today's build and tried branch-1.1 for both yarn and
> yarn-alpha.
>
> For yarn build, this command seem to work fine.
>
> sbt/sbt -Pyarn -Dhadoop.version=3D2.3.0-cdh5.0.1 projects
>
> for yarn-alpha
>
> sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alpha projects
>
> I got the following
>
> Any ideas
>
>
> Chester
>
> =E1=9A=9B |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-a=
lpha
> projects*
>
> Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Hom=
e
> as default JAVA_HOME.
>
> Note, this will be overridden by -java-home if it is set.
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project/project
>
> [info] Loading project definition from
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/proj=
ect
>
> [warn] Multiple resolvers having different access mechanism configured
> with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
> project resolvers (`resolvers`) or rename publishing resolver (`publishTo=
`).
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project
>
> org.apache.maven.model.building.ModelBuildingException: 1 problem was
> encountered while building the effective model for
> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>
> *[FATAL] Non-resolvable parent POM: Could not find artifact
> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>=
)
> and 'parent.relativePath' points at wrong local POM @ line 20, column 11*
>
>
>  at
> org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuil=
dingException(DefaultModelProblemCollector.java:195)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(=
DefaultModelBuilder.java:841)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultMod=
elBuilder.java:664)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:310)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:232)
>
> at
> com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.sca=
la:61)
>
> at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:128)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectH=
elper.scala:49)
>
> at
> com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:28)
>
> at SparkBuild$.projectDefinitions(SparkBuild.scala:165)
>
> at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at scala.collection.immutable.Stream.flatMap(Stream.scala:442)
>
> at sbt.Load$.loadUnit(Load.scala:415)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
>
> at sbt.BuildLoader.apply(BuildLoader.scala:143)
>
> at sbt.Load$.loadAll(Load.scala:312)
>
> at sbt.Load$.loadURI(Load.scala:264)
>
> at sbt.Load$.load(Load.scala:260)
>
> at sbt.Load$.load(Load.scala:251)
>
> at sbt.Load$.apply(Load.scala:134)
>
> at sbt.Load$.defaultLoad(Load.scala:37)
>
> at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>
> at sbt.Command$.process(Command.scala:95)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.State$$anon$1.process(State.scala:179)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>
> at sbt.MainLoop$.next(MainLoop.scala:100)
>
> at sbt.MainLoop$.run(MainLoop.scala:93)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)
>
> at sbt.Using.apply(Using.scala:25)
>
> at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)
>
> at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)
>
> at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)
>
> at sbt.MainLoop$.runLogged(MainLoop.scala:25)
>
> at sbt.StandardMain$.runManaged(Main.scala:57)
>
> at sbt.xMain.run(Main.scala:29)
>
> at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
>
> at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)
>
> at xsbt.boot.Launch$.run(Launch.scala:109)
>
> at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)
>
> at xsbt.boot.Launch$.launch(Launch.scala:117)
>
> at xsbt.boot.Launch$.apply(Launch.scala:19)
>
> at xsbt.boot.Boot$.runImpl(Boot.scala:44)
>
> at xsbt.boot.Boot$.main(Boot.scala:20)
>
> at xsbt.boot.Boot.main(Boot.scala)
>
> [error] org.apache.maven.model.building.ModelBuildingException: 1 problem
> was encountered while building the effective model for
> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>
> [error] [FATAL] Non-resolvable parent POM: Could not find artifact
> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
> wrong local POM @ line 20, column 11
>
> [error] Use 'last' for the full log.
>
> Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q
>

--089e0160b8a094a77505011826f6--

From dev-return-8966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 06:05:32 2014
Return-Path: <dev-return-8966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF55811600
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 06:05:32 +0000 (UTC)
Received: (qmail 18113 invoked by uid 500); 21 Aug 2014 06:05:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18049 invoked by uid 500); 21 Aug 2014 06:05:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18036 invoked by uid 99); 21 Aug 2014 06:05:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 06:05:31 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 54.254.200.128 as permitted sender)
Received: from [54.254.200.128] (HELO smtpbgsg2.qq.com) (54.254.200.128)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 06:05:27 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1408601098; bh=B735Y05JKt4UqRx7YWmCujSYzmKuhhzDrwllJ8LZEK4=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=rfJkaHYVIfjdubAfebZ9DbqKkoE0ZFywdilPOKaFKznvRQdoPuTtF9IXJ5QwQ/Cr5
	 P6KrWSI4QM2bAauzYdyc9QyeJBNlkAxPSCUO7Az6a07S3TVIXS7V0tHNkTXmotKuLi
	 /epEd7Y7Dao5ci90vbmAsh6XKk7DPDz0Ljd1NUNc=
X-QQ-FEAT: EMqaHXJuqZHMDebneSEg69vtAh94qDH6o7w9hnFEs14SDN+MuJiDo+jOuvGlg
	zi7v3yw/H8TwBB07h0I+M1Kg/ezVCcxaIcQI+1CCk3LQ+13hF1/jPcj+aJ+oNqE1JftavYL
	eYH6Tn36CqPaX4aFnZGhsSyb1F95PCZRAtD3+J00nmfOJWY7x9BISAUakP0OksXiQa4Yckw
	vDe3zk41nYg==
X-QQ-SSF: 000000000000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <CAPYnQ0Uf3qYR8w_CzpKtvp4G-ecNv=77ZfvQHuQvKhNGLNn8eQ@mail.gmail.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
	<CAPYnQ0Uf3qYR8w_CzpKtvp4G-ecNv=77ZfvQHuQvKhNGLNn8eQ@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1408601096t9262636
From: "=?gb18030?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?gb18030?B?Q2hlc3RlciBDaGVu?=" <chester@alpinenow.com>, "=?gb18030?B?ZGV2?=" <dev@spark.apache.org>
Subject: =?gb18030?B?u9i4tKO6IGlzIEJyYW5jaC0xLjEgU0JUIGJ1aWxk?=
 =?gb18030?B?IGJyb2tlbiBmb3IgeWFybi1hbHBoYSA/?=
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53F58C08_08DD4458_0DA45FBC"
Content-Transfer-Encoding: 8Bit
Date: Thu, 21 Aug 2014 14:04:56 +0800
X-Priority: 3
Message-ID: <tencent_09CCB00D336173BC48B19F86@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 393042031
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53F58C08_08DD4458_0DA45FBC
Content-Type: text/plain;
	charset="gb18030"
Content-Transfer-Encoding: base64
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------=_NextPart_53F58C08_08DD4458_0DA45FBC--




From dev-return-8967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 06:36:32 2014
Return-Path: <dev-return-8967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADA4D116AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 06:36:32 +0000 (UTC)
Received: (qmail 90617 invoked by uid 500); 21 Aug 2014 06:36:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90546 invoked by uid 500); 21 Aug 2014 06:36:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90534 invoked by uid 99); 21 Aug 2014 06:36:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 06:36:31 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 06:36:03 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so10438285vcb.1
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 23:36:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=uETXntA9FpVYgmSwbadIY6PNVnf2w1Dghh4VmbgGFRg=;
        b=aQmyq+63LuzBNRLn1VrD6fvv8nQZpyCePBBZmtrhCl++UDdwg1K/RuUn/t7p6gQhz4
         LMmSQiIbiEn4sWMIJtn0iDaLuIArle8Ce3RxgX1f3aZlgl/gom5vNqHBz2+b9SOySjf1
         dEDDobv7vuWnEKbEr+gsodwC73RTDoRf9oalyCqTpBKw/d17KPg6BkII5geSpGMiHM3B
         xRxX+I2xkXyPtICFxz9KwOQYeiLH3SsxNiOdeQ4KP0pQ4zRTSptPbNmaxoNaoalbEmkW
         ui1DydA7uSBR9i0feuytfZh9m9dhDlJtu2S3n5G2IdiOZwfri+P09px+Keids3pVo00z
         MHDA==
X-Gm-Message-State: ALoCoQkn/KnBWJAL1XXlAJE1+O6DEeUkcSbqiMnoF8SJkiiQDcaHT9jnabdCC7sbCnDl4h3qJ5/2
X-Received: by 10.52.156.100 with SMTP id wd4mr4641557vdb.39.1408602961114;
        Wed, 20 Aug 2014 23:36:01 -0700 (PDT)
Received: from mail-vc0-f178.google.com (mail-vc0-f178.google.com [209.85.220.178])
        by mx.google.com with ESMTPSA id tx6sm75247312vdb.28.2014.08.20.23.35.49
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 20 Aug 2014 23:35:49 -0700 (PDT)
Received: by mail-vc0-f178.google.com with SMTP id la4so10316368vcb.37
        for <dev@spark.apache.org>; Wed, 20 Aug 2014 23:35:49 -0700 (PDT)
X-Received: by 10.220.114.5 with SMTP id c5mr40461385vcq.28.1408602949573;
 Wed, 20 Aug 2014 23:35:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Wed, 20 Aug 2014 23:35:29 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Wed, 20 Aug 2014 23:35:29 -0700
Message-ID: <CA+-p3AF4Vsu0OEE8VX1jUWk-dUWs_5HPp1nFTFRc28qEeMyJwA@mail.gmail.com>
Subject: Hang on Executor classloader lookup for the remote REPL URL classloader
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122a71e7066b405011deee1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122a71e7066b405011deee1
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

I'm seeing a stacktrace where the classloader that reads from the REPL is
hung, and blocking all progress on that executor.  Below is that hung
thread's stacktrace, and also the stacktrace of another hung thread.

I thought maybe there was an issue with the REPL's JVM on the other side,
but didn't see anything useful in that stacktrace either.

Any ideas what I should be looking for?

Thanks!
Andrew


"Executor task launch worker-0" daemon prio=10 tid=0x00007f780c208000
nid=0x6ae9 runnable [0x00007f78c2eeb000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked <0x00007f7e13ea9560> (a java.io.BufferedInputStream)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
        at
sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
        - locked <0x00007f7e13e9eeb0> (a
sun.net.www.protocol.http.HttpURLConnection)
        at java.net.URL.openStream(URL.java:1037)
        at
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:86)
        at
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:63)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        - locked <0x00007f7fc9018980> (a
org.apache.spark.repl.ExecutorClassLoader)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:82)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)


And the other threads are stuck on the Class.forName0() method too:

"Executor task launch worker-4" daemon prio=10 tid=0x00007f780c20f000
nid=0x6aed waiting for monitor entry [0x00007f78c2ae8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:79)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

asdf

--089e0122a71e7066b405011deee1--

From dev-return-8968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 07:05:42 2014
Return-Path: <dev-return-8968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D09DE11750
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 07:05:42 +0000 (UTC)
Received: (qmail 48187 invoked by uid 500); 21 Aug 2014 07:05:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48119 invoked by uid 500); 21 Aug 2014 07:05:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48091 invoked by uid 99); 21 Aug 2014 07:05:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 07:05:41 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 07:05:16 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so7950350wiv.1
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 00:05:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Sa+4lGRmOVXtj/WkUvrKc5Uc1wyZijsZ54i1ti3nDH0=;
        b=zNgai8mY1XBB7bsD24Vax+M0b9rJgaAlbbJ57XP4HHqrAsh9HmNSreHgpN9uddxvTD
         2lxqnbfuygUgTfvoIZ2ujRgk5fTBqb+GDhfTQah7zXdgQUu8iUYpejh8tPlArjRu35S1
         iS8n3uyabB0B7TkXSu5KhDcIe0yeAmrHuJ/5gsDpkpHtxOWtBoSQH/QfAhf+ljfXh7Cg
         7hfXkT/hZ9U4E4gqRZaNecYtV/8kDUrFyNVtZ3Tic1vrzpg6GUo50dFoZm8bdKS/TN3O
         WeR/um3f7z+0jlikFFKRDRftHnu9koEoQNEsevkj0aYa6EmLYfcoAOnQYhk3dJr/wZn7
         d3AQ==
X-Received: by 10.180.24.35 with SMTP id r3mr2473806wif.71.1408604715831; Thu,
 21 Aug 2014 00:05:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.153.10 with HTTP; Thu, 21 Aug 2014 00:04:55 -0700 (PDT)
In-Reply-To: <CA+B-+fzCPA=G3+hFZqpfveEZ1oodNeYpmN08wQs+4mGDN+q0wQ@mail.gmail.com>
References: <CA+B-+fzL-fuunWV_kQxzSOtOn7+Wm+oa4Tf+XJRfHNXTWF-kaw@mail.gmail.com>
 <CABPQxstViWK8i4P3nUC9_P4S3XTu0cgHC-BdK6+ONFjgKh-ytQ@mail.gmail.com>
 <CA+B-+fwA7=ZYtAbU8r_m1BxRzfgeyN-zGWKvjfetEKhfqCpRPw@mail.gmail.com>
 <CAEYYnxbaY-g5yBa3=gT+VHu9LD2ooCrkPM_TQCCU1CLKBcr_0w@mail.gmail.com> <CA+B-+fzCPA=G3+hFZqpfveEZ1oodNeYpmN08wQs+4mGDN+q0wQ@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Thu, 21 Aug 2014 12:34:55 +0530
Message-ID: <CAAqHKj5Fgr6a=U9aXjpDiATXUPY2gGtDkutB4hHwV4d-QcU85Q@mail.gmail.com>
Subject: Re: Akka usage in Spark
To: Debasish Das <debasish.das83@gmail.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043bdf20b75c2e05011e5787
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043bdf20b75c2e05011e5787
Content-Type: text/plain; charset=UTF-8

The stream receiver seems to leverage actor receivers
     http://spark.apache.org/docs/0.8.1/streaming-custom-receivers.html
But spark system doesnt lend itself to a messaging kind of a structure..
more of a DAG kind
Just curious are you looking for the actor subsystem to act on messages or
just looking to use them as a local/distributed message bus

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Thu, Aug 21, 2014 at 4:04 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Yeah that's the one we discussed...sorry I pointed to a different one that
> I was reading...
>
>
> On Wed, Aug 20, 2014 at 3:28 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>
> > To be specific, I was discussing this PR with Debasish which reduces
> > lots of issues when sending big objects to executors without using
> > broadcast explicitly.
> >
> > Broadcast RDD object once per TaskSet (instead of sending it for every
> > task)
> > https://issues.apache.org/jira/browse/SPARK-2521
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Wed, Aug 20, 2014 at 3:19 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> > > Hi Patrick,
> > >
> > > Last few days I came across some bugs which got exposed due to ALS runs
> > on
> > > large scale data...although it was not related to the akka changes but
> > > during the debug I found across some akka related changes that might
> have
> > > an impact of overall performance...one example is the following:
> > >
> > > https://github.com/apache/spark/pull/1907
> > >
> > > @dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no
> longer
> > > sent through akka msgs but over http-channels...If there is a document
> > > detailing the architecture that is currently in-place (like how the
> core
> > > changed from 1.0 to 1.1) it will help a lot in debugging the jobs which
> > are
> > > built upon the libraries like mllib and optimize them further for
> > > efficiency...
> > >
> > > For using the Spark actor system directly:
> > >
> > > I spent few weeks December 2013 to make the Scalafish code (
> > > https://github.com/azymnis/scalafish) operational on 10 nodes...It
> uses
> > > scalding for matrix partitioning and actorSystem to coordinate the
> > > updates...It is a cool use of akka but getting an actor system
> > operational
> > > is difficult...
> > >
> > > Since Spark already has tested version of actor system running on both
> > > standalone and yarn modes, I am planning to port scalafish to spark
> using
> > > actor model...That's one of the use-cases I am looking for...
> > >
> > > Another use-case that I am considering is to send msgs directly from
> > kafka
> > > queues to spark actorSystem for processing to get Storm like
> > > latency...basically window sizes of 1-2 ms and no overhead of using an
> > RDD
> > > if possible...
> > >
> > > Thanks.
> > > Deb
> > >
> > >
> > > On Wed, Aug 20, 2014 at 1:42 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > >
> > >> Hey Deb,
> > >>
> > >> Can you be specific what changes you are mentioning? We have not, to
> my
> > >> knowledge, made major architectural changes around akka use.
> > >>
> > >> I think in general we don't want people to be using Spark's actor
> system
> > >> directly - it is an internal communication component in Spark and
> could
> > >> e.g. be re-factored later to not use akka at all. Could you elaborate
> a
> > bit
> > >> more on your use case?
> > >>
> > >> - Patrick
> > >>
> > >>
> > >> On Wed, Aug 20, 2014 at 9:02 AM, Debasish Das <
> debasish.das83@gmail.com
> > >
> > >> wrote:
> > >>
> > >>> Hi,
> > >>>
> > >>> There have been some recent changes in the way akka is used in spark
> > and I
> > >>> feel they are major changes...
> > >>>
> > >>> Is there a design document / JIRA / experiment on large datasets that
> > >>> highlight the impact of changes (1.0 vs 1.1) ? Basically it will be
> > great
> > >>> to understand where akka is used in the code base...
> > >>>
> > >>> If I don't have to broadcast big variables but use akka's programming
> > >>> model
> > >>> (use actors directly) on Spark's actorsystem is that allowed ? I
> > >>> understand
> > >>> that it might look hacky :-)
> > >>>
> > >>> Thanks.
> > >>> Deb
> > >>>
> > >>
> > >>
> >
>

--f46d043bdf20b75c2e05011e5787--

From dev-return-8969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 08:13:30 2014
Return-Path: <dev-return-8969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 667D51189C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 08:13:30 +0000 (UTC)
Received: (qmail 53603 invoked by uid 500); 21 Aug 2014 08:13:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53551 invoked by uid 500); 21 Aug 2014 08:13:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53539 invoked by uid 99); 21 Aug 2014 08:13:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 08:13:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 08:13:01 +0000
Received: by mail-oi0-f47.google.com with SMTP id x69so6451809oia.20
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 01:12:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MYlPYQbU4Qki1KvSzJHobgKUz2Zq2FpnmolIhpMA1RI=;
        b=MV80/tLXSMMJ5fyHvX/ekjs/vDRzIcmYGsPsZZ3HKR9eLcREXaVEMDsrzhpgUtxiWQ
         0XAnCTrxA5cH8lp5JR2M4qnrRF8iD1WhmkaFsGL9WMvBzhSd9NzyQlWnmpTouucjJMq3
         5Cr0SG9tCGcByFFopyhY+C3yl9vOU7A89ucdSMhJzvcgwKg1DkRXrEiQoWKWT7KxN6Yy
         mmutjYNUIg4OC38I3x1pfPIWZNU1TpDBqdU07tHtO48W69ajxHwlpIWacsB2AiqBYr75
         pkxgJer9qAdEUIkApcC+0NB857QRzaPYevBMwEWB4j6bLkReR9XQy3EhEPBTLAEafaXC
         2Qpg==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr52620810obb.34.1408608779561;
 Thu, 21 Aug 2014 01:12:59 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Thu, 21 Aug 2014 01:12:59 -0700 (PDT)
Date: Thu, 21 Aug 2014 01:12:59 -0700
Message-ID: <CABPQxsvVoEbkqo5nF2CPQ15E44g6pLTnuKJKxtQ=6gsCH7GF1A@mail.gmail.com>
Subject: [SNAPSHOT] Snapshot2 of Spark 1.1 has been posted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cf60eef15a05011f4987
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cf60eef15a05011f4987
Content-Type: text/plain; charset=ISO-8859-1

Hi All,

I've packaged and published a snapshot release of Spark 1.1 for testing.
This is very close to RC1 and we are distributing it for testing. Please
test this and report any issues on this thread.

The tag of this release is v1.1.0-snapshot1 (commit e1535ad3):
*https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba
<https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba>*

The release files, including signatures, digests, etc can be found at:
*http://people.apache.org/~pwendell/spark-1.1.0-snapshot2/
<http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/>*

Release artifacts are signed with the following key:
*https://people.apache.org/keys/committer/pwendell.asc
<https://people.apache.org/keys/committer/pwendell.asc>*

The staging repository for this release can be found at:
[NOTE: Apache Sonatype is down preventing us from cutting this]
https://repository.apache.org/content/repositories/orgapachespark-1026/
<https://repository.apache.org/content/repositories/orgapachespark-1024/>


To learn more about Apache Spark, please see
http://spark.apache.org/

--089e0149cf60eef15a05011f4987--

From dev-return-8970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 08:27:33 2014
Return-Path: <dev-return-8970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E1F5118D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 08:27:33 +0000 (UTC)
Received: (qmail 77079 invoked by uid 500); 21 Aug 2014 08:27:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77020 invoked by uid 500); 21 Aug 2014 08:27:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77007 invoked by uid 99); 21 Aug 2014 08:27:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 08:27:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 08:27:04 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so6349758oig.26
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 01:27:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WfH7NzKliKH1NW36bd1xy4LcksSbzNRdKWSnBUvIH9k=;
        b=rxW88wQAW7lx6KCbW5uL9uLVNbUEfJY7DPxxOrXDPH3oZeJnVLhhc8Vl308IgOTULV
         CjGouRPvilMBgPGB7Cet9BIf1xLVI2fhxMUrL0+9k0mKmzYXZLop3PhrAEpw5ylmZUqm
         wbrvggk+o4qJteSVuwNP6SysAOGMeQZnXHeBtF4v4HXSMWTNM7DHf4FX1zyoU5guavFk
         rk8wvXevfFac5z2a2xZ8aI9kYQE0zi5bN/Xnm/Owulv72hv3ite5yt4nUKIiORO5inRv
         65KYpgmFbkrVra9UmhlxlwvnfsbOUUhMZIOH0M0O3Gttt4gGtAZ9pI91U0H9+Dsc4Z6B
         pyHg==
MIME-Version: 1.0
X-Received: by 10.60.67.34 with SMTP id k2mr42910369oet.52.1408609622947; Thu,
 21 Aug 2014 01:27:02 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Thu, 21 Aug 2014 01:27:02 -0700 (PDT)
In-Reply-To: <CABPQxsvVoEbkqo5nF2CPQ15E44g6pLTnuKJKxtQ=6gsCH7GF1A@mail.gmail.com>
References: <CABPQxsvVoEbkqo5nF2CPQ15E44g6pLTnuKJKxtQ=6gsCH7GF1A@mail.gmail.com>
Date: Thu, 21 Aug 2014 01:27:02 -0700
Message-ID: <CABPQxssDMcjr2LDe22ZW0ow4ERrrVz4TEyw=JMA=mVL4RAscsQ@mail.gmail.com>
Subject: Re: [SNAPSHOT] Snapshot2 of Spark 1.1 has been posted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e29834145605011f7cf5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e29834145605011f7cf5
Content-Type: text/plain; charset=ISO-8859-1

The docs for this release are also available here:

http://people.apache.org/~pwendell/spark-1.1.0-snapshot2-docs/


On Thu, Aug 21, 2014 at 1:12 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hi All,
>
> I've packaged and published a snapshot release of Spark 1.1 for testing.
> This is very close to RC1 and we are distributing it for testing. Please
> test this and report any issues on this thread.
>
> The tag of this release is v1.1.0-snapshot1 (commit e1535ad3):
> *https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba
> <https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba>*
>
> The release files, including signatures, digests, etc can be found at:
> *http://people.apache.org/~pwendell/spark-1.1.0-snapshot2/
> <http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/>*
>
> Release artifacts are signed with the following key:
> *https://people.apache.org/keys/committer/pwendell.asc
> <https://people.apache.org/keys/committer/pwendell.asc>*
>
> The staging repository for this release can be found at:
> [NOTE: Apache Sonatype is down preventing us from cutting this]
> https://repository.apache.org/content/repositories/orgapachespark-1026/
> <https://repository.apache.org/content/repositories/orgapachespark-1024/>
>
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>

--001a11c2e29834145605011f7cf5--

From dev-return-8971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 10:18:36 2014
Return-Path: <dev-return-8971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A84411C05
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 10:18:36 +0000 (UTC)
Received: (qmail 83461 invoked by uid 500); 21 Aug 2014 10:18:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83407 invoked by uid 500); 21 Aug 2014 10:18:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83396 invoked by uid 99); 21 Aug 2014 10:18:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 10:18:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 10:18:10 +0000
Received: by mail-ig0-f169.google.com with SMTP id r2so12498660igi.2
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 03:18:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=MrxzsyhGvBDAF6CGCx16DMshNucVNuH25Zy9Hyzp6YE=;
        b=L8Qgbc6/RgHGSRXv+YrjQKzxwrKujJI47Ldxfc4V12j7DYkZnw0aLM1P61EN454+eJ
         n3EvfTGOp3TzxTwAXMdwFHOXL72bNJmcVaD5jhsTSNX1FChfM8uwTKXEyyGrracr2oGk
         08DDQA6j8+4TBKRH5UV4/fH3xkRJt8SMjma+MXJC+Nn16LgXQsjDO4vPZ8gMSE2Uer79
         JW0QcYNp+MeT7krMZLZShh15J0yEr1Ss5yQlbRp0SNfU7HW1b7dp+Y1+/D3QPTKBL2iZ
         X3DsMwc8eRrksuvxsR1wkm0KrZ35Ryy6tAfjUQeSzEEkg+1y6nToYapiQc2KDLUqmjsy
         Ku3w==
X-Gm-Message-State: ALoCoQkDKNBudKlPbjWPA/kpHRFp2w7cToCNWDYEGo9QlUWU0W2z+0ySXpUSEdlpinm4p8LQyK/Q
X-Received: by 10.42.126.82 with SMTP id d18mr40044ics.88.1408616288861; Thu,
 21 Aug 2014 03:18:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Thu, 21 Aug 2014 03:17:48 -0700 (PDT)
In-Reply-To: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 21 Aug 2014 11:17:48 +0100
Message-ID: <CAMAsSd+R6b=hAH8R7Vy=9P4YLpOK_b74WKhBeYb3VpvYEP+rxw@mail.gmail.com>
Subject: Re: is Branch-1.1 SBT build broken for yarn-alpha ?
To: Chester Chen <chester@alpinenow.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Maven is just telling you that there is no version 1.1.0 of
yarn-parent, and indeed, it has not been released. To build the branch
you would need to "mvn install" to compile and make available local
copies of artifacts along the way. (You may have these for
1.1.0-SNAPSHOT locally already). Use Maven, not SBT, for building
releases.

On Thu, Aug 21, 2014 at 12:39 AM, Chester Chen <chester@alpinenow.com> wrote:
> *[FATAL] Non-resolvable parent POM: Could not find artifact
> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>)
> and 'parent.relativePath' points at wrong local POM @ line 20, column 11*

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 10:25:41 2014
Return-Path: <dev-return-8972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8369D11CAD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 10:25:41 +0000 (UTC)
Received: (qmail 97557 invoked by uid 500); 21 Aug 2014 10:25:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97491 invoked by uid 500); 21 Aug 2014 10:25:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97479 invoked by uid 99); 21 Aug 2014 10:25:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 10:25:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of maisnam.ns@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 10:25:35 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so7881644qap.23
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 03:25:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=dKIPhKl13f8QPY4BsJjZS9aW2dhfsDRUSuHfrHcaYvs=;
        b=kP0GzQ53vLRhQmucuhjCedT69X+nGiMUe/qVE/f9diAiBErndGvEdADRBzvz8zyOxo
         XebA50KP0bd4Ehwyh6Bx8m7vUSEnSu99PYLOVExVpzDuL/dFrI7aiePggfbvh/P9TTos
         mQ7NxEUiP0JrGNBLKSxUnpDj+82bsAxIqUPFaruCKroYAfZ2RjwtgcGAuq0e+NhPvybO
         hc6zZszprb5ZBc0wn76nT3r7pe0R87B1u5XdEha/hErIYcj1P/ID4W0a7xlMwOLrbu7E
         tbQzdHB1FR79PeVG+q16U/NA7rJyD89QISLsX7XRkRlXUYBDzTswJ2M6EWnzFhr7k4UI
         95Mw==
MIME-Version: 1.0
X-Received: by 10.140.102.162 with SMTP id w31mr81420269qge.67.1408616714559;
 Thu, 21 Aug 2014 03:25:14 -0700 (PDT)
Received: by 10.140.94.214 with HTTP; Thu, 21 Aug 2014 03:25:14 -0700 (PDT)
Date: Thu, 21 Aug 2014 15:55:14 +0530
Message-ID: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
Subject: Spark Contribution
From: Maisnam Ns <maisnam.ns@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c169b8e5653605012122ee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c169b8e5653605012122ee
Content-Type: text/plain; charset=UTF-8

Hi,

Can someone help me with some links on how to contribute for Spark

Regards
mns

--001a11c169b8e5653605012122ee--

From dev-return-8973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 11:25:22 2014
Return-Path: <dev-return-8973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BCF3711E0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 11:25:21 +0000 (UTC)
Received: (qmail 6963 invoked by uid 500); 21 Aug 2014 11:25:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6892 invoked by uid 500); 21 Aug 2014 11:25:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6880 invoked by uid 99); 21 Aug 2014 11:25:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 11:25:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.217.175 as permitted sender)
Received: from [209.85.217.175] (HELO mail-lb0-f175.google.com) (209.85.217.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 11:25:14 +0000
Received: by mail-lb0-f175.google.com with SMTP id 10so8108425lbg.34
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 04:24:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=zWCwALFdH1piQKZZ71Z475hUp+5vuKHWsFLHZe8+NPs=;
        b=hiM2s0nmn6abav6xamxZbpMeMAZg087jeDMOYFc+L1f6Aozoh6l+HG5w6hgMh3Tenx
         4R1Wp02aFRAF+wNH28B/HLJ1XozadBNeWmimLBnxC4rewz5gMEt71mQ5TEHQzjChCtKs
         1E2FH+SLaoTgSUJWWHfCqWhlJ4nkwUp4WyzbgNM6+2Vb15FEb/Z4PsM0hbHEG0CIU/W4
         m4dUQt1FbB522c9jokKFi/Q3M388caxtTYcY5jSGHa6MOAf2JrRf9OQPEoB8ljEFES6i
         rhCZ1n698k+mahZZdsmUULd/Cew/QF9c1Lxnunt9J5ZCTDWEVGqVP1ZjxXO8YqcdzbBU
         SXnQ==
MIME-Version: 1.0
X-Received: by 10.152.120.38 with SMTP id kz6mr11971205lab.8.1408620293364;
 Thu, 21 Aug 2014 04:24:53 -0700 (PDT)
Received: by 10.153.3.8 with HTTP; Thu, 21 Aug 2014 04:24:53 -0700 (PDT)
Date: Thu, 21 Aug 2014 16:54:53 +0530
Message-ID: <CAJOb8bsxLa8c_zY3cZf_+hd9ebSjV=dsOa5cRfbjjOWGVyNv_w@mail.gmail.com>
Subject: Kinesis streaming integration in upcoming 1.1
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122aefa35a284050121f86d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122aefa35a284050121f86d
Content-Type: text/plain; charset=UTF-8

Hi everyone

I started looking at Kinesis integration and it looks promising.  However,
I feel like it can be improved. Here are my thoughts:

1. It assumes that AWS credentials are provided
by DefaultAWSCredentialsProviderChain and there is no way to change the
behavior. I would have liked to have an ability to provide a different
AWSCredentialsProvider.

2. I feel like modules in extras need to be independent from Spark build
and should perhaps be in separate repository/repositories. I had to
download most recent checkout of Spark and slap kinesis-asl into Spark
1.0.2 to create a custom spark-streaming-kinesis-asl_2.10-1.0.2.jar that I
can use in my Spark jobs. Ideally, people would want extra modules to be
cross built against different versions of Spark. Having independent
repositories can enable us to deliver build for extras packages faster than
Spark releases and they would be readily available to earlier versions of
Spark. This can free up Spark developers to focus on enhancements in the
core framework instead of managing spark-* integration pull requests.

3. Maybe it's just me, but I could have liked a Context like API for
creating Kinesis streams instead of using KinesisUtils. It makes it a
little more consistent with rest of the Spark API. We could have have
a KinesisContext which goes like this:
class KinesisStreamingContext(@transient ssc: StreamingContext,
endpointUrl: String, defaultCredentialsProvider: AWSCredentialsProvider) {

  def createStream(streamName: String,
      checkpointInterval: Duration,
      initialPositionInStream: InitialPositionInStream,
      storageLevel: StorageLevel,
      credentialsProvider: AWSCredentialsProvider =
defaultCredentialsProvider) {...}
}

4. The example KinesisWordCountASL creates numShards receiver instances
which makes sense. Maybe the API should provide ability to provide
parallelism and default to numShards?

I can submit pull requests for some of the above items, provided the
community agrees and nobody else is working on it.

Thanks,
Aniket

--089e0122aefa35a284050121f86d--

From dev-return-8974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 11:35:27 2014
Return-Path: <dev-return-8974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA0C511E3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 11:35:26 +0000 (UTC)
Received: (qmail 24653 invoked by uid 500); 21 Aug 2014 11:35:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24588 invoked by uid 500); 21 Aug 2014 11:35:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24572 invoked by uid 99); 21 Aug 2014 11:35:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 11:35:26 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 11:35:21 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XKQdx-0006C2-0a
	for dev@spark.incubator.apache.org; Thu, 21 Aug 2014 04:35:01 -0700
Date: Thu, 21 Aug 2014 04:35:01 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408620901005-7937.post@n3.nabble.com>
In-Reply-To: <1408467214979-7914.post@n3.nabble.com>
References: <1408467214979-7914.post@n3.nabble.com>
Subject: Re: Spark SQL Query and join different data sources.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

as far as i know, HQL queries try to find the schema info of all the tables
in this query from hive metastore, so it is not possible to join tables from
sqlContext using hiveContext.hql

but this should work:

hiveContext.hql("select ...").regAsTable("a")
sqlContext.jsonFile("xxx").regAsTable("b")

then

sqlContext.sql( a join b )


i created a ticket SPARK-2710 to add ResultSets from JDBC connection as a
new data source, but no predicate push down yet, also, it is not available
for HQL

so, if you are looking for something that can query different data sources
with full SQL92 syntax, facebook presto is still the only choice, they have
some kind of JDBC connector in deveopment, and there are some unofficial
implementations...

but i am looking forward to seeing the progress of Spark SQL, after
SPARK-2179 SQLContext can handle any
kind of structured data with a sequence of DataTypes as schema, although
turning the data into Rows is still a little bit tricky...



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914p7937.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 13:45:28 2014
Return-Path: <dev-return-8975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92D55111D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 13:45:28 +0000 (UTC)
Received: (qmail 997 invoked by uid 500); 21 Aug 2014 13:45:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 947 invoked by uid 500); 21 Aug 2014 13:45:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 933 invoked by uid 99); 21 Aug 2014 13:45:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 13:45:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 13:45:23 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so8294596qaq.13
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 06:45:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=z49Z6S533fBvbMwjGLro1M2emvWB+/AYT8BtbEWmGsY=;
        b=yOK+BILs6skAZvu2NuBVlh7YP4y525FiDjI1QVHWDNrjNM0/lYF2E1U4aA2KfbW3St
         OFnGwaa5uJcsMOLbGVSELOqWiCDBPlN1wU0ZYaXW/IPDXHbJXYu/3t9ixWFQGR1eSDo7
         oe6/IXdtCN9apkEWMYkcdhTTwZuxjxZitBrNth5siKplnCu+LmSNzMjL8auj3TtQWKdF
         YAmQOatZVPACJgpW7dy2soUoFHdWh+Hw5LzsM2eZomiwuPgvsNsXxY6JUSPiDf6wpClo
         +CumF0PD8yUWIdGVlyHAgRRYeoe4UV94MRr0H3+ldFifQRIgVbzDuWIsdknAexJXzMYH
         KD8Q==
MIME-Version: 1.0
X-Received: by 10.224.172.129 with SMTP id l1mr89029630qaz.90.1408628702484;
 Thu, 21 Aug 2014 06:45:02 -0700 (PDT)
Received: by 10.140.40.41 with HTTP; Thu, 21 Aug 2014 06:45:02 -0700 (PDT)
In-Reply-To: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
Date: Thu, 21 Aug 2014 19:15:02 +0530
Message-ID: <CAJiQeYKAUP3s72t1FfGn-w89TYYUtc+nyOaT1-Qgd35JX832JQ@mail.gmail.com>
Subject: Re: is Branch-1.1 SBT build broken for yarn-alpha ?
From: Mridul Muralidharan <mridul@gmail.com>
To: Chester Chen <chester@alpinenow.com>, Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Weird that Patrick did not face this while creating the RC.
Essentially the yarn alpha pom.xml has not been updated properly in
the 1.1 branch.

Just change version to '1.1.1-SNAPSHOT' for yarn/alpha/pom.xml (to
make it same as any other pom).


Regards,
Mridul


On Thu, Aug 21, 2014 at 5:09 AM, Chester Chen <chester@alpinenow.com> wrote=
:
> I just updated today's build and tried branch-1.1 for both yarn and
> yarn-alpha.
>
> For yarn build, this command seem to work fine.
>
> sbt/sbt -Pyarn -Dhadoop.version=3D2.3.0-cdh5.0.1 projects
>
> for yarn-alpha
>
> sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alpha projects
>
> I got the following
>
> Any ideas
>
>
> Chester
>
> =E1=9A=9B |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-a=
lpha
> projects*
>
> Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Hom=
e
> as default JAVA_HOME.
>
> Note, this will be overridden by -java-home if it is set.
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project/project
>
> [info] Loading project definition from
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/proj=
ect
>
> [warn] Multiple resolvers having different access mechanism configured wi=
th
> same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
> project resolvers (`resolvers`) or rename publishing resolver (`publishTo=
`).
>
> [info] Loading project definition from /Users/chester/projects/spark/proj=
ect
>
> org.apache.maven.model.building.ModelBuildingException: 1 problem was
> encountered while building the effective model for
> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>
> *[FATAL] Non-resolvable parent POM: Could not find artifact
> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>=
)
> and 'parent.relativePath' points at wrong local POM @ line 20, column 11*
>
>
>  at
> org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuil=
dingException(DefaultModelProblemCollector.java:195)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(=
DefaultModelBuilder.java:841)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultMod=
elBuilder.java:664)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:310)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:232)
>
> at
> com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.sca=
la:61)
>
> at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:128)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectH=
elper.scala:49)
>
> at com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:=
28)
>
> at SparkBuild$.projectDefinitions(SparkBuild.scala:165)
>
> at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at scala.collection.immutable.Stream.flatMap(Stream.scala:442)
>
> at sbt.Load$.loadUnit(Load.scala:415)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
>
> at sbt.BuildLoader.apply(BuildLoader.scala:143)
>
> at sbt.Load$.loadAll(Load.scala:312)
>
> at sbt.Load$.loadURI(Load.scala:264)
>
> at sbt.Load$.load(Load.scala:260)
>
> at sbt.Load$.load(Load.scala:251)
>
> at sbt.Load$.apply(Load.scala:134)
>
> at sbt.Load$.defaultLoad(Load.scala:37)
>
> at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>
> at sbt.Command$.process(Command.scala:95)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.State$$anon$1.process(State.scala:179)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>
> at sbt.MainLoop$.next(MainLoop.scala:100)
>
> at sbt.MainLoop$.run(MainLoop.scala:93)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)
>
> at sbt.Using.apply(Using.scala:25)
>
> at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)
>
> at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)
>
> at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)
>
> at sbt.MainLoop$.runLogged(MainLoop.scala:25)
>
> at sbt.StandardMain$.runManaged(Main.scala:57)
>
> at sbt.xMain.run(Main.scala:29)
>
> at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
>
> at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)
>
> at xsbt.boot.Launch$.run(Launch.scala:109)
>
> at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)
>
> at xsbt.boot.Launch$.launch(Launch.scala:117)
>
> at xsbt.boot.Launch$.apply(Launch.scala:19)
>
> at xsbt.boot.Boot$.runImpl(Boot.scala:44)
>
> at xsbt.boot.Boot$.main(Boot.scala:20)
>
> at xsbt.boot.Boot.main(Boot.scala)
>
> [error] org.apache.maven.model.building.ModelBuildingException: 1 problem
> was encountered while building the effective model for
> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>
> [error] [FATAL] Non-resolvable parent POM: Could not find artifact
> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
> wrong local POM @ line 20, column 11
>
> [error] Use 'last' for the full log.
>
> Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 15:04:24 2014
Return-Path: <dev-return-8976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 606A21148A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 15:04:24 +0000 (UTC)
Received: (qmail 15904 invoked by uid 500); 21 Aug 2014 15:04:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15839 invoked by uid 500); 21 Aug 2014 15:04:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15828 invoked by uid 99); 21 Aug 2014 15:04:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 15:04:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 15:04:13 +0000
Received: by mail-pa0-f52.google.com with SMTP id bj1so14412788pad.39
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 08:03:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:content-transfer-encoding:message-id:references
         :to;
        bh=9TaEJpfVxng8hl4GWFjXkzn0IWWNK1YK4UI5Vn4VN2E=;
        b=kNxT0lm0SlombltcIjvSXms6gy6oTJAQe08rJ1GNLha9YaJJVZOb6AHu0X0bjjaHA4
         TEl1Xyo7SWkh+0g2J7IiqSUILtRQJUtD5QwmpVHIyFDNiXx6p+UBlZgXnGH0kE2oZem4
         lQ3UsjO2ctEy03OA0M+pibgLq0pn+UOAJqNGclOg9axwJYNMJodrqLhRCevsc6C6Vqyn
         P30X2tu2A6o1VAZr55Fmnu+I9RfReH4M1f6VL4mLk0Y8tyqH7e87M9K5tczvpioLsTBn
         oZBb7CQGa+WWF5rM1ZbXvrXGCcOvoC73hwis4cJIfnv4oX09IoBtzopsuAk3dN4gOUQO
         Eowg==
X-Gm-Message-State: ALoCoQkyq/I/bWOmPnzMdmS9gpXSzwj3eoNXUB3f8Va+thpDYBm7q1RrkvpQP+gXBJYOqLELr78O
X-Received: by 10.68.68.225 with SMTP id z1mr61184958pbt.110.1408633432462;
        Thu, 21 Aug 2014 08:03:52 -0700 (PDT)
Received: from [30.38.83.16] ([172.56.16.172])
        by mx.google.com with ESMTPSA id zc5sm13928577pbc.43.2014.08.21.08.03.50
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 21 Aug 2014 08:03:50 -0700 (PDT)
Content-Type: text/plain;
	charset=utf-8
Mime-Version: 1.0 (1.0)
Subject: Re: is Branch-1.1 SBT build broken for yarn-alpha ?
From: "Chester @work" <chester@alpinenow.com>
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <CAJiQeYKAUP3s72t1FfGn-w89TYYUtc+nyOaT1-Qgd35JX832JQ@mail.gmail.com>
Date: Thu, 21 Aug 2014 08:03:51 -0700
Cc: Patrick Wendell <pwendell@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <294C270B-47F0-402A-BDB7-56051444E4BB@alpinenow.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com> <CAJiQeYKAUP3s72t1FfGn-w89TYYUtc+nyOaT1-Qgd35JX832JQ@mail.gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

Do we have Jenkins tests these ? Should be pretty easy to setup just to test=
 basic build

Sent from my iPhone

> On Aug 21, 2014, at 6:45 AM, Mridul Muralidharan <mridul@gmail.com> wrote:=

>=20
> Weird that Patrick did not face this while creating the RC.
> Essentially the yarn alpha pom.xml has not been updated properly in
> the 1.1 branch.
>=20
> Just change version to '1.1.1-SNAPSHOT' for yarn/alpha/pom.xml (to
> make it same as any other pom).
>=20
>=20
> Regards,
> Mridul
>=20
>=20
>> On Thu, Aug 21, 2014 at 5:09 AM, Chester Chen <chester@alpinenow.com> wro=
te:
>> I just updated today's build and tried branch-1.1 for both yarn and
>> yarn-alpha.
>>=20
>> For yarn build, this command seem to work fine.
>>=20
>> sbt/sbt -Pyarn -Dhadoop.version=3D2.3.0-cdh5.0.1 projects
>>=20
>> for yarn-alpha
>>=20
>> sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alpha projects
>>=20
>> I got the following
>>=20
>> Any ideas
>>=20
>>=20
>> Chester
>>=20
>> =E1=9A=9B |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-a=
lpha
>> projects*
>>=20
>> Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Hom=
e
>> as default JAVA_HOME.
>>=20
>> Note, this will be overridden by -java-home if it is set.
>>=20
>> [info] Loading project definition from
>> /Users/chester/projects/spark/project/project
>>=20
>> [info] Loading project definition from
>> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/proj=
ect
>>=20
>> [warn] Multiple resolvers having different access mechanism configured wi=
th
>> same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
>> project resolvers (`resolvers`) or rename publishing resolver (`publishTo=
`).
>>=20
>> [info] Loading project definition from /Users/chester/projects/spark/proj=
ect
>>=20
>> org.apache.maven.model.building.ModelBuildingException: 1 problem was
>> encountered while building the effective model for
>> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>>=20
>> *[FATAL] Non-resolvable parent POM: Could not find artifact
>> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
>> http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>=
)
>> and 'parent.relativePath' points at wrong local POM @ line 20, column 11*=

>>=20
>>=20
>> at
>> org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuil=
dingException(DefaultModelProblemCollector.java:195)
>>=20
>> at
>> org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(=
DefaultModelBuilder.java:841)
>>=20
>> at
>> org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultMod=
elBuilder.java:664)
>>=20
>> at
>> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:310)
>>=20
>> at
>> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:232)
>>=20
>> at
>> com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.sca=
la:61)
>>=20
>> at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:128)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>>=20
>> at
>> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>>=20
>> at
>> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>>=20
>> at
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>>=20
>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>=20
>> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>>=20
>> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
>>=20
>> at
>> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>>=20
>> at
>> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>>=20
>> at
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>>=20
>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>=20
>> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>>=20
>> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
>>=20
>> at
>> com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectH=
elper.scala:49)
>>=20
>> at com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:=
28)
>>=20
>> at SparkBuild$.projectDefinitions(SparkBuild.scala:165)
>>=20
>> at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)
>>=20
>> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>>=20
>> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>>=20
>> at scala.collection.immutable.Stream.flatMap(Stream.scala:442)
>>=20
>> at sbt.Load$.loadUnit(Load.scala:415)
>>=20
>> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>>=20
>> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>>=20
>> at
>> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
>>=20
>> at
>> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
>>=20
>> at sbt.BuildLoader.apply(BuildLoader.scala:143)
>>=20
>> at sbt.Load$.loadAll(Load.scala:312)
>>=20
>> at sbt.Load$.loadURI(Load.scala:264)
>>=20
>> at sbt.Load$.load(Load.scala:260)
>>=20
>> at sbt.Load$.load(Load.scala:251)
>>=20
>> at sbt.Load$.apply(Load.scala:134)
>>=20
>> at sbt.Load$.defaultLoad(Load.scala:37)
>>=20
>> at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)
>>=20
>> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>>=20
>> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>>=20
>> at
>> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>>=20
>> at
>> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
>>=20
>> at
>> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>>=20
>> at
>> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
>>=20
>> at sbt.Command$.process(Command.scala:95)
>>=20
>> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>>=20
>> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>>=20
>> at sbt.State$$anon$1.process(State.scala:179)
>>=20
>> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>>=20
>> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>>=20
>> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>>=20
>> at sbt.MainLoop$.next(MainLoop.scala:100)
>>=20
>> at sbt.MainLoop$.run(MainLoop.scala:93)
>>=20
>> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)
>>=20
>> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)
>>=20
>> at sbt.Using.apply(Using.scala:25)
>>=20
>> at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)
>>=20
>> at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)
>>=20
>> at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)
>>=20
>> at sbt.MainLoop$.runLogged(MainLoop.scala:25)
>>=20
>> at sbt.StandardMain$.runManaged(Main.scala:57)
>>=20
>> at sbt.xMain.run(Main.scala:29)
>>=20
>> at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
>>=20
>> at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)
>>=20
>> at xsbt.boot.Launch$.run(Launch.scala:109)
>>=20
>> at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)
>>=20
>> at xsbt.boot.Launch$.launch(Launch.scala:117)
>>=20
>> at xsbt.boot.Launch$.apply(Launch.scala:19)
>>=20
>> at xsbt.boot.Boot$.runImpl(Boot.scala:44)
>>=20
>> at xsbt.boot.Boot$.main(Boot.scala:20)
>>=20
>> at xsbt.boot.Boot.main(Boot.scala)
>>=20
>> [error] org.apache.maven.model.building.ModelBuildingException: 1 problem=

>> was encountered while building the effective model for
>> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
>>=20
>> [error] [FATAL] Non-resolvable parent POM: Could not find artifact
>> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
>> http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
>> wrong local POM @ line 20, column 11
>>=20
>> [error] Use 'last' for the full log.
>>=20
>> Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 17:48:16 2014
Return-Path: <dev-return-8977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6367311F7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 17:48:16 +0000 (UTC)
Received: (qmail 10063 invoked by uid 500); 21 Aug 2014 17:48:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9997 invoked by uid 500); 21 Aug 2014 17:48:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9985 invoked by uid 99); 21 Aug 2014 17:48:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 17:48:14 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 17:48:09 +0000
Received: from 172.18.9.243 (EHLO dfweml702-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id AWV27475;
	Thu, 21 Aug 2014 12:47:48 -0500 (CDT)
Received: from SJCEML703-CHM.china.huawei.com (10.212.94.49) by
 dfweml702-chm.china.huawei.com (10.193.5.72) with Microsoft SMTP Server (TLS)
 id 14.3.158.1; Thu, 21 Aug 2014 10:47:48 -0700
Received: from SJCEML702-CHM.china.huawei.com ([169.254.4.137]) by
 SJCEML703-CHM.china.huawei.com ([169.254.5.229]) with mapi id 14.03.0158.001;
 Thu, 21 Aug 2014 10:47:46 -0700
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: chutium <teng.qiu@gmail.com>,
        "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>
Subject: RE: Spark SQL Query and join different data sources.
Thread-Topic: Spark SQL Query and join different data sources.
Thread-Index: AQHPu94i8kloUpn+C06gz3cFqTNLEJvbZX+A///iruA=
Date: Thu, 21 Aug 2014 17:47:46 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D155857@SJCEML702-CHM.china.huawei.com>
References: <1408467214979-7914.post@n3.nabble.com>
 <1408620901005-7937.post@n3.nabble.com>
In-Reply-To: <1408620901005-7937.post@n3.nabble.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.103]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

I doubt it will work as expected.

Note that hiveContext.hql("select ...").regAsTable("a") will create a Schem=
aRDD before register the SchemaRDD with the (Hive) catalog;
While sqlContext.jsonFile("xxx").regAsTable("b") will create a SchemaRDD be=
fore register the SchemaRDD with the SparkSQL catalog(SimpleCatalog).
The logic plans of the two SchemaRDDs are of the same type; but the physica=
l plans are, and should be, different.
The issue is that the transformation of the logical plans to physical plans=
 are controlled by the "strategies" of "contexts"; namely the sqlContext
transforms a logical plan to a physical plan suitable for SchemaRDD's execu=
tion from an in-memory data source, while HiveContext=20
transforms a logical plan to a physical plan suitable for SchemaRDD's execu=
tion from a Hive data source. So

sqlContext.sql( a join b ) will generate a physical plan for the in-memory =
data source for both a and b; and
hiveContext.sql(a join b) will generate a physical plan for Hive data sourc=
e for both a and b.

What's really needed is a storage transparency from the semantic layer if S=
parkSQL wants to go the data federation route.


If one could manage to create a SchemaRDD on Hive data through just the SQL=
Context, not the HiveCOntext (being a subclass of SQLCOntext), seemingly
hinted by the SparkSQL web page https://spark.apache.org/sql/ in the follow=
ing code snippet:

sqlCtx.jsonFile("s3n://...")
  .registerAsTable("json")
 schema_rdd =3D sqlCtx.sql("""
   SELECT *=20
   FROM hiveTable
   JOIN json ...""")

he/she might be able to perform the join of data sets of different types. I=
 just have not tried.


In terms of SQL-92 conforming, Presto might be better than HiveQL; while in=
 terms of federation, Hive is actually very good at it.




-----Original Message-----
From: chutium [mailto:teng.qiu@gmail.com]=20
Sent: Thursday, August 21, 2014 4:35 AM
To: dev@spark.incubator.apache.org
Subject: Re: Spark SQL Query and join different data sources.

as far as i know, HQL queries try to find the schema info of all the tables=
 in this query from hive metastore, so it is not possible to join tables fr=
om sqlContext using hiveContext.hql

but this should work:

hiveContext.hql("select ...").regAsTable("a")
sqlContext.jsonFile("xxx").regAsTable("b")

then

sqlContext.sql( a join b )


i created a ticket SPARK-2710 to add ResultSets from JDBC connection as a n=
ew data source, but no predicate push down yet, also, it is not available f=
or HQL

so, if you are looking for something that can query different data sources =
with full SQL92 syntax, facebook presto is still the only choice, they have=
 some kind of JDBC connector in deveopment, and there are some unofficial i=
mplementations...

but i am looking forward to seeing the progress of Spark SQL, after
SPARK-2179 SQLContext can handle any
kind of structured data with a sequence of DataTypes as schema, although tu=
rning the data into Rows is still a little bit tricky...



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914p7937.ht=
ml
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional com=
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 18:36:36 2014
Return-Path: <dev-return-8978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B848811238
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 18:36:36 +0000 (UTC)
Received: (qmail 61782 invoked by uid 500); 21 Aug 2014 18:36:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61719 invoked by uid 500); 21 Aug 2014 18:36:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61703 invoked by uid 99); 21 Aug 2014 18:36:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 18:36:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of chester@alpinenow.com designates 209.85.215.54 as permitted sender)
Received: from [209.85.215.54] (HELO mail-la0-f54.google.com) (209.85.215.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 18:36:08 +0000
Received: by mail-la0-f54.google.com with SMTP id hz20so9186903lab.27
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 11:36:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=zD0Xs6GYa/sUorpvgl2Vz9btQ05oXuvttuIf2DfwOZw=;
        b=Khylrhsk7mOnaKOLlpNYmykq2Wwd+lOhu4bGEY4RBZlbEfx+mrNRwGXrbpg4lmOBme
         NubOP3o14upMnN6h/tXxCTPFltq28K8y+b5WsLPlH5/aB4tznSeg8Fz2T7q83BnrwVcF
         iUBNGa74wAmtcmjwFkPYHbvxnZ4Rq3lfnMkarI+E8sNseC0rLfyegWplHRWaG0iLVedb
         ++Y/XcJiby2YF79FTtheP4fMYLdgzOLHz1awh16YXKnAa5RgqA7S2ZoWfCsRx8Gp24ZY
         WRuIuGEn3UPgwz8LVwp7EYgD5Kq520CebruDg8lXMAvpPCIq2Y3SCuXQpRXmbjc3sLOQ
         ePYQ==
X-Gm-Message-State: ALoCoQkdJ/2rFoX0pQ7qmBmKBbayIdQOD1VAbV2Umw3saYAEMxsquxMe5S+XPE1WzI5eboCofK1r
MIME-Version: 1.0
X-Received: by 10.112.205.200 with SMTP id li8mr326573lbc.70.1408646167886;
 Thu, 21 Aug 2014 11:36:07 -0700 (PDT)
Received: by 10.25.17.151 with HTTP; Thu, 21 Aug 2014 11:36:07 -0700 (PDT)
In-Reply-To: <294C270B-47F0-402A-BDB7-56051444E4BB@alpinenow.com>
References: <CAPYnQ0X+m_gdomP=w7oyZc6nNvMQWeOL3v+E3gDYiE-Fr18XjQ@mail.gmail.com>
	<CAJiQeYKAUP3s72t1FfGn-w89TYYUtc+nyOaT1-Qgd35JX832JQ@mail.gmail.com>
	<294C270B-47F0-402A-BDB7-56051444E4BB@alpinenow.com>
Date: Thu, 21 Aug 2014 11:36:07 -0700
Message-ID: <CAPYnQ0VvVQzd-z8t8U__zUUx51jNn-19JFyzFBnK-+KHf3BD9g@mail.gmail.com>
Subject: Re: is Branch-1.1 SBT build broken for yarn-alpha ?
From: Chester Chen <chester@alpinenow.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3c844739100050127feea
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3c844739100050127feea
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Mridul,
     Thanks for the suggestion.

     I just updated the build today and changed the yarn/alpha/pom.xml to

   <version>1.1.1-SNAPSHOT</version>

then the command worked.

I will create a JIRA and PR for it.


Chester




On Thu, Aug 21, 2014 at 8:03 AM, Chester @work <chester@alpinenow.com>
wrote:

> Do we have Jenkins tests these ? Should be pretty easy to setup just to
> test basic build
>
> Sent from my iPhone
>
> > On Aug 21, 2014, at 6:45 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> >
> > Weird that Patrick did not face this while creating the RC.
> > Essentially the yarn alpha pom.xml has not been updated properly in
> > the 1.1 branch.
> >
> > Just change version to '1.1.1-SNAPSHOT' for yarn/alpha/pom.xml (to
> > make it same as any other pom).
> >
> >
> > Regards,
> > Mridul
> >
> >
> >> On Thu, Aug 21, 2014 at 5:09 AM, Chester Chen <chester@alpinenow.com>
> wrote:
> >> I just updated today's build and tried branch-1.1 for both yarn and
> >> yarn-alpha.
> >>
> >> For yarn build, this command seem to work fine.
> >>
> >> sbt/sbt -Pyarn -Dhadoop.version=3D2.3.0-cdh5.0.1 projects
> >>
> >> for yarn-alpha
> >>
> >> sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.5-alpha projects
> >>
> >> I got the following
> >>
> >> Any ideas
> >>
> >>
> >> Chester
> >>
> >> =E1=9A=9B |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=3D2.0.=
5-alpha
> >> projects*
> >>
> >> Using
> /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Home
> >> as default JAVA_HOME.
> >>
> >> Note, this will be overridden by -java-home if it is set.
> >>
> >> [info] Loading project definition from
> >> /Users/chester/projects/spark/project/project
> >>
> >> [info] Loading project definition from
> >>
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/proj=
ect
> >>
> >> [warn] Multiple resolvers having different access mechanism configured
> with
> >> same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
> >> project resolvers (`resolvers`) or rename publishing resolver
> (`publishTo`).
> >>
> >> [info] Loading project definition from
> /Users/chester/projects/spark/project
> >>
> >> org.apache.maven.model.building.ModelBuildingException: 1 problem was
> >> encountered while building the effective model for
> >> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
> >>
> >> *[FATAL] Non-resolvable parent POM: Could not find artifact
> >> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> >> http://repo.maven.apache.org/maven2 <
> http://repo.maven.apache.org/maven2>)
> >> and 'parent.relativePath' points at wrong local POM @ line 20, column
> 11*
> >>
> >>
> >> at
> >>
> org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuil=
dingException(DefaultModelProblemCollector.java:195)
> >>
> >> at
> >>
> org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(=
DefaultModelBuilder.java:841)
> >>
> >> at
> >>
> org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultMod=
elBuilder.java:664)
> >>
> >> at
> >>
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:310)
> >>
> >> at
> >>
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBui=
lder.java:232)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.sca=
la:61)
> >>
> >> at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:128)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
> >>
> >> at
> >>
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
> >>
> >> at
> >>
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
> >>
> >> at
> >>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
> >>
> >> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>
> >> at scala.collection.TraversableLike$class.map(TraversableLike.scala:24=
4)
> >>
> >> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHel=
per.scala:129)
> >>
> >> at
> >>
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
> >>
> >> at
> >>
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
> >>
> >> at
> >>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
> >>
> >> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>
> >> at scala.collection.TraversableLike$class.map(TraversableLike.scala:24=
4)
> >>
> >> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelp=
er.scala:129)
> >>
> >> at
> >>
> com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectH=
elper.scala:49)
> >>
> >> at
> com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:28)
> >>
> >> at SparkBuild$.projectDefinitions(SparkBuild.scala:165)
> >>
> >> at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)
> >>
> >> at sbt.Load$$anonfun$24.apply(Load.scala:415)
> >>
> >> at sbt.Load$$anonfun$24.apply(Load.scala:415)
> >>
> >> at scala.collection.immutable.Stream.flatMap(Stream.scala:442)
> >>
> >> at sbt.Load$.loadUnit(Load.scala:415)
> >>
> >> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
> >>
> >> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
> >>
> >> at
> >>
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
> >>
> >> at
> >>
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$appl=
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
> >>
> >> at sbt.BuildLoader.apply(BuildLoader.scala:143)
> >>
> >> at sbt.Load$.loadAll(Load.scala:312)
> >>
> >> at sbt.Load$.loadURI(Load.scala:264)
> >>
> >> at sbt.Load$.load(Load.scala:260)
> >>
> >> at sbt.Load$.load(Load.scala:251)
> >>
> >> at sbt.Load$.apply(Load.scala:134)
> >>
> >> at sbt.Load$.defaultLoad(Load.scala:37)
> >>
> >> at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)
> >>
> >> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467=
)
> >>
> >> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467=
)
> >>
> >> at
> >>
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
> >>
> >> at
> >>
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:6=
0)
> >>
> >> at
> >>
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
> >>
> >> at
> >>
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:6=
2)
> >>
> >> at sbt.Command$.process(Command.scala:95)
> >>
> >> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
> >>
> >> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
> >>
> >> at sbt.State$$anon$1.process(State.scala:179)
> >>
> >> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
> >>
> >> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
> >>
> >> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
> >>
> >> at sbt.MainLoop$.next(MainLoop.scala:100)
> >>
> >> at sbt.MainLoop$.run(MainLoop.scala:93)
> >>
> >> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)
> >>
> >> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)
> >>
> >> at sbt.Using.apply(Using.scala:25)
> >>
> >> at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)
> >>
> >> at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)
> >>
> >> at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)
> >>
> >> at sbt.MainLoop$.runLogged(MainLoop.scala:25)
> >>
> >> at sbt.StandardMain$.runManaged(Main.scala:57)
> >>
> >> at sbt.xMain.run(Main.scala:29)
> >>
> >> at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
> >>
> >> at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)
> >>
> >> at xsbt.boot.Launch$.run(Launch.scala:109)
> >>
> >> at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)
> >>
> >> at xsbt.boot.Launch$.launch(Launch.scala:117)
> >>
> >> at xsbt.boot.Launch$.apply(Launch.scala:19)
> >>
> >> at xsbt.boot.Boot$.runImpl(Boot.scala:44)
> >>
> >> at xsbt.boot.Boot$.main(Boot.scala:20)
> >>
> >> at xsbt.boot.Boot.main(Boot.scala)
> >>
> >> [error] org.apache.maven.model.building.ModelBuildingException: 1
> problem
> >> was encountered while building the effective model for
> >> org.apache.spark:spark-yarn-alpha_2.10:1.1.0
> >>
> >> [error] [FATAL] Non-resolvable parent POM: Could not find artifact
> >> org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
> >> http://repo.maven.apache.org/maven2) and 'parent.relativePath' points
> at
> >> wrong local POM @ line 20, column 11
> >>
> >> [error] Use 'last' for the full log.
> >>
> >> Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q
>

--001a11c3c844739100050127feea--

From dev-return-8979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 18:53:51 2014
Return-Path: <dev-return-8979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 95ED5113C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 18:53:51 +0000 (UTC)
Received: (qmail 12439 invoked by uid 500); 21 Aug 2014 18:53:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12371 invoked by uid 500); 21 Aug 2014 18:53:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12349 invoked by uid 99); 21 Aug 2014 18:53:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 18:53:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 18:53:25 +0000
Received: by mail-ig0-f175.google.com with SMTP id uq10so13785835igb.14
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 11:53:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=bqqEgx0B0GLR05eW9zPApThzJ6rK/ag0JxZsThz7On8=;
        b=OQyKiS53QU01UeEAwJX3HDZKnfAmfwKqBWWz9aOvK8PhW6v4wBZp6SHdokcUkWwiPf
         CaIaM5cc+kr84niBmtjfbKbLm/Mu6nF8e/w9+BIZn+GCWOXWaZbeygGLez0DCMmiPF3o
         ta8EoSRd7YraevF+tY45jpaT0shwSd7fYh9yNKUPVOE++N5wYMeWcSKVyZjQeGC/TEEO
         lIGmLDw8gE0WLbuO5UDoKoM5b03BvuU+Y5HwF4BVA2ZXMmR2UtWZaVkH+ZSiZRdDvf3k
         qJvIAIItqmpxG5SbxOrl/GXbq0TnsM2wj7NW+CuCPz0p6/D61lsrH2sVhJPsCSo13+Pr
         OMmA==
X-Gm-Message-State: ALoCoQmTFwVQ9FgPyoPK3JPO6VRsAGSf4Rg8kJujRL9VIRKUf/Wbr038b8WkeYut9SILd71BX5NB
X-Received: by 10.42.107.145 with SMTP id d17mr5004349icp.61.1408647204107;
 Thu, 21 Aug 2014 11:53:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.10.170 with HTTP; Thu, 21 Aug 2014 11:53:03 -0700 (PDT)
In-Reply-To: <CACBYxKLn_jFqdNXytkrE=MhgB+9RfOgMEHHmyCQn7tH+J6iU3g@mail.gmail.com>
References: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
 <CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
 <CA+B-+fx=pQtL59VsaO_moc3HsY9+sW1WhHs+JMg0iuNtrnddPA@mail.gmail.com> <CACBYxKLn_jFqdNXytkrE=MhgB+9RfOgMEHHmyCQn7tH+J6iU3g@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Thu, 21 Aug 2014 11:53:03 -0700
Message-ID: <CACfA1zVeYGajodrq9FKzcRyJmqRnsi=TnRJnjw0LoO3=tOspjg@mail.gmail.com>
Subject: Re: Lost executor on YARN ALS iterations
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Debasish Das <debasish.das83@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf301fb60d3709060501283c48
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301fb60d3709060501283c48
Content-Type: text/plain; charset=UTF-8

Can someone from Databricks test and commit this PR? This is not a complete
solution, but would provide some relief.
https://github.com/apache/spark/pull/1391

Thanks,
Nishkam


On Wed, Aug 20, 2014 at 12:39 AM, Sandy Ryza <sandy.ryza@cloudera.com>
wrote:

> Hi Debasish,
>
> The fix is to raise spark.yarn.executor.memoryOverhead until this goes
> away.  This controls the buffer between the JVM heap size and the amount of
> memory requested from YARN (JVMs can take up memory beyond their heap
> size). You should also make sure that, in the YARN NodeManager
> configuration, yarn.nodemanager.vmem-check-enabled is set to false.
>
> -Sandy
>
>
> On Wed, Aug 20, 2014 at 12:27 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is
> > definitely a YARN related problem...
> >
> > At least for me right now only deployment option possible is
> standalone...
> >
> >
> >
> > On Tue, Aug 19, 2014 at 11:29 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >
> >> Hi Deb,
> >>
> >> I think this may be the same issue as described in
> >> https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
> >> container got killed by YARN because it used much more memory that it
> >> requested. But we haven't figured out the root cause yet.
> >>
> >> +Sandy
> >>
> >> Best,
> >> Xiangrui
> >>
> >> On Tue, Aug 19, 2014 at 8:51 PM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > Hi,
> >> >
> >> > During the 4th ALS iteration, I am noticing that one of the executor
> >> gets
> >> > disconnected:
> >> >
> >> > 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
> >> > SendingConnectionManagerId not found
> >> >
> >> > 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
> >> > disconnected, so removing it
> >> >
> >> > 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost
> >> executor 5
> >> > on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client
> disassociated
> >> >
> >> > 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch
> >> 12)
> >> > Any idea if this is a bug related to akka on YARN ?
> >> >
> >> > I am using master
> >> >
> >> > Thanks.
> >> > Deb
> >>
> >
> >
>

--20cf301fb60d3709060501283c48--

From dev-return-8980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 20:36:45 2014
Return-Path: <dev-return-8980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C687D11940
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 20:36:45 +0000 (UTC)
Received: (qmail 42721 invoked by uid 500); 21 Aug 2014 20:36:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42657 invoked by uid 500); 21 Aug 2014 20:36:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42645 invoked by uid 99); 21 Aug 2014 20:36:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 20:36:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 20:36:18 +0000
Received: by mail-wg0-f46.google.com with SMTP id m15so9652111wgh.5
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 13:36:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/0LywzwLFEI/uXdtULx3EqTaqa3tKDHTNnAJPeZiC8c=;
        b=pFisRXbDUGsIcXzVoVbtTqm211PExcpl8B6B5pfdHgHU88bIO/G0IvZYQWwGugk1g3
         Qmamny0GJ5WuCwRFEI2E5ybo7CVWxwUYN97bXehp16/shwxDxgKva/nKuJcydCZ6rzYI
         fCHBkCkQ+isjoFS5GNVB7RzRcb1aeG8LamSEem6daT+fAoz0RDrqCUiXIo75GzhzXm7f
         W6t4QiQLBLcs/I1DlTHwsHD81rburizACPNVttwuAuLqBcro+hShdA0PXMDB0Okq3pfY
         5aB8sO0fDIpBljF8VzObhRafwhFV5CFrAzUN9OXOOgAlnKsFLaEzLUA0nmzegOfXLcaa
         nHog==
MIME-Version: 1.0
X-Received: by 10.194.58.244 with SMTP id u20mr938296wjq.36.1408653378042;
 Thu, 21 Aug 2014 13:36:18 -0700 (PDT)
Received: by 10.216.130.7 with HTTP; Thu, 21 Aug 2014 13:36:18 -0700 (PDT)
In-Reply-To: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
Date: Thu, 21 Aug 2014 13:36:18 -0700
Message-ID: <CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
Subject: Re: Spark Contribution
From: Henry Saputra <henry.saputra@gmail.com>
To: Maisnam Ns <maisnam.ns@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The Apache Spark wiki on how to contribute should be great place to
start: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

- Henry

On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com> wrote:
> Hi,
>
> Can someone help me with some links on how to contribute for Spark
>
> Regards
> mns

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 21:15:18 2014
Return-Path: <dev-return-8981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBA6811AC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 21:15:18 +0000 (UTC)
Received: (qmail 60645 invoked by uid 500); 21 Aug 2014 21:15:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60576 invoked by uid 500); 21 Aug 2014 21:15:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60563 invoked by uid 99); 21 Aug 2014 21:15:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 21:15:17 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 21:15:13 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1XKZh6-0007tb-GH
	for dev@spark.incubator.apache.org; Thu, 21 Aug 2014 14:14:52 -0700
Date: Thu, 21 Aug 2014 14:14:52 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408655692487-7944.post@n3.nabble.com>
Subject: PARSING_ERROR from kryo
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,

I am getting PARSING_ERROR while running my job on the code checked out up
to commit# db56f2df1b8027171da1b8d2571d1f2ef1e103b6. I am running this job
on EC2.

Any idea if there is something wrong with my config?

Here is my config: 
--
    .set("spark.executor.extraJavaOptions", "-XX:+UseCompressedOops
-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps")
      .set("spark.storage.memoryFraction", "0.2")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.kryo.registrator",
"org.apache.spark.graphx.GraphKryoRegistrator")
      .set("spark.akka.frameSize", "20")
      .set("spark.akka.timeout", "300")
      .set("spark.shuffle.memoryFraction", "0.5")
      .set("spark.core.connection.ack.wait.timeout", "1800")
--



--
Job aborted due to stage failure: Task 947 in stage 11.0 failed 4 times,
most recent failure: Lost task 947.3 in stage 11.0 (TID 12750,
ip-10-167-149-118.ec2.internal): com.esotericsoftware.kryo.KryoException:
java.io.IOException: failed to uncompress the chunk: PARSING_ERROR(2)
Serialization trace:
vids (org.apache.spark.graphx.impl.VertexAttributeBlock)
        com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
        com.esotericsoftware.kryo.io.Input.require(Input.java:169)
        com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:719)
        com.esotericsoftware.kryo.io.Input.readLong(Input.java:665)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:127)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:107)
        com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
       
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
       
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
       
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:119)
       
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:129)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
       
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1038)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
       
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
       
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
       
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:57)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:147)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:189)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
--



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/PARSING-ERROR-from-kryo-tp7944.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 23:06:36 2014
Return-Path: <dev-return-8982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7FF711E74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 23:06:36 +0000 (UTC)
Received: (qmail 20209 invoked by uid 500); 21 Aug 2014 23:06:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20146 invoked by uid 500); 21 Aug 2014 23:06:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20134 invoked by uid 99); 21 Aug 2014 23:06:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:06:35 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:06:08 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so9955003wgg.24
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 16:06:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=2zC9rsp/RVJJlNeMyc+DeqCDfUoeGkj4aa2Iv/WUYlE=;
        b=lrblQ4nrKM7ckzUH3gQUblCVNwN2oyztRn5f7kKF/OqhnGrUFzquip1IraD0ukFPbQ
         y+sey1BSH+OQ3UQfBsbWUiXtd3Wn84GjxTlqsWqAvlPamA2aLqLWnn8/Bf0DHnxW3zKa
         hVI96Ha82EVeoOB5Bib+gUsyjB/rQF5uucTFNwIChsSvVH/Z6sURGOKZ9XxukHLIXnZw
         sJ0YRXKxKafqsqflYsBUOSDLDB/q0VlLtYC9g0qV9mGrm8hQp46j/l3TCjRGDbYxAD4E
         03j6d08LAURfD3gAXuvAKRpVHvBC4xJqL5opnlCIjhed5UedIdbmwQm1XHPoZ+g0Q24c
         +hvA==
MIME-Version: 1.0
X-Received: by 10.180.21.235 with SMTP id y11mr6813107wie.75.1408662367552;
 Thu, 21 Aug 2014 16:06:07 -0700 (PDT)
Received: by 10.180.92.232 with HTTP; Thu, 21 Aug 2014 16:06:07 -0700 (PDT)
In-Reply-To: <CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
	<CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
Date: Thu, 21 Aug 2014 19:06:07 -0400
Message-ID: <CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
Subject: Re: Spark Contribution
From: Nicholas Chammas <nicholas.chammas@gmail.com>
To: Henry Saputra <henry.saputra@gmail.com>
Cc: Maisnam Ns <maisnam.ns@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e0406cce705012bc487
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e0406cce705012bc487
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

We should add this link to the readme on GitHub btw.

2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Henry Sap=
utra<henry.saputra@gmail.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=9C=
 =EB=A9=94=EC=8B=9C=EC=A7=80:

> The Apache Spark wiki on how to contribute should be great place to
> start:
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
>
> - Henry
>
> On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
> <javascript:;>> wrote:
> > Hi,
> >
> > Can someone help me with some links on how to contribute for Spark
> >
> > Regards
> > mns
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;>
>
>

--047d7b874e0406cce705012bc487--

From dev-return-8983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 23:43:52 2014
Return-Path: <dev-return-8983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 89D9F11F76
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 23:43:52 +0000 (UTC)
Received: (qmail 10237 invoked by uid 500); 21 Aug 2014 23:43:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10182 invoked by uid 500); 21 Aug 2014 23:43:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8306 invoked by uid 99); 21 Aug 2014 23:43:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:43:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of velvia.github@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:43:45 +0000
Received: by mail-we0-f176.google.com with SMTP id q58so9973577wes.21
        for <multiple recipients>; Thu, 21 Aug 2014 16:43:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=63r16vkAlo0RRz9+B6tOhvumRVZe91o8CvMSz7Gb+4c=;
        b=F0kaVS0JvMxGB9MRJtF2ynMm4HI1jRLUJYBk7EMU4PASeZW9gthO4E7dbRWLeYZr6M
         h6BrH84yKylayiuE/w/0xq/5VSdOnGhVCu8cXCqblWr4sxOu5A0+G3wOegZFYnIp2HoW
         /iQpmLxfJwaxG1daD5/Ny9wO9tszI6ub8ciWJ8z7wiuQfbS0/YkW3770gCFhhseJVJ/P
         W48Tvg+UiPPVFYZ6vdVdhvsCeRmDRd8n46dG9CC45+ZIZqdwxZAKVsPPWuHLYE/yRk4j
         Qs/giooLIdWkmEgiq9NfCLR6MNqNqsakZG3Re73JBLU3yO6ULUZy1ZzvhpAUyTOPJ6rO
         bJKw==
MIME-Version: 1.0
X-Received: by 10.194.108.41 with SMTP id hh9mr1611357wjb.68.1408664604643;
 Thu, 21 Aug 2014 16:43:24 -0700 (PDT)
Received: by 10.216.33.10 with HTTP; Thu, 21 Aug 2014 16:43:24 -0700 (PDT)
Date: Thu, 21 Aug 2014 16:43:24 -0700
Message-ID: <CAN6Vra2ia8VNmKfbbsBQy50F49JWdPo+WdienOzVEODUxw+3BA@mail.gmail.com>
Subject: Spark-JobServer moving to a new location
From: Evan Chan <velvia.github@gmail.com>
To: dev@spark.apache.org, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Dear community,

Wow, I remember when we first open sourced the job server, at the
first Spark Summit in December.  Since then, more and more of you have
started using it and contributing to it.   It is awesome to see!

If you are not familiar with the spark job server, it is a REST API
for managing your Spark jobs and job history and status.

In order to make sure the project can continue to move forward
independently, new features developed and contributions merged, we are
moving the project to a new github organization.  The new location is:

https://github.com/spark-jobserver/spark-jobserver


The git commit history is still there, but unfortunately the pull
requests don't migrate over.   I'll be contacting each of you with
open PRs to move them over to the new location.

Happy Hacking!

Evan (@velvia)
Kelvin (@kelvinchu)
Daniel (@dan-null)

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 21 23:47:34 2014
Return-Path: <dev-return-8984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C5A0411F8F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Aug 2014 23:47:34 +0000 (UTC)
Received: (qmail 17260 invoked by uid 500); 21 Aug 2014 23:47:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17194 invoked by uid 500); 21 Aug 2014 23:47:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17178 invoked by uid 99); 21 Aug 2014 23:47:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:47:33 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.176 as permitted sender)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Aug 2014 23:47:29 +0000
Received: by mail-lb0-f176.google.com with SMTP id u10so8683774lbd.21
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 16:47:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=x3DmzgXyZU86elDJtmxaX1M25SfFZQAjP9IBw6z0MuQ=;
        b=HfJltkKQ+jLfhKGfLt2pKXsrEGB9RIh+glX2q1FS3SoXtfxdkX9iT6fZ6UfwkxtHu2
         f8ddoYfzRsbuSXCeuOJVuLVM9dEC05HwDQ3SOKwYSub94/wdb2agNN+OWN8fxJ6c+UXL
         rAqxrXa2HW4xlswuuBW/KZqfxFwgGfL9dNAmbdxwV/a3LWB5EI+rnv+fckAwqwsaQ5tc
         X4/WB4r3WGB43Y6Pxd8njqfJ7f93s3FSQk0CkD4KEgiqHzCYGnfereQy7di2+WiH+jgB
         2V4IJ/jeMJ7ja2Ug6AQ5GXsJA0sA22zvmqsu+lRmt9IeN2wwTRd1JARqsJQ+QFFNDxXw
         +K0A==
MIME-Version: 1.0
X-Received: by 10.152.170.196 with SMTP id ao4mr1476561lac.44.1408664827859;
 Thu, 21 Aug 2014 16:47:07 -0700 (PDT)
Received: by 10.25.148.4 with HTTP; Thu, 21 Aug 2014 16:47:07 -0700 (PDT)
In-Reply-To: <CACBYxKLn_jFqdNXytkrE=MhgB+9RfOgMEHHmyCQn7tH+J6iU3g@mail.gmail.com>
References: <CA+B-+fwLb3xte0HMCZT4T2ix747=N1aZn1=JXQV56CACk0JXbQ@mail.gmail.com>
	<CAJgQjQ83DLvNKSwq9SExwZczQpOWFaMxPKTzWSn-d77-DLKYvw@mail.gmail.com>
	<CA+B-+fx=pQtL59VsaO_moc3HsY9+sW1WhHs+JMg0iuNtrnddPA@mail.gmail.com>
	<CACBYxKLn_jFqdNXytkrE=MhgB+9RfOgMEHHmyCQn7tH+J6iU3g@mail.gmail.com>
Date: Thu, 21 Aug 2014 16:47:07 -0700
Message-ID: <CA+B-+fztX8+PN6dy5FGOm+J=VNV4Nv72KpAB=52q-RvQJzHX4w@mail.gmail.com>
Subject: Re: Lost executor on YARN ALS iterations
From: Debasish Das <debasish.das83@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0117679dac199e05012c564c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0117679dac199e05012c564c
Content-Type: text/plain; charset=UTF-8

Sandy,

I put spark.yarn.executor.memoryOverhead 1024 on spark-defaults.conf but I
don't see environment variable on spark properties on the webui->environment

Does it need to go in spark-env.sh ?

Thanks.
Deb


On Wed, Aug 20, 2014 at 12:39 AM, Sandy Ryza <sandy.ryza@cloudera.com>
wrote:

> Hi Debasish,
>
> The fix is to raise spark.yarn.executor.memoryOverhead until this goes
> away.  This controls the buffer between the JVM heap size and the amount of
> memory requested from YARN (JVMs can take up memory beyond their heap
> size). You should also make sure that, in the YARN NodeManager
> configuration, yarn.nodemanager.vmem-check-enabled is set to false.
>
> -Sandy
>
>
> On Wed, Aug 20, 2014 at 12:27 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is
>> definitely a YARN related problem...
>>
>> At least for me right now only deployment option possible is standalone...
>>
>>
>>
>> On Tue, Aug 19, 2014 at 11:29 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> Hi Deb,
>>>
>>> I think this may be the same issue as described in
>>> https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
>>> container got killed by YARN because it used much more memory that it
>>> requested. But we haven't figured out the root cause yet.
>>>
>>> +Sandy
>>>
>>> Best,
>>> Xiangrui
>>>
>>> On Tue, Aug 19, 2014 at 8:51 PM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>> > Hi,
>>> >
>>> > During the 4th ALS iteration, I am noticing that one of the executor
>>> gets
>>> > disconnected:
>>> >
>>> > 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
>>> > SendingConnectionManagerId not found
>>> >
>>> > 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
>>> > disconnected, so removing it
>>> >
>>> > 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost
>>> executor 5
>>> > on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated
>>> >
>>> > 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch
>>> 12)
>>> > Any idea if this is a bug related to akka on YARN ?
>>> >
>>> > I am using master
>>> >
>>> > Thanks.
>>> > Deb
>>>
>>
>>
>

--089e0117679dac199e05012c564c--

From dev-return-8985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 00:14:17 2014
Return-Path: <dev-return-8985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A1D911065
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 00:14:17 +0000 (UTC)
Received: (qmail 78005 invoked by uid 500); 22 Aug 2014 00:14:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77946 invoked by uid 500); 22 Aug 2014 00:14:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77934 invoked by uid 99); 22 Aug 2014 00:14:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 00:14:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 00:14:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XKcUI-0008IE-SV
	for dev@spark.incubator.apache.org; Thu, 21 Aug 2014 17:13:50 -0700
Date: Thu, 21 Aug 2014 17:13:50 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408666430871-7949.post@n3.nabble.com>
Subject: saveAsTextFile makes no progress without caching RDD
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, 
Cross-posting this from users list.

I'm running on branch-1.1 and trying to do a simple transformation to a
relatively small dataset of 64GB and saveAsTextFile essentially hangs and
tasks are stuck in running mode with the following code: 

// Stalls with tasks running for over an hour with no tasks finishing.
Smallest partition is 10MB 
val data = sc.textFile("s3n://input") 
val reformatted = data.map(t =>
t.replace("Test(","").replace(")","").replaceAll(",","\t")) 
reformatted.saveAsTextFile("s3n://transformed") 

// This runs but stalls doing GC after filling up 150% of 650GB of memory 
val data = sc.textFile("s3n://input") 
val reformatted = data.map(t =>
t.replace("Test(","").replace(")","").replaceAll(",","\t")).cache 
reformatted.saveAsTextFile("s3n://transformed") 

Any idea if this is a parameter issue and there is something I should try
out? 

Thanks! 

- jerry 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-makes-no-progress-without-caching-RDD-tp7949.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 00:56:40 2014
Return-Path: <dev-return-8986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6890111283
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 00:56:40 +0000 (UTC)
Received: (qmail 55946 invoked by uid 500); 22 Aug 2014 00:56:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55883 invoked by uid 500); 22 Aug 2014 00:56:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55867 invoked by uid 99); 22 Aug 2014 00:56:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 00:56:39 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 00:56:35 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XKd9K-00023h-HX
	for dev@spark.incubator.apache.org; Thu, 21 Aug 2014 17:56:14 -0700
Date: Thu, 21 Aug 2014 17:56:14 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408668974535-7950.post@n3.nabble.com>
In-Reply-To: <1407667288137-7795.post@n3.nabble.com>
References: <1407667288137-7795.post@n3.nabble.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

bump.

I'm seeing the same issue with branch-1.1. Caching the RDD before running
saveAsTextFile gets things running but the job stalls 2/3 of the way by
using too much memory.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7950.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 01:06:23 2014
Return-Path: <dev-return-8987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A5FF112CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 01:06:23 +0000 (UTC)
Received: (qmail 79013 invoked by uid 500); 22 Aug 2014 01:06:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78949 invoked by uid 500); 22 Aug 2014 01:06:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 48037 invoked by uid 99); 21 Aug 2014 11:43:53 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of niranda@wso2.com designates 209.85.218.53 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=wso2.com; s=google;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=trQspQIjaDeJoNOBA3PM/IWGQHz+vfIAecR3R4LMn3I=;
        b=X7XXx2gGe7dpbIhMk5BPg0EEGQocmcGNUT7tdwZaEZJDIDX/CHdrrYg5jH1eNzNijV
         LFWGI4oxV8ktwvAhEWpuJEt1NBFzihWebBoMW1h5AT3mM90UdzclnBWuZoe37B/ojAxc
         NowAwO9KuJl4BNXWohSup/SaviIPKqTXTCVuQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=trQspQIjaDeJoNOBA3PM/IWGQHz+vfIAecR3R4LMn3I=;
        b=b1jjEYPEdSK4nI+eBQCbfyXDZcqV4SwmavwTAo3/Q0QTHDSUqvF/Ab0QmJrJTUXYJ4
         WqrVfYJotT9bCNqx2ZF8PnYqJzlRp/3mWTgTpuEQnG8g9fNKZRPaV5lePzev25E718vR
         vVIg5V6WHZkpOxlSzCrtxPFO58f/N4ctIoEF8+E96rcywWGGyHTFLwVPejUY33IZrQCx
         WCn/tw/bpwYFZ57bj+eI3MAzttdM4wPbp+wgC9/RLfV+mdEUhlWD9FRMluZP54Ocn+zo
         gml6twb1VaD3SE+Hnl1JNI24/l/eWGUem+ng9NpIS9tUA/ezvXQVa4SclebQc0/8VHm6
         F9Tw==
X-Gm-Message-State: ALoCoQnSuJ+mPoADwgjsHKnm2QWeKbrJmmDJk10JafTp+t/gTjw1yLogz7Bqi6g2My2Jqsf0w95m
X-Received: by 10.60.175.234 with SMTP id cd10mr1560942oec.80.1408621405868;
 Thu, 21 Aug 2014 04:43:25 -0700 (PDT)
MIME-Version: 1.0
From: Niranda Perera <niranda@wso2.com>
Date: Thu, 21 Aug 2014 17:13:05 +0530
Message-ID: <CADz3zK1iKca7PcBtFi8n2seO947Dxvk6L5yy4_5_D7MTUTVkVw@mail.gmail.com>
Subject: Storage Handlers in Spark SQL
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6c6f68526370501223aed
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6c6f68526370501223aed
Content-Type: text/plain; charset=UTF-8

Hi,

I have been playing around with Spark for the past few days, and evaluating
the possibility of migrating into Spark (Spark SQL) from Hive/Hadoop.

I am working on the WSO2 Business Activity Monitor (WSO2 BAM,
https://docs.wso2.com/display/BAM241/WSO2+Business+Activity+Monitor+Documentation
) which has currently employed Hive. We are considering Spark as a
successor for Hive, given it's performance enhancement.

We have currently employed several custom storage-handlers in Hive.
Example:
WSO2 JDBC and Cassandra storage handlers:
https://docs.wso2.com/display/BAM241/JDBC+Storage+Handler+for+Hive
https://docs.wso2.com/display/BAM241/Creating+Hive+Queries+to+Analyze+Data#CreatingHiveQueriestoAnalyzeData-cas

I would like to know where Spark SQL can work with these storage
handlers (while using HiveContext may be) ?

Best regards
-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>

--047d7bd6c6f68526370501223aed--

From dev-return-8988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 01:10:11 2014
Return-Path: <dev-return-8988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D2FDF112DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 01:10:11 +0000 (UTC)
Received: (qmail 86349 invoked by uid 500); 22 Aug 2014 01:10:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86286 invoked by uid 500); 22 Aug 2014 01:10:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 48457 invoked by uid 99); 21 Aug 2014 21:55:03 -0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,FREEMAIL_ENVFROM_END_DIGIT,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: 216.139.236.26 is neither permitted nor denied by domain of alex_liu68@yahoo.com)
Date: Thu, 21 Aug 2014 14:54:37 -0700 (PDT)
From: alexliu68 <alex_liu68@yahoo.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408658077945-7945.post@n3.nabble.com>
In-Reply-To: <C434A3773D08A842B26FED6A1BA2E6546D155857@SJCEML702-CHM.china.huawei.com>
References: <1408467214979-7914.post@n3.nabble.com> <1408620901005-7937.post@n3.nabble.com> <C434A3773D08A842B26FED6A1BA2E6546D155857@SJCEML702-CHM.china.huawei.com>
Subject: RE: Spark SQL Query and join different data sources.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Presto is so far good at joining different sources/databases.

I tried a simple join query in Spark SQL, it fails as the followings errors

val a = cql("select test.a  from test JOIN test1 on test.a = test1.a")
a: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[0] at RDD at SchemaRDD.scala:104
== Query Plan ==
Project [a#7]
 Filter (a#7 = a#21)
  CartesianProduct 

org.apache.spark.SparkException: Job aborted due to stage failure: Task
0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on
host 127.0.0.1:
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function
to evaluate expression. type: AttributeReference, tree: a#7
       
org.apache.spark.sql.catalyst.expressions.AttributeReference.eval(namedExpressions.scala:158)
       
org.apache.spark.sql.catalyst.expressions.EqualTo.eval(predicates.scala:146)
       
org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:54)
       
org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:54)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
       
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
       
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        scala.collection.AbstractIterator.to(Iterator.scala:1157)
       
scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
       
scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:731)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:731)
       
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1083)
       
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1083)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)

It looks like Spark SQL has long way to go to be compatible with SQL




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914p7945.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 06:44:17 2014
Return-Path: <dev-return-8989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 26666119AA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 06:44:17 +0000 (UTC)
Received: (qmail 74302 invoked by uid 500); 22 Aug 2014 06:44:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74234 invoked by uid 500); 22 Aug 2014 06:44:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74222 invoked by uid 99); 22 Aug 2014 06:44:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 06:44:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of velvia.github@gmail.com designates 74.125.82.194 as permitted sender)
Received: from [74.125.82.194] (HELO mail-we0-f194.google.com) (74.125.82.194)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 06:44:10 +0000
Received: by mail-we0-f194.google.com with SMTP id u56so3433099wes.9
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 23:43:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=udwyFJxF9ZLUsmdBjC23ZawMm9JqN43UUVv32Br5VN8=;
        b=YHv/GLAeGEEOEg1zOZufbvNrmrUaCx3Ia8na7o+kGGPMi+qZIzMy+QMWoUgSkVo+BI
         GmdBPoA/Y3HhuJTaQeQiyhVtV4TeFCHHjTnLui7l6UjjPvZm1QFPIgBUc7eUZ/ERJOap
         54UU5fKrXkUYNbuGvTMAbIVGwVCL87Mm30+a3XX0oS+Nusb1Hqqk6beZ7sx+gCPViSQI
         1dCWAXGP9TGRYeLjQG0PCzAV0ylXnR9F3DZbRcXQKeWzJ79ioe4sLLPPElSChOecd4QP
         phalQ37pAwgDLVhoOBVoru5c6lpkC3eaNANzfPeUNsu7ZTtEd3Or1kSeSh6GGbYQl/7n
         3+EA==
MIME-Version: 1.0
X-Received: by 10.194.209.169 with SMTP id mn9mr185532wjc.122.1408689829041;
 Thu, 21 Aug 2014 23:43:49 -0700 (PDT)
Received: by 10.216.33.10 with HTTP; Thu, 21 Aug 2014 23:43:49 -0700 (PDT)
Date: Thu, 21 Aug 2014 23:43:49 -0700
Message-ID: <CAN6Vra2Q3KxiYRZHKxWATFOT6hUexEB1B26jSQfJjnh6mbgciA@mail.gmail.com>
Subject: Too late to contribute for 1.1.0?
From: Evan Chan <velvia.github@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm hoping to get in some doc enhancements and small bug fixes for Spark SQL.

Also possibly a small new API to list the tables in sqlContext.

Oh, and to get the doc page I had talked about before, a list of
community Spark projects.

thanks,
Evan

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 06:46:42 2014
Return-Path: <dev-return-8990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D577119AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 06:46:42 +0000 (UTC)
Received: (qmail 78431 invoked by uid 500); 22 Aug 2014 06:46:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78364 invoked by uid 500); 22 Aug 2014 06:46:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78350 invoked by uid 99); 22 Aug 2014 06:46:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 06:46:41 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 06:46:14 +0000
Received: by mail-qc0-f174.google.com with SMTP id l6so10377608qcy.33
        for <dev@spark.apache.org>; Thu, 21 Aug 2014 23:46:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Z5OsQBP/9YqlkRGZSn0v2MVUAxu7Y/BOrjLQColV4R4=;
        b=RjpcsH4JXI5fnX0Ur8wL8YUxHcQTQcY0edFwTjg2Amz0ZT4Ed7ubg52ZH+Kx4jTFF9
         ZE/llARZyQScdbpi2VHMEslhC145I4DaY7V6/Bk7+SJ4ZACE1PFJ/nvb32Ok6mbNGi2a
         Tan3pA3kmaSAN/YMJsTtpgcnmMGO6Ovwfq3TKv00foM9054AeHEhtJlzLZZ1hMjgQmR0
         6Rejs9ekrG8rOjDnrjmsON/IUsL47QEBvZswoAPOhoiHAK0xaXlcMpvttBtQdnyeCCYz
         rdmKCedP8vUK3JHOOjR/ky3awy2k2GBgl3X9JFpGWjHO+ICmbs6HCzn9gXvyPZoktsvx
         tBYA==
X-Gm-Message-State: ALoCoQmYwAoBeMbbM4tLhJSw2T+/4asLw7KZ3P7coe9aTO3/Y0jOKOwPvFpVComw6mGHHJj+X+l0
X-Received: by 10.140.96.85 with SMTP id j79mr4905682qge.5.1408689973470; Thu,
 21 Aug 2014 23:46:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Thu, 21 Aug 2014 23:45:53 -0700 (PDT)
In-Reply-To: <CAN6Vra2Q3KxiYRZHKxWATFOT6hUexEB1B26jSQfJjnh6mbgciA@mail.gmail.com>
References: <CAN6Vra2Q3KxiYRZHKxWATFOT6hUexEB1B26jSQfJjnh6mbgciA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 21 Aug 2014 23:45:53 -0700
Message-ID: <CAPh_B=bgtWNZ9=74UrLruVrx16Bv+gfvDMO6Yy9DJeyXPMpvDQ@mail.gmail.com>
Subject: Re: Too late to contribute for 1.1.0?
To: Evan Chan <velvia.github@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ac46877f1b00501323198
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ac46877f1b00501323198
Content-Type: text/plain; charset=UTF-8

I believe docs changes can go in anytime (because we can just publish new
versions of docs).

Critical bug fixes can still go in too.


On Thu, Aug 21, 2014 at 11:43 PM, Evan Chan <velvia.github@gmail.com> wrote:

> I'm hoping to get in some doc enhancements and small bug fixes for Spark
> SQL.
>
> Also possibly a small new API to list the tables in sqlContext.
>
> Oh, and to get the doc page I had talked about before, a list of
> community Spark projects.
>
> thanks,
> Evan
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113ac46877f1b00501323198--

From dev-return-8991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 09:22:22 2014
Return-Path: <dev-return-8991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD58B11DDB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 09:22:22 +0000 (UTC)
Received: (qmail 83183 invoked by uid 500); 22 Aug 2014 09:22:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83112 invoked by uid 500); 22 Aug 2014 09:22:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83101 invoked by uid 99); 22 Aug 2014 09:22:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 09:22:21 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of apprajen@in.ibm.com designates 122.248.162.6 as permitted sender)
Received: from [122.248.162.6] (HELO e28smtp06.in.ibm.com) (122.248.162.6)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 09:22:13 +0000
Received: from /spool/local
	by e28smtp06.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <apprajen@in.ibm.com>;
	Fri, 22 Aug 2014 14:51:49 +0530
Received: from d28dlp01.in.ibm.com (9.184.220.126)
	by e28smtp06.in.ibm.com (192.168.1.136) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 22 Aug 2014 14:51:46 +0530
Received: from d28relay04.in.ibm.com (d28relay04.in.ibm.com [9.184.220.61])
	by d28dlp01.in.ibm.com (Postfix) with ESMTP id 0532AE0058
	for <dev@spark.apache.org>; Fri, 22 Aug 2014 14:53:54 +0530 (IST)
Received: from d28av05.in.ibm.com (d28av05.in.ibm.com [9.184.220.67])
	by d28relay04.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id s7M9M8Fq55115890
	for <dev@spark.apache.org>; Fri, 22 Aug 2014 14:52:08 +0530
Received: from d28av05.in.ibm.com (localhost [127.0.0.1])
	by d28av05.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s7M9LiKx010416
	for <dev@spark.apache.org>; Fri, 22 Aug 2014 14:51:44 +0530
Received: from d23ml176.in.ibm.com (d23ml176.in.ibm.com [9.182.8.66])
	by d28av05.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s7M9LiK4010413
	for <dev@spark.apache.org>; Fri, 22 Aug 2014 14:51:44 +0530
Subject: Adding support for a new object store
X-KeepSent: A76C4FB7:1BCB92D8-65257D3C:00325F6E;
 type=4; name=$KeepSent
To: dev@spark.apache.org
X-Mailer: IBM Notes Release 9.0.1SHF211 December 19, 2013
Message-ID: <OFA76C4FB7.1BCB92D8-ON65257D3C.00325F6E-65257D3C.00336AEC@in.ibm.com>
From: Rajendran Appavu <apprajen@in.ibm.com>
Date: Fri, 22 Aug 2014 14:51:37 +0530
X-MIMETrack: Serialize by Router on d23ml176/23/M/IBM(Release 8.5.3FP6HF485 | May 7, 2014) at
 22/08/2014 14:48:40
MIME-Version: 1.0
Content-type: text/plain; charset=US-ASCII
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14082209-9574-0000-0000-000000BFCF55
X-Virus-Checked: Checked by ClamAV on apache.org

                                                                                                                           
 I am new to Spark source code and looking to see if i can add push-down support of spark filters to the storage (in my    
 case an object store). I am willing to consider how this can be generically done for any store that we might want to      
 integrate with spark. I am looking to know the areas that I should look into to provide support for a new data store in   
 this context. Following below are some of the questions I have to start with:                                             
                                                                                                                           
 1. Do we need to create a new RDD class for the new store that we want to support? From Spark Context, we create an RDD   
 and the operations on data including the filter are performed through the RDD methods.                                    
                                                                                                                           
 2. When we specify the code for filter task in the RDD.filter() method, how does it get communicated to the Executor on   
 the data node? Does the Executor need to compile this code on the fly and execute it? or how does it work? ( I have       
 looked at the code for sometime, but not yet got to figuring this out, so i am looking for some pointers that can help me 
 come a little up-to-speed in this part of the code)                                                                       
                                                                                                                           
 3. How long the Executor holds the memory? and how does it decide when to release the memory/cache?                       
                                                                                                                           
 Thank you in advance.                                                                                                     
                                                                                                                           
                                                                                                                           



Regards,
Rajendran.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 13:08:22 2014
Return-Path: <dev-return-8992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE685113B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 13:08:21 +0000 (UTC)
Received: (qmail 73894 invoked by uid 500); 22 Aug 2014 13:08:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73837 invoked by uid 500); 22 Aug 2014 13:08:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73825 invoked by uid 99); 22 Aug 2014 13:08:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 13:08:20 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 13:07:54 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XKoZN-0003le-2A
	for dev@spark.incubator.apache.org; Fri, 22 Aug 2014 06:07:53 -0700
Date: Fri, 22 Aug 2014 06:07:53 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408712873055-7955.post@n3.nabble.com>
In-Reply-To: <C434A3773D08A842B26FED6A1BA2E6546D155857@SJCEML702-CHM.china.huawei.com>
References: <1408467214979-7914.post@n3.nabble.com> <1408620901005-7937.post@n3.nabble.com> <C434A3773D08A842B26FED6A1BA2E6546D155857@SJCEML702-CHM.china.huawei.com>
Subject: RE: Spark SQL Query and join different data sources.
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

oops, thanks Yan, you are right, i got

scala> sqlContext.sql("select * from a join b").take(10)
java.lang.RuntimeException: Table Not Found: b
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:90)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:90)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:90)

and with hql

scala> hiveContext.hql("select * from a join b").take(10)
warning: there were 1 deprecation warning(s); re-run with -deprecation for
details
14/08/22 14:48:45 INFO parse.ParseDriver: Parsing command: select * from a
join b
14/08/22 14:48:45 INFO parse.ParseDriver: Parse Completed
14/08/22 14:48:45 ERROR metadata.Hive:
NoSuchObjectException(message:default.a table not found)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:27129)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:27097)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result.read(ThriftHiveMetastore.java:27028)
        at
org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:936)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:922)
        at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:854)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
        at com.sun.proxy.$Proxy17.getTable(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:950)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:924)
        at
org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:59)


so sqlContext is looking up table from
org.apache.spark.sql.catalyst.analysis.SimpleCatalog, Catalog.scala
hiveContext looking up from org.apache.spark.sql.hive.HiveMetastoreCatalog,
HiveMetastoreCatalog.scala

maybe we can do something in sqlContext to register a hive table as
Spark-SQL-Table, need to read column info, partition info, location, SerDe,
Input/OutputFormat and maybe StorageHandler also, from the hive metastore...




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914p7955.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 16:50:46 2014
Return-Path: <dev-return-8993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE81B11C4F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 16:50:46 +0000 (UTC)
Received: (qmail 53733 invoked by uid 500); 22 Aug 2014 16:50:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53679 invoked by uid 500); 22 Aug 2014 16:50:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53668 invoked by uid 99); 22 Aug 2014 16:50:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 16:50:45 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of pnepywoda@palantir.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 16:50:41 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <pnepywoda@palantir.com>)
	id 1XKs2e-0002gS-3W
	for dev@spark.incubator.apache.org; Fri, 22 Aug 2014 09:50:20 -0700
Date: Fri, 22 Aug 2014 09:50:20 -0700 (PDT)
From: pnepywoda <pnepywoda@palantir.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408726220083-7956.post@n3.nabble.com>
Subject: take() reads every partition if the first one is empty
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

On line 777
https://github.com/apache/spark/commit/42571d30d0d518e69eecf468075e4c5a823a2ae8#diff-1d55e54678eff2076263f2fe36150c17R771
the logic for take() reads ALL partitions if the first one (or first k) are
empty. This has actually lead to OOMs when we had many partitions
(thousands) and unfortunately the first one was empty.

Wouldn't a better implementation strategy be

numPartsToTry = partsScanned * 2

instead of

numPartsToTry = totalParts - 1

(this doubling is similar to most memory allocation strategies)

Thanks!
- Paul



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/take-reads-every-partition-if-the-first-one-is-empty-tp7956.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 19:37:04 2014
Return-Path: <dev-return-8994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BCEF7112FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 19:37:04 +0000 (UTC)
Received: (qmail 35408 invoked by uid 500); 22 Aug 2014 19:37:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35342 invoked by uid 500); 22 Aug 2014 19:37:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35324 invoked by uid 99); 22 Aug 2014 19:37:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 19:37:03 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.176] (HELO mail-vc0-f176.google.com) (209.85.220.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 19:36:59 +0000
Received: by mail-vc0-f176.google.com with SMTP id id10so12676136vcb.21
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 12:36:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=V78pddb8YHSkmuIj08beIWzSxxzZxLJqr/iKihIdlnU=;
        b=MJPJThySN/E/AS4EzYo7FWpcmhPQvzetua56R7OXogjdXj0IIQOqla7jyUtJ5V28j3
         9PsR6KNk14sjJYMvq5xc9hfC2Um2Sp6LeJZgtFH2lr9RdJIIkoRMRw6jpRi1y2Wl2Dcg
         la6Qar5s8AGby0MOtKp//P/cqYBMtjgC2QUVhGpXzx5uS/yYk3Q2mf9pw86Llb1xjwCA
         QRruhjGntHCfIDn66stVg7ylG+A6bkjfHbZDhN7IxLqaE8c272EPX9ejqMHrNMEl3aCR
         RyTIUytlyMlW9+IPfRIv193eBqbvZHjqK4AcZc+qWMddmQMcgoTTODj9GCXaZLy2fMOM
         O5eA==
X-Gm-Message-State: ALoCoQk2AeCP1DHc4ktrXnSHhp4jtm7MAtdDWbx6aPfipNAt0PY9YwhtFJLhc74458ehcAadm5ZM
X-Received: by 10.220.112.143 with SMTP id w15mr1342022vcp.41.1408736198352;
        Fri, 22 Aug 2014 12:36:38 -0700 (PDT)
Received: from mail-vc0-f174.google.com (mail-vc0-f174.google.com [209.85.220.174])
        by mx.google.com with ESMTPSA id u10sm80139034vdv.24.2014.08.22.12.36.36
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 22 Aug 2014 12:36:36 -0700 (PDT)
Received: by mail-vc0-f174.google.com with SMTP id la4so12794570vcb.19
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 12:36:36 -0700 (PDT)
X-Received: by 10.52.147.15 with SMTP id tg15mr1106332vdb.53.1408736196497;
 Fri, 22 Aug 2014 12:36:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Fri, 22 Aug 2014 12:36:16 -0700 (PDT)
In-Reply-To: <1408726220083-7956.post@n3.nabble.com>
References: <1408726220083-7956.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Fri, 22 Aug 2014 12:36:16 -0700
Message-ID: <CA+-p3AFtG+Ut4zVFKrvON4-zqjCu_+V6G8SJpMbVkUss8_pr=g@mail.gmail.com>
Subject: Re: take() reads every partition if the first one is empty
To: pnepywoda <pnepywoda@palantir.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=bcaec51ba2a3931c6205013cf4ce
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec51ba2a3931c6205013cf4ce
Content-Type: text/plain; charset=UTF-8

Hi Paul,

I agree that jumping straight from reading N rows from 1 partition to N
rows from ALL partitions is pretty aggressive.  The exponential growth
strategy of doubling the partition count every time seems better -- 1, 2,
4, 8, 16, ... will be much more likely to prevent OOMs than the 1 -> ALL
strategy.

Andrew


On Fri, Aug 22, 2014 at 9:50 AM, pnepywoda <pnepywoda@palantir.com> wrote:

> On line 777
>
> https://github.com/apache/spark/commit/42571d30d0d518e69eecf468075e4c5a823a2ae8#diff-1d55e54678eff2076263f2fe36150c17R771
> the logic for take() reads ALL partitions if the first one (or first k) are
> empty. This has actually lead to OOMs when we had many partitions
> (thousands) and unfortunately the first one was empty.
>
> Wouldn't a better implementation strategy be
>
> numPartsToTry = partsScanned * 2
>
> instead of
>
> numPartsToTry = totalParts - 1
>
> (this doubling is similar to most memory allocation strategies)
>
> Thanks!
> - Paul
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/take-reads-every-partition-if-the-first-one-is-empty-tp7956.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--bcaec51ba2a3931c6205013cf4ce--

From dev-return-8995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 19:53:10 2014
Return-Path: <dev-return-8995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2379911379
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 19:53:10 +0000 (UTC)
Received: (qmail 70202 invoked by uid 500); 22 Aug 2014 19:53:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70134 invoked by uid 500); 22 Aug 2014 19:53:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70118 invoked by uid 99); 22 Aug 2014 19:53:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 19:53:08 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of pnepywoda@palantir.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 19:53:04 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <pnepywoda@palantir.com>)
	id 1XKut9-00048I-Ou
	for dev@spark.incubator.apache.org; Fri, 22 Aug 2014 12:52:43 -0700
Date: Fri, 22 Aug 2014 12:52:43 -0700 (PDT)
From: pnepywoda <pnepywoda@palantir.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408737163759-7958.post@n3.nabble.com>
In-Reply-To: <CA+-p3AFtG+Ut4zVFKrvON4-zqjCu_+V6G8SJpMbVkUss8_pr=g@mail.gmail.com>
References: <1408726220083-7956.post@n3.nabble.com> <CA+-p3AFtG+Ut4zVFKrvON4-zqjCu_+V6G8SJpMbVkUss8_pr=g@mail.gmail.com>
Subject: Re: take() reads every partition if the first one is empty
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

What's the process at this point? Does someone make a bug? Should I make a
bug? (do I even have permission to?)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/take-reads-every-partition-if-the-first-one-is-empty-tp7956p7958.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-8996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 20:07:28 2014
Return-Path: <dev-return-8996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD3A6113F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 20:07:28 +0000 (UTC)
Received: (qmail 11824 invoked by uid 500); 22 Aug 2014 20:07:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11760 invoked by uid 500); 22 Aug 2014 20:07:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11749 invoked by uid 99); 22 Aug 2014 20:07:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:07:27 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:07:01 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so12661095vcb.5
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 13:06:58 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=4Vxnr8Fl8frcesJT4t54ZKZFWMi4MxS+eeNelHXZMK4=;
        b=jEXgZgGdfl2HyhkQn+iDjresZLZOrdzg0VJ4yqhatFZaFtH9EeuBbCdr4ptiD44Fzy
         em+GUSZHAceU0zD093Bht1bayZHrJ3zbgI3g5gM4TCbnFbZ880uWGHdKUzXoxmizeRtL
         5xS7+gOp2e8Mo0qCWkmO6pPOiINMFnx2lEM5ArPUqAFk84uf0P8RlCa/on9d4gtkgsG+
         i/Y9M3+sMnBXbh2K97l/WGjCNTUMjyq643n96DwW47TeK18piN0lKDsrsXvh4nve3Lth
         Txgg0LebSgQYLyU6SOMm89SzTLOxdzrMm2UXoG8bjFxeC2M+U5TZkLLr+8Pa46pyje1d
         MvHA==
X-Gm-Message-State: ALoCoQmKTteGWGw25jAW38n9f3uc3FBu5BNMUCAKY/O90sozOQZG1TXIPjroMJOxej83TekjBJ0z
X-Received: by 10.220.163.130 with SMTP id a2mr5741vcy.52.1408738018183;
        Fri, 22 Aug 2014 13:06:58 -0700 (PDT)
Received: from mail-vc0-f170.google.com (mail-vc0-f170.google.com [209.85.220.170])
        by mx.google.com with ESMTPSA id em9sm90808208vdc.8.2014.08.22.13.06.57
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 22 Aug 2014 13:06:57 -0700 (PDT)
Received: by mail-vc0-f170.google.com with SMTP id lf12so13036548vcb.1
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 13:06:56 -0700 (PDT)
X-Received: by 10.52.87.144 with SMTP id ay16mr1425008vdb.43.1408738016785;
 Fri, 22 Aug 2014 13:06:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Fri, 22 Aug 2014 13:06:36 -0700 (PDT)
In-Reply-To: <1408737163759-7958.post@n3.nabble.com>
References: <1408726220083-7956.post@n3.nabble.com> <CA+-p3AFtG+Ut4zVFKrvON4-zqjCu_+V6G8SJpMbVkUss8_pr=g@mail.gmail.com>
 <1408737163759-7958.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Fri, 22 Aug 2014 13:06:36 -0700
Message-ID: <CA+-p3AGpj9ghR0Ci14bq3RJQCU2goCJsQG==aWECDahkyddc5w@mail.gmail.com>
Subject: Re: take() reads every partition if the first one is empty
To: pnepywoda <pnepywoda@palantir.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=bcaec5040984128e5705013d61ef
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5040984128e5705013d61ef
Content-Type: text/plain; charset=UTF-8

Yep, anyone can create a bug at https://issues.apache.org/jira/browse/SPARK

Then if you make a pull request on GitHub and have the bug number in the
header like "[SPARK-1234] Make take() less OOM-prone", then the PR gets
linked to the Jira ticket.  I think that's the best way to get feedback on
a fix.


On Fri, Aug 22, 2014 at 12:52 PM, pnepywoda <pnepywoda@palantir.com> wrote:

> What's the process at this point? Does someone make a bug? Should I make a
> bug? (do I even have permission to?)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/take-reads-every-partition-if-the-first-one-is-empty-tp7956p7958.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--bcaec5040984128e5705013d61ef--

From dev-return-8997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 20:15:00 2014
Return-Path: <dev-return-8997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 522BD11427
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 20:15:00 +0000 (UTC)
Received: (qmail 24832 invoked by uid 500); 22 Aug 2014 20:14:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24765 invoked by uid 500); 22 Aug 2014 20:14:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24754 invoked by uid 99); 22 Aug 2014 20:14:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:14:58 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:14:30 +0000
Received: by mail-qc0-f180.google.com with SMTP id l6so11348054qcy.39
        for <dev@spark.apache.org>; Fri, 22 Aug 2014 13:14:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:from:content-type:subject:message-id:date:to
         :mime-version;
        bh=4woo/aStw3WHeil3gbRegvhcUnjAE51G+snIiGQvbro=;
        b=ckGJ4JOL6IkSguRxDZY5cGjSOb845nFmdzoVOMsZqml/gxFKEVTOovElxO9W3pJDpP
         /Zi8kY2K3MJhLWlNmqszi4cKXO0AN7i3KNTbjURqNQ5IAdTH+8q9ZdYDHULahYMyMXS+
         VD4q5C6VUPjjIXKZPsrCCKmFcy6nT3xnLYLsMpZritY1lWrzIFKl1l4eMja27wKFkC+A
         i/svKUqN9TdRi99wW0ziBoUYXeM7Oj8JGCnr630gwgBFKWBr3mh5MLIkNfiEUSWtYZ3/
         NzmRFrPI3611lOUHS+/SGgi6SXRFUlM0AG+hgJiwwgOfENMt31e45/F2wsM2UQU3Nlt3
         3BfQ==
X-Gm-Message-State: ALoCoQnfJp97KHvVZqJhyLgUQsUcIrSbNEL8VQ29y08+BtAughncbFYINRP3BZHqy71b0qR/xidB
X-Received: by 10.229.212.194 with SMTP id gt2mr11600296qcb.6.1408738468859;
        Fri, 22 Aug 2014 13:14:28 -0700 (PDT)
Received: from [10.0.1.244] ([38.125.17.226])
        by mx.google.com with ESMTPSA id l76sm36409250qga.8.2014.08.22.13.14.27
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 22 Aug 2014 13:14:27 -0700 (PDT)
From: Jeffrey Picard <jpicard@placeiq.com>
Content-Type: multipart/signed; boundary="Apple-Mail=_A8076F29-AF58-47FF-A080-34A3CFAD8B04"; protocol="application/pgp-signature"; micalg=pgp-sha512
Subject: Graphx GraphLoader Coalesce Shuffle
Message-Id: <A640D7E8-33F9-41AC-A026-059E1EA72445@placeiq.com>
Date: Fri, 22 Aug 2014 16:14:26 -0400
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_A8076F29-AF58-47FF-A080-34A3CFAD8B04
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

Hey all,

I=92ve often found that my spark programs run much more stable with a =
higher number of partitions, and a lot of the graphs I deal with will =
have a few hundred large part files. I was wondering if having a =
parameter in GraphLoader, defaulting to false, to set the shuffle =
parameter in coalesce is something that might be added to graphx, or if =
there was a good reason for not including it? I=92ve been using this =
patch myself for a couple weeks.

=97Jeff

diff --git =
a/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala =
b/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
index f4c7936..b2f9e9c 100644
--- a/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
+++ b/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
@@ -58,13 +58,14 @@ object GraphLoader extends Logging {
       canonicalOrientation: Boolean =3D false,
       minEdgePartitions: Int =3D 1,
       edgeStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY,
-      vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY)
+      vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY,
+      shuffle: Boolean =3D false)
     : Graph[Int, Int] =3D
   {
     val startTime =3D System.currentTimeMillis

     // Parse the edge data table directly into edge partitions
-    val lines =3D sc.textFile(path, =
minEdgePartitions).coalesce(minEdgePartitions)
+    val lines =3D sc.textFile(path, =
minEdgePartitions).coalesce(minEdgePartitions, shuffle)
     val edges =3D lines.mapPartitionsWithIndex { (pid, iter) =3D>
       val builder =3D new EdgePartitionBuilder[Int, Int]
       iter.foreach { line =3D>

--Apple-Mail=_A8076F29-AF58-47FF-A080-34A3CFAD8B04
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJT96SiAAoJEC0tCmi2+GvECtgP/ie7AU/bZpARxC9h21z7wizl
aHbHn+aMWUe0KuhEfoYQb7IYxUYe2S2AQNbXI1DX7WGYU4RGyPmr2MtWQHwAcD5x
tFnSPD6UVOIsPhnXaIgaSCXKS62eXjHDUen3A3Tv5uCl2K6oldJyViIhAgoWTbxP
GLL4fwzWNETE9W3GcqGitDYse6fStEwpQ12xQZMc9DLXTvuQXKb+t56ecT0PHbuW
aW+BUrFVUPyLZWGXutWhsYu2cSXJdkaAOtqnzkBg1eXa8fppdrs5+NyXK2wsNjQL
mvMi4scGZv4ncvUMBzMiwym9/cDTWqvIK/R2AKxIcpy6jrZ1Pv0M40RQXM/45g7w
ab7Laeabhv/+npmfF0+PvpbAQC7nf3BoUjLrpliJ4z4kFl7CJGoN/Vsm2kR/cHRy
XD9ciIfOTsvO8riKYx/qZ1CttEr6DGqWfFipVdBuitpUwBluz19gbFJw6iRSAO7h
bKbUWaqpUeu3J363Q3V9tK9VhIcZV6EkJjKfSAcgHjW2ACHxdTHCz9olvGIaGX02
TCCS+lzo36SASh+UsnWh90sqr/kSS/lqb429CpksxEqHAm0IaB2dtahyzl+LdMcV
F7osszSXgYItb1wJ5i6NHXShQOG5NB3WBliy4m/+OwyuAEs3rwEXXCgAthmvzh+r
LPgpqqqf96cjNRGN9vvr
=RKGl
-----END PGP SIGNATURE-----

--Apple-Mail=_A8076F29-AF58-47FF-A080-34A3CFAD8B04--

From dev-return-8998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 20:55:55 2014
Return-Path: <dev-return-8998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 362D911587
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 20:55:55 +0000 (UTC)
Received: (qmail 29901 invoked by uid 500); 22 Aug 2014 20:55:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29829 invoked by uid 500); 22 Aug 2014 20:55:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29817 invoked by uid 99); 22 Aug 2014 20:55:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:55:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.46 as permitted sender)
Received: from [209.85.213.46] (HELO mail-yh0-f46.google.com) (209.85.213.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 20:55:47 +0000
Received: by mail-yh0-f46.google.com with SMTP id a41so9341879yho.5
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 13:55:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=0GtMsruEAs8sA4NwuHqvve9vOWDqOByYWB6b5xMIIDE=;
        b=eedAzaDZqYsblXXpZyPYhJSt3/0HfwKM9dBSGGv0nCg7J9maWZBY396Z5rq8X9FJ8s
         KoTHMCX820m35Gdy8ga98TydZo0DxnyBdLfB+2jalwsh/66n66aFZF+vmtCGKZFe3N7J
         QHUzVoWJPMIMhCIyausS9TfMWGVY9qxlzv/xY9jUH0EJpT8pnOmPlksXINFjQWX1nr+c
         g94iWFlQSPqypRtpS7/Lx/2PGWbGF5bYnIPSBnhtt8JETijA4XFXMpfnyIY6ixfKnh1w
         gG3uU6ZlH21QzS+d1CJPqX5W8Q3mndKZFVkWsnHWCJKehw4QP0itLfDu5MaYSGU35OxM
         oSwg==
MIME-Version: 1.0
X-Received: by 10.236.222.103 with SMTP id s97mr10256007yhp.40.1408740926751;
 Fri, 22 Aug 2014 13:55:26 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Fri, 22 Aug 2014 13:55:26 -0700 (PDT)
Date: Fri, 22 Aug 2014 13:55:26 -0700
Message-ID: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
Subject: reference to dstream in package org.apache.spark.streaming which is
 not available
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01634728850b8205013e0e50
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634728850b8205013e0e50
Content-Type: text/plain; charset=UTF-8

Hi,
Using the following command on (refreshed) master branch:
mvn clean package -DskipTests

I got:

constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
---------------------------------------------------
java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: scala.reflect.internal.Types$TypeError: bad symbolic reference.
A signature in TestSuiteBase.class refers to term dstream
in package org.apache.spark.streaming which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
TestSuiteBase.class.
at
scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847)
at
scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
at
scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
at
scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
at
scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
at scala.collection.immutable.List.forall(List.scala:84)
at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(Types.scala:4280)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
at
xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
at
xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.scala:296)
at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
at xsbt.Message$$anon$1.apply(Message.scala:8)
at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
at scala.collection.immutable.List.foreach(List.scala:318)
at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
at xsbt.API$ApiPhase.processUnit(API.scala:38)
at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at xsbt.API$ApiPhase.run(API.scala:34)
at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)
at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
at sbt.inc.Incremental$.compile(Incremental.scala:37)
at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
at
scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:303)
at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
at scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
... 8 more

Has anyone seen similar error ?

Cheers

--089e01634728850b8205013e0e50--

From dev-return-8999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 22:41:19 2014
Return-Path: <dev-return-8999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E462A118C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 22:41:18 +0000 (UTC)
Received: (qmail 87911 invoked by uid 500); 22 Aug 2014 22:41:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87841 invoked by uid 500); 22 Aug 2014 22:41:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87826 invoked by uid 99); 22 Aug 2014 22:41:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 22:41:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of velvia.github@gmail.com designates 209.85.212.195 as permitted sender)
Received: from [209.85.212.195] (HELO mail-wi0-f195.google.com) (209.85.212.195)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 22:40:51 +0000
Received: by mail-wi0-f195.google.com with SMTP id n3so96207wiv.2
        for <dev@spark.apache.org>; Fri, 22 Aug 2014 15:40:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=acUtNwkkvcNspaY+sI+LYfQdFD3XxevVQJfNMkekVyc=;
        b=MTlKbcYRMjC6ho/56+HSxwZhXH/tZBWRiuBoPBtBBAKgmqjjO+aVt7Ho1FjOqcuzK0
         670HAnb2qraHfOB4XxtTPw4b1zBMaaE+kfXzCnHyKI4Qorxwb1eRLtVZwuv6rTzI+I/r
         +c9BTlF2lnbt4UP7pBxAebKtpFkgl8t42Uqnpi7mTMIetcGzVcuH/bIXit9vWq19F/ur
         U+DJ3bXoEDbrKKgqB0ZWtcUlUoby4bsoRsCDCU9Yd8yX8ZLo1aUFMmMmhONe7F6ritmq
         aAEl/M9A2wBcpUKaL+Mzj87OJ7VnKXbVAVMY+7g7mYdVwLagMwcdVFNtpJs9qqworYLE
         gpjg==
MIME-Version: 1.0
X-Received: by 10.180.94.161 with SMTP id dd1mr1203477wib.22.1408747250377;
 Fri, 22 Aug 2014 15:40:50 -0700 (PDT)
Received: by 10.216.33.10 with HTTP; Fri, 22 Aug 2014 15:40:50 -0700 (PDT)
Date: Fri, 22 Aug 2014 15:40:50 -0700
Message-ID: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
Subject: [Spark SQL] off-heap columnar store
From: Evan Chan <velvia.github@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey guys,

What is the plan for getting Tachyon/off-heap support for the columnar
compressed store?  It's not in 1.1 is it?

In particular:
 - being able to set TACHYON as the caching mode
 - loading of hot columns or all columns
 - write-through of columnar store data to HDFS or backing store
 - being able to start a context and query directly from Tachyon's
cached columnar data

I think most of this was in Shark 0.9.1.

Also, how likely is the wire format for the columnar compressed data
to change?  That would be a problem for write-through or persistence.

thanks,
Evan

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 22:59:17 2014
Return-Path: <dev-return-9000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 255ED11961
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 22:59:17 +0000 (UTC)
Received: (qmail 33420 invoked by uid 500); 22 Aug 2014 22:59:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33353 invoked by uid 500); 22 Aug 2014 22:59:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33342 invoked by uid 99); 22 Aug 2014 22:59:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 22:59:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 22:58:50 +0000
Received: by mail-ie0-f171.google.com with SMTP id at1so7237378iec.2
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 15:58:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=f5ej0I6A7lOF3cHXXLCBwD/oC09mw3HgMKUzMUu+UjY=;
        b=ArVbX/kf6m1DVkhCD4jRwthOe/uSZEQa/G9Wp98+XbxFTqA5s4zIwsMEMb+YUrEVA1
         HH3zDzcvJEOCRochdURIUOOk2sRSu3nFv2EeeIf+AHEcJRCn2EQR+c2LlcpMfhbeTS6K
         6kVg8/LCQv0FxjRPm+ovUT90A38Rjv720O8oMam9sTiNNh2DLlyJ/stJ4gtJDxBV6HGT
         s5/RYbdBUz4eg2WlDHhGt+11sbEJsdJzaT+od3HNwqcUaw9pYg7NE11K1IRxYBe/F8ao
         xrcBEuhrJR+rL+DAEJ/yRZO6VeTXP+5RLmS0t8EzMLXw0wpcPw2IZZLy9mDYGVPMYcIZ
         Lz6w==
X-Gm-Message-State: ALoCoQkwLBNkrIXhwJdxke9KRPd0vCk9wvPrLyx5gaMt8InyDYhLhW4VVfNLRptZWgQfLb4Oob2F
X-Received: by 10.50.50.198 with SMTP id e6mr1523237igo.1.1408748328621; Fri,
 22 Aug 2014 15:58:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Fri, 22 Aug 2014 15:58:28 -0700 (PDT)
In-Reply-To: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
References: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 22 Aug 2014 23:58:28 +0100
Message-ID: <CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
Subject: Re: reference to dstream in package org.apache.spark.streaming which
 is not available
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, master hasn't compiled for me for a few days. It's fixed in:

https://github.com/apache/spark/pull/1726
https://github.com/apache/spark/pull/2075

Could a committer sort this out?

Sean


On Fri, Aug 22, 2014 at 9:55 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> Hi,
> Using the following command on (refreshed) master branch:
> mvn clean package -DskipTests
>
> I got:
>
> constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
> ---------------------------------------------------
> java.lang.reflect.InvocationTargetException
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
> at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
> Caused by: scala.reflect.internal.Types$TypeError: bad symbolic reference.
> A signature in TestSuiteBase.class refers to term dstream
> in package org.apache.spark.streaming which is not available.
> It may be completely missing from the current classpath, or the version on
> the classpath might be incompatible with the version used when compiling
> TestSuiteBase.class.
> at
> scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847)
> at
> scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854)
> at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
> at
> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
> at
> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
> at
> scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
> at scala.collection.immutable.List.forall(List.scala:84)
> at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(Types.scala:4280)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
> at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
> at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
> at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
> at
> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
> at
> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
> at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
> at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
> at
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
> at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
> at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
> at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
> at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.scala:296)
> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> at xsbt.Message$$anon$1.apply(Message.scala:8)
> at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
> at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
> at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
> at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> at scala.collection.immutable.List.foreach(List.scala:318)
> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
> at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
> at xsbt.API$ApiPhase.processUnit(API.scala:38)
> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> at xsbt.API$ApiPhase.run(API.scala:34)
> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
> at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> at sbt.inc.Incremental$.compile(Incremental.scala:37)
> at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
> at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
> at
> scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:303)
> at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
> at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
> at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
> at scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
> at
> org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
> at
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
> at
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
> at
> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
> at
> org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
> at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
> ... 8 more
>
> Has anyone seen similar error ?
>
> Cheers

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 22 23:07:59 2014
Return-Path: <dev-return-9001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC741119A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Aug 2014 23:07:58 +0000 (UTC)
Received: (qmail 55552 invoked by uid 500); 22 Aug 2014 23:07:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55487 invoked by uid 500); 22 Aug 2014 23:07:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55467 invoked by uid 99); 22 Aug 2014 23:07:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 23:07:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hshreedharan@cloudera.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Aug 2014 23:07:30 +0000
Received: by mail-pa0-f47.google.com with SMTP id kx10so17043761pab.34
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 16:07:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:message-id:date:from:user-agent:mime-version:to
         :cc:subject:references:in-reply-to:content-type;
        bh=oDDMdx5NCV3jigZOOHlXlbcaoSnzb6DiK91m2dg8gwg=;
        b=RXE9VbfxN+Rnp2zloGcnwqnTbwFq0LhSQ2q4DRXgJ7mFBhc7ZOm9PJiZZcfaCv8zql
         FWi6KxppyVmiILVrMaY8wTOJAnla+uyISHKuVXBdZmsp0TwmGW3L7XBGYBjIxA8ZY8YH
         ZuEX941iJVy2VM10lpIRk01ZZRYKe4zhhH7MtZGNDuHMmacfs8+v8M2WC7UEjQnyJPY2
         TLp5KW8MCb8AWQvurFiLj2l03qzFdDgRuFtjVz6GEE91cAd6dlRxj3tAa2u7ee9uFtMr
         AIbop3pHRtsjUPLxfMTZnYkNCa1vq1rVSN3KulVzQDa/hXDUdrfNhIc2u7Oa4quczqwS
         v48A==
X-Gm-Message-State: ALoCoQntPASFwz4KuD5w918wAWnyLZN/WzE/+w7EyBglnt1dZYU7zhcjN/M9mXOrQ0EQZI7pdao7
X-Received: by 10.69.26.68 with SMTP id iw4mr9631618pbd.137.1408748848804;
        Fri, 22 Aug 2014 16:07:28 -0700 (PDT)
Received: from Psychman.local ([74.217.76.11])
        by mx.google.com with ESMTPSA id ex1sm45416922pdb.26.2014.08.22.16.07.27
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 22 Aug 2014 16:07:28 -0700 (PDT)
Message-ID: <53F7CD2E.3080902@cloudera.com>
Date: Fri, 22 Aug 2014 16:07:26 -0700
From: Hari Shreedharan <hshreedharan@cloudera.com>
User-Agent: Postbox 3.0.11 (Macintosh/20140602)
MIME-Version: 1.0
To: Sean Owen <sowen@cloudera.com>
CC: Ted Yu <yuzhihong@gmail.com>, 
 "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: Re: reference to dstream in package org.apache.spark.streaming which
 is not available
References: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com> <CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
In-Reply-To: <CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------080304050306040504020205"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------080304050306040504020205
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit

Sean - I think only the ones in 1726 are enough. It is weird that any 
class that uses the test-jar actually requires the streaming jar to be 
added explicitly. Shouldn't maven take care of this?

I posted some comments on the PR.

-- 

Thanks,
Hari


> Sean Owen <mailto:sowen@cloudera.com>
> August 22, 2014 at 3:58 PM
> Yes, master hasn't compiled for me for a few days. It's fixed in:
>
> https://github.com/apache/spark/pull/1726
> https://github.com/apache/spark/pull/2075
>
> Could a committer sort this out?
>
> Sean
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
> Ted Yu <mailto:yuzhihong@gmail.com>
> August 22, 2014 at 1:55 PM
> Hi,
> Using the following command on (refreshed) master branch:
> mvn clean package -DskipTests
>
> I got:
>
> constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
> ---------------------------------------------------
> java.lang.reflect.InvocationTargetException
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
> at
> org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
> at 
> org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
> Caused by: scala.reflect.internal.Types$TypeError: bad symbolic reference.
> A signature in TestSuiteBase.class refers to term dstream
> in package org.apache.spark.streaming which is not available.
> It may be completely missing from the current classpath, or the version on
> the classpath might be incompatible with the version used when compiling
> TestSuiteBase.class.
> at
> scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847)
> at
> scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854)
> at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
> at
> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
> at
> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
> at
> scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
> at scala.collection.immutable.List.forall(List.scala:84)
> at 
> scala.reflect.internal.Types$TypeMap.noChangeToSymbols(Types.scala:4280)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
> at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
> at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
> at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
> at
> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
> at
> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
> at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
> at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
> at
> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
> at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
> at 
> scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
> at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
> at 
> xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.scala:296)
> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> at xsbt.Message$$anon$1.apply(Message.scala:8)
> at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
> at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
> at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
> at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> at scala.collection.immutable.List.foreach(List.scala:318)
> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
> at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
> at xsbt.API$ApiPhase.processUnit(API.scala:38)
> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> at xsbt.API$ApiPhase.run(API.scala:34)
> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
> at
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)
> at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
> at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> at sbt.inc.Incremental$.compile(Incremental.scala:37)
> at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
> at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
> at
> scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:303)
> at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
> at 
> scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
> at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
> at scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
> at
> org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
> at
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
> at
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
> at
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
> at
> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
> at
> org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
> at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
> ... 8 more
>
> Has anyone seen similar error ?
>
> Cheers
>

--------------080304050306040504020205--

From dev-return-9002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 00:09:10 2014
Return-Path: <dev-return-9002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE42811B44
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 00:09:10 +0000 (UTC)
Received: (qmail 71751 invoked by uid 500); 23 Aug 2014 00:09:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71690 invoked by uid 500); 23 Aug 2014 00:09:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71679 invoked by uid 99); 23 Aug 2014 00:09:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:09:09 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:08:43 +0000
Received: by mail-qa0-f52.google.com with SMTP id j15so10139971qaq.25
        for <dev@spark.apache.org>; Fri, 22 Aug 2014 17:08:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=qnOy+LUEgZrhAfyW/ohYEvjqkoFeQgkKBjQtf8vVbTk=;
        b=juR6v6VUQkICxO7k/CaBsUlw3pE9Bqth274gDoE8rItEbDeWjHd1uRcy9Iw6Esi4B8
         daWiOIYMCCMZ81/smR/5mvtOLH2wnDoNSKJofhUQ80mAw9Y5TTIhHFsffoM31salqKH6
         k7KzGu23hlJfyNrBW5/tXtE1h5UStTrjxxD7ojbqgfL0d0ggOvC2AbMWT/dAztSi+9Mo
         /34OSRbhx1f0OL3NHPeNnvmCcIVdfyyQf6TFvmRyBfNWwEf950YT9ppjz7GDh548jPA0
         r1/RqJGaYDZDokqVRWmbgSQmEsHlXMj7TS6tWEKt9jjn4i85NOhOe28owfDCSBRj/SBt
         RIyQ==
X-Gm-Message-State: ALoCoQlCVYwlA7e7PQDElq+ITwd+c+ms3Exmud9PG/uxOZOSepCc55wOnLheXu1CLoFHAB5yFDIL
X-Received: by 10.224.30.139 with SMTP id u11mr10701457qac.77.1408752522442;
 Fri, 22 Aug 2014 17:08:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.53.71 with HTTP; Fri, 22 Aug 2014 17:08:22 -0700 (PDT)
In-Reply-To: <CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
 <CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com> <CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 22 Aug 2014 17:08:22 -0700
Message-ID: <CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
Subject: Re: Spark Contribution
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Henry Saputra <henry.saputra@gmail.com>, Maisnam Ns <maisnam.ns@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bea2dd8ad523f050140c190
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bea2dd8ad523f050140c190
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Great idea. Added the link
https://github.com/apache/spark/blob/master/README.md



On Thu, Aug 21, 2014 at 4:06 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> We should add this link to the readme on GitHub btw.
>
> 2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Henry S=
aputra<henry.saputra@gmail.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=
=9C =EB=A9=94=EC=8B=9C=EC=A7=80:
>
> > The Apache Spark wiki on how to contribute should be great place to
> > start:
> > https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> >
> > - Henry
> >
> > On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
> > <javascript:;>> wrote:
> > > Hi,
> > >
> > > Can someone help me with some links on how to contribute for Spark
> > >
> > > Regards
> > > mns
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> > For additional commands, e-mail: dev-help@spark.apache.org
> <javascript:;>
> >
> >
>

--047d7bea2dd8ad523f050140c190--

From dev-return-9003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 00:44:05 2014
Return-Path: <dev-return-9003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 96C9A11BDC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 00:44:05 +0000 (UTC)
Received: (qmail 29801 invoked by uid 500); 23 Aug 2014 00:44:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29728 invoked by uid 500); 23 Aug 2014 00:44:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29717 invoked by uid 99); 23 Aug 2014 00:44:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:44:04 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:44:00 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1XKzQh-0001kH-GM
	for dev@spark.incubator.apache.org; Fri, 22 Aug 2014 17:43:39 -0700
Date: Fri, 22 Aug 2014 17:43:39 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408754619472-7966.post@n3.nabble.com>
Subject: Graphx seems to be broken while Creating a large graph(6B nodes in
 my case)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

While creating a graph with 6B nodes and 12B edges, I noticed that
*'numVertices' api returns incorrect result*; 'numEdges' reports correct
number. For few times(with different dataset > 2.5B nodes) I have also
notices that numVertices is returned as -ive number; so I suspect that there
is some overflow (may be we are using Int for some field?).

Environment: Standalone mode running on EC2 . Using latest code from master
branch upto commit #db56f2df1b8027171da1b8d2571d1f2ef1e103b6 .

Here is some details of experiments I have done so far: 
1. Input: numNodes=6101995593 ; noEdges=12163784626
Graph returns: numVertices=1807028297 ; numEdges=12163784626
2. Input : numNodes=*2157586441* ; noEdges=2747322705
Graph Returns: numVertices=*-2137380855* ; numEdges=2747322705
3. Input: numNodes=1725060105 ; noEdges=204176821
Graph: numVertices=1725060105 ; numEdges=2041768213 


You can find the code to generate this bug here:
https://gist.github.com/npanj/92e949d86d08715bf4bf

(I have also filed this jira ticket:
https://issues.apache.org/jira/browse/SPARK-3190)





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-seems-to-be-broken-while-Creating-a-large-graph-6B-nodes-in-my-case-tp7966.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 00:52:04 2014
Return-Path: <dev-return-9004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D23411C00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 00:52:04 +0000 (UTC)
Received: (qmail 46439 invoked by uid 500); 23 Aug 2014 00:52:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46376 invoked by uid 500); 23 Aug 2014 00:52:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46364 invoked by uid 99); 23 Aug 2014 00:52:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:52:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.44 as permitted sender)
Received: from [209.85.218.44] (HELO mail-oi0-f44.google.com) (209.85.218.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 00:51:36 +0000
Received: by mail-oi0-f44.google.com with SMTP id x69so8251479oia.3
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 17:51:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=wes5XBmsafqZScZPsL2wuUKj05l2Sb2xx4K6Zkpzqro=;
        b=donegFAQhJxLuXmjTDmBXv4hP+aQFwxYeVStkYxkUW8uk+cQx1y8z2sUjv96cq1h67
         NCbauQJz9YfkYx9JYIZ23ymyhnEcb/5IOytPcpJNOsf1I+Fp/4ZqEfl+mCpmyiYwElvU
         3sBEtnICvjSAdMK23eFFyyO2ceLRFgajmfFPqmqT9gQWPM4QgicK9cMGSYzMdvCVDj97
         YMVqqeM+gMwqWT5b576b8FYm8/aTBaEAQnK5gL7JWARU43rimWcOEGpnHf8CApfbGSIo
         fwITsImRTCUi1/9lY3MWyGKrJ6xOlWEityvHSixVWIOJiz+LlQfhsLVxoHzpVudj/1T9
         7k3g==
MIME-Version: 1.0
X-Received: by 10.182.22.82 with SMTP id b18mr8186394obf.32.1408755094947;
 Fri, 22 Aug 2014 17:51:34 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Fri, 22 Aug 2014 17:51:34 -0700 (PDT)
In-Reply-To: <53F7CD2E.3080902@cloudera.com>
References: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
	<CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
	<53F7CD2E.3080902@cloudera.com>
Date: Fri, 22 Aug 2014 17:51:34 -0700
Message-ID: <CABPQxsuWOwrJ_2WjaXSaB0LNeFZyxLVXPfN7dSxN+ZYCrffhZQ@mail.gmail.com>
Subject: Re: reference to dstream in package org.apache.spark.streaming which
 is not available
From: Patrick Wendell <pwendell@gmail.com>
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: Sean Owen <sowen@cloudera.com>, Ted Yu <yuzhihong@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11333348028ed70501415b80
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11333348028ed70501415b80
Content-Type: text/plain; charset=ISO-8859-1

Hey All,

We can sort this out ASAP. Many of the Spark committers were at a company
offsite for the last 72 hours, so sorry that it is broken.

- Patrick


On Fri, Aug 22, 2014 at 4:07 PM, Hari Shreedharan <hshreedharan@cloudera.com
> wrote:

> Sean - I think only the ones in 1726 are enough. It is weird that any
> class that uses the test-jar actually requires the streaming jar to be
> added explicitly. Shouldn't maven take care of this?
>
> I posted some comments on the PR.
>
> --
>
> Thanks,
> Hari
>
>
>  Sean Owen <mailto:sowen@cloudera.com>
>> August 22, 2014 at 3:58 PM
>>
>> Yes, master hasn't compiled for me for a few days. It's fixed in:
>>
>> https://github.com/apache/spark/pull/1726
>> https://github.com/apache/spark/pull/2075
>>
>> Could a committer sort this out?
>>
>> Sean
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>> Ted Yu <mailto:yuzhihong@gmail.com>
>> August 22, 2014 at 1:55 PM
>>
>> Hi,
>> Using the following command on (refreshed) master branch:
>> mvn clean package -DskipTests
>>
>> I got:
>>
>> constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
>> ---------------------------------------------------
>> java.lang.reflect.InvocationTargetException
>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> at
>> sun.reflect.NativeMethodAccessorImpl.invoke(
>> NativeMethodAccessorImpl.java:57)
>> at
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(
>> DelegatingMethodAccessorImpl.java:43)
>> at java.lang.reflect.Method.invoke(Method.java:606)
>> at
>> org.codehaus.plexus.classworlds.launcher.Launcher.
>> launchEnhanced(Launcher.java:289)
>> at
>> org.codehaus.plexus.classworlds.launcher.Launcher.
>> launch(Launcher.java:229)
>> at
>> org.codehaus.plexus.classworlds.launcher.Launcher.
>> mainWithExitCode(Launcher.java:415)
>> at org.codehaus.plexus.classworlds.launcher.Launcher.
>> main(Launcher.java:356)
>> Caused by: scala.reflect.internal.Types$TypeError: bad symbolic
>> reference.
>> A signature in TestSuiteBase.class refers to term dstream
>> in package org.apache.spark.streaming which is not available.
>> It may be completely missing from the current classpath, or the version on
>> the classpath might be incompatible with the version used when compiling
>> TestSuiteBase.class.
>> at
>> scala.reflect.internal.pickling.UnPickler$Scan.
>> toTypeError(UnPickler.scala:847)
>> at
>> scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(
>> UnPickler.scala:854)
>> at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
>> at
>> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
>> Types.scala:4280)
>> at
>> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
>> Types.scala:4280)
>> at
>> scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.
>> scala:70)
>> at scala.collection.immutable.List.forall(List.scala:84)
>> at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(
>> Types.scala:4280)
>> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
>> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
>> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
>> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
>> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
>> at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
>> at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
>> at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
>> at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
>> at
>> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
>> ExtractAPI.scala:296)
>> at
>> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
>> ExtractAPI.scala:296)
>> at
>> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
>> TraversableLike.scala:251)
>> at
>> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
>> TraversableLike.scala:251)
>> at
>> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.
>> scala:33)
>> at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>> at scala.collection.TraversableLike$class.flatMap(
>> TraversableLike.scala:251)
>> at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
>> at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.
>> scala:296)
>> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
>> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
>> at xsbt.Message$$anon$1.apply(Message.scala:8)
>> at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
>> at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
>> at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
>> at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
>> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
>> at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
>> at scala.collection.immutable.List.foreach(List.scala:318)
>> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
>> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
>> at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
>> at xsbt.API$ApiPhase.processUnit(API.scala:38)
>> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
>> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
>> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>> at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>> at xsbt.API$ApiPhase.run(API.scala:34)
>> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
>> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
>> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
>> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
>> at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
>> at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
>> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> at
>> sun.reflect.NativeMethodAccessorImpl.invoke(
>> NativeMethodAccessorImpl.java:57)
>> at
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(
>> DelegatingMethodAccessorImpl.java:43)
>> at java.lang.reflect.Method.invoke(Method.java:606)
>> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
>> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
>> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> apply$mcV$sp(AggressiveCompile.scala:99)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> apply(AggressiveCompile.scala:99)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> apply(AggressiveCompile.scala:99)
>> at
>> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$
>> timed(AggressiveCompile.scala:166)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(
>> AggressiveCompile.scala:98)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
>> 143)
>> at
>> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
>> 87)
>> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
>> Compile.scala:39)
>> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
>> Compile.scala:37)
>> at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
>> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
>> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
>> at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
>> at sbt.inc.Incremental$.compile(Incremental.scala:37)
>> at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
>> at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
>> at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
>> at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
>> at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
>> at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
>> at
>> scala_maven.ScalaCompilerSupport.incrementalCompile(
>> ScalaCompilerSupport.java:303)
>> at scala_maven.ScalaCompilerSupport.compile(
>> ScalaCompilerSupport.java:119)
>> at scala_maven.ScalaCompilerSupport.doExecute(
>> ScalaCompilerSupport.java:99)
>> at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
>> at scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
>> at
>> org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(
>> DefaultBuildPluginManager.java:106)
>> at
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> MojoExecutor.java:208)
>> at
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> MojoExecutor.java:153)
>> at
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> MojoExecutor.java:145)
>> at
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
>> LifecycleModuleBuilder.java:84)
>> at
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
>> LifecycleModuleBuilder.java:59)
>> at
>> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(
>> LifecycleStarter.java:183)
>> at
>> org.apache.maven.lifecycle.internal.LifecycleStarter.
>> execute(LifecycleStarter.java:161)
>> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
>> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
>> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
>> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
>> at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
>> ... 8 more
>>
>> Has anyone seen similar error ?
>>
>> Cheers
>>
>>

--001a11333348028ed70501415b80--

From dev-return-9005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 01:35:48 2014
Return-Path: <dev-return-9005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9FF2311CDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 01:35:48 +0000 (UTC)
Received: (qmail 3232 invoked by uid 500); 23 Aug 2014 01:35:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3179 invoked by uid 500); 23 Aug 2014 01:35:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3164 invoked by uid 99); 23 Aug 2014 01:35:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 01:35:47 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 01:35:20 +0000
Received: by mail-ie0-f181.google.com with SMTP id rp18so7387638iec.12
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 18:35:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=EfFxiYubIUOigX5xsvbGmeWurU5m5R7n4pNIn8+d9bw=;
        b=lsfchlTI1MpQMOV5n6ZzYXqYl6yBVAEY4OxAJDHMVywgUBMRhXW9J+7tjhs/uRp7Ad
         KBqHdQHO0vdiul4nq0mWYjN4UyZJOULLqpdjPYgHNnzAgMxOW4LqCQ0qCtzRSSD1U2ZK
         MHLa1DXwoo8+lfnzyFaoiAxIMfqiD6tayyZ7Ez0H1Q0No1VtOcIo5n7GAKQahacBGawu
         LLD8AIWtWLNRgXtlspCZvYMY3mhWTsvisW5p7//VqpuYYdYdv/LHueETH4rrQFTibtPN
         wqEWDsH8XJM3OO8yXw/XAObUWzqaMP1k6oxpor5K4Ul8U0a1I6raUYml33FDBVJQyxbQ
         3k7A==
X-Received: by 10.42.48.197 with SMTP id t5mr12807627icf.11.1408757719425;
 Fri, 22 Aug 2014 18:35:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.128.161 with HTTP; Fri, 22 Aug 2014 18:34:49 -0700 (PDT)
In-Reply-To: <CABPQxsuWOwrJ_2WjaXSaB0LNeFZyxLVXPfN7dSxN+ZYCrffhZQ@mail.gmail.com>
References: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
 <CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
 <53F7CD2E.3080902@cloudera.com> <CABPQxsuWOwrJ_2WjaXSaB0LNeFZyxLVXPfN7dSxN+ZYCrffhZQ@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Fri, 22 Aug 2014 18:34:49 -0700
Message-ID: <CAMwrk0k8aKNmvpexE4uRYwbK7JzJ2YRCFm7FJDWCNRtqTaEoZA@mail.gmail.com>
Subject: Re: reference to dstream in package org.apache.spark.streaming which
 is not available
To: Patrick Wendell <pwendell@gmail.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Sean Owen <sowen@cloudera.com>, 
	Ted Yu <yuzhihong@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba6e8e3e70eacd050141f7c0
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e8e3e70eacd050141f7c0
Content-Type: text/plain; charset=UTF-8

Figured it out. Fixing this ASAP.

TD


On Fri, Aug 22, 2014 at 5:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> We can sort this out ASAP. Many of the Spark committers were at a company
> offsite for the last 72 hours, so sorry that it is broken.
>
> - Patrick
>
>
> On Fri, Aug 22, 2014 at 4:07 PM, Hari Shreedharan <
> hshreedharan@cloudera.com
> > wrote:
>
> > Sean - I think only the ones in 1726 are enough. It is weird that any
> > class that uses the test-jar actually requires the streaming jar to be
> > added explicitly. Shouldn't maven take care of this?
> >
> > I posted some comments on the PR.
> >
> > --
> >
> > Thanks,
> > Hari
> >
> >
> >  Sean Owen <mailto:sowen@cloudera.com>
> >> August 22, 2014 at 3:58 PM
> >>
> >> Yes, master hasn't compiled for me for a few days. It's fixed in:
> >>
> >> https://github.com/apache/spark/pull/1726
> >> https://github.com/apache/spark/pull/2075
> >>
> >> Could a committer sort this out?
> >>
> >> Sean
> >>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >> Ted Yu <mailto:yuzhihong@gmail.com>
> >> August 22, 2014 at 1:55 PM
> >>
> >> Hi,
> >> Using the following command on (refreshed) master branch:
> >> mvn clean package -DskipTests
> >>
> >> I got:
> >>
> >> constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
> >> ---------------------------------------------------
> >> java.lang.reflect.InvocationTargetException
> >> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >> at
> >> sun.reflect.NativeMethodAccessorImpl.invoke(
> >> NativeMethodAccessorImpl.java:57)
> >> at
> >> sun.reflect.DelegatingMethodAccessorImpl.invoke(
> >> DelegatingMethodAccessorImpl.java:43)
> >> at java.lang.reflect.Method.invoke(Method.java:606)
> >> at
> >> org.codehaus.plexus.classworlds.launcher.Launcher.
> >> launchEnhanced(Launcher.java:289)
> >> at
> >> org.codehaus.plexus.classworlds.launcher.Launcher.
> >> launch(Launcher.java:229)
> >> at
> >> org.codehaus.plexus.classworlds.launcher.Launcher.
> >> mainWithExitCode(Launcher.java:415)
> >> at org.codehaus.plexus.classworlds.launcher.Launcher.
> >> main(Launcher.java:356)
> >> Caused by: scala.reflect.internal.Types$TypeError: bad symbolic
> >> reference.
> >> A signature in TestSuiteBase.class refers to term dstream
> >> in package org.apache.spark.streaming which is not available.
> >> It may be completely missing from the current classpath, or the version
> on
> >> the classpath might be incompatible with the version used when compiling
> >> TestSuiteBase.class.
> >> at
> >> scala.reflect.internal.pickling.UnPickler$Scan.
> >> toTypeError(UnPickler.scala:847)
> >> at
> >> scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(
> >> UnPickler.scala:854)
> >> at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
> >> at
> >> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
> >> Types.scala:4280)
> >> at
> >> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
> >> Types.scala:4280)
> >> at
> >> scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.
> >> scala:70)
> >> at scala.collection.immutable.List.forall(List.scala:84)
> >> at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(
> >> Types.scala:4280)
> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
> >> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
> >> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
> >> at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
> >> at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
> >> at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
> >> at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
> >> at
> >> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
> >> ExtractAPI.scala:296)
> >> at
> >> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
> >> ExtractAPI.scala:296)
> >> at
> >> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
> >> TraversableLike.scala:251)
> >> at
> >> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
> >> TraversableLike.scala:251)
> >> at
> >> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.
> >> scala:33)
> >> at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
> >> at scala.collection.TraversableLike$class.flatMap(
> >> TraversableLike.scala:251)
> >> at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
> >> at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.
> >> scala:296)
> >> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> >> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
> >> at xsbt.Message$$anon$1.apply(Message.scala:8)
> >> at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
> >> at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
> >> at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
> >> at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
> >> at
> xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> >> at
> xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
> >> at scala.collection.immutable.List.foreach(List.scala:318)
> >> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
> >> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
> >> at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
> >> at xsbt.API$ApiPhase.processUnit(API.scala:38)
> >> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> >> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
> >> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >> at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >> at xsbt.API$ApiPhase.run(API.scala:34)
> >> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> >> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> >> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> >> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> >> at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> >> at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> >> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> >> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >> at
> >> sun.reflect.NativeMethodAccessorImpl.invoke(
> >> NativeMethodAccessorImpl.java:57)
> >> at
> >> sun.reflect.DelegatingMethodAccessorImpl.invoke(
> >> DelegatingMethodAccessorImpl.java:43)
> >> at java.lang.reflect.Method.invoke(Method.java:606)
> >> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> >> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> >> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
> >> apply$mcV$sp(AggressiveCompile.scala:99)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
> >> apply(AggressiveCompile.scala:99)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
> >> apply(AggressiveCompile.scala:99)
> >> at
> >> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$
> >> timed(AggressiveCompile.scala:166)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(
> >> AggressiveCompile.scala:98)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
> >> 143)
> >> at
> >> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
> >> 87)
> >> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
> >> Compile.scala:39)
> >> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
> >> Compile.scala:37)
> >> at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> >> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> >> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> >> at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> >> at sbt.inc.Incremental$.compile(Incremental.scala:37)
> >> at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> >> at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> >> at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> >> at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> >> at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
> >> at
> sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
> >> at
> >> scala_maven.ScalaCompilerSupport.incrementalCompile(
> >> ScalaCompilerSupport.java:303)
> >> at scala_maven.ScalaCompilerSupport.compile(
> >> ScalaCompilerSupport.java:119)
> >> at scala_maven.ScalaCompilerSupport.doExecute(
> >> ScalaCompilerSupport.java:99)
> >> at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
> >> at
> scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
> >> at
> >> org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(
> >> DefaultBuildPluginManager.java:106)
> >> at
> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
> >> MojoExecutor.java:208)
> >> at
> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
> >> MojoExecutor.java:153)
> >> at
> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
> >> MojoExecutor.java:145)
> >> at
> >> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
> >> LifecycleModuleBuilder.java:84)
> >> at
> >> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
> >> LifecycleModuleBuilder.java:59)
> >> at
> >>
> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(
> >> LifecycleStarter.java:183)
> >> at
> >> org.apache.maven.lifecycle.internal.LifecycleStarter.
> >> execute(LifecycleStarter.java:161)
> >> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
> >> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
> >> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
> >> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
> >> at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
> >> ... 8 more
> >>
> >> Has anyone seen similar error ?
> >>
> >> Cheers
> >>
> >>
>

--90e6ba6e8e3e70eacd050141f7c0--

From dev-return-9006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 01:48:20 2014
Return-Path: <dev-return-9006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01F5D11D0C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 01:48:20 +0000 (UTC)
Received: (qmail 15283 invoked by uid 500); 23 Aug 2014 01:48:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15215 invoked by uid 500); 23 Aug 2014 01:48:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15203 invoked by uid 99); 23 Aug 2014 01:48:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 01:48:19 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.176 as permitted sender)
Received: from [209.85.223.176] (HELO mail-ie0-f176.google.com) (209.85.223.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 01:47:53 +0000
Received: by mail-ie0-f176.google.com with SMTP id tr6so7418403ieb.21
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 18:47:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=hIQiR7k8qNl7hBnmDCLoI9PA5sRtY28HeNfnCaD8ghk=;
        b=uN+5ORU8WNOC81II/ntaXr4uTwGNcu7LZwAkmvCqqKOKuLeEnWfR2MaMwAu+BQguOE
         4H1uU7YZ3lr7k8MDM3QupDWRMrH0E6vqdrhT3zgBasSjh1SKFNWUKKzmnYDM8ZgAE97b
         Ff5uNNMusQ+mH+OdnPJoCeNqLOB0CCUQKtDDbM3fMEzwqsXh88nJtdsZtjPg2gWfnrjS
         Ih4i48d7pMA7g9CJQCfDzRf6MRyti23LQ1fn374KOCcaVDquyWFP9S3yA8yAiDVOeZh0
         Nl7brG0O/uFD1bQ7Dj49B7IYSvWdKQ9DAKsfjWuRgOUo2Vv1nbKpELN9VJh7XiC7aHYC
         4yPA==
X-Received: by 10.50.80.45 with SMTP id o13mr2130693igx.7.1408758471706; Fri,
 22 Aug 2014 18:47:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.128.161 with HTTP; Fri, 22 Aug 2014 18:47:21 -0700 (PDT)
In-Reply-To: <CAMwrk0k8aKNmvpexE4uRYwbK7JzJ2YRCFm7FJDWCNRtqTaEoZA@mail.gmail.com>
References: <CALte62zTKBnKD0DqxD185cbWXr5G4NTdfb-33h+6Uyr2oM=mZA@mail.gmail.com>
 <CAMAsSd+hmBwXGTNv4-=AdbSe6ZK3YTup5yH8pYMOEmfwbSTNZg@mail.gmail.com>
 <53F7CD2E.3080902@cloudera.com> <CABPQxsuWOwrJ_2WjaXSaB0LNeFZyxLVXPfN7dSxN+ZYCrffhZQ@mail.gmail.com>
 <CAMwrk0k8aKNmvpexE4uRYwbK7JzJ2YRCFm7FJDWCNRtqTaEoZA@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Fri, 22 Aug 2014 18:47:21 -0700
Message-ID: <CAMwrk0kf=cAh2Z8Bcf870UYufE1Qrn9C7LPKnzr1zCc9YM7Qkg@mail.gmail.com>
Subject: Re: reference to dstream in package org.apache.spark.streaming which
 is not available
To: Patrick Wendell <pwendell@gmail.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Sean Owen <sowen@cloudera.com>, 
	Ted Yu <yuzhihong@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0153668a47cf1905014224e1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0153668a47cf1905014224e1
Content-Type: text/plain; charset=UTF-8

The real fix is that the spark sink suite does not really need to use to
the spark-streaming test jars. Removing that dependency altogether, and
submitting a PR.

TD


On Fri, Aug 22, 2014 at 6:34 PM, Tathagata Das <tathagata.das1565@gmail.com>
wrote:

> Figured it out. Fixing this ASAP.
>
> TD
>
>
> On Fri, Aug 22, 2014 at 5:51 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey All,
>>
>> We can sort this out ASAP. Many of the Spark committers were at a company
>> offsite for the last 72 hours, so sorry that it is broken.
>>
>> - Patrick
>>
>>
>> On Fri, Aug 22, 2014 at 4:07 PM, Hari Shreedharan <
>> hshreedharan@cloudera.com
>> > wrote:
>>
>> > Sean - I think only the ones in 1726 are enough. It is weird that any
>> > class that uses the test-jar actually requires the streaming jar to be
>> > added explicitly. Shouldn't maven take care of this?
>> >
>> > I posted some comments on the PR.
>> >
>> > --
>> >
>> > Thanks,
>> > Hari
>> >
>> >
>> >  Sean Owen <mailto:sowen@cloudera.com>
>> >> August 22, 2014 at 3:58 PM
>> >>
>> >> Yes, master hasn't compiled for me for a few days. It's fixed in:
>> >>
>> >> https://github.com/apache/spark/pull/1726
>> >> https://github.com/apache/spark/pull/2075
>> >>
>> >> Could a committer sort this out?
>> >>
>> >> Sean
>> >>
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >> Ted Yu <mailto:yuzhihong@gmail.com>
>> >> August 22, 2014 at 1:55 PM
>> >>
>> >> Hi,
>> >> Using the following command on (refreshed) master branch:
>> >> mvn clean package -DskipTests
>> >>
>> >> I got:
>> >>
>> >> constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
>> >> ---------------------------------------------------
>> >> java.lang.reflect.InvocationTargetException
>> >> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> >> at
>> >> sun.reflect.NativeMethodAccessorImpl.invoke(
>> >> NativeMethodAccessorImpl.java:57)
>> >> at
>> >> sun.reflect.DelegatingMethodAccessorImpl.invoke(
>> >> DelegatingMethodAccessorImpl.java:43)
>> >> at java.lang.reflect.Method.invoke(Method.java:606)
>> >> at
>> >> org.codehaus.plexus.classworlds.launcher.Launcher.
>> >> launchEnhanced(Launcher.java:289)
>> >> at
>> >> org.codehaus.plexus.classworlds.launcher.Launcher.
>> >> launch(Launcher.java:229)
>> >> at
>> >> org.codehaus.plexus.classworlds.launcher.Launcher.
>> >> mainWithExitCode(Launcher.java:415)
>> >> at org.codehaus.plexus.classworlds.launcher.Launcher.
>> >> main(Launcher.java:356)
>> >> Caused by: scala.reflect.internal.Types$TypeError: bad symbolic
>> >> reference.
>> >> A signature in TestSuiteBase.class refers to term dstream
>> >> in package org.apache.spark.streaming which is not available.
>> >> It may be completely missing from the current classpath, or the
>> version on
>> >> the classpath might be incompatible with the version used when
>> compiling
>> >> TestSuiteBase.class.
>> >> at
>> >> scala.reflect.internal.pickling.UnPickler$Scan.
>> >> toTypeError(UnPickler.scala:847)
>> >> at
>> >> scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(
>> >> UnPickler.scala:854)
>> >> at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
>> >> at
>> >>
>> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
>> >> Types.scala:4280)
>> >> at
>> >>
>> scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(
>> >> Types.scala:4280)
>> >> at
>> >> scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.
>> >> scala:70)
>> >> at scala.collection.immutable.List.forall(List.scala:84)
>> >> at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(
>> >> Types.scala:4280)
>> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
>> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
>> >> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
>> >> at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
>> >> at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
>> >> at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
>> >> at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
>> >> at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
>> >> at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
>> >> at
>> >> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
>> >> ExtractAPI.scala:296)
>> >> at
>> >> xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(
>> >> ExtractAPI.scala:296)
>> >> at
>> >> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
>> >> TraversableLike.scala:251)
>> >> at
>> >> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(
>> >> TraversableLike.scala:251)
>> >> at
>> >> scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.
>> >> scala:33)
>> >> at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
>> >> at scala.collection.TraversableLike$class.flatMap(
>> >> TraversableLike.scala:251)
>> >> at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
>> >> at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.
>> >> scala:296)
>> >> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
>> >> at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
>> >> at xsbt.Message$$anon$1.apply(Message.scala:8)
>> >> at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
>> >> at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
>> >> at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
>> >> at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
>> >> at
>> xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
>> >> at
>> xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
>> >> at scala.collection.immutable.List.foreach(List.scala:318)
>> >> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
>> >> at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
>> >> at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
>> >> at xsbt.API$ApiPhase.processUnit(API.scala:38)
>> >> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
>> >> at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
>> >> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>> >> at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>> >> at xsbt.API$ApiPhase.run(API.scala:34)
>> >> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
>> >> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
>> >> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
>> >> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
>> >> at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
>> >> at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
>> >> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
>> >> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> >> at
>> >> sun.reflect.NativeMethodAccessorImpl.invoke(
>> >> NativeMethodAccessorImpl.java:57)
>> >> at
>> >> sun.reflect.DelegatingMethodAccessorImpl.invoke(
>> >> DelegatingMethodAccessorImpl.java:43)
>> >> at java.lang.reflect.Method.invoke(Method.java:606)
>> >> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
>> >> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
>> >> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
>> >> at
>> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> >> apply$mcV$sp(AggressiveCompile.scala:99)
>> >> at
>> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> >> apply(AggressiveCompile.scala:99)
>> >> at
>> >> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.
>> >> apply(AggressiveCompile.scala:99)
>> >> at
>> >> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$
>> >> timed(AggressiveCompile.scala:166)
>> >> at
>> >> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(
>> >> AggressiveCompile.scala:98)
>> >> at
>> >>
>> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
>> >> 143)
>> >> at
>> >>
>> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:
>> >> 87)
>> >> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
>> >> Compile.scala:39)
>> >> at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(
>> >> Compile.scala:37)
>> >> at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
>> >> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
>> >> at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
>> >> at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
>> >> at sbt.inc.Incremental$.compile(Incremental.scala:37)
>> >> at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
>> >> at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
>> >> at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
>> >> at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
>> >> at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
>> >> at
>> sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
>> >> at
>> >> scala_maven.ScalaCompilerSupport.incrementalCompile(
>> >> ScalaCompilerSupport.java:303)
>> >> at scala_maven.ScalaCompilerSupport.compile(
>> >> ScalaCompilerSupport.java:119)
>> >> at scala_maven.ScalaCompilerSupport.doExecute(
>> >> ScalaCompilerSupport.java:99)
>> >> at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
>> >> at
>> scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
>> >> at
>> >> org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(
>> >> DefaultBuildPluginManager.java:106)
>> >> at
>> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> >> MojoExecutor.java:208)
>> >> at
>> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> >> MojoExecutor.java:153)
>> >> at
>> >> org.apache.maven.lifecycle.internal.MojoExecutor.execute(
>> >> MojoExecutor.java:145)
>> >> at
>> >>
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
>> >> LifecycleModuleBuilder.java:84)
>> >> at
>> >>
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(
>> >> LifecycleModuleBuilder.java:59)
>> >> at
>> >>
>> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(
>> >> LifecycleStarter.java:183)
>> >> at
>> >> org.apache.maven.lifecycle.internal.LifecycleStarter.
>> >> execute(LifecycleStarter.java:161)
>> >> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
>> >> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
>> >> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
>> >> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
>> >> at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
>> >> ... 8 more
>> >>
>> >> Has anyone seen similar error ?
>> >>
>> >> Cheers
>> >>
>> >>
>>
>
>

--089e0153668a47cf1905014224e1--

From dev-return-9007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 04:35:06 2014
Return-Path: <dev-return-9007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F7D811FDA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 04:35:06 +0000 (UTC)
Received: (qmail 99843 invoked by uid 500); 23 Aug 2014 04:35:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99770 invoked by uid 500); 23 Aug 2014 04:35:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99757 invoked by uid 99); 23 Aug 2014 04:35:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 04:35:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of maisnam.ns@gmail.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 04:34:39 +0000
Received: by mail-qc0-f171.google.com with SMTP id r5so11931310qcx.30
        for <dev@spark.apache.org>; Fri, 22 Aug 2014 21:34:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DHHEuG4DTgRPxz1UL/+bI9ahEOF9rK5Oy4CzRuFsS8w=;
        b=aMeobm9TDRr0u4HcD2RNY2cMgdiOPTd5LXiQudGYMUGGByVhyNim6CMViNHIuYGlmE
         3KeS0U+F6GQJwUE1hViRoL33WE4F8CxYCLpCHJxJDnt0MrCJuTdeurdJC7vu10ACXqla
         vGPN8p69pZtyXFoq9mwrJhd/odZuQ8QILkQ47YWlgnnyYxWEYogHmk+HvktMMAJQbJGl
         fEtlrnUEkn9q65TJnKGOCRJyfVNul3RKhcFwIVdHEY/7M/W3jFRGbVcScN1pkWTO9rC7
         raqGs0iypjd2atuUOCc48qpw6zjbA8HhReLDVtR6jzcXlzEJ4dPmk+VRO1E52+Zm1rAI
         rfWw==
MIME-Version: 1.0
X-Received: by 10.140.107.198 with SMTP id h64mr13200680qgf.42.1408768478640;
 Fri, 22 Aug 2014 21:34:38 -0700 (PDT)
Received: by 10.140.94.214 with HTTP; Fri, 22 Aug 2014 21:34:38 -0700 (PDT)
In-Reply-To: <CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
	<CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
	<CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
	<CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
Date: Sat, 23 Aug 2014 10:04:38 +0530
Message-ID: <CAON7oqQm+npzdRXHgsse99ky9EF_GrO2f__02Md-+mihXimN-g@mail.gmail.com>
Subject: Re: Spark Contribution
From: Maisnam Ns <maisnam.ns@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Henry Saputra <henry.saputra@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a3936bd802d0501447811
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a3936bd802d0501447811
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks all, for adding this link .


On Sat, Aug 23, 2014 at 5:38 AM, Reynold Xin <rxin@databricks.com> wrote:

> Great idea. Added the link
> https://github.com/apache/spark/blob/master/README.md
>
>
>
> On Thu, Aug 21, 2014 at 4:06 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> We should add this link to the readme on GitHub btw.
>>
>> 2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Henry =
Saputra<henry.saputra@gmail.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=
=9C =EB=A9=94=EC=8B=9C=EC=A7=80:
>>
>> > The Apache Spark wiki on how to contribute should be great place to
>> > start:
>> > https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spar=
k
>> >
>> > - Henry
>> >
>> > On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
>> > <javascript:;>> wrote:
>> > > Hi,
>> > >
>> > > Can someone help me with some links on how to contribute for Spark
>> > >
>> > > Regards
>> > > mns
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;=
>
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> <javascript:;>
>> >
>> >
>>
>
>

--001a113a3936bd802d0501447811--

From dev-return-9008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 06:43:27 2014
Return-Path: <dev-return-9008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43FBE111B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 06:43:27 +0000 (UTC)
Received: (qmail 20621 invoked by uid 500); 23 Aug 2014 06:43:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20545 invoked by uid 500); 23 Aug 2014 06:43:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20534 invoked by uid 99); 23 Aug 2014 06:43:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 06:43:26 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 06:43:21 +0000
Received: by mail-qg0-f44.google.com with SMTP id e89so11424647qgf.3
        for <dev@spark.incubator.apache.org>; Fri, 22 Aug 2014 23:43:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:message-id:references:to;
        bh=DCVoTmX7CPu5s5wmoYVckCXuXCmyK7gmIbCAghJmiN8=;
        b=h8Ci9OE1nwvWMrMYlHlP3mdP33NlPnIrekqQy2ZURVsuLno/E46LeuMAnLrzO9ihIY
         9cYU6ZZQCFPy4iwViKQqjyTxougMlh4f4VE4xU96A5HhO/wiJq/hLqj62vTUa+YdhNOi
         stHZpV88br+XGA/9372tcGXwDsV8aoJyx+eKXeJgfmz54FS7tFig9s09tNeP+0BZnLxS
         cSiLeOSScMjTx3yr3b7Ho+RRpUXfrtMA8MTCq+Xi/D7YOHY1VzjNFBANTMbeQJKRUoDa
         8M16IH1je8io8Ylq8O0uD5+TU2XmUAqq5R90oYu9NGel8scYwrZQG07R0yMqBmvtZSGD
         VWEw==
X-Gm-Message-State: ALoCoQligNoZuAImEjkMPD6lk150ZnrGsOW9JkncWVMt6P+0ryCYxIWR1RfO4TWpotVimtP85MXs
X-Received: by 10.140.101.142 with SMTP id u14mr13919362qge.48.1408776180730;
        Fri, 22 Aug 2014 23:43:00 -0700 (PDT)
Received: from jeffreycardsmbp.home (pool-72-69-16-49.nycmny.fios.verizon.net. [72.69.16.49])
        by mx.google.com with ESMTPSA id v2sm38464803qge.7.2014.08.22.23.42.59
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 22 Aug 2014 23:42:59 -0700 (PDT)
Content-Type: multipart/signed; boundary="Apple-Mail=_9C1D8DC6-5C5C-4FFE-BBB5-21BFC160DC94"; protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Graphx seems to be broken while Creating a large graph(6B nodes in my case)
From: Jeffrey Picard <jpicard@placeiq.com>
In-Reply-To: <1408754619472-7966.post@n3.nabble.com>
Date: Sat, 23 Aug 2014 02:42:55 -0400
Cc: dev@spark.incubator.apache.org
Message-Id: <F40C94BD-7BA9-428C-8BAE-721E9A7E2AEA@placeiq.com>
References: <1408754619472-7966.post@n3.nabble.com>
To: npanj <nitinpanj@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_9C1D8DC6-5C5C-4FFE-BBB5-21BFC160DC94
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

I=92m seeing this issue also. I have graph with with 5828339535 vertices =
and 7398447992 edges, graph.numVertices returns 1533266498 and =
graph.numEdges is correct and returns 7398447992. I also am having an =
issue that I=92m beginning to suspect is caused by the same underlying =
problem where connected components stops after one iteration, returning =
an incorrect graph.
On Aug 22, 2014, at 8:43 PM, npanj <nitinpanj@gmail.com> wrote:

> While creating a graph with 6B nodes and 12B edges, I noticed that
> *'numVertices' api returns incorrect result*; 'numEdges' reports =
correct
> number. For few times(with different dataset > 2.5B nodes) I have also
> notices that numVertices is returned as -ive number; so I suspect that =
there
> is some overflow (may be we are using Int for some field?).
>=20
> Environment: Standalone mode running on EC2 . Using latest code from =
master
> branch upto commit #db56f2df1b8027171da1b8d2571d1f2ef1e103b6 .
>=20
> Here is some details of experiments I have done so far:=20
> 1. Input: numNodes=3D6101995593 ; noEdges=3D12163784626
> Graph returns: numVertices=3D1807028297 ; numEdges=3D12163784626
> 2. Input : numNodes=3D*2157586441* ; noEdges=3D2747322705
> Graph Returns: numVertices=3D*-2137380855* ; numEdges=3D2747322705
> 3. Input: numNodes=3D1725060105 ; noEdges=3D204176821
> Graph: numVertices=3D1725060105 ; numEdges=3D2041768213=20
>=20
>=20
> You can find the code to generate this bug here:
> https://gist.github.com/npanj/92e949d86d08715bf4bf
>=20
> (I have also filed this jira ticket:
> https://issues.apache.org/jira/browse/SPARK-3190)
>=20
>=20
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-seems-to-=
be-broken-while-Creating-a-large-graph-6B-nodes-in-my-case-tp7966.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


--Apple-Mail=_9C1D8DC6-5C5C-4FFE-BBB5-21BFC160DC94
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJT+DfvAAoJEC0tCmi2+GvEeVkP/2rDHKvnp/XGhp2vgLfoKUV0
e/1ebytdt1PfNPLLceaVyfOiegpsKo7TgSjThMm0heBGU8/ATpxiDvs+5FTDOqfu
JIbvYCA4TVzNHu2ZawPffKKRIUdIUCmEo+jheWDWaNAEUG4PQWSghPWunOm0GrAA
d5YxZY8MIGx4jt3yAOQra767JDRUij236wUU98NkyNJNNwPVaQMgdBtDwmrnKeQ5
reqYWRGuXEglujiBRk4z80YSjduZw/vNd1O+PhPY24lvwjBQdsf3Thl47ZXQqKtv
Q3wmmiv8HEhteVIPB69siQWwpheWnC9vZ3up+kEwzncThhrh4YRrySXjK+L3jXrd
e0yNRN8dAckgi5qze6e1XIzrMEAuAtRtf+1ri1R+DSHBnml5yG1DrklexAQ/9BMX
DtY6zdhYYX51mHsmgPcRDxBzmAwpJpTCoKWYVCYqNhsEDXjBSmBgbIgwykkuAb9v
G7cRVB7cGuC3Bu1itz6n8f2eiaFeQ8vFHqIT1y3Eq5u1UzGi3WOcLesOWtl8rrXW
WfJddNN2TuRVHsaH6vhr9oURriJ/iIKIWshTByuxz0Cu3+4TIsF+zedd6Vk05hKp
AokCCRUDrjFYItYeR6sL08y5i/qSaJHuN0nowzuAs3SIx4SsHWQuEaBMeZ/z6vLw
wYY5Wl6ulzG3cwNdsDB5
=p9Xa
-----END PGP SIGNATURE-----

--Apple-Mail=_9C1D8DC6-5C5C-4FFE-BBB5-21BFC160DC94--

From dev-return-9009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 09:11:37 2014
Return-Path: <dev-return-9009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C828B1141C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 09:11:37 +0000 (UTC)
Received: (qmail 63844 invoked by uid 500); 23 Aug 2014 09:11:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63796 invoked by uid 500); 23 Aug 2014 09:11:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63779 invoked by uid 99); 23 Aug 2014 09:11:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 09:11:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.181 as permitted sender)
Received: from [209.85.213.181] (HELO mail-ig0-f181.google.com) (209.85.213.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 09:11:11 +0000
Received: by mail-ig0-f181.google.com with SMTP id h3so775959igd.2
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 02:11:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=elwtMVgQ5bj1bUXzvsLLDDZHcEW3JBgCCdmgl3OBAQ8=;
        b=bhKPftR44t184BRmwoWgzSbBIdeI9SBZJjDV4WwvYvuZbHGk4n7GLbx6e9EgN4/EFd
         6NxmRU4Xs2+aCvDTOuaK7+o1fbOsCzqb9xFcj425OThlkHhSd5SkhcjlnyANW9n4GN3k
         Jkk1BrLdtgOW0bRcNaNWTGP9Gfs8+8BeqrpATGkSr9XBBdD8I87YWDtJ2xuDVo1Hin35
         QFZtyNigakLVW8eOtEcsEKgS+uBPDU549eNkSom7L1S1NkvI7sGsif+5PtuY8WxMi6yN
         1uYjmCVTcOXmJSJHmF5J4MPMHJqqvcHMQ38X1T89GNyyNWOR+J3UjTRKKCUrjTp5h60/
         Gtag==
X-Gm-Message-State: ALoCoQk9f09K/QqJPp/HaGuQ1FOttr4XRqT2RbDG0/5D8tsVK9pWcRz7AWicCqu4mKnszaCYQ4N9
X-Received: by 10.42.61.146 with SMTP id u18mr13366333ich.1.1408785070427;
 Sat, 23 Aug 2014 02:11:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Sat, 23 Aug 2014 02:10:50 -0700 (PDT)
In-Reply-To: <CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
 <CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
 <CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com> <CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 23 Aug 2014 10:10:50 +0100
Message-ID: <CAMAsSdJRP8-tjuaqdZtNVk5dWgsyyBL6=CK_t5LEp1UwABPkGA@mail.gmail.com>
Subject: Re: Spark Contribution
To: Reynold Xin <rxin@databricks.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Henry Saputra <henry.saputra@gmail.com>, 
	Maisnam Ns <maisnam.ns@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Can I ask a related question, since I have a PR open to touch up
README.md as we speak (SPARK-3069)?

If this text is in a file called CONTRIBUTING.md, then it will cause a
link to appear on the pull request screen, inviting people to review
the contribution guidelines:

https://github.com/blog/1184-contributing-guidelines

This is mildly important as the project wants to make it clear that
you agree that your contribution is licensed under the AL2, since
there is no formal ICLA.

How about I propose moving the text to CONTRIBUTING.md with a pointer
in README.md? or keep it both places?

On Sat, Aug 23, 2014 at 1:08 AM, Reynold Xin <rxin@databricks.com> wrote:
> Great idea. Added the link
> https://github.com/apache/spark/blob/master/README.md
>
>
>
> On Thu, Aug 21, 2014 at 4:06 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> We should add this link to the readme on GitHub btw.
>>
>> 2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Henry =
Saputra<henry.saputra@gmail.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=
=9C =EB=A9=94=EC=8B=9C=EC=A7=80:
>>
>> > The Apache Spark wiki on how to contribute should be great place to
>> > start:
>> > https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spar=
k
>> >
>> > - Henry
>> >
>> > On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
>> > <javascript:;>> wrote:
>> > > Hi,
>> > >
>> > > Can someone help me with some links on how to contribute for Spark
>> > >
>> > > Regards
>> > > mns
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;=
>
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> <javascript:;>
>> >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 14:19:03 2014
Return-Path: <dev-return-9010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2761E11829
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 14:19:03 +0000 (UTC)
Received: (qmail 70587 invoked by uid 500); 23 Aug 2014 14:19:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70518 invoked by uid 500); 23 Aug 2014 14:19:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70505 invoked by uid 99); 23 Aug 2014 14:19:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 14:19:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 14:18:36 +0000
Received: by mail-wi0-f170.google.com with SMTP id f8so2028342wiw.1
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 07:18:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=57UxWmYQdVQtS007bZVNZexksE7NyHb3VTQVYpwJGOk=;
        b=f3c9uCy3+8omvTtwtb4V7/jpeHHccsiA4UcR6UkEqI3yJJQeczrwLnBEDWcz4m+F2j
         YYgK/sLunvunMOiKxAzKyy84KMbPZ55NZ2n37bmqMPQWLATjDDFnLZjkold+dOZOcsKm
         /PgGXNGJq/FhEzlP5Hf/PrVHbrEPm+w2Dwj+23zEiJWz83366tyUYBXC21XPea+i7Kzh
         ipB7awELfzB30chsAr6l0DFWf8WcG1OAG0iGU7yPqh5LxsCczHVGDvuYtX4FprIqUmGz
         WGbZ38dXfLTmYnMWVi1KyNDh99GhUP+nbm5HVm+ql9UU0qBoVA6dH8aBTCTVfMJNWrdo
         zAXw==
MIME-Version: 1.0
X-Received: by 10.180.21.235 with SMTP id y11mr4140098wie.75.1408803516107;
 Sat, 23 Aug 2014 07:18:36 -0700 (PDT)
Received: by 10.180.92.232 with HTTP; Sat, 23 Aug 2014 07:18:36 -0700 (PDT)
In-Reply-To: <CAMAsSdJRP8-tjuaqdZtNVk5dWgsyyBL6=CK_t5LEp1UwABPkGA@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
	<CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
	<CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
	<CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
	<CAMAsSdJRP8-tjuaqdZtNVk5dWgsyyBL6=CK_t5LEp1UwABPkGA@mail.gmail.com>
Date: Sat, 23 Aug 2014 10:18:36 -0400
Message-ID: <CAOhmDzfTMkRG_RK08yP==tTTmJL+4BHQEVJ6FuRKjeu2cHOCjw@mail.gmail.com>
Subject: Re: Spark Contribution
From: Nicholas Chammas <nicholas.chammas@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Reynold Xin <rxin@databricks.com>, Henry Saputra <henry.saputra@gmail.com>, 
	Maisnam Ns <maisnam.ns@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e0422d12a05014ca19f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e0422d12a05014ca19f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That sounds like a good idea.

Continuing along those lines, what do people think of moving the
contributing page entirely from the wiki to GitHub? It feels like the right
place for it since GitHub is where we take contributions, and it also lets
people make improvements to it.

Nick


2014=EB=85=84 8=EC=9B=94 23=EC=9D=BC =ED=86=A0=EC=9A=94=EC=9D=BC, Sean Owen=
<sowen@cloudera.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=9C =EB=A9=
=94=EC=8B=9C=EC=A7=80:

> Can I ask a related question, since I have a PR open to touch up
> README.md as we speak (SPARK-3069)?
>
> If this text is in a file called CONTRIBUTING.md, then it will cause a
> link to appear on the pull request screen, inviting people to review
> the contribution guidelines:
>
> https://github.com/blog/1184-contributing-guidelines
>
> This is mildly important as the project wants to make it clear that
> you agree that your contribution is licensed under the AL2, since
> there is no formal ICLA.
>
> How about I propose moving the text to CONTRIBUTING.md with a pointer
> in README.md? or keep it both places?
>
> On Sat, Aug 23, 2014 at 1:08 AM, Reynold Xin <rxin@databricks.com
> <javascript:;>> wrote:
> > Great idea. Added the link
> > https://github.com/apache/spark/blob/master/README.md
> >
> >
> >
> > On Thu, Aug 21, 2014 at 4:06 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com <javascript:;>> wrote:
> >
> >> We should add this link to the readme on GitHub btw.
> >>
> >> 2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Henr=
y Saputra<henry.saputra@gmail.com <javascript:;>>=EB=8B=98=EC=9D=B4
> =EC=9E=91=EC=84=B1=ED=95=9C =EB=A9=94=EC=8B=9C=EC=A7=80:
> >>
> >> > The Apache Spark wiki on how to contribute should be great place to
> >> > start:
> >> >
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> >> >
> >> > - Henry
> >> >
> >> > On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
> <javascript:;>
> >> > <javascript:;>> wrote:
> >> > > Hi,
> >> > >
> >> > > Can someone help me with some links on how to contribute for Spark
> >> > >
> >> > > Regards
> >> > > mns
> >> >
> >> > --------------------------------------------------------------------=
-
> >> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> <javascript:;> <javascript:;>
> >> > For additional commands, e-mail: dev-help@spark.apache.org
> <javascript:;>
> >> <javascript:;>
> >> >
> >> >
> >>
>

--047d7b874e0422d12a05014ca19f--

From dev-return-9011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 14:57:34 2014
Return-Path: <dev-return-9011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF4E61188F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 14:57:34 +0000 (UTC)
Received: (qmail 99436 invoked by uid 500); 23 Aug 2014 14:57:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99358 invoked by uid 500); 23 Aug 2014 14:57:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99344 invoked by uid 99); 23 Aug 2014 14:57:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 14:57:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 14:57:02 +0000
Received: by mail-qa0-f44.google.com with SMTP id f12so10609474qad.17
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 07:57:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=BPaalUrpLZSoMdnJ7dNMAciDB0AC5PmEjblRDjOWYLs=;
        b=bO7TPoGngxOeZq6kSyOtl9gID+okSAEboNaBTEfGeLmC3Kx7v7151dvWrqKXQbGLny
         DRE3kBQ0reGOBaHaFH0e3Foet9U9aKc6lVETH6fINOYVENAd9lgQLBB8njbtU1LD7Jsf
         /3+t9XQ358wgMzLJn4sqERHCxiqpnUZ0zyKiBS1v+PTSLoXMvcQZaKDa2UDg+nYSLbR5
         1PpTuGQa/N5pKXZlDX1eNWD4ipstoMWDBXIQUjgC/D3shWoJ4IhVvKy2nAntMxsxxMG/
         3qPSCuLNElR/qsVW0i3AMuAXMYIgND+xcXha0zYCIdo4QQBKdoeaCAOYA/7moSenGZAH
         zWVw==
MIME-Version: 1.0
X-Received: by 10.140.25.11 with SMTP id 11mr17327295qgs.9.1408805821291; Sat,
 23 Aug 2014 07:57:01 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Sat, 23 Aug 2014 07:57:01 -0700 (PDT)
Date: Sat, 23 Aug 2014 10:57:01 -0400
Message-ID: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
Subject: Mesos/Spark Deadlock
From: Gary Malouf <malouf.gary@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, dev@mesos.apache.org
Content-Type: multipart/alternative; boundary=001a11c02a8e8921a905014d2a6d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c02a8e8921a905014d2a6d
Content-Type: text/plain; charset=UTF-8

I just wanted to bring up a significant Mesos/Spark issue that makes the
combo difficult to use for teams larger than 4-5 people.  It's covered in
https://issues.apache.org/jira/browse/MESOS-1688.  My understanding is that
Spark's use of executors in fine-grained mode is a very different behavior
than many of the other common frameworks for Mesos.

--001a11c02a8e8921a905014d2a6d--

From dev-return-9012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 17:22:40 2014
Return-Path: <dev-return-9012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1ED2A11AFE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 17:22:40 +0000 (UTC)
Received: (qmail 48704 invoked by uid 500); 23 Aug 2014 17:22:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48629 invoked by uid 500); 23 Aug 2014 17:22:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48615 invoked by uid 99); 23 Aug 2014 17:22:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 17:22:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of maisnam.ns@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 17:22:13 +0000
Received: by mail-qc0-f176.google.com with SMTP id m20so12285581qcx.21
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 10:22:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Gg3QGjES8NwyH8R4oK1/THUzH0HT7M0I/lloRBO3Hv0=;
        b=eet7s8X2Q+fSgkpfo6fUJh5Jg27YZaBqIInlRVYlUvbtDbWy2lqgOpQaD5lHEKt09J
         BnOq+ER8cVDea9JLs4YPImqMPtQBFz7m4U19RRyjhkY1FJ9Vz3VErCbHPC+YUfCxVGK8
         2Os4M4uq6OpKnxTenX1HEAu6HY1oT8dYblh01Xh6He9zibgKNJBOhdknTrb1VFyGPOfg
         ghTjOfSNiRQe1SazCW2Nab2d0QWLmcGP5OqnDqiySTQMibJg3ScvulICSrc51fPdKid6
         uVCiXcvaZJV4MlWv9UugT08KADtrexCqORhGs5rYbSUJw+2BD8wLjg270no4JIST0seZ
         wRyg==
MIME-Version: 1.0
X-Received: by 10.224.66.7 with SMTP id l7mr19541578qai.46.1408814532051; Sat,
 23 Aug 2014 10:22:12 -0700 (PDT)
Received: by 10.140.94.214 with HTTP; Sat, 23 Aug 2014 10:22:11 -0700 (PDT)
In-Reply-To: <CAOhmDzfTMkRG_RK08yP==tTTmJL+4BHQEVJ6FuRKjeu2cHOCjw@mail.gmail.com>
References: <CAON7oqRpttHsCiWAEAC07iEdoDcNNQZ9AV1Lzt+Hm2Sh40fQQw@mail.gmail.com>
	<CALuGr6Yh0cZRiTK+ftb=FFH81=tkvPzFJ1PBRi+gudXBiGrEOQ@mail.gmail.com>
	<CAOhmDzeG4jGBK4v_Z-AbqrND=PdpjP-tEeFWBvbsrCrqx_gzaQ@mail.gmail.com>
	<CAPh_B=aHgN=ZRPDPU8DA_OKHV=rzWr64wUHs+96oiasQGKm-yg@mail.gmail.com>
	<CAMAsSdJRP8-tjuaqdZtNVk5dWgsyyBL6=CK_t5LEp1UwABPkGA@mail.gmail.com>
	<CAOhmDzfTMkRG_RK08yP==tTTmJL+4BHQEVJ6FuRKjeu2cHOCjw@mail.gmail.com>
Date: Sat, 23 Aug 2014 22:52:11 +0530
Message-ID: <CAON7oqSS7HOWgY_mq=n4OTmYf9EUJ7pBaRn19zsEidkyPq4WHA@mail.gmail.com>
Subject: Re: Spark Contribution
From: Maisnam Ns <maisnam.ns@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>, 
	Henry Saputra <henry.saputra@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c22b02bcc93005014f3186
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c22b02bcc93005014f3186
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sure, it's really a good idea to have a CONTRIBUTING.md file with details
on how to contribute e.g cloning,branching,changes , commit  along with
corresponding git commands. That way someone who wants to contribute will
surely get the benefit of a quick short documentation on contribution.

Maisnam


On Sat, Aug 23, 2014 at 7:48 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> That sounds like a good idea.
>
> Continuing along those lines, what do people think of moving the
> contributing page entirely from the wiki to GitHub? It feels like the rig=
ht
> place for it since GitHub is where we take contributions, and it also let=
s
> people make improvements to it.
>
> Nick
>
>
> 2014=EB=85=84 8=EC=9B=94 23=EC=9D=BC =ED=86=A0=EC=9A=94=EC=9D=BC, Sean Ow=
en<sowen@cloudera.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=9C =EB=A9=
=94=EC=8B=9C=EC=A7=80:
>
> Can I ask a related question, since I have a PR open to touch up
>> README.md as we speak (SPARK-3069)?
>>
>> If this text is in a file called CONTRIBUTING.md, then it will cause a
>> link to appear on the pull request screen, inviting people to review
>> the contribution guidelines:
>>
>> https://github.com/blog/1184-contributing-guidelines
>>
>> This is mildly important as the project wants to make it clear that
>> you agree that your contribution is licensed under the AL2, since
>> there is no formal ICLA.
>>
>> How about I propose moving the text to CONTRIBUTING.md with a pointer
>> in README.md? or keep it both places?
>>
>> On Sat, Aug 23, 2014 at 1:08 AM, Reynold Xin <rxin@databricks.com> wrote=
:
>> > Great idea. Added the link
>> > https://github.com/apache/spark/blob/master/README.md
>> >
>> >
>> >
>> > On Thu, Aug 21, 2014 at 4:06 PM, Nicholas Chammas <
>> > nicholas.chammas@gmail.com> wrote:
>> >
>> >> We should add this link to the readme on GitHub btw.
>> >>
>> >> 2014=EB=85=84 8=EC=9B=94 21=EC=9D=BC =EB=AA=A9=EC=9A=94=EC=9D=BC, Hen=
ry Saputra<henry.saputra@gmail.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=
=95=9C =EB=A9=94=EC=8B=9C=EC=A7=80:
>> >>
>> >> > The Apache Spark wiki on how to contribute should be great place to
>> >> > start:
>> >> >
>> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
>> >> >
>> >> > - Henry
>> >> >
>> >> > On Thu, Aug 21, 2014 at 3:25 AM, Maisnam Ns <maisnam.ns@gmail.com
>> >> > <javascript:;>> wrote:
>> >> > > Hi,
>> >> > >
>> >> > > Can someone help me with some links on how to contribute for Spar=
k
>> >> > >
>> >> > > Regards
>> >> > > mns
>> >> >
>> >> > -------------------------------------------------------------------=
--
>> >> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> <javascript:;>
>> >> > For additional commands, e-mail: dev-help@spark.apache.org
>> >> <javascript:;>
>> >> >
>> >> >
>> >>
>>
>

--001a11c22b02bcc93005014f3186--

From dev-return-9013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 23:16:40 2014
Return-Path: <dev-return-9013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19D7C112CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 23:16:39 +0000 (UTC)
Received: (qmail 78700 invoked by uid 500); 23 Aug 2014 23:16:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78655 invoked by uid 500); 23 Aug 2014 23:16:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78643 invoked by uid 99); 23 Aug 2014 23:16:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 23:16:37 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.171 as permitted sender)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 23:16:31 +0000
Received: by mail-pd0-f171.google.com with SMTP id z10so17939348pdj.16
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 16:16:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=J1FnYkeeYmdls/hIVHBqM+dIiBZt8NN63DzBeuTzFD4=;
        b=qTiNX4D/+p278TWSw7/JeJkOXC6Nv7VO/WEEtyPplZh//zWju2n6792x1E+yShnsFe
         cYENBgEz4nU+Bku0yNJ/xTc40Z0nPkYKyMIsfXGJWJ+LQeFJRUwuc7HoljncVUnV6XBK
         /pRLgaYsjNYcdi0zgJEaDYi8P9X3X9SmgWjv5JOOb0FxMQ132fzPuu1MHs9vBkh4V2VB
         y0Zl2CaZsOVYYu7JfkSck8U+ef4f/aaQqJ6lmpUU1BKvwjAtqMNeYoMwlS4LhYCKlkNB
         9EPMlS+WlI2HdXIy/sCJaxWj//C6JsXc/+HePQKwwE4zJq696mNmoQ6fTF6E+exgwfDG
         9iQw==
X-Received: by 10.70.47.197 with SMTP id f5mr36857pdn.141.1408835770676;
        Sat, 23 Aug 2014 16:16:10 -0700 (PDT)
Received: from mbp-3.local (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id ff1sm32820659pbd.93.2014.08.23.16.16.09
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 23 Aug 2014 16:16:10 -0700 (PDT)
Date: Sat, 23 Aug 2014 16:16:08 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, Gary
 Malouf <malouf.gary@gmail.com>, dev@mesos.apache.org
Message-ID: <etPan.53f920b8.ded7263.ae21@mbp-3.local>
In-Reply-To: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53f920b8_7fdcc233_ae21"
X-Virus-Checked: Checked by ClamAV on apache.org

--53f920b8_7fdcc233_ae21
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hey Gary, just as a workaround, note that you can use Mesos in coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold onto CPUs for the duration of the job.

Matei

On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com) wrote:

I just wanted to bring up a significant Mesos/Spark issue that makes the 
combo difficult to use for teams larger than 4-5 people. It's covered in 
https://issues.apache.org/jira/browse/MESOS-1688. My understanding is that 
Spark's use of executors in fine-grained mode is a very different behavior 
than many of the other common frameworks for Mesos. 

--53f920b8_7fdcc233_ae21--


From dev-return-9014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 23 23:31:02 2014
Return-Path: <dev-return-9014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F8D4112E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Aug 2014 23:31:02 +0000 (UTC)
Received: (qmail 90412 invoked by uid 500); 23 Aug 2014 23:30:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90313 invoked by uid 500); 23 Aug 2014 23:30:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89931 invoked by uid 99); 23 Aug 2014 23:30:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 23:30:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Aug 2014 23:30:28 +0000
Received: by mail-qc0-f172.google.com with SMTP id i8so12273656qcq.17
        for <dev@spark.apache.org>; Sat, 23 Aug 2014 16:30:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0XIELhcdIWbwg2op47JS7ZGccpO3CTEIFEX4dBiUInY=;
        b=1F5lXfMhOC0pIBFKnh9GBMVnhLbtRuN5qFcl+w/HInTciHe3P7Z9p3wowtgwbkE/p5
         0c5mrspdFALdnXGosL5XKwBa29njCx5lnPk9VWnbT71l2D5vSa/DbMgj52nnqhJyeb8Z
         Sl2JBeN9n4lxFNsO2I5uJ5oUU4VB8swbAol32My9MZ5XbanI7j7YowEHY8eoPt7l0Vch
         nXWqYxUALXN1piMblycbSl4KTKDoFl2SybNLwRWTCIcOYjcypu7dfW57gQyT72stIoo5
         T8eWqs1k1Awy5quce+zXbXs1rmji9PID2OgNNYHsIEweJ9feE/ajKMhgOXnQi88/SQYt
         uusw==
MIME-Version: 1.0
X-Received: by 10.140.43.245 with SMTP id e108mr19880944qga.76.1408836627849;
 Sat, 23 Aug 2014 16:30:27 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Sat, 23 Aug 2014 16:30:27 -0700 (PDT)
In-Reply-To: <etPan.53f920b8.ded7263.ae21@mbp-3.local>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
Date: Sat, 23 Aug 2014 19:30:27 -0400
Message-ID: <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Gary Malouf <malouf.gary@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, dev@mesos.apache.org
Content-Type: multipart/alternative; boundary=001a113a6664bfe8b40501545622
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a6664bfe8b40501545622
Content-Type: text/plain; charset=UTF-8

Hi Matei,

We have an analytics team that uses the cluster on a daily basis.  They use
two types of 'run modes':

1) For running actual queries, they set the spark.executor.memory to
something between 4 and 8GB of RAM/worker.

2) A shell that takes a minimal amount of memory on workers (128MB) for
prototyping out a larger query.  This allows them to not take up RAM on the
cluster when they do not really need it.

We see the deadlocks when there are a few shells in either case.  From the
usage patterns we have, coarse-grained mode would be a challenge as we have
to constantly remind people to kill their shells as soon as their queries
finish.

Am I correct in viewing Mesos in coarse-grained mode as being similar to
Spark Standalone's cpu allocation behavior?




On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Hey Gary, just as a workaround, note that you can use Mesos in
> coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold
> onto CPUs for the duration of the job.
>
> Matei
>
> On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com)
> wrote:
>
> I just wanted to bring up a significant Mesos/Spark issue that makes the
> combo difficult to use for teams larger than 4-5 people. It's covered in
> https://issues.apache.org/jira/browse/MESOS-1688. My understanding is
> that
> Spark's use of executors in fine-grained mode is a very different behavior
> than many of the other common frameworks for Mesos.
>
>

--001a113a6664bfe8b40501545622--

From dev-return-9015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 24 23:13:08 2014
Return-Path: <dev-return-9015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CC7C116B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 24 Aug 2014 23:13:08 +0000 (UTC)
Received: (qmail 4911 invoked by uid 500); 24 Aug 2014 23:13:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4834 invoked by uid 500); 24 Aug 2014 23:13:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4815 invoked by uid 99); 24 Aug 2014 23:13:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:13:07 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.126 as permitted sender)
Received: from [98.139.213.126] (HELO nm30-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:12:38 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=C5l0aJes/aCJobUhoMm6HPlDSuY+fNToJHWg/JCSqT8taFtVy2fODY+uok3+3i5COzjR2gKVXruO884b6ZaJULUIN6BB1VBwPkAru9l85hz03R7ugsFpuCg/Hj/gAQ5uiiY1kLp5VDVU0NZKqZXMz7gwZnzigYtEwf/3TfuW7w6Y38lZaQ+2t3vlf69HDkcU0g2npNm5zeAas8A33xT+MPYdNvhWQAJZgxVY8SwVAP92tS+u7/E8W6hEdQi/aos+S6rUmdt6D7rLCiAwoGkX4qOGjXvlNQZ7qUvOaMGbHBNMX9SmGRBsC4inSFxiq7KqLP27Wde5KS2p2zdSO2kSsA==;
Received: from [66.196.81.170] by nm30.bullet.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:12:36 -0000
Received: from [98.139.211.200] by tm16.bullet.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:12:36 -0000
Received: from [127.0.0.1] by smtp209.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:12:36 -0000
X-Yahoo-Newman-Id: 129228.37387.bm@smtp209.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: 6sRICBAVM1mgyj.yjuwtw.xjiA2Ec3S51kApxYvI8PojDis
 _xIA7bSeX53z4Wsl0Giw9bX572CAA83akCoralkN4l7IrXXB96mHHkofK1hG
 SwAfF6JtnDbDmykCx0Gs2SnbavSvrDu2yif8uXanOnDsQmHEGlJliEm3pLmu
 1f7Ptg.2d_38vJkFdsXHOTuUS5FdyWniJne7dcLL7fUPNaIF2sBSwk5Xpe0j
 fyanX7Xz7ZmzdQLSi8bYTYmnnP.0xbTuaujH7v9mCe2BxKKvV.yVVG_NXFcL
 .2l4ffbo1c3AmSRytFYMoV85SqfZrKIkp8sQwuEa.GW55Cf0.Wf.mbYgRkoy
 UAr9OzQFHz1tdt1g4FE9CZs6hjp.SouUpKhl53h1Nam6CJh67yX35IlOURjt
 HV6nwgHF5PpGPFb_tKjknyFWjs0e3B25ZnkoaINLKAxl8KmuqBLntOjpJJ5H
 JH5Hu4MV.x6JXwPJEDZwg9sRN_eY0FfDNnVyyP7TeuiRWH4pYhr6ReK85HPs
 Dz3RB_VkOIS5liExY5vXZC3m.D8znNQ--
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Message-ID: <53FA7168.3000300@yahoo.com>
Date: Sun, 24 Aug 2014 16:12:40 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Problems running examples in IDEA
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,
   After getting the code base to compile, I tried running some of the 
scala examples.
   They all fail since it can't find classes like SparkConf.
   If I change the iml file to convert provided scope from PROVIDED to 
COMPILE, I am able to run them. It's simple by doing the following in 
the root directory of the spark code base: find . -name "*.iml" | xargs 
sed -i.bak 's/PROVIDED/COMPILE/g'.
   Is this expected? I'd really rather not modify the iml files since 
they were sourced from the pom xml files, so if you guys have some tips 
on doing this better, that would be great...

Thanks,
Ron

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 24 23:22:33 2014
Return-Path: <dev-return-9016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 877AD116E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 24 Aug 2014 23:22:33 +0000 (UTC)
Received: (qmail 15706 invoked by uid 500); 24 Aug 2014 23:22:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15641 invoked by uid 500); 24 Aug 2014 23:22:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15630 invoked by uid 99); 24 Aug 2014 23:22:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:22:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:22:06 +0000
Received: by mail-ig0-f182.google.com with SMTP id c1so1998717igq.9
        for <dev@spark.apache.org>; Sun, 24 Aug 2014 16:22:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=zUaP3TQGU6+/cyyf1rcqS30MW3SOy2S4wqA19Tp33HI=;
        b=fevgSeNLuhL5xKXw9aAFGi0jmTuq8hxf+ogdFuB9fyFptSngRRzCo4qxeSDr1N99ja
         Uy7W1fR+1sEFDOZ1ts71pMSvUsBy9cUryfvMn+vziJ4TAkGxW8l5EOMmEL7+64v0+vNv
         x3FwjIdVleKMV4HH101/LCTzHiAnFcDTsyz5D66Elisr15feFAL4r1mL7HiKNfD8Jknx
         41buC1maslaxmoQjY/pY8ka4HvDDkDBktCZ8gweGDpnpwosAllgIPxu3GWvq2NcppMTG
         z1JQ/jka83wa7C12aBY9HO1Wfn7HJ5K8MDfS4FDO4dlPRofd7dR9sBGK4GE0YNlmsLoN
         0mqg==
X-Gm-Message-State: ALoCoQlwTQBGVNMcx5Yl60IuAMzFI8OUgblUiSkrebK2QborogLgeJwM1sltcOSzwiZpAStZdgJu
X-Received: by 10.50.66.197 with SMTP id h5mr12143221igt.34.1408922524952;
 Sun, 24 Aug 2014 16:22:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Sun, 24 Aug 2014 16:21:44 -0700 (PDT)
In-Reply-To: <53FA7168.3000300@yahoo.com>
References: <53FA7168.3000300@yahoo.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 25 Aug 2014 00:21:44 +0100
Message-ID: <CAMAsSd+0PYEhwCJek9F8MmdBDYfU-9=SaHQHXqqVG0Ubyu6S1A@mail.gmail.com>
Subject: Re: Problems running examples in IDEA
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The examples aren't runnable quite like this. It's intended that they
are submitted to a cluster with spark-submit, which would among other
things provide Spark at runtime.

I think you might get them to run this way if you set master to
"local[*]" and indeed made a run profile that also included Spark on
the classpath.

You would never modify the .iml files anyway. You can change the Maven
pom.xml files if you were to need to modify a dependency scope.

On Mon, Aug 25, 2014 at 12:12 AM, Ron Gonzalez
<zlgonzalez@yahoo.com.invalid> wrote:
> Hi,
>   After getting the code base to compile, I tried running some of the scala
> examples.
>   They all fail since it can't find classes like SparkConf.
>   If I change the iml file to convert provided scope from PROVIDED to
> COMPILE, I am able to run them. It's simple by doing the following in the
> root directory of the spark code base: find . -name "*.iml" | xargs sed
> -i.bak 's/PROVIDED/COMPILE/g'.
>   Is this expected? I'd really rather not modify the iml files since they
> were sourced from the pom xml files, so if you guys have some tips on doing
> this better, that would be great...
>
> Thanks,
> Ron
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 24 23:55:44 2014
Return-Path: <dev-return-9017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3600111766
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 24 Aug 2014 23:55:44 +0000 (UTC)
Received: (qmail 50532 invoked by uid 500); 24 Aug 2014 23:55:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50464 invoked by uid 500); 24 Aug 2014 23:55:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50447 invoked by uid 99); 24 Aug 2014 23:55:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:55:43 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.152 as permitted sender)
Received: from [98.139.213.152] (HELO nm11-vm1.bullet.mail.bf1.yahoo.com) (98.139.213.152)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Aug 2014 23:55:14 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=RIhmsGMbKXC79IM49kcSW1cqcQi2CBnwyt/0yKk9MagdsOdIlRd1hovhP5MqMadEOD4DJZpXzhAjciULSJbLceBjnxAy1W998QuJFvrg8MHj4nnZ32jF9zDeYgwBC8uQ2iNo7LCgvre7BmEFqKwe4QjSx6TjsFHKgr2t5lf5YzZnuHcDoRUlMKaRO3crob+QP9jLmLXw+J8qFiakjeawZD5oAItLnBFu15LsTMiyIWYgbw6gKgb5I0QYVWkTUonoSbe69X+cjGFp8/dvsjGkiktQDchnS4/jKvZ8BsjXxnuvH0hS5qsVSl/xsKtnFaRxInZw2Sg71HxsGE6F9WvsOA==;
Received: from [98.139.215.143] by nm11.bullet.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:55:12 -0000
Received: from [98.139.213.15] by tm14.bullet.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:55:12 -0000
Received: from [127.0.0.1] by smtp115.mail.bf1.yahoo.com with NNFMP; 24 Aug 2014 23:55:12 -0000
X-Yahoo-Newman-Id: 321290.40608.bm@smtp115.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: Ek.1UcIVM1m1Nnpcyvmnd2BCFLjvntiAzKyB8_KcSgB3Xdk
 xfWQFt1YhXPd7fwZef9bc.RrKJyAgPvzZumzULptVyBTiFp9VLuIL6GrXuvf
 6a9JS3sGMRXwq_ymBWlnfeeJQGo9axpNu1RBkWbF3PQMdlvQ7osnavQcIhjG
 MLhNgqJP_b6SdFdg7UNV2lwGvmbYTMGgvHbcp_SaELCQuXhe8koPdBlHgTaF
 oiBWJYo_dVSfC7hACz.MeBwo.sLpfGg3ZnJvT0FXdmBASqO3r68gPXOVKmhD
 UCy3h7BTKv1Yvek9d8kQwVMPRUuPV0_SI5MI.QPWBMqgY.bdkRvHEogtUwk1
 A1NW81C7qz74.dKws2TC42zbp7w.nCVAVVTx4E8pArHXOZqwHRT184KvEH3C
 GgLvCP_et4BCnLcKA1fPhn7NHHypwGQwV54I9TOcNTO9A024_a4Kz2p0XORe
 g9OgfOlKlf2fEQzYuHy1gXtRQVei2.L6Nrxb9pjBGQPKU.UjM3zvPB.WSyA8
 Nd01KCObm5yqWSSTlhPRR79z0wbf8lw--
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Message-ID: <53FA7B55.3040303@yahoo.com>
Date: Sun, 24 Aug 2014 16:55:01 -0700
From: Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.0
MIME-Version: 1.0
To: Sean Owen <sowen@cloudera.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Problems running examples in IDEA
References: <53FA7168.3000300@yahoo.com> <CAMAsSd+0PYEhwCJek9F8MmdBDYfU-9=SaHQHXqqVG0Ubyu6S1A@mail.gmail.com>
In-Reply-To: <CAMAsSd+0PYEhwCJek9F8MmdBDYfU-9=SaHQHXqqVG0Ubyu6S1A@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Oh ok. So from the code base, local execution is dependent on everyone's 
way then, right? I am indeed changing the code to add the master to 
local[*], but still getting the no classdef found errors.

If that's the case, then I think I'm ok then...

Thanks,
Ron


On 08/24/2014 04:21 PM, Sean Owen wrote:
> The examples aren't runnable quite like this. It's intended that they
> are submitted to a cluster with spark-submit, which would among other
> things provide Spark at runtime.
>
> I think you might get them to run this way if you set master to
> "local[*]" and indeed made a run profile that also included Spark on
> the classpath.
>
> You would never modify the .iml files anyway. You can change the Maven
> pom.xml files if you were to need to modify a dependency scope.
>
> On Mon, Aug 25, 2014 at 12:12 AM, Ron Gonzalez
> <zlgonzalez@yahoo.com.invalid> wrote:
>> Hi,
>>    After getting the code base to compile, I tried running some of the scala
>> examples.
>>    They all fail since it can't find classes like SparkConf.
>>    If I change the iml file to convert provided scope from PROVIDED to
>> COMPILE, I am able to run them. It's simple by doing the following in the
>> root directory of the spark code base: find . -name "*.iml" | xargs sed
>> -i.bak 's/PROVIDED/COMPILE/g'.
>>    Is this expected? I'd really rather not modify the iml files since they
>> were sourced from the pom xml files, so if you guys have some tips on doing
>> this better, that would be great...
>>
>> Thanks,
>> Ron
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 02:13:58 2014
Return-Path: <dev-return-9018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9FB711937
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 02:13:58 +0000 (UTC)
Received: (qmail 78705 invoked by uid 500); 25 Aug 2014 02:13:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78650 invoked by uid 500); 25 Aug 2014 02:13:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78638 invoked by uid 99); 25 Aug 2014 02:13:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 02:13:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wangfei1@huawei.com designates 119.145.14.64 as permitted sender)
Received: from [119.145.14.64] (HELO szxga01-in.huawei.com) (119.145.14.64)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 02:13:27 +0000
Received: from 172.24.2.119 (EHLO szxeml449-hub.china.huawei.com) ([172.24.2.119])
	by szxrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id CAS84915;
	Mon, 25 Aug 2014 10:13:22 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml449-hub.china.huawei.com
 (10.82.67.192) with Microsoft SMTP Server id 14.3.158.1; Mon, 25 Aug 2014
 10:13:17 +0800
Message-ID: <53FA9B5C.7060605@huawei.com>
Date: Mon, 25 Aug 2014 10:11:40 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: Michael Armbrust <michael@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Working Formula for Hive 0.13?
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com> <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com> <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com> <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com> <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com> <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com> <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com> <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
In-Reply-To: <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"; format=flowed
Content-Transfer-Encoding: 8bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

   I have worked for a branch update the hive version to hive-0.13(by org.apache.hive)---https://github.com/scwf/spark/tree/hive-0.13
I am wondering whether it's ok to make a PR now because hive-0.13 version is not compatible with hive-0.12 and here i used org.apache.hive.


On 2014/7/29 8:22, Michael Armbrust wrote:
> A few things:
>   - When we upgrade to Hive 0.13.0, Patrick will likely republish the
> hive-exec jar just as we did for 0.12.0
>   - Since we have to tie into some pretty low level APIs it is unsurprising
> that the code doesn't just compile out of the box against 0.13.0
>   - ScalaReflection is for determining Schema from Scala classes, not
> reflection based bridge code.  Either way its unclear to if there is any
> reason to use reflection to support multiple versions, instead of just
> upgrading to Hive 0.13.0
>
> One question I have is, What is the goal of upgrading to hive 0.13.0?  Is
> it purely because you are having problems connecting to newer metastores?
>   Are there some features you are hoping for?  This will help me prioritize
> this effort.
>
> Michael
>
>
> On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> I was looking for a class where reflection-related code should reside.
>>
>> I found this but don't think it is the proper class for bridging
>> differences between hive 0.12 and 0.13.1:
>>
>> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala
>>
>> Cheers
>>
>>
>> On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>>> After manually copying hive 0.13.1 jars to local maven repo, I got the
>>> following errors when building spark-hive_2.10 module :
>>>
>>> [ERROR]
>>>
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala:182:
>>> type mismatch;
>>>   found   : String
>>>   required: Array[String]
>>> [ERROR]       val proc: CommandProcessor =
>>> CommandProcessorFactory.get(tokens(0), hiveconf)
>>> [ERROR]
>>>     ^
>>> [ERROR]
>>>
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala:60:
>>> value getAllPartitionsForPruner is not a member of org.apache.
>>>   hadoop.hive.ql.metadata.Hive
>>> [ERROR]         client.getAllPartitionsForPruner(table).toSeq
>>> [ERROR]                ^
>>> [ERROR]
>>>
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala:267:
>>> overloaded method constructor TableDesc with alternatives:
>>>    (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
>>> Class[_],x$3:
>> java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
>>> <and>
>>>    ()org.apache.hadoop.hive.ql.plan.TableDesc
>>>   cannot be applied to (Class[org.apache.hadoop.hive.serde2.Deserializer],
>>> Class[(some other)?0(in value tableDesc)(in value tableDesc)],
>> Class[?0(in
>>> value tableDesc)(in   value tableDesc)], java.util.Properties)
>>> [ERROR]   val tableDesc = new TableDesc(
>>> [ERROR]                   ^
>>> [WARNING] Class org.antlr.runtime.tree.CommonTree not found - continuing
>>> with a stub.
>>> [WARNING] Class org.antlr.runtime.Token not found - continuing with a
>> stub.
>>> [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing with a
>>> stub.
>>> [ERROR]
>>>       while compiling:
>>>
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
>>>          during phase: typer
>>>       library version: version 2.10.4
>>>      compiler version: version 2.10.4
>>>
>>> The above shows incompatible changes between 0.12 and 0.13.1
>>> e.g. the first error corresponds to the following method
>>> in CommandProcessorFactory :
>>>    public static CommandProcessor get(String[] cmd, HiveConf conf)
>>>
>>> Cheers
>>>
>>>
>>> On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
>>> wrote:
>>>
>>>> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
>> adding
>>>> the hive-exec jar to the spark-project repo? It doesnt look like
>> theres
>>>> a release date schedule for 0.14.
>>>>
>>>>
>>>>
>>>> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>>>>
>>>>> Exactly, forgot to mention Hulu team also made changes to cope with
>> those
>>>>> incompatibility issues, but they said thats relatively easy once the
>>>>> re-packaging work is done.
>>>>>
>>>>>
>>>>> On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com>
>>>>
>>>>> wrote:
>>>>>
>>>>>> I've heard from Cloudera that there were hive internal changes
>> between
>>>>>> 0.12 and 0.13 that required code re-writing. Over time it might be
>>>>>> possible for us to integrate with hive using API's that are more
>>>>>> stable (this is the domain of Michael/Cheng/Yin more than me!). It
>>>>>> would be interesting to see what the Hulu folks did.
>>>>>>
>>>>>> - Patrick
>>>>>>
>>>>>> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com>
>>>>>> wrote:
>>>>>>> AFAIK, according a recent talk, Hulu team in China has built Spark
>>>> SQL
>>>>>>> against Hive 0.13 (or 0.13.1?) successfully. Basically they also
>>>>>>> re-packaged Hive 0.13 as what the Spark team did. The slides of the
>>>>>> talk
>>>>>>> hasn't been released yet though.
>>>>>>>
>>>>>>>
>>>>>>> On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
>> wrote:
>>>>>>>
>>>>>>>> Owen helped me find this:
>>>>>>>> https://issues.apache.org/jira/browse/HIVE-7423
>>>>>>>>
>>>>>>>> I guess this means that for Hive 0.14, Spark should be able to
>>>>>> directly
>>>>>>>> pull in hive-exec-core.jar
>>>>>>>>
>>>>>>>> Cheers
>>>>>>>>
>>>>>>>>
>>>>>>>> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
>>>> pwendell@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> It would be great if the hive team can fix that issue. If not,
>>>>>> we'll
>>>>>>>>> have to continue forking our own version of Hive to change the
>> way
>>>>>> it
>>>>>>>>> publishes artifacts.
>>>>>>>>>
>>>>>>>>> - Patrick
>>>>>>>>>
>>>>>>>>> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
>>>>>> wrote:
>>>>>>>>>> Talked with Owen offline. He confirmed that as of 0.13,
>>>>>> hive-exec is
>>>>>>>>> still
>>>>>>>>>> uber jar.
>>>>>>>>>>
>>>>>>>>>> Right now I am facing the following error building against
>> Hive
>>>>>> 0.13.1
>>>>>>>> :
>>>>>>>>>>
>>>>>>>>>> [ERROR] Failed to execute goal on project spark-hive_2.10:
>> Could
>>>>>> not
>>>>>>>>>> resolve dependencies for project
>>>>>>>>>> org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>>>>>> following
>>>>>>>>>> artifacts could not be resolved:
>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1,
>>>>>>>>>> org.spark-project.hive:hive-exec:jar:0.13.1,
>>>>>>>>>> org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1 in
>>>>>>>>>> http://repo.maven.apache.org/maven2 was cached in the local
>>>>>>>> repository,
>>>>>>>>>> resolution will not be reattempted until the update interval
>> of
>>>>>>>>> maven-repo
>>>>>>>>>> has elapsed or updates are forced -> [Help 1]
>>>>>>>>>>
>>>>>>>>>> Some hint would be appreciated.
>>>>>>>>>>
>>>>>>>>>> Cheers
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
>> sowen@cloudera.com>
>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>>> Yes, it is published. As of previous versions, at least,
>>>>>> hive-exec
>>>>>>>>>>> included all of its dependencies *in its artifact*, making it
>>>>>> unusable
>>>>>>>>>>> as-is because it contained copies of dependencies that clash
>>>>>> with
>>>>>>>>>>> versions present in other artifacts, and can't be managed
>> with
>>>>>> Maven
>>>>>>>>>>> mechanisms.
>>>>>>>>>>>
>>>>>>>>>>> I am not sure why hive-exec was not published normally, with
>>>>>> just
>>>>>> its
>>>>>>>>>>> own classes. That's why it was copied, into an artifact with
>>>>>> just
>>>>>>>>>>> hive-exec code.
>>>>>>>>>>>
>>>>>>>>>>> You could do the same thing for hive-exec 0.13.1.
>>>>>>>>>>> Or maybe someone knows that it's published more 'normally'
>> now.
>>>>>>>>>>> I don't think hive-metastore is related to this question?
>>>>>>>>>>>
>>>>>>>>>>> I am no expert on the Hive artifacts, just remembering what
>> the
>>>>>> issue
>>>>>>>>>>> was initially in case it helps you get to a similar solution.
>>>>>>>>>>>
>>>>>>>>>>> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com
>>>
>>>>>> wrote:
>>>>>>>>>>>> hive-exec (as of 0.13.1) is published here:
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>
>>>>>>
>>>>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
>>>>>> 0.13.1%7Cjar
>>>>>>>>>>>>
>>>>>>>>>>>> Should a JIRA be opened so that dependency on
>> hive-metastore
>>>>>> can
>>>>>> be
>>>>>>>>>>>> replaced by dependency on hive-exec ?
>>>>>>>>>>>>
>>>>>>>>>>>> Cheers
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>>>>>> <sowen@cloudera.com>
>>>>>>>>> wrote:
>>>>>>>>>>>>
>>>>>>>>>>>>> The reason for org.spark-project.hive is that Spark relies
>>>> on
>>>>>>>>>>>>> hive-exec, but the Hive project does not publish this
>>>>>> artifact
>>>>>> by
>>>>>>>>>>>>> itself, only with all its dependencies as an uber jar.
>> Maybe
>>>>>> that's
>>>>>>>>>>>>> been improved. If so, you need to point at the new
>> hive-exec
>>>>>> and
>>>>>>>>>>>>> perhaps sort out its dependencies manually in your build.
>>>>>>>>>>>>>
>>>>>>>>>>>>> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
>>>> yuzhihong@gmail.com>
>>>>>>>> wrote:
>>>>>>>>>>>>>> I found 0.13.1 artifacts in maven:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>
>>>>>>
>>>>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
>>>>>> re%7C0.13.1%7Cjar
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> However, Spark uses groupId of org.spark-project.hive,
>> not
>>>>>>>>>>>>> org.apache.hive
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Can someone tell me how it is supposed to work ?
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Cheers
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>>>>>>>>> snunez@hortonworks.com>
>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> I saw a note earlier, perhaps on the user list, that at
>>>>>> least
>>>>>>>> one
>>>>>>>>>>>>> person is
>>>>>>>>>>>>>>> using Hive 0.13. Anyone got a working build
>> configuration
>>>>>> for
>>>>>>>> this
>>>>>>>>>>>>> version
>>>>>>>>>>>>>>> of Hive?
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Regards,
>>>>>>>>>>>>>>> - Steve
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>> CONFIDENTIALITY NOTICE
>>>>>>>>>>>>>>> NOTICE: This message is intended for the use of the
>>>>>> individual
>>>>>>>> or
>>>>>>>>>>>>> entity to
>>>>>>>>>>>>>>> which it is addressed and may contain information that
>> is
>>>>>>>>>>> confidential,
>>>>>>>>>>>>>>> privileged and exempt from disclosure under applicable
>>>>>> law.
>>>>>> If
>>>>>>>> the
>>>>>>>>>>>>> reader
>>>>>>>>>>>>>>> of this message is not the intended recipient, you are
>>>>>> hereby
>>>>>>>>>>> notified
>>>>>>>>>>>>> that
>>>>>>>>>>>>>>> any printing, copying, dissemination, distribution,
>>>>>> disclosure
>>>>>>>> or
>>>>>>>>>>>>>>> forwarding of this communication is strictly
>> prohibited.
>>>>>> If
>>>>>> you
>>>>>>>>> have
>>>>>>>>>>>>>>> received this communication in error, please contact
>> the
>>>>>> sender
>>>>>>>>>>>>> immediately
>>>>>>>>>>>>>>> and delete it from your system. Thank You.
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> CONFIDENTIALITY NOTICE
>>>> NOTICE: This message is intended for the use of the individual or entity
>>>> to
>>>> which it is addressed and may contain information that is confidential,
>>>> privileged and exempt from disclosure under applicable law. If the
>> reader
>>>> of this message is not the intended recipient, you are hereby notified
>>>> that
>>>> any printing, copying, dissemination, distribution, disclosure or
>>>> forwarding of this communication is strictly prohibited. If you have
>>>> received this communication in error, please contact the sender
>>>> immediately
>>>> and delete it from your system. Thank You.
>>>>
>>>
>>>
>>
>



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 04:52:40 2014
Return-Path: <dev-return-9019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10B6111B49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 04:52:40 +0000 (UTC)
Received: (qmail 19482 invoked by uid 500); 25 Aug 2014 04:52:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19419 invoked by uid 500); 25 Aug 2014 04:52:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19407 invoked by uid 99); 25 Aug 2014 04:52:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 04:52:39 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 04:52:33 +0000
Received: by mail-pa0-f48.google.com with SMTP id et14so19970898pad.7
        for <dev@spark.apache.org>; Sun, 24 Aug 2014 21:52:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=eal/vK8thLoChjK+/7nwTCCjoAdYqyfI1pmxA3qWKCA=;
        b=G2tUc9gePNgKG1OQCmlgkdo8sv/OTarTdj4SU0wTmXuhSDGg4zgMEGrXoHMUWgc4g1
         9ygHjSHvZLeruJHNj0cdPaRWNF/l44Qh0jlQ/O8Fb3Ay72HTu+d94XchZnTAdlVMGfJy
         D3fn/quoGzhDh98im4o6c53nREwKswgURK1VG47D9PuZBH+ytCctmVkXPy+4GLiqNR5q
         5jJK2kOnS91TKI/8VcnevK8cIa9hhUT+FUmiLXw6sp/x+CmKvuWjNT2QM1pYQPoyhUgl
         dQg7UViVFuJ6UL76IKB2l0i+lPRD5Hc/r+GpBlpeJXIAY/O091Qh8DLKyW0hq2u0FZYY
         nONA==
X-Received: by 10.68.171.33 with SMTP id ar1mr7575709pbc.148.1408942328936;
        Sun, 24 Aug 2014 21:52:08 -0700 (PDT)
Received: from mbp-3.local (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id ov8sm46590385pdb.92.2014.08.24.21.52.01
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 24 Aug 2014 21:52:02 -0700 (PDT)
Date: Sun, 24 Aug 2014 21:52:00 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: dev@mesos.apache.org, "=?utf-8?Q?dev=40spark.apache.org?="
 <dev@spark.apache.org>
Message-ID: <etPan.53fac0f0.643c9869.9a@mbp-3.local>
In-Reply-To: <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
 <etPan.53f920b8.ded7263.ae21@mbp-3.local>
 <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fac0f0_66334873_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fac0f0_66334873_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's too =
bad that this happens in fine-grained mode -- would be really good to fix=
. I'll see if we can get the workaround in=C2=A0https://github.com/apache=
/spark/pull/1860 into Spark 1.1. Incidentally have you tried that=3F

Matei

On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary=40gmail.com) w=
rote:

Hi Matei,

We have an analytics team that uses the cluster on a daily basis. =C2=A0T=
hey use two types of 'run modes':

1) =46or running actual queries, they set the spark.executor.memory to so=
mething between 4 and 8GB of RAM/worker. =C2=A0

2) A shell that takes a minimal amount of memory on workers (128MB) for p=
rototyping out a larger query. =C2=A0This allows them to not take up RAM =
on the cluster when they do not really need it.

We see the deadlocks when there are a few shells in either case. =C2=A0=46=
rom the usage patterns we have, coarse-grained mode would be a challenge =
as we have to constantly remind people to kill their shells as soon as th=
eir queries finish. =C2=A0

Am I correct in viewing Mesos in coarse-grained mode as being similar to =
Spark Standalone's cpu allocation behavior=3F




On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia=40gmail.com=
> wrote:
Hey Gary, just as a workaround, note that you can use Mesos in coarse-gra=
ined mode by setting spark.mesos.coarse=3Dtrue. Then it will hold onto CP=
Us for the duration of the job.

Matei

On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary=40gmail.com) w=
rote:

I just wanted to bring up a significant Mesos/Spark issue that makes the
combo difficult to use for teams larger than 4-5 people. It's covered in
https://issues.apache.org/jira/browse/MESOS-1688. My understanding is tha=
t
Spark's use of executors in fine-grained mode is a very different behavio=
r
than many of the other common frameworks for Mesos.


--53fac0f0_66334873_9a--


From dev-return-9020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 04:54:44 2014
Return-Path: <dev-return-9020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFBD011B55
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 04:54:44 +0000 (UTC)
Received: (qmail 23845 invoked by uid 500); 25 Aug 2014 04:54:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23789 invoked by uid 500); 25 Aug 2014 04:54:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23777 invoked by uid 99); 25 Aug 2014 04:54:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 04:54:43 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tnachen@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 04:54:18 +0000
Received: by mail-ob0-f172.google.com with SMTP id wn1so10196148obc.17
        for <dev@spark.apache.org>; Sun, 24 Aug 2014 21:54:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=sgxK24DxNc/Hnz+WCgiwGerv86vGLjJG0Lo4NHEZguc=;
        b=HpNNlil1OmXaVuAPpFb671Jrq9BiVUijmLq3Bi24EzeWKIJews+7XDCXp9wd3UOik1
         4oEul3psvK2uAfoHIHNhAxuZrH61LkpJmqlWyyty07Sw6VUeBlZ0H5RGfOOldLR0zIjH
         6WfytsIGOp1QVumnvXe5ubv3Rcn2T2d1p7a6NU6qNycPRW0OngpK4u4Eprza5+9LvKQ5
         OvDUfxTyvnwUecFwCzSPYR7/dhIb/cmfmHObqMriG82+hVM8MSuf2yE8Wn0jLvjbnqLF
         2CMY3YLX5bSm/Lkiw2CYvF7zoxbpc6lx89Rp5uLqilgwWR93npGCdr1M+3m2YFRnmjNK
         wsuA==
MIME-Version: 1.0
X-Received: by 10.60.134.76 with SMTP id pi12mr5769599oeb.0.1408942456435;
 Sun, 24 Aug 2014 21:54:16 -0700 (PDT)
Received: by 10.60.37.4 with HTTP; Sun, 24 Aug 2014 21:54:16 -0700 (PDT)
In-Reply-To: <etPan.53fac0f0.643c9869.9a@mbp-3.local>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
	<CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
	<etPan.53fac0f0.643c9869.9a@mbp-3.local>
Date: Sun, 24 Aug 2014 21:54:16 -0700
Message-ID: <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Timothy Chen <tnachen@gmail.com>
To: "dev@mesos.apache.org" <dev@mesos.apache.org>
Cc: Gary Malouf <malouf.gary@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

+1 to have the work around in.

I'll be investigating from the Mesos side too.

Tim

On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's too =
bad that this happens in fine-grained mode -- would be really good to fix. =
I'll see if we can get the workaround in https://github.com/apache/spark/pu=
ll/1860 into Spark 1.1. Incidentally have you tried that?
>
> Matei
>
> On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com) wro=
te:
>
> Hi Matei,
>
> We have an analytics team that uses the cluster on a daily basis.  They u=
se two types of 'run modes':
>
> 1) For running actual queries, they set the spark.executor.memory to some=
thing between 4 and 8GB of RAM/worker.
>
> 2) A shell that takes a minimal amount of memory on workers (128MB) for p=
rototyping out a larger query.  This allows them to not take up RAM on the =
cluster when they do not really need it.
>
> We see the deadlocks when there are a few shells in either case.  From th=
e usage patterns we have, coarse-grained mode would be a challenge as we ha=
ve to constantly remind people to kill their shells as soon as their querie=
s finish.
>
> Am I correct in viewing Mesos in coarse-grained mode as being similar to =
Spark Standalone's cpu allocation behavior?
>
>
>
>
> On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:
> Hey Gary, just as a workaround, note that you can use Mesos in coarse-gra=
ined mode by setting spark.mesos.coarse=3Dtrue. Then it will hold onto CPUs=
 for the duration of the job.
>
> Matei
>
> On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com) wro=
te:
>
> I just wanted to bring up a significant Mesos/Spark issue that makes the
> combo difficult to use for teams larger than 4-5 people. It's covered in
> https://issues.apache.org/jira/browse/MESOS-1688. My understanding is tha=
t
> Spark's use of executors in fine-grained mode is a very different behavio=
r
> than many of the other common frameworks for Mesos.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 05:40:19 2014
Return-Path: <dev-return-9021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 09E5711C3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 05:40:18 +0000 (UTC)
Received: (qmail 84403 invoked by uid 500); 25 Aug 2014 05:40:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84337 invoked by uid 500); 25 Aug 2014 05:40:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84324 invoked by uid 99); 25 Aug 2014 05:40:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 05:40:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dibyendu.bhattachary@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 05:39:50 +0000
Received: by mail-ob0-f169.google.com with SMTP id uz6so8363514obc.0
        for <dev@spark.apache.org>; Sun, 24 Aug 2014 22:39:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=OYGatB7qgfJ5erf+hTsjhOy+2jDlBwFsgriBBvr/iAI=;
        b=Z1LyXmWuZpkEa9zWuHHX6njBUQAmix/1isuITwgzlMZuGE2tYYr2lJ6uomcw7pxIpL
         +SNync04LHasiXp4Kqrc0935lxOZZnWS6LEZ/pvO1qvshjV0jgB2v+a/bGQ/h2PYr8c/
         p+X+DjS2/YndoIEK9CvdKOHRgmj1jyl7FMb859heI1jB2Fdh8kEB7w3kAun2wXR/YtO0
         bv61Oru/p9mziSS9qsIuNjdfimIoFxMwcjY2LdwzPnYDMh0xDb8+K625wmh7Etw9oYdf
         +MeMHNgYzdKNVUXN9/APXrFHU9nl0u4ibsj/b1PJMddyIANH0M3InUwwrj+QdpddhZg4
         Wx7Q==
MIME-Version: 1.0
X-Received: by 10.202.0.194 with SMTP id 185mr180719oia.63.1408945189159; Sun,
 24 Aug 2014 22:39:49 -0700 (PDT)
Received: by 10.76.108.140 with HTTP; Sun, 24 Aug 2014 22:39:49 -0700 (PDT)
In-Reply-To: <CAMwrk0=8rH7Cfn+BSbq8O=nVqKsE4vZy=cUgdCzg=i2=FJWQFw@mail.gmail.com>
References: <CAFiYKR9-KfcYYKXeB0E0BxNTM+vJJnN3etKnSMzLSVjsM+wiJQ@mail.gmail.com>
	<CABPQxstS7yuJ38-FWXc3Ugt0gnmdCBe6MNt4_q64dN2M9MUDUw@mail.gmail.com>
	<CAOErhNTyNz-BGtJGd5iU-05j10jfmU81T7rpwLUH_ZQxVjZ7mQ@mail.gmail.com>
	<CAFVOukbZHMevfk9zNEcfXwA45-o4d024zPbEuM-51VV0oBVGfg@mail.gmail.com>
	<CAFiYKR-3GxeZVMutS2-+dxbmsEZ7WU4aV9SrLyDo6CHpGOs-Mg@mail.gmail.com>
	<64474308D680D540A4D8151B0F7C03F7026F9413@SHSMSX104.ccr.corp.intel.com>
	<CAFiYKR89TZ21tsjmp+cJ7UQEFg6=zDgYGJwMTQ4_bB7nj8DusA@mail.gmail.com>
	<CAMwrk0=8rH7Cfn+BSbq8O=nVqKsE4vZy=cUgdCzg=i2=FJWQFw@mail.gmail.com>
Date: Mon, 25 Aug 2014 11:09:49 +0530
Message-ID: <CAFiYKR_=JjfWcDbe+NxC7d+jvodpR+VAHBP7VuDE9t0_fpt8cw@mail.gmail.com>
Subject: Re: Low Level Kafka Consumer for Spark
From: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
To: Tathagata Das <tathagata.das1565@gmail.com>
Cc: "Shao, Saisai" <saisai.shao@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11379fca8207f105016d9d6e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11379fca8207f105016d9d6e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

Just to give you some update on this low level consumer  (
https://github.com/dibbhatt/kafka-spark-consumer), we at Pearson have been
doing good amount load and stress testing for last few weeks and initial
test results are very impressive. We did not see any data loss, no issue
related to "Out of Memory"  (as like existing consumer) with heavy load and
no issues related to Kafka offsets managements .

Regards,
Dibyendu


On Thu, Aug 7, 2014 at 4:46 AM, Tathagata Das <tathagata.das1565@gmail.com>
wrote:

> Hi Dibyendu,
>
> This is really awesome. I am still yet to go through the code to
> understand the details, but I want to do it really soon. In particular, I
> want to understand the improvements, over the existing Kafka receiver.
>
> And its fantastic to see such contributions from the community. :)
>
> TD
>
>
> On Tue, Aug 5, 2014 at 8:38 AM, Dibyendu Bhattacharya <
> dibyendu.bhattachary@gmail.com> wrote:
>
>> Hi
>>
>> This fault tolerant aspect already taken care in the Kafka-Spark Consume=
r
>> code , like if Leader of a partition changes etc.. in ZkCoordinator.java=
.
>> Basically it does a refresh of PartitionManagers every X seconds to make
>> sure Partition details is correct and consumer don't fail.
>>
>> Dib
>>
>>
>> On Tue, Aug 5, 2014 at 8:01 PM, Shao, Saisai <saisai.shao@intel.com>
>> wrote:
>>
>> > Hi,
>> >
>> > I think this is an awesome feature for Spark Streaming Kafka interface
>> to
>> > offer user the controllability of partition offset, so user can have
>> more
>> > applications based on this.
>> >
>> > What I concern is that if we want to do offset management, fault
>> tolerant
>> > related control and others, we have to take the role as current
>> > ZookeeperConsumerConnect did, that would be a big field we should take
>> care
>> > of, for example when node is failed, how to pass current partition to
>> > another consumer and some others. I=E2=80=99m not sure what is your th=
ought?
>> >
>> > Thanks
>> > Jerry
>> >
>> > From: Dibyendu Bhattacharya [mailto:dibyendu.bhattachary@gmail.com]
>> > Sent: Tuesday, August 05, 2014 5:15 PM
>> > To: Jonathan Hodges; dev@spark.apache.org
>> > Cc: user
>> > Subject: Re: Low Level Kafka Consumer for Spark
>> >
>> > Thanks Jonathan,
>> >
>> > Yes, till non-ZK based offset management is available in Kafka, I need
>> to
>> > maintain the offset in ZK. And yes, both cases explicit commit is
>> > necessary. I modified the Low Level Kafka Spark Consumer little bit to
>> have
>> > Receiver spawns threads for every partition of the topic and perform t=
he
>> > 'store' operation in multiple threads. It would be good if the
>> > receiver.store methods are made thread safe..which is not now presentl=
y
>> .
>> >
>> > Waiting for TD's comment on this Kafka Spark Low Level consumer.
>> >
>> >
>> > Regards,
>> > Dibyendu
>> >
>> >
>> > On Tue, Aug 5, 2014 at 5:32 AM, Jonathan Hodges <hodgesz@gmail.com
>> <mailto:
>> > hodgesz@gmail.com>> wrote:
>> > Hi Yan,
>> >
>> > That is a good suggestion.  I believe non-Zookeeper offset management
>> will
>> > be a feature in the upcoming Kafka 0.8.2 release tentatively scheduled
>> for
>> > September.
>> >
>> >
>> >
>> https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offse=
t+Management
>> >
>> > That should make this fairly easy to implement, but it will still
>> require
>> > explicit offset commits to avoid data loss which is different than the
>> > current KafkaUtils implementation.
>> >
>> > Jonathan
>> >
>> >
>> >
>> >
>> > On Mon, Aug 4, 2014 at 4:51 PM, Yan Fang <yanfang724@gmail.com<mailto:
>> > yanfang724@gmail.com>> wrote:
>> > Another suggestion that may help is that, you can consider use Kafka t=
o
>> > store the latest offset instead of Zookeeper. There are at least two
>> > benefits: 1) lower the workload of ZK 2) support replay from certain
>> > offset. This is how Samza<http://samza.incubator.apache.org/> deals
>> with
>> > the Kafka offset, the doc is here<
>> >
>> http://samza.incubator.apache.org/learn/documentation/0.7.0/container/ch=
eckpointing.html
>> >
>> > . Thank you.
>> >
>> > Cheers,
>> >
>> > Fang, Yan
>> > yanfang724@gmail.com<mailto:yanfang724@gmail.com>
>> > +1 (206) 849-4108<tel:%2B1%20%28206%29%20849-4108>
>> >
>> > On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com
>> > <mailto:pwendell@gmail.com>> wrote:
>> > I'll let TD chime on on this one, but I'm guessing this would be a
>> welcome
>> > addition. It's great to see community effort on adding new
>> > streams/receivers, adding a Java API for receivers was something we di=
d
>> > specifically to allow this :)
>> >
>> > - Patrick
>> >
>> > On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <
>> > dibyendu.bhattachary@gmail.com<mailto:dibyendu.bhattachary@gmail.com>>
>> > wrote:
>> > Hi,
>> >
>> > I have implemented a Low Level Kafka Consumer for Spark Streaming usin=
g
>> > Kafka Simple Consumer API. This API will give better control over the
>> Kafka
>> > offset management and recovery from failures. As the present Spark
>> > KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a bette=
r
>> > control over the offset management which is not possible in Kafka
>> HighLevel
>> > consumer.
>> >
>> > This Project is available in below Repo :
>> >
>> > https://github.com/dibbhatt/kafka-spark-consumer
>> >
>> >
>> > I have implemented a Custom Receiver
>> consumer.kafka.client.KafkaReceiver.
>> > The KafkaReceiver uses low level Kafka Consumer API (implemented in
>> > consumer.kafka packages) to fetch messages from Kafka and 'store' it i=
n
>> > Spark.
>> >
>> > The logic will detect number of partitions for a topic and spawn that
>> many
>> > threads (Individual instances of Consumers). Kafka Consumer uses
>> Zookeeper
>> > for storing the latest offset for individual partitions, which will
>> help to
>> > recover in case of failure. The Kafka Consumer logic is tolerant to ZK
>> > Failures, Kafka Leader of Partition changes, Kafka broker failures,
>> >  recovery from offset errors and other fail-over aspects.
>> >
>> > The consumer.kafka.client.Consumer is the sample Consumer which uses
>> this
>> > Kafka Receivers to generate DStreams from Kafka and apply a Output
>> > operation for every messages of the RDD.
>> >
>> > We are planning to use this Kafka Spark Consumer to perform Near Real
>> Time
>> > Indexing of Kafka Messages to target Search Cluster and also Near Real
>> Time
>> > Aggregation using target NoSQL storage.
>> >
>> > Kindly let me know your view. Also if this looks good, can I contribut=
e
>> to
>> > Spark Streaming project.
>> >
>> > Regards,
>> > Dibyendu
>> >
>> >
>> >
>> >
>> >
>>
>
>

--001a11379fca8207f105016d9d6e--

From dev-return-9022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 09:14:40 2014
Return-Path: <dev-return-9022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C1A8110DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 09:14:40 +0000 (UTC)
Received: (qmail 18818 invoked by uid 500); 25 Aug 2014 09:14:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18750 invoked by uid 500); 25 Aug 2014 09:14:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18735 invoked by uid 99); 25 Aug 2014 09:14:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 09:14:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhengbing.li@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 09:14:33 +0000
Received: from 172.24.2.119 (EHLO SZXEMA401-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BYP15748;
	Mon, 25 Aug 2014 17:14:10 +0800 (CST)
Received: from SZXEMA501-MBX.china.huawei.com ([169.254.1.108]) by
 SZXEMA401-HUB.china.huawei.com ([10.82.72.33]) with mapi id 14.03.0158.001;
 Mon, 25 Aug 2014 17:14:05 +0800
From: "Lizhengbing (bing, BIPA)" <zhengbing.li@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?
Thread-Topic: I want to contribute MLlib two quality measures(ARHR and HR)
 for top N recommendation system. Is this meaningful?
Thread-Index: Ac/AROnrhXBuBmCRQL+DlI8Z9vb1kw==
Date: Mon, 25 Aug 2014 09:14:03 +0000
Message-ID: <49229E870391FC49BBBED818C268753D705A644F@SZXEMA501-MBX.china.huawei.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: yes
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.170.84]
Content-Type: multipart/related;
	boundary="_004_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_";
	type="multipart/alternative"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_004_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_
Content-Type: multipart/alternative;
	boundary="_000_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_"

--_000_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable

Hi:
In paper "Item-Based Top-N Recommendation Algorithms"(https://stuyresearch.=
googlecode.com/hg/blake/resources/10.1.1.102.4451.pdf), there are two param=
eters measuring the quality of recommendation: HR and ARHR.
If I use ALS(Implicit) for top-N recommendation system, I want to check it'=
s quality. ARHR and HR are two good quality measures.
I want to contribute them to spark MLlib.  So I want to know whether this i=
s meaningful?


(1) If n is the total number of customers/users,  the hit-rate of the recom=
mendation algorithm was computed as
hit-rate (HR) =3D Number of hits / n

(2)If h is the number of hits that occurred at positions p1, p2, . . . , ph=
 within the top-N lists (i.e., 1 =98 pi =98 N), then the average reciprocal=
 hit-rank is equal to:
[cid:image001.png@01CFC086.8EE1FF40]i
.

--_000_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_
Content-Type: text/html; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable

<html xmlns:v=3D"urn:schemas-microsoft-com:vml" xmlns:o=3D"urn:schemas-micr=
osoft-com:office:office" xmlns:w=3D"urn:schemas-microsoft-com:office:word" =
xmlns:m=3D"http://schemas.microsoft.com/office/2004/12/omml" xmlns=3D"http:=
//www.w3.org/TR/REC-html40">
<head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dkoi8-r">
<meta name=3D"Generator" content=3D"Microsoft Word 12 (filtered medium)">
<!--[if !mso]><style>v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style><![endif]--><style><!--
/* Font Definitions */
@font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:MTSY;
	panose-1:0 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:RMTMI;
	panose-1:0 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"\@MTSY";
	panose-1:0 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"\@RMTMI";
	panose-1:0 0 0 0 0 0 0 0 0 0;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0cm;
	margin-bottom:.0001pt;
	text-align:justify;
	text-justify:inter-ideograph;
	font-size:10.5pt;
	font-family:"Calibri","sans-serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"\6279\6CE8\6846\6587\672C Char";
	margin:0cm;
	margin-bottom:.0001pt;
	text-align:justify;
	text-justify:inter-ideograph;
	font-size:9.0pt;
	font-family:"Calibri","sans-serif";}
span.EmailStyle17
	{mso-style-type:personal-compose;
	font-family:"Calibri","sans-serif";
	color:windowtext;}
span.Char
	{mso-style-name:"\6279\6CE8\6846\6587\672C Char";
	mso-style-priority:99;
	mso-style-link:\6279\6CE8\6846\6587\672C;}
.MsoChpDefault
	{mso-style-type:export-only;}
/* Page Definitions */
@page WordSection1
	{size:612.0pt 792.0pt;
	margin:72.0pt 90.0pt 72.0pt 90.0pt;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext=3D"edit" spidmax=3D"2050" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext=3D"edit">
<o:idmap v:ext=3D"edit" data=3D"1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang=3D"ZH-CN" link=3D"blue" vlink=3D"purple" style=3D"text-justify-t=
rim:punctuation">
<div class=3D"WordSection1">
<p class=3D"MsoNormal"><span lang=3D"EN-US">Hi:<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-indent:15.75pt"><span lang=3D"EN-US">I=
n paper &#8220;Item-Based Top-N Recommendation Algorithms&#8221;(https://st=
uyresearch.googlecode.com/hg/blake/resources/10.1.1.102.4451.pdf), there ar=
e two parameters measuring the quality of recommendation:
 HR and ARHR.<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-indent:15.75pt"><span lang=3D"EN-US">I=
f I use ALS(Implicit) for top-N recommendation system, I want to check it&#=
8217;s quality. ARHR and HR are two good quality measures.<o:p></o:p></span=
></p>
<p class=3D"MsoNormal" style=3D"text-indent:15.75pt"><span lang=3D"EN-US">I=
 want to contribute them to spark MLlib. &nbsp;So I want to know whether th=
is is meaningful?<o:p></o:p></span></p>
<p class=3D"MsoNormal" style=3D"text-indent:15.75pt"><span lang=3D"EN-US"><=
o:p>&nbsp;</o:p></span></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-autospa=
ce:none"><i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quo=
t;Times New Roman&quot;,&quot;serif&quot;"><o:p>&nbsp;</o:p></span></i></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-autospa=
ce:none"><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;T=
imes New Roman&quot;,&quot;serif&quot;">(1) If
<i>n </i>is the total number of customers/users, &nbsp;the hit-rate of the =
recommendation algorithm was computed as<o:p></o:p></span></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-indent:=
10.0pt;text-autospace:none">
<i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;Times N=
ew Roman&quot;,&quot;serif&quot;">hit-rate (HR)
</span></i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:MTSY"=
>=3D </span>
<i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;Times N=
ew Roman&quot;,&quot;serif&quot;">Number of hits / n<o:p></o:p></span></i><=
/p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-indent:=
10.0pt;text-autospace:none">
<i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;Times N=
ew Roman&quot;,&quot;serif&quot;"><o:p>&nbsp;</o:p></span></i></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-autospa=
ce:none"><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;T=
imes New Roman&quot;,&quot;serif&quot;">(2)If
<i>h </i>is the number of hits that occurred at positions <i>p</i></span><s=
pan lang=3D"EN-US" style=3D"font-size:7.0pt;font-family:&quot;Times New Rom=
an&quot;,&quot;serif&quot;">1</span><span lang=3D"EN-US" style=3D"font-size=
:10.0pt;font-family:&quot;Times New Roman&quot;,&quot;serif&quot;">,
<i>p</i></span><span lang=3D"EN-US" style=3D"font-size:7.0pt;font-family:&q=
uot;Times New Roman&quot;,&quot;serif&quot;">2</span><span lang=3D"EN-US" s=
tyle=3D"font-size:10.0pt;font-family:&quot;Times New Roman&quot;,&quot;seri=
f&quot;">,
</span><i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:RMTMI"=
>. . . </span>
</i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;Times =
New Roman&quot;,&quot;serif&quot;">,
<i>p</i></span><i><span lang=3D"EN-US" style=3D"font-size:7.0pt;font-family=
:&quot;Times New Roman&quot;,&quot;serif&quot;">h
</span></i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot=
;Times New Roman&quot;,&quot;serif&quot;">within the
<i>top-N </i>lists (i.e., 1 </span><span style=3D"font-size:10.0pt;font-fam=
ily:MTSY">=98
</span><i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;=
Times New Roman&quot;,&quot;serif&quot;">p</span></i><i><span lang=3D"EN-US=
" style=3D"font-size:7.0pt;font-family:&quot;Times New Roman&quot;,&quot;se=
rif&quot;">i
</span></i><span style=3D"font-size:10.0pt;font-family:MTSY">=98 </span><i>=
<span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;Times New =
Roman&quot;,&quot;serif&quot;">N</span></i><span lang=3D"EN-US" style=3D"fo=
nt-size:10.0pt;font-family:&quot;Times New Roman&quot;,&quot;serif&quot;">)=
, then the
 average reciprocal hit-rank is equal to:<o:p></o:p></span></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-autospa=
ce:none"><i><span lang=3D"EN-US" style=3D"font-size:7.0pt;font-family:&quot=
;Times New Roman&quot;,&quot;serif&quot;"><img width=3D"500" height=3D"85" =
id=3D"_x56fe__x7247__x0020_1" src=3D"cid:image001.png@01CFC086.8EE1FF40"></=
span></i><i><span lang=3D"EN-US" style=3D"font-size:7.0pt;font-family:&quot=
;Times New Roman&quot;,&quot;serif&quot;">i<o:p></o:p></span></i></p>
<p class=3D"MsoNormal" align=3D"left" style=3D"text-align:left;text-indent:=
10.0pt;text-autospace:none">
<i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:RMTMI">.</spa=
n></i><i><span lang=3D"EN-US" style=3D"font-size:10.0pt;font-family:&quot;T=
imes New Roman&quot;,&quot;serif&quot;"><o:p></o:p></span></i></p>
</div>
</body>
</html>

--_000_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_--

--_004_49229E870391FC49BBBED818C268753D705A644FSZXEMA501MBXchi_--

From dev-return-9023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 12:06:00 2014
Return-Path: <dev-return-9023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91225117D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 12:06:00 +0000 (UTC)
Received: (qmail 10163 invoked by uid 500); 25 Aug 2014 12:05:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10081 invoked by uid 500); 25 Aug 2014 12:05:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9926 invoked by uid 99); 25 Aug 2014 12:05:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 12:05:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 12:05:32 +0000
Received: by mail-qg0-f42.google.com with SMTP id j5so13167457qga.1
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 05:05:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=CyS0/9rBfQXrtRdbKWUiuU3/Y5S9c0GUg4fJu8wH1QE=;
        b=HYgM90h5EgRzfKX0TdWK7H2E7Htbm/jH+e6auf3QRblk0p50QWx/GlZfewrTICrDit
         cahU15/rkkG4ZZ6FpuVVYorOpPSV2sAbz0CRyj810uobAM6E8GC38qYJqSEweOvSk2Hq
         qsj/E02wmJqn7HGwFQV/OdkzAYo5REeTvLrj0kDFF1MxoPn6AN7owibZuKNy4cba3kQh
         BYZA4NVrz5BBSU30QLzc0RAnmf47XMv8gBLkdJYhoB4QXbzAZhwv9qIslm2hliQhCeVD
         Cn0WwXzab7maIxbjR6xNCBIcijrncCP7CVsFKKiHq43iAFgzrTcfNbayV8HAGXKGQNp1
         MK7g==
MIME-Version: 1.0
X-Received: by 10.140.27.173 with SMTP id 42mr18686233qgx.59.1408968330808;
 Mon, 25 Aug 2014 05:05:30 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Mon, 25 Aug 2014 05:05:30 -0700 (PDT)
In-Reply-To: <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
	<CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
	<etPan.53fac0f0.643c9869.9a@mbp-3.local>
	<CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
Date: Mon, 25 Aug 2014 08:05:30 -0400
Message-ID: <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Gary Malouf <malouf.gary@gmail.com>
To: Timothy Chen <tnachen@gmail.com>
Cc: "dev@mesos.apache.org" <dev@mesos.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c14792db967f050173009e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c14792db967f050173009e
Content-Type: text/plain; charset=UTF-8

We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.


On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote:

> +1 to have the work around in.
>
> I'll be investigating from the Mesos side too.
>
> Tim
>
> On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's too
> bad that this happens in fine-grained mode -- would be really good to fix.
> I'll see if we can get the workaround in
> https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
> have you tried that?
> >
> > Matei
> >
> > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com)
> wrote:
> >
> > Hi Matei,
> >
> > We have an analytics team that uses the cluster on a daily basis.  They
> use two types of 'run modes':
> >
> > 1) For running actual queries, they set the spark.executor.memory to
> something between 4 and 8GB of RAM/worker.
> >
> > 2) A shell that takes a minimal amount of memory on workers (128MB) for
> prototyping out a larger query.  This allows them to not take up RAM on the
> cluster when they do not really need it.
> >
> > We see the deadlocks when there are a few shells in either case.  From
> the usage patterns we have, coarse-grained mode would be a challenge as we
> have to constantly remind people to kill their shells as soon as their
> queries finish.
> >
> > Am I correct in viewing Mesos in coarse-grained mode as being similar to
> Spark Standalone's cpu allocation behavior?
> >
> >
> >
> >
> > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > Hey Gary, just as a workaround, note that you can use Mesos in
> coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold
> onto CPUs for the duration of the job.
> >
> > Matei
> >
> > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com)
> wrote:
> >
> > I just wanted to bring up a significant Mesos/Spark issue that makes the
> > combo difficult to use for teams larger than 4-5 people. It's covered in
> > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is
> that
> > Spark's use of executors in fine-grained mode is a very different
> behavior
> > than many of the other common frameworks for Mesos.
> >
>

--001a11c14792db967f050173009e--

From dev-return-9024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 18:07:15 2014
Return-Path: <dev-return-9024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DA521186A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 18:07:15 +0000 (UTC)
Received: (qmail 32618 invoked by uid 500); 25 Aug 2014 18:07:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32547 invoked by uid 500); 25 Aug 2014 18:07:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32522 invoked by uid 99); 25 Aug 2014 18:07:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 18:07:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 18:07:10 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so13267213wgg.4
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 11:06:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=qDa+IvXYQOK/vJ6grpmICrdd44hvwMkwwgtxSfferzo=;
        b=rBF3hIDKbKB8c2rk+fVTFRlLtpj/WXJCz0MOpAu5PXykCbhKuF/v2bWVkbcIh023+L
         m48YR8sjaz1FZhWu/aORGQVIyRg4JbzVYJOzlzmWvJ5x0tJZJtAVnmGWuF/i7MCb+34k
         TTE6AM28VxivCfc09WjDU+yeYteZQ+Z8PTWPQx++4ALhushW+vXufc/TsllVezjzMnAz
         Z2M1FcEdi/k1OiYuQIHXXLDwY7WsL6x+8IadD1wCEg5WdFoaPd9FwOdu688bTyq13t66
         qHtthsSwTLAbWIIiybiuxcwpjzU7KnreLlPzxtgE9JKq5+Yg8BYbb7cJFej17XpqbMfh
         xXAw==
X-Received: by 10.194.71.210 with SMTP id x18mr23867182wju.6.1408990008811;
 Mon, 25 Aug 2014 11:06:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 25 Aug 2014 11:06:08 -0700 (PDT)
In-Reply-To: <CAOhmDzdAcd6W5jCmHKkut9ER8ZZ08yNZioNe=JWGbRLypvW6RQ@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
 <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
 <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
 <CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
 <CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com>
 <CABPQxstBXJmwdPxawQEzTTGpqNfC7fTJpo2rp=9PSv3tp5Jc1g@mail.gmail.com> <CAOhmDzdAcd6W5jCmHKkut9ER8ZZ08yNZioNe=JWGbRLypvW6RQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 25 Aug 2014 14:06:08 -0400
Message-ID: <CAOhmDzfUANBEOe67nijdCxD3E9KBF3yC294xmdRkYONNkwUz7w@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd0776f7aa730501780cde
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0776f7aa730501780cde
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

FYI: Looks like the Mesos folk also have a bot to do automatic linking, but
it appears to have been provided to them somehow by ASF.

See this comment as an example:
https://issues.apache.org/jira/browse/MESOS-1688?focusedCommentId=3D1410907=
8&page=3Dcom.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#c=
omment-14109078

Might be a small win to push this work to a bot ASF manages if we can get
access to it (and if we have no concerns about depending on an another
external service).

Nick


On Mon, Aug 11, 2014 at 4:10 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Thanks for looking into this. I think little tools like this are super
> helpful.
>
> Would it hurt to open a request with INFRA to install/configure the
> JIRA-GitHub plugin while we continue to use the Python script we have? I
> wouldn't mind opening that JIRA issue with them.
>
> Nick
>
>
> On Mon, Aug 11, 2014 at 12:52 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> I spent some time on this and I'm not sure either of these is an option,
>> unfortunately.
>>
>> We typically can't use custom JIRA plug-in's because this JIRA is
>> controlled by the ASF and we don't have rights to modify most things abo=
ut
>> how it works (it's a large shared JIRA instance used by more than 50
>> projects). It's worth looking into whether they can do something. In
>> general we've tended to avoid going through ASF infra them whenever
>> possible, since they are generally overloaded and things move very slowl=
y,
>> even if there are outages.
>>
>> Here is the script we use to do the sync:
>> https://github.com/apache/spark/blob/master/dev/github_jira_sync.py
>>
>> It might be possible to modify this to support post-hoc changes, but we'=
d
>> need to think about how to do so while minimizing function calls to the =
ASF
>> JIRA API, which I found are very slow.
>>
>> - Patrick
>>
>>
>>
>> On Mon, Aug 11, 2014 at 7:51 AM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> It looks like this script doesn't catch PRs that are opened and *then*
>>> have
>>>
>>> the JIRA issue ID added to the name. Would it be easy to somehow have t=
he
>>> script trigger on PR name changes as well as PR creates?
>>>
>>> Alternately, is there a reason we can't or don't want to use the plugin
>>> mentioned below? (I'm assuming it covers cases like this, but I'm not
>>> sure.)
>>>
>>> Nick
>>>
>>>
>>>
>>> On Wed, Jul 23, 2014 at 12:52 PM, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>> > By the way, it looks like there=E2=80=99s a JIRA plugin that integrat=
es it with
>>> > GitHub:
>>> >
>>> >    -
>>> >
>>> https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.ji=
ra-bitbucket-connector-plugin
>>>
>>> >    -
>>> >
>>> https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+an=
d+GitHub+accounts+to+JIRA
>>> >
>>> > It does the automatic linking and shows some additional information
>>> > <
>>> https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.p=
lugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97=
.png
>>> >
>>>
>>> > that might be nice to have for heavy JIRA users.
>>> >
>>> > Nick
>>> >
>>> >
>>> >
>>> > On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com=
>
>>> > wrote:
>>> >
>>> >> Yeah it needs to have SPARK-XXX in the title (this is the format we
>>> >> request already). It just works with small synchronization script I
>>> >> wrote that we run every five minutes on Jeknins that uses the Github
>>> >> and Jenkins API:
>>> >>
>>> >>
>>> >>
>>> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd1=
39232929
>>> >>
>>> >> - Patrick
>>> >>
>>> >> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
>>> >> <nicholas.chammas@gmail.com> wrote:
>>> >> > That's pretty neat.
>>> >> >
>>> >> > How does it work? Do we just need to put the issue ID (e.g.
>>> SPARK-1234)
>>> >> > anywhere in the pull request?
>>> >> >
>>> >> > Nick
>>> >> >
>>> >> >
>>> >> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <
>>> pwendell@gmail.com>
>>> >> > wrote:
>>> >> >
>>> >> >> Just a small note, today I committed a tool that will automatical=
ly
>>> >> >> mirror pull requests to JIRA issues, so contributors will no long=
er
>>> >> >> have to manually post a pull request on the JIRA when they make
>>> one.
>>> >> >>
>>> >> >> It will create a "link" on the JIRA and also make a comment to
>>> trigger
>>> >> >> an e-mail to people watching.
>>> >> >>
>>> >> >> This should make some things easier, such as avoiding accidental
>>> >> >> duplicate effort on the same JIRA.
>>> >> >>
>>> >> >> - Patrick
>>> >> >>
>>> >>
>>> >
>>> >
>>>
>>
>>
>

--047d7bfd0776f7aa730501780cde--

From dev-return-9025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 18:14:08 2014
Return-Path: <dev-return-9025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19C87118AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 18:14:08 +0000 (UTC)
Received: (qmail 49229 invoked by uid 500); 25 Aug 2014 18:14:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49160 invoked by uid 500); 25 Aug 2014 18:14:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49143 invoked by uid 99); 25 Aug 2014 18:14:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 18:14:06 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 18:13:40 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1XLylv-0002cn-Hk
	for dev@spark.incubator.apache.org; Mon, 25 Aug 2014 11:13:39 -0700
Date: Mon, 25 Aug 2014 11:13:39 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408990419321-7989.post@n3.nabble.com>
In-Reply-To: <CA+B-+fzkk=RMbCT7m6veQwCWGq1Jn+-zegXD23sQwa91eSyVvw@mail.gmail.com>
References: <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com> <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com> <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com> <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com> <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com> <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com> <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com> <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com> <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com> <CA+B-+fzkk=RMbCT7m6veQwCWGq1Jn+-zegXD23sQwa91eSyVvw@mail.gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator
 failing
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am running the code with @rxin's patch in standalone mode.  In my case I am
registering "org.apache.spark.graphx.GraphKryoRegistrator" . 

Recently I started to see "com.esotericsoftware.kryo.KryoException:
java.io.IOException: failed to uncompress the chunk: PARSING_ERROR" . Has
anyone seen this? Could it be related to this issue? > Here it trace: 
--
vids (org.apache.spark.graphx.impl.VertexAttributeBlock)
        com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
        com.esotericsoftware.kryo.io.Input.require(Input.java:169)
        com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:710)
        com.esotericsoftware.kryo.io.Input.readLong(Input.java:665)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:127)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:107)
        com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
       
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
       
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
       
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
       
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
       
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1054)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
       
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
       
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
       
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
       
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:87)
       
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:85)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
       
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
       
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:202)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

--




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-2878-Kryo-serialisation-with-custom-Kryo-registrator-failing-tp7719p7989.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 19:03:15 2014
Return-Path: <dev-return-9026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39D8E11AFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 19:03:15 +0000 (UTC)
Received: (qmail 90832 invoked by uid 500); 25 Aug 2014 19:03:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90761 invoked by uid 500); 25 Aug 2014 19:03:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90738 invoked by uid 99); 25 Aug 2014 19:03:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:03:14 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:02:47 +0000
Received: by mail-pd0-f181.google.com with SMTP id g10so20201461pdj.26
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 12:02:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=+DB34a4zqwnuL6sxD9FD5qRKJvamO8PHOe7H1uGuUcM=;
        b=QnveODX+fKucBVZdt5VTJetqHdSEbezREdNPXGNbzDchMQsIuCUPFakYJ4uicllKjs
         X8SC7Dl/BSfw1SaSAyHPrSVBR5fmjs8FGfPLkyTpXJjCoXa7NCd4kUIxP5QRdDDH5ZnG
         nCIVkTBNtqGs8HC8rtjLCpUYaS9thXARUbqWDWrvjaBXKfq8KNkHRMplNoXOAWpwrIHT
         gsLYseCu6H/DisBqRZ7pKFRB5P/hJgjmYbzJt+ZVoC2jPd60PJRY7zibuFuy+mwSq0Ly
         p9e9DiBId9k0ujPLSvso3wzBexAyJiiQvEXaKuWAZLAlnXLTHoKC3lKUv7JoJPmF1O0c
         eO4w==
X-Received: by 10.70.65.34 with SMTP id u2mr21416490pds.58.1408993365752;
        Mon, 25 Aug 2014 12:02:45 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id fl15sm787218pdb.92.2014.08.25.12.02.39
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 12:02:40 -0700 (PDT)
Date: Mon, 25 Aug 2014 12:02:39 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Timothy Chen <tnachen@gmail.com>, Gary Malouf
 <malouf.gary@gmail.com>, dev@mesos.apache.org
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, 
 "=?utf-8?Q?dev=40mesos.apache.org?=" <dev@mesos.apache.org>
Message-ID: <etPan.53fb884f.1190cde7.9a@mbp-3>
In-Reply-To: <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
 <etPan.53f920b8.ded7263.ae21@mbp-3.local>
 <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
 <etPan.53fac0f0.643c9869.9a@mbp-3.local>
 <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
 <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fb884f_66ef438d_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fb884f_66ef438d_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

BTW it seems to me that even without that patch, you should be getting tasks launched as long as you leave at least 32 MB of memory free on each machine (that is, the sum of the executor memory sizes is not exactly the same as the total size of the machine). Then Mesos will be able to re-offer that machine whenever CPUs free up.

Matei

On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary@gmail.com) wrote:

We have not tried the work-around because there are other bugs in there 
that affected our set-up, though it seems it would help. 


On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote: 

> +1 to have the work around in. 
> 
> I'll be investigating from the Mesos side too. 
> 
> Tim 
> 
> On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com> 
> wrote: 
> > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's too 
> bad that this happens in fine-grained mode -- would be really good to fix. 
> I'll see if we can get the workaround in 
> https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally 
> have you tried that? 
> > 
> > Matei 
> > 
> > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com) 
> wrote: 
> > 
> > Hi Matei, 
> > 
> > We have an analytics team that uses the cluster on a daily basis. They 
> use two types of 'run modes': 
> > 
> > 1) For running actual queries, they set the spark.executor.memory to 
> something between 4 and 8GB of RAM/worker. 
> > 
> > 2) A shell that takes a minimal amount of memory on workers (128MB) for 
> prototyping out a larger query. This allows them to not take up RAM on the 
> cluster when they do not really need it. 
> > 
> > We see the deadlocks when there are a few shells in either case. From 
> the usage patterns we have, coarse-grained mode would be a challenge as we 
> have to constantly remind people to kill their shells as soon as their 
> queries finish. 
> > 
> > Am I correct in viewing Mesos in coarse-grained mode as being similar to 
> Spark Standalone's cpu allocation behavior? 
> > 
> > 
> > 
> > 
> > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia@gmail.com> 
> wrote: 
> > Hey Gary, just as a workaround, note that you can use Mesos in 
> coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold 
> onto CPUs for the duration of the job. 
> > 
> > Matei 
> > 
> > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com) 
> wrote: 
> > 
> > I just wanted to bring up a significant Mesos/Spark issue that makes the 
> > combo difficult to use for teams larger than 4-5 people. It's covered in 
> > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is 
> that 
> > Spark's use of executors in fine-grained mode is a very different 
> behavior 
> > than many of the other common frameworks for Mesos. 
> > 
> 

--53fb884f_66ef438d_9a--


From dev-return-9027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 19:09:25 2014
Return-Path: <dev-return-9027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 579E511B58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 19:09:25 +0000 (UTC)
Received: (qmail 8970 invoked by uid 500); 25 Aug 2014 19:09:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8898 invoked by uid 500); 25 Aug 2014 19:09:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8887 invoked by uid 99); 25 Aug 2014 19:09:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:09:24 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of amnon.is@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:09:19 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <amnon.is@gmail.com>)
	id 1XLzdT-0005mV-4C
	for dev@spark.incubator.apache.org; Mon, 25 Aug 2014 12:08:59 -0700
Date: Mon, 25 Aug 2014 12:08:59 -0700 (PDT)
From: amnonkhen <amnon.is@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1408993739123-7991.post@n3.nabble.com>
In-Reply-To: <1408668974535-7950.post@n3.nabble.com>
References: <1407667288137-7795.post@n3.nabble.com> <1408668974535-7950.post@n3.nabble.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi jerryye,
Maybe if you voted up my question on Stack Overflow it would get some
traction and we would get nearer to a solution.
Thanks,
  Amnon



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 19:17:13 2014
Return-Path: <dev-return-9028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53E3511BC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 19:17:13 +0000 (UTC)
Received: (qmail 26812 invoked by uid 500); 25 Aug 2014 19:17:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26748 invoked by uid 500); 25 Aug 2014 19:17:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26737 invoked by uid 99); 25 Aug 2014 19:17:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:17:12 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:16:46 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so15853488vcb.10
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 12:16:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6ynqO2/pVJAAjK8ptSkljZO2BShPlMBG3TxxY6+BB+Y=;
        b=WpwbNE2vOkUOq8095GrtYqeXzb/ipRdXY0gURJU/BI0QAZ/Di6JLRJHg8erVT1piHQ
         r8EOgHf2cVcM6eQ0pRLtA48gP/VmrctZOO1ETKN2lnJ3FoXWf8nYqgO5qz/x3mqUjwqx
         344U3KAw9SIKw0THpTULEUUTngy6O3c4W9pJQkGDNnZxSCMoUyeNzv+KigC0ACq9Qn3k
         okoKX/lhIzBI2JSZ9ywxMny9JxzWZ+M0Ijy0+RpZq5z99E5kb36KWR85uF2zaQcQ46wb
         22ZFa5vc776D5p08UPJwC5sCB9DCzhJRTpu7Plxs5vcDUF31qPLnyh6CeLKACKoZtV3D
         G2ng==
X-Gm-Message-State: ALoCoQm/OFiolhV1ghTa5pbiecfFQl+l6ya9pYPW0c72E0vuigkoZsuTd8gw/3ZsCfgaEcVXAPbM
X-Received: by 10.220.182.1 with SMTP id ca1mr20010737vcb.21.1408994205029;
        Mon, 25 Aug 2014 12:16:45 -0700 (PDT)
Received: from mail-vc0-f175.google.com (mail-vc0-f175.google.com [209.85.220.175])
        by mx.google.com with ESMTPSA id vi19sm2145492vdb.16.2014.08.25.12.16.43
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 12:16:43 -0700 (PDT)
Received: by mail-vc0-f175.google.com with SMTP id ik5so15531802vcb.20
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 12:16:43 -0700 (PDT)
X-Received: by 10.220.59.65 with SMTP id k1mr19888202vch.22.1408994203405;
 Mon, 25 Aug 2014 12:16:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Mon, 25 Aug 2014 12:16:23 -0700 (PDT)
In-Reply-To: <CA+-p3AGpj9ghR0Ci14bq3RJQCU2goCJsQG==aWECDahkyddc5w@mail.gmail.com>
References: <1408726220083-7956.post@n3.nabble.com> <CA+-p3AFtG+Ut4zVFKrvON4-zqjCu_+V6G8SJpMbVkUss8_pr=g@mail.gmail.com>
 <1408737163759-7958.post@n3.nabble.com> <CA+-p3AGpj9ghR0Ci14bq3RJQCU2goCJsQG==aWECDahkyddc5w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 25 Aug 2014 12:16:23 -0700
Message-ID: <CA+-p3AGyG6P9s3HOWNWPtOsixOqsmU+3JG=28-_K44qUFaDODw@mail.gmail.com>
Subject: Re: take() reads every partition if the first one is empty
To: pnepywoda <pnepywoda@palantir.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c29502fc14850501790695
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29502fc14850501790695
Content-Type: text/plain; charset=UTF-8

Filed as https://issues.apache.org/jira/browse/SPARK-3211


On Fri, Aug 22, 2014 at 1:06 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Yep, anyone can create a bug at
> https://issues.apache.org/jira/browse/SPARK
>
> Then if you make a pull request on GitHub and have the bug number in the
> header like "[SPARK-1234] Make take() less OOM-prone", then the PR gets
> linked to the Jira ticket.  I think that's the best way to get feedback on
> a fix.
>
>
> On Fri, Aug 22, 2014 at 12:52 PM, pnepywoda <pnepywoda@palantir.com>
> wrote:
>
>> What's the process at this point? Does someone make a bug? Should I make a
>> bug? (do I even have permission to?)
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/take-reads-every-partition-if-the-first-one-is-empty-tp7956p7958.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a11c29502fc14850501790695--

From dev-return-9029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 19:28:42 2014
Return-Path: <dev-return-9029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17D6711C2B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 19:28:42 +0000 (UTC)
Received: (qmail 50049 invoked by uid 500); 25 Aug 2014 19:28:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50001 invoked by uid 500); 25 Aug 2014 19:28:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49875 invoked by uid 99); 25 Aug 2014 19:28:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:28:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 19:28:30 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so3334311igd.17
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 12:28:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ylhxITOvNc4TLNQDvhpP1etSFJC0dRU/bTy8kG7RO4s=;
        b=C+tqSPchOrYl6eUJLWQhGlbM8A3R+zS/2xg03/fl5KL2IV2mr1hwAdZDn5ma/1F/TG
         Q9fWqwUQPdy/UUl6Of7Kicxy8IkQnTsTKY8U3ywwBchkEgd6YTRQMtpoSxzeE5JD5LtI
         T04Tqwv8RbO82JSx91JR2lPNXSiPS/deDQxavSMjfIUcHaM8rRZHzrZq9NkfnSJfMPfr
         io1c2Xsd6pAYaW3k0KB7ZrHXCtN1KP3EtSmxSOtSTi28dC2N8hCsmGDHacAoDCvgFwTL
         fYwV4g0A3IGJRFBCsFzjJ5VrHAs9zgEllyu2K5IXFvqj4NGEYDzj1BOPDkFcOuFvPlrA
         GUKw==
MIME-Version: 1.0
X-Received: by 10.50.66.197 with SMTP id h5mr17713023igt.34.1408994890060;
 Mon, 25 Aug 2014 12:28:10 -0700 (PDT)
Received: by 10.107.152.196 with HTTP; Mon, 25 Aug 2014 12:28:10 -0700 (PDT)
In-Reply-To: <49229E870391FC49BBBED818C268753D705A644F@SZXEMA501-MBX.china.huawei.com>
References: <49229E870391FC49BBBED818C268753D705A644F@SZXEMA501-MBX.china.huawei.com>
Date: Mon, 25 Aug 2014 12:28:10 -0700
Message-ID: <CAJgQjQ9YQzHnxN+HEN4FGbQe=4Pgq1NkU4+oyodYE7e547FKaw@mail.gmail.com>
Subject: Re: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?
From: Xiangrui Meng <mengxr@gmail.com>
To: "Lizhengbing (bing, BIPA)" <zhengbing.li@huawei.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd6be0ee99c790501792fcb
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6be0ee99c790501792fcb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The evaluation metrics are definitely useful. How do they differ from
traditional IR metrics like prec@k and ndcg@k? -Xiangrui


On Mon, Aug 25, 2014 at 2:14 AM, Lizhengbing (bing, BIPA) <
zhengbing.li@huawei.com> wrote:

>  Hi:
>
> In paper =E2=80=9CItem-Based Top-N Recommendation Algorithms=E2=80=9D(
> https://stuyresearch.googlecode.com/hg/blake/resources/10.1.1.102.4451.pd=
f),
> there are two parameters measuring the quality of recommendation: HR and
> ARHR.
>
> If I use ALS(Implicit) for top-N recommendation system, I want to check
> it=E2=80=99s quality. ARHR and HR are two good quality measures.
>
> I want to contribute them to spark MLlib.  So I want to know whether this
> is meaningful?
>
>
>
>
>
> (1) If *n *is the total number of customers/users,  the hit-rate of the
> recommendation algorithm was computed as
>
> *hit-rate (HR) *=3D *Number of hits / n*
>
>
>
> (2)If *h *is the number of hits that occurred at positions *p*1, *p*2, *.
> . . *, *p**h *within the *top-N *lists (i.e., 1 =E2=89=A4 *p**i *=E2=89=
=A4 *N*), then the
> average reciprocal hit-rank is equal to:
>
> *i*
>
> *.*
>

--047d7bd6be0ee99c790501792fcb--

From dev-return-9030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:00:25 2014
Return-Path: <dev-return-9030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B973F11DA3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:00:25 +0000 (UTC)
Received: (qmail 34695 invoked by uid 500); 25 Aug 2014 20:00:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34633 invoked by uid 500); 25 Aug 2014 20:00:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34612 invoked by uid 99); 25 Aug 2014 20:00:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:00:24 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:00:20 +0000
Received: by mail-oi0-f48.google.com with SMTP id v63so1061936oia.21
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 12:59:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=4gjYfGdOQBl2uzrw2dBVuvVRSPaI5KF9O9BKXjvyC3E=;
        b=QB1DahcbQzLf1zCi5+GySRzrhvWBPKFUc6nGXS2Twgx9MBERqgo5QOefHqTmTK2Jks
         66osYlMueqgf1GUaJNgmaWGh5Yw327N2L84/LvjV3NaCEHIa8rKL3ftfFIz3YcV2nZr5
         XEyCCagdUdlR2PZ+zu3zezqsfTVERbNUFcFw/s9Orlz9aeIXRlo3VnlWtFzF673JWSPY
         Qh6S+7ELBM85uPX8E0d10n2UN8wyF86laQozgKDQ395MmFbPBYBDo0Qz46OUU3rezXIu
         LIl7xxkRd0BrayaD7ITyqPRSNgYdlVtrTHx24pQT/pvmPp/M5DDIqjPCND2gPMMzEHTm
         yi+g==
X-Gm-Message-State: ALoCoQmZ7LGFbj66ZwGUbZONl7kgwq/w5kF0CjH44iYMB8pOkWdlYIKYHVepzUopmzDUeraRSa2t
MIME-Version: 1.0
X-Received: by 10.60.135.10 with SMTP id po10mr17127199oeb.42.1408996799380;
 Mon, 25 Aug 2014 12:59:59 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Mon, 25 Aug 2014 12:59:59 -0700 (PDT)
In-Reply-To: <etPan.53fb884f.1190cde7.9a@mbp-3>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
	<CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
	<etPan.53fac0f0.643c9869.9a@mbp-3.local>
	<CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
	<CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
	<etPan.53fb884f.1190cde7.9a@mbp-3>
Date: Mon, 25 Aug 2014 14:59:59 -0500
Message-ID: <CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Cody Koeninger <cody@koeninger.org>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Timothy Chen <tnachen@gmail.com>, Gary Malouf <malouf.gary@gmail.com>, dev@mesos.apache.org, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f50370cb7bbcb050179a1d8
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f50370cb7bbcb050179a1d8
Content-Type: text/plain; charset=UTF-8

I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

My job immediately stopped making progress.  Once user a killed his shells,
it started again.

This is on nodes with ~15G of memory, on which we have successfully run 8G
jobs.


On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> BTW it seems to me that even without that patch, you should be getting
> tasks launched as long as you leave at least 32 MB of memory free on each
> machine (that is, the sum of the executor memory sizes is not exactly the
> same as the total size of the machine). Then Mesos will be able to re-offer
> that machine whenever CPUs free up.
>
> Matei
>
> On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary@gmail.com)
> wrote:
>
> We have not tried the work-around because there are other bugs in there
> that affected our set-up, though it seems it would help.
>
>
> On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote:
>
> > +1 to have the work around in.
> >
> > I'll be investigating from the Mesos side too.
> >
> > Tim
> >
> > On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> > > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's
> too
> > bad that this happens in fine-grained mode -- would be really good to
> fix.
> > I'll see if we can get the workaround in
> > https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
> > have you tried that?
> > >
> > > Matei
> > >
> > > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com)
> > wrote:
> > >
> > > Hi Matei,
> > >
> > > We have an analytics team that uses the cluster on a daily basis. They
> > use two types of 'run modes':
> > >
> > > 1) For running actual queries, they set the spark.executor.memory to
> > something between 4 and 8GB of RAM/worker.
> > >
> > > 2) A shell that takes a minimal amount of memory on workers (128MB) for
> > prototyping out a larger query. This allows them to not take up RAM on
> the
> > cluster when they do not really need it.
> > >
> > > We see the deadlocks when there are a few shells in either case. From
> > the usage patterns we have, coarse-grained mode would be a challenge as
> we
> > have to constantly remind people to kill their shells as soon as their
> > queries finish.
> > >
> > > Am I correct in viewing Mesos in coarse-grained mode as being similar
> to
> > Spark Standalone's cpu allocation behavior?
> > >
> > >
> > >
> > >
> > > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <
> matei.zaharia@gmail.com>
> > wrote:
> > > Hey Gary, just as a workaround, note that you can use Mesos in
> > coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold
> > onto CPUs for the duration of the job.
> > >
> > > Matei
> > >
> > > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com)
> > wrote:
> > >
> > > I just wanted to bring up a significant Mesos/Spark issue that makes
> the
> > > combo difficult to use for teams larger than 4-5 people. It's covered
> in
> > > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is
> > that
> > > Spark's use of executors in fine-grained mode is a very different
> > behavior
> > > than many of the other common frameworks for Mesos.
> > >
> >
>

--e89a8f50370cb7bbcb050179a1d8--

From dev-return-9031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:07:44 2014
Return-Path: <dev-return-9031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DCC611DCE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:07:44 +0000 (UTC)
Received: (qmail 50125 invoked by uid 500); 25 Aug 2014 20:07:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50060 invoked by uid 500); 25 Aug 2014 20:07:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50048 invoked by uid 99); 25 Aug 2014 20:07:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:07:43 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:07:17 +0000
Received: by mail-pd0-f181.google.com with SMTP id g10so20614702pdj.40
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:07:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=iXiGPl6B2WXdeHVRW7FAgZWukDq17Ii4dDSvUzVu92k=;
        b=yAfNAOULeA2IdugmirMjeO0ygL6PGlIN7p0p0yPQ0VnZC1fEEdh6a0uZ+eFxRjivBF
         nUw0cQX31+5z2I31g3faDmwi6fPinP+2WQFmGeG/ba5cvM48S7Dm/rbyCdk06m6ArT4A
         v6+w0n5to1h6ce2Lbp8fii+IuWDuNBnaYHbrqOq299Y13S9hyCZVsUXnS1qjX2sW4wwx
         lHb8iDvGEywhM5y4pzDQ1EQu53VCdkN1rDi9g9/0yXRAD0IRd2zhPJSOhf5ww9edLyE/
         B2oy2ayo7b0y1dtT4pr6Ii9GpbKl5P7ABq01C5c2OXDO0OE0sMas+PwgkSMDFp59jYTw
         zvjw==
X-Received: by 10.68.195.228 with SMTP id ih4mr7063674pbc.167.1408997235439;
        Mon, 25 Aug 2014 13:07:15 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id tc2sm680642pbc.30.2014.08.25.13.07.13
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 13:07:14 -0700 (PDT)
Date: Mon, 25 Aug 2014 13:07:13 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, Timothy
 Chen <tnachen@gmail.com>, Gary Malouf <malouf.gary@gmail.com>, 
 dev@mesos.apache.org
Message-ID: <etPan.53fb9771.41a7c4c9.9a@mbp-3>
In-Reply-To: <CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
 <etPan.53f920b8.ded7263.ae21@mbp-3.local>
 <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
 <etPan.53fac0f0.643c9869.9a@mbp-3.local>
 <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
 <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
 <etPan.53fb884f.1190cde7.9a@mbp-3>
 <CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fb9771_6b68079a_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fb9771_6b68079a_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

This is kind of weird then, seems perhaps unrelated to this issue (or at =
least to the way I understood it). Is the problem maybe that Mesos saw 0 =
MB being freed and didn't re-offer the machine *even though there was mor=
e than 32 MB free overall*=3F

Matei

On August 25, 2014 at 12:59:59 PM, Cody Koeninger (cody=40koeninger.org) =
wrote:

I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

My job immediately stopped making progress.=C2=A0 Once user a killed his =
shells, it started again.

This is on nodes with =7E15G of memory, on which we have successfully run=
 8G jobs.


On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia=40gmail.com=
> wrote:
BTW it seems to me that even without that patch, you should be getting ta=
sks launched as long as you leave at least 32 MB of memory free on each m=
achine (that is, the sum of the executor memory sizes is not exactly the =
same as the total size of the machine). Then Mesos will be able to re-off=
er that machine whenever CPUs free up.

Matei

On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary=40gmail.com) w=
rote:

We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.


On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen=40gmail.com> wrot=
e:

> +1 to have the work around in.
>
> I'll be investigating from the Mesos side too.
>
> Tim
>
> On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia=40gmail.c=
om>
> wrote:
> > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's =
too
> bad that this happens in fine-grained mode -- would be really good to f=
ix.
> I'll see if we can get the workaround in
> https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
> have you tried that=3F
> >
> > Matei
> >
> > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary=40gmail.co=
m)
> wrote:
> >
> > Hi Matei,
> >
> > We have an analytics team that uses the cluster on a daily basis. The=
y
> use two types of 'run modes':
> >
> > 1) =46or running actual queries, they set the spark.executor.memory t=
o
> something between 4 and 8GB of RAM/worker.
> >
> > 2) A shell that takes a minimal amount of memory on workers (128MB) f=
or
> prototyping out a larger query. This allows them to not take up RAM on =
the
> cluster when they do not really need it.
> >
> > We see the deadlocks when there are a few shells in either case. =46r=
om
> the usage patterns we have, coarse-grained mode would be a challenge as=
 we
> have to constantly remind people to kill their shells as soon as their
> queries finish.
> >
> > Am I correct in viewing Mesos in coarse-grained mode as being similar=
 to
> Spark Standalone's cpu allocation behavior=3F
> >
> >
> >
> >
> > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia=40gmail=
.com>
> wrote:
> > Hey Gary, just as a workaround, note that you can use Mesos in
> coarse-grained mode by setting spark.mesos.coarse=3Dtrue. Then it will =
hold
> onto CPUs for the duration of the job.
> >
> > Matei
> >
> > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary=40gmail.co=
m)
> wrote:
> >
> > I just wanted to bring up a significant Mesos/Spark issue that makes =
the
> > combo difficult to use for teams larger than 4-5 people. It's covered=
 in
> > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is=

> that
> > Spark's use of executors in fine-grained mode is a very different
> behavior
> > than many of the other common frameworks for Mesos.
> >
>


--53fb9771_6b68079a_9a--


From dev-return-9032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:08:49 2014
Return-Path: <dev-return-9032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6A8011DD3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:08:49 +0000 (UTC)
Received: (qmail 51832 invoked by uid 500); 25 Aug 2014 20:08:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51761 invoked by uid 500); 25 Aug 2014 20:08:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51749 invoked by uid 99); 25 Aug 2014 20:08:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:08:48 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:08:44 +0000
Received: by mail-pd0-f180.google.com with SMTP id v10so20904033pde.11
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:08:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=rgI8kCyppCEarBd4w7UvB7MwpCWbsYGJW73P9k3AZFE=;
        b=ns7O4Gql5nvjW2MaIxDKjCYu2ESAsKEuLURGVKshrn1jX+Zg+pr9Zi/tT1M/kuUveI
         XDwTWXQgwTiyD6MhJ1m9URCOqkida1T4h5RKOvlCmqvpuM6ixv9PG59Q1eNTkDKfIPaQ
         ThW0yP/8delse9P8hQCJMylAjScn8sS3FDDOQpnSoGmTvAa1ak8E6hC5FFFIEnMgG0dy
         6Rhu/cyZxdniSo6UPPL2m8RAHCRcwJJZAbrMiMY6r8bsuIb1rNBGPiJha6P8j60fJwvy
         pMyjYjWyy5ofCFucH3yiO503YqnLPLW15jPnOUC0h59brlyYVy20Mit0DbZHsXerKRTp
         y8Sg==
X-Received: by 10.68.222.168 with SMTP id qn8mr31661016pbc.114.1408997303694;
        Mon, 25 Aug 2014 13:08:23 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ad10sm2731780pad.4.2014.08.25.13.08.22
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 13:08:23 -0700 (PDT)
Date: Mon, 25 Aug 2014 13:08:21 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: dev@mesos.apache.org, Timothy Chen <tnachen@gmail.com>, Gary Malouf
 <malouf.gary@gmail.com>, "=?utf-8?Q?dev=40spark.apache.org?="
 <dev@spark.apache.org>
Message-ID: <etPan.53fb97b5.4e6afb66.9a@mbp-3>
In-Reply-To: <etPan.53fb9771.41a7c4c9.9a@mbp-3>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
 <etPan.53f920b8.ded7263.ae21@mbp-3.local>
 <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
 <etPan.53fac0f0.643c9869.9a@mbp-3.local>
 <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
 <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
 <etPan.53fb884f.1190cde7.9a@mbp-3>
 <CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
 <etPan.53fb9771.41a7c4c9.9a@mbp-3>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fb97b5_25e45d32_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fb97b5_25e45d32_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Anyway it would be good if someone from the Mesos side investigates this =
and proposes a solution. The 32 MB per task hack isn't completely foolpro=
of either (e.g. people might allocate all the RAM to their executor and t=
hus stop being able to launch tasks), so maybe we wait on a Mesos fix for=
 this one.

Matei

On August 25, 2014 at 1:07:15 PM, Matei Zaharia (matei.zaharia=40gmail.co=
m) wrote:

This is kind of weird then, seems perhaps unrelated to this issue (or at =
least to the way I understood it). Is the problem maybe that Mesos saw 0 =
MB being freed and didn't re-offer the machine *even though there was mor=
e than 32 MB free overall*=3F

Matei

On August 25, 2014 at 12:59:59 PM, Cody Koeninger (cody=40koeninger.org) =
wrote:

I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

My job immediately stopped making progress.=C2=A0 Once user a killed his =
shells, it started again.

This is on nodes with =7E15G of memory, on which we have successfully run=
 8G jobs.


On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia=40gmail.com=
> wrote:
BTW it seems to me that even without that patch, you should be getting ta=
sks launched as long as you leave at least 32 MB of memory free on each m=
achine (that is, the sum of the executor memory sizes is not exactly the =
same as the total size of the machine). Then Mesos will be able to re-off=
er that machine whenever CPUs free up.

Matei

On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary=40gmail.com) w=
rote:

We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.


On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen=40gmail.com> wrot=
e:

> +1 to have the work around in.
>
> I'll be investigating from the Mesos side too.
>
> Tim
>
> On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia=40gmail.c=
om>
> wrote:
> > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's =
too
> bad that this happens in fine-grained mode -- would be really good to f=
ix.
> I'll see if we can get the workaround in
> https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
> have you tried that=3F
> >
> > Matei
> >
> > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary=40gmail.co=
m)
> wrote:
> >
> > Hi Matei,
> >
> > We have an analytics team that uses the cluster on a daily basis. The=
y
> use two types of 'run modes':
> >
> > 1) =46or running actual queries, they set the spark.executor.memory t=
o
> something between 4 and 8GB of RAM/worker.
> >
> > 2) A shell that takes a minimal amount of memory on workers (128MB) f=
or
> prototyping out a larger query. This allows them to not take up RAM on =
the
> cluster when they do not really need it.
> >
> > We see the deadlocks when there are a few shells in either case. =46r=
om
> the usage patterns we have, coarse-grained mode would be a challenge as=
 we
> have to constantly remind people to kill their shells as soon as their
> queries finish.
> >
> > Am I correct in viewing Mesos in coarse-grained mode as being similar=
 to
> Spark Standalone's cpu allocation behavior=3F
> >
> >
> >
> >
> > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia <matei.zaharia=40gmail=
.com>
> wrote:
> > Hey Gary, just as a workaround, note that you can use Mesos in
> coarse-grained mode by setting spark.mesos.coarse=3Dtrue. Then it will =
hold
> onto CPUs for the duration of the job.
> >
> > Matei
> >
> > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary=40gmail.co=
m)
> wrote:
> >
> > I just wanted to bring up a significant Mesos/Spark issue that makes =
the
> > combo difficult to use for teams larger than 4-5 people. It's covered=
 in
> > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is=

> that
> > Spark's use of executors in fine-grained mode is a very different
> behavior
> > than many of the other common frameworks for Mesos.
> >
>


--53fb97b5_25e45d32_9a--


From dev-return-9033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:09:30 2014
Return-Path: <dev-return-9033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30F8211DDB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:09:30 +0000 (UTC)
Received: (qmail 55381 invoked by uid 500); 25 Aug 2014 20:09:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55315 invoked by uid 500); 25 Aug 2014 20:09:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55303 invoked by uid 99); 25 Aug 2014 20:09:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:09:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:09:25 +0000
Received: by mail-lb0-f174.google.com with SMTP id u10so245335lbd.33
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:09:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=0vJBqPv8TDW8GqX641O2tRpP8mqyPT3QU80Pl8h9xiA=;
        b=TvjnWOi5lKUD1XcPrW+lWvyX8FmB3wZ/52B8fz7c9GO/F1768GvWndoNRGcimEkOOn
         dXrYgnCe+ZwameDYMTqjPX8dJRVoD/dbDKc3w6abzAnVUettdAgJqqXGLJ6WKENsBb/w
         QqSvCjEsEFyXrhTQighB+25a31rjzBb8ysjgaEO0um5YzpjFo0QvS8nez7UXj27GZmgB
         6rlK1cVZ55+z+ujtZnCIEkRF7ipmUhvs0NY0ncijS8SolKJXsYaHlKL9DeKtrSAegBBC
         Th9Hih83vtp2sizdJJ3IOro8csttfRCPTbM2Dn+DTIJAUEXCZvdkDY4WMKGrik0aB7NE
         Sc0Q==
X-Gm-Message-State: ALoCoQmpl8LRX7IC8cHEzLFP5xUja25HM3lw4RlQ7BlTB3AnR1CAX8ntDeKfPDErdhu6/gO/MRr5
X-Received: by 10.112.160.38 with SMTP id xh6mr22298835lbb.21.1408997342566;
 Mon, 25 Aug 2014 13:09:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.30.5 with HTTP; Mon, 25 Aug 2014 13:08:42 -0700 (PDT)
In-Reply-To: <53FA9B5C.7060605@huawei.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
 <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
 <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
 <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
 <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
 <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
 <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
 <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <53FA9B5C.7060605@huawei.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 25 Aug 2014 13:08:42 -0700
Message-ID: <CAAswR-79v9vWH7b_ik_KzMGigu0XeKW56OqVgnOT9ORFRhX=_Q@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: scwf <wangfei1@huawei.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2b81617ec66050179c288
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b81617ec66050179c288
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for working on this!  Its unclear at the moment exactly how we are
going to handle this, since the end goal is to be compatible with as many
versions of Hive as possible.  That said, I think it would be great to open
a PR in this case.  Even if we don't merge it, thats a good way to get it
on people's radar and have a discussion about the changes that are required=
.


On Sun, Aug 24, 2014 at 7:11 PM, scwf <wangfei1@huawei.com> wrote:

>   I have worked for a branch update the hive version to hive-0.13(by
> org.apache.hive)---https://github.com/scwf/spark/tree/hive-0.13
> I am wondering whether it's ok to make a PR now because hive-0.13 version
> is not compatible with hive-0.12 and here i used org.apache.hive.
>
>
>
> On 2014/7/29 8:22, Michael Armbrust wrote:
>
>> A few things:
>>   - When we upgrade to Hive 0.13.0, Patrick will likely republish the
>> hive-exec jar just as we did for 0.12.0
>>   - Since we have to tie into some pretty low level APIs it is
>> unsurprising
>> that the code doesn't just compile out of the box against 0.13.0
>>   - ScalaReflection is for determining Schema from Scala classes, not
>> reflection based bridge code.  Either way its unclear to if there is any
>> reason to use reflection to support multiple versions, instead of just
>> upgrading to Hive 0.13.0
>>
>> One question I have is, What is the goal of upgrading to hive 0.13.0?  I=
s
>> it purely because you are having problems connecting to newer metastores=
?
>>   Are there some features you are hoping for?  This will help me
>> prioritize
>> this effort.
>>
>> Michael
>>
>>
>> On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>>  I was looking for a class where reflection-related code should reside.
>>>
>>> I found this but don't think it is the proper class for bridging
>>> differences between hive 0.12 and 0.13.1:
>>>
>>> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/
>>> ScalaReflection.scala
>>>
>>> Cheers
>>>
>>>
>>> On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>>
>>>  After manually copying hive 0.13.1 jars to local maven repo, I got the
>>>> following errors when building spark-hive_2.10 module :
>>>>
>>>> [ERROR]
>>>>
>>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
>>> sql/hive/HiveContext.scala:182:
>>>
>>>> type mismatch;
>>>>   found   : String
>>>>   required: Array[String]
>>>> [ERROR]       val proc: CommandProcessor =3D
>>>> CommandProcessorFactory.get(tokens(0), hiveconf)
>>>> [ERROR]
>>>>     ^
>>>> [ERROR]
>>>>
>>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
>>> sql/hive/HiveMetastoreCatalog.scala:60:
>>>
>>>> value getAllPartitionsForPruner is not a member of org.apache.
>>>>   hadoop.hive.ql.metadata.Hive
>>>> [ERROR]         client.getAllPartitionsForPruner(table).toSeq
>>>> [ERROR]                ^
>>>> [ERROR]
>>>>
>>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
>>> sql/hive/HiveMetastoreCatalog.scala:267:
>>>
>>>> overloaded method constructor TableDesc with alternatives:
>>>>    (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
>>>> Class[_],x$3:
>>>>
>>> java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
>>>
>>>> <and>
>>>>    ()org.apache.hadoop.hive.ql.plan.TableDesc
>>>>   cannot be applied to (Class[org.apache.hadoop.hive.
>>>> serde2.Deserializer],
>>>> Class[(some other)?0(in value tableDesc)(in value tableDesc)],
>>>>
>>> Class[?0(in
>>>
>>>> value tableDesc)(in   value tableDesc)], java.util.Properties)
>>>> [ERROR]   val tableDesc =3D new TableDesc(
>>>> [ERROR]                   ^
>>>> [WARNING] Class org.antlr.runtime.tree.CommonTree not found -
>>>> continuing
>>>> with a stub.
>>>> [WARNING] Class org.antlr.runtime.Token not found - continuing with a
>>>>
>>> stub.
>>>
>>>> [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing wit=
h
>>>> a
>>>> stub.
>>>> [ERROR]
>>>>       while compiling:
>>>>
>>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
>>> sql/hive/HiveQl.scala
>>>
>>>>          during phase: typer
>>>>       library version: version 2.10.4
>>>>      compiler version: version 2.10.4
>>>>
>>>> The above shows incompatible changes between 0.12 and 0.13.1
>>>> e.g. the first error corresponds to the following method
>>>> in CommandProcessorFactory :
>>>>    public static CommandProcessor get(String[] cmd, HiveConf conf)
>>>>
>>>> Cheers
>>>>
>>>>
>>>> On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
>>>> wrote:
>>>>
>>>>  So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
>>>>>
>>>> adding
>>>
>>>> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look like
>>>>>
>>>> there=C2=B9s
>>>
>>>> a release date schedule for 0.14.
>>>>>
>>>>>
>>>>>
>>>>> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>>>>>
>>>>>  Exactly, forgot to mention Hulu team also made changes to cope with
>>>>>>
>>>>> those
>>>
>>>> incompatibility issues, but they said that=C2=B9s relatively easy once=
 the
>>>>>> re-packaging work is done.
>>>>>>
>>>>>>
>>>>>> On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com=
>
>>>>>>
>>>>>
>>>>>  wrote:
>>>>>>
>>>>>>  I've heard from Cloudera that there were hive internal changes
>>>>>>>
>>>>>> between
>>>
>>>>  0.12 and 0.13 that required code re-writing. Over time it might be
>>>>>>> possible for us to integrate with hive using API's that are more
>>>>>>> stable (this is the domain of Michael/Cheng/Yin more than me!). It
>>>>>>> would be interesting to see what the Hulu folks did.
>>>>>>>
>>>>>>> - Patrick
>>>>>>>
>>>>>>> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com=
>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> AFAIK, according a recent talk, Hulu team in China has built Spark
>>>>>>>>
>>>>>>> SQL
>>>>>
>>>>>> against Hive 0.13 (or 0.13.1?) successfully. Basically they also
>>>>>>>> re-packaged Hive 0.13 as what the Spark team did. The slides of th=
e
>>>>>>>>
>>>>>>> talk
>>>>>>>
>>>>>>>> hasn't been released yet though.
>>>>>>>>
>>>>>>>>
>>>>>>>> On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
>>>>>>>>
>>>>>>> wrote:
>>>
>>>>
>>>>>>>>  Owen helped me find this:
>>>>>>>>> https://issues.apache.org/jira/browse/HIVE-7423
>>>>>>>>>
>>>>>>>>> I guess this means that for Hive 0.14, Spark should be able to
>>>>>>>>>
>>>>>>>> directly
>>>>>>>
>>>>>>>> pull in hive-exec-core.jar
>>>>>>>>>
>>>>>>>>> Cheers
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
>>>>>>>>>
>>>>>>>> pwendell@gmail.com>
>>>>>
>>>>>>  wrote:
>>>>>>>>>
>>>>>>>>>  It would be great if the hive team can fix that issue. If not,
>>>>>>>>>>
>>>>>>>>> we'll
>>>>>>>
>>>>>>>> have to continue forking our own version of Hive to change the
>>>>>>>>>>
>>>>>>>>> way
>>>
>>>>  it
>>>>>>>
>>>>>>>> publishes artifacts.
>>>>>>>>>>
>>>>>>>>>> - Patrick
>>>>>>>>>>
>>>>>>>>>> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
>>>>>>>>>>
>>>>>>>>> wrote:
>>>>>>>
>>>>>>>>  Talked with Owen offline. He confirmed that as of 0.13,
>>>>>>>>>>>
>>>>>>>>>> hive-exec is
>>>>>>>
>>>>>>>> still
>>>>>>>>>>
>>>>>>>>>>> uber jar.
>>>>>>>>>>>
>>>>>>>>>>> Right now I am facing the following error building against
>>>>>>>>>>>
>>>>>>>>>> Hive
>>>
>>>>  0.13.1
>>>>>>>
>>>>>>>> :
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>> [ERROR] Failed to execute goal on project spark-hive_2.10:
>>>>>>>>>>>
>>>>>>>>>> Could
>>>
>>>>  not
>>>>>>>
>>>>>>>>  resolve dependencies for project
>>>>>>>>>>> org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>>>>>>>>>>>
>>>>>>>>>> following
>>>>>>>
>>>>>>>>  artifacts could not be resolved:
>>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1,
>>>>>>>>>>> org.spark-project.hive:hive-exec:jar:0.13.1,
>>>>>>>>>>> org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1 in
>>>>>>>>>>> http://repo.maven.apache.org/maven2 was cached in the local
>>>>>>>>>>>
>>>>>>>>>> repository,
>>>>>>>>>
>>>>>>>>>> resolution will not be reattempted until the update interval
>>>>>>>>>>>
>>>>>>>>>> of
>>>
>>>>  maven-repo
>>>>>>>>>>
>>>>>>>>>>> has elapsed or updates are forced -> [Help 1]
>>>>>>>>>>>
>>>>>>>>>>> Some hint would be appreciated.
>>>>>>>>>>>
>>>>>>>>>>> Cheers
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
>>>>>>>>>>>
>>>>>>>>>> sowen@cloudera.com>
>>>
>>>>  wrote:
>>>>>>>
>>>>>>>>
>>>>>>>>>>>  Yes, it is published. As of previous versions, at least,
>>>>>>>>>>>>
>>>>>>>>>>> hive-exec
>>>>>>>
>>>>>>>>  included all of its dependencies *in its artifact*, making it
>>>>>>>>>>>>
>>>>>>>>>>> unusable
>>>>>>>
>>>>>>>>  as-is because it contained copies of dependencies that clash
>>>>>>>>>>>>
>>>>>>>>>>> with
>>>>>>>
>>>>>>>>  versions present in other artifacts, and can't be managed
>>>>>>>>>>>>
>>>>>>>>>>> with
>>>
>>>>  Maven
>>>>>>>
>>>>>>>>  mechanisms.
>>>>>>>>>>>>
>>>>>>>>>>>> I am not sure why hive-exec was not published normally, with
>>>>>>>>>>>>
>>>>>>>>>>> just
>>>>>>> its
>>>>>>>
>>>>>>>>  own classes. That's why it was copied, into an artifact with
>>>>>>>>>>>>
>>>>>>>>>>> just
>>>>>>>
>>>>>>>>  hive-exec code.
>>>>>>>>>>>>
>>>>>>>>>>>> You could do the same thing for hive-exec 0.13.1.
>>>>>>>>>>>> Or maybe someone knows that it's published more 'normally'
>>>>>>>>>>>>
>>>>>>>>>>> now.
>>>
>>>>  I don't think hive-metastore is related to this question?
>>>>>>>>>>>>
>>>>>>>>>>>> I am no expert on the Hive artifacts, just remembering what
>>>>>>>>>>>>
>>>>>>>>>>> the
>>>
>>>>  issue
>>>>>>>
>>>>>>>>  was initially in case it helps you get to a similar solution.
>>>>>>>>>>>>
>>>>>>>>>>>> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>  wrote:
>>>>>>>
>>>>>>>>  hive-exec (as of 0.13.1) is published here:
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>  http://search.maven.org/#artifactdetails%7Corg.apache.
>>> hive%7Chive-exec%7C
>>>
>>>>  0.13.1%7Cjar
>>>>>>>
>>>>>>>>
>>>>>>>>>>>>> Should a JIRA be opened so that dependency on
>>>>>>>>>>>>>
>>>>>>>>>>>> hive-metastore
>>>
>>>>  can
>>>>>>> be
>>>>>>>
>>>>>>>>  replaced by dependency on hive-exec ?
>>>>>>>>>>>>>
>>>>>>>>>>>>> Cheers
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>>>>>>>>>>>>>
>>>>>>>>>>>> <sowen@cloudera.com>
>>>>>>>
>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>>>  The reason for org.spark-project.hive is that Spark relies
>>>>>>>>>>>>>>
>>>>>>>>>>>>> on
>>>>>
>>>>>>  hive-exec, but the Hive project does not publish this
>>>>>>>>>>>>>>
>>>>>>>>>>>>> artifact
>>>>>>> by
>>>>>>>
>>>>>>>>  itself, only with all its dependencies as an uber jar.
>>>>>>>>>>>>>>
>>>>>>>>>>>>> Maybe
>>>
>>>>  that's
>>>>>>>
>>>>>>>>  been improved. If so, you need to point at the new
>>>>>>>>>>>>>>
>>>>>>>>>>>>> hive-exec
>>>
>>>>  and
>>>>>>>
>>>>>>>>  perhaps sort out its dependencies manually in your build.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
>>>>>>>>>>>>>>
>>>>>>>>>>>>> yuzhihong@gmail.com>
>>>>>
>>>>>>  wrote:
>>>>>>>>>
>>>>>>>>>>  I found 0.13.1 artifacts in maven:
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>  http://search.maven.org/#artifactdetails%7Corg.apache.
>>> hive%7Chive-metasto
>>>
>>>>  re%7C0.13.1%7Cjar
>>>>>>>
>>>>>>>>
>>>>>>>>>>>>>>> However, Spark uses groupId of org.spark-project.hive,
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>> not
>>>
>>>>   org.apache.hive
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Can someone tell me how it is supposed to work ?
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Cheers
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>> snunez@hortonworks.com>
>>>>>>>>>>
>>>>>>>>>>>  wrote:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>  I saw a note earlier, perhaps on the user list, that at
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> least
>>>>>>>
>>>>>>>> one
>>>>>>>>>
>>>>>>>>>>  person is
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> using Hive 0.13. Anyone got a working build
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> configuration
>>>
>>>>  for
>>>>>>>
>>>>>>>> this
>>>>>>>>>
>>>>>>>>>>  version
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> of Hive?
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> Regards,
>>>>>>>>>>>>>>>> - Steve
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>>> CONFIDENTIALITY NOTICE
>>>>>>>>>>>>>>>> NOTICE: This message is intended for the use of the
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> individual
>>>>>>>
>>>>>>>> or
>>>>>>>>>
>>>>>>>>>>  entity to
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> which it is addressed and may contain information that
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> is
>>>
>>>>  confidential,
>>>>>>>>>>>>
>>>>>>>>>>>>>  privileged and exempt from disclosure under applicable
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> law.
>>>>>>> If
>>>>>>>
>>>>>>>> the
>>>>>>>>>
>>>>>>>>>>  reader
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> of this message is not the intended recipient, you are
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> hereby
>>>>>>>
>>>>>>>>  notified
>>>>>>>>>>>>
>>>>>>>>>>>>> that
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> any printing, copying, dissemination, distribution,
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> disclosure
>>>>>>>
>>>>>>>> or
>>>>>>>>>
>>>>>>>>>>  forwarding of this communication is strictly
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> prohibited.
>>>
>>>>  If
>>>>>>> you
>>>>>>>
>>>>>>>> have
>>>>>>>>>>
>>>>>>>>>>>  received this communication in error, please contact
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> the
>>>
>>>>  sender
>>>>>>>
>>>>>>>>  immediately
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> and delete it from your system. Thank You.
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> CONFIDENTIALITY NOTICE
>>>>> NOTICE: This message is intended for the use of the individual or
>>>>> entity
>>>>> to
>>>>> which it is addressed and may contain information that is confidentia=
l,
>>>>> privileged and exempt from disclosure under applicable law. If the
>>>>>
>>>> reader
>>>
>>>> of this message is not the intended recipient, you are hereby notified
>>>>> that
>>>>> any printing, copying, dissemination, distribution, disclosure or
>>>>> forwarding of this communication is strictly prohibited. If you have
>>>>> received this communication in error, please contact the sender
>>>>> immediately
>>>>> and delete it from your system. Thank You.
>>>>>
>>>>>
>>>>
>>>>
>>>
>>
>
>

--001a11c2b81617ec66050179c288--

From dev-return-9034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:14:47 2014
Return-Path: <dev-return-9034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C69C11DFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:14:47 +0000 (UTC)
Received: (qmail 66000 invoked by uid 500); 25 Aug 2014 20:14:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65935 invoked by uid 500); 25 Aug 2014 20:14:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65924 invoked by uid 99); 25 Aug 2014 20:14:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:14:46 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:14:40 +0000
Received: by mail-la0-f50.google.com with SMTP id pi18so14012512lab.9
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:14:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/6umvpQHwLfPWm/7asVn4vHx/M6TfNO1q+RnvR73GPU=;
        b=hTYJZWbjC1Tz0Mhg/poJsuduMh4cQMZFy/w6+zBmgtQAr4KKTYO9fwES6otKiQnrSE
         dupnpFc2XGl3QJhSAIJqsdUuL+vltAp2PZOKsuuASg5vqyJyWHHVZdW3181t6oXuE4Ve
         4WMZn/XIvAKWw1lRmtEUP94cnbGQgJWL0NAZwhmENw+rtJyx3hxQpwpjDhKswd9sxWQH
         WeJaYOBbln2RFh3IxQJVGMUezCwjEG7GPHw/v29lP1gTHJXNkM8dkBzYabYk92/ebqnH
         jHUbEew/ugFyqyQuWO+6nmrehzJ9AulQcgBgvd0KPyOAYc8yjyD9aIZVUqKXLsfeN7cL
         LPGg==
X-Gm-Message-State: ALoCoQn0LmUjJVMwVWuHw2dEH91yMgWXwMc3rBvZWFCZquwRQD/YMlBaSRBBscP9zITWOGGuvs1A
X-Received: by 10.153.11.162 with SMTP id ej2mr23235215lad.15.1408997657950;
 Mon, 25 Aug 2014 13:14:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.30.5 with HTTP; Mon, 25 Aug 2014 13:13:57 -0700 (PDT)
In-Reply-To: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
References: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 25 Aug 2014 13:13:57 -0700
Message-ID: <CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
Subject: Re: [Spark SQL] off-heap columnar store
To: Evan Chan <velvia.github@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11346d60e46511050179d46a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346d60e46511050179d46a
Content-Type: text/plain; charset=UTF-8

>
> What is the plan for getting Tachyon/off-heap support for the columnar
> compressed store?  It's not in 1.1 is it?


It is not in 1.1 and there are not concrete plans for adding it at this
point.  Currently, there is more engineering investment going into caching
parquet data in Tachyon instead.  This approach is going to have much
better support for nested data, leverages other work being done on parquet,
and alleviates your concerns about wire format compatibility.

That said, if someone really wants to try and implement it, I don't think
it would be very hard.  The primary issue is going to be designing a clean
interface that is not too tied to this one implementation.


> Also, how likely is the wire format for the columnar compressed data
> to change?  That would be a problem for write-through or persistence.
>

We aren't making any guarantees at the moment that it won't change.  Its
currently only intended for temporary caching of data.

--001a11346d60e46511050179d46a--

From dev-return-9035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:17:55 2014
Return-Path: <dev-return-9035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B1AC911E1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:17:55 +0000 (UTC)
Received: (qmail 78771 invoked by uid 500); 25 Aug 2014 20:17:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78717 invoked by uid 500); 25 Aug 2014 20:17:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77757 invoked by uid 99); 25 Aug 2014 20:17:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:17:53 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:17:49 +0000
Received: by mail-la0-f42.google.com with SMTP id pv20so13986888lab.15
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:17:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=I0JmWjGj63ajGbSmyZvOYUiZOrpfbiSO96GPD/FKdUA=;
        b=nC53zVQibhr2BKf4zJACW+Vbq1D64k+BhSf+oY5zCyAEU6DJAdE1KiNcF/OyM5Tr0A
         +YSPAnp6cDPtBPVUBobZqMr1D+ayD/zQ0W4xn4Iqx20Vl++ICiRO9zgLux4XI+4s1YsK
         AaWPi33VY6C3VzjnANz92VPVOdWxtFaCGTdY9Ed6YkXNs/boAAAldCCYqRCjtCqgv6ye
         W1O42sPJ6dYtazbdiL2IGHw8Q7qgm5afIAitVEuaJsfPwv4YT3NuXoZsQSiRupJrvHFQ
         1MkOHvrt/NhuYhMXiRzqQpMFeA1hHTB06mXBJ/TLUhommctfESTnf2CLWMn5E3F9wAym
         zVJQ==
X-Gm-Message-State: ALoCoQn00mAouek9WnrtoIDQEEFBNsIsD7njwKs6hoANC8tBoiw3kG7upbnXEahyTKRaqXm+wpkU
X-Received: by 10.112.1.136 with SMTP id 8mr22687864lbm.55.1408997847894; Mon,
 25 Aug 2014 13:17:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.30.5 with HTTP; Mon, 25 Aug 2014 13:17:07 -0700 (PDT)
In-Reply-To: <CADz3zK1iKca7PcBtFi8n2seO947Dxvk6L5yy4_5_D7MTUTVkVw@mail.gmail.com>
References: <CADz3zK1iKca7PcBtFi8n2seO947Dxvk6L5yy4_5_D7MTUTVkVw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 25 Aug 2014 13:17:07 -0700
Message-ID: <CAAswR-5g-jM_iS3HKYo-FYJGgtto26xxt6FrwFjNq2m6epwfaA@mail.gmail.com>
Subject: Re: Storage Handlers in Spark SQL
To: Niranda Perera <niranda@wso2.com>
Cc: user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae947382736a497050179e00b
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae947382736a497050179e00b
Content-Type: text/plain; charset=UTF-8

- dev list
+ user list

You should be able to query Spark SQL using JDBC, starting with the 1.1
release.  There is some documentation is the repo
<https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md#running-the-thrift-jdbc-server>,
and we'll update the official docs once the release is out.


On Thu, Aug 21, 2014 at 4:43 AM, Niranda Perera <niranda@wso2.com> wrote:

> Hi,
>
> I have been playing around with Spark for the past few days, and evaluating
> the possibility of migrating into Spark (Spark SQL) from Hive/Hadoop.
>
> I am working on the WSO2 Business Activity Monitor (WSO2 BAM,
>
> https://docs.wso2.com/display/BAM241/WSO2+Business+Activity+Monitor+Documentation
> ) which has currently employed Hive. We are considering Spark as a
> successor for Hive, given it's performance enhancement.
>
> We have currently employed several custom storage-handlers in Hive.
> Example:
> WSO2 JDBC and Cassandra storage handlers:
> https://docs.wso2.com/display/BAM241/JDBC+Storage+Handler+for+Hive
>
> https://docs.wso2.com/display/BAM241/Creating+Hive+Queries+to+Analyze+Data#CreatingHiveQueriestoAnalyzeData-cas
>
> I would like to know where Spark SQL can work with these storage
> handlers (while using HiveContext may be) ?
>
> Best regards
> --
> *Niranda Perera*
> Software Engineer, WSO2 Inc.
> Mobile: +94-71-554-8430
> Twitter: @n1r44 <https://twitter.com/N1R44>
>

--14dae947382736a497050179e00b--

From dev-return-9036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:26:42 2014
Return-Path: <dev-return-9036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6803D11E65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:26:42 +0000 (UTC)
Received: (qmail 6119 invoked by uid 500); 25 Aug 2014 20:26:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6046 invoked by uid 500); 25 Aug 2014 20:26:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6034 invoked by uid 99); 25 Aug 2014 20:26:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:26:41 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:26:14 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so21182886pdi.3
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 13:26:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=F07z759onmyVQI1qAha7FuOVji7xSxXRdKcKKIbs5bo=;
        b=NvcGoqSarcb9Ske0xK5hlmAi8E2iJHuOC1k7eZYy+8jWga1Sb0JyqrMbxSpr2B2U4D
         bmeWA2Vvxk4gKbEYq4FUBMNoiCAgB1BV7T4KLhRD/iEFIsn6BAf2LYZPeQZFQfygS0b4
         blDVu31sVKoUr3jzc5Dm3FemqwjxLTiorMQlV1k37toFlwcSkNT9DN9ejPXNZIzL0pUG
         UN22uKsFQbfDfb3aVMt9I4ZO8Mk16WnGcFUwTs1skB+IqwPNbuXKiSm2qnKKFYs/ORdR
         5EDn5jtnvrZGXiVKNPBWNty1RcHzaYMP5QGVWCQp0X+PXnRfzOIeeqVu0TiF8VYMAGCt
         O21w==
X-Received: by 10.68.171.33 with SMTP id ar1mr13962453pbc.148.1408998372032;
        Mon, 25 Aug 2014 13:26:12 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id j9sm1036098pdr.77.2014.08.25.13.26.02
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 13:26:03 -0700 (PDT)
Date: Mon, 25 Aug 2014 13:26:02 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: amnonkhen <amnon.is@gmail.com>, dev@spark.incubator.apache.org
Message-ID: <etPan.53fb9bda.519b500d.9a@mbp-3>
In-Reply-To: <1408993739123-7991.post@n3.nabble.com>
References: <1407667288137-7795.post@n3.nabble.com>
 <1408668974535-7950.post@n3.nabble.com>
 <1408993739123-7991.post@n3.nabble.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fb9bda_431bd7b7_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fb9bda_431bd7b7_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Was the original issue with Spark 1.1 (i.e. master branch) or an earlier release?

One possibility is that your S3 bucket is in a remote Amazon region, which would make it very slow. In my experience though saveAsTextFile has worked even for pretty large datasets in that situation, so maybe there's something else in your job causing a problem. Have you tried other operations on the data, like count(), or saving synthetic datasets (e.g. sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?

Matei

On August 25, 2014 at 12:09:25 PM, amnonkhen (amnon.is@gmail.com) wrote:

Hi jerryye, 
Maybe if you voted up my question on Stack Overflow it would get some 
traction and we would get nearer to a solution. 
Thanks, 
Amnon 



-- 
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html 
Sent from the Apache Spark Developers List mailing list archive at Nabble.com. 

--------------------------------------------------------------------- 
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
For additional commands, e-mail: dev-help@spark.apache.org 


--53fb9bda_431bd7b7_9a--


From dev-return-9037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:34:32 2014
Return-Path: <dev-return-9037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87DE711E96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:34:32 +0000 (UTC)
Received: (qmail 33404 invoked by uid 500); 25 Aug 2014 20:34:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33339 invoked by uid 500); 25 Aug 2014 20:34:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33326 invoked by uid 99); 25 Aug 2014 20:34:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:34:31 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:34:05 +0000
Received: by mail-ob0-f181.google.com with SMTP id va2so11044622obc.40
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 13:34:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=aWDyzekK6rp3Pmm+t6obN4L27PSVdpRru5u37eCBfOY=;
        b=mwGIFdFEGIyOjdUh7H/6n8+W0khSAXVUzZrOQmgwnfb5hs1rYIxGDj5IkwRP1qO6OH
         QgevhWxIDLjlVT9HlcefREnLrqAMWGyJdwvzVONBP/4oERZHHAfqWfA+c7xt3YLaZL9e
         v+mEsc6T+ickK7rwkmSP6SmFD8eJBZ7gU2/MZ81XM+Q8jNpFV+1RfD8dBjyKUrVBGcIr
         hqCzzL7Zllg2KPNb4BVDY6tt42TdJoY/GUVT62IRqcCaUbi1tkgH5pjTSn6mn64tXtTf
         ZXKp1s9Bxe49kOdH+DhmDn8HbUBEEQTpVhEw8Pj9nQfjvOphvhNMe/bDuOUids0pSk3j
         ZU9Q==
MIME-Version: 1.0
X-Received: by 10.60.51.106 with SMTP id j10mr4488155oeo.77.1408998844323;
 Mon, 25 Aug 2014 13:34:04 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Mon, 25 Aug 2014 13:34:04 -0700 (PDT)
In-Reply-To: <etPan.53fb9bda.519b500d.9a@mbp-3>
References: <1407667288137-7795.post@n3.nabble.com>
	<1408668974535-7950.post@n3.nabble.com>
	<1408993739123-7991.post@n3.nabble.com>
	<etPan.53fb9bda.519b500d.9a@mbp-3>
Date: Mon, 25 Aug 2014 13:34:04 -0700
Message-ID: <CABPQxstq=rnO-YvqrGQbw9cMASVvnbOT1mzvOFnEF_a0g9gYUQ@mail.gmail.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
From: Patrick Wendell <pwendell@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: amnonkhen <amnon.is@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c301529afda205017a1b03
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c301529afda205017a1b03
Content-Type: text/plain; charset=ISO-8859-1

One other idea - when things freeze up, try to run jstack on the spark
shell process and on the executors and attach the results. It could be that
somehow you are encountering a deadlock somewhere.


On Mon, Aug 25, 2014 at 1:26 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Was the original issue with Spark 1.1 (i.e. master branch) or an earlier
> release?
>
> One possibility is that your S3 bucket is in a remote Amazon region, which
> would make it very slow. In my experience though saveAsTextFile has worked
> even for pretty large datasets in that situation, so maybe there's
> something else in your job causing a problem. Have you tried other
> operations on the data, like count(), or saving synthetic datasets (e.g.
> sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
>
> Matei
>
> On August 25, 2014 at 12:09:25 PM, amnonkhen (amnon.is@gmail.com) wrote:
>
> Hi jerryye,
> Maybe if you voted up my question on Stack Overflow it would get some
> traction and we would get nearer to a solution.
> Thanks,
> Amnon
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c301529afda205017a1b03--

From dev-return-9038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:36:07 2014
Return-Path: <dev-return-9038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA0DB11E9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:36:07 +0000 (UTC)
Received: (qmail 38173 invoked by uid 500); 25 Aug 2014 20:36:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38107 invoked by uid 500); 25 Aug 2014 20:36:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38092 invoked by uid 99); 25 Aug 2014 20:36:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:36:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:35:41 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so11187983oag.33
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 13:35:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Fd47VQPVY5oIObCBwYwBfpdhnyikxXOnsGM730K1HnQ=;
        b=QFRq8XsKjh8uuTCwK/V+emHN2bl+63RYffiJU3cD2Gao57sOtbhBIJpGcs4PStxrOt
         puOVW2FAF5fSY1MBRr+7sTQM07c4Iz4AwZF2cg+3L0iWkzUwxR9mztUkXjYenxkzxvpP
         Tnh812VmZMK/4wbgthOCW0xa60VWaKsSOqT8bq5AZC1fIk/pA+shesxjeIhynNb7z13U
         eU0V5TsojWKALAtQ4EQWhgABKyKYStizQ0vMdyRDdkIKvKMLBeol/elWeUGn94f+j0as
         f9bRi6hME+M8624Y7M+T5S/h64q1wOySZ8OBmcP2dwDE9DnkQTBBq7kIHVRfCMYM0jG6
         DipA==
MIME-Version: 1.0
X-Received: by 10.60.67.34 with SMTP id k2mr23438557oet.52.1408998939750; Mon,
 25 Aug 2014 13:35:39 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Mon, 25 Aug 2014 13:35:39 -0700 (PDT)
In-Reply-To: <CAOhmDzfUANBEOe67nijdCxD3E9KBF3yC294xmdRkYONNkwUz7w@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
	<CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
	<CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
	<CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
	<CAOhmDzd_Q_PNPyBOvsgcJNDce1OZ7R5DdYODOEMqTMGADicciA@mail.gmail.com>
	<CABPQxstBXJmwdPxawQEzTTGpqNfC7fTJpo2rp=9PSv3tp5Jc1g@mail.gmail.com>
	<CAOhmDzdAcd6W5jCmHKkut9ER8ZZ08yNZioNe=JWGbRLypvW6RQ@mail.gmail.com>
	<CAOhmDzfUANBEOe67nijdCxD3E9KBF3yC294xmdRkYONNkwUz7w@mail.gmail.com>
Date: Mon, 25 Aug 2014 13:35:39 -0700
Message-ID: <CABPQxstqCqu-pk9eRVvrvkJN34nU9mO-tRhcALW77bYZsOJo6Q@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e2984b039005017a2162
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e2984b039005017a2162
Content-Type: text/plain; charset=ISO-8859-1

Hey Nicholas,

That seems promising - I prefer having a proper link to having that fairly
verbose comment though, because in some cases there will be dozens of
comments and it could get lost. I wonder if they could do something where
it posts a link instead...

- Patrick


On Mon, Aug 25, 2014 at 11:06 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> FYI: Looks like the Mesos folk also have a bot to do automatic linking,
> but it appears to have been provided to them somehow by ASF.
>
> See this comment as an example:
> https://issues.apache.org/jira/browse/MESOS-1688?focusedCommentId=14109078&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14109078
>
> Might be a small win to push this work to a bot ASF manages if we can get
> access to it (and if we have no concerns about depending on an another
> external service).
>
> Nick
>
>
> On Mon, Aug 11, 2014 at 4:10 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> Thanks for looking into this. I think little tools like this are super
>> helpful.
>>
>> Would it hurt to open a request with INFRA to install/configure the
>> JIRA-GitHub plugin while we continue to use the Python script we have? I
>> wouldn't mind opening that JIRA issue with them.
>>
>> Nick
>>
>>
>> On Mon, Aug 11, 2014 at 12:52 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> I spent some time on this and I'm not sure either of these is an option,
>>> unfortunately.
>>>
>>> We typically can't use custom JIRA plug-in's because this JIRA is
>>> controlled by the ASF and we don't have rights to modify most things about
>>> how it works (it's a large shared JIRA instance used by more than 50
>>> projects). It's worth looking into whether they can do something. In
>>> general we've tended to avoid going through ASF infra them whenever
>>> possible, since they are generally overloaded and things move very slowly,
>>> even if there are outages.
>>>
>>> Here is the script we use to do the sync:
>>> https://github.com/apache/spark/blob/master/dev/github_jira_sync.py
>>>
>>> It might be possible to modify this to support post-hoc changes, but
>>> we'd need to think about how to do so while minimizing function calls to
>>> the ASF JIRA API, which I found are very slow.
>>>
>>> - Patrick
>>>
>>>
>>>
>>> On Mon, Aug 11, 2014 at 7:51 AM, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> It looks like this script doesn't catch PRs that are opened and *then*
>>>> have
>>>>
>>>> the JIRA issue ID added to the name. Would it be easy to somehow have
>>>> the
>>>> script trigger on PR name changes as well as PR creates?
>>>>
>>>> Alternately, is there a reason we can't or don't want to use the plugin
>>>> mentioned below? (I'm assuming it covers cases like this, but I'm not
>>>> sure.)
>>>>
>>>> Nick
>>>>
>>>>
>>>>
>>>> On Wed, Jul 23, 2014 at 12:52 PM, Nicholas Chammas <
>>>> nicholas.chammas@gmail.com> wrote:
>>>>
>>>> > By the way, it looks like there's a JIRA plugin that integrates it
>>>> with
>>>> > GitHub:
>>>> >
>>>> >    -
>>>> >
>>>> https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin
>>>>
>>>> >    -
>>>> >
>>>> https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+and+GitHub+accounts+to+JIRA
>>>> >
>>>> > It does the automatic linking and shows some additional information
>>>> > <
>>>> https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png
>>>> >
>>>>
>>>> > that might be nice to have for heavy JIRA users.
>>>> >
>>>> > Nick
>>>> >
>>>> >
>>>> >
>>>> > On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com
>>>> >
>>>> > wrote:
>>>> >
>>>> >> Yeah it needs to have SPARK-XXX in the title (this is the format we
>>>> >> request already). It just works with small synchronization script I
>>>> >> wrote that we run every five minutes on Jeknins that uses the Github
>>>> >> and Jenkins API:
>>>> >>
>>>> >>
>>>> >>
>>>> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139232929
>>>> >>
>>>> >> - Patrick
>>>> >>
>>>> >> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
>>>> >> <nicholas.chammas@gmail.com> wrote:
>>>> >> > That's pretty neat.
>>>> >> >
>>>> >> > How does it work? Do we just need to put the issue ID (e.g.
>>>> SPARK-1234)
>>>> >> > anywhere in the pull request?
>>>> >> >
>>>> >> > Nick
>>>> >> >
>>>> >> >
>>>> >> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <
>>>> pwendell@gmail.com>
>>>> >> > wrote:
>>>> >> >
>>>> >> >> Just a small note, today I committed a tool that will
>>>> automatically
>>>> >> >> mirror pull requests to JIRA issues, so contributors will no
>>>> longer
>>>> >> >> have to manually post a pull request on the JIRA when they make
>>>> one.
>>>> >> >>
>>>> >> >> It will create a "link" on the JIRA and also make a comment to
>>>> trigger
>>>> >> >> an e-mail to people watching.
>>>> >> >>
>>>> >> >> This should make some things easier, such as avoiding accidental
>>>> >> >> duplicate effort on the same JIRA.
>>>> >> >>
>>>> >> >> - Patrick
>>>> >> >>
>>>> >>
>>>> >
>>>> >
>>>>
>>>
>>>
>>
>

--001a11c2e2984b039005017a2162--

From dev-return-9039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 20:37:54 2014
Return-Path: <dev-return-9039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DB8011EAA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 20:37:54 +0000 (UTC)
Received: (qmail 46867 invoked by uid 500); 25 Aug 2014 20:37:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46803 invoked by uid 500); 25 Aug 2014 20:37:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46792 invoked by uid 99); 25 Aug 2014 20:37:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:37:52 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 20:37:26 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XM112-0003qM-Jl
	for dev@spark.incubator.apache.org; Mon, 25 Aug 2014 13:37:24 -0700
Date: Mon, 25 Aug 2014 13:37:24 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAAziPCnU4cUqyWNMUF3RLXk3uqq=noFAD9a6AqG=k06m3j7GfQ@mail.gmail.com>
In-Reply-To: <etPan.53fb9bda.519b500d.9a@mbp-3>
References: <1407667288137-7795.post@n3.nabble.com> <1408668974535-7950.post@n3.nabble.com> <1408993739123-7991.post@n3.nabble.com> <etPan.53fb9bda.519b500d.9a@mbp-3>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_48954_6676519.1408999044605"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_48954_6676519.1408999044605
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Matei,
At least in my case, the s3 bucket is in the same region. Running count()
works and so does generating synthetic data. What I saw was that the job
would hang for over an hour with no progress but tasks would immediately
start finishing if I cached the data.

- jerry


On Mon, Aug 25, 2014 at 1:26 PM, Matei Zaharia [via Apache Spark Developers
List] <ml-node+s1001551n8000h39@n3.nabble.com> wrote:

> Was the original issue with Spark 1.1 (i.e. master branch) or an earlier
> release?
>
> One possibility is that your S3 bucket is in a remote Amazon region, which
> would make it very slow. In my experience though saveAsTextFile has worked
> even for pretty large datasets in that situation, so maybe there's
> something else in your job causing a problem. Have you tried other
> operations on the data, like count(), or saving synthetic datasets (e.g.
> sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
>
> Matei
>
> On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=0>) wrote:
>
> Hi jerryye,
> Maybe if you voted up my question on Stack Overflow it would get some
> traction and we would get nearer to a solution.
> Thanks,
> Amnon
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
>
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=1>
> For additional commands, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=2>
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h70@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=amVycnl5ZUBnbWFpbC5jb218MXwtNTI4OTc1MTAz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8003.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_48954_6676519.1408999044605--

From dev-return-9040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 21:28:44 2014
Return-Path: <dev-return-9040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 76DC611214
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 21:28:44 +0000 (UTC)
Received: (qmail 4931 invoked by uid 500); 25 Aug 2014 21:28:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4790 invoked by uid 500); 25 Aug 2014 21:28:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4765 invoked by uid 99); 25 Aug 2014 21:28:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:28:42 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tnachen@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:28:38 +0000
Received: by mail-oi0-f49.google.com with SMTP id u20so10056061oif.22
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 14:28:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=visdxAM2nL4XD9EQr8cHB4GxLWoWK9eX3GbeAvUpoNc=;
        b=X0G1zwoUnKtrvifLkNauLb910ZhblJmNCQ42xruStKXfjW/Joa07nRC7lprnMJ/qDT
         mkJLXMXkPFFJH1NvXaO/EqTMjUQAZiAl5gp86cptbE2Wqzrn1qshoUgNEiwXU4G8M41E
         IWp3LgyZhLHGmps2ivF/6JRwZpP9XZKHp791WdaBPRvqu2KHVpSCZfRjl2OJqXNdUMkf
         w7c5RtpOP5cVA9X+4jqcWq6l0F3KHD0otuQITt+I7O4va+GO9EmiYpBJsHKE+TLLYpZn
         Wr443NoetNQfetOGXzXPKxsFcGrNukZvwpkw43Ro8sz8p+C8YFCXrciaOxDBnnCUt+sf
         PKbw==
MIME-Version: 1.0
X-Received: by 10.182.51.229 with SMTP id n5mr23710015obo.47.1409002098355;
 Mon, 25 Aug 2014 14:28:18 -0700 (PDT)
Received: by 10.60.37.4 with HTTP; Mon, 25 Aug 2014 14:28:18 -0700 (PDT)
In-Reply-To: <etPan.53fb97b5.4e6afb66.9a@mbp-3>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
	<CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
	<etPan.53fac0f0.643c9869.9a@mbp-3.local>
	<CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
	<CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
	<etPan.53fb884f.1190cde7.9a@mbp-3>
	<CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
	<etPan.53fb9771.41a7c4c9.9a@mbp-3>
	<etPan.53fb97b5.4e6afb66.9a@mbp-3>
Date: Mon, 25 Aug 2014 14:28:18 -0700
Message-ID: <CAFx0iW-O_StJoA_GruURYQ_UghOQP5iLzKOxQKbqThyA9Pf5YQ@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Timothy Chen <tnachen@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Cody Koeninger <cody@koeninger.org>, "dev@mesos.apache.org" <dev@mesos.apache.org>, 
	Gary Malouf <malouf.gary@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Matei,

I'm going to investigate from both Mesos and Spark side will hopefully
have a good long term solution. In the mean time having a work around
to start with is going to unblock folks.

Tim

On Mon, Aug 25, 2014 at 1:08 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Anyway it would be good if someone from the Mesos side investigates this and
> proposes a solution. The 32 MB per task hack isn't completely foolproof
> either (e.g. people might allocate all the RAM to their executor and thus
> stop being able to launch tasks), so maybe we wait on a Mesos fix for this
> one.
>
> Matei
>
> On August 25, 2014 at 1:07:15 PM, Matei Zaharia (matei.zaharia@gmail.com)
> wrote:
>
> This is kind of weird then, seems perhaps unrelated to this issue (or at
> least to the way I understood it). Is the problem maybe that Mesos saw 0 MB
> being freed and didn't re-offer the machine *even though there was more than
> 32 MB free overall*?
>
> Matei
>
> On August 25, 2014 at 12:59:59 PM, Cody Koeninger (cody@koeninger.org)
> wrote:
>
> I definitely saw a case where
>
> a. the only job running was a 256m shell
> b. I started a 2g job
> c. a little while later the same user as in a started another 256m shell
>
> My job immediately stopped making progress.  Once user a killed his shells,
> it started again.
>
> This is on nodes with ~15G of memory, on which we have successfully run 8G
> jobs.
>
>
> On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>>
>> BTW it seems to me that even without that patch, you should be getting
>> tasks launched as long as you leave at least 32 MB of memory free on each
>> machine (that is, the sum of the executor memory sizes is not exactly the
>> same as the total size of the machine). Then Mesos will be able to re-offer
>> that machine whenever CPUs free up.
>>
>> Matei
>>
>> On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary@gmail.com)
>> wrote:
>>
>> We have not tried the work-around because there are other bugs in there
>> that affected our set-up, though it seems it would help.
>>
>>
>> On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote:
>>
>> > +1 to have the work around in.
>> >
>> > I'll be investigating from the Mesos side too.
>> >
>> > Tim
>> >
>> > On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> > wrote:
>> > > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's
>> > > too
>> > bad that this happens in fine-grained mode -- would be really good to
>> > fix.
>> > I'll see if we can get the workaround in
>> > https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
>> > have you tried that?
>> > >
>> > > Matei
>> > >
>> > > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com)
>> > wrote:
>> > >
>> > > Hi Matei,
>> > >
>> > > We have an analytics team that uses the cluster on a daily basis. They
>> > use two types of 'run modes':
>> > >
>> > > 1) For running actual queries, they set the spark.executor.memory to
>> > something between 4 and 8GB of RAM/worker.
>> > >
>> > > 2) A shell that takes a minimal amount of memory on workers (128MB)
>> > > for
>> > prototyping out a larger query. This allows them to not take up RAM on
>> > the
>> > cluster when they do not really need it.
>> > >
>> > > We see the deadlocks when there are a few shells in either case. From
>> > the usage patterns we have, coarse-grained mode would be a challenge as
>> > we
>> > have to constantly remind people to kill their shells as soon as their
>> > queries finish.
>> > >
>> > > Am I correct in viewing Mesos in coarse-grained mode as being similar
>> > > to
>> > Spark Standalone's cpu allocation behavior?
>> > >
>> > >
>> > >
>> > >
>> > > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia
>> > > <matei.zaharia@gmail.com>
>> > wrote:
>> > > Hey Gary, just as a workaround, note that you can use Mesos in
>> > coarse-grained mode by setting spark.mesos.coarse=true. Then it will
>> > hold
>> > onto CPUs for the duration of the job.
>> > >
>> > > Matei
>> > >
>> > > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com)
>> > wrote:
>> > >
>> > > I just wanted to bring up a significant Mesos/Spark issue that makes
>> > > the
>> > > combo difficult to use for teams larger than 4-5 people. It's covered
>> > > in
>> > > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is
>> > that
>> > > Spark's use of executors in fine-grained mode is a very different
>> > behavior
>> > > than many of the other common frameworks for Mesos.
>> > >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 21:29:40 2014
Return-Path: <dev-return-9041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E257C11233
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 21:29:40 +0000 (UTC)
Received: (qmail 11806 invoked by uid 500); 25 Aug 2014 21:29:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11743 invoked by uid 500); 25 Aug 2014 21:29:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11728 invoked by uid 99); 25 Aug 2014 21:29:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:29:39 +0000
X-ASF-Spam-Status: No, hits=3.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alee526@hotmail.com designates 65.55.111.93 as permitted sender)
Received: from [65.55.111.93] (HELO BLU004-OMC2S18.hotmail.com) (65.55.111.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:29:12 +0000
Received: from BLU184-W62 ([65.55.111.72]) by BLU004-OMC2S18.hotmail.com with Microsoft SMTPSVC(7.5.7601.22712);
	 Mon, 25 Aug 2014 14:29:11 -0700
X-TMN: [aH9j7vr15zwXE4UcsDNo+Ibixgo+L0dy]
X-Originating-Email: [alee526@hotmail.com]
Message-ID: <BLU184-W6246E0360DA15F574AD74EF3DF0@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_7ed266d7-174d-4e63-a4e7-d0e0a1e3b83d_"
From: Andrew Lee <alee526@hotmail.com>
To: Michael Armbrust <michael@databricks.com>, scwf <wangfei1@huawei.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Working Formula for Hive 0.13?
Date: Mon, 25 Aug 2014 14:29:12 -0700
Importance: Normal
In-Reply-To:
 <CAAswR-79v9vWH7b_ik_KzMGigu0XeKW56OqVgnOT9ORFRhX=_Q@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
 <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>,<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>,<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>,<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>,<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>,<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>,<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>,<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>,<CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>,<CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>,<CFFC00D1.2ED4%snunez@hortonworks.com>
 <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>,<CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>,<CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
 <53FA9B5C.7060605@huawei.com>,<CAAswR-79v9vWH7b_ik_KzMGigu0XeKW56OqVgnOT9ORFRhX=_Q@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 25 Aug 2014 21:29:11.0328 (UTC) FILETIME=[9C6BBA00:01CFC0AB]
X-Virus-Checked: Checked by ClamAV on apache.org

--_7ed266d7-174d-4e63-a4e7-d0e0a1e3b83d_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

>From my perspective=2C there're few benefits regarding Hive 0.13.1+. The fo=
llowing are the 4 major ones that I can see why people are asking to upgrad=
e to Hive 0.13.1 recently.
1. Performance and bug fix=2C patches. (Usual case)
2. Native support for Parquet format=2C no need to provide custom JARs and =
SerDe like Hive 0.12. (Depends=2C driven by data format and queries)
3. Support of Tez engine which gives performance improvement in several use=
 cases (Performance improvement)
4. Security enhancement in Hive 0.13.1 has improved a lot (Security concern=
s=2C ACLs=2C etc)
These are the major benefits I see to upgrade to Hive 0.13.1+ from Hive 0.1=
2.0.
There may be others out there that I'm not aware of=2C but I do see it comi=
ng.
my 2 cents.
> From: michael@databricks.com
> Date: Mon=2C 25 Aug 2014 13:08:42 -0700
> Subject: Re: Working Formula for Hive 0.13?
> To: wangfei1@huawei.com
> CC: dev@spark.apache.org
>=20
> Thanks for working on this!  Its unclear at the moment exactly how we are
> going to handle this=2C since the end goal is to be compatible with as ma=
ny
> versions of Hive as possible.  That said=2C I think it would be great to =
open
> a PR in this case.  Even if we don't merge it=2C thats a good way to get =
it
> on people's radar and have a discussion about the changes that are requir=
ed.
>=20
>=20
> On Sun=2C Aug 24=2C 2014 at 7:11 PM=2C scwf <wangfei1@huawei.com> wrote:
>=20
> >   I have worked for a branch update the hive version to hive-0.13(by
> > org.apache.hive)---https://github.com/scwf/spark/tree/hive-0.13
> > I am wondering whether it's ok to make a PR now because hive-0.13 versi=
on
> > is not compatible with hive-0.12 and here i used org.apache.hive.
> >
> >
> >
> > On 2014/7/29 8:22=2C Michael Armbrust wrote:
> >
> >> A few things:
> >>   - When we upgrade to Hive 0.13.0=2C Patrick will likely republish th=
e
> >> hive-exec jar just as we did for 0.12.0
> >>   - Since we have to tie into some pretty low level APIs it is
> >> unsurprising
> >> that the code doesn't just compile out of the box against 0.13.0
> >>   - ScalaReflection is for determining Schema from Scala classes=2C no=
t
> >> reflection based bridge code.  Either way its unclear to if there is a=
ny
> >> reason to use reflection to support multiple versions=2C instead of ju=
st
> >> upgrading to Hive 0.13.0
> >>
> >> One question I have is=2C What is the goal of upgrading to hive 0.13.0=
?  Is
> >> it purely because you are having problems connecting to newer metastor=
es?
> >>   Are there some features you are hoping for?  This will help me
> >> prioritize
> >> this effort.
> >>
> >> Michael
> >>
> >>
> >> On Mon=2C Jul 28=2C 2014 at 4:05 PM=2C Ted Yu <yuzhihong@gmail.com> wr=
ote:
> >>
> >>  I was looking for a class where reflection-related code should reside=
.
> >>>
> >>> I found this but don't think it is the proper class for bridging
> >>> differences between hive 0.12 and 0.13.1:
> >>>
> >>> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/
> >>> ScalaReflection.scala
> >>>
> >>> Cheers
> >>>
> >>>
> >>> On Mon=2C Jul 28=2C 2014 at 3:41 PM=2C Ted Yu <yuzhihong@gmail.com> w=
rote:
> >>>
> >>>  After manually copying hive 0.13.1 jars to local maven repo=2C I got=
 the
> >>>> following errors when building spark-hive_2.10 module :
> >>>>
> >>>> [ERROR]
> >>>>
> >>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
> >>> sql/hive/HiveContext.scala:182:
> >>>
> >>>> type mismatch=3B
> >>>>   found   : String
> >>>>   required: Array[String]
> >>>> [ERROR]       val proc: CommandProcessor =3D
> >>>> CommandProcessorFactory.get(tokens(0)=2C hiveconf)
> >>>> [ERROR]
> >>>>     ^
> >>>> [ERROR]
> >>>>
> >>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
> >>> sql/hive/HiveMetastoreCatalog.scala:60:
> >>>
> >>>> value getAllPartitionsForPruner is not a member of org.apache.
> >>>>   hadoop.hive.ql.metadata.Hive
> >>>> [ERROR]         client.getAllPartitionsForPruner(table).toSeq
> >>>> [ERROR]                ^
> >>>> [ERROR]
> >>>>
> >>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
> >>> sql/hive/HiveMetastoreCatalog.scala:267:
> >>>
> >>>> overloaded method constructor TableDesc with alternatives:
> >>>>    (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_=2C _]]=2C=
x$2:
> >>>> Class[_]=2Cx$3:
> >>>>
> >>> java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
> >>>
> >>>> <and>
> >>>>    ()org.apache.hadoop.hive.ql.plan.TableDesc
> >>>>   cannot be applied to (Class[org.apache.hadoop.hive.
> >>>> serde2.Deserializer]=2C
> >>>> Class[(some other)?0(in value tableDesc)(in value tableDesc)]=2C
> >>>>
> >>> Class[?0(in
> >>>
> >>>> value tableDesc)(in   value tableDesc)]=2C java.util.Properties)
> >>>> [ERROR]   val tableDesc =3D new TableDesc(
> >>>> [ERROR]                   ^
> >>>> [WARNING] Class org.antlr.runtime.tree.CommonTree not found -
> >>>> continuing
> >>>> with a stub.
> >>>> [WARNING] Class org.antlr.runtime.Token not found - continuing with =
a
> >>>>
> >>> stub.
> >>>
> >>>> [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing w=
ith
> >>>> a
> >>>> stub.
> >>>> [ERROR]
> >>>>       while compiling:
> >>>>
> >>>>  /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/
> >>> sql/hive/HiveQl.scala
> >>>
> >>>>          during phase: typer
> >>>>       library version: version 2.10.4
> >>>>      compiler version: version 2.10.4
> >>>>
> >>>> The above shows incompatible changes between 0.12 and 0.13.1
> >>>> e.g. the first error corresponds to the following method
> >>>> in CommandProcessorFactory :
> >>>>    public static CommandProcessor get(String[] cmd=2C HiveConf conf)
> >>>>
> >>>> Cheers
> >>>>
> >>>>
> >>>> On Mon=2C Jul 28=2C 2014 at 1:32 PM=2C Steve Nunez <snunez@hortonwor=
ks.com>
> >>>> wrote:
> >>>>
> >>>>  So=2C do we have a short-term fix until Hive 0.14 comes out? Perhap=
s
> >>>>>
> >>>> adding
> >>>
> >>>> the hive-exec jar to the spark-project repo? It doesn=B9t look like
> >>>>>
> >>>> there=B9s
> >>>
> >>>> a release date schedule for 0.14.
> >>>>>
> >>>>>
> >>>>>
> >>>>> On 7/28/14=2C 10:50=2C "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
> >>>>>
> >>>>>  Exactly=2C forgot to mention Hulu team also made changes to cope w=
ith
> >>>>>>
> >>>>> those
> >>>
> >>>> incompatibility issues=2C but they said that=B9s relatively easy onc=
e the
> >>>>>> re-packaging work is done.
> >>>>>>
> >>>>>>
> >>>>>> On Tue=2C Jul 29=2C 2014 at 1:20 AM=2C Patrick Wendell <pwendell@g=
mail.com>
> >>>>>>
> >>>>>
> >>>>>  wrote:
> >>>>>>
> >>>>>>  I've heard from Cloudera that there were hive internal changes
> >>>>>>>
> >>>>>> between
> >>>
> >>>>  0.12 and 0.13 that required code re-writing. Over time it might be
> >>>>>>> possible for us to integrate with hive using API's that are more
> >>>>>>> stable (this is the domain of Michael/Cheng/Yin more than me!). I=
t
> >>>>>>> would be interesting to see what the Hulu folks did.
> >>>>>>>
> >>>>>>> - Patrick
> >>>>>>>
> >>>>>>> On Mon=2C Jul 28=2C 2014 at 10:16 AM=2C Cheng Lian <lian.cs.zju@g=
mail.com>
> >>>>>>> wrote:
> >>>>>>>
> >>>>>>>> AFAIK=2C according a recent talk=2C Hulu team in China has built=
 Spark
> >>>>>>>>
> >>>>>>> SQL
> >>>>>
> >>>>>> against Hive 0.13 (or 0.13.1?) successfully. Basically they also
> >>>>>>>> re-packaged Hive 0.13 as what the Spark team did. The slides of =
the
> >>>>>>>>
> >>>>>>> talk
> >>>>>>>
> >>>>>>>> hasn't been released yet though.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> On Tue=2C Jul 29=2C 2014 at 1:01 AM=2C Ted Yu <yuzhihong@gmail.c=
om>
> >>>>>>>>
> >>>>>>> wrote:
> >>>
> >>>>
> >>>>>>>>  Owen helped me find this:
> >>>>>>>>> https://issues.apache.org/jira/browse/HIVE-7423
> >>>>>>>>>
> >>>>>>>>> I guess this means that for Hive 0.14=2C Spark should be able t=
o
> >>>>>>>>>
> >>>>>>>> directly
> >>>>>>>
> >>>>>>>> pull in hive-exec-core.jar
> >>>>>>>>>
> >>>>>>>>> Cheers
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Mon=2C Jul 28=2C 2014 at 9:55 AM=2C Patrick Wendell <
> >>>>>>>>>
> >>>>>>>> pwendell@gmail.com>
> >>>>>
> >>>>>>  wrote:
> >>>>>>>>>
> >>>>>>>>>  It would be great if the hive team can fix that issue. If not=
=2C
> >>>>>>>>>>
> >>>>>>>>> we'll
> >>>>>>>
> >>>>>>>> have to continue forking our own version of Hive to change the
> >>>>>>>>>>
> >>>>>>>>> way
> >>>
> >>>>  it
> >>>>>>>
> >>>>>>>> publishes artifacts.
> >>>>>>>>>>
> >>>>>>>>>> - Patrick
> >>>>>>>>>>
> >>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 9:34 AM=2C Ted Yu <yuzhihong@gmail=
.com>
> >>>>>>>>>>
> >>>>>>>>> wrote:
> >>>>>>>
> >>>>>>>>  Talked with Owen offline. He confirmed that as of 0.13=2C
> >>>>>>>>>>>
> >>>>>>>>>> hive-exec is
> >>>>>>>
> >>>>>>>> still
> >>>>>>>>>>
> >>>>>>>>>>> uber jar.
> >>>>>>>>>>>
> >>>>>>>>>>> Right now I am facing the following error building against
> >>>>>>>>>>>
> >>>>>>>>>> Hive
> >>>
> >>>>  0.13.1
> >>>>>>>
> >>>>>>>> :
> >>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>> [ERROR] Failed to execute goal on project spark-hive_2.10:
> >>>>>>>>>>>
> >>>>>>>>>> Could
> >>>
> >>>>  not
> >>>>>>>
> >>>>>>>>  resolve dependencies for project
> >>>>>>>>>>> org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
> >>>>>>>>>>>
> >>>>>>>>>> following
> >>>>>>>
> >>>>>>>>  artifacts could not be resolved:
> >>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1=2C
> >>>>>>>>>>> org.spark-project.hive:hive-exec:jar:0.13.1=2C
> >>>>>>>>>>> org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> >>>>>>>>>>> org.spark-project.hive:hive-metastore:jar:0.13.1 in
> >>>>>>>>>>> http://repo.maven.apache.org/maven2 was cached in the local
> >>>>>>>>>>>
> >>>>>>>>>> repository=2C
> >>>>>>>>>
> >>>>>>>>>> resolution will not be reattempted until the update interval
> >>>>>>>>>>>
> >>>>>>>>>> of
> >>>
> >>>>  maven-repo
> >>>>>>>>>>
> >>>>>>>>>>> has elapsed or updates are forced -> [Help 1]
> >>>>>>>>>>>
> >>>>>>>>>>> Some hint would be appreciated.
> >>>>>>>>>>>
> >>>>>>>>>>> Cheers
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 9:15 AM=2C Sean Owen <
> >>>>>>>>>>>
> >>>>>>>>>> sowen@cloudera.com>
> >>>
> >>>>  wrote:
> >>>>>>>
> >>>>>>>>
> >>>>>>>>>>>  Yes=2C it is published. As of previous versions=2C at least=
=2C
> >>>>>>>>>>>>
> >>>>>>>>>>> hive-exec
> >>>>>>>
> >>>>>>>>  included all of its dependencies *in its artifact*=2C making it
> >>>>>>>>>>>>
> >>>>>>>>>>> unusable
> >>>>>>>
> >>>>>>>>  as-is because it contained copies of dependencies that clash
> >>>>>>>>>>>>
> >>>>>>>>>>> with
> >>>>>>>
> >>>>>>>>  versions present in other artifacts=2C and can't be managed
> >>>>>>>>>>>>
> >>>>>>>>>>> with
> >>>
> >>>>  Maven
> >>>>>>>
> >>>>>>>>  mechanisms.
> >>>>>>>>>>>>
> >>>>>>>>>>>> I am not sure why hive-exec was not published normally=2C wi=
th
> >>>>>>>>>>>>
> >>>>>>>>>>> just
> >>>>>>> its
> >>>>>>>
> >>>>>>>>  own classes. That's why it was copied=2C into an artifact with
> >>>>>>>>>>>>
> >>>>>>>>>>> just
> >>>>>>>
> >>>>>>>>  hive-exec code.
> >>>>>>>>>>>>
> >>>>>>>>>>>> You could do the same thing for hive-exec 0.13.1.
> >>>>>>>>>>>> Or maybe someone knows that it's published more 'normally'
> >>>>>>>>>>>>
> >>>>>>>>>>> now.
> >>>
> >>>>  I don't think hive-metastore is related to this question?
> >>>>>>>>>>>>
> >>>>>>>>>>>> I am no expert on the Hive artifacts=2C just remembering wha=
t
> >>>>>>>>>>>>
> >>>>>>>>>>> the
> >>>
> >>>>  issue
> >>>>>>>
> >>>>>>>>  was initially in case it helps you get to a similar solution.
> >>>>>>>>>>>>
> >>>>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 4:47 PM=2C Ted Yu <yuzhihong@gma=
il.com
> >>>>>>>>>>>>
> >>>>>>>>>>>
> >>>>  wrote:
> >>>>>>>
> >>>>>>>>  hive-exec (as of 0.13.1) is published here:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>  http://search.maven.org/#artifactdetails%7Corg.apache.
> >>> hive%7Chive-exec%7C
> >>>
> >>>>  0.13.1%7Cjar
> >>>>>>>
> >>>>>>>>
> >>>>>>>>>>>>> Should a JIRA be opened so that dependency on
> >>>>>>>>>>>>>
> >>>>>>>>>>>> hive-metastore
> >>>
> >>>>  can
> >>>>>>> be
> >>>>>>>
> >>>>>>>>  replaced by dependency on hive-exec ?
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Cheers
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 8:26 AM=2C Sean Owen
> >>>>>>>>>>>>>
> >>>>>>>>>>>> <sowen@cloudera.com>
> >>>>>>>
> >>>>>>>> wrote:
> >>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>>>  The reason for org.spark-project.hive is that Spark relies
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>> on
> >>>>>
> >>>>>>  hive-exec=2C but the Hive project does not publish this
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>> artifact
> >>>>>>> by
> >>>>>>>
> >>>>>>>>  itself=2C only with all its dependencies as an uber jar.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>> Maybe
> >>>
> >>>>  that's
> >>>>>>>
> >>>>>>>>  been improved. If so=2C you need to point at the new
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>> hive-exec
> >>>
> >>>>  and
> >>>>>>>
> >>>>>>>>  perhaps sort out its dependencies manually in your build.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 4:01 PM=2C Ted Yu <
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>> yuzhihong@gmail.com>
> >>>>>
> >>>>>>  wrote:
> >>>>>>>>>
> >>>>>>>>>>  I found 0.13.1 artifacts in maven:
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>  http://search.maven.org/#artifactdetails%7Corg.apache.
> >>> hive%7Chive-metasto
> >>>
> >>>>  re%7C0.13.1%7Cjar
> >>>>>>>
> >>>>>>>>
> >>>>>>>>>>>>>>> However=2C Spark uses groupId of org.spark-project.hive=
=2C
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>> not
> >>>
> >>>>   org.apache.hive
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Can someone tell me how it is supposed to work ?
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Cheers
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> On Mon=2C Jul 28=2C 2014 at 7:44 AM=2C Steve Nunez <
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>> snunez@hortonworks.com>
> >>>>>>>>>>
> >>>>>>>>>>>  wrote:
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>  I saw a note earlier=2C perhaps on the user list=2C that=
 at
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> least
> >>>>>>>
> >>>>>>>> one
> >>>>>>>>>
> >>>>>>>>>>  person is
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> using Hive 0.13. Anyone got a working build
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> configuration
> >>>
> >>>>  for
> >>>>>>>
> >>>>>>>> this
> >>>>>>>>>
> >>>>>>>>>>  version
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> of Hive?
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Regards=2C
> >>>>>>>>>>>>>>>> - Steve
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> --
> >>>>>>>>>>>>>>>> CONFIDENTIALITY NOTICE
> >>>>>>>>>>>>>>>> NOTICE: This message is intended for the use of the
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> individual
> >>>>>>>
> >>>>>>>> or
> >>>>>>>>>
> >>>>>>>>>>  entity to
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> which it is addressed and may contain information that
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> is
> >>>
> >>>>  confidential=2C
> >>>>>>>>>>>>
> >>>>>>>>>>>>>  privileged and exempt from disclosure under applicable
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> law.
> >>>>>>> If
> >>>>>>>
> >>>>>>>> the
> >>>>>>>>>
> >>>>>>>>>>  reader
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> of this message is not the intended recipient=2C you are
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> hereby
> >>>>>>>
> >>>>>>>>  notified
> >>>>>>>>>>>>
> >>>>>>>>>>>>> that
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> any printing=2C copying=2C dissemination=2C distribution=
=2C
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> disclosure
> >>>>>>>
> >>>>>>>> or
> >>>>>>>>>
> >>>>>>>>>>  forwarding of this communication is strictly
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> prohibited.
> >>>
> >>>>  If
> >>>>>>> you
> >>>>>>>
> >>>>>>>> have
> >>>>>>>>>>
> >>>>>>>>>>>  received this communication in error=2C please contact
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> the
> >>>
> >>>>  sender
> >>>>>>>
> >>>>>>>>  immediately
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> and delete it from your system. Thank You.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>>
> >>>>>
> >>>>> --
> >>>>> CONFIDENTIALITY NOTICE
> >>>>> NOTICE: This message is intended for the use of the individual or
> >>>>> entity
> >>>>> to
> >>>>> which it is addressed and may contain information that is confident=
ial=2C
> >>>>> privileged and exempt from disclosure under applicable law. If the
> >>>>>
> >>>> reader
> >>>
> >>>> of this message is not the intended recipient=2C you are hereby noti=
fied
> >>>>> that
> >>>>> any printing=2C copying=2C dissemination=2C distribution=2C disclos=
ure or
> >>>>> forwarding of this communication is strictly prohibited. If you hav=
e
> >>>>> received this communication in error=2C please contact the sender
> >>>>> immediately
> >>>>> and delete it from your system. Thank You.
> >>>>>
> >>>>>
> >>>>
> >>>>
> >>>
> >>
> >
> >
 		 	   		  =

--_7ed266d7-174d-4e63-a4e7-d0e0a1e3b83d_--

From dev-return-9042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 21:55:54 2014
Return-Path: <dev-return-9042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A31731140F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 21:55:54 +0000 (UTC)
Received: (qmail 82822 invoked by uid 500); 25 Aug 2014 21:55:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82759 invoked by uid 500); 25 Aug 2014 21:55:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82747 invoked by uid 99); 25 Aug 2014 21:55:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:55:53 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jerryye@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:55:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <jerryye@gmail.com>)
	id 1XM2EY-0007hS-A0
	for dev@spark.incubator.apache.org; Mon, 25 Aug 2014 14:55:26 -0700
Date: Mon, 25 Aug 2014 14:55:26 -0700 (PDT)
From: jerryye <jerryye@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAAziPC=zSN8pz=bOhpKko_HY_hbDvu069tVys=T03WykRWOVPA@mail.gmail.com>
In-Reply-To: <CABPQxstq=rnO-YvqrGQbw9cMASVvnbOT1mzvOFnEF_a0g9gYUQ@mail.gmail.com>
References: <1407667288137-7795.post@n3.nabble.com> <1408668974535-7950.post@n3.nabble.com> <1408993739123-7991.post@n3.nabble.com> <etPan.53fb9bda.519b500d.9a@mbp-3> <CABPQxstq=rnO-YvqrGQbw9cMASVvnbOT1mzvOFnEF_a0g9gYUQ@mail.gmail.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_50941_17318172.1409003726304"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_50941_17318172.1409003726304
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Patrick,
Here's the process:
java -cp
/root/ephemeral-hdfs/conf::::/root/ephemeral-hdfs/conf:/root/spark/conf:/root/spark/assembly/target/scala-2.10/spark-assembly-1.1.1-SNAPSHOT-hadoop1.0.4.jar
-XX:MaxPermSize=128m -Djava.library.path=/root/ephemeral-hdfs/lib/native/
-Xms5g -Xmx10g -XX:MaxPermSize=10g -Dspark.akka.timeout=300
-Dspark.driver.port=59156 -Xms5g -Xmx10g -XX:MaxPermSize=10g -Xms58315M
-Xmx58315M org.apache.spark.executor.CoarseGrainedExecutorBackend
akka.tcp://spark@ip-10-226-198-178.us-west-2.compute.internal:59156/user/CoarseGrainedScheduler
5 ip-10-38-9-181.us-west-2.compute.internal 8
akka.tcp://sparkWorker@ip-10-38-9-181.us-west-2.compute.internal:34533/user/Worker
app-20140825214225-0001

Attached is the requested stack trace.



On Mon, Aug 25, 2014 at 1:35 PM, Patrick Wendell [via Apache Spark
Developers List] <ml-node+s1001551n8001h70@n3.nabble.com> wrote:

> One other idea - when things freeze up, try to run jstack on the spark
> shell process and on the executors and attach the results. It could be
> that
> somehow you are encountering a deadlock somewhere.
>
>
> On Mon, Aug 25, 2014 at 1:26 PM, Matei Zaharia <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=8001&i=0>>
> wrote:
>
> > Was the original issue with Spark 1.1 (i.e. master branch) or an earlier
> > release?
> >
> > One possibility is that your S3 bucket is in a remote Amazon region,
> which
> > would make it very slow. In my experience though saveAsTextFile has
> worked
> > even for pretty large datasets in that situation, so maybe there's
> > something else in your job causing a problem. Have you tried other
> > operations on the data, like count(), or saving synthetic datasets (e.g.
> > sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
> >
> > Matei
> >
> > On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email]
> <http://user/SendEmail.jtp?type=node&node=8001&i=1>) wrote:
> >
> > Hi jerryye,
> > Maybe if you voted up my question on Stack Overflow it would get some
> > traction and we would get nearer to a solution.
> > Thanks,
> > Amnon
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8001&i=2>
> > For additional commands, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8001&i=3>
> >
> >
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8001.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h70@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=amVycnl5ZUBnbWFpbC5jb218MXwtNTI4OTc1MTAz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>


jstack.txt (92K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/8006/0/jstack.txt>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8006.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_50941_17318172.1409003726304--

From dev-return-9043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:00:02 2014
Return-Path: <dev-return-9043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8855811461
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:00:02 +0000 (UTC)
Received: (qmail 99017 invoked by uid 500); 25 Aug 2014 22:00:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98947 invoked by uid 500); 25 Aug 2014 22:00:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98893 invoked by uid 99); 25 Aug 2014 22:00:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:00:00 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of graham.dennis@gmail.com designates 209.85.192.171 as permitted sender)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 21:59:55 +0000
Received: by mail-pd0-f171.google.com with SMTP id z10so21075895pdj.16
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 14:59:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=AEdm2nNr89Z5mfqs/MrPuwZHMOl7/Hs9yCDYvTlRfVg=;
        b=BHejAeh9Ywu5l3q2CSdNUnjrsz1PFXOkasbFsPY5TwiIsoFpBHtNe2REArZCV0lbNc
         lmV1Gdf9OO9cbc6Fn4V9MBgMYm+YT3omxo/GjrMECTBh06ebBUscmruEOPrLEnxZtOAC
         5VEtCQXhub8y2o9i9o1gOhsLJndmM1wzhhcFy1304pKeeLnwIV4Sv1JaDWMSIY5/odIZ
         gv+vdExKIg6xnvNC1B55t6lVvqwSXBlndik45SWbztszI96soKWJKNaK063BdN9Eh4mj
         1J6+pzjt4Jzjl/4XY82S/Rh57O9dWP6/dOOzOEZO1IEPwYWfsYdrW23sGtMJPRjDAovA
         y01Q==
X-Received: by 10.66.141.165 with SMTP id rp5mr15142903pab.115.1409003974578;
        Mon, 25 Aug 2014 14:59:34 -0700 (PDT)
Received: from [192.168.0.5] (ppp121-45-209-249.lns20.cbr1.internode.on.net. [121.45.209.249])
        by mx.google.com with ESMTPSA id pv10sm1301408pdb.60.2014.08.25.14.59.31
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 14:59:33 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing
From: Graham Dennis <graham.dennis@gmail.com>
In-Reply-To: <1408990419321-7989.post@n3.nabble.com>
Date: Tue, 26 Aug 2014 07:59:21 +1000
Cc: dev@spark.incubator.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <061475AE-BB75-4A6C-813F-68B70A1B8EED@gmail.com>
References: <CA+B-+fz5PN3z7+q3YaNKTODmZ047sVgb0n+6kShUh96MBZSxsw@mail.gmail.com> <CABpRO2dMTkrrH1_8bu+LKrrJmw_f2Q1Hr4P+E8cZqup1Hm4d5Q@mail.gmail.com> <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com> <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com> <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com> <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com> <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com> <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com> <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com> <CA+B-+fzkk=RMbCT7m6veQwCWGq1Jn+-zegXD23sQwa91eSyVvw@mail.gmail.com> <1408990419321-7989.post@n3.nabble.com>
To: npanj <nitinpanj@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Unless you manually patched Spark, if you have Reynold=92s patch for =
SPARK-2878, you also have the patch for SPARK-2893 which makes the =
underlying cause much more obvious and explicit.  So the below is =
unlikely to be related to SPARK-2878.

Graham

On 26 Aug 2014, at 4:13 am, npanj <nitinpanj@gmail.com> wrote:

> I am running the code with @rxin's patch in standalone mode.  In my =
case I am
> registering "org.apache.spark.graphx.GraphKryoRegistrator" .=20
>=20
> Recently I started to see "com.esotericsoftware.kryo.KryoException:
> java.io.IOException: failed to uncompress the chunk: PARSING_ERROR" . =
Has
> anyone seen this? Could it be related to this issue? > Here it trace:=20=

> --
> vids (org.apache.spark.graphx.impl.VertexAttributeBlock)
>        com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
>        com.esotericsoftware.kryo.io.Input.require(Input.java:169)
>        =
com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:710)
>        com.esotericsoftware.kryo.io.Input.readLong(Input.java:665)
>=20
> =
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySer=
ializer.read(DefaultArraySerializers.java:127)
>=20
> =
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySer=
ializer.read(DefaultArraySerializers.java:107)
>        com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
>=20
> =
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(Fie=
ldSerializer.java:611)
>=20
> =
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer=
.java:221)
>        =
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
>        =
com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
>        =
com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
>        =
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
>=20
> =
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSeria=
lizer.scala:133)
>=20
> =
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializ=
er.scala:133)
>        =
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>=20
> =
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockMan=
ager.scala:1054)
>        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>=20
> =
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:=
30)
>=20
> =
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala=
:39)
>        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
>        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>        scala.collection.Iterator$class.foreach(Iterator.scala:727)
>        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>=20
> =
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(Vert=
exPartitionBaseOps.scala:192)
>=20
> =
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.sc=
ala:78)
>=20
> =
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$appl=
y$1.apply(ReplicatedVertexView.scala:75)
>=20
> =
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$appl=
y$1.apply(ReplicatedVertexView.scala:73)
>        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>=20
> =
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD=
.scala:87)
>=20
> =
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD=
.scala:85)
>        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
>        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
>=20
> =
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>=20
> =
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
>        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>=20
> =
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)=

>=20
> =
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)=

>        org.apache.spark.scheduler.Task.run(Task.scala:54)
>=20
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:202)
>=20
> =
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:=
1145)
>=20
> =
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java=
:615)
>=20
> --
>=20
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-2878-Kryo-=
serialisation-with-custom-Kryo-registrator-failing-tp7719p7989.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:07:13 2014
Return-Path: <dev-return-9044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 367971148D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:07:13 +0000 (UTC)
Received: (qmail 12758 invoked by uid 500); 25 Aug 2014 22:07:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12691 invoked by uid 500); 25 Aug 2014 22:07:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12679 invoked by uid 99); 25 Aug 2014 22:07:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:07:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:07:07 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so20801427pdi.31
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 15:06:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=aIx1wSktsat6wXmfz6LK7OjMDN7Umk2Dfb7NSnC4cKY=;
        b=wZytypjtIMlmsJvBLdLv9mMBwn1Wa9KYySWvhI+T3x77CroTw5hJJ8PV4LhP+B0qhV
         /eukPdCsiZqKUpp5vFtS7scGjneZAQnTTEJ2fsJ36iPsjp45lbx9TbjprGs1c4l2nMs7
         i2PUcIxtugh04EvjuSwyJOS3lJAX5DFAWN/mdt1QwlCiKPqC7Dpzt0zSrFLAO0AX2khT
         x2ze9Dr7EB1Sb7KXVL5sgwLE5lm5oTr5jSFXATGhrS2RWvfqceoXTf22NxRD6GqUca9R
         C4T4XmPt8LcVX2dKScdXxIdg0BlaVLUyKSXNSJJNxhcrv8xoDRNY6h7HqscG7dOoVZP5
         I4Hw==
X-Received: by 10.70.118.9 with SMTP id ki9mr32313261pdb.104.1409004407200;
        Mon, 25 Aug 2014 15:06:47 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ba5sm787665pbd.72.2014.08.25.15.06.44
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 15:06:46 -0700 (PDT)
Date: Mon, 25 Aug 2014 15:06:44 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Timothy Chen <tnachen@gmail.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, Cody
 Koeninger <cody@koeninger.org>, Gary Malouf <malouf.gary@gmail.com>, 
 "=?utf-8?Q?dev=40mesos.apache.org?=" <dev@mesos.apache.org>
Message-ID: <etPan.53fbb374.333ab105.9a@mbp-3>
In-Reply-To: <CAFx0iW-O_StJoA_GruURYQ_UghOQP5iLzKOxQKbqThyA9Pf5YQ@mail.gmail.com>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
 <etPan.53f920b8.ded7263.ae21@mbp-3.local>
 <CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
 <etPan.53fac0f0.643c9869.9a@mbp-3.local>
 <CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
 <CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
 <etPan.53fb884f.1190cde7.9a@mbp-3>
 <CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
 <etPan.53fb9771.41a7c4c9.9a@mbp-3> <etPan.53fb97b5.4e6afb66.9a@mbp-3>
 <CAFx0iW-O_StJoA_GruURYQ_UghOQP5iLzKOxQKbqThyA9Pf5YQ@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fbb374_721da317_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fbb374_721da317_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

My problem is that I'm not sure this workaround would solve things, given the issue described here (where there was a lot of memory free but it didn't get re-offered). If you think it does, it would be good to explain why it behaves like that.

Matei

On August 25, 2014 at 2:28:18 PM, Timothy Chen (tnachen@gmail.com) wrote:

Hi Matei, 

I'm going to investigate from both Mesos and Spark side will hopefully 
have a good long term solution. In the mean time having a work around 
to start with is going to unblock folks. 

Tim 

On Mon, Aug 25, 2014 at 1:08 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote: 
> Anyway it would be good if someone from the Mesos side investigates this and 
> proposes a solution. The 32 MB per task hack isn't completely foolproof 
> either (e.g. people might allocate all the RAM to their executor and thus 
> stop being able to launch tasks), so maybe we wait on a Mesos fix for this 
> one. 
> 
> Matei 
> 
> On August 25, 2014 at 1:07:15 PM, Matei Zaharia (matei.zaharia@gmail.com) 
> wrote: 
> 
> This is kind of weird then, seems perhaps unrelated to this issue (or at 
> least to the way I understood it). Is the problem maybe that Mesos saw 0 MB 
> being freed and didn't re-offer the machine *even though there was more than 
> 32 MB free overall*? 
> 
> Matei 
> 
> On August 25, 2014 at 12:59:59 PM, Cody Koeninger (cody@koeninger.org) 
> wrote: 
> 
> I definitely saw a case where 
> 
> a. the only job running was a 256m shell 
> b. I started a 2g job 
> c. a little while later the same user as in a started another 256m shell 
> 
> My job immediately stopped making progress. Once user a killed his shells, 
> it started again. 
> 
> This is on nodes with ~15G of memory, on which we have successfully run 8G 
> jobs. 
> 
> 
> On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia@gmail.com> 
> wrote: 
>> 
>> BTW it seems to me that even without that patch, you should be getting 
>> tasks launched as long as you leave at least 32 MB of memory free on each 
>> machine (that is, the sum of the executor memory sizes is not exactly the 
>> same as the total size of the machine). Then Mesos will be able to re-offer 
>> that machine whenever CPUs free up. 
>> 
>> Matei 
>> 
>> On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary@gmail.com) 
>> wrote: 
>> 
>> We have not tried the work-around because there are other bugs in there 
>> that affected our set-up, though it seems it would help. 
>> 
>> 
>> On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote: 
>> 
>> > +1 to have the work around in. 
>> > 
>> > I'll be investigating from the Mesos side too. 
>> > 
>> > Tim 
>> > 
>> > On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia <matei.zaharia@gmail.com> 
>> > wrote: 
>> > > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's 
>> > > too 
>> > bad that this happens in fine-grained mode -- would be really good to 
>> > fix. 
>> > I'll see if we can get the workaround in 
>> > https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally 
>> > have you tried that? 
>> > > 
>> > > Matei 
>> > > 
>> > > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com) 
>> > wrote: 
>> > > 
>> > > Hi Matei, 
>> > > 
>> > > We have an analytics team that uses the cluster on a daily basis. They 
>> > use two types of 'run modes': 
>> > > 
>> > > 1) For running actual queries, they set the spark.executor.memory to 
>> > something between 4 and 8GB of RAM/worker. 
>> > > 
>> > > 2) A shell that takes a minimal amount of memory on workers (128MB) 
>> > > for 
>> > prototyping out a larger query. This allows them to not take up RAM on 
>> > the 
>> > cluster when they do not really need it. 
>> > > 
>> > > We see the deadlocks when there are a few shells in either case. From 
>> > the usage patterns we have, coarse-grained mode would be a challenge as 
>> > we 
>> > have to constantly remind people to kill their shells as soon as their 
>> > queries finish. 
>> > > 
>> > > Am I correct in viewing Mesos in coarse-grained mode as being similar 
>> > > to 
>> > Spark Standalone's cpu allocation behavior? 
>> > > 
>> > > 
>> > > 
>> > > 
>> > > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia 
>> > > <matei.zaharia@gmail.com> 
>> > wrote: 
>> > > Hey Gary, just as a workaround, note that you can use Mesos in 
>> > coarse-grained mode by setting spark.mesos.coarse=true. Then it will 
>> > hold 
>> > onto CPUs for the duration of the job. 
>> > > 
>> > > Matei 
>> > > 
>> > > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com) 
>> > wrote: 
>> > > 
>> > > I just wanted to bring up a significant Mesos/Spark issue that makes 
>> > > the 
>> > > combo difficult to use for teams larger than 4-5 people. It's covered 
>> > > in 
>> > > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is 
>> > that 
>> > > Spark's use of executors in fine-grained mode is a very different 
>> > behavior 
>> > > than many of the other common frameworks for Mesos. 
>> > > 
>> > 
> 
> 

--53fbb374_721da317_9a--


From dev-return-9045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:07:41 2014
Return-Path: <dev-return-9045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56F5011499
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:07:41 +0000 (UTC)
Received: (qmail 16042 invoked by uid 500); 25 Aug 2014 22:07:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15979 invoked by uid 500); 25 Aug 2014 22:07:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15967 invoked by uid 99); 25 Aug 2014 22:07:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:07:39 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of amnon.is@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:07:14 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <amnon.is@gmail.com>)
	id 1XM2Pw-0000Tc-FZ
	for dev@spark.incubator.apache.org; Mon, 25 Aug 2014 15:07:12 -0700
Date: Mon, 25 Aug 2014 15:07:12 -0700 (PDT)
From: amnonkhen <amnon.is@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAN6eJme7+ccOwLu1Y4Lw1a1yiQX83DHPOGmd6_61rXQ67N1yJQ@mail.gmail.com>
In-Reply-To: <etPan.53fb9bda.519b500d.9a@mbp-3>
References: <1407667288137-7795.post@n3.nabble.com> <1408668974535-7950.post@n3.nabble.com> <1408993739123-7991.post@n3.nabble.com> <etPan.53fb9bda.519b500d.9a@mbp-3>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_51289_3728329.1409004432474"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_51289_3728329.1409004432474
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Matei,
The original issue happened on a spark-1.0.2-bin-hadoop2 installation.
I will try the synthetic operation and see if I get the same results or not.
Amnon


On Mon, Aug 25, 2014 at 11:26 PM, Matei Zaharia [via Apache Spark
Developers List] <ml-node+s1001551n8000h46@n3.nabble.com> wrote:

> Was the original issue with Spark 1.1 (i.e. master branch) or an earlier
> release?
>
> One possibility is that your S3 bucket is in a remote Amazon region, which
> would make it very slow. In my experience though saveAsTextFile has worked
> even for pretty large datasets in that situation, so maybe there's
> something else in your job causing a problem. Have you tried other
> operations on the data, like count(), or saving synthetic datasets (e.g.
> sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
>
> Matei
>
> On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=0>) wrote:
>
> Hi jerryye,
> Maybe if you voted up my question on Stack Overflow it would get some
> traction and we would get nearer to a solution.
> Thanks,
> Amnon
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
>
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=1>
> For additional commands, e-mail: [hidden email]
> <http://user/SendEmail.jtp?type=node&node=8000&i=2>
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
>  To unsubscribe from saveAsTextFile to s3 on spark does not work, just
> hangs, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=7795&code=YW1ub24uaXNAZ21haWwuY29tfDc3OTV8LTkxODIwMjYzNg==>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_51289_3728329.1409004432474--

From dev-return-9046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:09:48 2014
Return-Path: <dev-return-9046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A18E8114A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:09:48 +0000 (UTC)
Received: (qmail 22966 invoked by uid 500); 25 Aug 2014 22:09:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22897 invoked by uid 500); 25 Aug 2014 22:09:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22884 invoked by uid 99); 25 Aug 2014 22:09:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:09:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:09:43 +0000
Received: by mail-la0-f50.google.com with SMTP id pi18so13982273lab.37
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 15:09:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Ah/ZWEiawfJ0UrN0oSddFrBaly748H+xKJSaIWIT4Po=;
        b=gJE7KQz378UELceD3wX+Pe+ubjKoQvKHsYTuRm0tpkxttSFNRbVj+4TRVYzAIPXecO
         U1XZSe82JdWwlFVc9WxE17LlWNdsv1ZbPo6HjVnHKk+LxXm+iVJI9ZPwGr3PErKbGCU8
         y4L7jYDfw7Xc9bhqPBPqlgjwHu3bc8R+OUyuWBjJnTqaJYRk5GpWWuOxZg7Fw1wWQGsW
         C/1H4YfxnLpFfjTBnhCft9cr4dAXFtUCb6bHOKq/NYYJVO7bWu+C16yaScMU+KKxxRzN
         iojUlzo2suhO2hWoBaGP3w4a2VXXyuW4pipDvQv+vnyD5Lf+K31WCgfywUmbfiR/U6AX
         pWBg==
MIME-Version: 1.0
X-Received: by 10.112.98.198 with SMTP id ek6mr22726042lbb.22.1409004561607;
 Mon, 25 Aug 2014 15:09:21 -0700 (PDT)
Received: by 10.25.205.204 with HTTP; Mon, 25 Aug 2014 15:09:21 -0700 (PDT)
In-Reply-To: <CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
References: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
	<CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
Date: Mon, 25 Aug 2014 15:09:21 -0700
Message-ID: <CALuGr6Yb7sd5Rx9CDCgBMnfonwG1iWxKfFpmahZUVoaJX1Xt+Q@mail.gmail.com>
Subject: Re: [Spark SQL] off-heap columnar store
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Michael,

This is great news.
Any initial proposal or design about the caching to Tachyon that you
can share so far?

I don't think there is a JIRA ticket open to track this feature yet.

- Henry

On Mon, Aug 25, 2014 at 1:13 PM, Michael Armbrust
<michael@databricks.com> wrote:
>>
>> What is the plan for getting Tachyon/off-heap support for the columnar
>> compressed store?  It's not in 1.1 is it?
>
>
> It is not in 1.1 and there are not concrete plans for adding it at this
> point.  Currently, there is more engineering investment going into caching
> parquet data in Tachyon instead.  This approach is going to have much
> better support for nested data, leverages other work being done on parquet,
> and alleviates your concerns about wire format compatibility.
>
> That said, if someone really wants to try and implement it, I don't think
> it would be very hard.  The primary issue is going to be designing a clean
> interface that is not too tied to this one implementation.
>
>
>> Also, how likely is the wire format for the columnar compressed data
>> to change?  That would be a problem for write-through or persistence.
>>
>
> We aren't making any guarantees at the moment that it won't change.  Its
> currently only intended for temporary caching of data.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:32:14 2014
Return-Path: <dev-return-9047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D483311585
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:32:14 +0000 (UTC)
Received: (qmail 75080 invoked by uid 500); 25 Aug 2014 22:32:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75015 invoked by uid 500); 25 Aug 2014 22:32:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75003 invoked by uid 99); 25 Aug 2014 22:32:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:32:13 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.175 as permitted sender)
Received: from [209.85.192.175] (HELO mail-pd0-f175.google.com) (209.85.192.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:32:07 +0000
Received: by mail-pd0-f175.google.com with SMTP id r10so21229160pdi.34
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 15:31:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=ReCKCSoinDI6d4BEqvbf6eoBymuTedTBp/rdkXh87Io=;
        b=JvhmnZ/f7I48NxkX5ALE4hE3JHl1b5redAgxhLhYJUb8Hc0RIruwa6JZFKrlEXVnCN
         eigYMgBQ4HmaXzczkbrefjMQCFPLeFuVNUf1M0ES7BtHvfxbdp5naBe0y9ScemkEywre
         +8gjrMDTdu7Uq+8U/IzmyX50yM7l9IuRUcsG8aeO1nQP7h3hUGMASXzE7zNYgCsDGC5N
         uFUm+5EPYF6Hxzhx9dfrtVnetv9Bw2zQ7l74+HXf5GjP5tTMVFB41W5DqV4yVDD2oUO3
         hIWOC/nB8Ka/5YkGeNbGkBwG2LYTr3UdFAZu2x/ryP3/ieovq+fmiRB9DnNfPNdrwpOq
         s+ZA==
X-Received: by 10.68.242.9 with SMTP id wm9mr32056345pbc.47.1409005907499;
        Mon, 25 Aug 2014 15:31:47 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id y4sm1489235pdm.1.2014.08.25.15.31.40
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 15:31:40 -0700 (PDT)
Date: Mon, 25 Aug 2014 15:31:39 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: amnonkhen <amnon.is@gmail.com>, dev@spark.incubator.apache.org
Message-ID: <etPan.53fbb94c.6763845e.9a@mbp-3>
In-Reply-To: <CAN6eJme7+ccOwLu1Y4Lw1a1yiQX83DHPOGmd6_61rXQ67N1yJQ@mail.gmail.com>
References: <1407667288137-7795.post@n3.nabble.com>
 <1408668974535-7950.post@n3.nabble.com>
 <1408993739123-7991.post@n3.nabble.com>
 <etPan.53fb9bda.519b500d.9a@mbp-3>
 <CAN6eJme7+ccOwLu1Y4Lw1a1yiQX83DHPOGmd6_61rXQ67N1yJQ@mail.gmail.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fbb94c_75a2a8d4_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fbb94c_75a2a8d4_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Got it. Another thing that would help is if you spot any exceptions or failed tasks in the web UI (http://<driver>:4040).

Matei

On August 25, 2014 at 3:07:41 PM, amnonkhen (amnon.is@gmail.com) wrote:

Hi Matei, 
The original issue happened on a spark-1.0.2-bin-hadoop2 installation. 
I will try the synthetic operation and see if I get the same results or not. 
Amnon 


On Mon, Aug 25, 2014 at 11:26 PM, Matei Zaharia [via Apache Spark 
Developers List] <ml-node+s1001551n8000h46@n3.nabble.com> wrote: 

> Was the original issue with Spark 1.1 (i.e. master branch) or an earlier 
> release? 
> 
> One possibility is that your S3 bucket is in a remote Amazon region, which 
> would make it very slow. In my experience though saveAsTextFile has worked 
> even for pretty large datasets in that situation, so maybe there's 
> something else in your job causing a problem. Have you tried other 
> operations on the data, like count(), or saving synthetic datasets (e.g. 
> sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)? 
> 
> Matei 
> 
> On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email] 
> <http://user/SendEmail.jtp?type=node&node=8000&i=0>) wrote: 
> 
> Hi jerryye, 
> Maybe if you voted up my question on Stack Overflow it would get some 
> traction and we would get nearer to a solution. 
> Thanks, 
> Amnon 
> 
> 
> 
> -- 
> View this message in context: 
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html 
> 
> Sent from the Apache Spark Developers List mailing list archive at 
> Nabble.com. 
> 
> --------------------------------------------------------------------- 
> To unsubscribe, e-mail: [hidden email] 
> <http://user/SendEmail.jtp?type=node&node=8000&i=1> 
> For additional commands, e-mail: [hidden email] 
> <http://user/SendEmail.jtp?type=node&node=8000&i=2> 
> 
> 
> 
> ------------------------------ 
> If you reply to this email, your message will be added to the discussion 
> below: 
> 
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html 
> To unsubscribe from saveAsTextFile to s3 on spark does not work, just 
> hangs, click here 
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=7795&code=YW1ub24uaXNAZ21haWwuY29tfDc3OTV8LTkxODIwMjYzNg==> 
> . 
> NAML 
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
> 




-- 
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html 
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
--53fbb94c_75a2a8d4_9a--


From dev-return-9048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 22:32:39 2014
Return-Path: <dev-return-9048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A53111586
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 22:32:39 +0000 (UTC)
Received: (qmail 76351 invoked by uid 500); 25 Aug 2014 22:32:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76282 invoked by uid 500); 25 Aug 2014 22:32:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76257 invoked by uid 99); 25 Aug 2014 22:32:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:32:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tnachen@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 22:32:12 +0000
Received: by mail-oa0-f52.google.com with SMTP id o6so11115217oag.25
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 15:32:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5YpPhx+Z5Ixjd+kGkQ1RfYtSZJ/g27Y/vj9q8x4XB/I=;
        b=eDJd1um1xyOxBKuMkSIq/i+vG3pFxU2E75guJQozcd3GoJtQC+sNEU4C0pLVlt/ws1
         TscnKtXanCVN+B2TUM8aUnKTFosm5/NFoavR/e+wweiRc5EJ/mbUdiDS9RtLsocbs2e+
         +caIULRl4JrSPJ3fqg/ssAikQIQPb0ejSYTs6iZvgA5kxgDowByCq6x17FY8v+DbZ7kk
         b2pv4rOg2yhEa7a/eU884SY71ONRaO/vEechnvZkDzidlTvXkfsvGVmp+nhD4lcLISDW
         3Abkw0JRIwvUOsaCkcujW9pgJPz9e03UFAuI8OZ2uQvtQSA18sRq7XpJwBoo8KPvUkRP
         nbWA==
MIME-Version: 1.0
X-Received: by 10.182.137.195 with SMTP id qk3mr19621091obb.5.1409005931646;
 Mon, 25 Aug 2014 15:32:11 -0700 (PDT)
Received: by 10.60.37.4 with HTTP; Mon, 25 Aug 2014 15:32:11 -0700 (PDT)
In-Reply-To: <etPan.53fbb374.333ab105.9a@mbp-3>
References: <CAGOvqiqB2no7SKqv4hy61vvmkwJodovNpsGfsekCiSKTVZmVsg@mail.gmail.com>
	<etPan.53f920b8.ded7263.ae21@mbp-3.local>
	<CAGOvqiqJrf4C7dOT9=XY+7TRQg0tPBWv6igt9Ls8eUruFV9cUw@mail.gmail.com>
	<etPan.53fac0f0.643c9869.9a@mbp-3.local>
	<CAFx0iW87RHimQn7PxEDng5Cq2v5OoHTS487s-kC6kYP07wnDvg@mail.gmail.com>
	<CAGOvqioSMykj4KtjDKOEaQvgV09inUMEKy5KyXN0r87p0Ohc7g@mail.gmail.com>
	<etPan.53fb884f.1190cde7.9a@mbp-3>
	<CAKWX9VVotbBPr6p7dTn0jn6NG9TKfAwUPP3AyUXmxbdokLxJzA@mail.gmail.com>
	<etPan.53fb9771.41a7c4c9.9a@mbp-3>
	<etPan.53fb97b5.4e6afb66.9a@mbp-3>
	<CAFx0iW-O_StJoA_GruURYQ_UghOQP5iLzKOxQKbqThyA9Pf5YQ@mail.gmail.com>
	<etPan.53fbb374.333ab105.9a@mbp-3>
Date: Mon, 25 Aug 2014 15:32:11 -0700
Message-ID: <CAFx0iW8ZDzt6OpuGcUX69Au54vJzjy0Ps_+WQaLwqRYzj2gSEA@mail.gmail.com>
Subject: Re: Mesos/Spark Deadlock
From: Timothy Chen <tnachen@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Cody Koeninger <cody@koeninger.org>, 
	Gary Malouf <malouf.gary@gmail.com>, "dev@mesos.apache.org" <dev@mesos.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think it solves Cody's problem which still need more
investigating, but I believe it does solve the problem you described
earlier.

I just confirmed with Mesos folks that we no longer need the minimum
memory requirement so we'll be dropping that soon and the workaround
might not be needed for the next mesos release.

Tim

On Mon, Aug 25, 2014 at 3:06 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> My problem is that I'm not sure this workaround would solve things, given
> the issue described here (where there was a lot of memory free but it didn't
> get re-offered). If you think it does, it would be good to explain why it
> behaves like that.
>
> Matei
>
> On August 25, 2014 at 2:28:18 PM, Timothy Chen (tnachen@gmail.com) wrote:
>
> Hi Matei,
>
> I'm going to investigate from both Mesos and Spark side will hopefully
> have a good long term solution. In the mean time having a work around
> to start with is going to unblock folks.
>
> Tim
>
> On Mon, Aug 25, 2014 at 1:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>> Anyway it would be good if someone from the Mesos side investigates this
>> and
>> proposes a solution. The 32 MB per task hack isn't completely foolproof
>> either (e.g. people might allocate all the RAM to their executor and thus
>> stop being able to launch tasks), so maybe we wait on a Mesos fix for this
>> one.
>>
>> Matei
>>
>> On August 25, 2014 at 1:07:15 PM, Matei Zaharia (matei.zaharia@gmail.com)
>> wrote:
>>
>> This is kind of weird then, seems perhaps unrelated to this issue (or at
>> least to the way I understood it). Is the problem maybe that Mesos saw 0
>> MB
>> being freed and didn't re-offer the machine *even though there was more
>> than
>> 32 MB free overall*?
>>
>> Matei
>>
>> On August 25, 2014 at 12:59:59 PM, Cody Koeninger (cody@koeninger.org)
>> wrote:
>>
>> I definitely saw a case where
>>
>> a. the only job running was a 256m shell
>> b. I started a 2g job
>> c. a little while later the same user as in a started another 256m shell
>>
>> My job immediately stopped making progress. Once user a killed his shells,
>> it started again.
>>
>> This is on nodes with ~15G of memory, on which we have successfully run 8G
>> jobs.
>>
>>
>> On Mon, Aug 25, 2014 at 2:02 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>>
>>> BTW it seems to me that even without that patch, you should be getting
>>> tasks launched as long as you leave at least 32 MB of memory free on each
>>> machine (that is, the sum of the executor memory sizes is not exactly the
>>> same as the total size of the machine). Then Mesos will be able to
>>> re-offer
>>> that machine whenever CPUs free up.
>>>
>>> Matei
>>>
>>> On August 25, 2014 at 5:05:56 AM, Gary Malouf (malouf.gary@gmail.com)
>>> wrote:
>>>
>>> We have not tried the work-around because there are other bugs in there
>>> that affected our set-up, though it seems it would help.
>>>
>>>
>>> On Mon, Aug 25, 2014 at 12:54 AM, Timothy Chen <tnachen@gmail.com> wrote:
>>>
>>> > +1 to have the work around in.
>>> >
>>> > I'll be investigating from the Mesos side too.
>>> >
>>> > Tim
>>> >
>>> > On Sun, Aug 24, 2014 at 9:52 PM, Matei Zaharia
>>> > <matei.zaharia@gmail.com>
>>> > wrote:
>>> > > Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's
>>> > > too
>>> > bad that this happens in fine-grained mode -- would be really good to
>>> > fix.
>>> > I'll see if we can get the workaround in
>>> > https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally
>>> > have you tried that?
>>> > >
>>> > > Matei
>>> > >
>>> > > On August 23, 2014 at 4:30:27 PM, Gary Malouf (malouf.gary@gmail.com)
>>> > wrote:
>>> > >
>>> > > Hi Matei,
>>> > >
>>> > > We have an analytics team that uses the cluster on a daily basis.
>>> > > They
>>> > use two types of 'run modes':
>>> > >
>>> > > 1) For running actual queries, they set the spark.executor.memory to
>>> > something between 4 and 8GB of RAM/worker.
>>> > >
>>> > > 2) A shell that takes a minimal amount of memory on workers (128MB)
>>> > > for
>>> > prototyping out a larger query. This allows them to not take up RAM on
>>> > the
>>> > cluster when they do not really need it.
>>> > >
>>> > > We see the deadlocks when there are a few shells in either case. From
>>> > the usage patterns we have, coarse-grained mode would be a challenge as
>>> > we
>>> > have to constantly remind people to kill their shells as soon as their
>>> > queries finish.
>>> > >
>>> > > Am I correct in viewing Mesos in coarse-grained mode as being similar
>>> > > to
>>> > Spark Standalone's cpu allocation behavior?
>>> > >
>>> > >
>>> > >
>>> > >
>>> > > On Sat, Aug 23, 2014 at 7:16 PM, Matei Zaharia
>>> > > <matei.zaharia@gmail.com>
>>> > wrote:
>>> > > Hey Gary, just as a workaround, note that you can use Mesos in
>>> > coarse-grained mode by setting spark.mesos.coarse=true. Then it will
>>> > hold
>>> > onto CPUs for the duration of the job.
>>> > >
>>> > > Matei
>>> > >
>>> > > On August 23, 2014 at 7:57:30 AM, Gary Malouf (malouf.gary@gmail.com)
>>> > wrote:
>>> > >
>>> > > I just wanted to bring up a significant Mesos/Spark issue that makes
>>> > > the
>>> > > combo difficult to use for teams larger than 4-5 people. It's covered
>>> > > in
>>> > > https://issues.apache.org/jira/browse/MESOS-1688. My understanding is
>>> > that
>>> > > Spark's use of executors in fine-grained mode is a very different
>>> > behavior
>>> > > than many of the other common frameworks for Mesos.
>>> > >
>>> >
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Aug 25 23:54:07 2014
Return-Path: <dev-return-9049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 498E3118D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Aug 2014 23:54:07 +0000 (UTC)
Received: (qmail 76778 invoked by uid 500); 25 Aug 2014 23:54:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76702 invoked by uid 500); 25 Aug 2014 23:54:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76690 invoked by uid 99); 25 Aug 2014 23:54:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 23:54:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Aug 2014 23:53:40 +0000
Received: by mail-ig0-f180.google.com with SMTP id l13so3643123iga.13
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 16:53:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MhtlbcngyN5+JQ5cBZOUJTf79gDZdI1QZGpV0x//rY4=;
        b=chjPWLGwCfkfqEQU6SV+vmLFrs16LTQWXG0qnpL3+kt5jiKaBBaVFRSj0KC+3l0Av3
         tAEvs3hthcElfZonCE0XKt8rpx76W2SBkjoPU4WTwcCxiMPtI3gflbMd1kYywswe/ANR
         SvW9QqGILv2Q2F5cGA9jvr4RooI0E3O6K2wuZTBUMs1DPyNUopcOeXMIPohZf1HhzmfW
         f+Dyx6TH/9cBYl69BPoj0B4TtRwkY5F5/OmAsLSGQAYoxGRLsJRQCP/AKHkjrLLd3mAN
         Epe6K+hIqwErqHrBi2E1ZbqucFyey4muxGbLvpmLwTdqXoZMg51/HEdvNHVptp+2XuCe
         bazw==
MIME-Version: 1.0
X-Received: by 10.42.216.135 with SMTP id hi7mr26434172icb.12.1409010819120;
 Mon, 25 Aug 2014 16:53:39 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Mon, 25 Aug 2014 16:53:39 -0700 (PDT)
Date: Mon, 25 Aug 2014 16:53:39 -0700
Message-ID: <CA+FETEJp-Xyj7g7u4B83i_oZb-zcka2YxsKH+2wwCQiMVZWWoQ@mail.gmail.com>
Subject: too many CancelledKeyException throwed from ConnectionManager
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf301d420c5bc62e05017ce591
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301d420c5bc62e05017ce591
Content-Type: text/plain; charset=UTF-8

Hi Folks,

We are testing our home-made KMeans algorithm using Spark on Yarn.
Recently, we've found that the application failed frequently when doing
clustering over 300,000,000 users (each user is represented by a feature
vector and the whole data set is around 600,000,000). After digging into
the job log, we've found that there are many CancelledKeyException throwed
by ConnectionManager but not observed other exceptions. We double frequent
CancelledKeyException brings the whole application down since the
application often failed on the third or fourth iteration for large
datasets. Welcome to any directional suggestions.

*Errors in job log*:
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,43199)
14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
SendingConnectionManagerId not found
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@2570cd62
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@2570cd62
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@37c8b85a
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@37c8b85a
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:287)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-668.rfiserve.net,41913)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-668.rfiserve.net,41913)
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@fcea3a4
14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
SendingConnectionManagerId not found
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@fcea3a4


Best
Shengzhe

--20cf301d420c5bc62e05017ce591--

From dev-return-9050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 01:23:51 2014
Return-Path: <dev-return-9050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA43411BBD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 01:23:51 +0000 (UTC)
Received: (qmail 66533 invoked by uid 500); 26 Aug 2014 01:23:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66477 invoked by uid 500); 26 Aug 2014 01:23:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66460 invoked by uid 99); 26 Aug 2014 01:23:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 01:23:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ankurdave@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 01:23:24 +0000
Received: by mail-pd0-f180.google.com with SMTP id v10so21105161pde.25
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 18:23:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:to:cc:subject:in-reply-to:references:user-agent:date
         :message-id:mime-version:content-type;
        bh=ASwWH0+gPdySa8swuXuGdLh0hW/YQyBWyHKC1nG+A6w=;
        b=cIsEtGQYyjMIU4jHBf+VzDH7D+jyxIq4njsx8vt6bu1zzKX2n18/b5ydXeilu1zv3N
         ++qgisN62gnTUo5kMaziTXjXADUoPpovCDX8Q/QMuUm3mVjcolv3uYHVvY0pMfgEkD41
         LLLO+PNIRxwtdOohYqYZhNEltJh1RREpnBOMS4FZtlgkV2e/5GUDDup9wvr7a5zxZmv4
         PJu6nOFb921HDfjh0O7UKcDyS2oAaCg6lXbmEpzzUiGl2PspRgSC07MxiKv80ARSqjd2
         sxqdyL4wiMdinzqoQfvZ8wHMi4vXl5v+EJ1G50SvNgKrvevOCNs2n8yY1JcNrvbW05yg
         CJpQ==
X-Received: by 10.70.89.43 with SMTP id bl11mr8765617pdb.163.1409016203035;
        Mon, 25 Aug 2014 18:23:23 -0700 (PDT)
Received: from ankur-mbp-2 (c-67-164-94-63.hsd1.ca.comcast.net. [67.164.94.63])
        by mx.google.com with ESMTPSA id y9sm4863202pas.23.2014.08.25.18.23.21
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 25 Aug 2014 18:23:22 -0700 (PDT)
From: Ankur Dave <ankurdave@gmail.com>
To: Jeffrey Picard <jpicard@placeiq.com>, npanj <nitinpanj@gmail.com>
Cc: dev@spark.incubator.apache.org
Subject: Re: Graphx seems to be broken while Creating a large graph(6B nodes in my case)
In-Reply-To: <F40C94BD-7BA9-428C-8BAE-721E9A7E2AEA@placeiq.com>
References: <1408754619472-7966.post@n3.nabble.com> <F40C94BD-7BA9-428C-8BAE-721E9A7E2AEA@placeiq.com>
User-Agent: Notmuch/0.18.1 (http://notmuchmail.org) Emacs/24.4.50.1 (x86_64-apple-darwin13.2.0)
Date: Mon, 25 Aug 2014 18:23:19 -0700
Message-ID: <m2r404xcvs.fsf@gmail.com>
MIME-Version: 1.0
Content-Type: text/plain
X-Virus-Checked: Checked by ClamAV on apache.org

I posted the fix on the JIRA ticket (https://issues.apache.org/jira/browse/SPARK-3190). To update the user list, this is indeed an integer overflow problem when summing up the partition sizes. The fix is to use Longs for the sum: https://github.com/apache/spark/pull/2106.

Ankur


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 04:03:08 2014
Return-Path: <dev-return-9051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD0E411FCA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 04:03:08 +0000 (UTC)
Received: (qmail 90970 invoked by uid 500); 26 Aug 2014 04:03:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90898 invoked by uid 500); 26 Aug 2014 04:03:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90882 invoked by uid 99); 26 Aug 2014 04:03:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 04:03:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 04:02:42 +0000
Received: by mail-wg0-f49.google.com with SMTP id k14so14164818wgh.32
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 21:02:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=OuEuqr5qkQgW+rOEEEwKuCqujbqb/6nAygS9h1BHlPY=;
        b=hEOXQ4v/0u8ws1f2Cw6r/BicRhejYperit9oG8uss27IiW14VcFKctWu04dF6zArRo
         3DcX6gMuot6aVH0rx83OzqGJQzGWtMcyOGapN/CgtouFpDD8T3yr0yy60NEzbyhEV5zX
         xwWfjpYb6UWRcrCItKuXwzKUltuBveYsbm7jn5fDRACa8YGucLaOAum681v3aENDdsK4
         7nHJZDlXzrP7yznCJIuW1eH3zbROcQqwF+4FXloHhzSsiggD/8+G2SGH2c/y2Ahv5WkZ
         bmZ7QtJGxViaN+MjhD+Lq+Z3yJfnssmrt82bgNF5w1zP7wPB4jjjt5DWcTiB/Mi9s7cW
         IVJQ==
X-Received: by 10.194.187.101 with SMTP id fr5mr27364613wjc.10.1409025761413;
 Mon, 25 Aug 2014 21:02:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 25 Aug 2014 21:02:01 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 26 Aug 2014 00:02:01 -0400
Message-ID: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
Subject: Handling stale PRs
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd6b9acfd0ed70501805f0c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6b9acfd0ed70501805f0c
Content-Type: text/plain; charset=UTF-8

Check this out:
https://github.com/apache/spark/pulls?q=is%3Aopen+is%3Apr+sort%3Aupdated-asc

We're hitting close to 300 open PRs. Those are the least recently updated
ones.

I think having a low number of stale (i.e. not recently updated) PRs is a
good thing to shoot for. It doesn't leave contributors hanging (which feels
bad for contributors), and reduces project clutter (which feels bad for
maintainers/committers).

What is our approach to tackling this problem?

I think communicating and enforcing a clear policy on how stale PRs are
handled might be a good way to reduce the number of stale PRs we have
without making contributors feel rejected.

I don't know what such a policy would look like, but it should be
enforceable and "lightweight"--i.e. it shouldn't feel like a hammer used to
reject people's work, but rather a necessary tool to keep the project's
contributions relevant and manageable.

Nick

--047d7bd6b9acfd0ed70501805f0c--

From dev-return-9052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 04:07:47 2014
Return-Path: <dev-return-9052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69BA111FFE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 04:07:47 +0000 (UTC)
Received: (qmail 1444 invoked by uid 500); 26 Aug 2014 04:07:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1380 invoked by uid 500); 26 Aug 2014 04:07:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 97516 invoked by uid 99); 26 Aug 2014 04:05:58 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kartheek.mbms@gmail.com designates 74.125.82.44 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=KHsjxRTGxvzxe/caiSuDQL/zy+DT/YqJoKjtq+JpoE8=;
        b=j9kmbYBsHREz0cWl6jJzWR4BsAYZDHsXARcOspLEJi0Gr3QVbYQYxcYVan7Ds2MOjR
         LdIxxKrvPiWSTjwB5EccdgUMI+r/dgcnnu/Dn9L+nS9UUCm5YAlfujcNJpCXyMtJipDe
         62yP5ojXLyS9zwkBHamQB+JpP5hrXi89HZWr3ub5U8u2d9xoLtW2OHcoqIDwoleIfJ1e
         MC0UbWe1GG5mM4lT+1eUtKmuXjqIGaoYqKRRs84g+4lfHeGLmeSfv8aD6L96rNBNvY8M
         oi4E60yOvpfY9xraltv8/arpb2K/DBpA/aWmevn79/ypocv30C/dvHUfSDaKncdMbZvh
         xjrQ==
MIME-Version: 1.0
X-Received: by 10.180.108.147 with SMTP id hk19mr18987041wib.4.1409025931899;
 Mon, 25 Aug 2014 21:05:31 -0700 (PDT)
Date: Tue, 26 Aug 2014 09:35:31 +0530
Message-ID: <CAAbaoBDmdBRPew5n3JWKp9=YT4pOS58fqZ7ceD73Eomu0iBJVA@mail.gmail.com>
Subject: RDD replication in Spark
From: rapelly kartheek <kartheek.mbms@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f3ba5b52678570501806ac7
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3ba5b52678570501806ac7
Content-Type: text/plain; charset=UTF-8

Hi,

I've exercised multiple options available for persist() including  RDD
replication. I have gone thru the classes that involve in caching/storing
the RDDS at different levels. StorageLevel class plays a pivotal role by
recording whether to use memory or disk or to replicate the RDD on multiple
nodes.
The class LocationIterator iterates over the preferred machines one by one  for
each partition that is replicated. I got a rough idea of CoalescedRDD.
Please correct me if I am wrong.

But I am looking for the code that chooses the resources to replicate the
RDDs. Can someone please tell me how replication takes place and how do we
choose the resources for replication. I just want to know as to where
should I look into to understand how the replication happens.



Thank you so much!!!

regards

-Karthik

--e89a8f3ba5b52678570501806ac7--

From dev-return-9053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 04:58:25 2014
Return-Path: <dev-return-9053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEFCD110E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 04:58:25 +0000 (UTC)
Received: (qmail 68715 invoked by uid 500); 26 Aug 2014 04:58:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68654 invoked by uid 500); 26 Aug 2014 04:58:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68642 invoked by uid 99); 26 Aug 2014 04:58:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 04:58:24 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 04:58:19 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so21952184pdi.3
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 21:57:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=kIBxsz8tCGugWHzrEe26reU3b9lguMllnKQPVFTZUh0=;
        b=eXy/SPQX0TlmAJe7UkMISaJT7DaDapbdaBgyzUTNHWoaFK4XvXcOHIC1NEitgi3NnQ
         jrOO9CMrsH+z9XYx+iH9Ac2mc8c5F9KmRFREs/H8hA7eSSPjUQgn5otDmckoJtawiskU
         Q7YDI9pggb4ardjoMW/0CYB3eDXmJzw7N/TIoqAQvUnRwJA8hIWTlkRvI3B7hYLE106W
         z/YLLjmr42q1GVFUeEfvT1lisCMQxBAJH1t1LjI4eNcS3IHPsfIEwLD8MB+c9FO4WCDq
         qu2U3p0dkCwDM/594iFDKLpF1RiQFf73K4vcp+UHX9IcK7k2d/XizCOUEmHGC2IUWAal
         XirA==
X-Received: by 10.68.215.106 with SMTP id oh10mr2010252pbc.98.1409029079163;
        Mon, 25 Aug 2014 21:57:59 -0700 (PDT)
Received: from mbp-3.local (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id uy2sm6614064pac.13.2014.08.25.21.57.57
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 25 Aug 2014 21:57:58 -0700 (PDT)
Date: Mon, 25 Aug 2014 21:57:56 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>, dev
 <dev@spark.apache.org>
Message-ID: <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
In-Reply-To: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
Subject: Re: Handling stale PRs
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fc13d4_6ceaf087_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fc13d4_6ceaf087_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hey Nicholas,

In general we've been looking at these periodically (at least I have) and asking people to close out of date ones, but it's true that the list has gotten fairly large. We should probably have an expiry time of a few months and close them automatically. I agree that it's daunting to see so many open PRs.

Matei

On August 25, 2014 at 9:03:09 PM, Nicholas Chammas (nicholas.chammas@gmail.com) wrote:

Check this out: 
https://github.com/apache/spark/pulls?q=is%3Aopen+is%3Apr+sort%3Aupdated-asc 

We're hitting close to 300 open PRs. Those are the least recently updated 
ones. 

I think having a low number of stale (i.e. not recently updated) PRs is a 
good thing to shoot for. It doesn't leave contributors hanging (which feels 
bad for contributors), and reduces project clutter (which feels bad for 
maintainers/committers). 

What is our approach to tackling this problem? 

I think communicating and enforcing a clear policy on how stale PRs are 
handled might be a good way to reduce the number of stale PRs we have 
without making contributors feel rejected. 

I don't know what such a policy would look like, but it should be 
enforceable and "lightweight"--i.e. it shouldn't feel like a hammer used to 
reject people's work, but rather a necessary tool to keep the project's 
contributions relevant and manageable. 

Nick 

--53fc13d4_6ceaf087_9a--


From dev-return-9054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 05:23:10 2014
Return-Path: <dev-return-9054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58D9A11180
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 05:23:10 +0000 (UTC)
Received: (qmail 98644 invoked by uid 500); 26 Aug 2014 05:23:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98576 invoked by uid 500); 26 Aug 2014 05:23:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98564 invoked by uid 99); 26 Aug 2014 05:23:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 05:23:09 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of amnon.is@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 05:22:43 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so13157554qac.33
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 22:22:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=e9deWK2LC2q/H7d9LEEcwM1Xhaa5TGBkx08xVhVa7UA=;
        b=mTz0wf0XH5RAX6EgcDfjF9ZDTi356RZOoibWL923Itdl4mXYQR7lnt0KXBX9ORzuIz
         V8lksCzdxMAEtA4/2g4hiOSwVgSER7SRqCJTdPi4KVTMzU36MJE5W+ZT3Hujm0dHM7E4
         AYJJBaKRV5W3mkMZOFg9y2fXz+LM9W9rd/IjrUR2OYaJAPG+6y9jhBwP6J4Zb942EEfr
         mdCnwO/OSxtwXNEjhMXxd6zoBxwpla8VAiJ4Pz6E52Ywh+U4pu7bTYn9e6iC/bXAHT9a
         +sMvIioe+jD/ML+k1LKtBh9m5Nbx/E5xYQocFO5grWJqlshmdiWb9kPemcBoM1rs0pBl
         2xFA==
X-Received: by 10.140.18.211 with SMTP id 77mr39413942qgf.57.1409030561996;
 Mon, 25 Aug 2014 22:22:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.229.104.136 with HTTP; Mon, 25 Aug 2014 22:22:01 -0700 (PDT)
In-Reply-To: <etPan.53fbb94c.6763845e.9a@mbp-3>
References: <1407667288137-7795.post@n3.nabble.com> <1408668974535-7950.post@n3.nabble.com>
 <1408993739123-7991.post@n3.nabble.com> <etPan.53fb9bda.519b500d.9a@mbp-3>
 <CAN6eJme7+ccOwLu1Y4Lw1a1yiQX83DHPOGmd6_61rXQ67N1yJQ@mail.gmail.com> <etPan.53fbb94c.6763845e.9a@mbp-3>
From: Amnon Khen <amnon.is@gmail.com>
Date: Tue, 26 Aug 2014 08:22:01 +0300
Message-ID: <CAN6eJmdqeUyQgstuUJSTnqR=S+f8M6T+E4B+YhbmakmvWgxO8g@mail.gmail.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11353dd02027a10501817e55
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11353dd02027a10501817e55
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

There were no failures nor exceptions.


On Tue, Aug 26, 2014 at 1:31 AM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Got it. Another thing that would help is if you spot any exceptions or
> failed tasks in the web UI (http://<driver>:4040).
>
> Matei
>
> On August 25, 2014 at 3:07:41 PM, amnonkhen (amnon.is@gmail.com) wrote:
>
> Hi Matei,
> The original issue happened on a spark-1.0.2-bin-hadoop2 installation.
> I will try the synthetic operation and see if I get the same results or
> not.
> Amnon
>
>
> On Mon, Aug 25, 2014 at 11:26 PM, Matei Zaharia [via Apache Spark
> Developers List] <ml-node+s1001551n8000h46@n3.nabble.com> wrote:
>
> > Was the original issue with Spark 1.1 (i.e. master branch) or an earlie=
r
> > release?
> >
> > One possibility is that your S3 bucket is in a remote Amazon region,
> which
> > would make it very slow. In my experience though saveAsTextFile has
> worked
> > even for pretty large datasets in that situation, so maybe there's
> > something else in your job causing a problem. Have you tried other
> > operations on the data, like count(), or saving synthetic datasets (e.g=
.
> > sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
> >
> > Matei
> >
> > On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email]
> > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D0>) wrote:
> >
> > Hi jerryye,
> > Maybe if you voted up my question on Stack Overflow it would get some
> > traction and we would get nearer to a solution.
> > Thanks,
> > Amnon
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
> >
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: [hidden email]
> > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D1>
> > For additional commands, e-mail: [hidden email]
> > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D2>
> >
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the discussio=
n
> > below:
> >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
> > To unsubscribe from saveAsTextFile to s3 on spark does not work, just
> > hangs, click here
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D7795&code=3DYW1ub24uaXNAZ21haW=
wuY29tfDc3OTV8LTkxODIwMjYzNg=3D=3D>
>
> > .
> > NAML
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=
=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNam=
espace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabbl=
eNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subs=
cribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_ins=
tant_email%21nabble%3Aemail.naml>
>
> >
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
>

--001a11353dd02027a10501817e55--

From dev-return-9055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 05:25:26 2014
Return-Path: <dev-return-9055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D152011189
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 05:25:26 +0000 (UTC)
Received: (qmail 1962 invoked by uid 500); 26 Aug 2014 05:25:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1891 invoked by uid 500); 26 Aug 2014 05:25:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1878 invoked by uid 99); 26 Aug 2014 05:25:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 05:25:25 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 05:25:21 +0000
Received: by mail-oi0-f53.google.com with SMTP id a3so203889oib.40
        for <dev@spark.incubator.apache.org>; Mon, 25 Aug 2014 22:25:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=9E3qnq+kDfSAx1ZFuZcxTTR03r1WVNl69xlUGFJRR7Y=;
        b=xwLwZ+H0pMbAGjbk2RJmSeGhnCotODNaxU/3RZEL5D+LOgg2PvylHXSO86o6UT0n60
         gNK+yPqDQcQO+WsazUAUCzObCxMQyIT2VUOe53dO7Q0RLuTD82WsiJfiiX0IMQBRw/Ia
         /MUkWiMkTLb7caqsH/cpRwiiXOj/9WGH3k2nHnSyiC97yzmFvV7CxcPJIWiKmIlp3Vr1
         0ZPNpcXhengnzimeWozCYoLjANVUp/lNn+ilHcSbDKL/1r6Vf07HRIpQmojYlrahPPK/
         epASbooM88ejbtii9I/fD9/qkseZZJHN0/KvVgE95LZSm6IaorOyYG1vbdgtKzSeaYzU
         wd5w==
MIME-Version: 1.0
X-Received: by 10.182.226.202 with SMTP id ru10mr188836obc.77.1409030700429;
 Mon, 25 Aug 2014 22:25:00 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Mon, 25 Aug 2014 22:25:00 -0700 (PDT)
In-Reply-To: <CAN6eJmdqeUyQgstuUJSTnqR=S+f8M6T+E4B+YhbmakmvWgxO8g@mail.gmail.com>
References: <1407667288137-7795.post@n3.nabble.com>
	<1408668974535-7950.post@n3.nabble.com>
	<1408993739123-7991.post@n3.nabble.com>
	<etPan.53fb9bda.519b500d.9a@mbp-3>
	<CAN6eJme7+ccOwLu1Y4Lw1a1yiQX83DHPOGmd6_61rXQ67N1yJQ@mail.gmail.com>
	<etPan.53fbb94c.6763845e.9a@mbp-3>
	<CAN6eJmdqeUyQgstuUJSTnqR=S+f8M6T+E4B+YhbmakmvWgxO8g@mail.gmail.com>
Date: Mon, 25 Aug 2014 22:25:00 -0700
Message-ID: <CABPQxsv3-VpSGR=YCrf-R7GQA3h23BPQNrj777OoVB9+nj6sWw@mail.gmail.com>
Subject: Re: saveAsTextFile to s3 on spark does not work, just hangs
From: Patrick Wendell <pwendell@gmail.com>
To: Amnon Khen <amnon.is@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c32efa6079600501818698
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c32efa6079600501818698
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Hey Amnon,

So just to make sure I understand - you also saw the same issue with 1.0.2?
Just asking because whether or not this regresses the 1.0.2 behavior is
important for our own bug tracking.

- Patrick


On Mon, Aug 25, 2014 at 10:22 PM, Amnon Khen <amnon.is@gmail.com> wrote:

> There were no failures nor exceptions.
>
>
> On Tue, Aug 26, 2014 at 1:31 AM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
> > Got it. Another thing that would help is if you spot any exceptions or
> > failed tasks in the web UI (http://<driver>:4040).
> >
> > Matei
> >
> > On August 25, 2014 at 3:07:41 PM, amnonkhen (amnon.is@gmail.com) wrote:
> >
> > Hi Matei,
> > The original issue happened on a spark-1.0.2-bin-hadoop2 installation.
> > I will try the synthetic operation and see if I get the same results or
> > not.
> > Amnon
> >
> >
> > On Mon, Aug 25, 2014 at 11:26 PM, Matei Zaharia [via Apache Spark
> > Developers List] <ml-node+s1001551n8000h46@n3.nabble.com> wrote:
> >
> > > Was the original issue with Spark 1.1 (i.e. master branch) or an
> earlier
> > > release?
> > >
> > > One possibility is that your S3 bucket is in a remote Amazon region,
> > which
> > > would make it very slow. In my experience though saveAsTextFile has
> > worked
> > > even for pretty large datasets in that situation, so maybe there's
> > > something else in your job causing a problem. Have you tried other
> > > operations on the data, like count(), or saving synthetic datasets
> (e.g.
> > > sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?
> > >
> > > Matei
> > >
> > > On August 25, 2014 at 12:09:25 PM, amnonkhen ([hidden email]
> > > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D0>) wrote:
> > >
> > > Hi jerryye,
> > > Maybe if you voted up my question on Stack Overflow it would get some
> > > traction and we would get nearer to a solution.
> > > Thanks,
> > > Amnon
> > >
> > >
> > >
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
> > >
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: [hidden email]
> > > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D1>
> > > For additional commands, e-mail: [hidden email]
> > > <http://user/SendEmail.jtp?type=3Dnode&node=3D8000&i=3D2>
> > >
> > >
> > >
> > > ------------------------------
> > > If you reply to this email, your message will be added to the
> discussion
> > > below:
> > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
> > > To unsubscribe from saveAsTextFile to s3 on spark does not work, just
> > > hangs, click here
> > > <
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D7795&code=3DYW1ub24uaXNAZ21haW=
wuY29tfDc3OTV8LTkxODIwMjYzNg=3D=3D
> >
> >
> > > .
> > > NAML
> > > <
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=
=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNam=
espace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabbl=
eNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subs=
cribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_ins=
tant_email%21nabble%3Aemail.naml
> >
> >
> > >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/saveAsTextFile-=
to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> >
>

--001a11c32efa6079600501818698--

From dev-return-9056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 06:02:26 2014
Return-Path: <dev-return-9056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFCAC112BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 06:02:26 +0000 (UTC)
Received: (qmail 71724 invoked by uid 500); 26 Aug 2014 06:02:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71646 invoked by uid 500); 26 Aug 2014 06:02:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71632 invoked by uid 99); 26 Aug 2014 06:02:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 06:02:25 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 06:02:21 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so11409177oag.7
        for <dev@spark.apache.org>; Mon, 25 Aug 2014 23:02:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+7BY7xyK9v7ZePD4q9ytlJXm6/CE3hOurw3Q28dqBQo=;
        b=VrkrqgM5oT0a8JJOBL650CfZhZphG8LdfK9+bOlezlY3/3AYwptmQBvpFU9/jnNjYL
         EpEe4uUCg7B39mQZyJRx9D+KV2/4umSe94FNXgWv7DMcTjE1YWaq1EqSTBW2hfP6VB6Z
         FT8j2wb9tND5by6Ekj+qAcolhRiG9c7MlUgxE1Zr6QgFpJl5TfpDq224y0UQwdCEcUHN
         j0woc7atGDVhtD+ADuyruHaLZIZxZ/QAS/vEJU0nUfmQk2uZpkw19A1ViR0mOYJU7gqG
         ESWGxmiqQFPsVQhcLng3Z8ka2f3dnn+1/LVMI9tJ6Je2ZLwC8i6s4ZFO7e2GXawJIF24
         V52Q==
MIME-Version: 1.0
X-Received: by 10.60.52.208 with SMTP id v16mr24753202oeo.15.1409032920861;
 Mon, 25 Aug 2014 23:02:00 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Mon, 25 Aug 2014 23:02:00 -0700 (PDT)
In-Reply-To: <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
	<etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
Date: Mon, 25 Aug 2014 23:02:00 -0700
Message-ID: <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
Subject: Re: Handling stale PRs
From: Patrick Wendell <pwendell@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133450ab9924f0501820aac
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133450ab9924f0501820aac
Content-Type: text/plain; charset=ISO-8859-1

Hey Nicholas,

Thanks for bringing this up. There are a few dimensions to this... one is
that it's actually precedurally difficult for us to close pull requests.
I've proposed several different solutions to ASF infra to streamline the
process, but thus far they haven't been open to any of my ideas:

https://issues.apache.org/jira/browse/INFRA-7918
https://issues.apache.org/jira/browse/INFRA-8241

The more important thing, maybe, is how we want to deal with this
culturally. And I think we need to do a better job of making sure no pull
requests go unattended (i.e. waiting for committer feedback). If patches go
stale, it should be because the user hasn't responded, not us.

Another thing is that we should, IMO, err on the side of explicitly saying
"no" or "not yet" to patches, rather than letting them linger without
attention. We do get patches where the user is well intentioned, but it is
a feature that doesn't make sense to add, or isn't well thought out or
explained, or the review effort would be so large it's not within our
capacity to look at just yet.

Most other ASF projects I know just ignore these patches. I'd prefer if we
took the approach of politely explaining why in the current form the patch
isn't acceptable and closing it (potentially w/ tips on how to improve it
or narrow the scope).

- Patrick




On Mon, Aug 25, 2014 at 9:57 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Hey Nicholas,
>
> In general we've been looking at these periodically (at least I have) and
> asking people to close out of date ones, but it's true that the list has
> gotten fairly large. We should probably have an expiry time of a few months
> and close them automatically. I agree that it's daunting to see so many
> open PRs.
>
> Matei
>
> On August 25, 2014 at 9:03:09 PM, Nicholas Chammas (
> nicholas.chammas@gmail.com) wrote:
>
> Check this out:
>
> https://github.com/apache/spark/pulls?q=is%3Aopen+is%3Apr+sort%3Aupdated-asc
>
> We're hitting close to 300 open PRs. Those are the least recently updated
> ones.
>
> I think having a low number of stale (i.e. not recently updated) PRs is a
> good thing to shoot for. It doesn't leave contributors hanging (which feels
> bad for contributors), and reduces project clutter (which feels bad for
> maintainers/committers).
>
> What is our approach to tackling this problem?
>
> I think communicating and enforcing a clear policy on how stale PRs are
> handled might be a good way to reduce the number of stale PRs we have
> without making contributors feel rejected.
>
> I don't know what such a policy would look like, but it should be
> enforceable and "lightweight"--i.e. it shouldn't feel like a hammer used to
> reject people's work, but rather a necessary tool to keep the project's
> contributions relevant and manageable.
>
> Nick
>

--001a1133450ab9924f0501820aac--

From dev-return-9057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 07:38:09 2014
Return-Path: <dev-return-9057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0454011551
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 07:38:09 +0000 (UTC)
Received: (qmail 67715 invoked by uid 500); 26 Aug 2014 07:38:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67649 invoked by uid 500); 26 Aug 2014 07:38:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67637 invoked by uid 99); 26 Aug 2014 07:38:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 07:38:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of velvia.github@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 07:37:42 +0000
Received: by mail-wi0-f171.google.com with SMTP id hi2so3695241wib.4
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 00:37:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0aJrDTR7wWXzCVdkTMzIyug9ht9nP6QVQTT4h4jMk9Q=;
        b=DVkZbLmFPs5cjgUw22Zxgvylc7wGpkaqJgEygPxgK9AMmZBp4PqduKVETE86Mnqj52
         t/37mxxJOdQ2ssoWzB2wU3oAKl0EgHfAdxCiyj85cBVK+UOrfT+jaCBxv8PA16qZH4FG
         quesD8R4ToQU9VFHoAFb+NyLJM1bLh4n11NjN/5Aw1r7l8YosIHr5QwxZhCRU/Nwo9Cl
         gl08WMuD7SvXFpg+LJcxxPzJnp6vthLcyfM0ZA6SOJyx6gqEHwgbojT46VbZsxEMa2by
         FgZ/LhhBIgGZb+LM8A1w9ll6VNk5fCSm4YyrSfmX3aQTGl48HCRvFgA60ACfpdZZHbnF
         fyQw==
MIME-Version: 1.0
X-Received: by 10.180.20.142 with SMTP id n14mr5602288wie.22.1409038661710;
 Tue, 26 Aug 2014 00:37:41 -0700 (PDT)
Received: by 10.216.33.10 with HTTP; Tue, 26 Aug 2014 00:37:41 -0700 (PDT)
In-Reply-To: <CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
References: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
	<CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
Date: Tue, 26 Aug 2014 00:37:41 -0700
Message-ID: <CAN6Vra0oSCPAr14mO3RfuPz5D1WnB1C8xFEcUmCuoqSiLzrGHw@mail.gmail.com>
Subject: Re: [Spark SQL] off-heap columnar store
From: Evan Chan <velvia.github@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

What would be the timeline for the parquet caching work?

The reason I'm asking about the columnar compressed format is that
there are some problems for which Parquet is not practical.

On Mon, Aug 25, 2014 at 1:13 PM, Michael Armbrust
<michael@databricks.com> wrote:
>> What is the plan for getting Tachyon/off-heap support for the columnar
>> compressed store?  It's not in 1.1 is it?
>
>
> It is not in 1.1 and there are not concrete plans for adding it at this
> point.  Currently, there is more engineering investment going into caching
> parquet data in Tachyon instead.  This approach is going to have much better
> support for nested data, leverages other work being done on parquet, and
> alleviates your concerns about wire format compatibility.
>
> That said, if someone really wants to try and implement it, I don't think it
> would be very hard.  The primary issue is going to be designing a clean
> interface that is not too tied to this one implementation.
>
>>
>> Also, how likely is the wire format for the columnar compressed data
>> to change?  That would be a problem for write-through or persistence.
>
>
> We aren't making any guarantees at the moment that it won't change.  Its
> currently only intended for temporary caching of data.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 07:48:13 2014
Return-Path: <dev-return-9058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4EEBA115A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 07:48:13 +0000 (UTC)
Received: (qmail 93701 invoked by uid 500); 26 Aug 2014 07:48:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93632 invoked by uid 500); 26 Aug 2014 07:48:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93621 invoked by uid 99); 26 Aug 2014 07:48:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 07:48:10 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.171] (HELO mail-lb0-f171.google.com) (209.85.217.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 07:48:06 +0000
Received: by mail-lb0-f171.google.com with SMTP id w7so903680lbi.2
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 00:47:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=+eyyf/XHQByRZxcGHWJrlw4Z/rjCEtlOLBZI7iL1WbQ=;
        b=dR3RFhywDrphVvPIvDuavBoXRPm+SbaYrFDxZyGIwVFsSE/vJHIUX97H91Icfx8gc8
         x9t1cHWPedvRg9qzLs/SIyoyOjiiUU5/ORPnVeSoFP390zZ7vS0si0nXWj5t7p3qb5Wk
         st4xXU9lXDp031gzTD/PHaUE7sva7b5vWACTSTFOVU6TNDdKq1O1C6Af2dJq7liAN9D9
         WME9eU+lDLqrhqnpEOFqiwGE7iAAcy/TQHB8g4Jyvmkm7gG9qB12n7FbEzSJEUEA2lMO
         2lYtFCjRwl4fL85FtDbrnAlA9xL6+UCD1XzNdWzA7eYp8ScN1PTtlFCo+pXiVTNhIIv/
         fZbQ==
X-Gm-Message-State: ALoCoQmE98CnrMc5sK6YszuuAZeKWfLDg79aEc2YT7X/JkkrwaKxWY2botcfjEuHNivAKae3M7+B
X-Received: by 10.112.183.162 with SMTP id en2mr24355218lbc.51.1409039264247;
 Tue, 26 Aug 2014 00:47:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.30.5 with HTTP; Tue, 26 Aug 2014 00:47:24 -0700 (PDT)
In-Reply-To: <CAN6Vra0oSCPAr14mO3RfuPz5D1WnB1C8xFEcUmCuoqSiLzrGHw@mail.gmail.com>
References: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
 <CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com> <CAN6Vra0oSCPAr14mO3RfuPz5D1WnB1C8xFEcUmCuoqSiLzrGHw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 26 Aug 2014 00:47:24 -0700
Message-ID: <CAAswR-60n8cj5WU47TNw-AvmVBAmdveb7XEu4YmjyTjqbR=hCg@mail.gmail.com>
Subject: Re: [Spark SQL] off-heap columnar store
To: Evan Chan <velvia.github@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11348b36d202890501838475
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11348b36d202890501838475
Content-Type: text/plain; charset=UTF-8

>
> Any initial proposal or design about the caching to Tachyon that you
> can share so far?


Caching parquet files in tachyon with saveAsParquetFile and then reading
them with parquetFile should already work. You can use SQL on these tables
by using registerTempTable.

Some of the general parquet work that we have been doing includes: #1935
<https://github.com/apache/spark/pull/1935>, SPARK-2721
<https://issues.apache.org/jira/browse/SPARK-2721>, SPARK-3036
<https://issues.apache.org/jira/browse/SPARK-3036>, SPARK-3037
<https://issues.apache.org/jira/browse/SPARK-3037> and #1819
<https://github.com/apache/spark/pull/1819>

The reason I'm asking about the columnar compressed format is that
> there are some problems for which Parquet is not practical.


Can you elaborate?

--001a11348b36d202890501838475--

From dev-return-9059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 08:01:31 2014
Return-Path: <dev-return-9059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77B5E11611
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 08:01:31 +0000 (UTC)
Received: (qmail 22906 invoked by uid 500); 26 Aug 2014 08:01:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22840 invoked by uid 500); 26 Aug 2014 08:01:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22828 invoked by uid 99); 26 Aug 2014 08:01:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 08:01:30 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of milos.nikolic83@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 08:01:23 +0000
Received: by mail-wi0-f177.google.com with SMTP id ho1so3693339wib.4
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 01:01:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:content-transfer-encoding:subject:message-id:date
         :to:mime-version;
        bh=qsWJixOGnOTBrKJkWKZXBTcrHchMyuR+WybRo8mMfpU=;
        b=K9pcN5EWzn4XB2Sh45lpUYusDbbEYhaE9J/imeUBN7MXPz/4w26K70ciwge+B9NcTj
         vAHvWP68UfUulebN0Xt4Zz06Xpproiqikd0Ll79A+Ut2YH76RRjDRRDR/8wgLUl9svfP
         s82Yde70od0aUcMG4rySQ//lXM+PG5x+GqtZhHquiIm9xm3btclIjmQoH4HfhAU1fGhu
         tVnOVn2nMC0q2uc0BbQ/JwEhBXQH0pAg2QUYTkOL+IFjzQA8mBC252CVZlAacOla7MHe
         coPfo9yTx6plFWlbuJlgbTf1RHeAIMnMoGsz7iJubuGsB8DIR74ctvFaBe8Yk3ITiKts
         jltg==
X-Received: by 10.180.99.97 with SMTP id ep1mr19797700wib.33.1409040062577;
        Tue, 26 Aug 2014 01:01:02 -0700 (PDT)
Received: from icdhcp-1-032.epfl.ch (icdhcp-1-032.epfl.ch. [128.178.116.32])
        by mx.google.com with ESMTPSA id dc9sm9488620wib.5.2014.08.26.01.01.01
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 26 Aug 2014 01:01:01 -0700 (PDT)
From: Milos Nikolic <milos.nikolic83@gmail.com>
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
Subject: Partitioning strategy changed in Spark 1.0.x?
Message-Id: <EBBB1012-E630-40A5-9A72-404A93D05590@gmail.com>
Date: Tue, 26 Aug 2014 10:01:00 +0200
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi guys,=20

I=92ve noticed some changes in the behavior of partitioning under Spark =
1.0.x.=20
I=92d appreciate if someone could explain what has changed in the =
meantime.=20

Here is a small example. I want to create two RDD[(K, V)] objects and =
then=20
collocate partitions with the same K on one node. When the same =
partitioner=20
for two RDDs is used, partitions with the same K end up being on =
different nodes.

    // Let's say I have 10 nodes=20
    val partitioner =3D new HashPartitioner(10)    =20
   =20
    // Create RDD=20
    val rdd =3D sc.parallelize(0 until 10).map(k =3D> (k, =
computeValue(k)))=20
   =20
    // Partition twice using the same partitioner=20
    rdd.partitionBy(partitioner).foreach { case (k, v) =3D> =
println("Dummy1 -> k =3D " + k) }=20
    rdd.partitionBy(partitioner).foreach { case (k, v) =3D> =
println("Dummy2 -> k =3D " + k) }=20

The output on one node is:=20
    Dummy1 -> k =3D 2=20
    Dummy2 -> k =3D 7=20

I was expecting to see the same keys on each node. That was happening =
under Spark 0.9.2, but not under Spark 1.0.x.=20

Anyone has an idea what has changed in the meantime? Or how to get =
corresponding partitions on one node?=20

Thanks in advance,
Milos=

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 08:58:03 2014
Return-Path: <dev-return-9060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7001B117F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 08:58:03 +0000 (UTC)
Received: (qmail 56242 invoked by uid 500); 26 Aug 2014 08:58:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56169 invoked by uid 500); 26 Aug 2014 08:58:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56153 invoked by uid 99); 26 Aug 2014 08:58:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 08:58:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 08:57:56 +0000
Received: by mail-ie0-f178.google.com with SMTP id rd18so11264431iec.9
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 01:57:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=w7UBUI+gLnlOMfLyvyvV3nxhqf4QUggwbYCOGxrrINY=;
        b=OyFupcMJeqBpYOVRQsgYACk05k6oWSwHGlGQQSjQ/65SrNiN4juTclsxHH4Mjzb8ty
         bj8UXpo/B8SH52P12vctEAzpbXgtshgzw7m4LhvxCL7Ee6bYUkmRvVQ0lL4SWOSEsYlB
         nsNVakk4IHnYgaZ/Bad2LH4cgePpwAnT7Tekzbm8UVrvsdnxAyysWLmW37HdlOqcoPtL
         3np0keJXhOL6IBbrjFw8cEU0p4dbJLJ1+MfUdRVlqPDFv3sQSCZ1E3ce/Mq4UhJLRhB7
         h+e4m90c8Y4xRzj/a3cPglykNtwL6QIzN2VQ3ICfoMFfAzmg1TaxoRS91MQOtkXk1N3j
         6ALA==
X-Gm-Message-State: ALoCoQkbg+3bd1gS9ZeoX8fuASqfu6+MZiZs6BYqxJIEt88dbtr52lIHr0unMSeRNfGpFrgQ7Nmz
X-Received: by 10.43.65.195 with SMTP id xn3mr6773132icb.48.1409043456140;
 Tue, 26 Aug 2014 01:57:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.11.37 with HTTP; Tue, 26 Aug 2014 01:57:16 -0700 (PDT)
In-Reply-To: <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 26 Aug 2014 09:57:16 +0100
Message-ID: <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com>
Subject: Re: Handling stale PRs
To: dev <dev@spark.apache.org>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, Aug 26, 2014 at 7:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Most other ASF projects I know just ignore these patches. I'd prefer if we

Agree, this drives me crazy. It kills part of JIRA's usefulness. Spark
is blessed/cursed with incredible inbound load, but would love to
still see the project get this right-er than, say, Hadoop.

> The more important thing, maybe, is how we want to deal with this
> culturally. And I think we need to do a better job of making sure no pull
> requests go unattended (i.e. waiting for committer feedback). If patches go
> stale, it should be because the user hasn't responded, not us.

Stale JIRAs are a symptom, not a problem per se. I also want to see
the backlog cleared, but automatically closing doesn't help, if the
problem is too many JIRAs and not enough committer-hours to look at
them. Some noise gets closed, but some easy or important fixes may
disappear as well.

> Another thing is that we should, IMO, err on the side of explicitly saying
> "no" or "not yet" to patches, rather than letting them linger without
> attention. We do get patches where the user is well intentioned, but it is

Completely agree. The solution is partly more supply of committer time
on JIRAs. But that is detracting from the work the committers
themselves want to do. More of the solution is reducing demand by
helping people create useful, actionable, non-duplicate JIRAs from the
start. Or encouraging people to resolve existing JIRAs and shepherding
those in.

Elsewhere, I've found people reluctant to close JIRA for fear of
offending or turning off contributors. I think the opposite is true.
There is nothing wrong with "no" or "not now" especially accompanied
with constructive feedback. Better to state for the record what is not
being looked at and why, than let people work on and open the same
JIRAs repeatedly.

I have also found in the past that a culture of tolerating eternal
JIRAs led people to file JIRAs in order to forget about a problem --
it's in JIRA, and it's "in progress", so it feels like someone else is
going to fix it later and so it can be forgotten now.

For what it's worth, I think these project and culture mechanics are
so important and it's my #1 concern for Spark at this stage. This
challenge exists so much more here, exactly because there is so much
potential. I'd love to help by trying to identify and close stale
JIRAs but am afraid that tagging them is just adding to the heap of
work.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 10:55:10 2014
Return-Path: <dev-return-9061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3B5111B74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 10:55:09 +0000 (UTC)
Received: (qmail 34330 invoked by uid 500); 26 Aug 2014 10:55:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34267 invoked by uid 500); 26 Aug 2014 10:55:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34256 invoked by uid 99); 26 Aug 2014 10:55:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 10:55:08 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 10:54:38 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id E5930E0
	for <dev@spark.apache.org>; Tue, 26 Aug 2014 10:54:36 +0000 (UTC)
Received: from G4W6303.americas.hpqcorp.net (16.210.26.228) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 26 Aug 2014 10:53:12 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.222]) by
 G4W6303.americas.hpqcorp.net ([16.210.26.228]) with mapi id 14.03.0169.001;
 Tue, 26 Aug 2014 10:53:12 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Gradient descent and runMiniBatchSGD
Thread-Topic: Gradient descent and runMiniBatchSGD
Thread-Index: Ac/BGakk7EDh0bW5SIe9VrxUpkwvcg==
Date: Tue, 26 Aug 2014 10:53:10 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36G4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I've implemented back propagation algorithm using Gradient class and a simp=
le update using Updater class. Then I run the algorithm with mllib's Gradie=
ntDescent class. I have troubles in scaling out this implementation. I thou=
ght that if I partition my data into the number of workers then performance=
 will increase, because each worker will run a step of gradient descent on =
its partition of data. But this does not happen and each worker seems to pr=
ocess all data (if miniBatchFraction =3D=3D 1.0 as in mllib's logisic regre=
ssion implementation). For me, this doesn't make sense, because then only s=
ingle Worker will provide the same performance. Could someone elaborate on =
this and correct me if I am wrong. How can I scale out the algorithm with m=
any Workers?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36G4W3292americas_--

From dev-return-9062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 11:43:42 2014
Return-Path: <dev-return-9062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28D9A11DB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 11:43:42 +0000 (UTC)
Received: (qmail 18010 invoked by uid 500); 26 Aug 2014 11:43:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17949 invoked by uid 500); 26 Aug 2014 11:43:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17937 invoked by uid 99); 26 Aug 2014 11:43:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 11:43:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 11:43:36 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so13418740qap.37
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 04:43:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=dADHTZP7opQJeby6oQWLm9pZSvg4FEXW4kdrSqM0s+4=;
        b=qFzMCFklzYS8NLmLwoEQ9n7ttBQJz8eV3J3cxIDK1kF/KEdzneoMJdGHlWWGjhkwYH
         qSj7ozIz58PdwAvWRpC74nK8LBlOiegtuub2Ord4ZLD4tdWANY3sr76mhSsBAGSlTtWT
         eJiyt3ufR1WwLa9qwe7U6BsNF9zCYnObg67ywLpMbFIWU7lDP5xSakI1P48sdv1QDiw3
         eekCR7gqsHlDE3GzcWR0ICOvELpGlvnDe9MLbgDAT20DaHuBO1Hy8wg5uhQylA/D8/Mm
         zX9It8gg9XBFLWVMlgeBi9NxtJ1HazDiRNy6z3SAJITCk1mRwSQclHk8CPVl8SAHGpEB
         ed+w==
MIME-Version: 1.0
X-Received: by 10.224.46.68 with SMTP id i4mr20499137qaf.73.1409053396038;
 Tue, 26 Aug 2014 04:43:16 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Tue, 26 Aug 2014 04:43:15 -0700 (PDT)
Date: Tue, 26 Aug 2014 07:43:15 -0400
Message-ID: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
Subject: CoHadoop Papers
From: Gary Malouf <malouf.gary@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1f53823fcbf050186cf54
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f53823fcbf050186cf54
Content-Type: text/plain; charset=UTF-8

One of my colleagues has been questioning me as to why Spark/HDFS makes no
attempts to try to co-locate related data blocks.  He pointed to this
paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on the
CoHadoop research and the performance improvements it yielded for
Map/Reduce jobs.

Would leveraging these ideas for writing data from Spark make sense/be
worthwhile?

--001a11c1f53823fcbf050186cf54--

From dev-return-9063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 12:19:09 2014
Return-Path: <dev-return-9063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 048C511ED9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 12:19:09 +0000 (UTC)
Received: (qmail 91017 invoked by uid 500); 26 Aug 2014 12:19:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90904 invoked by uid 500); 26 Aug 2014 12:19:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90833 invoked by uid 99); 26 Aug 2014 12:19:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:19:07 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matt@redhat.com designates 209.132.183.28 as permitted sender)
Received: from [209.132.183.28] (HELO mx1.redhat.com) (209.132.183.28)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:19:01 +0000
Received: from int-mx11.intmail.prod.int.phx2.redhat.com (int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QCIZCj019986
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
	Tue, 26 Aug 2014 08:18:35 -0400
Received: from eeyore.local (ovpn01.gateway.prod.ext.phx2.redhat.com [10.5.9.1])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s7QCIWa6016649;
	Tue, 26 Aug 2014 08:18:33 -0400
Message-ID: <53FC7B18.2090900@redhat.com>
Date: Tue, 26 Aug 2014 08:18:32 -0400
From: Matthew Farrellee <matt@redhat.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.7.0
MIME-Version: 1.0
To: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
CC: Nicholas Chammas <nicholas.chammas@gmail.com>
Subject: Re: Handling stale PRs
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com> <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com> <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com>
In-Reply-To: <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Virus-Checked: Checked by ClamAV on apache.org

On 08/26/2014 04:57 AM, Sean Owen wrote:
> On Tue, Aug 26, 2014 at 7:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Most other ASF projects I know just ignore these patches. I'd prefer if we
>
> Agree, this drives me crazy. It kills part of JIRA's usefulness. Spark
> is blessed/cursed with incredible inbound load, but would love to
> still see the project get this right-er than, say, Hadoop.

totally agree, this applies to patches as well as jiras. i'll add that 
projects who let things simply linger are missing an opportunity to 
engage their community.

spark should capitalize on its momentum to build a smoothly running 
community (vs not and accept an unbounded backlog as inevitable).


>> The more important thing, maybe, is how we want to deal with this
>> culturally. And I think we need to do a better job of making sure no pull
>> requests go unattended (i.e. waiting for committer feedback). If patches go
>> stale, it should be because the user hasn't responded, not us.
>
> Stale JIRAs are a symptom, not a problem per se. I also want to see
> the backlog cleared, but automatically closing doesn't help, if the
> problem is too many JIRAs and not enough committer-hours to look at
> them. Some noise gets closed, but some easy or important fixes may
> disappear as well.

engagement in the community really needs to go both ways. it's 
reasonable for PRs that stop merging or have open comments that need 
resolution by the PRer to be loudly timed out. a similar thing goes for 
jiras, if there's a request for more information to resolve a bug and 
that information does not appear, half of the communication is gone and 
a loud time out is reasonable.

easy and important are in the eyes of the beholder. timeouts can go both 
ways. a jira or pr that has been around for a period of time (say 1/3 
the to-close timeout) should bump up for evaluation, hopefully resulting 
in few "easy" or "important" issues falling through the cracks.

fyi, i'm periodically going through the pyspark jiras, trying to 
reproduce issues, coalesce duplicates and ask for details. i've not been 
given any sort of permission to do this, i don't have any special 
position in the community to do this - in a well functioning community 
everyone should feel free to jump in and help.


>> Another thing is that we should, IMO, err on the side of explicitly saying
>> "no" or "not yet" to patches, rather than letting them linger without
>> attention. We do get patches where the user is well intentioned, but it is
>
> Completely agree. The solution is partly more supply of committer time
> on JIRAs. But that is detracting from the work the committers
> themselves want to do. More of the solution is reducing demand by
> helping people create useful, actionable, non-duplicate JIRAs from the
> start. Or encouraging people to resolve existing JIRAs and shepherding
> those in.

saying no/not-yet is a vitally important piece of information.


> Elsewhere, I've found people reluctant to close JIRA for fear of
> offending or turning off contributors. I think the opposite is true.
> There is nothing wrong with "no" or "not now" especially accompanied
> with constructive feedback. Better to state for the record what is not
> being looked at and why, than let people work on and open the same
> JIRAs repeatedly.

well stated!


> I have also found in the past that a culture of tolerating eternal
> JIRAs led people to file JIRAs in order to forget about a problem --
> it's in JIRA, and it's "in progress", so it feels like someone else is
> going to fix it later and so it can be forgotten now.

there's some value in these now-i-can-forget jira, though i'm not 
personally a fan. it can be good to keep them around and reachable by 
search, but they should be clearly marked as no/not-yet or something 
similar.


> For what it's worth, I think these project and culture mechanics are
> so important and it's my #1 concern for Spark at this stage. This
> challenge exists so much more here, exactly because there is so much
> potential. I'd love to help by trying to identify and close stale
> JIRAs but am afraid that tagging them is just adding to the heap of
> work.

+1 concern and potential!


best,


matt

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 12:21:14 2014
Return-Path: <dev-return-9064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5729911EE6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 12:21:14 +0000 (UTC)
Received: (qmail 95438 invoked by uid 500); 26 Aug 2014 12:21:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95370 invoked by uid 500); 26 Aug 2014 12:21:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95358 invoked by uid 99); 26 Aug 2014 12:21:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:21:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:20:46 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so14721873qgf.35
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 05:20:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9DoRcZXYtZQAQEccKWClVlIp9+xOQEQZxwU1fI12mqY=;
        b=uMaIKMhGMnoCrk9mquMh5ZK75Zgmgu+vwR1LsrDZeoHYSvB1BUmblKRRBYTxp+o2eI
         d53da7f2wsmg639M9yPSOstSUN4ZRvRO1eksb9hSkL53B7+sFdfD7vePUyuhKog33X+/
         ow+4n5DBLsmBAiImIVFM/CJyUZnoIm6HnRUR0S/dkE8nz2e9dQxMXKk6N5y3qDS8B5Zj
         xH5YKv5M/9Nk2NjU99qmvE7ZnkT5vKAwlfL6NONvoD6N/ozD/hwgqV5Fmab+BJSJZpEl
         hXvvUIN1fnTdqKUXCmTBwHs0qv/TEqe9cHAj6IabqY/jb94x1qdZ8qFFtpR6U0oczdr4
         W89A==
MIME-Version: 1.0
X-Received: by 10.140.43.245 with SMTP id e108mr42106788qga.76.1409055645486;
 Tue, 26 Aug 2014 05:20:45 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Tue, 26 Aug 2014 05:20:45 -0700 (PDT)
In-Reply-To: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
References: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
Date: Tue, 26 Aug 2014 08:20:45 -0400
Message-ID: <CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
Subject: Re: CoHadoop Papers
From: Gary Malouf <malouf.gary@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a666437d4a80501875584
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a666437d4a80501875584
Content-Type: text/plain; charset=UTF-8

It appears support for this type of control over block placement is going
out in the next version of HDFS:
https://issues.apache.org/jira/browse/HDFS-2576


On Tue, Aug 26, 2014 at 7:43 AM, Gary Malouf <malouf.gary@gmail.com> wrote:

> One of my colleagues has been questioning me as to why Spark/HDFS makes no
> attempts to try to co-locate related data blocks.  He pointed to this
> paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on the
> CoHadoop research and the performance improvements it yielded for
> Map/Reduce jobs.
>
> Would leveraging these ideas for writing data from Spark make sense/be
> worthwhile?
>
>
>

--001a113a666437d4a80501875584--

From dev-return-9065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 12:32:19 2014
Return-Path: <dev-return-9065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B75011F2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 12:32:19 +0000 (UTC)
Received: (qmail 20632 invoked by uid 500); 26 Aug 2014 12:32:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20568 invoked by uid 500); 26 Aug 2014 12:32:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20557 invoked by uid 99); 26 Aug 2014 12:32:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:32:17 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [49.212.34.109] (HELO oss.nttdata.co.jp) (49.212.34.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 12:32:12 +0000
Received: from Kousuke-no-MacBook-Pro.local (113x43x180x139.ap113.ftth.ucom.ne.jp [113.43.180.139])
	by oss.nttdata.co.jp (Postfix) with ESMTP id 0294F17EE07;
	Tue, 26 Aug 2014 21:31:45 +0900 (JST)
Message-ID: <53FC7E2E.8020802@oss.nttdata.co.jp>
Date: Tue, 26 Aug 2014 21:31:42 +0900
From: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: yao <yaoshengzhe@gmail.com>, dev@spark.apache.org
Subject: Re: too many CancelledKeyException throwed from ConnectionManager
References: <CA+FETEJp-Xyj7g7u4B83i_oZb-zcka2YxsKH+2wwCQiMVZWWoQ@mail.gmail.com>
In-Reply-To: <CA+FETEJp-Xyj7g7u4B83i_oZb-zcka2YxsKH+2wwCQiMVZWWoQ@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: clamav-milter 0.98.4 at oss.nttdata.co.jp
X-Virus-Status: Clean
X-Spam-Checker-Version: SpamAssassin 3.2.5 (2008-06-10) on oss.nttdata.co.jp
X-Virus-Checked: Checked by ClamAV on apache.org
X-Old-Spam-Status: No, score=-99.6 required=13.0 tests=CONTENT_TYPE_PRESENT,
	UNPARSEABLE_RELAY,USER_IN_WHITELIST autolearn=ham version=3.2.5

Hi Shengzhe

I faced to same situation.

I think, Connection and ConnectionManager have some race condition issues
and the error you mentioned may be caused by the issues.
Now I'm trying to resolve the issue in 
https://github.com/apache/spark/pull/2019.
Please check it out.

- Kousuke

(2014/08/26 8:53), yao wrote:
> Hi Folks,
>
> We are testing our home-made KMeans algorithm using Spark on Yarn.
> Recently, we've found that the application failed frequently when doing
> clustering over 300,000,000 users (each user is represented by a feature
> vector and the whole data set is around 600,000,000). After digging into
> the job log, we've found that there are many CancelledKeyException throwed
> by ConnectionManager but not observed other exceptions. We double frequent
> CancelledKeyException brings the whole application down since the
> application often failed on the third or fourth iteration for large
> datasets. Welcome to any directional suggestions.
>
> *Errors in job log*:
> java.nio.channels.CancelledKeyException
>          at
> org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
>          at
> org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
> ConnectionManagerId(lsv-289.rfiserve.net,43199)
> 14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
> SendingConnectionManagerId not found
> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
> sun.nio.ch.SelectionKeyImpl@2570cd62
> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
> sun.nio.ch.SelectionKeyImpl@2570cd62
> java.nio.channels.CancelledKeyException
>          at
> org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
>          at
> org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
> ConnectionManagerId(lsv-289.rfiserve.net,56727)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
> ConnectionManagerId(lsv-289.rfiserve.net,56727)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
> ConnectionManagerId(lsv-289.rfiserve.net,56727)
> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
> sun.nio.ch.SelectionKeyImpl@37c8b85a
> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
> sun.nio.ch.SelectionKeyImpl@37c8b85a
> java.nio.channels.CancelledKeyException
>          at
> org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:287)
>          at
> org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
> ConnectionManagerId(lsv-668.rfiserve.net,41913)
> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
> ConnectionManagerId(lsv-668.rfiserve.net,41913)
> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
> sun.nio.ch.SelectionKeyImpl@fcea3a4
> 14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
> SendingConnectionManagerId not found
> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
> sun.nio.ch.SelectionKeyImpl@fcea3a4
>
>
> Best
> Shengzhe
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 13:22:37 2014
Return-Path: <dev-return-9066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0827111104
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 13:22:37 +0000 (UTC)
Received: (qmail 56021 invoked by uid 500); 26 Aug 2014 13:22:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55955 invoked by uid 500); 26 Aug 2014 13:22:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55939 invoked by uid 99); 26 Aug 2014 13:22:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 13:22:36 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 13:22:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1XMGhN-0005ag-OF
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 06:22:09 -0700
Date: Tue, 26 Aug 2014 06:22:09 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409059329658-8031.post@n3.nabble.com>
In-Reply-To: <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com> <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com> <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com>
Subject: Re: Handling stale PRs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Sean Owen wrote
> Stale JIRAs are a symptom, not a problem per se. I also want to see
> the backlog cleared, but automatically closing doesn't help, if the
> problem is too many JIRAs and not enough committer-hours to look at
> them. Some noise gets closed, but some easy or important fixes may
> disappear as well.

Agreed. All of the problems mentioned in this thread are symptoms. There's
no shortage of talent and enthusiasm within the Spark community. The people
and the product are wonderful. The process: not so much. Spark has been
wildly successful, some growing pains are to be expected.

Given 100+ contributors, Spark is a big project. As with big data, big
projects can run into scaling issues. There's no magic to running a
successful big project, but it does require greater planning and discipline.
JIRA is great for issue tracking, but it's not a replacement for a project
plan. Quarterly releases are a great idea, everyone knows the schedule. What
we need is concise plan for each release with a clear scope statement.
Without knowing what is in scope and out of scope for a release, we end up
with a laundry list of things to do, but no clear goal. Laundry lists don't
scale well.

I don't mind helping with planning and documenting releases. This is
especially helpful for new contributors who don't know where to start. I
have done that successfully on many projects using Jira and Confluence, so I
know it can be done. To address immediate concerns of open PRs and
excessive, overlapping Jira issues, we probably have to create a meta issue
and assign resources to fix it. I don't mind helping with that also.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-stale-PRs-tp8015p8031.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 13:48:46 2014
Return-Path: <dev-return-9067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 564C3112AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 13:48:46 +0000 (UTC)
Received: (qmail 18736 invoked by uid 500); 26 Aug 2014 13:48:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18668 invoked by uid 500); 26 Aug 2014 13:48:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18656 invoked by uid 99); 26 Aug 2014 13:48:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 13:48:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of eerlands@redhat.com designates 209.132.183.39 as permitted sender)
Received: from [209.132.183.39] (HELO mx6-phx2.redhat.com) (209.132.183.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 13:48:18 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx6-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s7QDmERA021973;
	Tue, 26 Aug 2014 09:48:14 -0400
Date: Tue, 26 Aug 2014 09:48:12 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: Matthew Farrellee <matt@redhat.com>
Cc: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>,
        Nicholas Chammas <nicholas.chammas@gmail.com>
Message-ID: <568375341.11725705.1409060892322.JavaMail.zimbra@redhat.com>
In-Reply-To: <53FC7B18.2090900@redhat.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com> <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com> <CAMAsSd+HRwRC5dWusS3s7h1N58hBaL7UcFeCLoEbuY7a5FrWOg@mail.gmail.com> <53FC7B18.2090900@redhat.com>
Subject: Re: Handling stale PRs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.6]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC36 (Linux)/8.0.6_GA_5922)
Thread-Topic: Handling stale PRs
Thread-Index: 7X52XKl9lAana/AzOCOCn8FBAueiqg==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----

> >> Another thing is that we should, IMO, err on the side of explicitly saying
> >> "no" or "not yet" to patches, rather than letting them linger without
> >> attention. We do get patches where the user is well intentioned, but it is
> >
> > Completely agree. The solution is partly more supply of committer time
> > on JIRAs. But that is detracting from the work the committers
> > themselves want to do. More of the solution is reducing demand by
> > helping people create useful, actionable, non-duplicate JIRAs from the
> > start. Or encouraging people to resolve existing JIRAs and shepherding
> > those in.
> 
> saying no/not-yet is a vitally important piece of information.

+1, when I propose a contribution to a project, I consider an articulate (and hopefully polite) "no thanks", or "not-yet", or "needs-work", to be far more useful and pleasing than just radio silence.  It allows me to either address feedback, or just move on.

Although it takes effort to keep abreast of community contributions, I don't think it needs to be an overbearing or heavy-weight process.  I've seen other communities where they talked themselves out of better management because they conceived the ticket workflow as being more effort than it needed to be.  Much better to keep ticket triage and workflow fast/simple, but actually do it.



> 
> 
> > Elsewhere, I've found people reluctant to close JIRA for fear of
> > offending or turning off contributors. I think the opposite is true.
> > There is nothing wrong with "no" or "not now" especially accompanied
> > with constructive feedback. Better to state for the record what is not
> > being looked at and why, than let people work on and open the same
> > JIRAs repeatedly.
> 
> well stated!
> 
> 
> > I have also found in the past that a culture of tolerating eternal
> > JIRAs led people to file JIRAs in order to forget about a problem --
> > it's in JIRA, and it's "in progress", so it feels like someone else is
> > going to fix it later and so it can be forgotten now.
> 
> there's some value in these now-i-can-forget jira, though i'm not
> personally a fan. it can be good to keep them around and reachable by
> search, but they should be clearly marked as no/not-yet or something
> similar.
> 
> 
> > For what it's worth, I think these project and culture mechanics are
> > so important and it's my #1 concern for Spark at this stage. This
> > challenge exists so much more here, exactly because there is so much
> > potential. I'd love to help by trying to identify and close stale
> > JIRAs but am afraid that tagging them is just adding to the heap of
> > work.
> 
> +1 concern and potential!
> 
> 
> best,
> 
> 
> matt
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 
> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 14:40:35 2014
Return-Path: <dev-return-9068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB4731151D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 14:40:35 +0000 (UTC)
Received: (qmail 65679 invoked by uid 500); 26 Aug 2014 14:40:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65610 invoked by uid 500); 26 Aug 2014 14:40:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65594 invoked by uid 99); 26 Aug 2014 14:40:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:40:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:40:28 +0000
Received: by mail-ig0-f180.google.com with SMTP id l13so4573843iga.7
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 07:40:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=S8jbSs+0NfrB9pgkEsn60oDEyfAcFDeUob4jf6B+zSY=;
        b=c9RJsauNM20S3SivLnr0aBLjT6vnUcMSw6BTkVzZtq9RMvDy2SXXaTd6v+6JvfGi2t
         ntzic5NjCdo3TarLla55e3nkXFgDPZthNkYmn50Y2iSbDIPXpG0NLlveYHtcRoodp4A9
         XAawzT+4Ky8stU4xI4oc5rkwt0nYixDS8Hdhg=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=S8jbSs+0NfrB9pgkEsn60oDEyfAcFDeUob4jf6B+zSY=;
        b=AHIdrWr+vRkSIZH9O1RwUZSoMMcWPdo0bVA/REu2GUkhe0NCmu9aAbLH11J3HReACE
         5hTzvJXOJf6FWEfRPe0qYRxBNbjTlem0DkJNOXlohW2cCTM+dBGDKGGsvR//USLO7/Cc
         nD+WcC2DDFu9QW0N58yVfD7eJ5K5qxvHSrwwetgFSg9mIojGdRUYMtIp+8u3nK1uAM3F
         umlvjo6ABi0fguJy0Mtj11iG9Y1hzbLFXT94324ICAI2xnGFphUkKkoZo/XMwvtccdRp
         cbtWs9WyIYJeBDf5rbPQcI9IErieMc/NGN+OIVH1wt7deCXJRaa96V5YOPOo/nwd1s6n
         7AmQ==
X-Gm-Message-State: ALoCoQlcuclhhbxIxyfB6yNaNBGXSvueylEh/ePsio3AMwwzZ2jpFDL1qLAjhzlfk6+jgbTasxrQ
MIME-Version: 1.0
X-Received: by 10.50.143.101 with SMTP id sd5mr23183452igb.18.1409064005952;
 Tue, 26 Aug 2014 07:40:05 -0700 (PDT)
Received: by 10.64.13.83 with HTTP; Tue, 26 Aug 2014 07:40:05 -0700 (PDT)
X-Originating-IP: [76.21.63.199]
Received: by 10.64.13.83 with HTTP; Tue, 26 Aug 2014 07:40:05 -0700 (PDT)
In-Reply-To: <CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
References: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
	<CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
Date: Tue, 26 Aug 2014 07:40:05 -0700
Message-ID: <CAGh_TuPDEEck-N-LiCvw0GLYi9S+-rqRieUxt_HLN1KhGwBFdQ@mail.gmail.com>
Subject: Re: CoHadoop Papers
From: Christopher Nguyen <ctn@adatao.com>
To: Gary Malouf <malouf.gary@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134bad08b025005018947e2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134bad08b025005018947e2
Content-Type: text/plain; charset=UTF-8

Gary, do you mean Spark and HDFS separately, or Spark's use of HDFS?

If the former, Spark does support copartitioning.

If the latter, it's an HDFS scope that's outside of Spark. On that note,
Hadoop does also make attempts to collocate data, e.g., rack awareness. I'm
sure the paper makes useful contributions for its set of use cases.

Sent while mobile. Pls excuse typos etc.
On Aug 26, 2014 5:21 AM, "Gary Malouf" <malouf.gary@gmail.com> wrote:

> It appears support for this type of control over block placement is going
> out in the next version of HDFS:
> https://issues.apache.org/jira/browse/HDFS-2576
>
>
> On Tue, Aug 26, 2014 at 7:43 AM, Gary Malouf <malouf.gary@gmail.com>
> wrote:
>
> > One of my colleagues has been questioning me as to why Spark/HDFS makes
> no
> > attempts to try to co-locate related data blocks.  He pointed to this
> > paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on the
> > CoHadoop research and the performance improvements it yielded for
> > Map/Reduce jobs.
> >
> > Would leveraging these ideas for writing data from Spark make sense/be
> > worthwhile?
> >
> >
> >
>

--001a1134bad08b025005018947e2--

From dev-return-9069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 14:53:39 2014
Return-Path: <dev-return-9069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C92F1159C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 14:53:39 +0000 (UTC)
Received: (qmail 95874 invoked by uid 500); 26 Aug 2014 14:53:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95808 invoked by uid 500); 26 Aug 2014 14:53:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95789 invoked by uid 99); 26 Aug 2014 14:53:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:53:38 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,LOTS_OF_MONEY,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:53:34 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XMI7V-0002d5-UL
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 07:53:13 -0700
Date: Tue, 26 Aug 2014 07:53:13 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409064793909-8034.post@n3.nabble.com>
Subject: HiveContext, schemaRDD.printSchema get different dataTypes, feature
 or a bug? really strange and surprised...
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_75912_4527717.1409064793931"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_75912_4527717.1409064793931
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

is there any dataType auto convert or detect or something in HiveContext ?all
columns of a table is defined as string in hive metastoreone column is
total_price with values like 123.45, then this column will be recognized as
dataType Float in HiveContext...this is a feature or a bug? it really
surprised me... how is it implemented? if it is a feature, can i turn it
off? i want to get a schemaRDD with exactly the same datatype defined in
hive metadata, i know the column total_price should be float values, but
they must not be, and what happens if there is some broken line in my huge
CSV file? or maybe some total_price is 9,123.45 or $123.45 or
something==============================================================some
example for this in our env.MapR v3 cluster, newest spark github master
clone from yesterdaybuilt withsbt/sbt -Dhadoop.version=1.0.3-mapr-3.0.3
-Phive assemblyhive-site.xml
configured==============================================================spark-shell
scripts:val hiveContext = new
org.apache.spark.sql.hive.HiveContext(sc)hiveContext.sql("use
our_live_db")hiveContext.sql("desc formatted
et_fullorders").collect.foreach(println)......14/08/26 15:47:09 INFO
SparkContext: Job finished: collect at SparkPlan.scala:85, took 0.0305408
s[# col_name             data_type               comment             ][               
][sid                    string                  from deserializer  
][request_id             string                  from deserializer  
][*times_dq               string*                  from deserializer  
][*total_price            string*                  from deserializer  
][order_id               string                  from deserializer   ][               
][# Partition Information                 ][# col_name             data_type              
comment             ][                ][wt_date                string                 
None                ][country                string                  None               
][                ][# Detailed Table Information            ][Database:             
our_live_db            ][Owner:                 client02             
][CreateTime:            Fri Jan 31 12:23:40 CET 2014     ][LastAccessTime:       
UNKNOWN                  ][Protect Mode:          None                    
][Retention:             0                        ][Location:             
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders    
][Table Type:            EXTERNAL_TABLE           ][Table Parameters:              
][       EXTERNAL                TRUE                ][      
transient_lastDdlTime   1391167420          ][                ][# Storage
Information           ][SerDe Library:        
com.bizo.hive.serde.csv.CSVSerde         ][InputFormat:          
org.apache.hadoop.mapred.TextInputFormat         ][OutputFormat:         
org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat      
][Compressed:            No                       ][Num Buckets:          
-1                       ][Bucket Columns:        []                      
][Sort Columns:          []                       ][Storage Desc Params:           
][       separatorChar           ;                   ][      
serialization.format    1                   ]then, create a schemaRDD from
this tableval result = hiveContext.sql("select sid, order_id, total_price,
times_dq from et_fullorders where wt_date='2014-04-14' and country='uk'
limit 5")ok now, printSchema...scala> result.printSchemaroot |-- sid: string
(nullable = true) |-- order_id: string (nullable = true) |-- *total_price:
float* (nullable = true) |-- *times_dq: timestamp* (nullable =
true)total_price was STRING but now in schemaRDD is FLOATandtimes_dq, now is
TIMESTAMPreally strange and surprised...and more strange is:scala>
result.map(row => row.getString(2)).collect.foreach(println)i
got240.0045.8321.6795.83120.83butscala> result.map(row =>
row.getFloat(2)).collect.foreach(println)14/08/26 16:01:24 ERROR Executor:
Exception in task 0.0 in stage 9.0 (TID 8)java.lang.ClassCastException:
java.lang.String cannot be cast to java.lang.Float        at
scala.runtime.BoxesRunTime.unboxToFloat(BoxesRunTime.java:114)==============================================================btw,
files in this external table are gzipped csv files:14/08/26 15:49:56 INFO
HadoopRDD: Input split:
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders/wt_date=2014-04-14/country=uk/getFullOrders_2014-04-14.csv.gz:0+16990and
the data in it:scala>
result.collect.foreach(println)[5000000001402123123,12344000123454,240.00,2014-04-14
00:03:49.082000][5000000001402110123,12344000123455,45.83,2014-04-14
00:04:13.639000][5000000001402129123,12344000123458,21.67,2014-04-14
00:09:12.276000][5000000001402092123,12344000132457,95.83,2014-04-14
00:09:42.228000][5000000001402135123,12344000123460,120.83,2014-04-14
00:12:44.742000]we use CSVSerDe
https://drone.io/github.com/ogrodnek/csv-serde/files/target/csv-serde-1.1.2-0.11.0-all.jarmaybe
this is a reason?but why the 1st and 2nd column, will not be recognized as
bigint or double or something...?Thanks for any idea



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/HiveContext-schemaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8034.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_75912_4527717.1409064793931--

From dev-return-9070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 14:55:02 2014
Return-Path: <dev-return-9070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4861115A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 14:55:02 +0000 (UTC)
Received: (qmail 98735 invoked by uid 500); 26 Aug 2014 14:55:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98669 invoked by uid 500); 26 Aug 2014 14:55:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98658 invoked by uid 99); 26 Aug 2014 14:55:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:55:01 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=LOTS_OF_MONEY,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:54:36 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XMI8p-0002he-C3
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 07:54:35 -0700
Date: Tue, 26 Aug 2014 07:54:35 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409064875366-8035.post@n3.nabble.com>
Subject: HiveContext, schemaRDD.printSchema get different dataTypes, feature
 or a bug? really strange and surprised...
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

is there any dataType auto convert or detect or something in HiveContext ?

all columns of a table is defined as string in hive metastore

one column is total_price with values like 123.45, then this column will be
recognized as dataType Float in HiveContext...

this is a feature or a bug? it really surprised me... how is it implemented?
if it is a feature, can i turn it off? i want to get a schemaRDD with
exactly the same datatype defined in hive metadata, i know the column
total_price should be float values, but they must not be, and what happens
if there is some broken line in my huge CSV file? or maybe some total_price
is 9,123.45 or $123.45 or something

==============================================================

some example for this in our env.

MapR v3 cluster, newest spark github master clone from yesterday

built with
sbt/sbt -Dhadoop.version=1.0.3-mapr-3.0.3 -Phive assembly

hive-site.xml configured

==============================================================

spark-shell scripts:

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("use our_live_db")
hiveContext.sql("desc formatted et_fullorders").collect.foreach(println)
...
...
14/08/26 15:47:09 INFO SparkContext: Job finished: collect at
SparkPlan.scala:85, took 0.0305408 s
[# col_name             data_type               comment             ]
[                ]
[sid                    string                  from deserializer   ]
[request_id             string                  from deserializer   ]
[*times_dq               string*                  from deserializer   ]
[*total_price            string*                  from deserializer   ]
[order_id               string                  from deserializer   ]
[                ]
[# Partition Information                 ]
[# col_name             data_type               comment             ]
[                ]
[wt_date                string                  None                ]
[country                string                  None                ]
[                ]
[# Detailed Table Information            ]
[Database:              our_live_db            ]
[Owner:                 client02              ]
[CreateTime:            Fri Jan 31 12:23:40 CET 2014     ]
[LastAccessTime:        UNKNOWN                  ]
[Protect Mode:          None                     ]
[Retention:             0                        ]
[Location:             
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders     ]
[Table Type:            EXTERNAL_TABLE           ]
[Table Parameters:               ]
[       EXTERNAL                TRUE                ]
[       transient_lastDdlTime   1391167420          ]
[                ]
[# Storage Information           ]
[SerDe Library:         com.bizo.hive.serde.csv.CSVSerde         ]
[InputFormat:           org.apache.hadoop.mapred.TextInputFormat         ]
[OutputFormat:         
org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       ]
[Compressed:            No                       ]
[Num Buckets:           -1                       ]
[Bucket Columns:        []                       ]
[Sort Columns:          []                       ]
[Storage Desc Params:            ]
[       separatorChar           ;                   ]
[       serialization.format    1                   ]

then, create a schemaRDD from this table

val result = hiveContext.sql("select sid, order_id, total_price, times_dq
from et_fullorders where wt_date='2014-04-14' and country='uk' limit 5")

ok now, printSchema...

scala> result.printSchema
root
 |-- sid: string (nullable = true)
 |-- order_id: string (nullable = true)
 |-- *total_price: float* (nullable = true)
 |-- *times_dq: timestamp* (nullable = true)


total_price was STRING but now in schemaRDD is FLOAT
and
times_dq, now is TIMESTAMP

really strange and surprised...

and more strange is:

scala> result.map(row => row.getString(2)).collect.foreach(println)

i got
240.00
45.83
21.67
95.83
120.83

but

scala> result.map(row => row.getFloat(2)).collect.foreach(println)

14/08/26 16:01:24 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 8)
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Float
        at scala.runtime.BoxesRunTime.unboxToFloat(BoxesRunTime.java:114)

==============================================================

btw, files in this external table are gzipped csv files:
14/08/26 15:49:56 INFO HadoopRDD: Input split:
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders/wt_date=2014-04-14/country=uk/getFullOrders_2014-04-14.csv.gz:0+16990

and the data in it:

scala> result.collect.foreach(println)
[5000000001402123123,12344000123454,240.00,2014-04-14 00:03:49.082000]
[5000000001402110123,12344000123455,45.83,2014-04-14 00:04:13.639000]
[5000000001402129123,12344000123458,21.67,2014-04-14 00:09:12.276000]
[5000000001402092123,12344000132457,95.83,2014-04-14 00:09:42.228000]
[5000000001402135123,12344000123460,120.83,2014-04-14 00:12:44.742000]

we use CSVSerDe
https://drone.io/github.com/ogrodnek/csv-serde/files/target/csv-serde-1.1.2-0.11.0-all.jar

maybe this is a reason?

but why the 1st and 2nd column, will not be recognized as bigint or double
or something...?

Thanks for any idea




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/HiveContext-schemaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8035.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 14:59:20 2014
Return-Path: <dev-return-9071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75D0D115C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 14:59:20 +0000 (UTC)
Received: (qmail 8363 invoked by uid 500); 26 Aug 2014 14:59:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8299 invoked by uid 500); 26 Aug 2014 14:59:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8287 invoked by uid 99); 26 Aug 2014 14:59:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:59:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 14:58:52 +0000
Received: by mail-we0-f177.google.com with SMTP id w62so14816589wes.22
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 07:58:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mJkEILRmbn6FqEzQps3roSmqkh9fAfVWqZ3VIrAZtoU=;
        b=hSCrCn67TytsKhz1KmCgyVi5Wwhu5AuC6psI3xg/5B12AvnKrXLztndIFRFnQDh+7L
         9q0lQQ+fJ41AMoOcMgNo0uK0yZkH3Z0c5/nWHATI1+EwgIIThxEFya2w1KCDxJI8ZwHC
         ppIVMG7D7HBajrgY3QdZpvz4/bHbgCrfy+5yJ28NU7nMK0bKoDBIlq0Fe0247uM48m2l
         HWHe5xm9/1vNbrorHAa/v6LsXZTxLKOXLQyBAeBORY+FIaINdGE73xCZvxkprEqH3Iq/
         paR/PkvF6pF4z/Ghu+vJAGZnIzL2gPPd+ViU+19SvMzxpbXpUEzQeRid0fueoh3tDw1s
         bgvw==
MIME-Version: 1.0
X-Received: by 10.180.21.101 with SMTP id u5mr22079487wie.68.1409065131849;
 Tue, 26 Aug 2014 07:58:51 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Tue, 26 Aug 2014 07:58:51 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
Date: Tue, 26 Aug 2014 10:58:51 -0400
Message-ID: <CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
Subject: Re: Gradient descent and runMiniBatchSGD
From: RJ Nowling <rnowling@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb70ca2a6405d0501898aa5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb70ca2a6405d0501898aa5
Content-Type: text/plain; charset=UTF-8

Hi Alexander,

Can you post a link to the code?

RJ


On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

> Hi,
>
> I've implemented back propagation algorithm using Gradient class and a
> simple update using Updater class. Then I run the algorithm with mllib's
> GradientDescent class. I have troubles in scaling out this implementation.
> I thought that if I partition my data into the number of workers then
> performance will increase, because each worker will run a step of gradient
> descent on its partition of data. But this does not happen and each worker
> seems to process all data (if miniBatchFraction == 1.0 as in mllib's
> logisic regression implementation). For me, this doesn't make sense,
> because then only single Worker will provide the same performance. Could
> someone elaborate on this and correct me if I am wrong. How can I scale out
> the algorithm with many Workers?
>
> Best regards, Alexander
>



-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bb70ca2a6405d0501898aa5--

From dev-return-9072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 15:16:39 2014
Return-Path: <dev-return-9072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B10D11765
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 15:16:39 +0000 (UTC)
Received: (qmail 64400 invoked by uid 500); 26 Aug 2014 15:16:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64331 invoked by uid 500); 26 Aug 2014 15:16:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64320 invoked by uid 99); 26 Aug 2014 15:16:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:16:38 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:16:32 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id 960E17B;
	Tue, 26 Aug 2014 15:16:11 +0000 (UTC)
Received: from G4W6304.americas.hpqcorp.net (16.210.26.229) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 26 Aug 2014 15:15:03 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.222]) by
 G4W6304.americas.hpqcorp.net ([16.210.26.229]) with mapi id 14.03.0169.001;
 Tue, 26 Aug 2014 15:15:03 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: RJ Nowling <rnowling@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Gradient descent and runMiniBatchSGD
Thread-Topic: Gradient descent and runMiniBatchSGD
Thread-Index: Ac/BGakk7EDh0bW5SIe9VrxUpkwvcgAJJYWAAACGR/A=
Date: Tue, 26 Aug 2014 15:15:02 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
 <CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
In-Reply-To: <CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF702AG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF702AG4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

SGksIFJKDQoNCmh0dHBzOi8vZ2l0aHViLmNvbS9hdnVsYW5vdi9zcGFyay9ibG9iL25ldXJhbG5l
dHdvcmsvbWxsaWIvc3JjL21haW4vc2NhbGEvb3JnL2FwYWNoZS9zcGFyay9tbGxpYi9jbGFzc2lm
aWNhdGlvbi9OZXVyYWxOZXR3b3JrLnNjYWxhDQoNClVuaXQgdGVzdHMgYXJlIGluIHRoZSBzYW1l
IGJyYW5jaC4NCg0KQWxleGFuZGVyDQoNCkZyb206IFJKIE5vd2xpbmcgW21haWx0bzpybm93bGlu
Z0BnbWFpbC5jb21dDQpTZW50OiBUdWVzZGF5LCBBdWd1c3QgMjYsIDIwMTQgNjo1OSBQTQ0KVG86
IFVsYW5vdiwgQWxleGFuZGVyDQpDYzogZGV2QHNwYXJrLmFwYWNoZS5vcmcNClN1YmplY3Q6IFJl
OiBHcmFkaWVudCBkZXNjZW50IGFuZCBydW5NaW5pQmF0Y2hTR0QNCg0KSGkgQWxleGFuZGVyLA0K
DQpDYW4geW91IHBvc3QgYSBsaW5rIHRvIHRoZSBjb2RlPw0KDQpSSg0KDQpPbiBUdWUsIEF1ZyAy
NiwgMjAxNCBhdCA2OjUzIEFNLCBVbGFub3YsIEFsZXhhbmRlciA8YWxleGFuZGVyLnVsYW5vdkBo
cC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPj4gd3JvdGU6DQpIaSwNCg0KSSd2
ZSBpbXBsZW1lbnRlZCBiYWNrIHByb3BhZ2F0aW9uIGFsZ29yaXRobSB1c2luZyBHcmFkaWVudCBj
bGFzcyBhbmQgYSBzaW1wbGUgdXBkYXRlIHVzaW5nIFVwZGF0ZXIgY2xhc3MuIFRoZW4gSSBydW4g
dGhlIGFsZ29yaXRobSB3aXRoIG1sbGliJ3MgR3JhZGllbnREZXNjZW50IGNsYXNzLiBJIGhhdmUg
dHJvdWJsZXMgaW4gc2NhbGluZyBvdXQgdGhpcyBpbXBsZW1lbnRhdGlvbi4gSSB0aG91Z2h0IHRo
YXQgaWYgSSBwYXJ0aXRpb24gbXkgZGF0YSBpbnRvIHRoZSBudW1iZXIgb2Ygd29ya2VycyB0aGVu
IHBlcmZvcm1hbmNlIHdpbGwgaW5jcmVhc2UsIGJlY2F1c2UgZWFjaCB3b3JrZXIgd2lsbCBydW4g
YSBzdGVwIG9mIGdyYWRpZW50IGRlc2NlbnQgb24gaXRzIHBhcnRpdGlvbiBvZiBkYXRhLiBCdXQg
dGhpcyBkb2VzIG5vdCBoYXBwZW4gYW5kIGVhY2ggd29ya2VyIHNlZW1zIHRvIHByb2Nlc3MgYWxs
IGRhdGEgKGlmIG1pbmlCYXRjaEZyYWN0aW9uID09IDEuMCBhcyBpbiBtbGxpYidzIGxvZ2lzaWMg
cmVncmVzc2lvbiBpbXBsZW1lbnRhdGlvbikuIEZvciBtZSwgdGhpcyBkb2Vzbid0IG1ha2Ugc2Vu
c2UsIGJlY2F1c2UgdGhlbiBvbmx5IHNpbmdsZSBXb3JrZXIgd2lsbCBwcm92aWRlIHRoZSBzYW1l
IHBlcmZvcm1hbmNlLiBDb3VsZCBzb21lb25lIGVsYWJvcmF0ZSBvbiB0aGlzIGFuZCBjb3JyZWN0
IG1lIGlmIEkgYW0gd3JvbmcuIEhvdyBjYW4gSSBzY2FsZSBvdXQgdGhlIGFsZ29yaXRobSB3aXRo
IG1hbnkgV29ya2Vycz8NCg0KQmVzdCByZWdhcmRzLCBBbGV4YW5kZXINCg0KDQoNCi0tDQplbSBy
bm93bGluZ0BnbWFpbC5jb208bWFpbHRvOnJub3dsaW5nQGdtYWlsLmNvbT4NCmMgOTU0LjQ5Ni4y
MzE0DQo=

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCF702AG4W3292americas_--

From dev-return-9073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 15:16:46 2014
Return-Path: <dev-return-9073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D78CE11766
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 15:16:46 +0000 (UTC)
Received: (qmail 65586 invoked by uid 500); 26 Aug 2014 15:16:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65521 invoked by uid 500); 26 Aug 2014 15:16:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65508 invoked by uid 99); 26 Aug 2014 15:16:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:16:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:16:40 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so14914845wgg.24
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 08:16:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=CGO4SW0ZcCyWmMqv52tEVy3X3IFtTW4NaP0LVf9c47Y=;
        b=BKpHAdW6Sn+oEcReemI71YJV17YvOM/R4js+kXHKGFxZrGXacWAwqssrWZywl3Uyhy
         ImJeAoenDRKeNqWiIUGUiBCW/qYVBl0rATihEeim/z7kNNI/tDZMm1eI+yxOqoK0th68
         XGSC/5D0AYmtw2oV4fcX+uUzvsekxme/RpIEUdkX/zTH+jzVVFTyA252bJdCJc2VTW47
         9UFsVfTxjVi64FkYSuG/r17+399uACDsdKEtEcOpSdmYRqNu77Hbpsq3yy6oq0D2lDDp
         AvvDDekG3Y7790wTrMv7WxSNDQGg2gTVXPovtqaoEZGklDQECaEMAHvLCpsRzrkIo9m8
         nIxg==
X-Received: by 10.180.210.172 with SMTP id mv12mr22670853wic.45.1409066179308;
 Tue, 26 Aug 2014 08:16:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 26 Aug 2014 08:15:39 -0700 (PDT)
In-Reply-To: <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 26 Aug 2014 11:15:39 -0400
Message-ID: <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Patrick Wendell <pwendell@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37ede15356e050189c906
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37ede15356e050189c906
Content-Type: text/plain; charset=UTF-8

On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'd prefer if we took the approach of politely explaining why in the
> current form the patch isn't acceptable and closing it (potentially w/ tips
> on how to improve it or narrow the scope).


Amen to this. Aiming for such a culture would set Spark apart from other
projects in a great way.

I've proposed several different solutions to ASF infra to streamline the
> process, but thus far they haven't been open to any of my ideas:


I've added myself as a watcher on those 2 INFRA issues. Sucks that the only
solution on offer right now requires basically polluting the commit history.

Short of moving Spark's repo to a non-ASF-managed GitHub account, do you
think another bot could help us manage the number of stale PRs?

I'm thinking a solution as follows might be very helpful:

   - Extend Spark QA / Jenkins to run on a weekly schedule and check for
   stale PRs. Let's say a stale PR is an open one that hasn't been updated in
   N months.
   - Spark QA maintains a list of known committers on its side.
   - During its weekly check of stale PRs, Spark QA takes the following
   action:
      - If the last person to comment on a PR was a committer, post to the
      PR asking for an update from the contributor.
      - If the last person to comment on a PR was a contributor, add the PR
      to a list. Email this list of *hanging PRs* out to the dev list on a
      weekly basis and ask committers to update them.
      - If the last person to comment on a PR was Spark QA asking the
      contributor to update it, then add the PR to a list. Email this
list of *abandoned
      PRs* to the dev list for the record (or for closing, if that becomes
      possible in the future).

This doesn't solve the problem of not being able to close PRs, but it does
help make sure no PR is left hanging for long.

What do you think? I'd be interested in implementing this solution if we
like it.

Nick

--001a11c37ede15356e050189c906--

From dev-return-9074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 15:30:10 2014
Return-Path: <dev-return-9074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0A4211828
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 15:30:10 +0000 (UTC)
Received: (qmail 6239 invoked by uid 500); 26 Aug 2014 15:30:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6174 invoked by uid 500); 26 Aug 2014 15:30:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6163 invoked by uid 99); 26 Aug 2014 15:30:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:30:09 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:30:05 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XMIgq-0004fl-Rt
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 08:29:44 -0700
Date: Tue, 26 Aug 2014 08:29:44 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409066984856-8039.post@n3.nabble.com>
In-Reply-To: <1409064875366-8035.post@n3.nabble.com>
References: <1409064875366-8035.post@n3.nabble.com>
Subject: Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

oops, i tried on a managed table, column types will not be changed

so it is mostly due to the serde lib CSVSerDe
(https://github.com/ogrodnek/csv-serde/blob/master/src/main/java/com/bizo/hive/serde/csv/CSVSerde.java#L123)
or maybe CSVReader from opencsv?...

but if the columns are defined as string, no matter what type returned from
custom SerDe or CSVReader, they should be cast to string at the end right?

why do not use the schema from hive metadata directly?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/HiveContext-schemaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8035p8039.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 15:37:55 2014
Return-Path: <dev-return-9075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B2ED11877
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 15:37:55 +0000 (UTC)
Received: (qmail 33405 invoked by uid 500); 26 Aug 2014 15:37:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33337 invoked by uid 500); 26 Aug 2014 15:37:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33325 invoked by uid 99); 26 Aug 2014 15:37:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:37:54 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 15:37:27 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so13702788qaq.38
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 08:37:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:cc
         :content-type;
        bh=EoXbG3XlEKCb4ZDcw4992qgHUcVXf1YHs/rC+w5G5Do=;
        b=ePAaWA5X6Mf4xyORLuqD3NAmlKQL88GisnVG8bqH3nu/qGLZ1phGK81rXQwTSspyUU
         Y0rCsIJGmwof3HB9P2ZXgKLuJyPJ1yAv5m+W3AXFsp19W4GFKdZS/jfGaSfd9g38EQiU
         W5c3cIMd5pX1LeW6Kt9/zLlWXAuEQab44URbZ5jMtTOdirTlB+v8ic2M9oiS1s7Xd3cb
         PDdbNwPzM00kc75DwYy5yTOZrLBORxlt871v+qxoaKcE4yTqcs8b018Lzgv54rlAAIwT
         T59xN6lKpQTy3jeBF+t/F+OhaQ+n/ZBSXVBNI4X9RsPquOsg5jUQ32RG5VcCW5qBIOAG
         LeXg==
MIME-Version: 1.0
X-Received: by 10.224.112.1 with SMTP id u1mr46939883qap.7.1409067445988; Tue,
 26 Aug 2014 08:37:25 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Tue, 26 Aug 2014 08:37:25 -0700 (PDT)
In-Reply-To: <CAGh_TuPDEEck-N-LiCvw0GLYi9S+-rqRieUxt_HLN1KhGwBFdQ@mail.gmail.com>
References: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
	<CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
	<CAGh_TuPDEEck-N-LiCvw0GLYi9S+-rqRieUxt_HLN1KhGwBFdQ@mail.gmail.com>
Date: Tue, 26 Aug 2014 11:37:25 -0400
Message-ID: <CAGOvqiqH2FhNW-Hi5cjvLz81Tr3jYoG6BzX79s01=gsJak6bKQ@mail.gmail.com>
Subject: Re: CoHadoop Papers
From: Gary Malouf <malouf.gary@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b673a4e9532eb05018a1444
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b673a4e9532eb05018a1444
Content-Type: text/plain; charset=UTF-8

Christopher, can you expand on the co-partitioning support?

We have a number of spark SQL tables (saved in parquet format) that all
could be considered to have a common hash key.  Our analytics team wants to
do frequent joins across these different data-sets based on this key.  It
makes sense that if the data for each key across 'tables' was co-located on
the same server, shuffles could be minimized and ultimately performance
could be much better.

>From reading the HDFS issue I posted before, the way is being paved for
implementing this type of behavior though there are a lot of complications
to make it work I believe.


On Tue, Aug 26, 2014 at 10:40 AM, Christopher Nguyen <ctn@adatao.com> wrote:

> Gary, do you mean Spark and HDFS separately, or Spark's use of HDFS?
>
> If the former, Spark does support copartitioning.
>
> If the latter, it's an HDFS scope that's outside of Spark. On that note,
> Hadoop does also make attempts to collocate data, e.g., rack awareness. I'm
> sure the paper makes useful contributions for its set of use cases.
>
> Sent while mobile. Pls excuse typos etc.
> On Aug 26, 2014 5:21 AM, "Gary Malouf" <malouf.gary@gmail.com> wrote:
>
>> It appears support for this type of control over block placement is going
>> out in the next version of HDFS:
>> https://issues.apache.org/jira/browse/HDFS-2576
>>
>>
>> On Tue, Aug 26, 2014 at 7:43 AM, Gary Malouf <malouf.gary@gmail.com>
>> wrote:
>>
>> > One of my colleagues has been questioning me as to why Spark/HDFS makes
>> no
>> > attempts to try to co-locate related data blocks.  He pointed to this
>> > paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on
>> the
>> > CoHadoop research and the performance improvements it yielded for
>> > Map/Reduce jobs.
>> >
>> > Would leveraging these ideas for writing data from Spark make sense/be
>> > worthwhile?
>> >
>> >
>> >
>>
>

--047d7b673a4e9532eb05018a1444--

From dev-return-9076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 16:14:12 2014
Return-Path: <dev-return-9076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7ECA5119E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 16:14:12 +0000 (UTC)
Received: (qmail 25908 invoked by uid 500); 26 Aug 2014 16:14:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25822 invoked by uid 500); 26 Aug 2014 16:14:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24844 invoked by uid 99); 26 Aug 2014 16:14:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 16:14:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.51 as permitted sender)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 16:14:04 +0000
Received: by mail-oi0-f51.google.com with SMTP id v63so3906844oia.10
        for <multiple recipients>; Tue, 26 Aug 2014 09:13:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=REuos0TCZeOhDIAsj3A3x6A194AO5cP7GD8sm1hM7Lg=;
        b=KXQdwYEwJTxFKDaWiLeVNw+1E3k++zt7kprEPVsrRL8OHhpZvB3KPbNHJBEndTEUUn
         U5jMWjDl+jOLJTv33YTi88zeEUctof5USY1hfhlb0mluhrGEr7JlagCOj66qXUj4zafc
         oiOpxm5Oq13WNbsEiWrFjlp025Sf0VymLu59+4IKl1/ltEy8NcKGn3+vouftLKpfCEmX
         CpqOY09cAmd8KtkiHtHgyrV0HjJPXEDyCgVo7YI7e7QMwWQrWlCvzib9aD1vbxidkINR
         821s2/A9APPgHSbHaqGS1kbIAItcIYTth2TPDx2Eizbfr3dkCes3PmeMXAxN4/mOrkBw
         cd9A==
MIME-Version: 1.0
X-Received: by 10.182.27.5 with SMTP id p5mr28523527obg.42.1409069623889; Tue,
 26 Aug 2014 09:13:43 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Tue, 26 Aug 2014 09:13:43 -0700 (PDT)
Received: by 10.202.187.86 with HTTP; Tue, 26 Aug 2014 09:13:43 -0700 (PDT)
In-Reply-To: <CABPQxsshE2K7pXogkp-wozv598znHvcNc6twDKkymiLybxno1w@mail.gmail.com>
References: <CABPQxsshE2K7pXogkp-wozv598znHvcNc6twDKkymiLybxno1w@mail.gmail.com>
Date: Tue, 26 Aug 2014 09:13:43 -0700
Message-ID: <CABPQxsv6Y5=TL0LUXR_kkbrY7kzxjU5SKsu_C_1cYr4M=h2mow@mail.gmail.com>
Subject: Submit to the "Powered By Spark" Page!
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0115f6cc65565205018a964d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f6cc65565205018a964d
Content-Type: text/plain; charset=ISO-8859-1

Hi All,

I want to invite users to submit to the Spark "Powered By" page. This page
is a great way for people to learn about Spark use cases. Since Spark
activity has increased a lot in the higher level libraries and people often
ask who uses each one, we'll include information about which components
each organization uses as well. If you are interested, simply respond to
this e-mail (or e-mail me off-list) with:

1) Organization name
2) URL
3) Which Spark components you use: Core, SQL, Streaming, MLlib, GraphX
4) A 1-2 sentence description of your use case.

I'll post any new entries here:
https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark

- Patrick

--089e0115f6cc65565205018a964d--

From dev-return-9077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 16:55:00 2014
Return-Path: <dev-return-9077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C923211BE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 16:55:00 +0000 (UTC)
Received: (qmail 52202 invoked by uid 500); 26 Aug 2014 16:55:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52143 invoked by uid 500); 26 Aug 2014 16:55:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52131 invoked by uid 99); 26 Aug 2014 16:54:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 16:54:59 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 16:54:32 +0000
Received: by mail-ig0-f169.google.com with SMTP id r2so6011856igi.4
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 09:54:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=IkGNh3d/XGzagp+L7efjJtVuQywvgYeQpxK5CKGcyck=;
        b=QjzG+tm2zTQ7ItSAZQgFSQqmLGBm1YA51by3ZTtfqOZRJYelmSJ7MPMPzTmiwpTP54
         EvoAqds5CC9k7jt5F4MI0/fcOXrnYId0lxGk1I6XyyHmubSgHYFE/ZT7WQ8Ya/WWEG/R
         /aqCZnflfKx2PUMniGIcow3n4otCXNjGhEMulri4ru1klPbHxOK0oKR0oFV/wOMPa3rY
         3dufIeKlhpsO7bbkC+A9AmwbD+mXES4uftJAqFInbUoX7qFusl+cjXgegU8T6xq77g7Y
         O5iLVHDp8jj5EvfMA2VMtWUPke2XkEWzOsRPsSewvguyfDwFL5yxLmUktKm6ZZBTO2ds
         zqHw==
MIME-Version: 1.0
X-Received: by 10.50.56.38 with SMTP id x6mr23570896igp.30.1409072071498; Tue,
 26 Aug 2014 09:54:31 -0700 (PDT)
Received: by 10.107.152.196 with HTTP; Tue, 26 Aug 2014 09:54:31 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
	<CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>
Date: Tue, 26 Aug 2014 09:54:31 -0700
Message-ID: <CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
Subject: Re: Gradient descent and runMiniBatchSGD
From: Xiangrui Meng <mengxr@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: RJ Nowling <rnowling@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

miniBatchFraction uses RDD.sample to get the mini-batch, and sample
still needs to visit the elements one after another. So it is not
efficient if the task is not computation heavy and this is why
setMiniBatchFraction is marked as experimental. If we can detect that
the partition iterator is backed by an ArrayBuffer, maybe we can do a
skip iterator to skip elements. -Xiangrui

On Tue, Aug 26, 2014 at 8:15 AM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Hi, RJ
>
> https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala=
/org/apache/spark/mllib/classification/NeuralNetwork.scala
>
> Unit tests are in the same branch.
>
> Alexander
>
> From: RJ Nowling [mailto:rnowling@gmail.com]
> Sent: Tuesday, August 26, 2014 6:59 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org
> Subject: Re: Gradient descent and runMiniBatchSGD
>
> Hi Alexander,
>
> Can you post a link to the code?
>
> RJ
>
> On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <alexander.ulanov@hp.c=
om<mailto:alexander.ulanov@hp.com>> wrote:
> Hi,
>
> I've implemented back propagation algorithm using Gradient class and a si=
mple update using Updater class. Then I run the algorithm with mllib's Grad=
ientDescent class. I have troubles in scaling out this implementation. I th=
ought that if I partition my data into the number of workers then performan=
ce will increase, because each worker will run a step of gradient descent o=
n its partition of data. But this does not happen and each worker seems to =
process all data (if miniBatchFraction =3D=3D 1.0 as in mllib's logisic reg=
ression implementation). For me, this doesn't make sense, because then only=
 single Worker will provide the same performance. Could someone elaborate o=
n this and correct me if I am wrong. How can I scale out the algorithm with=
 many Workers?
>
> Best regards, Alexander
>
>
>
> --
> em rnowling@gmail.com<mailto:rnowling@gmail.com>
> c 954.496.2314

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 18:22:03 2014
Return-Path: <dev-return-9078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 369D611154
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 18:22:03 +0000 (UTC)
Received: (qmail 9014 invoked by uid 500); 26 Aug 2014 18:22:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8928 invoked by uid 500); 26 Aug 2014 18:22:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8301 invoked by uid 99); 26 Aug 2014 18:22:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:22:01 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:21:35 +0000
Received: by mail-pd0-f179.google.com with SMTP id v10so23033336pde.24
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 11:21:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=OaNvANK69ZOF6fEk+5ngd5oHJuLzxYE+q4xI0G2OgGs=;
        b=JSOUzwHlR04urZ5UDcpCfpmCjHrFdIgps6Gt0GWsV9KrHfzcf+YUCUAmD9Iysg5jZt
         XvYRZ8Kf7DWs5vRKI+3QKg7t/7kOBg60UlnHeBtxwImCbIkFQtvPBFhR92AnUkbzlOeY
         gKY1zMDGFX8RBIG1xluiMaz3IG/OJqfROkKleFy0IUGPb/hkst43/wGdalCOlPxWnV4b
         ijuOidWu2tEeX6h7udhlerOjhMqrA/Hm3tBNaq7sSUpP/FRtMJ20/K+fIZsg2aEoWgu9
         28f8r+dRWM2YFRiG32FSe/7Tci+0eMOTcZ/JkIMaXei6QCWp/v9iTlyGYhNyFDNGrcHw
         M08A==
X-Received: by 10.70.103.42 with SMTP id ft10mr11021760pdb.2.1409077293313;
        Tue, 26 Aug 2014 11:21:33 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id o4sm5855526pdh.56.2014.08.26.11.21.29
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Tue, 26 Aug 2014 11:21:30 -0700 (PDT)
Date: Tue, 26 Aug 2014 11:21:29 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>, Patrick Wendell
 <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>
Message-ID: <etPan.53fcd029.74b0dc51.104@joshs-mbp>
In-Reply-To: <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
 <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
Subject: Re: Handling stale PRs
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fcd029_19495cff_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fcd029_19495cff_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Last weekend, I started hacking on a Google App Engine app for helping wi=
th pull request review (screenshot:=C2=A0http://i.imgur.com/wwpZKYZ.png).=
 =C2=A0Some of my basic goals (not all implemented yet):

- Users sign in using GitHub and can browse a list of pull requests, incl=
uding links to associated JIRAs, Jenkins statuses, a quick preview of the=
 last comment, etc.

- Pull requests are auto-classified based on which components they modify=
 (by looking at the diff).

- =46rom the app=E2=80=99s own internal database of PRs, we can build das=
hboards to find =E2=80=9Cabandoned=E2=80=9D PRs, graph average time to fi=
rst review, etc.

- Since we authenticate users with GitHub, we can enable administrative f=
unctions via this dashboard (e.g. =E2=80=9Cassign this PR to me=E2=80=9D,=
 =E2=80=9Cvote to close in the weekly auto-close commit=E2=80=9D, etc.

Right now, I=E2=80=99ve implemented GItHub OAuth support and code to upda=
te the issues database using the GitHub API. =C2=A0Because we have access=
 to the full API, it=E2=80=99s pretty easy to do fancy things like parsin=
g the reason for Jenkins failure, etc. =C2=A0You could even imagine some =
fancy mashup tools to pull up JIRAs and pull requests side-by in iframes.=


After I hack on this a bit more, I plan to release a public preview versi=
on; if we find this tool useful, I=E2=80=99ll clean it up and open-source=
 the app so folks can contribute to it.

- Josh

On August 26, 2014 at 8:16:46 AM, Nicholas Chammas (nicholas.chammas=40gm=
ail.com) wrote:

On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell=40gmail.com> w=
rote: =20

> I'd prefer if we took the approach of politely explaining why in the =20
> current form the patch isn't acceptable and closing it (potentially w/ =
tips =20
> on how to improve it or narrow the scope). =20


Amen to this. Aiming for such a culture would set Spark apart from other =
=20
projects in a great way. =20

I've proposed several different solutions to AS=46 infra to streamline th=
e =20
> process, but thus far they haven't been open to any of my ideas: =20


I've added myself as a watcher on those 2 IN=46RA issues. Sucks that the =
only =20
solution on offer right now requires basically polluting the commit histo=
ry. =20

Short of moving Spark's repo to a non-AS=46-managed GitHub account, do yo=
u =20
think another bot could help us manage the number of stale PRs=3F =20

I'm thinking a solution as follows might be very helpful: =20

- Extend Spark QA / Jenkins to run on a weekly schedule and check for =20
stale PRs. Let's say a stale PR is an open one that hasn't been updated i=
n =20
N months. =20
- Spark QA maintains a list of known committers on its side. =20
- During its weekly check of stale PRs, Spark QA takes the following =20
action: =20
- If the last person to comment on a PR was a committer, post to the =20
PR asking for an update from the contributor. =20
- If the last person to comment on a PR was a contributor, add the PR =20
to a list. Email this list of *hanging PRs* out to the dev list on a =20
weekly basis and ask committers to update them. =20
- If the last person to comment on a PR was Spark QA asking the =20
contributor to update it, then add the PR to a list. Email this =20
list of *abandoned =20
PRs* to the dev list for the record (or for closing, if that becomes =20
possible in the future). =20

This doesn't solve the problem of not being able to close PRs, but it doe=
s =20
help make sure no PR is left hanging for long. =20

What do you think=3F I'd be interested in implementing this solution if w=
e =20
like it. =20

Nick =20

--53fcd029_19495cff_104--


From dev-return-9079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 18:38:36 2014
Return-Path: <dev-return-9079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BCF9C1124E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 18:38:36 +0000 (UTC)
Received: (qmail 60154 invoked by uid 500); 26 Aug 2014 18:38:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60086 invoked by uid 500); 26 Aug 2014 18:38:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60069 invoked by uid 99); 26 Aug 2014 18:38:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:38:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:38:08 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so4683090wiv.5
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 11:38:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=e4ieFaSwz9vpC4DwXrt4rDjinUNHCiFSSJ+7ippoXjA=;
        b=XFDLWbP6zVip8+zJmkSr7+af1rAe9CXsRvt8nB5iG8ZSrGCXoxK90A+mbMNQ5IZZa3
         gSa7cl+HTgy7ZPisdsys46UstHT+Q8I1aJjJIkvjfwoQUKdEdly0KqoP+LKaZsur1j6r
         o+QsmKC8d5PJmUIp+9oinEcQcdN7RqKRfgU/l/hsH+2I3Eb1AgKhM5M3byqMS67YPrfc
         14So46vIHLpAbRSIIToJztrf6NRULLADDltrIQ09NMQWhVZhztcpmFcPGCE3onR5jI8W
         VM2nJQD4jB97dAGNymy26dkLtFdxde31hEkvUVQhBcLamuTjArsehLowY0I8PXfpQLnG
         0Wmg==
X-Received: by 10.194.90.4 with SMTP id bs4mr32062555wjb.71.1409078288110;
 Tue, 26 Aug 2014 11:38:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 26 Aug 2014 11:37:28 -0700 (PDT)
In-Reply-To: <etPan.53fcd029.74b0dc51.104@joshs-mbp>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com> <etPan.53fcd029.74b0dc51.104@joshs-mbp>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 26 Aug 2014 14:37:28 -0400
Message-ID: <CAOhmDzcwt7J9dhqea_sP5wuug_2nXCJfsDnyXpE=1s8byLD24g@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Josh Rosen <rosenville@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>, 
	Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary=047d7bfcfc18d2dc9f05018c9af1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc18d2dc9f05018c9af1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

OK, that sounds pretty cool.

Josh,

Do you see this app as encompassing or supplanting the functionality I
described as well?

Nick


On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote:

> Last weekend, I started hacking on a Google App Engine app for helping
> with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
>  Some of my basic goals (not all implemented yet):
>
> - Users sign in using GitHub and can browse a list of pull requests,
> including links to associated JIRAs, Jenkins statuses, a quick preview of
> the last comment, etc.
>
> - Pull requests are auto-classified based on which components they modify
> (by looking at the diff).
>
> - From the app=E2=80=99s own internal database of PRs, we can build dashb=
oards to
> find =E2=80=9Cabandoned=E2=80=9D PRs, graph average time to first review,=
 etc.
>
> - Since we authenticate users with GitHub, we can enable administrative
> functions via this dashboard (e.g. =E2=80=9Cassign this PR to me=E2=80=9D=
, =E2=80=9Cvote to close
> in the weekly auto-close commit=E2=80=9D, etc.
>
> Right now, I=E2=80=99ve implemented GItHub OAuth support and code to upda=
te the
> issues database using the GitHub API.  Because we have access to the full
> API, it=E2=80=99s pretty easy to do fancy things like parsing the reason =
for
> Jenkins failure, etc.  You could even imagine some fancy mashup tools to
> pull up JIRAs and pull requests side-by in iframes.
>
> After I hack on this a bit more, I plan to release a public preview
> version; if we find this tool useful, I=E2=80=99ll clean it up and open-s=
ource the
> app so folks can contribute to it.
>
> - Josh
>
> On August 26, 2014 at 8:16:46 AM, Nicholas Chammas (
> nicholas.chammas@gmail.com) wrote:
>
> On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > I'd prefer if we took the approach of politely explaining why in the
> > current form the patch isn't acceptable and closing it (potentially w/
> tips
> > on how to improve it or narrow the scope).
>
>
> Amen to this. Aiming for such a culture would set Spark apart from other
> projects in a great way.
>
> I've proposed several different solutions to ASF infra to streamline the
> > process, but thus far they haven't been open to any of my ideas:
>
>
> I've added myself as a watcher on those 2 INFRA issues. Sucks that the
> only
> solution on offer right now requires basically polluting the commit
> history.
>
> Short of moving Spark's repo to a non-ASF-managed GitHub account, do you
> think another bot could help us manage the number of stale PRs?
>
> I'm thinking a solution as follows might be very helpful:
>
> - Extend Spark QA / Jenkins to run on a weekly schedule and check for
> stale PRs. Let's say a stale PR is an open one that hasn't been updated i=
n
> N months.
> - Spark QA maintains a list of known committers on its side.
> - During its weekly check of stale PRs, Spark QA takes the following
> action:
> - If the last person to comment on a PR was a committer, post to the
> PR asking for an update from the contributor.
> - If the last person to comment on a PR was a contributor, add the PR
> to a list. Email this list of *hanging PRs* out to the dev list on a
> weekly basis and ask committers to update them.
> - If the last person to comment on a PR was Spark QA asking the
> contributor to update it, then add the PR to a list. Email this
> list of *abandoned
> PRs* to the dev list for the record (or for closing, if that becomes
> possible in the future).
>
> This doesn't solve the problem of not being able to close PRs, but it doe=
s
> help make sure no PR is left hanging for long.
>
> What do you think? I'd be interested in implementing this solution if we
> like it.
>
> Nick
>
>

--047d7bfcfc18d2dc9f05018c9af1--

From dev-return-9080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 18:41:00 2014
Return-Path: <dev-return-9080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8198911264
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 18:41:00 +0000 (UTC)
Received: (qmail 67264 invoked by uid 500); 26 Aug 2014 18:40:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67194 invoked by uid 500); 26 Aug 2014 18:40:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67181 invoked by uid 99); 26 Aug 2014 18:40:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:40:59 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:40:30 +0000
Received: by mail-pa0-f54.google.com with SMTP id fa1so24315877pad.27
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 11:40:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=pq84NPIckH6s1TKcTNjm3cvcjSIoy3oAG2Af+rRILjw=;
        b=NT8l9Q47WCmZqE1QxMpvDsk2zxx6VHsre/ynp9355MOWT+FYH45WX00MR2fA0mXSVr
         B32HxZZyJCf4m9yEEdolSVlm1TpWQOIUAUQ9DWc3R8FnuDVmd7zci6P/hV4bO/PymTFP
         LqsAJBV07kUxYS/KkkfBYzQofbrc2af0LSDiJXaGdKwQYnd0GbsoaobdL8q3H/OMwPoR
         3FsZ1v0NTE00HNf7rsfJ28JUBuS2r7hDyMNZniVC1QnvR9DKI6pdbs787rZbEPAggAfm
         CBzMeqNoViaisBpDjGpTKWaQrJoR12lvIykaS98HzjVtbsPTl97Um6zou+Gl00/E54V4
         ARXg==
X-Received: by 10.68.95.196 with SMTP id dm4mr39442278pbb.95.1409078428942;
        Tue, 26 Aug 2014 11:40:28 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id du5sm3919563pbb.80.2014.08.26.11.40.28
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Tue, 26 Aug 2014 11:40:28 -0700 (PDT)
Date: Tue, 26 Aug 2014 11:40:27 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>, 
 Patrick Wendell <pwendell@gmail.com>
Message-ID: <etPan.53fcd49b.2ae8944a.104@joshs-mbp>
In-Reply-To: <CAOhmDzcwt7J9dhqea_sP5wuug_2nXCJfsDnyXpE=1s8byLD24g@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
 <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp>
 <CAOhmDzcwt7J9dhqea_sP5wuug_2nXCJfsDnyXpE=1s8byLD24g@mail.gmail.com>
Subject: Re: Handling stale PRs
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fcd49b_625558ec_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fcd49b_625558ec_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Sure; App Engine supports cron and sending emails. =C2=A0We can configure=
 the app with Spark QA=E2=80=99s credentials in order to allow it to post=
 comments on issues, etc.

- Josh

On August 26, 2014 at 11:38:08 AM, Nicholas Chammas (nicholas.chammas=40g=
mail.com) wrote:

OK, that sounds pretty cool.

Josh,

Do you see this app as encompassing or supplanting the functionality I de=
scribed as well=3F

Nick


On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville=40gmail.com> wrot=
e:
Last weekend, I started hacking on a Google App Engine app for helping wi=
th pull request review (screenshot:=C2=A0http://i.imgur.com/wwpZKYZ.png).=
 =C2=A0Some of my basic goals (not all implemented yet):

- Users sign in using GitHub and can browse a list of pull requests, incl=
uding links to associated JIRAs, Jenkins statuses, a quick preview of the=
 last comment, etc.

- Pull requests are auto-classified based on which components they modify=
 (by looking at the diff).

- =46rom the app=E2=80=99s own internal database of PRs, we can build das=
hboards to find =E2=80=9Cabandoned=E2=80=9D PRs, graph average time to fi=
rst review, etc.

- Since we authenticate users with GitHub, we can enable administrative f=
unctions via this dashboard (e.g. =E2=80=9Cassign this PR to me=E2=80=9D,=
 =E2=80=9Cvote to close in the weekly auto-close commit=E2=80=9D, etc.

Right now, I=E2=80=99ve implemented GItHub OAuth support and code to upda=
te the issues database using the GitHub API. =C2=A0Because we have access=
 to the full API, it=E2=80=99s pretty easy to do fancy things like parsin=
g the reason for Jenkins failure, etc. =C2=A0You could even imagine some =
fancy mashup tools to pull up JIRAs and pull requests side-by in iframes.=


After I hack on this a bit more, I plan to release a public preview versi=
on; if we find this tool useful, I=E2=80=99ll clean it up and open-source=
 the app so folks can contribute to it.

- Josh

On August 26, 2014 at 8:16:46 AM, Nicholas Chammas (nicholas.chammas=40gm=
ail.com) wrote:

On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell=40gmail.com> w=
rote:

> I'd prefer if we took the approach of politely explaining why in the
> current form the patch isn't acceptable and closing it (potentially w/ =
tips
> on how to improve it or narrow the scope).


Amen to this. Aiming for such a culture would set Spark apart from other
projects in a great way.

I've proposed several different solutions to AS=46 infra to streamline th=
e
> process, but thus far they haven't been open to any of my ideas:


I've added myself as a watcher on those 2 IN=46RA issues. Sucks that the =
only
solution on offer right now requires basically polluting the commit histo=
ry.

Short of moving Spark's repo to a non-AS=46-managed GitHub account, do yo=
u
think another bot could help us manage the number of stale PRs=3F

I'm thinking a solution as follows might be very helpful:

- Extend Spark QA / Jenkins to run on a weekly schedule and check for
stale PRs. Let's say a stale PR is an open one that hasn't been updated i=
n
N months.
- Spark QA maintains a list of known committers on its side.
- During its weekly check of stale PRs, Spark QA takes the following
action:
- If the last person to comment on a PR was a committer, post to the
PR asking for an update from the contributor.
- If the last person to comment on a PR was a contributor, add the PR
to a list. Email this list of *hanging PRs* out to the dev list on a
weekly basis and ask committers to update them.
- If the last person to comment on a PR was Spark QA asking the
contributor to update it, then add the PR to a list. Email this
list of *abandoned
PRs* to the dev list for the record (or for closing, if that becomes
possible in the future).

This doesn't solve the problem of not being able to close PRs, but it doe=
s
help make sure no PR is left hanging for long.

What do you think=3F I'd be interested in implementing this solution if w=
e
like it.

Nick


--53fcd49b_625558ec_104--


From dev-return-9081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 18:56:35 2014
Return-Path: <dev-return-9081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF40E113DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 18:56:35 +0000 (UTC)
Received: (qmail 31761 invoked by uid 500); 26 Aug 2014 18:56:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31688 invoked by uid 500); 26 Aug 2014 18:56:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31677 invoked by uid 99); 26 Aug 2014 18:56:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:56:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 18:56:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1XMLuZ-0001Vg-AK
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 11:56:07 -0700
Date: Tue, 26 Aug 2014 11:56:07 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409079367310-8046.post@n3.nabble.com>
In-Reply-To: <061475AE-BB75-4A6C-813F-68B70A1B8EED@gmail.com>
References: <CAPh_B=aL0UinEJRWAy8oh7byLbuOKocC1g88mjdb60LYVP6MQQ@mail.gmail.com> <CABpRO2cf-LTJ2n8Z=6kk-GfiVp3DoLsYrDCUe4hkiX2gqwPbkg@mail.gmail.com> <CAPh_B=YnjnUEAkRyOikACYcTPL7=jkYqz14Gkaq-qtF=8aoP=w@mail.gmail.com> <CABpRO2eznzJF7DHTEuPSeP+2JYzD-A0HKpB2X-qHSo6nt8tKkw@mail.gmail.com> <CAPh_B=bCedf3j2K5JYNxznkn174aJXHaWMY2DiiVJDe_QikvRg@mail.gmail.com> <CA+B-+fzM+G+FRAiZgzfVSJaeCUPJRTWwNpZyouoUDya1OMyvJw@mail.gmail.com> <CAPh_B=Z8Xn_XQR1kuohCoiRCdjepGJF2=--eSFhH4Y2HAk8ZkA@mail.gmail.com> <CA+B-+fzkk=RMbCT7m6veQwCWGq1Jn+-zegXD23sQwa91eSyVvw@mail.gmail.com> <1408990419321-7989.post@n3.nabble.com> <061475AE-BB75-4A6C-813F-68B70A1B8EED@gmail.com>
Subject: Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator
 failing
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have both SPARK-2878 and SPARK-2893. 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-2878-Kryo-serialisation-with-custom-Kryo-registrator-failing-tp7719p8046.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 20:17:09 2014
Return-Path: <dev-return-9082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7306111879
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 20:17:09 +0000 (UTC)
Received: (qmail 71103 invoked by uid 500); 26 Aug 2014 20:17:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71028 invoked by uid 500); 26 Aug 2014 20:17:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71016 invoked by uid 99); 26 Aug 2014 20:17:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:17:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:17:03 +0000
Received: by mail-wg0-f43.google.com with SMTP id l18so14837201wgh.2
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 13:16:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=z1ikmoyC4YnLYVibhxXsMhoXWK4SfOJQM5DKSiqETwc=;
        b=A2A0FckF7NEOxrZtAvHn90IAZ8LC4S/rbH84AgDdqm1dtMJGgfQGxZodjzNxQaM44a
         p6CJr+nHqXM7ZN1LzASIUu5lAk6PehEmsHiMaBsf7ypuyBeuT5I6u7AD9H63dLrEnVvv
         lyTIgFBrANa4hyx7xs9NVAmDLQaklxFU99Fi6Nyd1jftIgP2wsR5uLOT6WFvt7/fzKEK
         58B/r+E3UFbEQgcAumHOCn13Y7c60pch7qUHtFAU2Nb+YuhVCX5E7oPM30Kg77Whjqfy
         J7ef/42Zxt36RgUikHbsJzDxPa1pCUg4H6ns1wgQo/cWwpwRFa2zFd2m54z+NujuJZNG
         TBHQ==
X-Received: by 10.180.99.97 with SMTP id ep1mr23207917wib.33.1409084202407;
 Tue, 26 Aug 2014 13:16:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 26 Aug 2014 13:16:02 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 26 Aug 2014 16:16:02 -0400
Message-ID: <CAOhmDzd_h_Q0FpTF0AVqQcS1Q7XLXVMDprmJ8k=XvPceKJAGSw@mail.gmail.com>
Subject: spark-ec2 1.0.2 creates EC2 cluster at wrong version
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04428e7657dcc005018dfb01
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428e7657dcc005018dfb01
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I downloaded the source code release for 1.0.2 from here
<http://spark.apache.org/downloads.html> and launched an EC2 cluster using
spark-ec2.

After the cluster finishes launching, I fire up the shell and check the
version:

scala> sc.version
res1: String =3D 1.0.1

The startup banner also shows the same thing. Hmm...

So I dig around and find that the spark_ec2.py script has the default Spark
version set to 1.0.1.

Derp.

  parser.add_option("-v", "--spark-version", default=3D"1.0.1",
      help=3D"Version of Spark to use: 'X.Y.Z' or a specific git hash")

Is there any way to fix the release? It=E2=80=99s a minor issue, but could =
be very
confusing. And how can we prevent this from happening again?

Nick
=E2=80=8B

--f46d04428e7657dcc005018dfb01--

From dev-return-9083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 20:24:44 2014
Return-Path: <dev-return-9083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9FEC91191C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 20:24:44 +0000 (UTC)
Received: (qmail 19336 invoked by uid 500); 26 Aug 2014 20:24:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19266 invoked by uid 500); 26 Aug 2014 20:24:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19255 invoked by uid 99); 26 Aug 2014 20:24:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:24:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:24:38 +0000
Received: by mail-wg0-f48.google.com with SMTP id x13so15112276wgg.7
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 13:24:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=C9ZAnebPWbwCeFG5Y/U8a1zp/TnMFAji/B/lTUmWO+w=;
        b=RysWTv39B6Deb3332CAMtkeWfC5BWx3SvCa1CK6EQVjI7CfkGHKHepGorVosNXCUK7
         zHXtlK2e+3cU1zAL6dz9tc70au3bDcMGBQO7XhHTtIeiuCdLsvngNgsAOi8Xsce2X4QE
         hPgrmmTuEXdPlqwVh+QhoFPLwRzQRhhxRnXvyUZ1UY/FmFwcFni3FkFG6/mSGIO122Jm
         z2mPSd8yToumkzx4Dj3QcWiDsPv8p3e6a90r5JZHbx1OoKGlXTgwZODTZwlqwscRVzpI
         8fA0zF9guRpWw+m6CHhn4tsWgg1/nBhhaMho6x9vO5I4VNNBO4uPLm3vSwkABZP3uX7d
         Pq/Q==
X-Gm-Message-State: ALoCoQmNLd853WgK0sm3meZt2IIlHq/XJx16964Pmt2JhxdM1JFo5xXgPO70i6DPeaKIhKMESw//
MIME-Version: 1.0
X-Received: by 10.180.20.40 with SMTP id k8mr24298361wie.54.1409084657748;
 Tue, 26 Aug 2014 13:24:17 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.108.198 with HTTP; Tue, 26 Aug 2014 13:24:17 -0700 (PDT)
In-Reply-To: <CAOhmDzd_h_Q0FpTF0AVqQcS1Q7XLXVMDprmJ8k=XvPceKJAGSw@mail.gmail.com>
References: <CAOhmDzd_h_Q0FpTF0AVqQcS1Q7XLXVMDprmJ8k=XvPceKJAGSw@mail.gmail.com>
Date: Tue, 26 Aug 2014 13:24:17 -0700
Message-ID: <CAKx7Bf_x3qE8d1m2Kh_eHboxFW-6H79w=x_0frnKTBaoUXOJTQ@mail.gmail.com>
Subject: Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec53d57af7be10905018e1636
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d57af7be10905018e1636
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This is a chicken and egg problem in some sense. We can't change the ec2
script till we have made the release and uploaded the binaries -- But once
that is done, we can't update the script.

I think the model we support so far  is that you can launch the latest
spark version from the master branch on github. I guess we can try to add
something in the release process that updates the script but doesn't commit
it ? The release managers might be able to add more.

Thanks
Shivaram


On Tue, Aug 26, 2014 at 1:16 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> I downloaded the source code release for 1.0.2 from here
> <http://spark.apache.org/downloads.html> and launched an EC2 cluster usin=
g
> spark-ec2.
>
> After the cluster finishes launching, I fire up the shell and check the
> version:
>
> scala> sc.version
> res1: String =3D 1.0.1
>
> The startup banner also shows the same thing. Hmm...
>
> So I dig around and find that the spark_ec2.py script has the default Spa=
rk
> version set to 1.0.1.
>
> Derp.
>
>   parser.add_option("-v", "--spark-version", default=3D"1.0.1",
>       help=3D"Version of Spark to use: 'X.Y.Z' or a specific git hash")
>
> Is there any way to fix the release? It=E2=80=99s a minor issue, but coul=
d be very
> confusing. And how can we prevent this from happening again?
>
> Nick
> =E2=80=8B
>

--bcaec53d57af7be10905018e1636--

From dev-return-9084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 20:36:51 2014
Return-Path: <dev-return-9084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E462119C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 20:36:51 +0000 (UTC)
Received: (qmail 59057 invoked by uid 500); 26 Aug 2014 20:36:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58987 invoked by uid 500); 26 Aug 2014 20:36:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58975 invoked by uid 99); 26 Aug 2014 20:36:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:36:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:36:24 +0000
Received: by mail-wi0-f181.google.com with SMTP id bs8so4779844wib.2
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 13:36:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pAmnjOlO863VQwzQE9ndK0EuinINstZ7w3hA/6Cap0Q=;
        b=sOAHUiTKYOQKX05dCPQ2SM//gmUYYepdgv8a/rusiucha7vEp251d8rpOZcxTPIcCE
         7i+SsSJCYKUl93qvHF50qCCBj54FjhmErQ6DDXd1FUjT8PTaq67eSsGzTbHCLEwVKa0y
         GkVNiQjEICU3Panu8PbXxJWna9mbppzPtSoUkgHqxFunM9Xm4L62eOTBf70gyPKvlLtg
         /NdfyeY051UWvIKJU2OUWFhWtPzkkRSBvuGtlCPFa9f4bLmyuAWoCUfgvgVICuLrDwOj
         NanEltuQ3OFNfh9J8s78dVyauB+mLqOWSeGqFeAxow2EKwBqnpJMD+IrrGswwnTYZFGM
         UW6g==
MIME-Version: 1.0
X-Received: by 10.180.38.2 with SMTP id c2mr22655666wik.24.1409085383686; Tue,
 26 Aug 2014 13:36:23 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Tue, 26 Aug 2014 13:36:23 -0700 (PDT)
In-Reply-To: <CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
	<CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>
	<CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
Date: Tue, 26 Aug 2014 16:36:23 -0400
Message-ID: <CADtDQQJx2XtxDoLNCSd0xw5Bg28ARCEs4hvuz-LY0ca6XNtX2g@mail.gmail.com>
Subject: Re: Gradient descent and runMiniBatchSGD
From: RJ Nowling <rnowling@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: "Ulanov, Alexander" <alexander.ulanov@hp.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f643374c0c2d305018e4199
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f643374c0c2d305018e4199
Content-Type: text/plain; charset=UTF-8

Xiangrui,

I posted a note on my JIRA for MiniBatch KMeans about the same problem --
sampling running in O(n).

Can you elaborate on ways to get more efficient sampling?  I think this
will be important for a variety of stochastic algorithms.

RJ


On Tue, Aug 26, 2014 at 12:54 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> miniBatchFraction uses RDD.sample to get the mini-batch, and sample
> still needs to visit the elements one after another. So it is not
> efficient if the task is not computation heavy and this is why
> setMiniBatchFraction is marked as experimental. If we can detect that
> the partition iterator is backed by an ArrayBuffer, maybe we can do a
> skip iterator to skip elements. -Xiangrui
>
> On Tue, Aug 26, 2014 at 8:15 AM, Ulanov, Alexander
> <alexander.ulanov@hp.com> wrote:
> > Hi, RJ
> >
> >
> https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala/org/apache/spark/mllib/classification/NeuralNetwork.scala
> >
> > Unit tests are in the same branch.
> >
> > Alexander
> >
> > From: RJ Nowling [mailto:rnowling@gmail.com]
> > Sent: Tuesday, August 26, 2014 6:59 PM
> > To: Ulanov, Alexander
> > Cc: dev@spark.apache.org
> > Subject: Re: Gradient descent and runMiniBatchSGD
> >
> > Hi Alexander,
> >
> > Can you post a link to the code?
> >
> > RJ
> >
> > On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> > Hi,
> >
> > I've implemented back propagation algorithm using Gradient class and a
> simple update using Updater class. Then I run the algorithm with mllib's
> GradientDescent class. I have troubles in scaling out this implementation.
> I thought that if I partition my data into the number of workers then
> performance will increase, because each worker will run a step of gradient
> descent on its partition of data. But this does not happen and each worker
> seems to process all data (if miniBatchFraction == 1.0 as in mllib's
> logisic regression implementation). For me, this doesn't make sense,
> because then only single Worker will provide the same performance. Could
> someone elaborate on this and correct me if I am wrong. How can I scale out
> the algorithm with many Workers?
> >
> > Best regards, Alexander
> >
> >
> >
> > --
> > em rnowling@gmail.com<mailto:rnowling@gmail.com>
> > c 954.496.2314
>



-- 
em rnowling@gmail.com
c 954.496.2314

--e89a8f643374c0c2d305018e4199--

From dev-return-9085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 20:38:01 2014
Return-Path: <dev-return-9085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F1C3119E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 20:38:01 +0000 (UTC)
Received: (qmail 62170 invoked by uid 500); 26 Aug 2014 20:38:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62102 invoked by uid 500); 26 Aug 2014 20:38:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62083 invoked by uid 99); 26 Aug 2014 20:38:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:38:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:37:56 +0000
Received: by mail-we0-f181.google.com with SMTP id k48so15307273wev.40
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 13:37:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=JEYt1zv2bQIpXDmxAXrrYjogyPITqQo2zLEDinZmtDw=;
        b=GStEVUB6w2e9tPA9LNRBkvr7WeQXQpXkIteVDu/p7FnpRKW5Le1tfbpjfw49OjVpmT
         W61qrHC/pjXhvwhN2lsv/BffjEpqHHZ72lolgnkFuxN4UfN3j51rD7k/YXLt5UEuMmxm
         iUzajLfbpkENhVGe3L8JyvbQNLDGhRl45TgFyRfpaLJBWlRPDelbRV67u7AP3DAPqL1A
         7ZjwlC+4KKJeRZukeSV2tIzgPEgej4qBQ+8zIkH2MGwESSVcUUMOOKzi2G8M0l4BIjzC
         q7rzp/HWBY0w18OhG0oqoEd3ylhDM9dDMu1Ms+c6p+SlUrZbQ52dkqL6Id/p1nudjKne
         MYpw==
MIME-Version: 1.0
X-Received: by 10.194.71.210 with SMTP id x18mr30965987wju.6.1409085454903;
 Tue, 26 Aug 2014 13:37:34 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Tue, 26 Aug 2014 13:37:34 -0700 (PDT)
In-Reply-To: <CADtDQQJx2XtxDoLNCSd0xw5Bg28ARCEs4hvuz-LY0ca6XNtX2g@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
	<CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>
	<CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
	<CADtDQQJx2XtxDoLNCSd0xw5Bg28ARCEs4hvuz-LY0ca6XNtX2g@mail.gmail.com>
Date: Tue, 26 Aug 2014 16:37:34 -0400
Message-ID: <CADtDQQKnZLJvBfg=n+OrGJgqGnKtSAqNxK9Pqm2x7qW3T8KZow@mail.gmail.com>
Subject: Re: Gradient descent and runMiniBatchSGD
From: RJ Nowling <rnowling@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: "Ulanov, Alexander" <alexander.ulanov@hp.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd0776ff703c05018e45c1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0776ff703c05018e45c1
Content-Type: text/plain; charset=UTF-8

Also, another idea: may algorithms that use sampling tend to do so multiple
times.  It may be beneficial to allow a transformation to a representation
that is more efficient for multiple rounds of sampling.


On Tue, Aug 26, 2014 at 4:36 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Xiangrui,
>
> I posted a note on my JIRA for MiniBatch KMeans about the same problem --
> sampling running in O(n).
>
> Can you elaborate on ways to get more efficient sampling?  I think this
> will be important for a variety of stochastic algorithms.
>
> RJ
>
>
> On Tue, Aug 26, 2014 at 12:54 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> miniBatchFraction uses RDD.sample to get the mini-batch, and sample
>> still needs to visit the elements one after another. So it is not
>> efficient if the task is not computation heavy and this is why
>> setMiniBatchFraction is marked as experimental. If we can detect that
>> the partition iterator is backed by an ArrayBuffer, maybe we can do a
>> skip iterator to skip elements. -Xiangrui
>>
>> On Tue, Aug 26, 2014 at 8:15 AM, Ulanov, Alexander
>> <alexander.ulanov@hp.com> wrote:
>> > Hi, RJ
>> >
>> >
>> https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala/org/apache/spark/mllib/classification/NeuralNetwork.scala
>> >
>> > Unit tests are in the same branch.
>> >
>> > Alexander
>> >
>> > From: RJ Nowling [mailto:rnowling@gmail.com]
>> > Sent: Tuesday, August 26, 2014 6:59 PM
>> > To: Ulanov, Alexander
>> > Cc: dev@spark.apache.org
>> > Subject: Re: Gradient descent and runMiniBatchSGD
>> >
>> > Hi Alexander,
>> >
>> > Can you post a link to the code?
>> >
>> > RJ
>> >
>> > On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> > Hi,
>> >
>> > I've implemented back propagation algorithm using Gradient class and a
>> simple update using Updater class. Then I run the algorithm with mllib's
>> GradientDescent class. I have troubles in scaling out this implementation.
>> I thought that if I partition my data into the number of workers then
>> performance will increase, because each worker will run a step of gradient
>> descent on its partition of data. But this does not happen and each worker
>> seems to process all data (if miniBatchFraction == 1.0 as in mllib's
>> logisic regression implementation). For me, this doesn't make sense,
>> because then only single Worker will provide the same performance. Could
>> someone elaborate on this and correct me if I am wrong. How can I scale out
>> the algorithm with many Workers?
>> >
>> > Best regards, Alexander
>> >
>> >
>> >
>> > --
>> > em rnowling@gmail.com<mailto:rnowling@gmail.com>
>> > c 954.496.2314
>>
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>



-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bfd0776ff703c05018e45c1--

From dev-return-9086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 20:51:49 2014
Return-Path: <dev-return-9086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EEDD011B11
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 20:51:48 +0000 (UTC)
Received: (qmail 11981 invoked by uid 500); 26 Aug 2014 20:51:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11935 invoked by uid 500); 26 Aug 2014 20:51:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11924 invoked by uid 99); 26 Aug 2014 20:51:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:51:47 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 20:51:42 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 2E150B3
	for <dev@spark.apache.org>; Tue, 26 Aug 2014 20:51:21 +0000 (UTC)
Received: from G4W6301.americas.hpqcorp.net (16.210.26.226) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 26 Aug 2014 20:49:50 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.222]) by
 G4W6301.americas.hpqcorp.net ([16.210.26.226]) with mapi id 14.03.0169.001;
 Tue, 26 Aug 2014 20:49:49 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Xiangrui Meng <mengxr@gmail.com>
CC: RJ Nowling <rnowling@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Re: Gradient descent and runMiniBatchSGD
Thread-Topic: Gradient descent and runMiniBatchSGD
Thread-Index: Ac/BGakk7EDh0bW5SIe9VrxUpkwvcgAJJYWAAACGR/AAA4PdgAAIN70Q
Date: Tue, 26 Aug 2014 20:49:48 +0000
Message-ID: <6492CAC1-7628-41B0-ABD2-4A3D489DC9F0@hp.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCF5F36@G4W3292.americas.hpqcorp.net>
	<CADtDQQ+oXLQQY+wth=FtgD6NettXpED2fM+vWnpJoyt7BCbMjg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FCF702A@G4W3292.americas.hpqcorp.net>,<CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
In-Reply-To: <CAJgQjQ-Co8Sv-uWNfuy3hg4YJO4rzfQaeGaJC17CamfauFTwQg@mail.gmail.com>
Accept-Language: en-US
Content-Language: ru-RU
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Xiangrui,

Thanks for explanation, but I'm still missing something. In my experiments,=
 if miniBatchFraction =3D=3D 1.0, no matter how the data is partitioned (2,=
 4, 8, 16 partitions), the algorithm executes more or less in the same time=
. (I have 16 Workers). Reduce from runMiniBatchSGD takes most of the time f=
or 2 partitions, mapPartitionWithIndex -- for 16. What I would expect is th=
at the time reduces proportional to the number of data partitions because e=
ach partition will be processed on separate Worker hopefully. Why the time =
does not reduce?

Btw processing of one instance in my algorithm is a heavy computation, this=
 is exact reason why I want to parallelize it.

Best regards, Alexander

26.08.2014, =D7 20:54, "Xiangrui Meng" <mengxr@gmail.com<mailto:mengxr@gmai=
l.com>> =CE=C1=D0=C9=D3=C1=CC(=C1):

miniBatchFraction uses RDD.sample to get the mini-batch, and sample
still needs to visit the elements one after another. So it is not
efficient if the task is not computation heavy and this is why
setMiniBatchFraction is marked as experimental. If we can detect that
the partition iterator is backed by an ArrayBuffer, maybe we can do a
skip iterator to skip elements. -Xiangrui

On Tue, Aug 26, 2014 at 8:15 AM, Ulanov, Alexander
<alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi, RJ

https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala/o=
rg/apache/spark/mllib/classification/NeuralNetwork.scala

Unit tests are in the same branch.

Alexander

From: RJ Nowling [mailto:rnowling@gmail.com]
Sent: Tuesday, August 26, 2014 6:59 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Gradient descent and runMiniBatchSGD

Hi Alexander,

Can you post a link to the code?

RJ

On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <alexander.ulanov@hp.com=
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com>> wrote:
Hi,

I've implemented back propagation algorithm using Gradient class and a simp=
le update using Updater class. Then I run the algorithm with mllib's Gradie=
ntDescent class. I have troubles in scaling out this implementation. I thou=
ght that if I partition my data into the number of workers then performance=
 will increase, because each worker will run a step of gradient descent on =
its partition of data. But this does not happen and each worker seems to pr=
ocess all data (if miniBatchFraction =3D=3D 1.0 as in mllib's logisic regre=
ssion implementation). For me, this doesn't make sense, because then only s=
ingle Worker will provide the same performance. Could someone elaborate on =
this and correct me if I am wrong. How can I scale out the algorithm with m=
any Workers?

Best regards, Alexander



--
em rnowling@gmail.com<mailto:rnowling@gmail.com><mailto:rnowling@gmail.com>
c 954.496.2314

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 22:16:48 2014
Return-Path: <dev-return-9087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C04E11F5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 22:16:48 +0000 (UTC)
Received: (qmail 66265 invoked by uid 500); 26 Aug 2014 22:16:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66203 invoked by uid 500); 26 Aug 2014 22:16:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66191 invoked by uid 99); 26 Aug 2014 22:16:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 22:16:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 22:16:20 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so4935180wib.8
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 15:16:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=7MJogT1t9zagW9NUeQeg3Ve+eFBLRPMZUoG0GyVGcMw=;
        b=NfgAclwWPQOtsjm+Lofc2MX65hoq+YksPC3p000ml5LEjocFxihXRoABsBlEwb7a70
         r2KM60HppBxN1M3bJGS/MXE9i0nmF81BMKRIBSUTUxW8ySNX2QABm8op2UyWsPWWKMJb
         GBPqmufNYRkX9Tm84Pj7DjmZJqKP1jQ+T/EuBkvJe4IF+arJwAAIqS0lRDrSNwapUyCX
         hqWhGEDyKOduiRA/g2At+iQueakL91rc0dcNU+QL1boqej9w6m/yzkIKxcTc6mE/ai/6
         f9zXt0ChuHrNbmUQypbtlACeZXWdsE5j4eKPK+6kuUAcxr8qY9wv266NtGh4qzj3ZJ8L
         H1iA==
X-Received: by 10.194.122.6 with SMTP id lo6mr32150235wjb.17.1409091380113;
 Tue, 26 Aug 2014 15:16:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 26 Aug 2014 15:15:39 -0700 (PDT)
In-Reply-To: <etPan.53fcd49b.2ae8944a.104@joshs-mbp>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp> <CAOhmDzcwt7J9dhqea_sP5wuug_2nXCJfsDnyXpE=1s8byLD24g@mail.gmail.com>
 <etPan.53fcd49b.2ae8944a.104@joshs-mbp>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 26 Aug 2014 18:15:39 -0400
Message-ID: <CAOhmDzcwzHV76hm+L+X0TmofNvZ3tkivR2B1fdz4WQkM-RKDoA@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Josh Rosen <rosenville@gmail.com>
Cc: dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=089e0117643d2af7c005018fa7f8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0117643d2af7c005018fa7f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

By the way, as a reference point, I just stumbled across the Discourse
GitHub project and their list of pull requests
<https://github.com/discourse/discourse/pulls> looks pretty neat.

~2,200 closed PRs, 6 open. Least recently updated PR dates to 8 days ago.
Project started ~1.5 years ago.

Dunno how many committers Discourse has, but it looks like they've managed
their PRs well. I hope we can do as well in this regard as they have.

Nick


On Tue, Aug 26, 2014 at 2:40 PM, Josh Rosen <rosenville@gmail.com> wrote:

> Sure; App Engine supports cron and sending emails.  We can configure the
> app with Spark QA=E2=80=99s credentials in order to allow it to post comm=
ents on
> issues, etc.
>
> - Josh
>
> On August 26, 2014 at 11:38:08 AM, Nicholas Chammas (
> nicholas.chammas@gmail.com) wrote:
>
>  OK, that sounds pretty cool.
>
> Josh,
>
> Do you see this app as encompassing or supplanting the functionality I
> described as well?
>
> Nick
>
>
> On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote:
>
>>  Last weekend, I started hacking on a Google App Engine app for helping
>> with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
>>  Some of my basic goals (not all implemented yet):
>>
>>  - Users sign in using GitHub and can browse a list of pull requests,
>> including links to associated JIRAs, Jenkins statuses, a quick preview o=
f
>> the last comment, etc.
>>
>>  - Pull requests are auto-classified based on which components they
>> modify (by looking at the diff).
>>
>>  - From the app=E2=80=99s own internal database of PRs, we can build das=
hboards
>> to find =E2=80=9Cabandoned=E2=80=9D PRs, graph average time to first rev=
iew, etc.
>>
>>  - Since we authenticate users with GitHub, we can enable administrative
>> functions via this dashboard (e.g. =E2=80=9Cassign this PR to me=E2=80=
=9D, =E2=80=9Cvote to close
>> in the weekly auto-close commit=E2=80=9D, etc.
>>
>>  Right now, I=E2=80=99ve implemented GItHub OAuth support and code to up=
date the
>> issues database using the GitHub API.  Because we have access to the ful=
l
>> API, it=E2=80=99s pretty easy to do fancy things like parsing the reason=
 for
>> Jenkins failure, etc.  You could even imagine some fancy mashup tools to
>> pull up JIRAs and pull requests side-by in iframes.
>>
>> After I hack on this a bit more, I plan to release a public preview
>> version; if we find this tool useful, I=E2=80=99ll clean it up and open-=
source the
>> app so folks can contribute to it.
>>
>> - Josh
>>
>> On August 26, 2014 at 8:16:46 AM, Nicholas Chammas (
>> nicholas.chammas@gmail.com) wrote:
>>
>>  On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > I'd prefer if we took the approach of politely explaining why in the
>> > current form the patch isn't acceptable and closing it (potentially w/
>> tips
>> > on how to improve it or narrow the scope).
>>
>>
>> Amen to this. Aiming for such a culture would set Spark apart from other
>> projects in a great way.
>>
>> I've proposed several different solutions to ASF infra to streamline the
>> > process, but thus far they haven't been open to any of my ideas:
>>
>>
>> I've added myself as a watcher on those 2 INFRA issues. Sucks that the
>> only
>> solution on offer right now requires basically polluting the commit
>> history.
>>
>> Short of moving Spark's repo to a non-ASF-managed GitHub account, do you
>> think another bot could help us manage the number of stale PRs?
>>
>> I'm thinking a solution as follows might be very helpful:
>>
>> - Extend Spark QA / Jenkins to run on a weekly schedule and check for
>> stale PRs. Let's say a stale PR is an open one that hasn't been updated =
in
>> N months.
>> - Spark QA maintains a list of known committers on its side.
>> - During its weekly check of stale PRs, Spark QA takes the following
>> action:
>> - If the last person to comment on a PR was a committer, post to the
>> PR asking for an update from the contributor.
>> - If the last person to comment on a PR was a contributor, add the PR
>> to a list. Email this list of *hanging PRs* out to the dev list on a
>> weekly basis and ask committers to update them.
>> - If the last person to comment on a PR was Spark QA asking the
>> contributor to update it, then add the PR to a list. Email this
>> list of *abandoned
>> PRs* to the dev list for the record (or for closing, if that becomes
>> possible in the future).
>>
>> This doesn't solve the problem of not being able to close PRs, but it do=
es
>> help make sure no PR is left hanging for long.
>>
>> What do you think? I'd be interested in implementing this solution if we
>> like it.
>>
>> Nick
>>
>>
>

--089e0117643d2af7c005018fa7f8--

From dev-return-9088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 22:51:04 2014
Return-Path: <dev-return-9088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1664B110E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 22:51:04 +0000 (UTC)
Received: (qmail 62965 invoked by uid 500); 26 Aug 2014 22:51:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62899 invoked by uid 500); 26 Aug 2014 22:51:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62888 invoked by uid 99); 26 Aug 2014 22:51:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 22:51:00 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.217.179] (HELO mail-lb0-f179.google.com) (209.85.217.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 22:50:34 +0000
Received: by mail-lb0-f179.google.com with SMTP id v6so2154251lbi.38
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 15:50:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=AGfwEIKMnsSgEvneeedsdE27Te0P7Ut+dlSMipji2o0=;
        b=YS158r6dSi9aBcW1DdnhF8o+pM//gw+EjqG8xn+jPrdAGW+7CfgACHfAT80FhrxzrV
         C8jvhBWNB23LX/cjnrmMV/4IKVCGzvH++57YZYtT2mLeDkzbOSixkaNxsUfFMtNliuJ1
         ucdpOnlGm+jwlb8Fob1sjiW/BMZNZ9n2C7dHwz3spaBORhz4sGp8AzOhz7eVhQbgqqX6
         DtHFfmujk6xQT16KoRH3q+wLJKduZXJuYkW90nDoSBgI7b35KccMuaS1FaIgeVxZOfcJ
         70YIOHb3RyZloKpiXfyQnfpXLRDAQ1+p4zt/0v+Fb0TKo13hOs5chlXuCFy1oftjIlRk
         Ht0Q==
X-Gm-Message-State: ALoCoQmTliJAz7shHPCUZ2fdO21fyNXJeNwSGj8B3iwW82GR11kB2FaLnBwwBPVhzHK2NtLpU2sL
X-Received: by 10.112.202.69 with SMTP id kg5mr28995287lbc.33.1409093433479;
 Tue, 26 Aug 2014 15:50:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.30.5 with HTTP; Tue, 26 Aug 2014 15:50:13 -0700 (PDT)
In-Reply-To: <CAGOvqiqH2FhNW-Hi5cjvLz81Tr3jYoG6BzX79s01=gsJak6bKQ@mail.gmail.com>
References: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
 <CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
 <CAGh_TuPDEEck-N-LiCvw0GLYi9S+-rqRieUxt_HLN1KhGwBFdQ@mail.gmail.com> <CAGOvqiqH2FhNW-Hi5cjvLz81Tr3jYoG6BzX79s01=gsJak6bKQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 26 Aug 2014 15:50:13 -0700
Message-ID: <CAAswR-706RXhMC0NAgUCKUn6_tkZLcCKxCLV0Ln0HJ9HJd563A@mail.gmail.com>
Subject: Re: CoHadoop Papers
To: Gary Malouf <malouf.gary@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3712e8ee8550501902160
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3712e8ee8550501902160
Content-Type: text/plain; charset=UTF-8

It seems like there are two things here:
 - Co-locating blocks with the same keys to avoid network transfer.
 - Leveraging partitioning information to avoid a shuffle when data is
already partitioned correctly (even if those partitions aren't yet on the
same machine).

The former seems more complicated and probably requires the support from
Hadoop you linked to.  However, the latter might be easier as there is
already a framework for reasoning about partitioning and the need to
shuffle in the Spark SQL planner.


On Tue, Aug 26, 2014 at 8:37 AM, Gary Malouf <malouf.gary@gmail.com> wrote:

> Christopher, can you expand on the co-partitioning support?
>
> We have a number of spark SQL tables (saved in parquet format) that all
> could be considered to have a common hash key.  Our analytics team wants to
> do frequent joins across these different data-sets based on this key.  It
> makes sense that if the data for each key across 'tables' was co-located on
> the same server, shuffles could be minimized and ultimately performance
> could be much better.
>
> From reading the HDFS issue I posted before, the way is being paved for
> implementing this type of behavior though there are a lot of complications
> to make it work I believe.
>
>
> On Tue, Aug 26, 2014 at 10:40 AM, Christopher Nguyen <ctn@adatao.com>
> wrote:
>
> > Gary, do you mean Spark and HDFS separately, or Spark's use of HDFS?
> >
> > If the former, Spark does support copartitioning.
> >
> > If the latter, it's an HDFS scope that's outside of Spark. On that note,
> > Hadoop does also make attempts to collocate data, e.g., rack awareness.
> I'm
> > sure the paper makes useful contributions for its set of use cases.
> >
> > Sent while mobile. Pls excuse typos etc.
> > On Aug 26, 2014 5:21 AM, "Gary Malouf" <malouf.gary@gmail.com> wrote:
> >
> >> It appears support for this type of control over block placement is
> going
> >> out in the next version of HDFS:
> >> https://issues.apache.org/jira/browse/HDFS-2576
> >>
> >>
> >> On Tue, Aug 26, 2014 at 7:43 AM, Gary Malouf <malouf.gary@gmail.com>
> >> wrote:
> >>
> >> > One of my colleagues has been questioning me as to why Spark/HDFS
> makes
> >> no
> >> > attempts to try to co-locate related data blocks.  He pointed to this
> >> > paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on
> >> the
> >> > CoHadoop research and the performance improvements it yielded for
> >> > Map/Reduce jobs.
> >> >
> >> > Would leveraging these ideas for writing data from Spark make sense/be
> >> > worthwhile?
> >> >
> >> >
> >> >
> >>
> >
>

--001a11c3712e8ee8550501902160--

From dev-return-9089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Aug 26 23:43:42 2014
Return-Path: <dev-return-9089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB309112A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Aug 2014 23:43:42 +0000 (UTC)
Received: (qmail 85370 invoked by uid 500); 26 Aug 2014 23:43:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85301 invoked by uid 500); 26 Aug 2014 23:43:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85288 invoked by uid 99); 26 Aug 2014 23:43:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 23:43:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Aug 2014 23:43:15 +0000
Received: by mail-ie0-f174.google.com with SMTP id rp18so12105653iec.5
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 16:43:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=CqWlzNN+e8ZLFxuGCYztWutbfRb7KvQbkj+c7H1pDy4=;
        b=wwbSK0nW2aWQkpa6VPe5S4x7cANDOrZ4aRag7hVrfG59E1gpdstVfabwN6rkgOYZl5
         uuvxeC4MoV+FNoZESslpMqTd4Y0wacz8sxqMfoUY567irebz4LYod2yuyjewBgO2mL2R
         /G+n0yOf4R+3PxuFcN7YEkUoJNrPZ7qZLV9SCYlbLGmHmW+mMyYirmNh0rTJ4lnMhj2G
         jLbrnPCk4wIFn782TBppV/dkY6QBa9HMaZeJ5mx5Ll6DJvqHBcnm44FKkyD2sXGfBdes
         epC+VT1IDZ1Iwp6Bs104tam2+dCDSEK/wOnqWpvbo1kXj1ZAS316Qe2wMpuj6qDbojnN
         G3ig==
MIME-Version: 1.0
X-Received: by 10.50.117.106 with SMTP id kd10mr25779433igb.5.1409096594003;
 Tue, 26 Aug 2014 16:43:14 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Tue, 26 Aug 2014 16:43:13 -0700 (PDT)
In-Reply-To: <53FC7E2E.8020802@oss.nttdata.co.jp>
References: <CA+FETEJp-Xyj7g7u4B83i_oZb-zcka2YxsKH+2wwCQiMVZWWoQ@mail.gmail.com>
	<53FC7E2E.8020802@oss.nttdata.co.jp>
Date: Tue, 26 Aug 2014 16:43:13 -0700
Message-ID: <CA+FETE+DbZFWM+uGwWP9FDzubA9tH2mM24YvCP9HyWtggho5Tg@mail.gmail.com>
Subject: Re: too many CancelledKeyException throwed from ConnectionManager
From: yao <yaoshengzhe@gmail.com>
To: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0139fb62f09eed050190dde8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0139fb62f09eed050190dde8
Content-Type: text/plain; charset=UTF-8

Wow, great job. We will take a look and try our application again with your
patch.


On Tue, Aug 26, 2014 at 5:31 AM, Kousuke Saruta <sarutak@oss.nttdata.co.jp>
wrote:

> Hi Shengzhe
>
> I faced to same situation.
>
> I think, Connection and ConnectionManager have some race condition issues
> and the error you mentioned may be caused by the issues.
> Now I'm trying to resolve the issue in https://github.com/apache/
> spark/pull/2019.
> Please check it out.
>
> - Kousuke
>
>
> (2014/08/26 8:53), yao wrote:
>
>> Hi Folks,
>>
>> We are testing our home-made KMeans algorithm using Spark on Yarn.
>> Recently, we've found that the application failed frequently when doing
>> clustering over 300,000,000 users (each user is represented by a feature
>> vector and the whole data set is around 600,000,000). After digging into
>> the job log, we've found that there are many CancelledKeyException throwed
>> by ConnectionManager but not observed other exceptions. We double frequent
>> CancelledKeyException brings the whole application down since the
>> application often failed on the third or fourth iteration for large
>> datasets. Welcome to any directional suggestions.
>>
>> *Errors in job log*:
>>
>> java.nio.channels.CancelledKeyException
>>          at
>> org.apache.spark.network.ConnectionManager.run(
>> ConnectionManager.scala:363)
>>          at
>> org.apache.spark.network.ConnectionManager$$anon$4.run(
>> ConnectionManager.scala:116)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
>> ConnectionManagerId(lsv-289.rfiserve.net,43199)
>> 14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
>> SendingConnectionManagerId not found
>> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
>> sun.nio.ch.SelectionKeyImpl@2570cd62
>> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
>> sun.nio.ch.SelectionKeyImpl@2570cd62
>> java.nio.channels.CancelledKeyException
>>          at
>> org.apache.spark.network.ConnectionManager.run(
>> ConnectionManager.scala:363)
>>          at
>> org.apache.spark.network.ConnectionManager$$anon$4.run(
>> ConnectionManager.scala:116)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
>> ConnectionManagerId(lsv-289.rfiserve.net,56727)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
>> ConnectionManagerId(lsv-289.rfiserve.net,56727)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
>> ConnectionManagerId(lsv-289.rfiserve.net,56727)
>> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
>> sun.nio.ch.SelectionKeyImpl@37c8b85a
>> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
>> sun.nio.ch.SelectionKeyImpl@37c8b85a
>> java.nio.channels.CancelledKeyException
>>          at
>> org.apache.spark.network.ConnectionManager.run(
>> ConnectionManager.scala:287)
>>          at
>> org.apache.spark.network.ConnectionManager$$anon$4.run(
>> ConnectionManager.scala:116)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
>> ConnectionManagerId(lsv-668.rfiserve.net,41913)
>> 14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
>> ConnectionManagerId(lsv-668.rfiserve.net,41913)
>> 14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
>> sun.nio.ch.SelectionKeyImpl@fcea3a4
>> 14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
>> SendingConnectionManagerId not found
>> 14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
>> sun.nio.ch.SelectionKeyImpl@fcea3a4
>>
>>
>> Best
>> Shengzhe
>>
>>
>

--089e0139fb62f09eed050190dde8--

From dev-return-9090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 00:19:33 2014
Return-Path: <dev-return-9090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B50B6113D1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 00:19:33 +0000 (UTC)
Received: (qmail 54444 invoked by uid 500); 27 Aug 2014 00:19:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54373 invoked by uid 500); 27 Aug 2014 00:19:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54359 invoked by uid 99); 27 Aug 2014 00:19:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:19:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:19:28 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so14289412qap.23
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 17:19:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=uHrNu1xhuI0a3TTW2aTW1zF8rSm2XJ5FZBGtsA9RIUM=;
        b=LDes6q3QH+EE3oMxTkzWv2NoTAywNioX2O7y+WDwn+c4LGZOkhUfxszbbkvOGytkr9
         sWpyAelHr+L4nAsrjfIQN778aeNhMr/Oic0VWDbVrIUwc9JK/Q9qwb9MX+JofjNJxTBO
         GnXtKockhisqVlnLZG3S2j1JcMf8PMZjvJ5RaBlpfC0oH+PSO8NTJoHP1lONqFyfVge7
         cOfKI1M9uwUh+bVO2VRGoYUxbI4jKT6alTUih+ciOshRWlW1pF4nzDr83ovkts3yfIJ3
         wLjs8bc2BCIUjPGWV/9DhueFj+BtXT93YU2BwGtADO9eseSkh/cZqvMeLp+Eo+fh1vpM
         8QqA==
MIME-Version: 1.0
X-Received: by 10.140.44.67 with SMTP id f61mr2980485qga.44.1409098748084;
 Tue, 26 Aug 2014 17:19:08 -0700 (PDT)
Received: by 10.140.29.102 with HTTP; Tue, 26 Aug 2014 17:19:08 -0700 (PDT)
In-Reply-To: <CAAswR-706RXhMC0NAgUCKUn6_tkZLcCKxCLV0Ln0HJ9HJd563A@mail.gmail.com>
References: <CAGOvqipJEM=biUE5W7r_f+2q5CTh_Ndnq=PE3rHfVMruBWN05g@mail.gmail.com>
	<CAGOvqio9vdiDApeV=XshexXtCnkwbBbBZPeCeAo2EaOhFNMGng@mail.gmail.com>
	<CAGh_TuPDEEck-N-LiCvw0GLYi9S+-rqRieUxt_HLN1KhGwBFdQ@mail.gmail.com>
	<CAGOvqiqH2FhNW-Hi5cjvLz81Tr3jYoG6BzX79s01=gsJak6bKQ@mail.gmail.com>
	<CAAswR-706RXhMC0NAgUCKUn6_tkZLcCKxCLV0Ln0HJ9HJd563A@mail.gmail.com>
Date: Tue, 26 Aug 2014 20:19:08 -0400
Message-ID: <CAGOvqipzHoWca=+aY=qSJeBUV6Gj6-JFy9h=C6ENeygjMf-A8Q@mail.gmail.com>
Subject: Re: CoHadoop Papers
From: Gary Malouf <malouf.gary@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113943d85547a50501915ebf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113943d85547a50501915ebf
Content-Type: text/plain; charset=UTF-8

Hi Michael,

I think once that work is into HDFS, it will be great to expose this
functionality via Spark.  This is something worth pursuing because it could
grant orders of magnitude perf improvements in cases when people need to
join data.

The second item would be very interesting, could yield significant
performance boosts.

Best,

Gary


On Tue, Aug 26, 2014 at 6:50 PM, Michael Armbrust <michael@databricks.com>
wrote:

> It seems like there are two things here:
>  - Co-locating blocks with the same keys to avoid network transfer.
>  - Leveraging partitioning information to avoid a shuffle when data is
> already partitioned correctly (even if those partitions aren't yet on the
> same machine).
>
> The former seems more complicated and probably requires the support from
> Hadoop you linked to.  However, the latter might be easier as there is
> already a framework for reasoning about partitioning and the need to
> shuffle in the Spark SQL planner.
>
>
> On Tue, Aug 26, 2014 at 8:37 AM, Gary Malouf <malouf.gary@gmail.com>
> wrote:
>
>> Christopher, can you expand on the co-partitioning support?
>>
>> We have a number of spark SQL tables (saved in parquet format) that all
>> could be considered to have a common hash key.  Our analytics team wants
>> to
>> do frequent joins across these different data-sets based on this key.  It
>> makes sense that if the data for each key across 'tables' was co-located
>> on
>> the same server, shuffles could be minimized and ultimately performance
>> could be much better.
>>
>> From reading the HDFS issue I posted before, the way is being paved for
>> implementing this type of behavior though there are a lot of complications
>> to make it work I believe.
>>
>>
>> On Tue, Aug 26, 2014 at 10:40 AM, Christopher Nguyen <ctn@adatao.com>
>> wrote:
>>
>> > Gary, do you mean Spark and HDFS separately, or Spark's use of HDFS?
>> >
>> > If the former, Spark does support copartitioning.
>> >
>> > If the latter, it's an HDFS scope that's outside of Spark. On that note,
>> > Hadoop does also make attempts to collocate data, e.g., rack awareness.
>> I'm
>> > sure the paper makes useful contributions for its set of use cases.
>> >
>> > Sent while mobile. Pls excuse typos etc.
>> > On Aug 26, 2014 5:21 AM, "Gary Malouf" <malouf.gary@gmail.com> wrote:
>> >
>> >> It appears support for this type of control over block placement is
>> going
>> >> out in the next version of HDFS:
>> >> https://issues.apache.org/jira/browse/HDFS-2576
>> >>
>> >>
>> >> On Tue, Aug 26, 2014 at 7:43 AM, Gary Malouf <malouf.gary@gmail.com>
>> >> wrote:
>> >>
>> >> > One of my colleagues has been questioning me as to why Spark/HDFS
>> makes
>> >> no
>> >> > attempts to try to co-locate related data blocks.  He pointed to this
>> >> > paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on
>> >> the
>> >> > CoHadoop research and the performance improvements it yielded for
>> >> > Map/Reduce jobs.
>> >> >
>> >> > Would leveraging these ideas for writing data from Spark make
>> sense/be
>> >> > worthwhile?
>> >> >
>> >> >
>> >> >
>> >>
>> >
>>
>
>

--001a113943d85547a50501915ebf--

From dev-return-9091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 00:50:19 2014
Return-Path: <dev-return-9091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E4C8511534
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 00:50:18 +0000 (UTC)
Received: (qmail 20955 invoked by uid 500); 27 Aug 2014 00:50:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20853 invoked by uid 500); 27 Aug 2014 00:50:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20557 invoked by uid 99); 27 Aug 2014 00:50:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:50:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jayunit100.apache@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:49:51 +0000
Received: by mail-pa0-f41.google.com with SMTP id rd3so24490039pab.28
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 17:49:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JcEsAZb4HSRVEkIm3fyTEiTuZx124CzDyOBoKVa84Nk=;
        b=gCKDHTTa59qg3ETKztDzfo9WRvu+9VEmlJy4Tym9lWuMVlz+xiL4QsyHOPuqKEQ0/f
         1wpEk3FB68B7xTgJz+yuaJ2jFqRB3PSJdG8v8srmKQHlhEMPRgDOa2ZaNprr3gtYbfPe
         29ygnnyKQ5HJNLhqhTonUIBLlWtWl4PRL1Z3jNo4VQPveEcTW5/NvqA8ANgWynbITSuh
         kC7CC/DIOAfTHkVAbrkmzHOZsjXNwPWFYVPSZGMFBQ5VwQ9l0JTM8xPW+M7+wsj2Rd94
         zUje/yu7W/93h5jTO3VLH1zEQMD76yFpZwZIITnfUFGHkBKViknIs1Ayom9LrS27j9Ru
         vsYg==
MIME-Version: 1.0
X-Received: by 10.66.66.2 with SMTP id b2mr8330297pat.65.1409100590048; Tue,
 26 Aug 2014 17:49:50 -0700 (PDT)
Received: by 10.70.17.195 with HTTP; Tue, 26 Aug 2014 17:49:50 -0700 (PDT)
Date: Tue, 26 Aug 2014 20:49:50 -0400
Message-ID: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com>
Subject: OutOfMemoryError when running sbt/sbt test
From: jay vyas <jayunit100.apache@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11362f521f6b3b050191cceb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11362f521f6b3b050191cceb
Content-Type: text/plain; charset=UTF-8

Hi spark.

I've been trying to build spark, but I've been getting lots of oome
exceptions.

https://gist.github.com/jayunit100/d424b6b825ce8517d68c

For the most part, they are of the form:

java.lang.OutOfMemoryError: unable to create new native thread

I've attempted to hard code the "get_mem_opts" function, which is in the
sbt-launch-lib.bash file, to
have various very high parameter sizes (i.e. -Xms5g") with high
MaxPermSize, etc... and to no avail.

Any thoughts on this would be appreciated.

I know of others having the same problem as well.

Thanks!

-- 
jay vyas

--001a11362f521f6b3b050191cceb--

From dev-return-9092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 00:58:35 2014
Return-Path: <dev-return-9092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D916F1158E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 00:58:35 +0000 (UTC)
Received: (qmail 41618 invoked by uid 500); 27 Aug 2014 00:58:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41532 invoked by uid 500); 27 Aug 2014 00:58:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41472 invoked by uid 99); 27 Aug 2014 00:58:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:58:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of spark.devuser@gmail.com designates 209.85.160.180 as permitted sender)
Received: from [209.85.160.180] (HELO mail-yk0-f180.google.com) (209.85.160.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 00:58:09 +0000
Received: by mail-yk0-f180.google.com with SMTP id 200so12069339ykr.39
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 17:58:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=O+e5gr1v+nR/GhnRMg5tRm7eSzZNBVcgQt+C3I3WRw4=;
        b=E4KAE5/s0ElUh4D2DByaQObDvilcYwiG35uwhyLP/yfYQBU/rErNI9FIm32Fcgbskd
         wd7VYBOc3lKtwsYm3/ngjTUMW5a5Zj1Cpp6gjVcLab7UZ6d7urgSf4wwf8kvtoEZD+sr
         uZEk6XRf7aFPexMXtEeSN2PAgR8cPm2pqQCRmgarqN56CmbFvhGigdkD3ko1fwSWL47l
         dbFJkpoloFcAexXgvWm5vYSBS3wejG8+87ISgGaTf5kGa0itYPdEUG5bd6Gt6uc700ll
         sGuXhGBjXDkjO85Dr1vIYxBU3mgjwukqY4oNR/ejKV97mkZ4WAEkwE/2EZnEbg/w0HHv
         RnWA==
MIME-Version: 1.0
X-Received: by 10.220.182.1 with SMTP id ca1mr26447617vcb.21.1409101088072;
 Tue, 26 Aug 2014 17:58:08 -0700 (PDT)
Received: by 10.220.247.132 with HTTP; Tue, 26 Aug 2014 17:58:08 -0700 (PDT)
In-Reply-To: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com>
References: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com>
Date: Tue, 26 Aug 2014 17:58:08 -0700
Message-ID: <CAPV_Hnx6bUqivetY_6GDEedbig0x2idHMtNnKSz8eubr67qpHA@mail.gmail.com>
Subject: Re: OutOfMemoryError when running sbt/sbt test
From: Mubarak Seyed <spark.devuser@gmail.com>
To: jay vyas <jayunit100.apache@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1132f2a2cea8a3050191e9a9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f2a2cea8a3050191e9a9
Content-Type: text/plain; charset=UTF-8

What is your ulimit value?


On Tue, Aug 26, 2014 at 5:49 PM, jay vyas <jayunit100.apache@gmail.com>
wrote:

> Hi spark.
>
> I've been trying to build spark, but I've been getting lots of oome
> exceptions.
>
> https://gist.github.com/jayunit100/d424b6b825ce8517d68c
>
> For the most part, they are of the form:
>
> java.lang.OutOfMemoryError: unable to create new native thread
>
> I've attempted to hard code the "get_mem_opts" function, which is in the
> sbt-launch-lib.bash file, to
> have various very high parameter sizes (i.e. -Xms5g") with high
> MaxPermSize, etc... and to no avail.
>
> Any thoughts on this would be appreciated.
>
> I know of others having the same problem as well.
>
> Thanks!
>
> --
> jay vyas
>

--001a1132f2a2cea8a3050191e9a9--

From dev-return-9093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 01:40:01 2014
Return-Path: <dev-return-9093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4FCE61169F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 01:40:01 +0000 (UTC)
Received: (qmail 27498 invoked by uid 500); 27 Aug 2014 01:40:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27425 invoked by uid 500); 27 Aug 2014 01:40:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27409 invoked by uid 99); 27 Aug 2014 01:40:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 01:40:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 01:39:34 +0000
Received: by mail-pa0-f49.google.com with SMTP id hz1so24306227pad.36
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 18:39:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=CuC1dXQ7YGnIPEbr+YYL45M9LnZKkUSo3u2Iy21TpO8=;
        b=w7b8fzMblrc4TC2aACd5cDRPMTcS3M5VJH3IvDD3er4mZcS/zL6LG0fTOq8mt4UuPM
         FabJhnPWjjduOZIhZwfR5ApWSAjMK4FB02Gx8N4Tn14a36cjOWwa16o+lF73JpOFvcps
         ZkCEN+wDFV4t+7MIzuifKlNGSoOKV8516zPV5bqFfFzVwzA55bDZzDtdQBW8Xw+i6DcJ
         +g5JdtWY34fVnFJ+TOzQwUQTxTL/a+/VBHTJhPmf/s6Se6r92pl8KpW/HGIBrS+ogGwU
         KwXjXIsmKr7ZqN9lqGM7sEHRmcD4UpMxWSXRYtQOjzuN/8hKddsK0SBN8h5x2vgftih1
         RcKw==
X-Received: by 10.66.249.34 with SMTP id yr2mr17211733pac.149.1409103572848;
        Tue, 26 Aug 2014 18:39:32 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ef4sm15848505pac.2.2014.08.26.18.39.31
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 26 Aug 2014 18:39:32 -0700 (PDT)
Date: Tue, 26 Aug 2014 18:39:30 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: shivaram@eecs.berkeley.edu, Nicholas Chammas
 <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Message-ID: <etPan.53fd36d2.649bb77c.9a@mbp-3>
In-Reply-To: <CAKx7Bf_x3qE8d1m2Kh_eHboxFW-6H79w=x_0frnKTBaoUXOJTQ@mail.gmail.com>
References: <CAOhmDzd_h_Q0FpTF0AVqQcS1Q7XLXVMDprmJ8k=XvPceKJAGSw@mail.gmail.com>
 <CAKx7Bf_x3qE8d1m2Kh_eHboxFW-6H79w=x_0frnKTBaoUXOJTQ@mail.gmail.com>
Subject: Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fd36d2_275ac794_9a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fd36d2_275ac794_9a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

This shouldn't be a chicken-and-egg problem, since the script fetches the=
 AMI from a known URL. Seems like an issue in publishing this release.

On August 26, 2014 at 1:24:45 PM, Shivaram Venkataraman (shivaram=40eecs.=
berkeley.edu) wrote:

This is a chicken and egg problem in some sense. We can't change the ec2 =
=20
script till we have made the release and uploaded the binaries -- But onc=
e =20
that is done, we can't update the script. =20

I think the model we support so far is that you can launch the latest =20
spark version from the master branch on github. I guess we can try to add=
 =20
something in the release process that updates the script but doesn't comm=
it =20
it =3F The release managers might be able to add more. =20

Thanks =20
Shivaram =20


On Tue, Aug 26, 2014 at 1:16 PM, Nicholas Chammas < =20
nicholas.chammas=40gmail.com> wrote: =20

> I downloaded the source code release for 1.0.2 from here =20
> <http://spark.apache.org/downloads.html> and launched an EC2 cluster us=
ing =20
> spark-ec2. =20
> =20
> After the cluster finishes launching, I fire up the shell and check the=
 =20
> version: =20
> =20
> scala> sc.version =20
> res1: String =3D 1.0.1 =20
> =20
> The startup banner also shows the same thing. Hmm... =20
> =20
> So I dig around and find that the spark=5Fec2.py script has the default=
 Spark =20
> version set to 1.0.1. =20
> =20
> Derp. =20
> =20
> parser.add=5Foption(=22-v=22, =22--spark-version=22, default=3D=221.0.1=
=22, =20
> help=3D=22Version of Spark to use: 'X.Y.Z' or a specific git hash=22) =20
> =20
> Is there any way to fix the release=3F It=E2=80=99s a minor issue, but =
could be very =20
> confusing. And how can we prevent this from happening again=3F =20
> =20
> Nick =20
> =E2=80=8B =20
> =20

--53fd36d2_275ac794_9a--


From dev-return-9094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 02:35:26 2014
Return-Path: <dev-return-9094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9ADA1117A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 02:35:26 +0000 (UTC)
Received: (qmail 3236 invoked by uid 500); 27 Aug 2014 02:35:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3167 invoked by uid 500); 27 Aug 2014 02:35:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3155 invoked by uid 99); 27 Aug 2014 02:35:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 02:35:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jayunit100.apache@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 02:34:56 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so14570262qac.19
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 19:34:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=8tTAn9ROhjP+BuwnAZbP4Fcq4XbyHYVxWuksUR68NnU=;
        b=ESIlc2raOP0QGK6tDZJSSKv9U+kaQVKM/+YgvgyJ3534kahtN8vmFVbjNvLlzIJ6Zt
         BYeK4XZVdGiJP0wBTNPbUNuCFFBbQSyaki2CH9oWVYkMxid+YjeclsP4s/1QzXoVvnbn
         58eQy+Y2U3k4EiPL8lhb/ne9TDITG4SKySSCR8z2ZfxTSNiCYEIVrRbj5VXoPUmEoRZF
         yAZXNvqSCq1keHjqnCDAtnNiWXDa9x4vBUU5255S+ZZ+TxzbI72t8qaUctg/4Jx9H0OA
         z6r9yEU7yTca/f8oU+g/vZZ7dxkWtgfDnqvE/iiVDRaVMEIdmMhxbw6uguuscs9rVupu
         RXyQ==
X-Received: by 10.140.88.243 with SMTP id t106mr48829031qgd.12.1409106895772;
        Tue, 26 Aug 2014 19:34:55 -0700 (PDT)
Received: from [10.0.1.7] (c-71-235-54-127.hsd1.ct.comcast.net. [71.235.54.127])
        by mx.google.com with ESMTPSA id l30sm8136464qgf.9.2014.08.26.19.34.54
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 26 Aug 2014 19:34:54 -0700 (PDT)
Content-Type: multipart/alternative;
	boundary=Apple-Mail-4F67EB84-1BB6-4E32-850D-D6C38EAA2705
Mime-Version: 1.0 (1.0)
Subject: Re: OutOfMemoryError when running sbt/sbt test
From: Jay Vyas <jayunit100.apache@gmail.com>
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <CAPV_Hnx6bUqivetY_6GDEedbig0x2idHMtNnKSz8eubr67qpHA@mail.gmail.com>
Date: Tue, 26 Aug 2014 22:34:53 -0400
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: 7bit
Message-Id: <E40E76D7-7ED4-44F9-AFBD-72780DC71131@gmail.com>
References: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com> <CAPV_Hnx6bUqivetY_6GDEedbig0x2idHMtNnKSz8eubr67qpHA@mail.gmail.com>
To: Mubarak Seyed <spark.devuser@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-4F67EB84-1BB6-4E32-850D-D6C38EAA2705
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

Thanks...! Some questions below.

1) you are suggesting that maybe this OOME is a symptom/red herring , and th=
e true cause of it is that a thread can't span because of ulimit... If so po=
ssibly this could be flagged early on in the build.  And -- where are so man=
y threads coming from that I need to up my limit?   Is this a new feature ad=
ded to spark recently, and if so will it effect deployments scenarios as wel=
l?

And=20

2) possibly SBT_OPTS is where the memory settings should be ? If so, then wh=
y do we have the get_mem_opts wrapper function coded to send memory manually=
 as Xmx/Xms options?
  execRunner "$java_cmd" \
    ${SBT_OPTS:-$default_sbt_opts} \
    $(get_mem_opts $sbt_mem) \
    ${java_opts} \
    ${java_args[@]} \
    -jar "$sbt_jar" \
    "${sbt_commands[@]}" \
    "${residual_args[@]}"



> On Aug 26, 2014, at 8:58 PM, Mubarak Seyed <spark.devuser@gmail.com> wrote=
:
>=20
> What is your ulimit value?
>=20
>=20
>> On Tue, Aug 26, 2014 at 5:49 PM, jay vyas <jayunit100.apache@gmail.com> w=
rote:
>> Hi spark.
>>=20
>> I've been trying to build spark, but I've been getting lots of oome
>> exceptions.
>>=20
>> https://gist.github.com/jayunit100/d424b6b825ce8517d68c
>>=20
>> For the most part, they are of the form:
>>=20
>> java.lang.OutOfMemoryError: unable to create new native thread
>>=20
>> I've attempted to hard code the "get_mem_opts" function, which is in the
>> sbt-launch-lib.bash file, to
>> have various very high parameter sizes (i.e. -Xms5g") with high
>> MaxPermSize, etc... and to no avail.
>>=20
>> Any thoughts on this would be appreciated.
>>=20
>> I know of others having the same problem as well.
>>=20
>> Thanks!
>>=20
>> --
>> jay vyas
>=20

--Apple-Mail-4F67EB84-1BB6-4E32-850D-D6C38EAA2705--

From dev-return-9095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 03:01:56 2014
Return-Path: <dev-return-9095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D29411815
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 03:01:56 +0000 (UTC)
Received: (qmail 28066 invoked by uid 500); 27 Aug 2014 03:01:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27279 invoked by uid 500); 27 Aug 2014 03:01:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27207 invoked by uid 99); 27 Aug 2014 03:01:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:01:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anand.avati@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:01:18 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so12349445obc.5
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 20:01:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=BzveghVOfoNr3b6cyGyFoUYhhaqP6/fuJWidvJt3txg=;
        b=PkuFgWsm6Vh8xgjCE3Pv/2dHlDuifBUVg5Oj/PeQKK3ZDALpTNoH+uySuSWB4VJlWe
         aQhLHkNlaPmm83GaGCpn21RwNDuYOhCD0lzJvtrjmI75hpPVPEWK5PbbVvZI6pSx02pg
         nsziivg7J+85QZ7jHPl4/9aRkPRkIB6QkvPzuIbFnESWOIHLpglfbFExrNIlRWnMXtV4
         LpZqmZPZzHtZrjKRCMwHGuEG+RTl1qcUZaMQuOJ/+SwiCDoXO1+tmzypwOeVFSwY4RsI
         8PtQ9zb8l+jqBxtTu8aDvMwqqnsqMJ0heqLJKwP2w8m0FD+EBfJtYqy3AtM2bACOyVyw
         HfvQ==
MIME-Version: 1.0
X-Received: by 10.60.57.193 with SMTP id k1mr13247044oeq.66.1409108476927;
 Tue, 26 Aug 2014 20:01:16 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.202.226.147 with HTTP; Tue, 26 Aug 2014 20:01:16 -0700 (PDT)
In-Reply-To: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com>
References: <CACVCA=ewYV_Qb1ymqOCYOEHkHzis94r4u79gTHGahNL-x=yDCQ@mail.gmail.com>
Date: Tue, 26 Aug 2014 20:01:16 -0700
X-Google-Sender-Auth: QBDeypT4XSIBw-W_ryzp9UYJbGM
Message-ID: <CAFboF2xhMsnTwThqBthv-nVYTH-Ni6rYwD13_=8J+U=5ro6TbQ@mail.gmail.com>
Subject: Re: OutOfMemoryError when running sbt/sbt test
From: Anand Avati <avati@gluster.org>
To: jay vyas <jayunit100.apache@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a02d437a791050193a284
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a02d437a791050193a284
Content-Type: text/plain; charset=UTF-8

Hi Jay,
The recommended way to build spark from source is through the maven system.
You would want to follow the steps in
https://spark.apache.org/docs/latest/building-with-maven.html to set the
MAVEN_OPTS to prevent OOM build errors.

Thanks


On Tue, Aug 26, 2014 at 5:49 PM, jay vyas <jayunit100.apache@gmail.com>
wrote:

> Hi spark.
>
> I've been trying to build spark, but I've been getting lots of oome
> exceptions.
>
> https://gist.github.com/jayunit100/d424b6b825ce8517d68c
>
> For the most part, they are of the form:
>
> java.lang.OutOfMemoryError: unable to create new native thread
>
> I've attempted to hard code the "get_mem_opts" function, which is in the
> sbt-launch-lib.bash file, to
> have various very high parameter sizes (i.e. -Xms5g") with high
> MaxPermSize, etc... and to no avail.
>
> Any thoughts on this would be appreciated.
>
> I know of others having the same problem as well.
>
> Thanks!
>
> --
> jay vyas
>

--089e013a02d437a791050193a284--

From dev-return-9096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 03:11:52 2014
Return-Path: <dev-return-9096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55F391186A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 03:11:52 +0000 (UTC)
Received: (qmail 38439 invoked by uid 500); 27 Aug 2014 03:11:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38394 invoked by uid 500); 27 Aug 2014 03:11:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38383 invoked by uid 99); 27 Aug 2014 03:11:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:11:50 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:11:25 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1XMTdr-0000Jj-Mp
	for dev@spark.incubator.apache.org; Tue, 26 Aug 2014 20:11:23 -0700
Date: Tue, 26 Aug 2014 20:11:23 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409109083674-8061.post@n3.nabble.com>
In-Reply-To: <CAOhmDzcwzHV76hm+L+X0TmofNvZ3tkivR2B1fdz4WQkM-RKDoA@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com> <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com> <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com> <etPan.53fcd029.74b0dc51.104@joshs-mbp> <CAOhmDzcwt7J9dhqea_sP5wuug_2nXCJfsDnyXpE=1s8byLD24g@mail.gmail.com> <etPan.53fcd49b.2ae8944a.104@joshs-mbp> <CAOhmDzcwzHV76hm+L+X0TmofNvZ3tkivR2B1fdz4WQkM-RKDoA@mail.gmail.com>
Subject: Re: Handling stale PRs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Nicholas Chammas wrote
> Dunno how many committers Discourse has, but it looks like they've managed
> their PRs well. I hope we can do as well in this regard as they have.

Discourse developers appear to  eat their own dog food
<https://meta.discourse.org>  .
Improved collaboration and a shared vision might be a reason for their
success.




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-stale-PRs-tp8015p8061.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 03:17:35 2014
Return-Path: <dev-return-9097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41FA411883
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 03:17:35 +0000 (UTC)
Received: (qmail 43252 invoked by uid 500); 27 Aug 2014 03:17:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43152 invoked by uid 500); 27 Aug 2014 03:17:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43135 invoked by uid 99); 27 Aug 2014 03:17:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:17:34 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 03:17:08 +0000
Received: by mail-ig0-f174.google.com with SMTP id c1so5585572igq.7
        for <dev@spark.apache.org>; Tue, 26 Aug 2014 20:17:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=D9jpVNagy7QODrppG2PxEOP/SOOVwQXP/WS/e8hUrjI=;
        b=cRmStTiY0/iAQ41DKh3ZR4qYEfGKP9thPr9Hf09rAxqYtAaZjX8J32PY0U6w8OKgno
         Q76VXhxuoRLdWl1u3xOeskKg7BWctVAnzhpYod3XJvOmV65gs46x2QFc6fidrQKTBOnU
         FRiESUlBdqE66PJcVsyAhGC3yvzfP/vvyHpoSQ0E/K60KXuQv4+mpkZi8MMAHh/tzlzz
         vf0EYD8eqPZD6p1Rv+EE2IemcxLHx7Q+2SFTmsgANEyKLP7n6cONFLIm0gt1CIqrRZTX
         xOWQS2U+yhmjrw5c9VVXQW22phBqQ4ef8AvD02g8PG5h0xiKnBTUVvEET6CaGhumwGTq
         dimg==
X-Received: by 10.50.137.104 with SMTP id qh8mr14391405igb.3.1409109427014;
 Tue, 26 Aug 2014 20:17:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.128.161 with HTTP; Tue, 26 Aug 2014 20:16:36 -0700 (PDT)
In-Reply-To: <etPan.53fd36d2.649bb77c.9a@mbp-3>
References: <CAOhmDzd_h_Q0FpTF0AVqQcS1Q7XLXVMDprmJ8k=XvPceKJAGSw@mail.gmail.com>
 <CAKx7Bf_x3qE8d1m2Kh_eHboxFW-6H79w=x_0frnKTBaoUXOJTQ@mail.gmail.com> <etPan.53fd36d2.649bb77c.9a@mbp-3>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 26 Aug 2014 20:16:36 -0700
Message-ID: <CAMwrk0kfY_d+hecWaDQVfNdxbE6spRLZe1itowaMML2wQvM_8g@mail.gmail.com>
Subject: Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3b8e6d8e6fb050193da1d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3b8e6d8e6fb050193da1d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes, this was an oversight on my part. I have opened a JIRA for this.
https://issues.apache.org/jira/browse/SPARK-3242

For the time being the workaround should be providing the version 1.0.2
explicitly as part of the script.

TD


On Tue, Aug 26, 2014 at 6:39 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> This shouldn't be a chicken-and-egg problem, since the script fetches the
> AMI from a known URL. Seems like an issue in publishing this release.
>
> On August 26, 2014 at 1:24:45 PM, Shivaram Venkataraman (
> shivaram@eecs.berkeley.edu) wrote:
>
> This is a chicken and egg problem in some sense. We can't change the ec2
> script till we have made the release and uploaded the binaries -- But onc=
e
> that is done, we can't update the script.
>
> I think the model we support so far is that you can launch the latest
> spark version from the master branch on github. I guess we can try to add
> something in the release process that updates the script but doesn't comm=
it
> it ? The release managers might be able to add more.
>
> Thanks
> Shivaram
>
>
> On Tue, Aug 26, 2014 at 1:16 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > I downloaded the source code release for 1.0.2 from here
> > <http://spark.apache.org/downloads.html> and launched an EC2 cluster
> using
> > spark-ec2.
> >
> > After the cluster finishes launching, I fire up the shell and check the
> > version:
> >
> > scala> sc.version
> > res1: String =3D 1.0.1
> >
> > The startup banner also shows the same thing. Hmm...
> >
> > So I dig around and find that the spark_ec2.py script has the default
> Spark
> > version set to 1.0.1.
> >
> > Derp.
> >
> > parser.add_option("-v", "--spark-version", default=3D"1.0.1",
> > help=3D"Version of Spark to use: 'X.Y.Z' or a specific git hash")
> >
> > Is there any way to fix the release? It=E2=80=99s a minor issue, but co=
uld be
> very
> > confusing. And how can we prevent this from happening again?
> >
> > Nick
> > =E2=80=8B
> >
>

--001a11c3b8e6d8e6fb050193da1d--

From dev-return-9098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 07:06:13 2014
Return-Path: <dev-return-9098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5637211DE2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 07:06:13 +0000 (UTC)
Received: (qmail 7309 invoked by uid 500); 27 Aug 2014 07:06:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7244 invoked by uid 500); 27 Aug 2014 07:06:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7228 invoked by uid 99); 27 Aug 2014 07:06:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 07:06:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhengbing.li@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 07:06:07 +0000
Received: from 172.24.2.119 (EHLO SZXEMA405-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BYR40579;
	Wed, 27 Aug 2014 14:58:57 +0800 (CST)
Received: from SZXEMA501-MBX.china.huawei.com ([169.254.1.108]) by
 SZXEMA405-HUB.china.huawei.com ([10.82.72.37]) with mapi id 14.03.0158.001;
 Wed, 27 Aug 2014 14:58:56 +0800
From: "Lizhengbing (bing, BIPA)" <zhengbing.li@huawei.com>
To: Xiangrui Meng <mengxr@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: reply: I want to contribute MLlib two quality measures(ARHR and HR)
 for top N recommendation system. Is this meaningful?
Thread-Topic: reply: I want to contribute MLlib two quality measures(ARHR
 and HR) for top N recommendation system. Is this meaningful?
Thread-Index: Ac/AROnrhXBuBmCRQL+DlI8Z9vb1kwAErxEAAFmqO1A=
Date: Wed, 27 Aug 2014 06:58:55 +0000
Message-ID: <49229E870391FC49BBBED818C268753D705A9D11@SZXEMA501-MBX.china.huawei.com>
References: <49229E870391FC49BBBED818C268753D705A644F@SZXEMA501-MBX.china.huawei.com>
 <CAJgQjQ9YQzHnxN+HEN4FGbQe=4Pgq1NkU4+oyodYE7e547FKaw@mail.gmail.com>
In-Reply-To: <CAJgQjQ9YQzHnxN+HEN4FGbQe=4Pgq1NkU4+oyodYE7e547FKaw@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.170.84]
Content-Type: multipart/alternative;
	boundary="_000_49229E870391FC49BBBED818C268753D705A9D11SZXEMA501MBXchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_49229E870391FC49BBBED818C268753D705A9D11SZXEMA501MBXchi_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_49229E870391FC49BBBED818C268753D705A9D11SZXEMA501MBXchi_--

From dev-return-9099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 13:05:18 2014
Return-Path: <dev-return-9099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A6F591183C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 13:05:18 +0000 (UTC)
Received: (qmail 4058 invoked by uid 500); 27 Aug 2014 13:05:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4013 invoked by uid 500); 27 Aug 2014 13:05:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4001 invoked by uid 99); 27 Aug 2014 13:05:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 13:05:16 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 13:04:51 +0000
Received: by mail-we0-f176.google.com with SMTP id q58so184588wes.35
        for <dev@spark.incubator.apache.org>; Wed, 27 Aug 2014 06:04:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0FCuvNVpzkFG+O0BH3GKDxh0QOeJY+C6aAlz7ypkKnE=;
        b=TSG4tjqBWK0QcH7z7BiS1NrRsJz2glsBr6BkMpBkFogK0T2vBkEmTKLTm7OGiqUBGR
         5YMo8QezxQPEm9YycOvSh4L30B/9W8slbN8V9uTQLhSwZeESNA9LLfKpXoIJo4v2pkDE
         HZorrDrLqKUQpgPFZeMtW6dnUYhFBruKcrxq/6gCQgyfxDGK/lxbyhixjt6/db4IL+I1
         +Yga8X4jIoC5uF8S1p2G1htJdMogdVtmamS0NIsLuygJgKzQJ+bI55bT7apuZbE2qaaj
         8x7Ep2OSp/9hL7jvICpmCeqMkcGS5m4jo+Ls3hZvfiSxIPjgGUXxOtN77xR4Q9NHka/a
         czSg==
MIME-Version: 1.0
X-Received: by 10.194.95.8 with SMTP id dg8mr19551495wjb.1.1409144689960; Wed,
 27 Aug 2014 06:04:49 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Wed, 27 Aug 2014 06:04:49 -0700 (PDT)
In-Reply-To: <1407936001588-7822.post@n3.nabble.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<1407936001588-7822.post@n3.nabble.com>
Date: Wed, 27 Aug 2014 09:04:49 -0400
Message-ID: <CADtDQQKAYFktZksg=L4PTbyZTEADoDeOF47BFzwbH1f_d0Jbxw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc0f56aeb1ea05019c10e4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc0f56aeb1ea05019c10e4
Content-Type: text/plain; charset=UTF-8

Hi Yu,

A standardized API has not been implemented yet.  I think it would be
better to implement the other clustering algorithms then extract a common
API.  Others may feel differently.  :)

Just a note, there was a pre-existing JIRA for hierarchical KMeans
SPARK-2429 <https://issues.apache.org/jira/browse/SPARK-2429> I filed.  I
added a comment about previous discussion on the mailing list, example code
provided by a Jeremy Freeman, and a couple of papers I found.

Feel free to take this over -- I've played with it but haven't had time to
finish it.  I'd be happy to review the resulting code and discuss
approaches with you.

RJ



On Wed, Aug 13, 2014 at 9:20 AM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
wrote:

> Hi all,
>
> I am also interested in specifying a common framework.
> And I am trying to implement a hierarchical k-means and a hierarchical
> clustering like single-link method with LSH.
> https://issues.apache.org/jira/browse/SPARK-2966
>
> If you have designed the standardized clustering algorithms API, please let
> me know.
>
>
> best,
> Yu Ishikawa
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7822.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bdc0f56aeb1ea05019c10e4--

From dev-return-9100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 16:19:15 2014
Return-Path: <dev-return-9100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3CB67110D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 16:19:15 +0000 (UTC)
Received: (qmail 27320 invoked by uid 500); 27 Aug 2014 16:19:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27254 invoked by uid 500); 27 Aug 2014 16:19:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27240 invoked by uid 99); 27 Aug 2014 16:19:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:19:14 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:19:09 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so490031qge.11
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 09:18:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=CvjV6txvmEQ5rAvf7KhhNR3pppcYCaxUse5t+K//Ez4=;
        b=t+gMCHhqa+amwv0NCUVObNkgWGzanA73nlx5kWAE4E2BJ9mB/OdUaZYoviPF7VIScf
         trbt434c0wCcpX3WYxDH5n7z8TkdOxl1uaTGqlZVQ8jdtP3BGTtJuAKzpYOWUfzVv75G
         vv7300lIjUBnU5HVhjbezWAwebh1Ev0vOhE4qG9I6QoONqshAmAE7DnGzAbjhjvrKVNU
         JhBdKyjovcQl/Kop1LZbbeMr1Z/WsPsGqfKkn+623XiZVLkqesVWR8YBCjQlDbPPj3T6
         xgG1jJliXQbdt07EHEP6v1sA4odgU+peWJT6tuyed8muXYmLAF1WacoxAIwgqLgak2jm
         wqnw==
X-Received: by 10.224.76.9 with SMTP id a9mr55043169qak.38.1409156328581;
        Wed, 27 Aug 2014 09:18:48 -0700 (PDT)
Received: from freemanj-wm1.hhmi.org (simcoe.janelia.org. [206.241.0.254])
        by mx.google.com with ESMTPSA id e64sm1242445qgd.37.2014.08.27.09.18.47
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 27 Aug 2014 09:18:47 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
Date: Wed, 27 Aug 2014 12:18:46 -0400
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
 dev@spark.incubator.apache.org
Message-Id: <897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com> <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com> <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com> <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com> <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com> <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com> <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com> <1404909567855.c2a8fd87@Nodemailer> <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com> <1405003674571.192c3983@Nodemailer> <1405643464811-7398.post@n3.nabble.com> <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com> <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
To: RJ Nowling <rnowling@gmail.com>
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hey RJ,

Sorry for the delay, I'd be happy to take a look at this if you can post =
the code!

I think splitting the largest cluster in each round is fairly common, =
but ideally it would be an option to do it one way or the other.

-- Jeremy

---------------------
jeremy freeman, phd
neuroscientist
@thefreemanlab

On Aug 12, 2014, at 2:20 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Hi all,
>=20
> I wanted to follow up.
>=20
> I have a prototype for an optimized version of hierarchical k-means.  =
I
> wanted to get some feedback on my apporach.
>=20
> Jeremy's implementation splits the largest cluster in each round.  Is =
it
> better to do it that way or to split each cluster in half?
>=20
> Are there are any open-source examples that are being widely used in
> production?
>=20
> Thanks!
>=20
>=20
>=20
> On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> =
wrote:
>=20
>> Nice to meet you, Jeremy!
>>=20
>> This is great!  Hierarchical clustering was next on my list --
>> currently trying to get my PR for MiniBatch KMeans accepted.
>>=20
>> If it's cool with you, I'll try converting your code to fit in with
>> the existing MLLib code as you suggest. I also need to review the
>> Decision Tree code (as suggested above) to see how much of that can =
be
>> reused.
>>=20
>> Maybe I can ask you to do a code review for me when I'm done?
>>=20
>>=20
>>=20
>>=20
>>=20
>> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
>> <freeman.jeremy@gmail.com> wrote:
>>> Hi all,
>>>=20
>>> Cool discussion! I agree that a more standardized API for =
clustering, and
>>> easy access to underlying routines, would be useful (we've also been
>>> discussing this when trying to develop streaming clustering =
algorithms,
>>> similar to https://github.com/apache/spark/pull/1361)
>>>=20
>>> For divisive, hierarchical clustering I implemented something awhile
>> back,
>>> here's a gist.
>>>=20
>>> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>>>=20
>>> It does bisecting k-means clustering (with k=3D2), with a recursive =
class
>> for
>>> keeping track of the tree. I also found this much better than
>> agglomerative
>>> methods (for the reasons Hector points out).
>>>=20
>>> This needs to be cleaned up, and can surely be optimized (esp. by
>> replacing
>>> the core KMeans step with existing MLLib code), but I can say I was
>> running
>>> it successfully on quite large data sets.
>>>=20
>>> RJ, depending on where you are in your progress, I'd be happy to =
help
>> work
>>> on this piece and / or have you use this as a jumping off point, if
>> useful.
>>>=20
>>> -- Jeremy
>>>=20
>>>=20
>>>=20
>>> --
>>> View this message in context:
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
>>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>=20
>>=20
>>=20
>> --
>> em rnowling@gmail.com
>> c 954.496.2314
>>=20
>=20
>=20
>=20
> --=20
> em rnowling@gmail.com
> c 954.496.2314


--Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820--

From dev-return-9101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 16:19:20 2014
Return-Path: <dev-return-9101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98519110D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 16:19:20 +0000 (UTC)
Received: (qmail 28508 invoked by uid 500); 27 Aug 2014 16:19:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28448 invoked by uid 500); 27 Aug 2014 16:19:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28432 invoked by uid 99); 27 Aug 2014 16:19:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:19:15 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.216.43 as permitted sender)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:18:49 +0000
Received: by mail-qa0-f43.google.com with SMTP id w8so428352qac.2
        for <dev@spark.incubator.apache.org>; Wed, 27 Aug 2014 09:18:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=CvjV6txvmEQ5rAvf7KhhNR3pppcYCaxUse5t+K//Ez4=;
        b=t+gMCHhqa+amwv0NCUVObNkgWGzanA73nlx5kWAE4E2BJ9mB/OdUaZYoviPF7VIScf
         trbt434c0wCcpX3WYxDH5n7z8TkdOxl1uaTGqlZVQ8jdtP3BGTtJuAKzpYOWUfzVv75G
         vv7300lIjUBnU5HVhjbezWAwebh1Ev0vOhE4qG9I6QoONqshAmAE7DnGzAbjhjvrKVNU
         JhBdKyjovcQl/Kop1LZbbeMr1Z/WsPsGqfKkn+623XiZVLkqesVWR8YBCjQlDbPPj3T6
         xgG1jJliXQbdt07EHEP6v1sA4odgU+peWJT6tuyed8muXYmLAF1WacoxAIwgqLgak2jm
         wqnw==
X-Received: by 10.224.76.9 with SMTP id a9mr55043169qak.38.1409156328581;
        Wed, 27 Aug 2014 09:18:48 -0700 (PDT)
Received: from freemanj-wm1.hhmi.org (simcoe.janelia.org. [206.241.0.254])
        by mx.google.com with ESMTPSA id e64sm1242445qgd.37.2014.08.27.09.18.47
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 27 Aug 2014 09:18:47 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
Date: Wed, 27 Aug 2014 12:18:46 -0400
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
 dev@spark.incubator.apache.org
Message-Id: <897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com> <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com> <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com> <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com> <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com> <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com> <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com> <1404909567855.c2a8fd87@Nodemailer> <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com> <1405003674571.192c3983@Nodemailer> <1405643464811-7398.post@n3.nabble.com> <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com> <CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
To: RJ Nowling <rnowling@gmail.com>
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hey RJ,

Sorry for the delay, I'd be happy to take a look at this if you can post =
the code!

I think splitting the largest cluster in each round is fairly common, =
but ideally it would be an option to do it one way or the other.

-- Jeremy

---------------------
jeremy freeman, phd
neuroscientist
@thefreemanlab

On Aug 12, 2014, at 2:20 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Hi all,
>=20
> I wanted to follow up.
>=20
> I have a prototype for an optimized version of hierarchical k-means.  =
I
> wanted to get some feedback on my apporach.
>=20
> Jeremy's implementation splits the largest cluster in each round.  Is =
it
> better to do it that way or to split each cluster in half?
>=20
> Are there are any open-source examples that are being widely used in
> production?
>=20
> Thanks!
>=20
>=20
>=20
> On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> =
wrote:
>=20
>> Nice to meet you, Jeremy!
>>=20
>> This is great!  Hierarchical clustering was next on my list --
>> currently trying to get my PR for MiniBatch KMeans accepted.
>>=20
>> If it's cool with you, I'll try converting your code to fit in with
>> the existing MLLib code as you suggest. I also need to review the
>> Decision Tree code (as suggested above) to see how much of that can =
be
>> reused.
>>=20
>> Maybe I can ask you to do a code review for me when I'm done?
>>=20
>>=20
>>=20
>>=20
>>=20
>> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
>> <freeman.jeremy@gmail.com> wrote:
>>> Hi all,
>>>=20
>>> Cool discussion! I agree that a more standardized API for =
clustering, and
>>> easy access to underlying routines, would be useful (we've also been
>>> discussing this when trying to develop streaming clustering =
algorithms,
>>> similar to https://github.com/apache/spark/pull/1361)
>>>=20
>>> For divisive, hierarchical clustering I implemented something awhile
>> back,
>>> here's a gist.
>>>=20
>>> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>>>=20
>>> It does bisecting k-means clustering (with k=3D2), with a recursive =
class
>> for
>>> keeping track of the tree. I also found this much better than
>> agglomerative
>>> methods (for the reasons Hector points out).
>>>=20
>>> This needs to be cleaned up, and can surely be optimized (esp. by
>> replacing
>>> the core KMeans step with existing MLLib code), but I can say I was
>> running
>>> it successfully on quite large data sets.
>>>=20
>>> RJ, depending on where you are in your progress, I'd be happy to =
help
>> work
>>> on this piece and / or have you use this as a jumping off point, if
>> useful.
>>>=20
>>> -- Jeremy
>>>=20
>>>=20
>>>=20
>>> --
>>> View this message in context:
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
>>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>=20
>>=20
>>=20
>> --
>> em rnowling@gmail.com
>> c 954.496.2314
>>=20
>=20
>=20
>=20
> --=20
> em rnowling@gmail.com
> c 954.496.2314


--Apple-Mail=_F37593B3-A2BC-4CE2-B390-92DBD6D54820--

From dev-return-9102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 16:27:33 2014
Return-Path: <dev-return-9102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A17611134
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 16:27:33 +0000 (UTC)
Received: (qmail 48287 invoked by uid 500); 27 Aug 2014 16:27:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48228 invoked by uid 500); 27 Aug 2014 16:27:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48203 invoked by uid 99); 27 Aug 2014 16:27:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:27:29 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:27:24 +0000
Received: by mail-we0-f171.google.com with SMTP id p10so467353wes.30
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 09:27:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gciEMOH2OYjM9R1k/GyVFZL0XVOBb0UU5IeIpYpDd/g=;
        b=L21sInawQsJvMl+afa8ZtLUsTVIqyii1Q0b64fJCwhGw3In8BzOmZSXh0vgFZJTN48
         jzpjkAw4oyc/hGUOEAvoUlE3Aogb9GrV7H6JgJLkzZKW9SXvhlR7d7OrKi3d9gMlYKrO
         bNC1MrqyaRxNarrA4h9BUcK60aPIOd6E398niLtzIJHobd9YUEPQhwAutEgQGNqPhaF4
         mcwHLXE91GgBHyDoVJo4Tf+w4Vvz7JMBLBe7ZH2YprhtRQ1X+lbcUvdvD4c/RhQY/lbm
         /Y7sdaRZ2m0B4KbehKnkZw3iSwZrxrFYzOm/+mcqcC1pfe5ySFCq4q6Q+8uvIkWp+6Yh
         nt5Q==
MIME-Version: 1.0
X-Received: by 10.194.78.100 with SMTP id a4mr4681887wjx.106.1409156821056;
 Wed, 27 Aug 2014 09:27:01 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Wed, 27 Aug 2014 09:27:00 -0700 (PDT)
In-Reply-To: <897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
	<CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
	<CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
	<897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
Date: Wed, 27 Aug 2014 12:27:00 -0400
Message-ID: <CADtDQQ+9-ms4anUnA4xtEL-0Jw2vdyJGoKyOgCXxffXoDxOFHw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: Jeremy Freeman <freeman.jeremy@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd0582c0848305019ee33e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0582c0848305019ee33e
Content-Type: text/plain; charset=UTF-8

Thanks, Jeremy.  I'm abandoning my initial approach, and I'll work on
optimizing your example (so it doesn't do the breeze-vector conversions
every time KMeans is called).  I need to finish a few other projects first,
though, so it may be a couple weeks.

In the mean time, Yu also created a JIRA for a hierarchical KMeans
implementation.  I pointed him to your example and a couple papers I found.

If you or Yu beat me to getting an implementation in, I'd be happy to
review it.  :)


On Wed, Aug 27, 2014 at 12:18 PM, Jeremy Freeman <freeman.jeremy@gmail.com>
wrote:

> Hey RJ,
>
> Sorry for the delay, I'd be happy to take a look at this if you can post
> the code!
>
> I think splitting the largest cluster in each round is fairly common, but
> ideally it would be an option to do it one way or the other.
>
> -- Jeremy
>
> ---------------------
> jeremy freeman, phd
> neuroscientist
> @thefreemanlab
>
> On Aug 12, 2014, at 2:20 PM, RJ Nowling <rnowling@gmail.com> wrote:
>
> Hi all,
>
> I wanted to follow up.
>
> I have a prototype for an optimized version of hierarchical k-means.  I
> wanted to get some feedback on my apporach.
>
> Jeremy's implementation splits the largest cluster in each round.  Is it
> better to do it that way or to split each cluster in half?
>
> Are there are any open-source examples that are being widely used in
> production?
>
> Thanks!
>
>
>
> On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> wrote:
>
> Nice to meet you, Jeremy!
>
> This is great!  Hierarchical clustering was next on my list --
> currently trying to get my PR for MiniBatch KMeans accepted.
>
> If it's cool with you, I'll try converting your code to fit in with
> the existing MLLib code as you suggest. I also need to review the
> Decision Tree code (as suggested above) to see how much of that can be
> reused.
>
> Maybe I can ask you to do a code review for me when I'm done?
>
>
>
>
>
> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
> <freeman.jeremy@gmail.com> wrote:
>
> Hi all,
>
> Cool discussion! I agree that a more standardized API for clustering, and
> easy access to underlying routines, would be useful (we've also been
> discussing this when trying to develop streaming clustering algorithms,
> similar to https://github.com/apache/spark/pull/1361)
>
> For divisive, hierarchical clustering I implemented something awhile
>
> back,
>
> here's a gist.
>
> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>
> It does bisecting k-means clustering (with k=2), with a recursive class
>
> for
>
> keeping track of the tree. I also found this much better than
>
> agglomerative
>
> methods (for the reasons Hector points out).
>
> This needs to be cleaned up, and can surely be optimized (esp. by
>
> replacing
>
> the core KMeans step with existing MLLib code), but I can say I was
>
> running
>
> it successfully on quite large data sets.
>
> RJ, depending on where you are in your progress, I'd be happy to help
>
> work
>
> on this piece and / or have you use this as a jumping off point, if
>
> useful.
>
>
> -- Jeremy
>
>
>
> --
> View this message in context:
>
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
>
> Sent from the Apache Spark Developers List mailing list archive at
>
> Nabble.com.
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>
>
>


-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bfd0582c0848305019ee33e--

From dev-return-9103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 16:27:38 2014
Return-Path: <dev-return-9103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0281B1113C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 16:27:38 +0000 (UTC)
Received: (qmail 48382 invoked by uid 500); 27 Aug 2014 16:27:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48231 invoked by uid 500); 27 Aug 2014 16:27:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48214 invoked by uid 99); 27 Aug 2014 16:27:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:27:29 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 16:27:03 +0000
Received: by mail-we0-f177.google.com with SMTP id w62so470287wes.8
        for <dev@spark.incubator.apache.org>; Wed, 27 Aug 2014 09:27:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gciEMOH2OYjM9R1k/GyVFZL0XVOBb0UU5IeIpYpDd/g=;
        b=L21sInawQsJvMl+afa8ZtLUsTVIqyii1Q0b64fJCwhGw3In8BzOmZSXh0vgFZJTN48
         jzpjkAw4oyc/hGUOEAvoUlE3Aogb9GrV7H6JgJLkzZKW9SXvhlR7d7OrKi3d9gMlYKrO
         bNC1MrqyaRxNarrA4h9BUcK60aPIOd6E398niLtzIJHobd9YUEPQhwAutEgQGNqPhaF4
         mcwHLXE91GgBHyDoVJo4Tf+w4Vvz7JMBLBe7ZH2YprhtRQ1X+lbcUvdvD4c/RhQY/lbm
         /Y7sdaRZ2m0B4KbehKnkZw3iSwZrxrFYzOm/+mcqcC1pfe5ySFCq4q6Q+8uvIkWp+6Yh
         nt5Q==
MIME-Version: 1.0
X-Received: by 10.194.78.100 with SMTP id a4mr4681887wjx.106.1409156821056;
 Wed, 27 Aug 2014 09:27:01 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Wed, 27 Aug 2014 09:27:00 -0700 (PDT)
In-Reply-To: <897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
	<CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
	<CADtDQQJF0BxcqvpCPnPkmouKcejfr0ZnwejocxdGXXGcoz7kbg@mail.gmail.com>
	<897B425C-7225-4128-86E1-8273CA12F813@gmail.com>
Date: Wed, 27 Aug 2014 12:27:00 -0400
Message-ID: <CADtDQQ+9-ms4anUnA4xtEL-0Jw2vdyJGoKyOgCXxffXoDxOFHw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: Jeremy Freeman <freeman.jeremy@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd0582c0848305019ee33e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0582c0848305019ee33e
Content-Type: text/plain; charset=UTF-8

Thanks, Jeremy.  I'm abandoning my initial approach, and I'll work on
optimizing your example (so it doesn't do the breeze-vector conversions
every time KMeans is called).  I need to finish a few other projects first,
though, so it may be a couple weeks.

In the mean time, Yu also created a JIRA for a hierarchical KMeans
implementation.  I pointed him to your example and a couple papers I found.

If you or Yu beat me to getting an implementation in, I'd be happy to
review it.  :)


On Wed, Aug 27, 2014 at 12:18 PM, Jeremy Freeman <freeman.jeremy@gmail.com>
wrote:

> Hey RJ,
>
> Sorry for the delay, I'd be happy to take a look at this if you can post
> the code!
>
> I think splitting the largest cluster in each round is fairly common, but
> ideally it would be an option to do it one way or the other.
>
> -- Jeremy
>
> ---------------------
> jeremy freeman, phd
> neuroscientist
> @thefreemanlab
>
> On Aug 12, 2014, at 2:20 PM, RJ Nowling <rnowling@gmail.com> wrote:
>
> Hi all,
>
> I wanted to follow up.
>
> I have a prototype for an optimized version of hierarchical k-means.  I
> wanted to get some feedback on my apporach.
>
> Jeremy's implementation splits the largest cluster in each round.  Is it
> better to do it that way or to split each cluster in half?
>
> Are there are any open-source examples that are being widely used in
> production?
>
> Thanks!
>
>
>
> On Fri, Jul 18, 2014 at 8:05 AM, RJ Nowling <rnowling@gmail.com> wrote:
>
> Nice to meet you, Jeremy!
>
> This is great!  Hierarchical clustering was next on my list --
> currently trying to get my PR for MiniBatch KMeans accepted.
>
> If it's cool with you, I'll try converting your code to fit in with
> the existing MLLib code as you suggest. I also need to review the
> Decision Tree code (as suggested above) to see how much of that can be
> reused.
>
> Maybe I can ask you to do a code review for me when I'm done?
>
>
>
>
>
> On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
> <freeman.jeremy@gmail.com> wrote:
>
> Hi all,
>
> Cool discussion! I agree that a more standardized API for clustering, and
> easy access to underlying routines, would be useful (we've also been
> discussing this when trying to develop streaming clustering algorithms,
> similar to https://github.com/apache/spark/pull/1361)
>
> For divisive, hierarchical clustering I implemented something awhile
>
> back,
>
> here's a gist.
>
> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>
> It does bisecting k-means clustering (with k=2), with a recursive class
>
> for
>
> keeping track of the tree. I also found this much better than
>
> agglomerative
>
> methods (for the reasons Hector points out).
>
> This needs to be cleaned up, and can surely be optimized (esp. by
>
> replacing
>
> the core KMeans step with existing MLLib code), but I can say I was
>
> running
>
> it successfully on quite large data sets.
>
> RJ, depending on where you are in your progress, I'd be happy to help
>
> work
>
> on this piece and / or have you use this as a jumping off point, if
>
> useful.
>
>
> -- Jeremy
>
>
>
> --
> View this message in context:
>
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
>
> Sent from the Apache Spark Developers List mailing list archive at
>
> Nabble.com.
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>
>
>


-- 
em rnowling@gmail.com
c 954.496.2314

--047d7bfd0582c0848305019ee33e--

From dev-return-9104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 20:57:42 2014
Return-Path: <dev-return-9104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D039E11E2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 20:57:42 +0000 (UTC)
Received: (qmail 20204 invoked by uid 500); 27 Aug 2014 20:57:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20142 invoked by uid 500); 27 Aug 2014 20:57:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20131 invoked by uid 99); 27 Aug 2014 20:57:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 20:57:41 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 20:57:37 +0000
Received: by mail-qa0-f46.google.com with SMTP id w8so56020qac.33
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 13:57:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=0GRbUhunTMdEe6QhW9qBnhiUUyp69dz9pegRfRnI/qI=;
        b=YiPZfadoaZE4VvTH+gTVzXCm1xcpnjHzEkzxgINsYnEu7ELwMVt1xlLL1HLvB5zOg3
         Q8gCXjZ03tfoe23osgx1goBJoitlWT9wYDNXfIc9kB7V7IEJjgDS7v/LgFUxNuOgOyZf
         yma8mc0POExnck8vBl35nNq6IOX40SZa/mgEQFncBo0YyOyE4/h+OkN/+lBA9vEVdZGR
         0Ti8v6xJuvmpjvPv5Rub/uKPm+J4XGdLpuSvlwMC3V9jtcxX0Q5huNmnKY4VSKLjVHSP
         Og1vMKTqu7Xf1dF6wrnphl527f5/w5svH2S/vAAf7Gl/0bzBGjUnxNhuRekbKk/Q52D5
         pbDw==
X-Gm-Message-State: ALoCoQlf2qFO3a02SrInboANtPWFLU2qDY+PJHpaULSZ7KBUlKGuVnAntOlAKszLOEDX9jLd9LbB
X-Received: by 10.140.47.129 with SMTP id m1mr57259109qga.95.1409173035637;
 Wed, 27 Aug 2014 13:57:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.41.34 with HTTP; Wed, 27 Aug 2014 13:56:55 -0700 (PDT)
In-Reply-To: <OFA76C4FB7.1BCB92D8-ON65257D3C.00325F6E-65257D3C.00336AEC@in.ibm.com>
References: <OFA76C4FB7.1BCB92D8-ON65257D3C.00325F6E-65257D3C.00336AEC@in.ibm.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 27 Aug 2014 13:56:55 -0700
Message-ID: <CAPh_B=aO-JEHER58yJbf_ArRsd7E1xAf_DjqoK=Tz1tDjqGw3w@mail.gmail.com>
Subject: Re: Adding support for a new object store
To: Rajendran Appavu <apprajen@in.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>
Content-Type: multipart/alternative; boundary=001a11c16b7e3772c80501a2aa04
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16b7e3772c80501a2aa04
Content-Type: text/plain; charset=UTF-8

Hi Rajendran,

I'm assuming you have some concept of schema and you are intending to
integrate with SchemaRDD instead of normal RDDs.

More responses inline below.


On Fri, Aug 22, 2014 at 2:21 AM, Rajendran Appavu <apprajen@in.ibm.com>
wrote:

>
>  I am new to Spark source code and looking to see if i can add push-down
> support of spark filters to the storage (in my
>  case an object store). I am willing to consider how this can be
> generically done for any store that we might want to
>  integrate with spark. I am looking to know the areas that I should look
> into to provide support for a new data store in
>  this context. Following below are some of the questions I have to start
> with:
>
>  1. Do we need to create a new RDD class for the new store that we want to
> support? From Spark Context, we create an RDD
>  and the operations on data including the filter are performed through the
> RDD methods.
>

You can create a new RDD type for a new storage system, and you can create
a new table scan operator in sql to read.


>  2. When we specify the code for filter task in the RDD.filter() method,
> how does it get communicated to the Executor on
>  the data node? Does the Executor need to compile this code on the fly and
> execute it? or how does it work? ( I have
>  looked at the code for sometime, but not yet got to figuring this out, so
> i am looking for some pointers that can help me
>  come a little up-to-speed in this part of the code)
>

Right now the best way to do this is to hack the sql strategies, which does
some predicate pushdown into table scan:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala

We are in the process of proposing an API that allows external data stores
to hook into the planner. Expect a design proposal in early/mid Sept.

Once that is in place, you wouldn't need to hack the planner anymore. It is
a good idea to start prototyping by hacking the planner, and migrate to the
planner hook API once that is ready.


>
>  3. How long the Executor holds the memory? and how does it decide when to
> release the memory/cache?
>

Executors by default actually don't hold any data in memory. Spark requires
explicit caching of data, i.e. it's only when rdd.cache() is called then
will Spark executors put the content of that RDD in-memory. The executor
has a thing called BlockManager that does eviction based on LRU.



>
>  Thank you in advance.
>
>
>
>
>
> Regards,
> Rajendran.
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c16b7e3772c80501a2aa04--

From dev-return-9105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 21:07:43 2014
Return-Path: <dev-return-9105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43D8411F2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 21:07:43 +0000 (UTC)
Received: (qmail 52210 invoked by uid 500); 27 Aug 2014 21:07:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52143 invoked by uid 500); 27 Aug 2014 21:07:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52115 invoked by uid 99); 27 Aug 2014 21:07:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:07:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:07:37 +0000
Received: by mail-qa0-f50.google.com with SMTP id cm18so63953qab.37
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 14:07:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=X0l6HS/5vvBlI2xdnCgnvK5Rhqheig3BJgFy/2gSY7w=;
        b=zOpvnvrB1XolDU+4CpQiRgmEoOg6j07mokyZmwI+jcRKZduQbPBxo4gzTOLRuTMOsS
         CU9O+tpWWmyW3azMng21Vo0fkV+TlGx2cesjXMs4BwMpnKBFibBLz3vSmPEDE0qxkGeT
         FwgNsgNrLvQ9hrygEeJ4hVzqKLcCHcRwYi12VFY4FNBr9ar0zPykeb+cZ18WZpnHUVlp
         loGkvf2dEuijA8jmlzohe9JSK6BswTJbDgxr8FM8tTlqpR5XC42XmlY/eWJR9fKMZmGW
         +uxeVgLtop1Q/Cl+OB+E8k1yQ1tDTko7tBUCn9/pikmMdOFcfAHavOwYrwnNaCLvBVa2
         1QHg==
X-Received: by 10.140.88.41 with SMTP id s38mr57030490qgd.73.1409173633693;
 Wed, 27 Aug 2014 14:07:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Wed, 27 Aug 2014 14:06:52 -0700 (PDT)
In-Reply-To: <CAAbaoBDmdBRPew5n3JWKp9=YT4pOS58fqZ7ceD73Eomu0iBJVA@mail.gmail.com>
References: <CAAbaoBDmdBRPew5n3JWKp9=YT4pOS58fqZ7ceD73Eomu0iBJVA@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Wed, 27 Aug 2014 14:06:52 -0700
Message-ID: <CAA_qdLpKdvvWNmKtNG2GJGax0omPJLRUgr3kKu2NMqF9W2RcQA@mail.gmail.com>
Subject: Re: RDD replication in Spark
To: rapelly kartheek <kartheek.mbms@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c13e7edcfe640501a2cd65
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13e7edcfe640501a2cd65
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

You may start from here
<https://github.com/apache/spark/blob/4fa2fda88fc7beebb579ba808e400113b5125=
33b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L706-L7=
12>
.
=E2=80=8B


On Mon, Aug 25, 2014 at 9:05 PM, rapelly kartheek <kartheek.mbms@gmail.com>
wrote:

> Hi,
>
> I've exercised multiple options available for persist() including  RDD
> replication. I have gone thru the classes that involve in caching/storing
> the RDDS at different levels. StorageLevel class plays a pivotal role by
> recording whether to use memory or disk or to replicate the RDD on multip=
le
> nodes.
> The class LocationIterator iterates over the preferred machines one by
> one  for
> each partition that is replicated. I got a rough idea of CoalescedRDD.
> Please correct me if I am wrong.
>
> But I am looking for the code that chooses the resources to replicate the
> RDDs. Can someone please tell me how replication takes place and how do w=
e
> choose the resources for replication. I just want to know as to where
> should I look into to understand how the replication happens.
>
>
>
> Thank you so much!!!
>
> regards
>
> -Karthik
>

--001a11c13e7edcfe640501a2cd65--

From dev-return-9106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 21:12:09 2014
Return-Path: <dev-return-9106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEE5C11F54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 21:12:09 +0000 (UTC)
Received: (qmail 63800 invoked by uid 500); 27 Aug 2014 21:12:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63730 invoked by uid 500); 27 Aug 2014 21:12:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63692 invoked by uid 99); 27 Aug 2014 21:12:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:12:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:12:04 +0000
Received: by mail-wg0-f45.google.com with SMTP id k14so12354wgh.16
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 14:11:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=qcU3wgdRYed6AUrxIkB7Nzb/ilfCYs4xp36MlgyTz2s=;
        b=JaaVPAXUiLKO6jkl22LblPxAHaX/TqjDX5p46tG5Aopg4mdVVxTuKb3etKo+LlgTx8
         Rl45ORlSElFCTkTc04FyTP/UusUlMhm5kzww+FmTuzrg1iJ3FPOjc4MkcpaBdWEsPI5g
         lJltyX0IoIpfYDAKVNTwodG6M9SrlthdBKJ/QGxJWur0wM2ewhlNE+ckFC6+HmICkJb7
         pvqrNhLcpZhQj3d9S06bCZlc6djC8ImJjxMASXeKCrZBiQXvJnWIhdJamj/vVyo8nYXm
         dp4TE2WoZez6TveA+HOzUkT+LV2+VNkuU6WsCRF+LrR/ZrLwEGjyGnfiXI9nMwH/fxXu
         dFVw==
X-Received: by 10.194.59.18 with SMTP id v18mr41318259wjq.64.1409173901862;
 Wed, 27 Aug 2014 14:11:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Wed, 27 Aug 2014 14:11:01 -0700 (PDT)
In-Reply-To: <etPan.53fcd029.74b0dc51.104@joshs-mbp>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com> <etPan.53fcd029.74b0dc51.104@joshs-mbp>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 27 Aug 2014 17:11:01 -0400
Message-ID: <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Josh Rosen <rosenville@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>, 
	Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary=047d7bacb0a6d8ee1d0501a2dd4e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb0a6d8ee1d0501a2dd4e
Content-Type: text/plain; charset=UTF-8

On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote:

> Last weekend, I started hacking on a Google App Engine app for helping
> with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
>

BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
issue, perhaps?

Nick

--047d7bacb0a6d8ee1d0501a2dd4e--

From dev-return-9107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 21:18:27 2014
Return-Path: <dev-return-9107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4872311F81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 21:18:27 +0000 (UTC)
Received: (qmail 76772 invoked by uid 500); 27 Aug 2014 21:18:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76700 invoked by uid 500); 27 Aug 2014 21:18:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76676 invoked by uid 99); 27 Aug 2014 21:18:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:18:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:18:21 +0000
Received: by mail-we0-f172.google.com with SMTP id q59so62350wes.3
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 14:18:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=qejothP6jIgvNZYrofa8ytj/vj3lXguVDPVLVVbjCVY=;
        b=pIf9294kb9aCAxXhpsNsGpmCd4ziIDZrrML9DnI4+s2DzfTDuvcKxdWG0FNefVMCG7
         6CCDjYRWw+3aDiEJHDTK4jSbW/BsVxjbMERiikq/yLYBjh5N20lHLDMXURS8qXD9oyXg
         h/PCuCMcbwJ4LleuRXG/n1GWfoK7AR1Xru9LML3vsd8QbQz59e8I5P0u+Dbq9UIk/2nZ
         M2ZnQsSjGs8P+Ny3mh1LJ+6qqhL4UMbb/8xWd5dJJh7RR6Pda4epF0+kJ4SAMFSxureR
         RrH/m052Zj4tP9T96BEh/JlwrEMjZqkJgfcwcnXaDGPEwZhnzzbnQw0q8GRpgHzpnoG9
         lACg==
MIME-Version: 1.0
X-Received: by 10.194.170.227 with SMTP id ap3mr40486667wjc.30.1409174279939;
 Wed, 27 Aug 2014 14:17:59 -0700 (PDT)
Received: by 10.194.14.137 with HTTP; Wed, 27 Aug 2014 14:17:59 -0700 (PDT)
Date: Wed, 27 Aug 2014 17:17:59 -0400
Message-ID: <CADtDQQLUXhH3DSiFpnUkXTtf0fw7-ARSe2j8+ux1uHKhLqGOXw@mail.gmail.com>
Subject: [GraphX] JIRA / PR to fix breakage in GraphGenerator.logNormalGraph
 in PR #720
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122e92261ef640501a2f431
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122e92261ef640501a2f431
Content-Type: text/plain; charset=UTF-8

Hi all,

 PR #720 <https://github.com/apache/spark/pull/720> made multiple changes
to GraphGenerator.logNormalGraph including:

   - Replacing the call to functions for generating random vertices and
   edges with in-line implementations with different equations. Based on
   reading the Pregel paper, I believe the in-line functions are incorrect.
   - Hard-coding of RNG seeds so that method now generates the same graph
   for a given number of vertices, edges, mu, and sigma -- user is not able to
   override seed or specify that seed should be randomly generated.
   - Backwards-incompatible change to logNormalGraph signature with
   introduction of new required parameter.
   - Failed to update scala docs and programming guide for API changes
   - Added a Synthetic Benchmark in the examples.

I submitted JIRA SPARK-3263
<https://issues.apache.org/jira/browse/SPARK-3263> and PR #2168
<https://github.com/apache/spark/pull/2168> to revert some of these changes
and fix usage of the RNGs:

   - Removes the in-line calls and calls original vertex / edge generation
   functions again
   - Adds an optional seed parameter for deterministic behavior (when
   desired)
   - Keeps the number of partitions parameter that was added.
   - Keeps compatibility with the synthetic benchmark example
   - Maintains backwards-compatible API

 I would appreciate feedback and people taking a look.  :)

Thanks!
RJ

-- 
em rnowling@gmail.com
c 954.496.2314

--089e0122e92261ef640501a2f431--

From dev-return-9108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 21:22:57 2014
Return-Path: <dev-return-9108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4382D11FA5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 21:22:57 +0000 (UTC)
Received: (qmail 84554 invoked by uid 500); 27 Aug 2014 21:22:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84482 invoked by uid 500); 27 Aug 2014 21:22:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84469 invoked by uid 99); 27 Aug 2014 21:22:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:22:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:22:30 +0000
Received: by mail-qc0-f176.google.com with SMTP id m20so37350qcx.7
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 14:22:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=B1F3QnK4d655kups3nBbPaydXAsvwl++WyaEgyVVGQQ=;
        b=ZwzdLCwo9wGwIMSuIYsJcS3svqLcSHdO1tEOgI0m/9bEv5vpsugGnChzYoURgWKQ6+
         Zmx+JJx4XeTXR0ROuHa6P3a3LL3TdCOiR/VT1CiwoCaXSDZthQYaNiEjRQRGOFP7Z4eU
         tToeZhgPl2RSH3g4mmhjmaDdMQyOMEvJJpbRGM++uJjcx3BaSqLWLlQyzoG/vNKMYZlH
         BdpfTeVyHIjgvuM4zCO2oJf17PG+kAmjCBzOrSMM9mjxe0XlTYunnyNEyHRyO7YchW9K
         61KPhUirvzl+C8taAS3gs0q5ymQc92K7c67ad02duwFjtQVlGMr3hP8+8zys1FDSo82k
         zICw==
X-Gm-Message-State: ALoCoQnbdgJLocYZf0q1RrPyRZRR70yU9qfkqk/oNKEaPFHBGn4K7gqZjQ4bm22B1ypNQRamAEps
X-Received: by 10.140.30.136 with SMTP id d8mr28433177qgd.55.1409174547979;
 Wed, 27 Aug 2014 14:22:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.41.34 with HTTP; Wed, 27 Aug 2014 14:22:07 -0700 (PDT)
In-Reply-To: <CAPh_B=aO-JEHER58yJbf_ArRsd7E1xAf_DjqoK=Tz1tDjqGw3w@mail.gmail.com>
References: <OFA76C4FB7.1BCB92D8-ON65257D3C.00325F6E-65257D3C.00336AEC@in.ibm.com>
 <CAPh_B=aO-JEHER58yJbf_ArRsd7E1xAf_DjqoK=Tz1tDjqGw3w@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 27 Aug 2014 14:22:07 -0700
Message-ID: <CAPh_B=aXpwG8S4Q4Qx_NL4EKLfbG8cVhfkdzYc3bGNwPVm3XKA@mail.gmail.com>
Subject: Re: Adding support for a new object store
To: Rajendran Appavu <apprajen@in.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>
Content-Type: multipart/alternative; boundary=001a113a921a5c1c170501a304e6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a921a5c1c170501a304e6
Content-Type: text/plain; charset=UTF-8

Linking to the JIRA tracking APIs to hook into the planner:
https://issues.apache.org/jira/browse/SPARK-3248




On Wed, Aug 27, 2014 at 1:56 PM, Reynold Xin <rxin@databricks.com> wrote:

> Hi Rajendran,
>
> I'm assuming you have some concept of schema and you are intending to
> integrate with SchemaRDD instead of normal RDDs.
>
> More responses inline below.
>
>
> On Fri, Aug 22, 2014 at 2:21 AM, Rajendran Appavu <apprajen@in.ibm.com>
> wrote:
>
>>
>>  I am new to Spark source code and looking to see if i can add push-down
>> support of spark filters to the storage (in my
>>  case an object store). I am willing to consider how this can be
>> generically done for any store that we might want to
>>  integrate with spark. I am looking to know the areas that I should look
>> into to provide support for a new data store in
>>  this context. Following below are some of the questions I have to start
>> with:
>>
>>  1. Do we need to create a new RDD class for the new store that we want
>> to support? From Spark Context, we create an RDD
>>  and the operations on data including the filter are performed through
>> the RDD methods.
>>
>
> You can create a new RDD type for a new storage system, and you can create
> a new table scan operator in sql to read.
>
>
>>  2. When we specify the code for filter task in the RDD.filter() method,
>> how does it get communicated to the Executor on
>>  the data node? Does the Executor need to compile this code on the fly
>> and execute it? or how does it work? ( I have
>>  looked at the code for sometime, but not yet got to figuring this out,
>> so i am looking for some pointers that can help me
>>  come a little up-to-speed in this part of the code)
>>
>
> Right now the best way to do this is to hack the sql strategies, which
> does some predicate pushdown into table scan:
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
>
> We are in the process of proposing an API that allows external data stores
> to hook into the planner. Expect a design proposal in early/mid Sept.
>
> Once that is in place, you wouldn't need to hack the planner anymore. It
> is a good idea to start prototyping by hacking the planner, and migrate to
> the planner hook API once that is ready.
>
>
>>
>>  3. How long the Executor holds the memory? and how does it decide when
>> to release the memory/cache?
>>
>
> Executors by default actually don't hold any data in memory. Spark
> requires explicit caching of data, i.e. it's only when rdd.cache() is
> called then will Spark executors put the content of that RDD in-memory. The
> executor has a thing called BlockManager that does eviction based on LRU.
>
>
>
>>
>>  Thank you in advance.
>>
>>
>>
>>
>>
>> Regards,
>> Rajendran.
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a113a921a5c1c170501a304e6--

From dev-return-9109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 21:37:12 2014
Return-Path: <dev-return-9109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 819D111033
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 21:37:12 +0000 (UTC)
Received: (qmail 10807 invoked by uid 500); 27 Aug 2014 21:37:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10730 invoked by uid 500); 27 Aug 2014 21:37:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10708 invoked by uid 99); 27 Aug 2014 21:37:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:37:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nravi@cloudera.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 21:37:07 +0000
Received: by mail-ie0-f175.google.com with SMTP id y20so51501ier.34
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 14:36:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Pg7pw/VYtTt7rmvNqvshYAjxF9Q1XmrPSYOlsTzMk+A=;
        b=mrh5CZqbxzLNwdJ6yKhJuQOeTXBXVn4dZAD0tXqyNUfvdB9gg/rQfZsA7WS8urv3bJ
         IqP3zjJiRHePFvjOmLBlchb4mDgiTUNdVZr9vkFVWwU+W2BsGHKzWelGiMzxIqTXBvkZ
         SZJBFxjxJlKnnueqQ5RuWgg5n79DF4yftVOalTn+3qfG5+d9zcvQWWmXKuX5bGGs/Ost
         Z0a0FSASoXb2N5pJgb9CELZFALCyd2E4WpuYJbvLtXaILSgQlsZh6qB3tpq2o8PQtu3t
         HaVmh19428GrTaSxAldtQtOYDYzOU8l4cl4jRWh25PydzPPnpK8R266QGC+NI2OpUNy+
         Gtcw==
X-Gm-Message-State: ALoCoQmMGPusUcL+j1MmhDxv72QsMAAHCUk+xNR9jgy8LMXW5besOuFmQD9jn5WLFZqMkT15aW3w
X-Received: by 10.42.199.197 with SMTP id et5mr670923icb.56.1409175406681;
 Wed, 27 Aug 2014 14:36:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.10.170 with HTTP; Wed, 27 Aug 2014 14:36:26 -0700 (PDT)
In-Reply-To: <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp> <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Wed, 27 Aug 2014 14:36:26 -0700
Message-ID: <CACfA1zVTkdwzq--4h+N7hG9DRMBE5if5UtAph_j1wVN2Atv8Lw@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Josh Rosen <rosenville@gmail.com>, Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>, 
	Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary=20cf30223caf8aaf8c0501a337fa
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30223caf8aaf8c0501a337fa
Content-Type: text/plain; charset=UTF-8

Wonder if it would make sense to introduce a notion of 'Reviewers' as an
intermediate tier to help distribute the load? While anyone can review and
comment on an open PR, reviewers would be able to say aye or nay subject to
confirmation by a committer?

Thanks,
Nishkam


On Wed, Aug 27, 2014 at 2:11 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote:
>
> > Last weekend, I started hacking on a Google App Engine app for helping
> > with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
> >
>
> BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
> issue, perhaps?
>
> Nick
>

--20cf30223caf8aaf8c0501a337fa--

From dev-return-9110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 22:31:45 2014
Return-Path: <dev-return-9110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49D04111FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 22:31:45 +0000 (UTC)
Received: (qmail 28171 invoked by uid 500); 27 Aug 2014 22:31:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28096 invoked by uid 500); 27 Aug 2014 22:31:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28084 invoked by uid 99); 27 Aug 2014 22:31:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:31:44 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:31:18 +0000
Received: by mail-qa0-f45.google.com with SMTP id f12so179140qad.18
        for <dev@spark.incubator.apache.org>; Wed, 27 Aug 2014 15:31:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=J9vXMtL2Z8xJ6OD9WROnqWv7N0EEJGlsNlaO+/LEaho=;
        b=hxgx7wlo0jx4jcezgOOTMwh1V7EdqX84jnVgDG2gz5dZQyyu74hp23JDVLKeoqZg24
         nS4RQnkDYsmUq5fNtKOTjnsnNO1YdZywfxvHqw4wvAuRmbVh2EErLMwiRy4Wotmwkw7B
         JFHPKi8WVrNhoDl7WEC5E09Rb6qw9ffibQt45mT01Xm22xuBZHI+7s70la7xN2Jf0xy7
         X3xuI3eK52ATu7Z28f7OuTRtlangIakYmGruBE4eEqkxUbRvUt4y4m5189eIBatIgOyY
         9tXdRh3+lOHZPW7znCwf2Jv/Ah91c1zHLBBRNdAFMoMgemmMc8YO9suhIGL9NPARKcmd
         ckXA==
X-Received: by 10.224.137.6 with SMTP id u6mr62051120qat.91.1409178677381;
 Wed, 27 Aug 2014 15:31:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Wed, 27 Aug 2014 15:30:57 -0700 (PDT)
In-Reply-To: <1409066984856-8039.post@n3.nabble.com>
References: <1409064875366-8035.post@n3.nabble.com> <1409066984856-8039.post@n3.nabble.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Wed, 27 Aug 2014 15:30:57 -0700
Message-ID: <CAA_qdLp9FWi-ot6PzV-HWkP9bsZfbjHE48C7kjo15XLtCgKEyA@mail.gmail.com>
Subject: Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...
To: chutium <teng.qiu@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2a3ec7da1590501a3fa56
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a3ec7da1590501a3fa56
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I believe in your case, the =E2=80=9Cmagic=E2=80=9D happens in TableReader.=
fillObject
<https://github.com/apache/spark/blob/4fa2fda88fc7beebb579ba808e400113b5125=
33b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L706-L7=
12>.
Here we unwrap the field value according to the object inspector of that
field. It seems that somehow a FloatObjectInspector is specified for the
total_price field. I don=E2=80=99t think CSVSerde is responsible for this, =
since it
sets all field object inspectors to javaStringObjectInspector (here
<https://github.com/ogrodnek/csv-serde/blob/f315c1ae4b21a8288eb939e7c10f3b2=
9c1a854ef/src/main/java/com/bizo/hive/serde/csv/CSVSerde.java#L59-L61>
).

Which version of Spark SQL are you using? If you are using a snapshot
version, please provide the exact Git commit hash. Thanks!
=E2=80=8B


On Tue, Aug 26, 2014 at 8:29 AM, chutium <teng.qiu@gmail.com> wrote:

> oops, i tried on a managed table, column types will not be changed
>
> so it is mostly due to the serde lib CSVSerDe
> (
> https://github.com/ogrodnek/csv-serde/blob/master/src/main/java/com/bizo/=
hive/serde/csv/CSVSerde.java#L123
> )
> or maybe CSVReader from opencsv?...
>
> but if the columns are defined as string, no matter what type returned fr=
om
> custom SerDe or CSVReader, they should be cast to string at the end right=
?
>
> why do not use the schema from hive metadata directly?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/HiveContext-sch=
emaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-=
and-surpri-tp8035p8039.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c2a3ec7da1590501a3fa56--

From dev-return-9111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 22:39:59 2014
Return-Path: <dev-return-9111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A94651125C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 22:39:59 +0000 (UTC)
Received: (qmail 55147 invoked by uid 500); 27 Aug 2014 22:39:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55075 invoked by uid 500); 27 Aug 2014 22:39:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55063 invoked by uid 99); 27 Aug 2014 22:39:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:39:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:39:32 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so10704oag.17
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 15:39:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5sLc99/WG5qIK/VjD6IfZ5e7gBKNRtftBOhSrii6LcY=;
        b=ilYery3sioMw8+372mw2Uu3wFMGe4ex2zkEVb+PlI3FLGq97T0kECQgUvi4kEc7Pt1
         xqLJ3aV0HUu+ytDeqoElAJL3A2EUArqKMuiyHxLPq3Vm1POzcHNbYGwKBss3hMztwAmU
         nKqdr0UTwbt/VA9Lai4EP5HlxLn0crf1E53CngU2ayOm3Tgr78tDsVd/mwWJnLXxban3
         2IQOdJyFXsWUoGLAdDg/v+CdkemEYDyPqkA8SI3fKvTj+cjvzNPUw7tWNO7TkrIKH80w
         MC0J4dpUeRIHOpFsvPLYsVwg0HUDZwV8t7XM7dk8E8aVDzIsgXx2ODfGZeplfFvg5OY9
         yXRg==
MIME-Version: 1.0
X-Received: by 10.182.66.16 with SMTP id b16mr37211473obt.49.1409179171493;
 Wed, 27 Aug 2014 15:39:31 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Wed, 27 Aug 2014 15:39:31 -0700 (PDT)
In-Reply-To: <CACfA1zVTkdwzq--4h+N7hG9DRMBE5if5UtAph_j1wVN2Atv8Lw@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
	<etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
	<CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
	<CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
	<etPan.53fcd029.74b0dc51.104@joshs-mbp>
	<CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
	<CACfA1zVTkdwzq--4h+N7hG9DRMBE5if5UtAph_j1wVN2Atv8Lw@mail.gmail.com>
Date: Wed, 27 Aug 2014 15:39:31 -0700
Message-ID: <CABPQxsv+pYpYootPFyOT6-G=LCgWvAKCB5gLOtRSk=k5j7EZZw@mail.gmail.com>
Subject: Re: Handling stale PRs
From: Patrick Wendell <pwendell@gmail.com>
To: Nishkam Ravi <nravi@cloudera.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Josh Rosen <rosenville@gmail.com>, 
	dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nishkam,

To some extent we already have this process - many community members
help review patches and some earn a reputation where committer's will
take an LGTM from them seriously. I'd be interested in seeing if any
other projects recognize people who do this.

- Patrick

On Wed, Aug 27, 2014 at 2:36 PM, Nishkam Ravi <nravi@cloudera.com> wrote:
> Wonder if it would make sense to introduce a notion of 'Reviewers' as an
> intermediate tier to help distribute the load? While anyone can review and
> comment on an open PR, reviewers would be able to say aye or nay subject to
> confirmation by a committer?
>
> Thanks,
> Nishkam
>
>
> On Wed, Aug 27, 2014 at 2:11 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
>>
>> On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote:
>>
>> > Last weekend, I started hacking on a Google App Engine app for helping
>> > with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
>> >
>>
>> BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
>> issue, perhaps?
>>
>> Nick
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 22:48:39 2014
Return-Path: <dev-return-9112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CA38112D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 22:48:39 +0000 (UTC)
Received: (qmail 78229 invoked by uid 500); 27 Aug 2014 22:48:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78154 invoked by uid 500); 27 Aug 2014 22:48:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78137 invoked by uid 99); 27 Aug 2014 22:48:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:48:38 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 22:48:12 +0000
Received: by mail-pa0-f44.google.com with SMTP id rd3so61632pab.31
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 15:48:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=DZnIN2h3+j/ctsuz4ztkpMjQSToD298KcG9wER3+wic=;
        b=riZcPqhd6UxXQQvdKHNXEuwohiIWV/VvCtIh7To2Qj0vhVy8QJ7NW4AtkhAuFOvTj8
         +V4MO9E424ywVua4uX5P/Ua7zMWFPDuGNuaa3xPiNpoMKglsZAyeBRlOC+UmwWv3a/kJ
         ivgWe7J2xq8BGP2TXj/2NGONGxIQRzyFKqrgaFVVmpu4bgxBRnOOMRzs5Nak4t1luvAV
         fb4ERK4YEXkgS7yyaPr30VywhFP+itq5jL0LE1v4RPVU0ZRHX3Y4itF3qlUrMzQr6OqF
         l9QTFHi4ey/0YTkyIhat72TVAehKf1c68JOTjrRpsnBTMKf69KlkIOTNXu/qUBKFYP6A
         Pk5w==
X-Received: by 10.68.221.166 with SMTP id qf6mr235084pbc.12.1409179689994;
        Wed, 27 Aug 2014 15:48:09 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id xr10sm5695943pab.35.2014.08.27.15.48.08
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Wed, 27 Aug 2014 15:48:09 -0700 (PDT)
Date: Wed, 27 Aug 2014 15:48:07 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>, 
 Patrick Wendell <pwendell@gmail.com>
Message-ID: <etPan.53fe6027.6b68079a.104@joshs-mbp>
In-Reply-To: <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
 <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp>
 <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
Subject: Re: Handling stale PRs
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53fe6027_4e6afb66_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--53fe6027_4e6afb66_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I have a very simple dashboard running at=C2=A0http://spark-prs.appspot.c=
om/. =C2=A0Currently, this mirrors the functionality of Patrick=E2=80=99s=
 github-shim, but it should be very easy to extend with other features.

The source is at=C2=A0https://github.com/databricks/spark-pr-dashboard=C2=
=A0(pull requests and issues welcome=21)

On August 27, 2014 at 2:11:41 PM, Nicholas Chammas (nicholas.chammas=40gm=
ail.com) wrote:

On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville=40gmail.com> wrot=
e:
Last weekend, I started hacking on a Google App Engine app for helping wi=
th pull request review (screenshot:=C2=A0http://i.imgur.com/wwpZKYZ.png).=


BTW Josh, how can we stay up-to-date on your work on this tool=3F A JIRA =
issue, perhaps=3F

Nick
--53fe6027_4e6afb66_104--


From dev-return-9113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 23:14:25 2014
Return-Path: <dev-return-9113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D661E113FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 23:14:25 +0000 (UTC)
Received: (qmail 54343 invoked by uid 500); 27 Aug 2014 23:14:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54281 invoked by uid 500); 27 Aug 2014 23:14:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54269 invoked by uid 99); 27 Aug 2014 23:14:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:14:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:14:20 +0000
Received: by mail-la0-f53.google.com with SMTP id s18so214297lam.40
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 16:13:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=597MlnvtpLN8cKZ/uPt37UUmmjvZxzi8RppJZ/qeWL0=;
        b=lKnzf8tjhaOYO1RwDkRUxLlXL180I4UD7WRuz57tVfdmmEuafK455MJhBeEKxI9mZ8
         O2MxZR4T6nOSnITGFmJgI/ldM1DD8m4mqHebYlz5k9VOREgtKCmMzEjMjDGqk/MH3BRD
         BIdmHUDk+lkUhKuxEP/KnCeWKJb8pPm67Hodz+vqWmx2SL7DvbjRmKPgjqM/xNS/0nFr
         QqLAZ5QmrJlUG1C+n7kyI9nydmmkYQMjsLIb9nSHgXtyz3q0PdmrqTL6mPgy3rn0TNuW
         vktz5hw1o3iC1k81F19xiNY6d0rxXk2Q5Z7hXYaPR4T2be8+ePCH5QdYymDVWjmslUds
         pOPg==
X-Gm-Message-State: ALoCoQmejl7EZblr7Frec3uhW9lbure9yVVcjOFQ56HQRw/+rnLXhQ/6sFRPobiW/+lqClnKnVP7
X-Received: by 10.112.105.168 with SMTP id gn8mr57210lbb.77.1409181238919;
 Wed, 27 Aug 2014 16:13:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Wed, 27 Aug 2014 16:13:38 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 27 Aug 2014 16:13:38 -0700
Message-ID: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
Subject: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev@spark.apache.org, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11345e222b971e0501a49333
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11345e222b971e0501a49333
Content-Type: text/plain; charset=UTF-8

tomorrow morning i will be upgrading jenkins to the latest/greatest (1.577).

at 730am, i will put jenkins in to a quiet period, so no new builds will be
accepted.  once any running builds are finished, i will be taking jenkins
down for the upgrade.

depending on what and how many jobs are running, i'm expecting this to
take, at most, an hour.

i'll send out an update tomorrow morning right before i begin, and will
send out updates and an all-clear once we're up and running again.

1.577 release notes:
http://jenkins-ci.org/changelog

please let me know if there are any questions/concerns.  thanks in advance!

shane

--001a11345e222b971e0501a49333--

From dev-return-9114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 23:34:23 2014
Return-Path: <dev-return-9114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11EA01148A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 23:34:23 +0000 (UTC)
Received: (qmail 88152 invoked by uid 500); 27 Aug 2014 23:34:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88082 invoked by uid 500); 27 Aug 2014 23:34:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88069 invoked by uid 99); 27 Aug 2014 23:34:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:34:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:34:18 +0000
Received: by mail-wg0-f50.google.com with SMTP id x12so5514wgg.21
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 16:33:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=vATB8roj6HUYEl664F4NhdoTaAAA4GMfQWwP6eS4cQY=;
        b=CPqj/Ek2va8/j4uhQ7yDHF+Lp72ruhCQEkEpqwwfLzw5PM84KYcK3fx5x441KHzjt2
         vk+t0p41C+h+XuTVdPNH6lCtJSWWieyVn7SkhnRdbPdvkhP1Qf+BQ0GD8po9OpYky+dZ
         14lXcsXnhILXtxAjjSBFVF3avSoiqDuM3XdACJoKCs9188x39EB/U4dXcqV/ucqag4Zv
         i/5T4qs99yk/PsaNyziPG9yXHSwUVdPV9yDTFyY7kMkZvrLVPClqzJFczUudKxJZlqks
         oHqg9JSOwbhfCkRDNsmgV+EKt4K5bV8/thd1iAgPqGceUByyT4nQmyH7ZpQPxPXM2RmO
         35Ug==
X-Received: by 10.180.211.233 with SMTP id nf9mr31932985wic.33.1409182437119;
 Wed, 27 Aug 2014 16:33:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Wed, 27 Aug 2014 16:33:17 -0700 (PDT)
In-Reply-To: <etPan.53fe6027.6b68079a.104@joshs-mbp>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp> <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
 <etPan.53fe6027.6b68079a.104@joshs-mbp>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 27 Aug 2014 19:33:17 -0400
Message-ID: <CAOhmDzfQ37zS+aTy=o14m3ZbP-0H1_SzOiVBFyAcphX9bFO3hQ@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Josh Rosen <rosenville@gmail.com>
Cc: dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c338d0969fd30501a4da17
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c338d0969fd30501a4da17
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alright! That was quick. :)


On Wed, Aug 27, 2014 at 6:48 PM, Josh Rosen <rosenville@gmail.com> wrote:

> I have a very simple dashboard running at http://spark-prs.appspot.com/.
>  Currently, this mirrors the functionality of Patrick=E2=80=99s github-sh=
im, but it
> should be very easy to extend with other features.
>
> The source is at https://github.com/databricks/spark-pr-dashboard (pull
> requests and issues welcome!)
>
> On August 27, 2014 at 2:11:41 PM, Nicholas Chammas (
> nicholas.chammas@gmail.com) wrote:
>
>  On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com> wrote=
:
>
>>  Last weekend, I started hacking on a Google App Engine app for helping
>> with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png).
>>
>
> BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
> issue, perhaps?
>
> Nick
>
>

--001a11c338d0969fd30501a4da17--

From dev-return-9115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Aug 27 23:47:44 2014
Return-Path: <dev-return-9115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 14AFF114FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Aug 2014 23:47:44 +0000 (UTC)
Received: (qmail 20765 invoked by uid 500); 27 Aug 2014 23:47:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20692 invoked by uid 500); 27 Aug 2014 23:47:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20679 invoked by uid 99); 27 Aug 2014 23:47:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:47:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Aug 2014 23:47:39 +0000
Received: by mail-wg0-f41.google.com with SMTP id l18so16829wgh.0
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 16:47:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=GD4OSvDlC4plbyFW7j713oh14KDEnB7CndoMA9iyLqo=;
        b=uJhqOs7aTp7MLt87jle5un7NpBf3Hwq9hboJO5H5rgr5VLtlzlSHCskJStySZZps+G
         +j8+9D/zjnJA91I5IAVKrzJ62bO6sJ3ZrNOCv31mMCw08hawNVRTBtnYaUZY/5PMRQja
         fZXpyhx3a+q9/k3vJQm6bCv1oJahfWE+E3ZYJa+JN0x+BuohrnXuotJluWpArIQlgoKs
         8QHJlK4XsmsV9b4v4acTF39Rij9ShBkgVLi7weORIixbn2sh0n2cQEnblCp/1D0tuLTY
         MyjOxtJUOv6CtyfyeYp/HoYqnnGH5FsW+Kg87zgsmMW1/P8PAt1UafWM7jJlJKqC7eYc
         1NRg==
X-Received: by 10.194.71.210 with SMTP id x18mr581856wju.6.1409183238048; Wed,
 27 Aug 2014 16:47:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Wed, 27 Aug 2014 16:46:38 -0700 (PDT)
In-Reply-To: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 27 Aug 2014 19:46:38 -0400
Message-ID: <CAOhmDzfTT8n5bMv9aNzEZn1kLGJ4bhfxMTVxCknjNaadY6J1KA@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: shane knapp <sknapp@berkeley.edu>
Cc: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=047d7bfd077653d2b90501a50a32
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd077653d2b90501a50a32
Content-Type: text/plain; charset=UTF-8

Looks like we're currently at 1.568 so we should be getting a nice slew of
UI tweaks and bug fixes. Neat!


On Wed, Aug 27, 2014 at 7:13 PM, shane knapp <sknapp@berkeley.edu> wrote:

> tomorrow morning i will be upgrading jenkins to the latest/greatest
> (1.577).
>
> at 730am, i will put jenkins in to a quiet period, so no new builds will be
> accepted.  once any running builds are finished, i will be taking jenkins
> down for the upgrade.
>
> depending on what and how many jobs are running, i'm expecting this to
> take, at most, an hour.
>
> i'll send out an update tomorrow morning right before i begin, and will
> send out updates and an all-clear once we're up and running again.
>
> 1.577 release notes:
> http://jenkins-ci.org/changelog
>
> please let me know if there are any questions/concerns.  thanks in advance!
>
> shane
>

--047d7bfd077653d2b90501a50a32--

From dev-return-9116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 01:27:19 2014
Return-Path: <dev-return-9116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AC534117A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 01:27:19 +0000 (UTC)
Received: (qmail 87007 invoked by uid 500); 28 Aug 2014 01:27:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86944 invoked by uid 500); 28 Aug 2014 01:27:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86926 invoked by uid 99); 28 Aug 2014 01:27:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 01:27:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nravi@cloudera.com designates 209.85.223.179 as permitted sender)
Received: from [209.85.223.179] (HELO mail-ie0-f179.google.com) (209.85.223.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 01:27:14 +0000
Received: by mail-ie0-f179.google.com with SMTP id tr6so130827ieb.10
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 18:26:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=vH5tmwwMIF/08DptpoFDlfsCoj6vgggf67y8f6a5V+0=;
        b=i1p5RdF9Qg69avD6hlJ8FYVz4IICe66vIPqEdmwijEF5H5hr+a4dy/n8GssA9Lucw5
         w1xnVk282Ce8NCEVwTAW67O7YYQDU/7npwoy7/GaZOP3rCGc/oBdowciklAokEQVog34
         CpfifDexvuRafNLJsmaCPwiBeTF3vSypomUW7lWl/8PB5nnkNJX82T1IiHjKkZiCKOMI
         TrlJnPohJ/scCLogmTkJl2qP5ELgpqBLcQrRt1bEgRwdblib9v009dNCHl4d+uujIfyi
         XwdmR5Nx4MwLEQu9ZJvCKyDRSgFamF0pibj20siO/sGHcDgK3MaSxJLnHjaiFqjBJ/B8
         brzQ==
X-Gm-Message-State: ALoCoQkIQTw+nWpisEbEzK9evdI6Dzo/Q79cvMxoFcejo7VIUoLlV/4nF9z6Do3Bky97PW+Axk75
X-Received: by 10.50.134.106 with SMTP id pj10mr34432700igb.25.1409189214108;
 Wed, 27 Aug 2014 18:26:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.10.170 with HTTP; Wed, 27 Aug 2014 18:26:34 -0700 (PDT)
In-Reply-To: <CABPQxsv+pYpYootPFyOT6-G=LCgWvAKCB5gLOtRSk=k5j7EZZw@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzd66wgjjtbzjT8qGsO0+-efDWGoSsx3E5GrZ8wKm9fG2A@mail.gmail.com>
 <etPan.53fcd029.74b0dc51.104@joshs-mbp> <CAOhmDzck+j+i0dXxWmg3RBJv1cBM-ubWsGUxs=FqfKhWEHqwBQ@mail.gmail.com>
 <CACfA1zVTkdwzq--4h+N7hG9DRMBE5if5UtAph_j1wVN2Atv8Lw@mail.gmail.com> <CABPQxsv+pYpYootPFyOT6-G=LCgWvAKCB5gLOtRSk=k5j7EZZw@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Wed, 27 Aug 2014 18:26:34 -0700
Message-ID: <CACfA1zWQf_0DL2wJY9QL1PSVH8DgKPNarNtku8+XHo72L44XDw@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Josh Rosen <rosenville@gmail.com>, 
	dev <dev@spark.apache.org>, Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary=047d7b3a955287a97a0501a66edd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a955287a97a0501a66edd
Content-Type: text/plain; charset=UTF-8

I see. Yeah, it would be interesting to know if any other project has
considered formalizing this notion. It may also enable assignment of
reviews (potentially automated using Josh's system) and maybe anonymity as
well? On the downside, it isn't easily implemented and probably doesn't
come without certain undesired side-effects.

Thanks,
Nishkam


On Wed, Aug 27, 2014 at 3:39 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Nishkam,
>
> To some extent we already have this process - many community members
> help review patches and some earn a reputation where committer's will
> take an LGTM from them seriously. I'd be interested in seeing if any
> other projects recognize people who do this.
>
> - Patrick
>
> On Wed, Aug 27, 2014 at 2:36 PM, Nishkam Ravi <nravi@cloudera.com> wrote:
> > Wonder if it would make sense to introduce a notion of 'Reviewers' as an
> > intermediate tier to help distribute the load? While anyone can review
> and
> > comment on an open PR, reviewers would be able to say aye or nay subject
> to
> > confirmation by a committer?
> >
> > Thanks,
> > Nishkam
> >
> >
> > On Wed, Aug 27, 2014 at 2:11 PM, Nicholas Chammas
> > <nicholas.chammas@gmail.com> wrote:
> >>
> >> On Tue, Aug 26, 2014 at 2:21 PM, Josh Rosen <rosenville@gmail.com>
> wrote:
> >>
> >> > Last weekend, I started hacking on a Google App Engine app for helping
> >> > with pull request review (screenshot: http://i.imgur.com/wwpZKYZ.png
> ).
> >> >
> >>
> >> BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
> >> issue, perhaps?
> >>
> >> Nick
> >
> >
>

--047d7b3a955287a97a0501a66edd--

From dev-return-9117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 05:04:13 2014
Return-Path: <dev-return-9117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 401A811CE7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 05:04:13 +0000 (UTC)
Received: (qmail 37358 invoked by uid 500); 28 Aug 2014 05:04:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37305 invoked by uid 500); 28 Aug 2014 05:04:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36226 invoked by uid 99); 28 Aug 2014 05:04:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:04:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:03:46 +0000
Received: by mail-wg0-f44.google.com with SMTP id m15so201384wgh.27
        for <multiple recipients>; Wed, 27 Aug 2014 22:03:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=uEn2NeLdv7+KcaIdUPf/AaPYNpPmA+k93sDTX5hSlkI=;
        b=YwKBDFLraiCifLICnjMNWUJf7PJxiK0929NFicwrDIGLFW6Ldee36bf2F+axg1QX8Q
         beJRJD9DJ78dnLBAjdochbovGFxmfRXsm+UvlLzhK0ND1n7Nywr7rk5lojF8VTS76PFP
         vQUtb9JMdHfyGATfjwhUzuBH1stCnORboubVDcBxCmPEDMJsOqGXSDTmLSUo3ep8irU1
         fAc3v8O+ubwZtE5qv7lw6g/XK9E4YM2VP01r+21M080qvNIuSfpswhYGcXDn1adNWoA4
         38y0WLUmBqaV/QTgFOVQE8Lmq1ex6XYfLMdytHSWS5mT0nL6YMSUZdEqteVuj/7Mkjvq
         ANmw==
X-Received: by 10.180.95.66 with SMTP id di2mr3081687wib.60.1409202225864;
 Wed, 27 Aug 2014 22:03:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.153.10 with HTTP; Wed, 27 Aug 2014 22:03:25 -0700 (PDT)
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Thu, 28 Aug 2014 10:33:25 +0530
Message-ID: <CAAqHKj7vXh1XPTsOg5yZVU2fgLzSkyvXuecfgn8x9oa-Z7vurg@mail.gmail.com>
Subject: Update on Pig on Spark initiative
To: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0418252416e9550501a9760b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0418252416e9550501a9760b
Content-Type: text/plain; charset=UTF-8

Hi,
We have migrated Pig functionality on top of Spark passing 100% e2e for
success cases in pig test suite. That means UDF, Joins & other
functionality is working quite nicely. We are in the process of merging
with Apache Pig trunk(something that should happen over the next 2 weeks).
Meanwhile if you are interested in giving it a go, you can try it at
https://github.com/sigmoidanalytics/spork
This contains all the major changes but may not have all the patches
required for 100% e2e, if you are trying it out let me know any issues you
face

Whole bunch of folks contributed on this

Julien Le Dem (Twitter),  Praveen R (Sigmoid Analytics), Akhil Das (Sigmoid
Analytics), Bill Graham (Twitter), Dmitriy Ryaboy (Twitter), Kamal Banga
(Sigmoid Analytics), Anish Haldiya (Sigmoid Analytics),  Aniket Mokashi
 (Google), Greg Owen (DataBricks), Amit Kumar Behera (Sigmoid Analytics),
Mahesh Kalakoti (Sigmoid Analytics)

Not to mention Spark & Pig communities.

Regards
Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>

--f46d0418252416e9550501a9760b--

From dev-return-9118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 05:12:09 2014
Return-Path: <dev-return-9118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F358711D0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 05:12:08 +0000 (UTC)
Received: (qmail 47237 invoked by uid 500); 28 Aug 2014 05:12:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47189 invoked by uid 500); 28 Aug 2014 05:12:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46067 invoked by uid 99); 28 Aug 2014 05:12:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:12:07 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:11:41 +0000
Received: by mail-pa0-f52.google.com with SMTP id eu11so993162pac.39
        for <multiple recipients>; Wed, 27 Aug 2014 22:11:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=D5BUFCQ+wzocZJojJNNdUC+L4zjejKX5dpgrb/w0tyg=;
        b=biWJ8flxGIMXNXfG+NbKm4Vpzq00A2pJlrU5eB0h75fXOiX2sDYoiLAouDO75tGYri
         kuRn0PnxhK6/OfIsEn011KAIxyjGyyUoixKhOqwS0EtVawrLCVM+0N1M0sx5JcDG1OpZ
         Xv132wG69RMbH1b90hUeMX8nI+3/l+Kp9yvFf7YnY6iPwA6qIK9/r4Wr5r3M0OUXs6S+
         7A3xrRTO54U9aNZ6lntsGBgE8Lkc2Blr9jLke1WX9Vbslee6qpGIuFQgtOdPFRnMN/ww
         3HeEkwRoEmW92k12xLpkOX+1jjbHs1RSpK+e1Hb7TkOFLd5I0PTMmI1r0TWVEXesz0Za
         LuPw==
X-Received: by 10.66.141.77 with SMTP id rm13mr2006016pab.91.1409202699268;
        Wed, 27 Aug 2014 22:11:39 -0700 (PDT)
Received: from ip-192-168-1-106.us-west-2.compute.internal (ec2-54-201-102-27.us-west-2.compute.amazonaws.com. [54.201.102.27])
        by mx.google.com with ESMTPSA id do6sm3513182pdb.39.2014.08.27.22.11.37
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 27 Aug 2014 22:11:38 -0700 (PDT)
Date: Wed, 27 Aug 2014 22:11:36 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: "=?utf-8?Q?user=40spark.apache.org?=" <user@spark.apache.org>, dev
 <dev@spark.apache.org>, Mayur Rustagi <mayur.rustagi@gmail.com>
Message-ID: <etPan.53feba08.628c895d.470a@ip-192-168-1-106.us-west-2.compute.internal>
In-Reply-To: <CAAqHKj7vXh1XPTsOg5yZVU2fgLzSkyvXuecfgn8x9oa-Z7vurg@mail.gmail.com>
References: <CAAqHKj7vXh1XPTsOg5yZVU2fgLzSkyvXuecfgn8x9oa-Z7vurg@mail.gmail.com>
Subject: Re: Update on Pig on Spark initiative
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53feba08_333ab105_470a"
X-Virus-Checked: Checked by ClamAV on apache.org

--53feba08_333ab105_470a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Awesome to hear this, Mayur=21 Thanks for putting this together.

Matei

On August 27, 2014 at 10:04:12 PM, Mayur Rustagi (mayur.rustagi=40gmail.c=
om) wrote:

Hi,
We have migrated Pig functionality on top of Spark passing 100% e2e for s=
uccess cases in pig test suite. That means UD=46, Joins & other functiona=
lity is working quite nicely. We are in the process of merging with Apach=
e Pig trunk(something that should happen over the next 2 weeks).=C2=A0
Meanwhile if you are interested in giving it a go, you can try it at=C2=A0=
https://github.com/sigmoidanalytics/spork
This contains all the major changes but may not have all the patches requ=
ired for 100% e2e, if you are trying it out let me know any issues you fa=
ce

Whole bunch of folks contributed on this=C2=A0

Julien Le Dem (Twitter), =C2=A0Praveen R (Sigmoid Analytics), Akhil Das (=
Sigmoid Analytics), Bill Graham (Twitter), Dmitriy Ryaboy (Twitter), Kama=
l Banga (Sigmoid Analytics), Anish Haldiya (Sigmoid Analytics), =C2=A0Ani=
ket Mokashi =C2=A0(Google), Greg Owen (DataBricks),=C2=A0Amit Kumar Beher=
a (Sigmoid Analytics), Mahesh Kalakoti (Sigmoid Analytics)

Not to mention Spark & Pig communities.=C2=A0

Regards
Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
=40mayur=5Frustagi


--53feba08_333ab105_470a--


From dev-return-9119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 05:21:27 2014
Return-Path: <dev-return-9119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1DAA311D19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 05:21:27 +0000 (UTC)
Received: (qmail 54998 invoked by uid 500); 28 Aug 2014 05:21:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54930 invoked by uid 500); 28 Aug 2014 05:21:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54917 invoked by uid 99); 28 Aug 2014 05:21:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:21:25 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of cloud0fan@outlook.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 05:21:00 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <cloud0fan@outlook.com>)
	id 1XMs8p-0001rK-EA
	for dev@spark.incubator.apache.org; Wed, 27 Aug 2014 22:20:59 -0700
Date: Wed, 27 Aug 2014 22:20:59 -0700 (PDT)
From: wenchen <cloud0fan@outlook.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409203259402-8083.post@n3.nabble.com>
Subject: [Spark SQL] query nested structure data
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am going to dig into this issue:
https://issues.apache.org/jira/browse/SPARK-2096
However, I noticed that there is already a NestedSqlParser in sql/core/test
org.apache.spark.sql.parquet. 
I checked this parser and it could solve the issue I mentioned before. But
why the author of the parser mark it as temporarily? Does this parser break
some spark sql grammar which I haven't noticed?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-query-nested-structure-data-tp8083.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 06:51:49 2014
Return-Path: <dev-return-9120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF27D1100D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 06:51:48 +0000 (UTC)
Received: (qmail 61816 invoked by uid 500); 28 Aug 2014 06:51:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61773 invoked by uid 500); 28 Aug 2014 06:51:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61761 invoked by uid 99); 28 Aug 2014 06:51:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 06:51:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of graham.dennis@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 06:51:21 +0000
Received: by mail-wg0-f50.google.com with SMTP id x12so285494wgg.21
        for <dev@spark.apache.org>; Wed, 27 Aug 2014 23:51:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=j4FiSHDEb+D2RIlMiySObvOvhWmqFQQu4qzRQUSyDAM=;
        b=KFtpryqgEhF1rBXTSbXKgioyjlRK1wtHHtoiJjJNhM/16rf88p6GA8paQ02xZhDCS7
         Pte42SwM9h7pzmpeRNuQra7NajX5ERrNbF/0OiNCxjCZNyUOy05tI1LvBsFZuXQlN12G
         WZiH+8iu+IHdqPDw6dpFZuA3H0lj2TgHLvBwrYpVu57RXjYYpXUw2hhQVmD9pkmDK6xW
         ZGChc4NZozuonVBDbDmnGoz61+6r4RSYaUU0Zs9Y682sKarFlnoXBJmV4t/geH2yfmII
         w5LSUx34CAl4VOX1Kqi8LuAGP/6IV+WAvOVJkumedC0YT35TpQLfqoMqIoiMXorlUBww
         P5Cw==
MIME-Version: 1.0
X-Received: by 10.194.157.10 with SMTP id wi10mr2388518wjb.31.1409208680430;
 Wed, 27 Aug 2014 23:51:20 -0700 (PDT)
Received: by 10.194.30.163 with HTTP; Wed, 27 Aug 2014 23:51:20 -0700 (PDT)
Date: Thu, 28 Aug 2014 16:51:20 +1000
Message-ID: <CABpRO2esoqYuO0b9tACR3juCs36PRCHXgOvNDz8spmjbwKh6sA@mail.gmail.com>
Subject: Preferred Executor launch path?
From: Graham Dennis <graham.dennis@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122ebe2cfc12b0501aaf61f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122ebe2cfc12b0501aaf61f
Content-Type: text/plain; charset=UTF-8

Hi all,

In the process of trying to resolve SPARK-3166 (inability to ship custom
serialisers in application jars)
https://issues.apache.org/jira/browse/SPARK-3166 I've discovered that
there's a bit of duplicated code for building the command for launching
Executors across SparkDeploySchedulerBackend.scala,
MesosSchedulerBackend.scala, and CoarseMesosSchedulerBackend.scala

Importantly, there is a slight difference in their behaviour where
SparkDeploySchedulerBackend doesn't launch the Executor with the
spark-class script, but instead tries to do something similar in
CommandUtils.scala.  MesosSchedulerBackend.scala and
CoarseMesosSchedulerBackend.scala both use the spark-class script.  Is the
latter the preferred approach?  So should I refactor all of these to use
spark-class, or is there a reason for the differing behaviour?

Secondly, the goal of SPARK-3166 is to have the user jar available to the
executor process at launch time (rather than when the first task is
received).  I'd like to get some feedback on what the preferred classpath
order should be.  The items to be ordered to determine the classpath are:

* The output of the compute-classpath script
* The config option spark.executor.extraClassPath
* The application jar (and anything added via SparkContext.addJar)

Complicating the matter is that the 'deploy' backend currently supports the
"spark.files.userClassPathFirst" option, but this is not supported by the
Mesos backends (and I don't think it's supported by the YARN backend).

Ignoring the "userClassPathFirst" option, the current behaviour for the
classpath is effectively:
1. The output of compute-classpath
2. The config option spark.executor.extraClassPath
3. The application jar (and anything added via SparkContext.addJar).

What should the preferred order be if userClassPathFirst is true?
 Currently the behaviour for the Deploy backend is effectively:
1. The application jar (and anything added via SparkContext.addJar)
2. The output of compute-classpath
3. The config option spark.executor.extraClassPath

To me it makes more sense for this to be in the order (application jar;
spark.executor.extraClassPath; compute-classpath).  Agree? Disagree?

Thanks,
Graham

--089e0122ebe2cfc12b0501aaf61f--

From dev-return-9121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 08:26:36 2014
Return-Path: <dev-return-9121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47BFC11302
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 08:26:36 +0000 (UTC)
Received: (qmail 66781 invoked by uid 500); 28 Aug 2014 08:26:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66711 invoked by uid 500); 28 Aug 2014 08:26:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66698 invoked by uid 99); 28 Aug 2014 08:26:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 08:26:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of qiping.lqp@alibaba-inc.com designates 42.120.133.82 as permitted sender)
Received: from [42.120.133.82] (HELO out4133-82.mail.aliyun.com) (42.120.133.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 08:26:30 +0000
DKIM-Signature:v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=alibaba-inc.com; s=default;
	t=1409214367; h=Date:From:To:Message-ID:Subject:MIME-Version:Content-Type;
	bh=YyuhLWSxOLSuAeQzQ/+j1h7dWkO2SnQeAx6pq25/2oE=;
	b=eQPUUk7WlcIZ3B9gkm8WrgcNtVFdAnlgRNVWhiFbX4YKdiQ+pP8wxVlZz4EpBTn8iLfGlaLAeFwI4EyHqIL3e+qKecBlLSfEm2bDwU1NqOI+6Rq9Mt1lEWy4zP/9S4wvB9gxGTmWOk49/RuUWiZjwplS1rHvjEunE5WjgX1rLgw=
Received: from WS-web (qiping.lqp@alibaba-inc.com[42.120.74.156]) by r46d02013.xy2.aliyun.com at Thu, 28 Aug 2014 16:26:05 +0800
Date: Thu, 28 Aug 2014 16:26:05 +0800
From: "=?UTF-8?B?5rSq5aWH?=" <qiping.lqp@alibaba-inc.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Reply-To: "=?UTF-8?B?5rSq5aWH?=" <qiping.lqp@alibaba-inc.com>
Message-ID: <80f33100-c915-4fda-a003-c2103462f003@alibaba-inc.com>
Subject: =?UTF-8?B?ZGVsZXRlZDogICAgc3FsL2hpdmUvc3JjL3Rlc3QvcmVzb3VyY2VzL2dvbGRlbi9jYXNlIHNl?=
  =?UTF-8?B?bnNpdGl2aXR5IG9uIHdpbmRvd3M=?=
X-Mailer: Alimail-Mailagent revision 2652425
MIME-Version: 1.0
x-aliyun-mail-creator: Webmail4_2660504_hLSTW96aWxsYS81LjAgKFdpbmRvd3MgTlQgNi4xOyBXT1c2NCkgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzM2LjAuMTk4NS4xNDMgU2FmYXJpLzUzNy4zNg==2I
Content-Type: multipart/alternative;
  boundary="----=ALIBOUNDARY_27606_4e058940_53fee79d_cb25e"
X-Virus-Checked: Checked by ClamAV on apache.org

------=ALIBOUNDARY_27606_4e058940_53fee79d_cb25e
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi,=0A=0AI want to contribute some code to mllib, I forked apache/spark to my =
own repository (chouqin/spark), =0Aand used `git clone https://github.com/chou=
qin/spark.git` to checkout the code my windows system.=0AIn this directory, I =
run `git status` before doing anything, it output this:=0A=0A```=0AOn branch m=
aster=0AYour branch is up-to-date with 'origin/master'.=0A=0AChanges not stage=
d for commit:=0A(use "git add/rm <file>..." to update what will be committed)=0A=
(use "git checkout -- <file>..." to discard changes in working directory)=0A=0A=
deleted: sql/hive/src/test/resources/golden/case sensitivity: Hive table-0-5d1=
4d21a239daa42b086cc895215009a=0A``` =0A=0AI don't know why because nothing has=
 been done. If I want to make some change, I have to be careful not to commit =
this deletion of file,=0AThis is every inconvenient for me because I always us=
e `git add .` to stage all changes, now I have to add every file individually.=
=0A=0ACan someone give me any suggestions to deal with this, my system is Wind=
ows 7 and git version is 1.9.2.msysgit.0.=0AThanks for your help.Qiping
------=ALIBOUNDARY_27606_4e058940_53fee79d_cb25e--


From dev-return-9122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 08:47:59 2014
Return-Path: <dev-return-9122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5407F11374
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 08:47:59 +0000 (UTC)
Received: (qmail 16871 invoked by uid 500); 28 Aug 2014 08:47:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16806 invoked by uid 500); 28 Aug 2014 08:47:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16777 invoked by uid 99); 28 Aug 2014 08:47:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 08:47:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 08:47:32 +0000
Received: by mail-qc0-f177.google.com with SMTP id i8so436127qcq.36
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 01:47:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=UX8/YTgomd4LirS8ocLo/ShjszeztUgJWqOlLwXo9cg=;
        b=rfk/2EmzDwunC41rSm7/3SeSST3l6l6MdgIMUqrlnb5ssCPl4td31KEijnFW1Stb5d
         n5T9kgTetOsRkq3zOTnsFMH4hyo6LlGcefSK5V7Ow4/L61abZmXkN+Gdak86IKujIARx
         0mFfpBGGjujcANhzcH6dsW1D6aIF+LMEWJE/dCGl6jDE9fSUP4blZk6TIcJB+ZmUdVXf
         +7mIb1GQon/GIIc1LVoyI1gkccT2AAUn+24AVKacZipuNO00D3jlfssV8RgkuGQ6rBCX
         USTmKyLa7ITJVhF6NLPf6bC0ElVhtlKxW9Lk/zAvGk7iTiFIUhivDRwpKL5Wo627Dlk/
         p/LQ==
X-Received: by 10.224.62.8 with SMTP id v8mr4155103qah.9.1409215651394; Thu,
 28 Aug 2014 01:47:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Thu, 28 Aug 2014 01:47:11 -0700 (PDT)
In-Reply-To: <80f33100-c915-4fda-a003-c2103462f003@alibaba-inc.com>
References: <80f33100-c915-4fda-a003-c2103462f003@alibaba-inc.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Thu, 28 Aug 2014 01:47:11 -0700
Message-ID: <CAA_qdLoiDe7G9eMRJQH05ETJ+0D5Cn3a2GUMUKj+regtBP=F0Q@mail.gmail.com>
Subject: Re: deleted: sql/hive/src/test/resources/golden/case sensitivity on windows
To: =?UTF-8?B?5rSq5aWH?= <qiping.lqp@alibaba-inc.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cb705096400501ac9623
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cb705096400501ac9623
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Colon is not allowed to be part of a Windows file name and I think Git just
cannot create this file while cloning. Remove the colon in the name string
of this test case
<https://github.com/chouqin/spark/blob/76e3ba4264c4a0bc2c33ae6ac862fc40bc30=
2d83/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQueryS=
uite.scala#L312>
should solve the problem.

Would you mind to file a JIRA and a PR to fix this?
=E2=80=8B


On Thu, Aug 28, 2014 at 1:26 AM, =E6=B4=AA=E5=A5=87 <qiping.lqp@alibaba-inc=
.com> wrote:

> Hi,
>
> I want to contribute some code to mllib, I forked apache/spark to my own
> repository (chouqin/spark),
> and used `git clone https://github.com/chouqin/spark.git` to checkout the
> code my windows system.
> In this directory, I run `git status` before doing anything, it output
> this:
>
> ```
> On branch master
> Your branch is up-to-date with 'origin/master'.
>
> Changes not staged for commit:
> (use "git add/rm <file>..." to update what will be committed)
> (use "git checkout -- <file>..." to discard changes in working directory)
>
> deleted: sql/hive/src/test/resources/golden/case sensitivity: Hive
> table-0-5d14d21a239daa42b086cc895215009a
> ```
>
> I don't know why because nothing has been done. If I want to make some
> change, I have to be careful not to commit this deletion of file,
> This is every inconvenient for me because I always use `git add .` to
> stage all changes, now I have to add every file individually.
>
> Can someone give me any suggestions to deal with this, my system is
> Windows 7 and git version is 1.9.2.msysgit.0.
> Thanks for your help.Qiping

--089e0149cb705096400501ac9623--

From dev-return-9123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 14:19:56 2014
Return-Path: <dev-return-9123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0ACC11010
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 14:19:56 +0000 (UTC)
Received: (qmail 14399 invoked by uid 500); 28 Aug 2014 14:19:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14355 invoked by uid 500); 28 Aug 2014 14:19:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13887 invoked by uid 99); 28 Aug 2014 14:19:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:19:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:19:45 +0000
Received: by mail-lb0-f177.google.com with SMTP id z11so967134lbi.22
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 07:19:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QCsRE2WphUzwPbpmjsWX/9B+gZBcV4XQ91zldxf4jm8=;
        b=Y1iqYj9JAte9jRX7lSAH7035VweF+LcuIiBLKdJT8XGHb0L74gu6bKM8UqDdLij8GS
         HzcMpqtAl4oKqbDi7ix+tVhH3yw5OZ+nPlJNhs9spc9MjngkR+UJDzK0xaDFMvMOOCMl
         UfsyZkAcKqVgAYofxgeePMlaDNB2CASYMuY4tDbbFlIBQNPzW17NwsV734kIvOVZlMRf
         qn0/0dyMsnKocl0/nGua6XNKAxkl6j6Bz1EYShS40NrFW+Pnq0VHu8ufALP84xZSJFig
         b+26e+LrUtk9hl1vU2pclGzAqvHiHHjkAS2R6uiU2ze3YlC6eJUHrtG/91oLEPtQjBHI
         uqNQ==
X-Gm-Message-State: ALoCoQnK1Osuh2+8KXL/FJ7CUYkkQQ4uKIvzhlyieu+O3r+yUvgM2jdVbcEqcXZ9ysqXSvgujz19
X-Received: by 10.112.184.161 with SMTP id ev1mr4339698lbc.82.1409235562018;
 Thu, 28 Aug 2014 07:19:22 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 07:19:01 -0700 (PDT)
In-Reply-To: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 07:19:01 -0700
Message-ID: <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c31d0e1450bb0501b13944
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c31d0e1450bb0501b13944
Content-Type: text/plain; charset=UTF-8

reminder:  this is starting in 10 minutes


On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu> wrote:

> tomorrow morning i will be upgrading jenkins to the latest/greatest
> (1.577).
>
> at 730am, i will put jenkins in to a quiet period, so no new builds will
> be accepted.  once any running builds are finished, i will be taking
> jenkins down for the upgrade.
>
> depending on what and how many jobs are running, i'm expecting this to
> take, at most, an hour.
>
> i'll send out an update tomorrow morning right before i begin, and will
> send out updates and an all-clear once we're up and running again.
>
> 1.577 release notes:
> http://jenkins-ci.org/changelog
>
> please let me know if there are any questions/concerns.  thanks in advance!
>
> shane
>

--001a11c31d0e1450bb0501b13944--

From dev-return-9124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 14:46:57 2014
Return-Path: <dev-return-9124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42945110E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 14:46:57 +0000 (UTC)
Received: (qmail 536 invoked by uid 500); 28 Aug 2014 14:46:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 472 invoked by uid 500); 28 Aug 2014 14:46:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 456 invoked by uid 99); 28 Aug 2014 14:46:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:46:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:46:30 +0000
Received: by mail-la0-f48.google.com with SMTP id gl10so1086914lab.35
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 07:46:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=D9wFzcIo8XlI7a7+B6c9VpBwJVoEvR+HoE0WMte8dxA=;
        b=FAB89GpE50V69Yk7ayqSOL38xFJb7RJQicx9Lcbz5kBVk5KyW7lNPuX1wAk1OCHeZn
         40dwEiEZ7nRAh2FtSgvt/yRMU87pSpG/MNYw/RaFYEnT0tgeODyeAmEXv0mmVp1jYC0P
         AY+60Gu0GW6xo21OKeNdTxPQlU93H86yLz+n9e51VW/1H5P2fe22NsWhzo9TCgNdt9jD
         td/xGEGqVJFfZfTNXZ144/+6J05XICFP44M+pMN7IVOft2TR/+PhDfAMS3LL4WT9neS7
         fyLNFpDPpzyiK0VRGSN8KMckvT/RBiy1InVgZlcUkM6RjAZervco3xP5o6QNxG1PJpcn
         GofA==
X-Gm-Message-State: ALoCoQmwxVa6mvPdu/rfY7Le+A5Fiouu4F5AQjaX1hEoUrFGI0PNL75EX6Fbk02bUXH8OBVTZcPs
X-Received: by 10.112.184.161 with SMTP id ev1mr4502815lbc.82.1409237186294;
 Thu, 28 Aug 2014 07:46:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 07:46:06 -0700 (PDT)
In-Reply-To: <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
 <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 07:46:06 -0700
Message-ID: <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c31d0ee4bf420501b199b5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c31d0ee4bf420501b199b5
Content-Type: text/plain; charset=UTF-8

jenkins is now coming down.


On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu> wrote:

> reminder:  this is starting in 10 minutes
>
>
> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> tomorrow morning i will be upgrading jenkins to the latest/greatest
>> (1.577).
>>
>> at 730am, i will put jenkins in to a quiet period, so no new builds will
>> be accepted.  once any running builds are finished, i will be taking
>> jenkins down for the upgrade.
>>
>> depending on what and how many jobs are running, i'm expecting this to
>> take, at most, an hour.
>>
>> i'll send out an update tomorrow morning right before i begin, and will
>> send out updates and an all-clear once we're up and running again.
>>
>> 1.577 release notes:
>> http://jenkins-ci.org/changelog
>>
>> please let me know if there are any questions/concerns.  thanks in
>> advance!
>>
>> shane
>>
>
>

--001a11c31d0ee4bf420501b199b5--

From dev-return-9125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 14:52:48 2014
Return-Path: <dev-return-9125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ECCA41110B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 14:52:47 +0000 (UTC)
Received: (qmail 8523 invoked by uid 500); 28 Aug 2014 14:52:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8455 invoked by uid 500); 28 Aug 2014 14:52:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8444 invoked by uid 99); 28 Aug 2014 14:52:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:52:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.54 as permitted sender)
Received: from [209.85.215.54] (HELO mail-la0-f54.google.com) (209.85.215.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 14:52:21 +0000
Received: by mail-la0-f54.google.com with SMTP id b17so1074738lan.13
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 07:52:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=NeDKuQbUu3RntFPR4O8VGCceKidMvYUvzfKczUMLOmg=;
        b=N45WZIv+22623DbmmeRnuyEMcAzUJugHyfTp08ou5xtXeLQweiHBrg24dcfMQEqRLw
         fx9rKwG0bq3oIDP4nYH8zYrmriVowfJFkTDn39YGpg1uubrAk1WwbWeL/Mka2q+CAvQm
         O46VXK8k7LXtW1YGTJK5jO/6tJ5/Hrvnh7QizywGvAt6pGVLR2l5U0cBCrJnxh0mkOhb
         GRLsFLeIBQ6STXWpGQUZA/+YHrStw9tCZL5UBN3aFF/M0WPwwbt3mqZL3r+19ocO2i5c
         bQQ14buRfM0/36UHUBf2wlyPRIrBQz3tJZH1rwEW78dnT4GUhl7zvj8D/D1K3/U82SMB
         rvHQ==
X-Gm-Message-State: ALoCoQnNtdjnEYLA1u49Ay4iujtBzwC9i8wr/nu2H/d22yfrN7BOOe0/CETtc7OXh5K6VsreCy2L
X-Received: by 10.112.184.161 with SMTP id ev1mr4539410lbc.82.1409237537440;
 Thu, 28 Aug 2014 07:52:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 07:51:57 -0700 (PDT)
In-Reply-To: <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
 <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com> <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 07:51:57 -0700
Message-ID: <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c31d0ed2d0fd0501b1ae4e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c31d0ed2d0fd0501b1ae4e
Content-Type: text/plain; charset=UTF-8

jenkins is upgraded, but a few jobs sneaked in before i could do the plugin
updates.  i've put jenkins in quiet mode again, and once the spark builds
finish, i'll restart jenkins to enable the plugin updates and we'll be good
to go.

let's all take a moment to bask in the glory of the shiny new UI!  :)


On Thu, Aug 28, 2014 at 7:46 AM, shane knapp <sknapp@berkeley.edu> wrote:

> jenkins is now coming down.
>
>
> On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> reminder:  this is starting in 10 minutes
>>
>>
>> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>>> tomorrow morning i will be upgrading jenkins to the latest/greatest
>>> (1.577).
>>>
>>> at 730am, i will put jenkins in to a quiet period, so no new builds will
>>> be accepted.  once any running builds are finished, i will be taking
>>> jenkins down for the upgrade.
>>>
>>> depending on what and how many jobs are running, i'm expecting this to
>>> take, at most, an hour.
>>>
>>> i'll send out an update tomorrow morning right before i begin, and will
>>> send out updates and an all-clear once we're up and running again.
>>>
>>> 1.577 release notes:
>>> http://jenkins-ci.org/changelog
>>>
>>> please let me know if there are any questions/concerns.  thanks in
>>> advance!
>>>
>>> shane
>>>
>>
>>
>

--001a11c31d0ed2d0fd0501b1ae4e--

From dev-return-9126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 16:03:00 2014
Return-Path: <dev-return-9126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD47111393
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 16:03:00 +0000 (UTC)
Received: (qmail 71314 invoked by uid 500); 28 Aug 2014 16:03:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71242 invoked by uid 500); 28 Aug 2014 16:02:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71230 invoked by uid 99); 28 Aug 2014 16:02:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:02:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:02:33 +0000
Received: by mail-oi0-f46.google.com with SMTP id h136so692273oig.19
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 09:02:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=YlIUGCWoo06R2XFKlxKGa2K5KeWCLhV1iAy+gkj4Hjg=;
        b=kWCoUEJW6kt9N2CMvu0MnxeJwlFY8qpVesc77Ixhr5uVwbV8HPKdruI7KbBqZ6vafK
         8A20hK3t0vt9iw4Im4Gh33q0ED8PNTe3a+gqQbZp/KeUY9uIOgib2iKvrBQ82q1BVjrN
         VbeFfGY+aDiYGw52lDyOOlir0DVc9AYzsM0uq08lbm1luPBQBRlH3SBXpr7jR2QTETw+
         DW1/skwgh7umNbYfDpjEjtbpTP7M2eKYniNjHJwq1ycZgS8UHP/uKPBBkCSN58xkP87e
         tpXFolU934k87k78gXhZ3SmTfzCtTBm4VKbwn1EQ3ocF2VF+epfzad8BKcvP8xC8HgWf
         GiTg==
MIME-Version: 1.0
X-Received: by 10.60.44.65 with SMTP id c1mr2985855oem.83.1409241751950; Thu,
 28 Aug 2014 09:02:31 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 09:02:31 -0700 (PDT)
Date: Thu, 28 Aug 2014 09:02:31 -0700
Message-ID: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.1.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc1 (commit f0718324):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1028/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Sunday, August 31, at 17:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of "spark.io.compression.codec" is now "snappy"
--> Old behavior can be restored by switching to "lzf"

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting "spark.shuffle.spill" to "false".

I'll send a bit more later today with feature information for the
release. In the mean time I want to put this out there for
consideration.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 16:13:00 2014
Return-Path: <dev-return-9127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83FB2113E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 16:13:00 +0000 (UTC)
Received: (qmail 92934 invoked by uid 500); 28 Aug 2014 16:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92871 invoked by uid 500); 28 Aug 2014 16:12:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92860 invoked by uid 99); 28 Aug 2014 16:12:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:12:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:12:33 +0000
Received: by mail-la0-f48.google.com with SMTP id gl10so1224659lab.7
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 09:12:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Ueo/2loXwSMJGMvxI5dRAOmoiu2KbUgfHVXy6msN7hs=;
        b=BiFE+E2gej+UujKm9fgE7Iz7eoUxN3t25Jum29x4msnXG8GZcSwCVCCf2Bh6vm1gYB
         Barnmiwn83w20LzQjqM3E5SM1AREqTGtRAlReXnl8Y60STaWFtpNt1qqwF/UsoF7/gwS
         AhMO0lTszM6E4Q161sWgpyShZUEwcxyoqvqXU/ZoO6enY2OEcsSW38vAbdKEEGzzdZrS
         krmCkZ+HvelQLTENWOPtOhY/2yQsMHF5m21W6/xt+khtVS+SnmBOtWg7gwRV118n1iWZ
         WyQFAJbveTDJYTRGpp89O4alrjajzNDrwvzQ5twHPdXZc5qdpQWC4ckghtxFUPBF1w1U
         ERpw==
X-Gm-Message-State: ALoCoQlYB7eiCU1cFp+5XcAdt1vWRCiLlh085WAo9ubK6XFuw3H/0A8EOEeE2sIP+h51VPM44mc2
X-Received: by 10.152.161.225 with SMTP id xv1mr5463782lab.59.1409242351807;
 Thu, 28 Aug 2014 09:12:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 09:12:11 -0700 (PDT)
In-Reply-To: <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
 <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
 <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com> <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 09:12:11 -0700
Message-ID: <CACdU-dQyiEkOuZPb19wfRk5FtayKZ9ALeLweeHLu-6CFEzDDcA@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a113467f4c841520501b2cdc8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113467f4c841520501b2cdc8
Content-Type: text/plain; charset=UTF-8

this one job is blocking the jenkins restart:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19406/

i'm about to kill it so that i can get this done.  i'll restart the job
after jenkins is back up.


On Thu, Aug 28, 2014 at 7:51 AM, shane knapp <sknapp@berkeley.edu> wrote:

> jenkins is upgraded, but a few jobs sneaked in before i could do the
> plugin updates.  i've put jenkins in quiet mode again, and once the spark
> builds finish, i'll restart jenkins to enable the plugin updates and we'll
> be good to go.
>
> let's all take a moment to bask in the glory of the shiny new UI!  :)
>
>
> On Thu, Aug 28, 2014 at 7:46 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> jenkins is now coming down.
>>
>>
>> On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>>> reminder:  this is starting in 10 minutes
>>>
>>>
>>> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu>
>>> wrote:
>>>
>>>> tomorrow morning i will be upgrading jenkins to the latest/greatest
>>>> (1.577).
>>>>
>>>> at 730am, i will put jenkins in to a quiet period, so no new builds
>>>> will be accepted.  once any running builds are finished, i will be taking
>>>> jenkins down for the upgrade.
>>>>
>>>> depending on what and how many jobs are running, i'm expecting this to
>>>> take, at most, an hour.
>>>>
>>>> i'll send out an update tomorrow morning right before i begin, and will
>>>> send out updates and an all-clear once we're up and running again.
>>>>
>>>> 1.577 release notes:
>>>> http://jenkins-ci.org/changelog
>>>>
>>>> please let me know if there are any questions/concerns.  thanks in
>>>> advance!
>>>>
>>>> shane
>>>>
>>>
>>>
>>
>

--001a113467f4c841520501b2cdc8--

From dev-return-9128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 16:16:45 2014
Return-Path: <dev-return-9128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B69B3113FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 16:16:45 +0000 (UTC)
Received: (qmail 99549 invoked by uid 500); 28 Aug 2014 16:16:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99482 invoked by uid 500); 28 Aug 2014 16:16:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99465 invoked by uid 99); 28 Aug 2014 16:16:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:16:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:16:40 +0000
Received: by mail-la0-f43.google.com with SMTP id ty20so1222296lab.2
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 09:16:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=EBVdaB8oY3NYH1L40IYe/IpIVdaV8YqjnHE76vrwbok=;
        b=Z/0lYNx0zvMkV/dLZlI+5T5zyc36Y8iTk9lyQEthhgkv8haNDcW8Lu1qMsLkKbX88S
         JL98DeXAPe9JZ/MVnkhCxGcEd9xmcUPHXzKDnhmf4HunMawF8L6TJYfJybE2/d6duCYt
         KThwb1ekVb4UZUgtCydiY0f/C9UUeN5ETo8lqB7alJALQxWSVk5cY+V7JLoV+FSOtxsa
         Z4N3XeO/WHb019+Nu4qZUnk+kFMiWH5CE2nAq8ogZJFw1Cj9fkWiYJXTMx0AfaBeUcfd
         IXWBMI+3pFUeRUcNqQsaygxJibM0S+PWxmEIlNTjJFbpF7xOOfQRhAqMu00O9dM3f8mk
         6bBQ==
X-Gm-Message-State: ALoCoQmqkSF6Yk07nicOWJJjZq1SkErNZqSrFN5w2hnfc2Embsj3v8U/rOWe3N93+s5494cgnupO
X-Received: by 10.152.6.40 with SMTP id x8mr5469940lax.18.1409242578309; Thu,
 28 Aug 2014 09:16:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 09:15:58 -0700 (PDT)
In-Reply-To: <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
 <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
 <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com> <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 09:15:58 -0700
Message-ID: <CACdU-dT-KBnij3g3SY-F3SyG3ztCffDJgqcVA6vwZBNf5wJ4eg@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=089e0141a020485f6f0501b2dbe6
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0141a020485f6f0501b2dbe6
Content-Type: text/plain; charset=UTF-8

all clear:  jenkins and all plugins have been updated!


On Thu, Aug 28, 2014 at 7:51 AM, shane knapp <sknapp@berkeley.edu> wrote:

> jenkins is upgraded, but a few jobs sneaked in before i could do the
> plugin updates.  i've put jenkins in quiet mode again, and once the spark
> builds finish, i'll restart jenkins to enable the plugin updates and we'll
> be good to go.
>
> let's all take a moment to bask in the glory of the shiny new UI!  :)
>
>
> On Thu, Aug 28, 2014 at 7:46 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> jenkins is now coming down.
>>
>>
>> On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>>> reminder:  this is starting in 10 minutes
>>>
>>>
>>> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu>
>>> wrote:
>>>
>>>> tomorrow morning i will be upgrading jenkins to the latest/greatest
>>>> (1.577).
>>>>
>>>> at 730am, i will put jenkins in to a quiet period, so no new builds
>>>> will be accepted.  once any running builds are finished, i will be taking
>>>> jenkins down for the upgrade.
>>>>
>>>> depending on what and how many jobs are running, i'm expecting this to
>>>> take, at most, an hour.
>>>>
>>>> i'll send out an update tomorrow morning right before i begin, and will
>>>> send out updates and an all-clear once we're up and running again.
>>>>
>>>> 1.577 release notes:
>>>> http://jenkins-ci.org/changelog
>>>>
>>>> please let me know if there are any questions/concerns.  thanks in
>>>> advance!
>>>>
>>>> shane
>>>>
>>>
>>>
>>
>

--089e0141a020485f6f0501b2dbe6--

From dev-return-9129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 16:27:44 2014
Return-Path: <dev-return-9129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 762781144F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 16:27:44 +0000 (UTC)
Received: (qmail 20573 invoked by uid 500); 28 Aug 2014 16:27:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20506 invoked by uid 500); 28 Aug 2014 16:27:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20494 invoked by uid 99); 28 Aug 2014 16:27:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:27:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:27:38 +0000
Received: by mail-qa0-f46.google.com with SMTP id w8so947838qac.33
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 09:27:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ChFQPYZREC7QaN2PVLrWMQBQaHkjALLPJF3wOajOjWw=;
        b=jIdfZrJVcjlyfoidCfis6A5uh1X3BbrHOjaCUHgRRN7L6ZKDH6pPFT4RKi1WeGQjWl
         ESp7zWsxaZAMA/PryPSxva1+CtHK9ZNZnFQOvujfbepRXx6eINXQ2WJQi/Ka0mRAIuCe
         voevQP3LrZNgLbjEq1Jv+Hf1KL8NyDcbxyWXzQRZxPr0No2EoE+f+w/Vn16s6OoSlq6J
         gCuq6iiPslXZ2v1vfTXW8Nb9RodozwCNQ2FMwcYgyhtGY1OJECrWfj+Ql2H2Fn9ABM5/
         XgHKh+0xOKYrgyb7QRkBOYlcoturTFr2yjB5oBDbZxXKiBnkzdqQeoYlUQdTyqAwA3X0
         2T/Q==
MIME-Version: 1.0
X-Received: by 10.140.22.243 with SMTP id 106mr7775346qgn.86.1409243237165;
 Thu, 28 Aug 2014 09:27:17 -0700 (PDT)
Received: by 10.140.40.41 with HTTP; Thu, 28 Aug 2014 09:27:17 -0700 (PDT)
Received: by 10.140.40.41 with HTTP; Thu, 28 Aug 2014 09:27:17 -0700 (PDT)
In-Reply-To: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
References: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
Date: Thu, 28 Aug 2014 21:57:17 +0530
Message-ID: <CAJiQeYJErGeKP_91c4AAFhHXQQcQK14+-K3=6uZBbmO7sJ7RtA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC1)
From: Mridul Muralidharan <mridul@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c158008db6ad0501b302d8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c158008db6ad0501b302d8
Content-Type: text/plain; charset=UTF-8

Is SPARK-3277 applicable to 1.1 ?
If yes, until it is fixed, I am -1 on the release (I am on break, so can't
verify or help fix, sorry).

Regards
Mridul
 On 28-Aug-2014 9:33 pm, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.1.0!
>
> The tag to be voted on is v1.1.0-rc1 (commit f0718324):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1028/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Sunday, August 31, at 17:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to
> "false".
>
> I'll send a bit more later today with feature information for the
> release. In the mean time I want to put this out there for
> consideration.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c158008db6ad0501b302d8--

From dev-return-9130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 16:46:44 2014
Return-Path: <dev-return-9130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB1A211512
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 16:46:43 +0000 (UTC)
Received: (qmail 85594 invoked by uid 500); 28 Aug 2014 16:46:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85525 invoked by uid 500); 28 Aug 2014 16:46:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85514 invoked by uid 99); 28 Aug 2014 16:46:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:46:42 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 16:46:38 +0000
Received: by mail-qa0-f41.google.com with SMTP id m5so981076qaj.0
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 09:46:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=VDfoRWeXYAU/MPL2Ksw4HnClhnG1AaFeUu3F977fsm8=;
        b=m9RmZmdsSVx5tT8Ko3yIFrf5iZBdJ8xz017Hvl9WfB5K0aF79AVaPQJJ4GMp1MMNMw
         3Hbjxl0dCCXKgMnmnEaVWM9D1NjWGEEYdIwk7O9qd/xA4p3nR4BxZyHt8ybWs1nT+Km6
         6Ik31N3MR6tJYtXGjAebuAIJZvhYwo4psgpBXMd8BqmIIoFC5MeCb2iRv7wuTCM6hiWi
         5D4ilxCPKZpmiwX/YEYHFrax5dUM6it2Y0zFZ0XlWy7gxEhLASqYO92qvz1TUGjekpuv
         Rhchgu7+Aq+uAIvCEmhHHP92zCeKBwNwdH6cmBXoVDzamYkLfRG/lHX25ti6uCyF5tO+
         277g==
X-Gm-Message-State: ALoCoQleKu5sTXXcxSBsV+fQrXml2a5d9lRQeHSIn4Q98MSC/rJiPecndGsir/JajuuaLogA5K4O
MIME-Version: 1.0
X-Received: by 10.140.44.67 with SMTP id f61mr8119015qga.44.1409244377095;
 Thu, 28 Aug 2014 09:46:17 -0700 (PDT)
Received: by 10.96.41.34 with HTTP; Thu, 28 Aug 2014 09:46:16 -0700 (PDT)
In-Reply-To: <CACdU-dT-KBnij3g3SY-F3SyG3ztCffDJgqcVA6vwZBNf5wJ4eg@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
	<CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
	<CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com>
	<CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
	<CACdU-dT-KBnij3g3SY-F3SyG3ztCffDJgqcVA6vwZBNf5wJ4eg@mail.gmail.com>
Date: Thu, 28 Aug 2014 09:46:16 -0700
Message-ID: <CAPh_B=ZrYS32GBxwNHMLSPc302Oq3CJG+QYQ-MQkgjG9Jwb9Jw@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
From: Reynold Xin <rxin@databricks.com>
To: shane knapp <sknapp@berkeley.edu>
Cc: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a113943d8806e050501b34646
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113943d8806e050501b34646
Content-Type: text/plain; charset=UTF-8

Thanks for doing this, Shane.

On Thursday, August 28, 2014, shane knapp <sknapp@berkeley.edu> wrote:

> all clear:  jenkins and all plugins have been updated!
>
>
> On Thu, Aug 28, 2014 at 7:51 AM, shane knapp <sknapp@berkeley.edu
> <javascript:;>> wrote:
>
> > jenkins is upgraded, but a few jobs sneaked in before i could do the
> > plugin updates.  i've put jenkins in quiet mode again, and once the spark
> > builds finish, i'll restart jenkins to enable the plugin updates and
> we'll
> > be good to go.
> >
> > let's all take a moment to bask in the glory of the shiny new UI!  :)
> >
> >
> > On Thu, Aug 28, 2014 at 7:46 AM, shane knapp <sknapp@berkeley.edu
> <javascript:;>> wrote:
> >
> >> jenkins is now coming down.
> >>
> >>
> >> On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu
> <javascript:;>> wrote:
> >>
> >>> reminder:  this is starting in 10 minutes
> >>>
> >>>
> >>> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu
> <javascript:;>>
> >>> wrote:
> >>>
> >>>> tomorrow morning i will be upgrading jenkins to the latest/greatest
> >>>> (1.577).
> >>>>
> >>>> at 730am, i will put jenkins in to a quiet period, so no new builds
> >>>> will be accepted.  once any running builds are finished, i will be
> taking
> >>>> jenkins down for the upgrade.
> >>>>
> >>>> depending on what and how many jobs are running, i'm expecting this to
> >>>> take, at most, an hour.
> >>>>
> >>>> i'll send out an update tomorrow morning right before i begin, and
> will
> >>>> send out updates and an all-clear once we're up and running again.
> >>>>
> >>>> 1.577 release notes:
> >>>> http://jenkins-ci.org/changelog
> >>>>
> >>>> please let me know if there are any questions/concerns.  thanks in
> >>>> advance!
> >>>>
> >>>> shane
> >>>>
> >>>
> >>>
> >>
> >
>

--001a113943d8806e050501b34646--

From dev-return-9131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 17:00:36 2014
Return-Path: <dev-return-9131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFAD7115A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 17:00:36 +0000 (UTC)
Received: (qmail 28173 invoked by uid 500); 28 Aug 2014 17:00:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28110 invoked by uid 500); 28 Aug 2014 17:00:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28094 invoked by uid 99); 28 Aug 2014 17:00:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:00:35 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:00:30 +0000
Received: by mail-pa0-f54.google.com with SMTP id fb1so3309503pad.27
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 10:00:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=cXRlNKtywT4E6hK93fk1pDpJGdle1pT0Q6LARymH7d8=;
        b=zVDFY/47spO5cgmeivdFwrf/kN6t7d9mYqrhaXYV28AOA/EXO8pl05SCZ1+2PVdb9u
         zC990gM+Pa7qDxFIL/K8WSUgQhNP8q5gsnBHzjiFn4lCFnR5dWiZ3ZHnhh3exnZmWnpk
         hVCxv5AZtspmtePE3DP4NsboSSu7lFsYngn9capF25OVuvv0lkr0YIAkrv7bIlRm8o/P
         5tgAbg4vspVvG1Hmv9jnNt0pJHIVT9lT6zmkENAtrPXlYQik4FX7U7p6zm0S2xAjhtcQ
         Oigaa7q392vqx4YVoqSIrBCnEqVRFRDWViaMkaJPuqdaVo7/raNyNpeUX4Guoz2IJl0n
         hnMQ==
X-Received: by 10.66.167.105 with SMTP id zn9mr7818037pab.103.1409245206179;
        Thu, 28 Aug 2014 10:00:06 -0700 (PDT)
Received: from joshs-mbp ([50.1.48.48])
        by mx.google.com with ESMTPSA id ou6sm4036043pbb.88.2014.08.28.10.00.05
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Thu, 28 Aug 2014 10:00:05 -0700 (PDT)
Date: Thu, 28 Aug 2014 10:00:02 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Cheng Lian <lian.cs.zju@gmail.com>, =?utf-8?Q?=E6=B4=AA=E5=A5=87?=
 <qiping.lqp@alibaba-inc.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.53ff6012.3f2dba31.104@joshs-mbp>
In-Reply-To: <CAA_qdLoiDe7G9eMRJQH05ETJ+0D5Cn3a2GUMUKj+regtBP=F0Q@mail.gmail.com>
References: <80f33100-c915-4fda-a003-c2103462f003@alibaba-inc.com>
 <CAA_qdLoiDe7G9eMRJQH05ETJ+0D5Cn3a2GUMUKj+regtBP=F0Q@mail.gmail.com>
Subject: Re: deleted: sql/hive/src/test/resources/golden/case
 sensitivity on windows
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53ff6012_7c83e458_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--53ff6012_7c83e458_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

RE: building Spark on Windows: earlier this week, I tried running the Mav=
en build on Windows 8 using the master branch and ran into a few issues. =
=C2=A0I=E2=80=99ve opened a PR to fix them=C2=A0https://github.com/apache=
/spark/pull/2165.

On August 28, 2014 at 1:47:58 AM, Cheng Lian (lian.cs.zju=40gmail.com) wr=
ote:
Colon is not allowed to be part of a Windows file name and I think Git ju=
st =20
cannot create this file while cloning. Remove the colon in the name strin=
g =20
of this test case =20
<https://github.com/chouqin/spark/blob/76e3ba4264c4a0bc2c33ae6ac862fc40bc=
302d83/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQu=
erySuite.scala=23L312> =20
should solve the problem. =20

Would you mind to file a JIRA and a PR to fix this=3F =20
=E2=80=8B =20


On Thu, Aug 28, 2014 at 1:26 AM, =E6=B4=AA=E5=A5=87 <qiping.lqp=40alibaba=
-inc.com> wrote: =20

> Hi, =20
> =20
> I want to contribute some code to mllib, I forked apache/spark to my ow=
n =20
> repository (chouqin/spark), =20
> and used =60git clone https://github.com/chouqin/spark.git=60 to checko=
ut the =20
> code my windows system. =20
> In this directory, I run =60git status=60 before doing anything, it out=
put =20
> this: =20
> =20
> =60=60=60 =20
> On branch master =20
> Your branch is up-to-date with 'origin/master'. =20
> =20
> Changes not staged for commit: =20
> (use =22git add/rm <file>...=22 to update what will be committed) =20
> (use =22git checkout -- <file>...=22 to discard changes in working dire=
ctory) =20
> =20
> deleted: sql/hive/src/test/resources/golden/case sensitivity: Hive =20
> table-0-5d14d21a239daa42b086cc895215009a =20
> =60=60=60 =20
> =20
> I don't know why because nothing has been done. If I want to make some =
=20
> change, I have to be careful not to commit this deletion of file, =20
> This is every inconvenient for me because I always use =60git add .=60 =
to =20
> stage all changes, now I have to add every file individually. =20
> =20
> Can someone give me any suggestions to deal with this, my system is =20
> Windows 7 and git version is 1.9.2.msysgit.0. =20
> Thanks for your help.Qiping =20

--53ff6012_7c83e458_104--


From dev-return-9132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 17:00:58 2014
Return-Path: <dev-return-9132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21B01115A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 17:00:58 +0000 (UTC)
Received: (qmail 29484 invoked by uid 500); 28 Aug 2014 17:00:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29409 invoked by uid 500); 28 Aug 2014 17:00:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29391 invoked by uid 99); 28 Aug 2014 17:00:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:00:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.46 as permitted sender)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:00:53 +0000
Received: by mail-la0-f46.google.com with SMTP id pv20so1301212lab.33
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 10:00:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=p83Ja6KrPc30eQke7jb2toR7CEtOkiDSYN/HHTRc+RY=;
        b=hE2SI2BcD2UWO6vxHKQGI6yWzhgXmHiQFki9MXrymWys0QbZj8tBMfN6OeYfy6eBOM
         6s/K8G8Z/T6H751i1tNS3kQ6BBSXGS/tfaOko9QPHm/P/wDEbIqydiugzv5ekUR1WVmT
         AE8zQYl8fRGvMvpuq5jc7u2ZvcRjZNNJEhagUDj3IqGYKXKkizr85iI+EQfcPPX92QTT
         unU/RAC5OInSC7hymHY2FWVMmk1IVMI9w2dr+L/JPksRAagtIUb/EQ+ZxZZzOQe+HWUI
         jfuS5vsNBB6WO9YkQYA9xL4ncPSPcgLnEElqdFZGvSst1uXyUMh6XtS4XHfg0KR14xxJ
         atZg==
X-Gm-Message-State: ALoCoQm+OaThNGUqSLcxUufjEce0HNqhQbKs+DgkLgt4fGPKjGTTPnmj1Ia1u2jK1CqsnuUveNBO
X-Received: by 10.152.161.225 with SMTP id xv1mr5770682lab.59.1409245230545;
 Thu, 28 Aug 2014 10:00:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 10:00:10 -0700 (PDT)
In-Reply-To: <CAPh_B=ZrYS32GBxwNHMLSPc302Oq3CJG+QYQ-MQkgjG9Jwb9Jw@mail.gmail.com>
References: <CACdU-dQ0NVjk4vfT+X7Z_eJK2i+eFdtLSCbvnWnx4LUxvF0rjA@mail.gmail.com>
 <CACdU-dQpzpU=4snf7Dq-8hFm6skQBRTQQj+Z_PNktNHyD7aCyw@mail.gmail.com>
 <CACdU-dQqVco7p0Gakw6Q7syk+1tqOxCZJpvAHEwUzq_Y7H4tkw@mail.gmail.com>
 <CACdU-dQXfOpSoyNwfM1ruceNsp4fGuynLS-7-2Ab5sWkZbfZZQ@mail.gmail.com>
 <CACdU-dT-KBnij3g3SY-F3SyG3ztCffDJgqcVA6vwZBNf5wJ4eg@mail.gmail.com> <CAPh_B=ZrYS32GBxwNHMLSPc302Oq3CJG+QYQ-MQkgjG9Jwb9Jw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 10:00:10 -0700
Message-ID: <CACdU-dSSGz=2nVwrga=NZzbjZNxn2S04Oco-230aXL1XS_dTkg@mail.gmail.com>
Subject: Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT
To: amp-infra <amp-infra@googlegroups.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113467f45e4c100501b379dc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113467f45e4c100501b379dc
Content-Type: text/plain; charset=UTF-8

no problem!

also, i retriggered:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19406
it's currently:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19411


On Thu, Aug 28, 2014 at 9:46 AM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for doing this, Shane.
>
>
> On Thursday, August 28, 2014, shane knapp <sknapp@berkeley.edu> wrote:
>
>> all clear:  jenkins and all plugins have been updated!
>>
>>
>> On Thu, Aug 28, 2014 at 7:51 AM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>> > jenkins is upgraded, but a few jobs sneaked in before i could do the
>> > plugin updates.  i've put jenkins in quiet mode again, and once the
>> spark
>> > builds finish, i'll restart jenkins to enable the plugin updates and
>> we'll
>> > be good to go.
>> >
>> > let's all take a moment to bask in the glory of the shiny new UI!  :)
>> >
>> >
>> > On Thu, Aug 28, 2014 at 7:46 AM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>> >
>> >> jenkins is now coming down.
>> >>
>> >>
>> >> On Thu, Aug 28, 2014 at 7:19 AM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>> >>
>> >>> reminder:  this is starting in 10 minutes
>> >>>
>> >>>
>> >>> On Wed, Aug 27, 2014 at 4:13 PM, shane knapp <sknapp@berkeley.edu>
>> >>> wrote:
>> >>>
>> >>>> tomorrow morning i will be upgrading jenkins to the latest/greatest
>> >>>> (1.577).
>> >>>>
>> >>>> at 730am, i will put jenkins in to a quiet period, so no new builds
>> >>>> will be accepted.  once any running builds are finished, i will be
>> taking
>> >>>> jenkins down for the upgrade.
>> >>>>
>> >>>> depending on what and how many jobs are running, i'm expecting this
>> to
>> >>>> take, at most, an hour.
>> >>>>
>> >>>> i'll send out an update tomorrow morning right before i begin, and
>> will
>> >>>> send out updates and an all-clear once we're up and running again.
>> >>>>
>> >>>> 1.577 release notes:
>> >>>> http://jenkins-ci.org/changelog
>> >>>>
>> >>>> please let me know if there are any questions/concerns.  thanks in
>> >>>> advance!
>> >>>>
>> >>>> shane
>> >>>>
>> >>>
>> >>>
>> >>
>> >
>>
>  --
> You received this message because you are subscribed to the Google Groups
> "amp-infra" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to amp-infra+unsubscribe@googlegroups.com.
> For more options, visit https://groups.google.com/d/optout.
>

--001a113467f45e4c100501b379dc--

From dev-return-9133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 17:27:46 2014
Return-Path: <dev-return-9133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 34C48116AA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 17:27:46 +0000 (UTC)
Received: (qmail 3024 invoked by uid 500); 28 Aug 2014 17:27:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2900 invoked by uid 500); 28 Aug 2014 17:27:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2878 invoked by uid 99); 28 Aug 2014 17:27:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:27:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 17:27:38 +0000
Received: by mail-we0-f176.google.com with SMTP id q59so1096024wes.35
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 10:27:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:date:message-id:subject
         :from:to:content-type;
        bh=qPiewfMgsTenqvKW26JBjaSD+CjV7OfMTQToQPcs7kI=;
        b=hKF9lxRgFD65g0tgMfbXo/kZJcuXkonTPWPEtL8mJZTW7IP7ttO5nbcJjQjmO44oON
         ZfrkRukJ2iu/cJMCoJRuJLlrJOl/LOa+/KiuxkYZlgv4AL/9Wjs+M0/vo4a9PPGvkdl6
         qbmu6HN9ApKZi3Flb1/rbxiKFqtm2MO/FLLSzyf8dJPfogxUkUeNVhiwFSQFOX7/1rmI
         xCpi7IboERNtqQ1LW6fd3ZwIpHeLDj0L8VNIwpdwVcfShKPjceTo0XmX488AyBPzX+jJ
         r76Xud2SP6sxhKafwHlcYcmKNOslnuqg+XwXuG1SXzjk25AICjDQM0gskfvbcV0wu2Na
         atIQ==
X-Gm-Message-State: ALoCoQkfWqg5loXZyJ66JI00gweMp9RxAPbXiAwIrZz1rt29ymVulzppMD6zBYM3KNmz0JET0YzK
MIME-Version: 1.0
X-Received: by 10.194.8.168 with SMTP id s8mr5011014wja.129.1409246836747;
 Thu, 28 Aug 2014 10:27:16 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.108.198 with HTTP; Thu, 28 Aug 2014 10:27:16 -0700 (PDT)
Date: Thu, 28 Aug 2014 10:27:16 -0700
Message-ID: <CAKx7Bf-ZNj2fXB=NdOy09pc7djRh5BzteeuwUizZPFSBprVZoQ@mail.gmail.com>
Subject: New SparkR mailing list, JIRA
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d50b01b034b0501b3d9e4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d50b01b034b0501b3d9e4
Content-Type: text/plain; charset=UTF-8

Hi

I'd like to announce a couple of updates to the SparkR project. In order to
facilitate better collaboration for new features and development we have a
new mailing list, issue tracker for SparkR.

- The new JIRA is hosted at https://sparkr.atlassian.net/browse/SPARKR/ and
we have migrated all existing Github issues to the JIRA. Please submit any
bugs / improvements to this JIRA going forward.

- There is a new mailing list sparkr-dev@googlegroups.com that will be used
for design discussions for new features and development related issues. We
will still be answering to user issues on Apache Spark mailing lists.

Please let me know if have any questions.

Thanks
Shivaram

--047d7b5d50b01b034b0501b3d9e4--

From dev-return-9134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 20:01:27 2014
Return-Path: <dev-return-9134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A82C811F7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 20:01:27 +0000 (UTC)
Received: (qmail 32542 invoked by uid 500); 28 Aug 2014 20:01:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32469 invoked by uid 500); 28 Aug 2014 20:01:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32448 invoked by uid 99); 28 Aug 2014 20:01:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 20:01:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 20:01:00 +0000
Received: by mail-ob0-f172.google.com with SMTP id wo20so1056437obc.17
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 13:00:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=THIhZYkKa+QmYl9dHthvFNMEUXlNEGuVE/YrHQinG34=;
        b=ynHwNZnvy3Q3B4nTTKwvwPmnKVH1UD+E31cRL8znqgVOFjbkDGRdHu/l2ADINrcNOY
         6cSFKm8dj3mEispNsHJQ2DnHnRe0gQZtZYlQzmL1GjxWwYdE5KVrc4dIBe9Rnjz1dLrB
         0bXTNlbancMaD7uzMxbMyJM2IT2HKnt5iVsg/ofNOz2B2KlNwuCKe0ZoI+yED8kKr2IX
         +lx2BUM2QuYFZLuVHKV6G73wPYvCs+rddB3Y07ZG911f1AiM/PNGlfvpRJxqtnzsI7p9
         XKwPNG2mI5MPwI/lFxgizbWlhLfqK52LChpSVJh4+6ntNIFjKtOLBktMDoXTvV41Zf4F
         Mxqg==
MIME-Version: 1.0
X-Received: by 10.182.66.16 with SMTP id b16mr5922806obt.49.1409256058829;
 Thu, 28 Aug 2014 13:00:58 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 13:00:58 -0700 (PDT)
In-Reply-To: <CAJiQeYJErGeKP_91c4AAFhHXQQcQK14+-K3=6uZBbmO7sJ7RtA@mail.gmail.com>
References: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
	<CAJiQeYJErGeKP_91c4AAFhHXQQcQK14+-K3=6uZBbmO7sJ7RtA@mail.gmail.com>
Date: Thu, 28 Aug 2014 13:00:58 -0700
Message-ID: <CABPQxsvxVojEMBQk0wfohy-4FjeaYuSuXcCJYDDrYmTTe89TwQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Mridul - thanks for sending this along and for the debugging comments
on the JIRA. I think we have a handle on the issue and we'll patch it
and spin a new RC. We can also update the test coverage to cover LZ4.

- Patrick

On Thu, Aug 28, 2014 at 9:27 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Is SPARK-3277 applicable to 1.1 ?
> If yes, until it is fixed, I am -1 on the release (I am on break, so can't
> verify or help fix, sorry).
>
> Regards
> Mridul
>
> On 28-Aug-2014 9:33 pm, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.1.0!
>>
>> The tag to be voted on is v1.1.0-rc1 (commit f0718324):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1028/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.1.0!
>>
>> The vote is open until Sunday, August 31, at 17:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>> this release.
>>
>> == What default changes should I be aware of? ==
>> 1. The default value of "spark.io.compression.codec" is now "snappy"
>> --> Old behavior can be restored by switching to "lzf"
>>
>> 2. PySpark now performs external spilling during aggregations.
>> --> Old behavior can be restored by setting "spark.shuffle.spill" to
>> "false".
>>
>> I'll send a bit more later today with feature information for the
>> release. In the mean time I want to put this out there for
>> consideration.
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 20:04:39 2014
Return-Path: <dev-return-9135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 227BC11FC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 20:04:39 +0000 (UTC)
Received: (qmail 43762 invoked by uid 500); 28 Aug 2014 20:04:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43695 invoked by uid 500); 28 Aug 2014 20:04:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43679 invoked by uid 99); 28 Aug 2014 20:04:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 20:04:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.173 as permitted sender)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 20:04:12 +0000
Received: by mail-lb0-f173.google.com with SMTP id c11so1517021lbj.18
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 13:04:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=q4XtsJEwiTbLxnn8NCJVZT9AGfHcOwThib1uWdfxXhE=;
        b=XOdGw0KdOlYYQ3tb3A9QXmXXwjleRtu7oixrpspF4Dckf5wdKYTOCmZyd+0LuD3kkE
         N0KcTjrZxCIHZJiAZ1pnrQWPG8Xz1PfTxlyRb4nVmBNrcWS9ePf93Zum8rzv6KReZt3A
         i+0Qe8GjaX3J0UAs5n/B3GQ9M5n1SWelqYMkCTLE78fAAcXsAjOX6KfBaOjUoGODwngZ
         JBt8h8x434PeNygFO+WXhRRF2mOofRwkbpcOs9WwZ7x5gbl7k1iARUJtymixnuBYryHZ
         R3qjsDCw4/boM2MTKZchMVpold+0LQxHLcjm8eBmc3KOkwx44wqdw0Ca/R/4wJ2VYb2H
         g7kQ==
X-Gm-Message-State: ALoCoQmH0K0tGdLLmPPsDj+vV7soxS1Rf+s0iK5tCScvQoYPSggQpAcBWz78n18kjxftz04+zcOU
X-Received: by 10.112.4.70 with SMTP id i6mr6620510lbi.54.1409256251679; Thu,
 28 Aug 2014 13:04:11 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Thu, 28 Aug 2014 13:03:51 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 28 Aug 2014 13:03:51 -0700
Message-ID: <CACdU-dTZx3ey=1OHuYFjf8LNcO64bw2VQ8VJ40ssKvAGr8nUcw@mail.gmail.com>
Subject: "emergency" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae94ed6cd4779400501b60a34
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae94ed6cd4779400501b60a34
Content-Type: text/plain; charset=UTF-8

as with all software upgrades, sometimes things don't always work as
expected.

a recent change to stapler[1], to verbosely
report NotExportableExceptions[2] is spamming our jenkins log file with
stack traces, which is growing rather quickly (1.2G since 9am).  this has
been reported to the jenkins jira[3], and a fix has been pushed and will be
rolled out "soon"[4].

this isn't affecting any builds, and jenkins is happily humming along.

in the interim, so that we don't run out of disk space, i will be
redirecting the jenkins logs tommorow morning to /dev/null for the long
weekend.

once a real fix has been released, i will update any packages needed and
redirect the logging back to the log file.

other than a short downtime, this will have no user-facing impact.

please let me know if you have any questions/concerns.

thanks for your patience!

shane "the new guy"  :)

[1] -- https://wiki.jenkins-ci.org/display/JENKINS/Architecture
[2] --
https://github.com/stapler/stapler/commit/ed2cb8b04c1514377f3a8bfbd567f050a67c6e1c
[3] --
https://issues.jenkins-ci.org/browse/JENKINS-24458?focusedCommentId=209247
[4] --
https://github.com/stapler/stapler/commit/e2b39098ca1f61a58970b8a41a3ae79053cf30e3

--14dae94ed6cd4779400501b60a34--

From dev-return-9136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 22:27:50 2014
Return-Path: <dev-return-9136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 393FF11598
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 22:27:50 +0000 (UTC)
Received: (qmail 37114 invoked by uid 500); 28 Aug 2014 22:27:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37042 invoked by uid 500); 28 Aug 2014 22:27:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37022 invoked by uid 99); 28 Aug 2014 22:27:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:27:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:27:43 +0000
Received: by mail-qg0-f42.google.com with SMTP id a108so1497634qge.29
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 15:27:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Snj6tiPwpJcHQn9Iwyi09sYCPEcdw5TDEm55dFp2VRY=;
        b=bRfKL8Tl+pj87xWVAoFkFY2Ek5iD8i/FSFUwNIH90D+TeNncjfbKzuA+JYynRVNx88
         4QbGshF6Fr1DU2Pr4b3hMpnq/bD11Q77KnskRdADePopxmnh1Urm+rvcub8w+rwY+NZi
         VhbVQTAN/xH3JbcB5n2nU5vIn/75J2hDCw+lcqqejZB7Blrl1cKL33Tm2WMRfPuRZgAv
         p8c4YbJ9lPICzgq/U5LL/TtFs4O7Msthk/D/jYk23+qk1nd5o/F0mmfO3j8Qs91XnELk
         Q8O4Mk/mvEWRwehqPx94NbCrZVSyMSo1QjkVfkWR3ljhoAVcyzmHStGdZVb1w3E1i+Z6
         W8FA==
MIME-Version: 1.0
X-Received: by 10.229.231.68 with SMTP id jp4mr11759252qcb.4.1409264843081;
 Thu, 28 Aug 2014 15:27:23 -0700 (PDT)
Received: by 10.140.40.41 with HTTP; Thu, 28 Aug 2014 15:27:23 -0700 (PDT)
Received: by 10.140.40.41 with HTTP; Thu, 28 Aug 2014 15:27:23 -0700 (PDT)
In-Reply-To: <CABPQxsvxVojEMBQk0wfohy-4FjeaYuSuXcCJYDDrYmTTe89TwQ@mail.gmail.com>
References: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
	<CAJiQeYJErGeKP_91c4AAFhHXQQcQK14+-K3=6uZBbmO7sJ7RtA@mail.gmail.com>
	<CABPQxsvxVojEMBQk0wfohy-4FjeaYuSuXcCJYDDrYmTTe89TwQ@mail.gmail.com>
Date: Fri, 29 Aug 2014 03:57:23 +0530
Message-ID: <CAJiQeYL1Ht95fQ8D+pLzrVRNgFq+O+4PVTELoVrWu0aD+tMaAQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC1)
From: Mridul Muralidharan <mridul@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134497a5dcf010501b80a07
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134497a5dcf010501b80a07
Content-Type: text/plain; charset=UTF-8

Thanks for being on top of this Patrick ! And apologies for not being able
to help more.

Regards,
Mridul
On Aug 29, 2014 1:30 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Mridul - thanks for sending this along and for the debugging comments
> on the JIRA. I think we have a handle on the issue and we'll patch it
> and spin a new RC. We can also update the test coverage to cover LZ4.
>
> - Patrick
>
> On Thu, Aug 28, 2014 at 9:27 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > Is SPARK-3277 applicable to 1.1 ?
> > If yes, until it is fixed, I am -1 on the release (I am on break, so
> can't
> > verify or help fix, sorry).
> >
> > Regards
> > Mridul
> >
> > On 28-Aug-2014 9:33 pm, "Patrick Wendell" <pwendell@gmail.com> wrote:
> >>
> >> Please vote on releasing the following candidate as Apache Spark version
> >> 1.1.0!
> >>
> >> The tag to be voted on is v1.1.0-rc1 (commit f0718324):
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.1.0-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1028/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.1.0!
> >>
> >> The vote is open until Sunday, August 31, at 17:00 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.1.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening very late into the QA period compared with
> >> previous votes, so -1 votes should only occur for significant
> >> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> >> this release.
> >>
> >> == What default changes should I be aware of? ==
> >> 1. The default value of "spark.io.compression.codec" is now "snappy"
> >> --> Old behavior can be restored by switching to "lzf"
> >>
> >> 2. PySpark now performs external spilling during aggregations.
> >> --> Old behavior can be restored by setting "spark.shuffle.spill" to
> >> "false".
> >>
> >> I'll send a bit more later today with feature information for the
> >> release. In the mean time I want to put this out there for
> >> consideration.
> >>
> >> - Patrick
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >
>

--001a1134497a5dcf010501b80a07--

From dev-return-9137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 22:52:57 2014
Return-Path: <dev-return-9137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C68F3116C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 22:52:57 +0000 (UTC)
Received: (qmail 14023 invoked by uid 500); 28 Aug 2014 22:52:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13956 invoked by uid 500); 28 Aug 2014 22:52:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13944 invoked by uid 99); 28 Aug 2014 22:52:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:52:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of bbejeck@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:52:30 +0000
Received: by mail-ig0-f171.google.com with SMTP id l13so8807940iga.10
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 15:52:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=FgvzzNTh+LzxR+GDuudEGglff6tPN+o45G4g8kl/yuI=;
        b=a/JVhNXgrpQPKHWIv1Vbzux3cAQ7cxCrIUwwK13ev+3n0ZtRaMWOAc8j9Ps8sQXft3
         F1ZKrIdpRfzVNA8CMPkin0e2pwQglO/oFwqWqtDi4ShQo4TsYRcGGXNJSySsmPnqtUb7
         WmgKqjUnMvW9C2T8vvSpfoUafLXBJOL9Yc8/NOEDMx0rAC7dv2Cm+DSDMldWYpQQ1yHy
         ouHK0qDv4dU3u6y7cyOmO4rLYsFBQ9e+3Lkb9s3l7je3XNPcLLkxO0Fy824b2igOwHCa
         A56gCCdBAJsLCBgCAGLPhu4U5i9fMt+LlQfBEn/gNU++Q2OY0t/JE/4fctRYVsnKrzV5
         SeHQ==
MIME-Version: 1.0
X-Received: by 10.50.25.41 with SMTP id z9mr81706igf.0.1409266349168; Thu, 28
 Aug 2014 15:52:29 -0700 (PDT)
Received: by 10.64.249.164 with HTTP; Thu, 28 Aug 2014 15:52:29 -0700 (PDT)
Date: Thu, 28 Aug 2014 18:52:29 -0400
Message-ID: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
Subject: Jira tickets for starter tasks
From: Bill Bejeck <bbejeck@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd7689422fb0f0501b864bf
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd7689422fb0f0501b864bf
Content-Type: text/plain; charset=UTF-8

Hi,

How do I get a starter task jira ticket assigned to myself? Or do I just do
the work and issue a pull request with the associated jira number?

Thanks,
Bill

--047d7bd7689422fb0f0501b864bf--

From dev-return-9138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 22:56:04 2014
Return-Path: <dev-return-9138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9AAE8116FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 22:56:04 +0000 (UTC)
Received: (qmail 34582 invoked by uid 500); 28 Aug 2014 22:56:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34525 invoked by uid 500); 28 Aug 2014 22:56:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34503 invoked by uid 99); 28 Aug 2014 22:56:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:56:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:55:35 +0000
Received: by mail-qc0-f169.google.com with SMTP id l6so1634469qcy.28
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 15:55:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=i5OocAIGPIMyQkPXsN9OyfMDEIpK0G268A2r6U9IWmk=;
        b=QrgGeqodXgaLZqsxzmBK240lJ4JuK/lLYELIuq5cO5ELhz1XjaPKOVHVwBgGEtrvYF
         79TTVZR7bB67iwjTqagL+6fRK6d724A1igpEDKkvh9FcEVbLI7vN9+UpvXIyhrGnU2wX
         uuzGs+IYYrHtKyI4YCQc8dPc+1GL4k2sfAVVdr5WpZf4qdhwxT6yUV7E5+6C2pM0PUGq
         xS0XvGLekyXfe76xbWfcPLYpVrE7kqjt/v6QBsb+ouZhR4bIgCDVvPFLcXieOQlU9btG
         Seiren+6cCD3+b3VGebr0dMyNIG4ZieIC9iRl6oXh/VOwcl9NJnIw0fszHr/AJYnSxhs
         6NdQ==
X-Received: by 10.224.26.146 with SMTP id e18mr11855973qac.13.1409266534467;
 Thu, 28 Aug 2014 15:55:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Thu, 28 Aug 2014 15:55:14 -0700 (PDT)
In-Reply-To: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
References: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Thu, 28 Aug 2014 15:55:14 -0700
Message-ID: <CAA_qdLpTbEypB9Ni_n-xoVpyz18uqS=X0qD9DJQNSgiFBU=u-w@mail.gmail.com>
Subject: Re: Jira tickets for starter tasks
To: Bill Bejeck <bbejeck@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e015375102e50470501b86f6c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015375102e50470501b86f6c
Content-Type: text/plain; charset=UTF-8

You can just start the work :)


On Thu, Aug 28, 2014 at 3:52 PM, Bill Bejeck <bbejeck@gmail.com> wrote:

> Hi,
>
> How do I get a starter task jira ticket assigned to myself? Or do I just do
> the work and issue a pull request with the associated jira number?
>
> Thanks,
> Bill
>

--089e015375102e50470501b86f6c--

From dev-return-9139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Aug 28 22:57:28 2014
Return-Path: <dev-return-9139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA43511712
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Aug 2014 22:57:28 +0000 (UTC)
Received: (qmail 41002 invoked by uid 500); 28 Aug 2014 22:57:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40933 invoked by uid 500); 28 Aug 2014 22:57:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40920 invoked by uid 99); 28 Aug 2014 22:57:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:57:26 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.220.46 as permitted sender)
Received: from [209.85.220.46] (HELO mail-pa0-f46.google.com) (209.85.220.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Aug 2014 22:57:19 +0000
Received: by mail-pa0-f46.google.com with SMTP id eu11so4418148pac.33
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 15:56:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=PURJkcIHYN6B30IOUAaaQjiN0sdXVBpWIKD/yFWpKj0=;
        b=kS/ju+oS9djyvPbj5ACurtOAbbGXfxcy4T0te7wcsvvMqS5BfgTjxSqgLpeeWT+axA
         tUAH+ll4ZXAT0SCjtM3txcqkSl4ChUhJ804WyCxLtja3pboPl5g3/Kr7cC8NqD06cTss
         VJM5URLEwhkmCjVQYBCQ9sHaD9dVA6N7UJYuBLdA06BuGgOLEF+NY3lc5FRvj0BRdniw
         8BROUkB4ffykDaZ07ia0+CBt9Jw+ybKPR0EdL5V8KG4VM4/o77jSj+g8UE0OeXFdc0UK
         quO+QwMEJMzc4308zSBzmYJEiZAoLW6pRaCZiCDY+svBEWK5311IUqJefdgDSj4v0nog
         dmKw==
X-Received: by 10.68.192.35 with SMTP id hd3mr10361349pbc.144.1409266619241;
        Thu, 28 Aug 2014 15:56:59 -0700 (PDT)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id z1sm7085879pdp.80.2014.08.28.15.56.57
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Thu, 28 Aug 2014 15:56:57 -0700 (PDT)
Date: Thu, 28 Aug 2014 15:56:56 -0700
From: Josh Rosen <rosenville@gmail.com>
To: Bill Bejeck <bbejeck@gmail.com>, dev@spark.apache.org
Message-ID: <etPan.53ffb3b8.2443a858.104@joshs-mbp>
In-Reply-To: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
References: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
Subject: Re: Jira tickets for starter tasks
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53ffb3b8_2d1d5ae9_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--53ffb3b8_2d1d5ae9_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

A JIRA admin needs to add you to the =E2=80=98=E2=80=99Contributors=E2=80=
=9D role group in order to allow you to assign issues to yourself. =C2=A0=
I=E2=80=99ve added this email address to that group, so you should be set=
=21

- Josh


On August 28, 2014 at 3:52:57 PM, Bill Bejeck (bbejeck=40gmail.com) wrote=
:

Hi, =20

How do I get a starter task jira ticket assigned to myself=3F Or do I jus=
t do =20
the work and issue a pull request with the associated jira number=3F =20

Thanks, =20
Bill =20

--53ffb3b8_2d1d5ae9_104--


From dev-return-9140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 01:15:17 2014
Return-Path: <dev-return-9140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D1CE11C56
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 01:15:17 +0000 (UTC)
Received: (qmail 76675 invoked by uid 500); 29 Aug 2014 01:15:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76614 invoked by uid 500); 29 Aug 2014 01:15:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76601 invoked by uid 99); 29 Aug 2014 01:15:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 01:15:15 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of qiping.lqp@alibaba-inc.com)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 01:15:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <qiping.lqp@alibaba-inc.com>)
	id 1XNAmA-0004Sn-9f
	for dev@spark.incubator.apache.org; Thu, 28 Aug 2014 18:14:50 -0700
Date: Thu, 28 Aug 2014 18:14:50 -0700 (PDT)
From: HongQi <qiping.lqp@alibaba-inc.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409274890275-8105.post@n3.nabble.com>
In-Reply-To: <CAA_qdLoiDe7G9eMRJQH05ETJ+0D5Cn3a2GUMUKj+regtBP=F0Q@mail.gmail.com>
References: <80f33100-c915-4fda-a003-c2103462f003@alibaba-inc.com> <CAA_qdLoiDe7G9eMRJQH05ETJ+0D5Cn3a2GUMUKj+regtBP=F0Q@mail.gmail.com>
Subject: Re: deleted: sql/hive/src/test/resources/golden/case sensitivity on
 windows
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

OK, I will create a PR to fix this. thanks for your comments.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/deleted-sql-hive-src-test-resources-golden-case-sensitivity-on-windows-tp8085p8105.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 02:13:10 2014
Return-Path: <dev-return-9141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1163F11DBA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 02:13:10 +0000 (UTC)
Received: (qmail 74793 invoked by uid 500); 29 Aug 2014 02:13:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74725 invoked by uid 500); 29 Aug 2014 02:13:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74710 invoked by uid 99); 29 Aug 2014 02:13:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 02:13:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 02:12:42 +0000
Received: by mail-ob0-f176.google.com with SMTP id wn1so1320435obc.7
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 19:12:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=n/bPuQDREK6scW0ISAbmMnm7L7UHQF/xqaBvxn6mie0=;
        b=ZVmnRIAhqmR89iHiNSuO85izXMU3GUQ1v+7iXXalxYdMlTWUXQT4O0Xas/rCz4l3Q3
         tPIPYo4BhSeWQ0bqh7txYpR+IdOoAkrWg23g6yGpnFdUrHgF5ECuymoXzmrcjrh3qZR/
         SeXZTuscnX6tbOclZ6GG2eyMAHjAjBWuFfelxUexvnJVBkIVyT/7dg+tGfgLS0AjUJvT
         2wjGtb530KASgLGMuHPM+enU7/MqTcbi2RjPaYVnJtrXIR9KQu2138Uht2xB+XQYWhJt
         aSYSJ5B7qL552tmfOzC2Op4gHB+zEXG7Wx0VEn2IxmDyzbkuLlggsbFJexhusxpaBy35
         H8Ow==
MIME-Version: 1.0
X-Received: by 10.182.186.73 with SMTP id fi9mr7760151obc.0.1409278360712;
 Thu, 28 Aug 2014 19:12:40 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 19:12:40 -0700 (PDT)
In-Reply-To: <CAJiQeYL1Ht95fQ8D+pLzrVRNgFq+O+4PVTELoVrWu0aD+tMaAQ@mail.gmail.com>
References: <CABPQxstFxsSyQ3abxYwtD4OCnBv3ShsTvsQCXRY00ivD8f3D1w@mail.gmail.com>
	<CAJiQeYJErGeKP_91c4AAFhHXQQcQK14+-K3=6uZBbmO7sJ7RtA@mail.gmail.com>
	<CABPQxsvxVojEMBQk0wfohy-4FjeaYuSuXcCJYDDrYmTTe89TwQ@mail.gmail.com>
	<CAJiQeYL1Ht95fQ8D+pLzrVRNgFq+O+4PVTELoVrWu0aD+tMaAQ@mail.gmail.com>
Date: Thu, 28 Aug 2014 19:12:40 -0700
Message-ID: <CABPQxstAqoo3NUATQu2jKNveUfq1-eoGiW98OHcwxGQPrRrAAQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Okay I'm cancelling this vote in favor of RC2.

On Thu, Aug 28, 2014 at 3:27 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Thanks for being on top of this Patrick ! And apologies for not being able
> to help more.
>
> Regards,
> Mridul
>
> On Aug 29, 2014 1:30 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>
>> Mridul - thanks for sending this along and for the debugging comments
>> on the JIRA. I think we have a handle on the issue and we'll patch it
>> and spin a new RC. We can also update the test coverage to cover LZ4.
>>
>> - Patrick
>>
>> On Thu, Aug 28, 2014 at 9:27 AM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>> > Is SPARK-3277 applicable to 1.1 ?
>> > If yes, until it is fixed, I am -1 on the release (I am on break, so
>> > can't
>> > verify or help fix, sorry).
>> >
>> > Regards
>> > Mridul
>> >
>> > On 28-Aug-2014 9:33 pm, "Patrick Wendell" <pwendell@gmail.com> wrote:
>> >>
>> >> Please vote on releasing the following candidate as Apache Spark
>> >> version
>> >> 1.1.0!
>> >>
>> >> The tag to be voted on is v1.1.0-rc1 (commit f0718324):
>> >>
>> >>
>> >> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585
>> >>
>> >> The release files, including signatures, digests, etc. can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.1.0-rc1/
>> >>
>> >> Release artifacts are signed with the following key:
>> >> https://people.apache.org/keys/committer/pwendell.asc
>> >>
>> >> The staging repository for this release can be found at:
>> >> https://repository.apache.org/content/repositories/orgapachespark-1028/
>> >>
>> >> The documentation corresponding to this release can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/
>> >>
>> >> Please vote on releasing this package as Apache Spark 1.1.0!
>> >>
>> >> The vote is open until Sunday, August 31, at 17:00 UTC and passes if
>> >> a majority of at least 3 +1 PMC votes are cast.
>> >>
>> >> [ ] +1 Release this package as Apache Spark 1.1.0
>> >> [ ] -1 Do not release this package because ...
>> >>
>> >> To learn more about Apache Spark, please see
>> >> http://spark.apache.org/
>> >>
>> >> == What justifies a -1 vote for this release? ==
>> >> This vote is happening very late into the QA period compared with
>> >> previous votes, so -1 votes should only occur for significant
>> >> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>> >> this release.
>> >>
>> >> == What default changes should I be aware of? ==
>> >> 1. The default value of "spark.io.compression.codec" is now "snappy"
>> >> --> Old behavior can be restored by switching to "lzf"
>> >>
>> >> 2. PySpark now performs external spilling during aggregations.
>> >> --> Old behavior can be restored by setting "spark.shuffle.spill" to
>> >> "false".
>> >>
>> >> I'll send a bit more later today with feature information for the
>> >> release. In the mean time I want to put this out there for
>> >> consideration.
>> >>
>> >> - Patrick
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 02:14:57 2014
Return-Path: <dev-return-9142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8497D11DBD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 02:14:57 +0000 (UTC)
Received: (qmail 76943 invoked by uid 500); 29 Aug 2014 02:14:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76882 invoked by uid 500); 29 Aug 2014 02:14:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76870 invoked by uid 99); 29 Aug 2014 02:14:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 02:14:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 02:14:30 +0000
Received: by mail-oa0-f50.google.com with SMTP id o6so1297597oag.37
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 19:14:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=6Kj4EoZP2cjcOHOtlAZA4vIgCkVgVZ0drraeUDxgA2E=;
        b=Wk93KXZaZUr8voUNcXsG3tgsINrkxUPxnmn2485RM4f4vp0v1hT5s4R5yhMIYNgiFc
         q+zqSVz+0aHG39n57drT5AtSKWS3M47i0O5Ly5eBcCSv58hITCJGe+/fGUFIUgmzvbFf
         wqVbvznrClugLcCdypJZs2oAFTAcMXOLEUkjalk5kdCAXqq8p1UwgxfBtAEf5/1zrZiT
         FHkWmnlDVGvrQf+cR2iiGaJGNhW0OQYk90Qj8PJqCqndfsm4MCGAdW6pIowhcu8a3rZ6
         YlcAwi9eRy/Xhy0zvzQuWdTzNawx5O8sfDTzRKxLaRYlaiKHtHCaxOefHCkxlY+Kc/6+
         UWnQ==
MIME-Version: 1.0
X-Received: by 10.182.29.200 with SMTP id m8mr7581486obh.13.1409278468798;
 Thu, 28 Aug 2014 19:14:28 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 19:14:28 -0700 (PDT)
Date: Thu, 28 Aug 2014 19:14:28 -0700
Message-ID: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1029/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Monday, September 01, at 03:11 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== Regressions fixed since RC1 ==
LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of "spark.io.compression.codec" is now "snappy"
--> Old behavior can be restored by switching to "lzf"

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting "spark.shuffle.spill" to "false".

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 03:32:39 2014
Return-Path: <dev-return-9143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 23ED511FE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 03:32:39 +0000 (UTC)
Received: (qmail 87621 invoked by uid 500); 29 Aug 2014 03:32:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87555 invoked by uid 500); 29 Aug 2014 03:32:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87541 invoked by uid 99); 29 Aug 2014 03:32:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 03:32:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 03:32:32 +0000
Received: by mail-ob0-f178.google.com with SMTP id uy5so1368405obc.23
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 20:32:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=w76pjz/3qjMOrd2mrDJid4x78mdYDEJ9g3iD+JVd2zk=;
        b=mWPn1uscqRVor8k4FCYXRPA78QLi+UhRwCHHiqnBp20d0Za3UrJsggz5tYtTHJOlPs
         nezyXJQiKZ+shHMvxmo7O2neXbYX9896ed1TuZKVbb9GWuiEOeAz7iUdHpCLr5VnEOBi
         g2D6Rz6Ln+QTqYrYANl59SiTZGHzROnftnM9lDVZH6DQ6bsPX0zOnAIItIzs3SvnGo8H
         IJUoSSEwTcFrVP0EWHYZv/VHDvuZJa10RYFuUpd4r9CQBHZR8GVFkaqoOyepUIi+4//4
         boFLiCwcpeuEJDkZqx2LSjnvIBaPmUYzLnIVz8D2WcobWyDDemmzkwTrem1yFeyVwB9t
         /WYQ==
MIME-Version: 1.0
X-Received: by 10.60.63.201 with SMTP id i9mr785739oes.8.1409283131831; Thu,
 28 Aug 2014 20:32:11 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 20:32:11 -0700 (PDT)
In-Reply-To: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
Date: Thu, 28 Aug 2014 20:32:11 -0700
Message-ID: <CABPQxsvcuQMigktFQDq_dPefpvQX1ow7Lm5RyAGVP_uZy=D0QQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll kick off the vote with a +1.

On Thu, Aug 28, 2014 at 7:14 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>
> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1029/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Monday, September 01, at 03:11 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == Regressions fixed since RC1 ==
> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 03:54:04 2014
Return-Path: <dev-return-9144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C73311031
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 03:54:04 +0000 (UTC)
Received: (qmail 10992 invoked by uid 500); 29 Aug 2014 03:54:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10907 invoked by uid 500); 29 Aug 2014 03:54:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10896 invoked by uid 99); 29 Aug 2014 03:54:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 03:54:02 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of byavuz@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 03:53:33 +0000
Received: from codegreen1.stanford.edu (codegreen1.Stanford.EDU [171.67.224.2])
	(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by smtp.stanford.edu (Postfix) with ESMTPS id E83DB101B46;
	Thu, 28 Aug 2014 20:53:30 -0700 (PDT)
Received: from codegreen1.stanford.edu (localhost.localdomain [127.0.0.1])
	by codegreen1.stanford.edu (Postfix) with ESMTP id D374F84;
	Thu, 28 Aug 2014 20:53:30 -0700 (PDT)
Received: from smtp.stanford.edu (smtp2.Stanford.EDU [171.67.219.82])
	(using TLSv1 with cipher ADH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by codegreen1.stanford.edu (Postfix) with ESMTP id C7EFB84;
	Thu, 28 Aug 2014 20:53:30 -0700 (PDT)
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id B4DA7342326;
	Thu, 28 Aug 2014 20:53:30 -0700 (PDT)
Received: from zm01.stanford.edu (zm01.Stanford.EDU [171.67.219.145])
	by smtp.stanford.edu (Postfix) with ESMTP id 71631342307;
	Thu, 28 Aug 2014 20:53:30 -0700 (PDT)
Date: Thu, 28 Aug 2014 20:53:29 -0700 (PDT)
From: Burak Yavuz <byavuz@stanford.edu>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev@spark.apache.org
Message-ID: <1335816532.5019537.1409284409982.JavaMail.zimbra@stanford.edu>
In-Reply-To: <CABPQxsvcuQMigktFQDq_dPefpvQX1ow7Lm5RyAGVP_uZy=D0QQ@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com> <CABPQxsvcuQMigktFQDq_dPefpvQX1ow7Lm5RyAGVP_uZy=D0QQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [184.33.98.218]
X-Mailer: Zimbra 8.0.7_GA_6021 (ZimbraWebClient - GC36 (Mac)/8.0.7_GA_6021)
X-Authenticated-User: byavuz@stanford.edu
Thread-Topic: Release Apache Spark 1.1.0 (RC2)
Thread-Index: IRK2bKSCBRETbmxv+DaOYFFDMu3pOQ==
X-Virus-Checked: Checked by ClamAV on apache.org

+1. Tested MLlib algorithms on Amazon EC2, algorithms show speed-ups between 1.5-5x compared to the 1.0.2 release.

----- Original Message -----
From: "Patrick Wendell" <pwendell@gmail.com>
To: dev@spark.apache.org
Sent: Thursday, August 28, 2014 8:32:11 PM
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)

I'll kick off the vote with a +1.

On Thu, Aug 28, 2014 at 7:14 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>
> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1029/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Monday, September 01, at 03:11 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == Regressions fixed since RC1 ==
> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 04:27:44 2014
Return-Path: <dev-return-9145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49B0C1109A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 04:27:44 +0000 (UTC)
Received: (qmail 63331 invoked by uid 500); 29 Aug 2014 04:27:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63264 invoked by uid 500); 29 Aug 2014 04:27:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63242 invoked by uid 99); 29 Aug 2014 04:27:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 04:27:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tnachen@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 04:27:17 +0000
Received: by mail-ob0-f177.google.com with SMTP id wo20so1385404obc.22
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 21:27:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=oQo0gt3m4r5NjFdjRpgBXPjsN0c6bjmOFwIx2oy0wHU=;
        b=l5MGGxvSYIRjDYbn9djGMKQ3wQdDcq+Xsf1YRJh2WTbwzE6SQ3abBsTQFpEsf06jKZ
         zI8i+Hc3vTRBsCHJ97e/2KBtANDCHpyXjuLvLnujgkVSGcXPp8pRqo98dgD7GBqUYMX5
         h+JhS3BtrSjZGovvw5JMn3bAAYhCByhv9CSn+EDYhRxhK/MRpRCg7ZZ9HQppC66s1vAo
         ysausUeiA4lvn2sq5KiF60udS6tZMqSZdzz77mc+G/bteNr0tVpfL2+StpIXy5ufR7c3
         nJk09TbG6chiEm9cE44nwE7XjoDn+aFz6PJTrYm+3X5M/CY4XtM7lhLY1lwuYi7V/tXa
         5LHw==
MIME-Version: 1.0
X-Received: by 10.182.32.5 with SMTP id e5mr6676078obi.73.1409286436221; Thu,
 28 Aug 2014 21:27:16 -0700 (PDT)
Received: by 10.60.37.4 with HTTP; Thu, 28 Aug 2014 21:27:16 -0700 (PDT)
In-Reply-To: <1335816532.5019537.1409284409982.JavaMail.zimbra@stanford.edu>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CABPQxsvcuQMigktFQDq_dPefpvQX1ow7Lm5RyAGVP_uZy=D0QQ@mail.gmail.com>
	<1335816532.5019537.1409284409982.JavaMail.zimbra@stanford.edu>
Date: Thu, 28 Aug 2014 21:27:16 -0700
Message-ID: <CAFx0iW_9Pv8XTznZ=5hwdZcR6xLh-BKWD9pnZNvW0TqSZGx17Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Timothy Chen <tnachen@gmail.com>
To: Burak Yavuz <byavuz@stanford.edu>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 Make-distrubtion works, and also tested simple spark jobs on Spark
on Mesos on 8 node Mesos cluster.

Tim

On Thu, Aug 28, 2014 at 8:53 PM, Burak Yavuz <byavuz@stanford.edu> wrote:
> +1. Tested MLlib algorithms on Amazon EC2, algorithms show speed-ups between 1.5-5x compared to the 1.0.2 release.
>
> ----- Original Message -----
> From: "Patrick Wendell" <pwendell@gmail.com>
> To: dev@spark.apache.org
> Sent: Thursday, August 28, 2014 8:32:11 PM
> Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
>
> I'll kick off the vote with a +1.
>
> On Thu, Aug 28, 2014 at 7:14 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>>
>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1029/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.1.0!
>>
>> The vote is open until Monday, September 01, at 03:11 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == Regressions fixed since RC1 ==
>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>> this release.
>>
>> == What default changes should I be aware of? ==
>> 1. The default value of "spark.io.compression.codec" is now "snappy"
>> --> Old behavior can be restored by switching to "lzf"
>>
>> 2. PySpark now performs external spilling during aggregations.
>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 04:58:45 2014
Return-Path: <dev-return-9146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CC131110A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 04:58:45 +0000 (UTC)
Received: (qmail 4330 invoked by uid 500); 29 Aug 2014 04:58:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4268 invoked by uid 500); 29 Aug 2014 04:58:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4256 invoked by uid 99); 29 Aug 2014 04:58:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 04:58:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 04:58:40 +0000
Received: by mail-qg0-f43.google.com with SMTP id f51so1781217qge.2
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 21:58:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZUFSYFeeB2EbVQBIKpKGZRrmMHpFhX7JYvt8HvpLMmU=;
        b=pk/FeTvWM1pF9f8/pXWCDxhLWmLuwZUmlF1hp2ZH9jJVApTmbzHuEDQizQNbLqWWR/
         CXeAdL8Mm6F8X09zn90byshTbktRGe2cz8svcQPiT8hvXv/n9r6HklI0Y5lquTwbsGLU
         QHreK5TIwrIaYO2SpDqYvMl2ViUmTU8KPt+gMUTiOY4Re16ffeAkqfv/sQoy5GLb/orZ
         PW32k35+cBf8/KXOo5HzfGwEudTeug9y0Fjgjwj9yLGvQ3sm9yWvYbNjx99R/mvzuN8N
         Wt+UtZPmJLnV7WwlLnIrdTdiH1xQ7kCaVy/Rt50ScxvuLDXv0EVP8AsB5s8if0Y5XP1e
         8RRg==
X-Received: by 10.224.26.146 with SMTP id e18mr14163960qac.13.1409288299456;
 Thu, 28 Aug 2014 21:58:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Thu, 28 Aug 2014 21:57:59 -0700 (PDT)
In-Reply-To: <CAFx0iW_9Pv8XTznZ=5hwdZcR6xLh-BKWD9pnZNvW0TqSZGx17Q@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CABPQxsvcuQMigktFQDq_dPefpvQX1ow7Lm5RyAGVP_uZy=D0QQ@mail.gmail.com>
 <1335816532.5019537.1409284409982.JavaMail.zimbra@stanford.edu> <CAFx0iW_9Pv8XTznZ=5hwdZcR6xLh-BKWD9pnZNvW0TqSZGx17Q@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Thu, 28 Aug 2014 21:57:59 -0700
Message-ID: <CAA_qdLr4BZs30vhggQkH-xM7pNxKQv3thdG9Fz_t4N01vQKoXA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Timothy Chen <tnachen@gmail.com>
Cc: Burak Yavuz <byavuz@stanford.edu>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0153751079ad800501bd8001
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0153751079ad800501bd8001
Content-Type: text/plain; charset=UTF-8

+1. Tested Spark SQL Thrift server and CLI against a single node standalone
cluster.


On Thu, Aug 28, 2014 at 9:27 PM, Timothy Chen <tnachen@gmail.com> wrote:

> +1 Make-distrubtion works, and also tested simple spark jobs on Spark
> on Mesos on 8 node Mesos cluster.
>
> Tim
>
> On Thu, Aug 28, 2014 at 8:53 PM, Burak Yavuz <byavuz@stanford.edu> wrote:
> > +1. Tested MLlib algorithms on Amazon EC2, algorithms show speed-ups
> between 1.5-5x compared to the 1.0.2 release.
> >
> > ----- Original Message -----
> > From: "Patrick Wendell" <pwendell@gmail.com>
> > To: dev@spark.apache.org
> > Sent: Thursday, August 28, 2014 8:32:11 PM
> > Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
> >
> > I'll kick off the vote with a +1.
> >
> > On Thu, Aug 28, 2014 at 7:14 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.1.0!
> >>
> >> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1029/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.1.0!
> >>
> >> The vote is open until Monday, September 01, at 03:11 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.1.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == Regressions fixed since RC1 ==
> >> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening very late into the QA period compared with
> >> previous votes, so -1 votes should only occur for significant
> >> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> >> this release.
> >>
> >> == What default changes should I be aware of? ==
> >> 1. The default value of "spark.io.compression.codec" is now "snappy"
> >> --> Old behavior can be restored by switching to "lzf"
> >>
> >> 2. PySpark now performs external spilling during aggregations.
> >> --> Old behavior can be restored by setting "spark.shuffle.spill" to
> "false".
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e0153751079ad800501bd8001--

From dev-return-9147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:05:44 2014
Return-Path: <dev-return-9147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 164A21124D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:05:44 +0000 (UTC)
Received: (qmail 84570 invoked by uid 500); 29 Aug 2014 06:05:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84504 invoked by uid 500); 29 Aug 2014 06:05:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84493 invoked by uid 99); 29 Aug 2014 06:05:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:05:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:05:17 +0000
Received: by mail-ie0-f175.google.com with SMTP id y20so2167857ier.6
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:05:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZIGxAKPcsX3J6XNJJ3XY8F2o/p3IAZPXCXNBOd2/Sb8=;
        b=W5Upn3GAJNhZF1WhM97qX64DZkMr9oSz2/mCdprpBreiLtMebeujZq46kARvo/Tx8/
         J1DUmQ/4PZaNgldhVSvJhYAX2fEYh3Xw/fEiA5FzuRtFKvvUj/HB0eLxOpWcSRs0KkVD
         tsUZvGp74eR/zMcpTNZleE5hDYGT5kRNiqc5qvM1cgCJR7wQ1MJaKaPIgN6EgxEoD5tI
         bNPvXuHtqb3G4TlE8UYJk7GF76E0EYwFxFHtXb/qWGlylOjWluwyoAg1rBbo2EUXQn/5
         jfTzcojrEzCMwjcVDrxc2K6mQZIV6CwsBYptaQSJB4wUifTVzeWPNmRWGsLqdKCqBiGa
         Q4iQ==
X-Gm-Message-State: ALoCoQlDfkEMoO7Th6cYo8D6QbwOM5LOi2DeaOwEQuN06KBZ8x73Xxh9/ybHd3Osz1O6Kr5jrdLC
X-Received: by 10.50.50.198 with SMTP id e6mr1710150igo.1.1409292316080; Thu,
 28 Aug 2014 23:05:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Thu, 28 Aug 2014 23:04:56 -0700 (PDT)
In-Reply-To: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 29 Aug 2014 07:04:56 +0100
Message-ID: <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 I tested the source and Hadoop 2.4 release. Checksums and
signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
fail any more than usual.

FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
project and have encountered no problems.


I notice that the 1.1.0 release removes the CDH4-specific build, but
adds two MapR-specific builds. Compare with
https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
commented on the commit:
https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc

I'm in favor of removing all vendor-specific builds. This change
*looks* a bit funny as there was no JIRA (?) and appears to swap one
vendor for another. Of course there's nothing untoward going on, but
what was the reasoning? It's best avoided, and MapR already
distributes Spark just fine, no?

This is a gray area with ASF projects. I mention it as well because it
came up with Apache Flink recently
(http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E)
Another vendor rightly noted this could look like favoritism. They
changed to remove vendor releases.

On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>
> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1029/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Monday, September 01, at 03:11 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == Regressions fixed since RC1 ==
> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:27:08 2014
Return-Path: <dev-return-9148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F2A8D112C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:27:08 +0000 (UTC)
Received: (qmail 34963 invoked by uid 500); 29 Aug 2014 06:27:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34895 invoked by uid 500); 29 Aug 2014 06:27:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34883 invoked by uid 99); 29 Aug 2014 06:27:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:27:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:27:03 +0000
Received: by mail-oa0-f46.google.com with SMTP id m19so1446515oag.33
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:26:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=vpSDhH9ju1crVBEiCi2ab51b4hLthjiE0JHVyG/IO5Q=;
        b=BtiWqnOBJcGlxbRWt1QLsg9wxCRCGDGGSTWtudPlPYu+SvUgnGm3CJOUDBTrhrVlY+
         t5tSu45gt1RfcywksT+78CgpAU5RIBhld074jgfNuiRj1Hh+tZno/oOeD1YQTvv6t4Oj
         ht6U9f8uXYZNrMxzNXXS1YdFH9nbxa5l8nZsPxg4VjjI0Esf7i8x2xxIKP0+8chtNytW
         54INh45G8xPa5S9m2ys5rvh53muLwUrW/KpY2nk5qvwQlG/AjFOp9Eg8IWZqN2KgANvm
         78tZfxE9pbMxPYSGSwh63VmT7ryPlCGREGbQ8PrGfosVZezNZTPZCk5n4shZrtFAy/LQ
         8Fqg==
MIME-Version: 1.0
X-Received: by 10.182.114.169 with SMTP id jh9mr8377692obb.25.1409293602878;
 Thu, 28 Aug 2014 23:26:42 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 23:26:42 -0700 (PDT)
In-Reply-To: <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
Date: Thu, 28 Aug 2014 23:26:42 -0700
Message-ID: <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

The reason there are no longer CDH-specific builds is that all newer
versions of CDH and HDP work with builds for the upstream Hadoop
projects. I dropped CDH4 in favor of a  newer Hadoop version (2.4) and
the Hadoop-without-Hive (also 2.4) build.

For MapR - we can't officially post those artifacts on ASF web space
when we make the final release, we can only link to them as being
hosted by MapR specifically since they use non-compatible licenses.
However, I felt that providing these during a testing period was
alright, with the goal of increasing test coverage. I couldn't find
any policy against posting these on personal web space during RC
voting. However, we can remove them if there is one.

Dropping CDH4 was more because it is now pretty old, but we can add it
back if people want. The binary packaging is a slightly separate
question from release votes, so I can always add more binary packages
whenever. And on this, my main concern is covering the most popular
Hadoop versions to lower the bar for users to build and test Spark.

- Patrick

On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com> wrote:
> +1 I tested the source and Hadoop 2.4 release. Checksums and
> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
> fail any more than usual.
>
> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
> project and have encountered no problems.
>
>
> I notice that the 1.1.0 release removes the CDH4-specific build, but
> adds two MapR-specific builds. Compare with
> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
> commented on the commit:
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc
>
> I'm in favor of removing all vendor-specific builds. This change
> *looks* a bit funny as there was no JIRA (?) and appears to swap one
> vendor for another. Of course there's nothing untoward going on, but
> what was the reasoning? It's best avoided, and MapR already
> distributes Spark just fine, no?
>
> This is a gray area with ASF projects. I mention it as well because it
> came up with Apache Flink recently
> (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E)
> Another vendor rightly noted this could look like favoritism. They
> changed to remove vendor releases.
>
> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>>
>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1029/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.1.0!
>>
>> The vote is open until Monday, September 01, at 03:11 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == Regressions fixed since RC1 ==
>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>> this release.
>>
>> == What default changes should I be aware of? ==
>> 1. The default value of "spark.io.compression.codec" is now "snappy"
>> --> Old behavior can be restored by switching to "lzf"
>>
>> 2. PySpark now performs external spilling during aggregations.
>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:31:08 2014
Return-Path: <dev-return-9149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0401112E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:31:08 +0000 (UTC)
Received: (qmail 46122 invoked by uid 500); 29 Aug 2014 06:31:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46051 invoked by uid 500); 29 Aug 2014 06:31:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46033 invoked by uid 99); 29 Aug 2014 06:31:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:31:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:31:03 +0000
Received: by mail-ig0-f179.google.com with SMTP id r2so2134422igi.0
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:30:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/041gd4EiK8ucZWPu/8CH9epgGvcBvH1ScYwoDWwLU4=;
        b=NJjoOkz9RDWbMDAcV+kvmY4FWNz5Z0IMj4WTuSLeyQtyPXXhITQjnwvagnCsAKPuSA
         PvDrm/BQBuKn7CA7OCMhtRG5IoHAb33ulnNdbGTvWCRSfJX5E1SX0/cZb2wB1IGIy72O
         J93mnC7MXQyzFXpDUeMou0Y3U1Dj8ks6ZzJgZ6poJHC3YRI2hL9ci9DYjocJt5/Qh7HN
         lqGsnHYQvQpMPc0o7AKPvsxgltvI3v2ng4flRlfKDZEpWVhsTNy+BVTzzVf/hz+2OCDG
         ebTwn/A3eKYj8Vbf6FDRo93q5nf2Fl8Hg2IQLRzYDIdN13krPsVDfwz0rhlDDqrrWbKP
         /fYw==
X-Gm-Message-State: ALoCoQl6MZhLGC8/vwKNyKxNOong6MB29PuBDM+n1trf4jdEqScqT4D+iERja8LvDN7fKxeeoPGE
X-Received: by 10.50.117.106 with SMTP id kd10mr1781983igb.5.1409293842915;
 Thu, 28 Aug 2014 23:30:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Thu, 28 Aug 2014 23:30:21 -0700 (PDT)
In-Reply-To: <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com> <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 29 Aug 2014 07:30:21 +0100
Message-ID: <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

(Copying my reply since I don't know if it goes to the mailing list)

Great, thanks for explaining the reasoning. You're saying these aren't
going into the final release? I think that moots any issue surrounding
distributing them then.

This is all I know of from the ASF:
https://community.apache.org/projectIndependence.html I don't read it
as expressly forbidding this kind of thing although you can see how it
bumps up against the spirit. There's not a bright line -- what about
Tomcat providing binaries compiled for Windows for example? does that
favor an OS vendor?

>From this technical ASF perspective only the releases matter -- do
what you want with snapshots and RCs. The only issue there is maybe
releasing something different than was in the RC; is that at all
confusing? Just needs a note.

I think this theoretical issue doesn't exist if these binaries aren't
released, so I see no reason to not proceed.

The rest is a different question about whether you want to spend time
maintaining this profile and candidate. The vendor already manages
their build I think and -- and I don't know -- may even prefer not to
have a different special build floating around. There's also the
theoretical argument that this turns off other vendors from adopting
Spark if it's perceived to be too connected to other vendors. I'd like
to maximize Spark's distribution and there's some argument you do this
by not making vendor profiles. But as I say a different question to
just think about over time...

(oh and PS for my part I think it's a good thing that CDH4 binaries
were removed. I wasn't arguing for resurrecting them)

On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Sean,
>
> The reason there are no longer CDH-specific builds is that all newer
> versions of CDH and HDP work with builds for the upstream Hadoop
> projects. I dropped CDH4 in favor of a  newer Hadoop version (2.4) and
> the Hadoop-without-Hive (also 2.4) build.
>
> For MapR - we can't officially post those artifacts on ASF web space
> when we make the final release, we can only link to them as being
> hosted by MapR specifically since they use non-compatible licenses.
> However, I felt that providing these during a testing period was
> alright, with the goal of increasing test coverage. I couldn't find
> any policy against posting these on personal web space during RC
> voting. However, we can remove them if there is one.
>
> Dropping CDH4 was more because it is now pretty old, but we can add it
> back if people want. The binary packaging is a slightly separate
> question from release votes, so I can always add more binary packages
> whenever. And on this, my main concern is covering the most popular
> Hadoop versions to lower the bar for users to build and test Spark.
>
> - Patrick
>
> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com> wrote:
>> +1 I tested the source and Hadoop 2.4 release. Checksums and
>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
>> fail any more than usual.
>>
>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
>> project and have encountered no problems.
>>
>>
>> I notice that the 1.1.0 release removes the CDH4-specific build, but
>> adds two MapR-specific builds. Compare with
>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
>> commented on the commit:
>> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc
>>
>> I'm in favor of removing all vendor-specific builds. This change
>> *looks* a bit funny as there was no JIRA (?) and appears to swap one
>> vendor for another. Of course there's nothing untoward going on, but
>> what was the reasoning? It's best avoided, and MapR already
>> distributes Spark just fine, no?
>>
>> This is a gray area with ASF projects. I mention it as well because it
>> came up with Apache Flink recently
>> (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E)
>> Another vendor rightly noted this could look like favoritism. They
>> changed to remove vendor releases.
>>
>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>>>
>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>>>
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>>>
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1029/
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>>>
>>> Please vote on releasing this package as Apache Spark 1.1.0!
>>>
>>> The vote is open until Monday, September 01, at 03:11 UTC and passes if
>>> a majority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 1.1.0
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> == Regressions fixed since RC1 ==
>>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>>>
>>> == What justifies a -1 vote for this release? ==
>>> This vote is happening very late into the QA period compared with
>>> previous votes, so -1 votes should only occur for significant
>>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>>> this release.
>>>
>>> == What default changes should I be aware of? ==
>>> 1. The default value of "spark.io.compression.codec" is now "snappy"
>>> --> Old behavior can be restored by switching to "lzf"
>>>
>>> 2. PySpark now performs external spilling during aggregations.
>>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:31:57 2014
Return-Path: <dev-return-9150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B80F112F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:31:57 +0000 (UTC)
Received: (qmail 49853 invoked by uid 500); 29 Aug 2014 06:31:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49782 invoked by uid 500); 29 Aug 2014 06:31:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49201 invoked by uid 99); 29 Aug 2014 06:31:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:31:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of velvia.github@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:31:50 +0000
Received: by mail-wg0-f43.google.com with SMTP id a1so1695904wgh.26
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:31:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+V/SdXP8YuTbG63xe+mPKJ4Dpe/gTjQgxA53Sqq0fFk=;
        b=bMoPojg0w9NWGxA7ZaGr5WQvhcH0cK8y13bGUCsX0R4CfLTZq+HK5C+q1zaN5FsgbK
         WmFc4PADqDMQVmLUeVBGnwz1wkRNEvgAr6KSWWJYkoOe/D+RuA+0JrfKFRFgaZwGOqxR
         1/KA7+o80DdVIfgpcp7gUCAvKJJGxgazIolwvBwQSqp7Rd3vvLhIT1Jl6oMe60OVRpIf
         XKn9ff8MnN4Qlr0nUmQJPGw6A2O4dEwFmgABliGhBnvr22Fbe+ovKb21QHVPyp94qJVq
         85sXZGLudzzCP7FFDwQyFHRCxvNaR0c48IHd3lpCQiT0T5YEnyrVHKKFHv6uxSmL93U5
         7htQ==
MIME-Version: 1.0
X-Received: by 10.194.94.165 with SMTP id dd5mr10835975wjb.75.1409293889311;
 Thu, 28 Aug 2014 23:31:29 -0700 (PDT)
Received: by 10.216.33.10 with HTTP; Thu, 28 Aug 2014 23:31:29 -0700 (PDT)
In-Reply-To: <CAAswR-60n8cj5WU47TNw-AvmVBAmdveb7XEu4YmjyTjqbR=hCg@mail.gmail.com>
References: <CAN6Vra27gzyifyrVVTnoGddRmysN-rnfCHBTDCT1KuX-ZcOjPw@mail.gmail.com>
	<CAAswR-43krAdVUDZ3ZLod1qFWYzAsZhQBv87s17vuD2Sc4A6WA@mail.gmail.com>
	<CAN6Vra0oSCPAr14mO3RfuPz5D1WnB1C8xFEcUmCuoqSiLzrGHw@mail.gmail.com>
	<CAAswR-60n8cj5WU47TNw-AvmVBAmdveb7XEu4YmjyTjqbR=hCg@mail.gmail.com>
Date: Thu, 28 Aug 2014 23:31:29 -0700
Message-ID: <CAN6Vra0xEZV7XYpCqWYgJABC2CHLtau423yc69_=WVwPXAMV3A@mail.gmail.com>
Subject: Re: [Spark SQL] off-heap columnar store
From: Evan Chan <velvia.github@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

>
>> The reason I'm asking about the columnar compressed format is that
>> there are some problems for which Parquet is not practical.
>
>
> Can you elaborate?

Sure.

- Organization or co has no Hadoop, but significant investment in some
other NoSQL store.
- Need to efficiently add a new column to existing data
- Need to mark some existing rows as deleted or replace small bits of
existing data

For these use cases, it would be much more efficient and practical if
we didn't have to take the origin of the data from the datastore,
convert it to Parquet first.  Doing so loses significant latency and
causes Ops headaches in having to maintain HDFS.     It would be great
to be able to load data directly into the columnar format, into the
InMemoryColumnarCache.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:40:13 2014
Return-Path: <dev-return-9151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D80B611327
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:40:13 +0000 (UTC)
Received: (qmail 76737 invoked by uid 500); 29 Aug 2014 06:40:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76674 invoked by uid 500); 29 Aug 2014 06:40:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76662 invoked by uid 99); 29 Aug 2014 06:40:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:40:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:39:46 +0000
Received: by mail-pa0-f44.google.com with SMTP id rd3so5728944pab.17
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:39:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=FM/FpAEoDFx3oM7SLV4+EMEprwdJ5Nb4V65LLK243Lo=;
        b=eAvHLsNyNq9vyN0BnfHNgCg90mfkD7Ac0A4vyQQRhTvzl1qu4XQz7WIV/ubjF45UdL
         wLRd134I2FSAWU1+1KrwVbgOBo7DkQfV6NeDcpTL7fWVaABZBqQ6G/QxljlIRqGzlYUC
         med885bS/Gn30wQ3qY2MnkwvOkQsoETBacFlf1GjX/pPiZMRIKT+oTeskpE2+0Stduf/
         8yzIPcdj1wN0xPcWcpukPfMvblsEd5uZOgkVK2+LVfg/gaEwaF3fjH3/zFf3ewX7Pq5p
         cXDAR2c+JbRW38LFEkkpC80BJu6E4VssQ4reIGEWbZ+zFNIs86tsHyv7PYpaBeXj+gQy
         3QdQ==
X-Received: by 10.70.102.175 with SMTP id fp15mr12807829pdb.52.1409294384487;
        Thu, 28 Aug 2014 23:39:44 -0700 (PDT)
Received: from mbp-3.local (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id cx2sm5501703pbb.52.2014.08.28.23.39.43
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 28 Aug 2014 23:39:43 -0700 (PDT)
Date: Thu, 28 Aug 2014 23:39:42 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Sean Owen <sowen@cloudera.com>, Patrick Wendell
 <pwendell@gmail.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.5400202e.579478fe.470a@mbp-3.local>
In-Reply-To: <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5400202e_749abb43_470a"
X-Virus-Checked: Checked by ClamAV on apache.org

--5400202e_749abb43_470a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Personally I'd actually consider putting CDH4 back if there are still users on it. It's always better to be inclusive, and the convenience of a one-click download is high. Do we have a sense on what % of CDH users still use CDH4?

Matei

On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com) wrote:

(Copying my reply since I don't know if it goes to the mailing list) 

Great, thanks for explaining the reasoning. You're saying these aren't 
going into the final release? I think that moots any issue surrounding 
distributing them then. 

This is all I know of from the ASF: 
https://community.apache.org/projectIndependence.html I don't read it 
as expressly forbidding this kind of thing although you can see how it 
bumps up against the spirit. There's not a bright line -- what about 
Tomcat providing binaries compiled for Windows for example? does that 
favor an OS vendor? 

>From this technical ASF perspective only the releases matter -- do 
what you want with snapshots and RCs. The only issue there is maybe 
releasing something different than was in the RC; is that at all 
confusing? Just needs a note. 

I think this theoretical issue doesn't exist if these binaries aren't 
released, so I see no reason to not proceed. 

The rest is a different question about whether you want to spend time 
maintaining this profile and candidate. The vendor already manages 
their build I think and -- and I don't know -- may even prefer not to 
have a different special build floating around. There's also the 
theoretical argument that this turns off other vendors from adopting 
Spark if it's perceived to be too connected to other vendors. I'd like 
to maximize Spark's distribution and there's some argument you do this 
by not making vendor profiles. But as I say a different question to 
just think about over time... 

(oh and PS for my part I think it's a good thing that CDH4 binaries 
were removed. I wasn't arguing for resurrecting them) 

On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com> wrote: 
> Hey Sean, 
> 
> The reason there are no longer CDH-specific builds is that all newer 
> versions of CDH and HDP work with builds for the upstream Hadoop 
> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4) and 
> the Hadoop-without-Hive (also 2.4) build. 
> 
> For MapR - we can't officially post those artifacts on ASF web space 
> when we make the final release, we can only link to them as being 
> hosted by MapR specifically since they use non-compatible licenses. 
> However, I felt that providing these during a testing period was 
> alright, with the goal of increasing test coverage. I couldn't find 
> any policy against posting these on personal web space during RC 
> voting. However, we can remove them if there is one. 
> 
> Dropping CDH4 was more because it is now pretty old, but we can add it 
> back if people want. The binary packaging is a slightly separate 
> question from release votes, so I can always add more binary packages 
> whenever. And on this, my main concern is covering the most popular 
> Hadoop versions to lower the bar for users to build and test Spark. 
> 
> - Patrick 
> 
> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com> wrote: 
>> +1 I tested the source and Hadoop 2.4 release. Checksums and 
>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't 
>> fail any more than usual. 
>> 
>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another 
>> project and have encountered no problems. 
>> 
>> 
>> I notice that the 1.1.0 release removes the CDH4-specific build, but 
>> adds two MapR-specific builds. Compare with 
>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I 
>> commented on the commit: 
>> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc 
>> 
>> I'm in favor of removing all vendor-specific builds. This change 
>> *looks* a bit funny as there was no JIRA (?) and appears to swap one 
>> vendor for another. Of course there's nothing untoward going on, but 
>> what was the reasoning? It's best avoided, and MapR already 
>> distributes Spark just fine, no? 
>> 
>> This is a gray area with ASF projects. I mention it as well because it 
>> came up with Apache Flink recently 
>> (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E) 
>> Another vendor rightly noted this could look like favoritism. They 
>> changed to remove vendor releases. 
>> 
>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> wrote: 
>>> Please vote on releasing the following candidate as Apache Spark version 1.1.0! 
>>> 
>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3): 
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327 
>>> 
>>> The release files, including signatures, digests, etc. can be found at: 
>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/ 
>>> 
>>> Release artifacts are signed with the following key: 
>>> https://people.apache.org/keys/committer/pwendell.asc 
>>> 
>>> The staging repository for this release can be found at: 
>>> https://repository.apache.org/content/repositories/orgapachespark-1029/ 
>>> 
>>> The documentation corresponding to this release can be found at: 
>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/ 
>>> 
>>> Please vote on releasing this package as Apache Spark 1.1.0! 
>>> 
>>> The vote is open until Monday, September 01, at 03:11 UTC and passes if 
>>> a majority of at least 3 +1 PMC votes are cast. 
>>> 
>>> [ ] +1 Release this package as Apache Spark 1.1.0 
>>> [ ] -1 Do not release this package because ... 
>>> 
>>> To learn more about Apache Spark, please see 
>>> http://spark.apache.org/ 
>>> 
>>> == Regressions fixed since RC1 == 
>>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277 
>>> 
>>> == What justifies a -1 vote for this release? == 
>>> This vote is happening very late into the QA period compared with 
>>> previous votes, so -1 votes should only occur for significant 
>>> regressions from 1.0.2. Bugs already present in 1.0.X will not block 
>>> this release. 
>>> 
>>> == What default changes should I be aware of? == 
>>> 1. The default value of "spark.io.compression.codec" is now "snappy" 
>>> --> Old behavior can be restored by switching to "lzf" 
>>> 
>>> 2. PySpark now performs external spilling during aggregations. 
>>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false". 
>>> 
>>> --------------------------------------------------------------------- 
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
>>> For additional commands, e-mail: dev-help@spark.apache.org 
>>> 

--------------------------------------------------------------------- 
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
For additional commands, e-mail: dev-help@spark.apache.org 


--5400202e_749abb43_470a--


From dev-return-9152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:43:20 2014
Return-Path: <dev-return-9152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD0EB1133B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:43:20 +0000 (UTC)
Received: (qmail 85279 invoked by uid 500); 29 Aug 2014 06:43:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85212 invoked by uid 500); 29 Aug 2014 06:43:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85198 invoked by uid 99); 29 Aug 2014 06:43:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:43:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:43:15 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so1437411oag.21
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:42:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7oEUvuWSvxAhFDuqJlTV2CEHqkezFgJKM4NlM+/ArdE=;
        b=LgAVkmUJopmpM4XG37PmIXtfeTFbKuEl7cqb71ma10xSV2JP1kl/Ur8RY9g6JExV2X
         QhFY6m9gERwrWfZw8CMqZhdXfLMGDTSpNIc57JHYbW+6ha+3MUPDNrMRSQ2rkN4xPQcf
         m3M3Qvix8wNF/NystHLtbfJWMkPDICa8CLS04P+rmWUpLuK0zozLn/Q7U3Rf/nopKmjY
         KLoLbuarDUfLjs0BmK3thY+mjkU/HKpzsCd2N4015Z0NGnAWtOlIzGPUOLvg0+h5/F99
         FxTctZKd927K/SjGKXX5qeGTLAS90AXHcZmPNW9KhCFjf46w383BbI+Opsh/mipHrnOd
         +nFw==
MIME-Version: 1.0
X-Received: by 10.60.135.233 with SMTP id pv9mr367623oeb.75.1409294575013;
 Thu, 28 Aug 2014 23:42:55 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Thu, 28 Aug 2014 23:42:54 -0700 (PDT)
In-Reply-To: <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
Date: Thu, 28 Aug 2014 23:42:54 -0700
Message-ID: <CABPQxsuz50kcueEQEJ3ek0Mf3zmjy4q14dGMdNu6v9dHb6PGgQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, we can't/won't post MapR binaries on the ASF web space for the
release. However, I have been linking to them (at their request) with
a clear identifier that it is an incompatible license and a 3rd party
build.

The only "vendor specific" build property we provide is compatibility
with different Hadoop FileSystem clients, since unfortunately there is
not a universally adopted client/server protocol. I think our goal has
always been to provide a path for using "ASF Spark" with
vendor-specific filesystems. Some vendors perform backports or
enhancements... and this of course we would never want to manage in
the upstream project.

In terms of vendor support for this approach - In the early days
Cloudera asked us to add CDH4 repository and more recently Pivotal and
MapR also asked us to allow linking against their hadoop-client
libraries. So we've added these based on direct requests from vendors.
Given the ubiquity of the Hadoop FileSystem API, it's hard for me to
imagine ruffling feathers by supporting this. But if we get feedback
in that direction over time we can of course consider a different
approach.

- Patrick



On Thu, Aug 28, 2014 at 11:30 PM, Sean Owen <sowen@cloudera.com> wrote:
> (Copying my reply since I don't know if it goes to the mailing list)
>
> Great, thanks for explaining the reasoning. You're saying these aren't
> going into the final release? I think that moots any issue surrounding
> distributing them then.
>
> This is all I know of from the ASF:
> https://community.apache.org/projectIndependence.html I don't read it
> as expressly forbidding this kind of thing although you can see how it
> bumps up against the spirit. There's not a bright line -- what about
> Tomcat providing binaries compiled for Windows for example? does that
> favor an OS vendor?
>
> From this technical ASF perspective only the releases matter -- do
> what you want with snapshots and RCs. The only issue there is maybe
> releasing something different than was in the RC; is that at all
> confusing? Just needs a note.
>
> I think this theoretical issue doesn't exist if these binaries aren't
> released, so I see no reason to not proceed.
>
> The rest is a different question about whether you want to spend time
> maintaining this profile and candidate. The vendor already manages
> their build I think and -- and I don't know -- may even prefer not to
> have a different special build floating around. There's also the
> theoretical argument that this turns off other vendors from adopting
> Spark if it's perceived to be too connected to other vendors. I'd like
> to maximize Spark's distribution and there's some argument you do this
> by not making vendor profiles. But as I say a different question to
> just think about over time...
>
> (oh and PS for my part I think it's a good thing that CDH4 binaries
> were removed. I wasn't arguing for resurrecting them)
>
> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Hey Sean,
>>
>> The reason there are no longer CDH-specific builds is that all newer
>> versions of CDH and HDP work with builds for the upstream Hadoop
>> projects. I dropped CDH4 in favor of a  newer Hadoop version (2.4) and
>> the Hadoop-without-Hive (also 2.4) build.
>>
>> For MapR - we can't officially post those artifacts on ASF web space
>> when we make the final release, we can only link to them as being
>> hosted by MapR specifically since they use non-compatible licenses.
>> However, I felt that providing these during a testing period was
>> alright, with the goal of increasing test coverage. I couldn't find
>> any policy against posting these on personal web space during RC
>> voting. However, we can remove them if there is one.
>>
>> Dropping CDH4 was more because it is now pretty old, but we can add it
>> back if people want. The binary packaging is a slightly separate
>> question from release votes, so I can always add more binary packages
>> whenever. And on this, my main concern is covering the most popular
>> Hadoop versions to lower the bar for users to build and test Spark.
>>
>> - Patrick
>>
>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com> wrote:
>>> +1 I tested the source and Hadoop 2.4 release. Checksums and
>>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
>>> fail any more than usual.
>>>
>>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
>>> project and have encountered no problems.
>>>
>>>
>>> I notice that the 1.1.0 release removes the CDH4-specific build, but
>>> adds two MapR-specific builds. Compare with
>>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
>>> commented on the commit:
>>> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc
>>>
>>> I'm in favor of removing all vendor-specific builds. This change
>>> *looks* a bit funny as there was no JIRA (?) and appears to swap one
>>> vendor for another. Of course there's nothing untoward going on, but
>>> what was the reasoning? It's best avoided, and MapR already
>>> distributes Spark just fine, no?
>>>
>>> This is a gray area with ASF projects. I mention it as well because it
>>> came up with Apache Flink recently
>>> (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E)
>>> Another vendor rightly noted this could look like favoritism. They
>>> changed to remove vendor releases.
>>>
>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>>>>
>>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327
>>>>
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>>>>
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1029/
>>>>
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>>>>
>>>> Please vote on releasing this package as Apache Spark 1.1.0!
>>>>
>>>> The vote is open until Monday, September 01, at 03:11 UTC and passes if
>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>
>>>> [ ] +1 Release this package as Apache Spark 1.1.0
>>>> [ ] -1 Do not release this package because ...
>>>>
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>>
>>>> == Regressions fixed since RC1 ==
>>>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277
>>>>
>>>> == What justifies a -1 vote for this release? ==
>>>> This vote is happening very late into the QA period compared with
>>>> previous votes, so -1 votes should only occur for significant
>>>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
>>>> this release.
>>>>
>>>> == What default changes should I be aware of? ==
>>>> 1. The default value of "spark.io.compression.codec" is now "snappy"
>>>> --> Old behavior can be restored by switching to "lzf"
>>>>
>>>> 2. PySpark now performs external spilling during aggregations.
>>>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:51:29 2014
Return-Path: <dev-return-9153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A60E11360
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:51:29 +0000 (UTC)
Received: (qmail 94162 invoked by uid 500); 29 Aug 2014 06:51:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94090 invoked by uid 500); 29 Aug 2014 06:51:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94079 invoked by uid 99); 29 Aug 2014 06:51:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:51:28 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:51:02 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1XNG1V-0001yE-Lk
	for dev@spark.incubator.apache.org; Thu, 28 Aug 2014 23:51:01 -0700
Date: Thu, 28 Aug 2014 23:51:01 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409295061665-8118.post@n3.nabble.com>
In-Reply-To: <BLU184-W6246E0360DA15F574AD74EF3DF0@phx.gbl>
References: <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com> <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com> <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com> <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com> <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com> <53FA9B5C.7060605@huawei.com> <CAAswR-79v9vWH7b_ik_KzMGigu0XeKW56OqVgnOT9ORFRhX=_Q@mail.gmail.com> <BLU184-W6246E0360DA15F574AD74EF3DF0@phx.gbl>
Subject: RE: Working Formula for Hive 0.13?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have preliminary patch against spark1.0.2, which is attached to spark-2706.
Now I am working on supporting both hive-0.12 and hive-0.13.1 with
non-intrusive way (not breaking any existing hive-0.12 when introduce
supporting new version). I will attach a proposal to solve multi-version
support issue to spark-2706 soon.

Thanks.

Zhan Zhang



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Working-Formula-for-Hive-0-13-tp7551p8118.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 06:57:22 2014
Return-Path: <dev-return-9154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A45D111379
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 06:57:22 +0000 (UTC)
Received: (qmail 6833 invoked by uid 500); 29 Aug 2014 06:57:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6781 invoked by uid 500); 29 Aug 2014 06:57:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6770 invoked by uid 99); 29 Aug 2014 06:57:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:57:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 06:56:55 +0000
Received: by mail-ig0-f174.google.com with SMTP id a13so12325igq.13
        for <dev@spark.apache.org>; Thu, 28 Aug 2014 23:56:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=w0Fa5KoRh37FXIk1Fdmm7xLfMwBPFJhBmEcdoxrSHbE=;
        b=UaPrvDg+0Hu/BBr0BFfDnIWlPbTWGQ0Olo3QP+2wtpj1YDcRPeJozra81ofo81hCTO
         C2LktRQpFaqK021+5MDa2GTRfP1He9HOJmjLxvTCEvbQ41cN0oIyqd6eAXPQUU+AXb4R
         XN5jm2IJCz0uY0/ddtrrF6pPKu/mYbbI7LGVJUA8y+WSOwJhZlXqUPTXQWmIhUTAsKu3
         jUZjb1zk0m5DgcwpR3M1rczpzddMC19TmCWMt8/5N5fwxTDKvoHZ89wBnkyYc0BwJuHy
         Hj7XS/Ncu7LKJ6X36XwSQ1RhQIFr5Te39IrxeRiSNEjLSw7WBjXzahcPEmltXxEy7jpZ
         gUdQ==
X-Gm-Message-State: ALoCoQl9xn9c6zyAcSs1uhAC4QKyMzfiF4v66I/bzewsNcHDZxta2dyqZKJooTliMk2EHXKso0qO
X-Received: by 10.50.66.197 with SMTP id h5mr1974456igt.34.1409295414074; Thu,
 28 Aug 2014 23:56:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Thu, 28 Aug 2014 23:56:34 -0700 (PDT)
In-Reply-To: <CABPQxsuz50kcueEQEJ3ek0Mf3zmjy4q14dGMdNu6v9dHb6PGgQ@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com> <CABPQxsuz50kcueEQEJ3ek0Mf3zmjy4q14dGMdNu6v9dHb6PGgQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 29 Aug 2014 07:56:34 +0100
Message-ID: <CAMAsSdLbMpXzBcii_53sbYvw13314-f6DSTfURc_BST426kNeg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, Aug 29, 2014 at 7:42 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> In terms of vendor support for this approach - In the early days
> Cloudera asked us to add CDH4 repository and more recently Pivotal and
> MapR also asked us to allow linking against their hadoop-client
> libraries. So we've added these based on direct requests from vendors.
> Given the ubiquity of the Hadoop FileSystem API, it's hard for me to
> imagine ruffling feathers by supporting this. But if we get feedback
> in that direction over time we can of course consider a different
> approach.

By this, you mean that it's easy to control the Hadoop version in the
build and set it to some other vendor-specific release? Yes that seems
ideal. Making the build flexible, and adding the repository references
to pom.xml is part of enabling that -- to me, no question that's good.

So you can always roll your own build for your cluster, if you need
to. I understand the role of the cdh4 / mapr3 / mapr4 binaries as just
a convenience.

But it's a convenience for people who...
- are installing Spark on a cluster (i.e. not an end user)
- that doesn't have it in their distro already
- whose distro isn't compatible with a plain vanilla Hadoop distro

That can't be many. CDH4.6+ is most of the installed CDH base and it
already has Spark. I thought MapR already had Spark built in. The
audience seems small enough, and the convenience relatively small
enough (is it hard to run the distribution script?) that it caused me
to ask whether it was worth bothering providing these, especially give
the possible ASF sensitivity.

I say crack on; you get my point.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 07:57:48 2014
Return-Path: <dev-return-9155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7FD571154D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 07:57:48 +0000 (UTC)
Received: (qmail 97192 invoked by uid 500); 29 Aug 2014 07:57:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97128 invoked by uid 500); 29 Aug 2014 07:57:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97107 invoked by uid 99); 29 Aug 2014 07:57:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 07:57:47 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 07:57:20 +0000
Received: by mail-vc0-f169.google.com with SMTP id hq11so2108267vcb.0
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 00:57:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=F92TUjR1LW9O1PupfUvuksihbZ3l4KfIE2LRGJdjIkI=;
        b=YMdNkxs1uB7cKUelRJ2oVgJyct7CKsaqTx9nDbRzXxFTAE7apmOQWgx7qSIagHKb6j
         G3T9YCaVpLuXJ2CZ+gKbv2K40uIjUzMKdsVc5dc57ApGzFradbvQQxt53BYCTwE51pfM
         cPZG1n02OhrtuzyKFGtxIKg2r/tT31Vs0GfTJW0/LM+JOrqQRyt9UPo3icCnmfcxD2l/
         lpMUIROaQHWaT6R5B3bXBIMCLwT593t9Gr9usrmF8B5bVdux/gwh93oRrS79t6zSDCyL
         GAmfgB0IlUyNne1pDGYamKPGDBVd4Nj3pk5x1bg93EZzFTNdbikHa0St6HWpmBuEHNHC
         vtFQ==
X-Gm-Message-State: ALoCoQnjeCpsOLNsfcdrT+8z8pmj/ME+WdR4lD/rb5TdnBIJYDiI6627L2t6tv0/e8tYz4OBJjZJ
X-Received: by 10.220.251.200 with SMTP id mt8mr8841408vcb.24.1409299038810;
        Fri, 29 Aug 2014 00:57:18 -0700 (PDT)
Received: from mail-vc0-f180.google.com (mail-vc0-f180.google.com [209.85.220.180])
        by mx.google.com with ESMTPSA id vi19sm18956341vdb.16.2014.08.29.00.57.17
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 29 Aug 2014 00:57:17 -0700 (PDT)
Received: by mail-vc0-f180.google.com with SMTP id lf12so2041917vcb.25
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 00:57:17 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.52.36.80 with SMTP id o16mr66895vdj.58.1409299037301; Fri,
 29 Aug 2014 00:57:17 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Fri, 29 Aug 2014 00:57:17 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Fri, 29 Aug 2014 00:57:17 -0700 (PDT)
In-Reply-To: <CAMAsSdLbMpXzBcii_53sbYvw13314-f6DSTfURc_BST426kNeg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<CABPQxsuz50kcueEQEJ3ek0Mf3zmjy4q14dGMdNu6v9dHb6PGgQ@mail.gmail.com>
	<CAMAsSdLbMpXzBcii_53sbYvw13314-f6DSTfURc_BST426kNeg@mail.gmail.com>
Date: Fri, 29 Aug 2014 00:57:17 -0700
Message-ID: <CA+-p3AHhkd55f0utoay2nXaJN9snm=n574TKNatnV764TOzu0A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Andrew Ash <andrew@andrewash.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf307c9f108035530501c00044
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307c9f108035530501c00044
Content-Type: text/plain; charset=UTF-8

FWIW we use CDH4 extensively and would very much appreciate having a
prebuilt version of Spark for it.

We're doing a CDH 4.4 to 4.7 upgrade across all the clusters now and have
plans for a 5.x transition after that.
On Aug 28, 2014 11:57 PM, "Sean Owen" <sowen@cloudera.com> wrote:

> On Fri, Aug 29, 2014 at 7:42 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > In terms of vendor support for this approach - In the early days
> > Cloudera asked us to add CDH4 repository and more recently Pivotal and
> > MapR also asked us to allow linking against their hadoop-client
> > libraries. So we've added these based on direct requests from vendors.
> > Given the ubiquity of the Hadoop FileSystem API, it's hard for me to
> > imagine ruffling feathers by supporting this. But if we get feedback
> > in that direction over time we can of course consider a different
> > approach.
>
> By this, you mean that it's easy to control the Hadoop version in the
> build and set it to some other vendor-specific release? Yes that seems
> ideal. Making the build flexible, and adding the repository references
> to pom.xml is part of enabling that -- to me, no question that's good.
>
> So you can always roll your own build for your cluster, if you need
> to. I understand the role of the cdh4 / mapr3 / mapr4 binaries as just
> a convenience.
>
> But it's a convenience for people who...
> - are installing Spark on a cluster (i.e. not an end user)
> - that doesn't have it in their distro already
> - whose distro isn't compatible with a plain vanilla Hadoop distro
>
> That can't be many. CDH4.6+ is most of the installed CDH base and it
> already has Spark. I thought MapR already had Spark built in. The
> audience seems small enough, and the convenience relatively small
> enough (is it hard to run the distribution script?) that it caused me
> to ask whether it was worth bothering providing these, especially give
> the possible ASF sensitivity.
>
> I say crack on; you get my point.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--20cf307c9f108035530501c00044--

From dev-return-9156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 08:34:06 2014
Return-Path: <dev-return-9156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A041011601
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 08:34:06 +0000 (UTC)
Received: (qmail 46645 invoked by uid 500); 29 Aug 2014 08:34:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46567 invoked by uid 500); 29 Aug 2014 08:34:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45972 invoked by uid 99); 29 Aug 2014 08:34:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 08:34:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of archit279thakur@gmail.com designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 08:33:58 +0000
Received: by mail-la0-f50.google.com with SMTP id mc6so2324541lab.37
        for <multiple recipients>; Fri, 29 Aug 2014 01:33:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=0ZSqOcZoKfb37+TzMH9I6AuadINa05uJqMkle35EFOw=;
        b=yRlgiDy4Q880ec0s8Vqac9miRVLS7eUH+I75WvtHlGyS8hB7Jn4hLadCullGTy3enU
         6zgTLxFjxPn3tGiNi3zTSxE0PPGhYnnO6cagLohnYRVnaIgkjsjiD9TqdJg8YtkNYER/
         lm+O/qTgiNv4dt+koSfhc6TzRABOTjcTke/G+J4RLggLu44cyYkn9g8qgFfCfNYoVmCU
         Zc2G97oFrjCy35m7BzZAUvOmfKZ1QVOksGiaJuc1hawPJmKJjPTSJVZsf5jvc7ZLC1LY
         LzICHO6kMSIKzQHAbXaZMG9rj406E8zFff8Q63JVzF/GOMRsLDzPfXB73lH479ZkwtZ3
         +Siw==
MIME-Version: 1.0
X-Received: by 10.152.121.37 with SMTP id lh5mr9852146lab.43.1409301216801;
 Fri, 29 Aug 2014 01:33:36 -0700 (PDT)
Received: by 10.112.130.132 with HTTP; Fri, 29 Aug 2014 01:33:36 -0700 (PDT)
Date: Fri, 29 Aug 2014 14:03:36 +0530
Message-ID: <CA+KkaUaqDGHhzKSf4zF9iM7_9saV-7G8RNjYoipOBLz=+9V4Hg@mail.gmail.com>
Subject: Running Spark On Yarn without Spark-Submit
From: Archit Thakur <archit279thakur@gmail.com>
To: dev@spark.incubator.apache.org, user@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0122797c68b8310501c082a4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122797c68b8310501c082a4
Content-Type: text/plain; charset=UTF-8

Hi,

My requirement is to run Spark on Yarn without using the script
spark-submit.

I have a servlet and a tomcat server. As and when request comes, it creates
a new SC and keeps it alive for the further requests, I ma setting my
master in sparkConf

as sparkConf.setMaster("yarn-cluster")

but the request is stuck indefinitely.

This works when I set
sparkConf.setMaster("yarn-client")

I am not sure, why is it not launching job in yarn-cluster mode.

Any thoughts?

Thanks and Regards,
Archit Thakur.

--089e0122797c68b8310501c082a4--

From dev-return-9157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 09:37:09 2014
Return-Path: <dev-return-9157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABF081173A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 09:37:09 +0000 (UTC)
Received: (qmail 59470 invoked by uid 500); 29 Aug 2014 09:37:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59391 invoked by uid 500); 29 Aug 2014 09:37:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58261 invoked by uid 99); 29 Aug 2014 09:37:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 09:37:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of archit279thakur@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 09:36:56 +0000
Received: by mail-lb0-f177.google.com with SMTP id z11so2316519lbi.36
        for <multiple recipients>; Fri, 29 Aug 2014 02:36:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=hLh84PpihntkqvMRCcPXD0jvZ0dcnPnsPTtc/grif/c=;
        b=R+tpOprdNtrqKMn/CcRFkr+giSBOEf9srvJ4KVwFPFNX7pHwEykN7ALO7JgrxN+7Q5
         CfiI25L0nfy71oFrvDSuJgLGSCCdgz7ve9WKa/bi8LKoNiy9DuIMRAz8ukRtenDJldje
         4KN9SP3CbwgO9av15feNEGnF+yjQPb8iKJFgwRzeuvd9lIOiMljrtY7IwIPehtwikcwV
         XwKwSpB2JmTd/9fzOnorZljibi1regEbngsQ4mnqaGJt1/6ixPbJANBHIDVD1C6mkfBr
         nZJ8S+YK4ids0od3wgtwbRsXRtYGvWbOuH5Bu8QFcepj21ISHQSf+aQbQIs+z0sqwSBs
         nzMQ==
MIME-Version: 1.0
X-Received: by 10.112.53.199 with SMTP id d7mr1157630lbp.106.1409304994847;
 Fri, 29 Aug 2014 02:36:34 -0700 (PDT)
Received: by 10.112.130.132 with HTTP; Fri, 29 Aug 2014 02:36:34 -0700 (PDT)
In-Reply-To: <CA+KkaUaqDGHhzKSf4zF9iM7_9saV-7G8RNjYoipOBLz=+9V4Hg@mail.gmail.com>
References: <CA+KkaUaqDGHhzKSf4zF9iM7_9saV-7G8RNjYoipOBLz=+9V4Hg@mail.gmail.com>
Date: Fri, 29 Aug 2014 15:06:34 +0530
Message-ID: <CA+KkaUbHrbbSEm2LF+byEPxd3ZN_LF6gwMqadrCv2x3dP-d36A@mail.gmail.com>
Subject: Re: Running Spark On Yarn without Spark-Submit
From: Archit Thakur <archit279thakur@gmail.com>
To: dev@spark.incubator.apache.org, user@spark.incubator.apache.org, 
	user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133d09e991f5e0501c16328
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133d09e991f5e0501c16328
Content-Type: text/plain; charset=UTF-8

including user@spark.apache.org.


On Fri, Aug 29, 2014 at 2:03 PM, Archit Thakur <archit279thakur@gmail.com>
wrote:

> Hi,
>
> My requirement is to run Spark on Yarn without using the script
> spark-submit.
>
> I have a servlet and a tomcat server. As and when request comes, it
> creates a new SC and keeps it alive for the further requests, I ma setting
> my master in sparkConf
>
> as sparkConf.setMaster("yarn-cluster")
>
> but the request is stuck indefinitely.
>
> This works when I set
> sparkConf.setMaster("yarn-client")
>
> I am not sure, why is it not launching job in yarn-cluster mode.
>
> Any thoughts?
>
> Thanks and Regards,
> Archit Thakur.
>
>
>
>

--001a1133d09e991f5e0501c16328--

From dev-return-9158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 13:42:36 2014
Return-Path: <dev-return-9158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B63A911D81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 13:42:36 +0000 (UTC)
Received: (qmail 30780 invoked by uid 500); 29 Aug 2014 13:42:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30719 invoked by uid 500); 29 Aug 2014 13:42:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30708 invoked by uid 99); 29 Aug 2014 13:42:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 13:42:35 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 13:42:30 +0000
Received: by mail-wg0-f46.google.com with SMTP id x13so2162453wgg.17
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 06:42:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=Xq8SWIDkdbt/8V0JPCTBcOPm/dC52yPQjSvbABqVOQ8=;
        b=b6rS9P5P5HKDZ3qJWKagZWt5wGVJ1BnfDfYJENPd1jU9B2NfuZeSv7SP5MHT65Fscz
         eH2RhT49Q71nutuaibK6s1ApdEKXp+XU3k9xDSppnjrwiwjx2w4X3n0keYgCqDxQrPU4
         eb7CvtCv63swHcifWTYEmKCwSzuYdAZAoHCBHMzIxq4z3uJ8VdBaHMyhzB06YH/2med2
         R/Cqn1D+5ZbRzVay3ki3EVslO7yiiCZH5q7RHSwtFxeflAvyKWHDyA3TPL4vDVkoXzOX
         jdiEhm66AL9l9vfGqK64yLqIu8mhmOwiNfk3YiFetKKiIvTo74qrV+897ZzF0JCGKqgw
         3RoQ==
X-Gm-Message-State: ALoCoQmTsq4ItrP7UCFzIPYadqwIIT9UzjcH3i0w1tr0PA4MCA4YgwC3zJtbleXNuV40otiklyJd
MIME-Version: 1.0
X-Received: by 10.180.94.161 with SMTP id dd1mr4084652wib.22.1409319729362;
 Fri, 29 Aug 2014 06:42:09 -0700 (PDT)
Received: by 10.216.174.197 with HTTP; Fri, 29 Aug 2014 06:42:09 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CA+-p3AHhkd55f0utoay2nXaJN9snm=n574TKNatnV764TOzu0A@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<CABPQxsuz50kcueEQEJ3ek0Mf3zmjy4q14dGMdNu6v9dHb6PGgQ@mail.gmail.com>
	<CAMAsSdLbMpXzBcii_53sbYvw13314-f6DSTfURc_BST426kNeg@mail.gmail.com>
	<CA+-p3AHhkd55f0utoay2nXaJN9snm=n574TKNatnV764TOzu0A@mail.gmail.com>
Date: Fri, 29 Aug 2014 09:42:09 -0400
Message-ID: <CANx3uAiZn8wosXk4EQ+amh_vG-RzxYK0kps4ShYg-LgfsTmvbg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Koert Kuipers <koert@tresata.com>
To: Andrew Ash <andrew@andrewash.com>
Cc: Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04462e9ed8157d0501c4d1b0
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04462e9ed8157d0501c4d1b0
Content-Type: text/plain; charset=UTF-8

i suspect there are more cdh4 than cdh5 clusters. most people plan to move
to cdh5 within say 6 months.


On Fri, Aug 29, 2014 at 3:57 AM, Andrew Ash <andrew@andrewash.com> wrote:

> FWIW we use CDH4 extensively and would very much appreciate having a
> prebuilt version of Spark for it.
>
> We're doing a CDH 4.4 to 4.7 upgrade across all the clusters now and have
> plans for a 5.x transition after that.
> On Aug 28, 2014 11:57 PM, "Sean Owen" <sowen@cloudera.com> wrote:
>
> > On Fri, Aug 29, 2014 at 7:42 AM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > In terms of vendor support for this approach - In the early days
> > > Cloudera asked us to add CDH4 repository and more recently Pivotal and
> > > MapR also asked us to allow linking against their hadoop-client
> > > libraries. So we've added these based on direct requests from vendors.
> > > Given the ubiquity of the Hadoop FileSystem API, it's hard for me to
> > > imagine ruffling feathers by supporting this. But if we get feedback
> > > in that direction over time we can of course consider a different
> > > approach.
> >
> > By this, you mean that it's easy to control the Hadoop version in the
> > build and set it to some other vendor-specific release? Yes that seems
> > ideal. Making the build flexible, and adding the repository references
> > to pom.xml is part of enabling that -- to me, no question that's good.
> >
> > So you can always roll your own build for your cluster, if you need
> > to. I understand the role of the cdh4 / mapr3 / mapr4 binaries as just
> > a convenience.
> >
> > But it's a convenience for people who...
> > - are installing Spark on a cluster (i.e. not an end user)
> > - that doesn't have it in their distro already
> > - whose distro isn't compatible with a plain vanilla Hadoop distro
> >
> > That can't be many. CDH4.6+ is most of the installed CDH base and it
> > already has Spark. I thought MapR already had Spark built in. The
> > audience seems small enough, and the convenience relatively small
> > enough (is it hard to run the distribution script?) that it caused me
> > to ask whether it was worth bothering providing these, especially give
> > the possible ASF sensitivity.
> >
> > I say crack on; you get my point.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--f46d04462e9ed8157d0501c4d1b0--

From dev-return-9159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 13:59:37 2014
Return-Path: <dev-return-9159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E896011DDD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 13:59:36 +0000 (UTC)
Received: (qmail 57205 invoked by uid 500); 29 Aug 2014 13:59:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57148 invoked by uid 500); 29 Aug 2014 13:59:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56222 invoked by uid 99); 29 Aug 2014 13:59:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 13:59:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 13:59:22 +0000
Received: by mail-pa0-f54.google.com with SMTP id fb1so6577019pad.27
        for <dev@spark.incubator.apache.org>; Fri, 29 Aug 2014 06:58:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:content-transfer-encoding:message-id:references
         :to;
        bh=x28DxIP3l7XsEA3V61FaW7sN0iMkRdxwfcjUeao3GnQ=;
        b=Z2B3r3QKGox9JJu16M3gy/ItMwll+QpeKGNBb0yY7cv/nJ2puRGPa923LFNbTgI2ZR
         jZbavgkM2UqHUAp0DrKwatmsCcDAGgDFCa79soPIu9OcmUe4v7FmjiyXsc95jD6p3AUM
         DfpSQRuiLrZDsIZsahKuFb0o/FF7ntZQS+DjQWljCJJuPg8u1ov6jSL6b96EiTMPVijv
         Fr/tm9UODvCubx764+9Tt1XvnZsGbltiL2Q1UYqYCTwg7EcgXXdKoR3i13flDcmji6JX
         oPVT7wLEqPVb8Kor0NGdMWBaagG5MLDM01eao0E0pCaLX04zHndJ1KlSHpuqR4yskIfZ
         pG2A==
X-Gm-Message-State: ALoCoQmFFMF7c2nh4Mj2KbiN5or6iMAYY24bVZu6FRhN3sOKHbAuYGAf0Z4c+tiL3gdXGw0vkxVE
X-Received: by 10.66.160.233 with SMTP id xn9mr16124332pab.29.1409320739206;
        Fri, 29 Aug 2014 06:58:59 -0700 (PDT)
Received: from [192.168.2.11] ([24.5.225.89])
        by mx.google.com with ESMTPSA id a4sm39943pbu.42.2014.08.29.06.58.57
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 29 Aug 2014 06:58:57 -0700 (PDT)
Content-Type: multipart/alternative;
	boundary=Apple-Mail-A769A8B2-E3E4-4E10-A1A7-EC3B1925508F
Mime-Version: 1.0 (1.0)
Subject: Re: Running Spark On Yarn without Spark-Submit
From: "Chester @work" <chester@alpinenow.com>
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <CA+KkaUbHrbbSEm2LF+byEPxd3ZN_LF6gwMqadrCv2x3dP-d36A@mail.gmail.com>
Date: Fri, 29 Aug 2014 06:58:57 -0700
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>,
 "user@spark.incubator.apache.org" <user@spark.incubator.apache.org>,
 "user@spark.apache.org" <user@spark.apache.org>
Content-Transfer-Encoding: 7bit
Message-Id: <52B7520B-7B9C-4A59-9063-6E6EB5A3F6E4@alpinenow.com>
References: <CA+KkaUaqDGHhzKSf4zF9iM7_9saV-7G8RNjYoipOBLz=+9V4Hg@mail.gmail.com> <CA+KkaUbHrbbSEm2LF+byEPxd3ZN_LF6gwMqadrCv2x3dP-d36A@mail.gmail.com>
To: Archit Thakur <archit279thakur@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-A769A8B2-E3E4-4E10-A1A7-EC3B1925508F
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

Archit
     We are using yarn-cluster mode , and calling spark via Client class dir=
ectly from servlet server. It works fine.=20
    To establish a communication channel to give further requests,=20
     It should be possible with yarn client, but not with yarn server. Yarn c=
lient mode, spark driver is outside the yarn cluster; so it can issue more c=
ommands. In yarn cluster, all programs including spark driver is running ins=
ide the yarn cluster. There is no communication channel with the client unti=
l the job finishes.

If you job is to keep spark context alive, and wait for other commands, then=
 this should wait forever.=20

I am actually working on some improvements on this and experiment in our pro=
duct, I will create PRs when I feel conformable with the solution

1) change Client API to allow the caller to know yarn app resource capacity b=
efore passing arguments
2) add YarnApplicationListener to the Client=20
3) provide communication channel between application and spark Yarn client i=
n cluster.=20

The #1) is not directly related to the communication discussed here

#2) allows the application to have application life cycle call back as to ap=
p start end in progress failure etc with yarn resources allocations=20

I changed #1 and #2 in forked spark, and it's worked well in cdh5, and I am t=
esting against 2.0.5-alpha as well.=20

For #3) I did not change in spark currently, as I am not sure the best appro=
ach yet. I put the change in the application runner which launch the spark y=
arn client in the cluster.=20

The runner in yarn cluster get applications host and port information  from t=
he passed configuration (args), then creates an Akka actor using spark conte=
xt actor system, send a hand shake message to the caller outside the cluster=
, after that you will have a two way communications=20

With this approach, I can send spark listener call backs to the app, error m=
essages, app level messages etc.=20

The runner inside the cluster can also receive requests from outside cluster=
 such as stop.=20

We are not sure Akka approach is the best, so I am still experimenting it. S=
o far it does what we wants .

Hope this helps

Chester


Sent from my iPhone

> On Aug 29, 2014, at 2:36 AM, Archit Thakur <archit279thakur@gmail.com> wro=
te:
>=20
> including user@spark.apache.org.
>=20
>=20
>> On Fri, Aug 29, 2014 at 2:03 PM, Archit Thakur <archit279thakur@gmail.com=
> wrote:
>> Hi,
>>=20
>> My requirement is to run Spark on Yarn without using the script spark-sub=
mit.
>>=20
>> I have a servlet and a tomcat server. As and when request comes, it creat=
es a new SC and keeps it alive for the further requests, I ma setting my mas=
ter in sparkConf
>>=20
>> as sparkConf.setMaster("yarn-cluster")
>>=20
>> but the request is stuck indefinitely.=20
>>=20
>> This works when I set
>> sparkConf.setMaster("yarn-client")
>>=20
>> I am not sure, why is it not launching job in yarn-cluster mode.
>>=20
>> Any thoughts?
>>=20
>> Thanks and Regards,
>> Archit Thakur.=20
>=20

--Apple-Mail-A769A8B2-E3E4-4E10-A1A7-EC3B1925508F--

From dev-return-9160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 14:33:19 2014
Return-Path: <dev-return-9160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC48811ED0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 14:33:19 +0000 (UTC)
Received: (qmail 28905 invoked by uid 500); 29 Aug 2014 14:33:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28837 invoked by uid 500); 29 Aug 2014 14:33:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28825 invoked by uid 99); 29 Aug 2014 14:33:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 14:33:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 14:33:14 +0000
Received: by mail-lb0-f170.google.com with SMTP id w7so2743893lbi.29
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 07:32:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=C+sm4uYe+QdyRiB8MP5PT4CVHAgt4TY8vCEzdKMjkaY=;
        b=ZPrHf4JVgNS9wgi3Tsz/Opb8bT6NJcbDxxcacWyfrlLRhHnM0sFtP4wvxk0Q2n/RuE
         043SxSbiou60IX6BJGAv1u1qsfe0MyMliVAOlfdFeYB65q9mLXwuBXPLE3tsrYbM1g74
         xXU8d8v1QjC6Jk6YWqFNZZbipXhis9XZ7b2jSOP2htREWOiWysYu/cBQXq1DypOHGc9Z
         k5xp80IqVqKwzAzimz8F6hH30kxrKfMERPk/IEyouBG0YJUyYyWFA2bfhd+Ov+qUjrX3
         CRNjURopoBxMZjxvU9knap/7luNogudHUHzjZmxg8jduXUb1Xp70WuohGp3Ru1nEiLbw
         FPww==
X-Gm-Message-State: ALoCoQkFSXmN7vrdnKvYLEpeNaDBqwIAr/qLvEOWMdxavkPXBDemaNdBDNc2CvKKPf4IB5GsNGEp
X-Received: by 10.152.4.9 with SMTP id g9mr11878271lag.14.1409322772665; Fri,
 29 Aug 2014 07:32:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Fri, 29 Aug 2014 07:32:32 -0700 (PDT)
In-Reply-To: <CACdU-dTZx3ey=1OHuYFjf8LNcO64bw2VQ8VJ40ssKvAGr8nUcw@mail.gmail.com>
References: <CACdU-dTZx3ey=1OHuYFjf8LNcO64bw2VQ8VJ40ssKvAGr8nUcw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 29 Aug 2014 07:32:32 -0700
Message-ID: <CACdU-dQZsSUwAgPxpuSdHdQEDC++ERTZOza18xMzQabiBGwDgQ@mail.gmail.com>
Subject: Re: "emergency" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013d17083d26b80501c5870e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d17083d26b80501c5870e
Content-Type: text/plain; charset=UTF-8

reminder:   this is happening right now.  jenkins is currently in quiet
mode, and in ~30 minutes, will be briefly going down.


On Thu, Aug 28, 2014 at 1:03 PM, shane knapp <sknapp@berkeley.edu> wrote:

> as with all software upgrades, sometimes things don't always work as
> expected.
>
> a recent change to stapler[1], to verbosely
> report NotExportableExceptions[2] is spamming our jenkins log file with
> stack traces, which is growing rather quickly (1.2G since 9am).  this has
> been reported to the jenkins jira[3], and a fix has been pushed and will be
> rolled out "soon"[4].
>
> this isn't affecting any builds, and jenkins is happily humming along.
>
> in the interim, so that we don't run out of disk space, i will be
> redirecting the jenkins logs tommorow morning to /dev/null for the long
> weekend.
>
> once a real fix has been released, i will update any packages needed and
> redirect the logging back to the log file.
>
> other than a short downtime, this will have no user-facing impact.
>
> please let me know if you have any questions/concerns.
>
> thanks for your patience!
>
> shane "the new guy"  :)
>
> [1] -- https://wiki.jenkins-ci.org/display/JENKINS/Architecture
> [2] --
> https://github.com/stapler/stapler/commit/ed2cb8b04c1514377f3a8bfbd567f050a67c6e1c
> [3] --
> https://issues.jenkins-ci.org/browse/JENKINS-24458?focusedCommentId=209247
> [4] --
> https://github.com/stapler/stapler/commit/e2b39098ca1f61a58970b8a41a3ae79053cf30e3
>

--089e013d17083d26b80501c5870e--

From dev-return-9161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 15:07:12 2014
Return-Path: <dev-return-9161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2718911FB2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 15:07:12 +0000 (UTC)
Received: (qmail 97145 invoked by uid 500); 29 Aug 2014 15:07:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97080 invoked by uid 500); 29 Aug 2014 15:07:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97061 invoked by uid 99); 29 Aug 2014 15:07:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:07:11 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:07:07 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1XNNlG-0004GA-3L
	for dev@spark.incubator.apache.org; Fri, 29 Aug 2014 08:06:46 -0700
Date: Fri, 29 Aug 2014 08:06:46 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409324806012-8127.post@n3.nabble.com>
In-Reply-To: <CAA_qdLpTbEypB9Ni_n-xoVpyz18uqS=X0qD9DJQNSgiFBU=u-w@mail.gmail.com>
References: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com> <CAA_qdLpTbEypB9Ni_n-xoVpyz18uqS=X0qD9DJQNSgiFBU=u-w@mail.gmail.com>
Subject: Re: Jira tickets for starter tasks
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Cheng Lian-2 wrote
> You can just start the work :)

Given 100+ contributors, starting work without a JIRA issue assigned to you
could lead to duplication of effort by well meaning people that have no idea
they are working on the same issue. This does happen and I don't think it's
a good thing.

Just my $0.02



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Jira-tickets-for-starter-tasks-tp8102p8127.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 15:19:49 2014
Return-Path: <dev-return-9162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A418D11034
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 15:19:49 +0000 (UTC)
Received: (qmail 27173 invoked by uid 500); 29 Aug 2014 15:19:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27103 invoked by uid 500); 29 Aug 2014 15:19:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27086 invoked by uid 99); 29 Aug 2014 15:19:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:19:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:19:23 +0000
Received: by mail-la0-f48.google.com with SMTP id gl10so2937768lab.35
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 08:19:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=yqmxxIFLnZFdPuVekfNwpeW1KWXVOEghftFA4qrjknE=;
        b=kotqpd1n9BEm11iBTOiWrGuX13shWmSkBXgn8/L5VXJ6rGbSs5oAos2+Vtn8agskGE
         of1wsn59Ch5b0hYxNNxdOQqY9m+pUG/N5rOECMdDpZlTD3XyliwGYEMa6nQvVqKZic4Q
         WnGC0xef6KLuMHT0DpSGxvpS4EW7xfiC+0bKZOkE9bkYLDax/46LeENq44u1E/hmm1Hd
         DwRUZ1gj9pA4jhDZsqLiHYfirsdGmPpJqbUz4VqhXO2emDprZFsmVG5SOVWwl0vHdMEO
         N3SDj7UzKp3EvI2Sz9EY9wdkFzdFVK7+9jkmmLoLEpHZFW08SqDSN7qOU/p9iNm+buN+
         3tHA==
X-Gm-Message-State: ALoCoQkvuoTB5D/xrIRmJye+FhXexhw92TkdMTJQxNlq8iNIs2w4antmZvsR06pGmmm0tkXbQc4b
X-Received: by 10.152.6.40 with SMTP id x8mr11771924lax.18.1409325561490; Fri,
 29 Aug 2014 08:19:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Fri, 29 Aug 2014 08:19:01 -0700 (PDT)
In-Reply-To: <CACdU-dQZsSUwAgPxpuSdHdQEDC++ERTZOza18xMzQabiBGwDgQ@mail.gmail.com>
References: <CACdU-dTZx3ey=1OHuYFjf8LNcO64bw2VQ8VJ40ssKvAGr8nUcw@mail.gmail.com>
 <CACdU-dQZsSUwAgPxpuSdHdQEDC++ERTZOza18xMzQabiBGwDgQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 29 Aug 2014 08:19:01 -0700
Message-ID: <CACdU-dR9Zxo3jhcs4MzBHHv5TCuKehWShT4AS3EsDQ=NjvgQog@mail.gmail.com>
Subject: Re: "emergency" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0141a020773df80501c62dde
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0141a020773df80501c62dde
Content-Type: text/plain; charset=UTF-8

this is done.


On Fri, Aug 29, 2014 at 7:32 AM, shane knapp <sknapp@berkeley.edu> wrote:

> reminder:   this is happening right now.  jenkins is currently in quiet
> mode, and in ~30 minutes, will be briefly going down.
>
>
> On Thu, Aug 28, 2014 at 1:03 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> as with all software upgrades, sometimes things don't always work as
>> expected.
>>
>> a recent change to stapler[1], to verbosely
>> report NotExportableExceptions[2] is spamming our jenkins log file with
>> stack traces, which is growing rather quickly (1.2G since 9am).  this has
>> been reported to the jenkins jira[3], and a fix has been pushed and will be
>> rolled out "soon"[4].
>>
>> this isn't affecting any builds, and jenkins is happily humming along.
>>
>> in the interim, so that we don't run out of disk space, i will be
>> redirecting the jenkins logs tommorow morning to /dev/null for the long
>> weekend.
>>
>> once a real fix has been released, i will update any packages needed and
>> redirect the logging back to the log file.
>>
>> other than a short downtime, this will have no user-facing impact.
>>
>> please let me know if you have any questions/concerns.
>>
>> thanks for your patience!
>>
>> shane "the new guy"  :)
>>
>> [1] -- https://wiki.jenkins-ci.org/display/JENKINS/Architecture
>> [2] --
>> https://github.com/stapler/stapler/commit/ed2cb8b04c1514377f3a8bfbd567f050a67c6e1c
>> [3] --
>> https://issues.jenkins-ci.org/browse/JENKINS-24458?focusedCommentId=209247
>> [4] --
>> https://github.com/stapler/stapler/commit/e2b39098ca1f61a58970b8a41a3ae79053cf30e3
>>
>
>

--089e0141a020773df80501c62dde--

From dev-return-9163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 15:27:09 2014
Return-Path: <dev-return-9163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00D1511075
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 15:27:09 +0000 (UTC)
Received: (qmail 44661 invoked by uid 500); 29 Aug 2014 15:27:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44594 invoked by uid 500); 29 Aug 2014 15:27:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44577 invoked by uid 99); 29 Aug 2014 15:27:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:27:08 +0000
X-ASF-Spam-Status: No, hits=0.4 required=10.0
	tests=DATE_IN_PAST_03_06,MIME_QP_LONG_LINE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of advancedxy@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 15:27:03 +0000
Received: by mail-pa0-f43.google.com with SMTP id et14so6728270pad.16
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 08:26:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:content-transfer-encoding:from:mime-version:subject
         :date:message-id:references:cc:in-reply-to:to;
        bh=Aem+2bttT/M9W2M7K91CJfbP/dD1axu4BoxP03wVOCk=;
        b=OjWj46ALJMirYahHW2NYCk9a/G/XH4Whi1r6t2yRqbW1tjquWyUCuxX9A60xenjBUu
         bkexVc1k6F0OREUubITTVdkCP1o95/WYTirOeE0pN/Cb2lv+knZJZfEXVqAzocI2VsxC
         9KtTUuptCHJ1VUj5jnKtLuW6pX/BLs5m1b6sFkYKne9H8Kg4gNpn/gxzOQPijvxi6o13
         jUZLgqtNCh8+eDnXrjk3A2n7O6XHmh2Z2HOBob+FzEeZYZQ/oAKhJodLFiIQraikRJ07
         2WGT3NnQz4NgBWdS9LgSEyO1RR+AKsmqg+DemJLIHurGcylOPXhkHLC3CctKuNfkyjZA
         EZOg==
X-Received: by 10.70.102.175 with SMTP id fp15mr16484493pdb.52.1409325997654;
        Fri, 29 Aug 2014 08:26:37 -0700 (PDT)
Received: from [172.16.100.191] (li580-54.members.linode.com. [106.186.22.54])
        by mx.google.com with ESMTPSA id rk11sm881366pab.22.2014.08.29.08.26.35
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 29 Aug 2014 08:26:36 -0700 (PDT)
Content-Type: text/plain; charset=gb2312
Content-Transfer-Encoding: quoted-printable
From: Ye Xianjin <advancedxy@gmail.com>
Mime-Version: 1.0 (1.0)
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
Date: Fri, 29 Aug 2014 19:02:22 +0800
Message-Id: <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com> <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com> <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com> <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com> <etPan.5400202e.579478fe.470a@mbp-3.local>
Cc: Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <etPan.5400202e.579478fe.470a@mbp-3.local>
To: Matei Zaharia <matei.zaharia@gmail.com>
X-Mailer: iPhone Mail (11D257)
X-Virus-Checked: Checked by ClamAV on apache.org

We just used CDH 4.7 for our production cluster. And I believe we won't use C=
DH 5 in the next year.

Sent from my iPhone

> On 2014=C4=EA8=D4=C229=C8=D5, at 14:39, Matei Zaharia <matei.zaharia@gmail=
.com> wrote:
>=20
> Personally I'd actually consider putting CDH4 back if there are still user=
s on it. It's always better to be inclusive, and the convenience of a one-cl=
ick download is high. Do we have a sense on what % of CDH users still use CD=
H4?
>=20
> Matei
>=20
> On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com) wrote:
>=20
> (Copying my reply since I don't know if it goes to the mailing list)=20
>=20
> Great, thanks for explaining the reasoning. You're saying these aren't=20
> going into the final release? I think that moots any issue surrounding=20
> distributing them then.=20
>=20
> This is all I know of from the ASF:=20
> https://community.apache.org/projectIndependence.html I don't read it=20
> as expressly forbidding this kind of thing although you can see how it=20
> bumps up against the spirit. There's not a bright line -- what about=20
> Tomcat providing binaries compiled for Windows for example? does that=20
> favor an OS vendor?=20
>=20
> =46rom this technical ASF perspective only the releases matter -- do=20
> what you want with snapshots and RCs. The only issue there is maybe=20
> releasing something different than was in the RC; is that at all=20
> confusing? Just needs a note.=20
>=20
> I think this theoretical issue doesn't exist if these binaries aren't=20
> released, so I see no reason to not proceed.=20
>=20
> The rest is a different question about whether you want to spend time=20
> maintaining this profile and candidate. The vendor already manages=20
> their build I think and -- and I don't know -- may even prefer not to=20
> have a different special build floating around. There's also the=20
> theoretical argument that this turns off other vendors from adopting=20
> Spark if it's perceived to be too connected to other vendors. I'd like=20
> to maximize Spark's distribution and there's some argument you do this=20
> by not making vendor profiles. But as I say a different question to=20
> just think about over time...=20
>=20
> (oh and PS for my part I think it's a good thing that CDH4 binaries=20
> were removed. I wasn't arguing for resurrecting them)=20
>=20
>> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com> wro=
te:=20
>> Hey Sean,=20
>>=20
>> The reason there are no longer CDH-specific builds is that all newer=20
>> versions of CDH and HDP work with builds for the upstream Hadoop=20
>> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4) and=20
>> the Hadoop-without-Hive (also 2.4) build.=20
>>=20
>> For MapR - we can't officially post those artifacts on ASF web space=20
>> when we make the final release, we can only link to them as being=20
>> hosted by MapR specifically since they use non-compatible licenses.=20
>> However, I felt that providing these during a testing period was=20
>> alright, with the goal of increasing test coverage. I couldn't find=20
>> any policy against posting these on personal web space during RC=20
>> voting. However, we can remove them if there is one.=20
>>=20
>> Dropping CDH4 was more because it is now pretty old, but we can add it=20=

>> back if people want. The binary packaging is a slightly separate=20
>> question from release votes, so I can always add more binary packages=20
>> whenever. And on this, my main concern is covering the most popular=20
>> Hadoop versions to lower the bar for users to build and test Spark.=20
>>=20
>> - Patrick=20
>>=20
>>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com> wrote:=20=

>>> +1 I tested the source and Hadoop 2.4 release. Checksums and=20
>>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't=20
>>> fail any more than usual.=20
>>>=20
>>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another=20=

>>> project and have encountered no problems.=20
>>>=20
>>>=20
>>> I notice that the 1.1.0 release removes the CDH4-specific build, but=20
>>> adds two MapR-specific builds. Compare with=20
>>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I=20
>>> commented on the commit:=20
>>> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede7=
13a73cc=20
>>>=20
>>> I'm in favor of removing all vendor-specific builds. This change=20
>>> *looks* a bit funny as there was no JIRA (?) and appears to swap one=20
>>> vendor for another. Of course there's nothing untoward going on, but=20
>>> what was the reasoning? It's best avoided, and MapR already=20
>>> distributes Spark just fine, no?=20
>>>=20
>>> This is a gray area with ASF projects. I mention it as well because it=20=

>>> came up with Apache Flink recently=20
>>> (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.=
mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.g=
mail.com%3E)
>>> Another vendor rightly noted this could look like favoritism. They=20
>>> changed to remove vendor releases.=20
>>>=20
>>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com> w=
rote:=20
>>>> Please vote on releasing the following candidate as Apache Spark versio=
n 1.1.0!=20
>>>>=20
>>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):=20
>>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D71=
1aebb329ca28046396af1e34395a0df92b5327=20
>>>>=20
>>>> The release files, including signatures, digests, etc. can be found at:=
=20
>>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/=20
>>>>=20
>>>> Release artifacts are signed with the following key:=20
>>>> https://people.apache.org/keys/committer/pwendell.asc=20
>>>>=20
>>>> The staging repository for this release can be found at:=20
>>>> https://repository.apache.org/content/repositories/orgapachespark-1029/=
=20
>>>>=20
>>>> The documentation corresponding to this release can be found at:=20
>>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/=20
>>>>=20
>>>> Please vote on releasing this package as Apache Spark 1.1.0!=20
>>>>=20
>>>> The vote is open until Monday, September 01, at 03:11 UTC and passes if=
=20
>>>> a majority of at least 3 +1 PMC votes are cast.=20
>>>>=20
>>>> [ ] +1 Release this package as Apache Spark 1.1.0=20
>>>> [ ] -1 Do not release this package because ...=20
>>>>=20
>>>> To learn more about Apache Spark, please see=20
>>>> http://spark.apache.org/=20
>>>>=20
>>>> =3D=3D Regressions fixed since RC1 =3D=3D=20
>>>> LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277=
=20
>>>>=20
>>>> =3D=3D What justifies a -1 vote for this release? =3D=3D=20
>>>> This vote is happening very late into the QA period compared with=20
>>>> previous votes, so -1 votes should only occur for significant=20
>>>> regressions from 1.0.2. Bugs already present in 1.0.X will not block=20=

>>>> this release.=20
>>>>=20
>>>> =3D=3D What default changes should I be aware of? =3D=3D=20
>>>> 1. The default value of "spark.io.compression.codec" is now "snappy"=20=

>>>> --> Old behavior can be restored by switching to "lzf"=20
>>>>=20
>>>> 2. PySpark now performs external spilling during aggregations.=20
>>>> --> Old behavior can be restored by setting "spark.shuffle.spill" to "f=
alse".=20
>>>>=20
>>>> ---------------------------------------------------------------------=20=

>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>=20
> ---------------------------------------------------------------------=20
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org=20
> For additional commands, e-mail: dev-help@spark.apache.org=20
>=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 16:18:22 2014
Return-Path: <dev-return-9164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 480011128E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 16:18:22 +0000 (UTC)
Received: (qmail 92859 invoked by uid 500); 29 Aug 2014 16:18:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92793 invoked by uid 500); 29 Aug 2014 16:18:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92771 invoked by uid 99); 29 Aug 2014 16:18:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:18:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:17:55 +0000
Received: by mail-ob0-f171.google.com with SMTP id wn1so1959665obc.2
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 09:17:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=WLvnJCyfQnfkrYQQDtcHOE/r7VdfwqK5BTwfUikx6oU=;
        b=eGcAuZUbrp/IYz+1v4M3+k6PVgWU0bCWWHBBLXP1ZXH+VbxWDA1DybyB6ZCopDYiLk
         Qv64qvzxIl4Zqs/0MHW6RlujYyAT+flZ58AzoufuUXkOYr68Ow8vaX3iiZ8+9XUgL3lQ
         Oi5yZa3/o2KuFnmsLgBRy/fsGjVl5uF+D+GaEBzccrlKvnIj+W0VwXwQa2C2E01HBcDW
         s6YEDMmjUR+sUtpV+CWAv6DK0dtlzzUvyJU0xz5gBQWYUi19N9y1Xvy/4Ko4wNQWK0pn
         CFTeISO9aoK2OXgud53fMq4uPkSuGMbrleIbee6sdWS450erghIxUyZ+qi2/g+fg30Cl
         QqQg==
MIME-Version: 1.0
X-Received: by 10.60.161.49 with SMTP id xp17mr11794983oeb.18.1409329073743;
 Fri, 29 Aug 2014 09:17:53 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Fri, 29 Aug 2014 09:17:53 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Fri, 29 Aug 2014 09:17:53 -0700 (PDT)
In-Reply-To: <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<etPan.5400202e.579478fe.470a@mbp-3.local>
	<BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
Date: Fri, 29 Aug 2014 09:17:53 -0700
Message-ID: <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Ye Xianjin <advancedxy@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01176e0dcff75e0501c6fe55
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01176e0dcff75e0501c6fe55
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Okay I'll plan to add cdh4 binary as well for the final release!

---
sent from my phone
On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:

> We just used CDH 4.7 for our production cluster. And I believe we won't
> use CDH 5 in the next year.
>
> Sent from my iPhone
>
> > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia <matei.z=
aharia@gmail.com> wrote:
> >
> > Personally I'd actually consider putting CDH4 back if there are still
> users on it. It's always better to be inclusive, and the convenience of a
> one-click download is high. Do we have a sense on what % of CDH users sti=
ll
> use CDH4?
> >
> > Matei
> >
> > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com) wrote=
:
> >
> > (Copying my reply since I don't know if it goes to the mailing list)
> >
> > Great, thanks for explaining the reasoning. You're saying these aren't
> > going into the final release? I think that moots any issue surrounding
> > distributing them then.
> >
> > This is all I know of from the ASF:
> > https://community.apache.org/projectIndependence.html I don't read it
> > as expressly forbidding this kind of thing although you can see how it
> > bumps up against the spirit. There's not a bright line -- what about
> > Tomcat providing binaries compiled for Windows for example? does that
> > favor an OS vendor?
> >
> > From this technical ASF perspective only the releases matter -- do
> > what you want with snapshots and RCs. The only issue there is maybe
> > releasing something different than was in the RC; is that at all
> > confusing? Just needs a note.
> >
> > I think this theoretical issue doesn't exist if these binaries aren't
> > released, so I see no reason to not proceed.
> >
> > The rest is a different question about whether you want to spend time
> > maintaining this profile and candidate. The vendor already manages
> > their build I think and -- and I don't know -- may even prefer not to
> > have a different special build floating around. There's also the
> > theoretical argument that this turns off other vendors from adopting
> > Spark if it's perceived to be too connected to other vendors. I'd like
> > to maximize Spark's distribution and there's some argument you do this
> > by not making vendor profiles. But as I say a different question to
> > just think about over time...
> >
> > (oh and PS for my part I think it's a good thing that CDH4 binaries
> > were removed. I wasn't arguing for resurrecting them)
> >
> >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >> Hey Sean,
> >>
> >> The reason there are no longer CDH-specific builds is that all newer
> >> versions of CDH and HDP work with builds for the upstream Hadoop
> >> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4) and
> >> the Hadoop-without-Hive (also 2.4) build.
> >>
> >> For MapR - we can't officially post those artifacts on ASF web space
> >> when we make the final release, we can only link to them as being
> >> hosted by MapR specifically since they use non-compatible licenses.
> >> However, I felt that providing these during a testing period was
> >> alright, with the goal of increasing test coverage. I couldn't find
> >> any policy against posting these on personal web space during RC
> >> voting. However, we can remove them if there is one.
> >>
> >> Dropping CDH4 was more because it is now pretty old, but we can add it
> >> back if people want. The binary packaging is a slightly separate
> >> question from release votes, so I can always add more binary packages
> >> whenever. And on this, my main concern is covering the most popular
> >> Hadoop versions to lower the bar for users to build and test Spark.
> >>
> >> - Patrick
> >>
> >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
> >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
> >>> fail any more than usual.
> >>>
> >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
> >>> project and have encountered no problems.
> >>>
> >>>
> >>> I notice that the 1.1.0 release removes the CDH4-specific build, but
> >>> adds two MapR-specific builds. Compare with
> >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
> >>> commented on the commit:
> >>>
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede71=
3a73cc
> >>>
> >>> I'm in favor of removing all vendor-specific builds. This change
> >>> *looks* a bit funny as there was no JIRA (?) and appears to swap one
> >>> vendor for another. Of course there's nothing untoward going on, but
> >>> what was the reasoning? It's best avoided, and MapR already
> >>> distributes Spark just fine, no?
> >>>
> >>> This is a gray area with ASF projects. I mention it as well because i=
t
> >>> came up with Apache Flink recently
> >>> (
> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mb=
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gm=
ail.com%3E
> )
> >>> Another vendor rightly noted this could look like favoritism. They
> >>> changed to remove vendor releases.
> >>>
> >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <pwendell@gmail.com=
>
> wrote:
> >>>> Please vote on releasing the following candidate as Apache Spark
> version 1.1.0!
> >>>>
> >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> >>>>
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711a=
ebb329ca28046396af1e34395a0df92b5327
> >>>>
> >>>> The release files, including signatures, digests, etc. can be found
> at:
> >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> >>>>
> >>>> Release artifacts are signed with the following key:
> >>>> https://people.apache.org/keys/committer/pwendell.asc
> >>>>
> >>>> The staging repository for this release can be found at:
> >>>>
> https://repository.apache.org/content/repositories/orgapachespark-1029/
> >>>>
> >>>> The documentation corresponding to this release can be found at:
> >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> >>>>
> >>>> Please vote on releasing this package as Apache Spark 1.1.0!
> >>>>
> >>>> The vote is open until Monday, September 01, at 03:11 UTC and passes
> if
> >>>> a majority of at least 3 +1 PMC votes are cast.
> >>>>
> >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> >>>> [ ] -1 Do not release this package because ...
> >>>>
> >>>> To learn more about Apache Spark, please see
> >>>> http://spark.apache.org/
> >>>>
> >>>> =3D=3D Regressions fixed since RC1 =3D=3D
> >>>> LZ4 compression issue:
> https://issues.apache.org/jira/browse/SPARK-3277
> >>>>
> >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> >>>> This vote is happening very late into the QA period compared with
> >>>> previous votes, so -1 votes should only occur for significant
> >>>> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> >>>> this release.
> >>>>
> >>>> =3D=3D What default changes should I be aware of? =3D=3D
> >>>> 1. The default value of "spark.io.compression.codec" is now "snappy"
> >>>> --> Old behavior can be restored by switching to "lzf"
> >>>>
> >>>> 2. PySpark now performs external spilling during aggregations.
> >>>> --> Old behavior can be restored by setting "spark.shuffle.spill" to
> "false".
> >>>>
> >>>> --------------------------------------------------------------------=
-
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>

--089e01176e0dcff75e0501c6fe55--

From dev-return-9165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 16:25:14 2014
Return-Path: <dev-return-9165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF1A2112F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 16:25:14 +0000 (UTC)
Received: (qmail 19578 invoked by uid 500); 29 Aug 2014 16:25:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19517 invoked by uid 500); 29 Aug 2014 16:25:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19505 invoked by uid 99); 29 Aug 2014 16:25:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:25:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:24:47 +0000
Received: by mail-wi0-f173.google.com with SMTP id cc10so2445006wib.6
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 09:24:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=a/pA3GahyyweEiGl+I7ERFcY9lxrUtGAeXdWKlwDeOc=;
        b=hZSZ6FYi3W/ibfp871g/7zbk3zxgKjwSDMeYTKc7ZV/RqsnsadDki3mhBWPacgOOTd
         s/+cRxBCWZVWlg4aISVLE8l/kPKqrRgiSSJVthLfMxaqUypLOtEkRRuVpe5f2mOeSz7W
         jAZTDpGbourSXfI8Gr25ZjW6oGYEOVCFtuEADYZ1nXU8WNDM/2iZuiXZM279zd46+E4R
         mEg/kzI2IP+PB/9wpxegFkUM76sLjGZklwrvdeZjBhbL0Ux/dAiZLGIUn37iejYNPm1c
         Y/b+1LQWFYAdkSHzyZy/a4b28gaYJ6SxQ+PFX3npysbv8T58KhzlH+1kofCw7TuABDFX
         ELXA==
X-Received: by 10.195.11.200 with SMTP id ek8mr14727831wjd.85.1409329486790;
 Fri, 29 Aug 2014 09:24:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 29 Aug 2014 09:24:06 -0700 (PDT)
In-Reply-To: <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
 <etPan.5400202e.579478fe.470a@mbp-3.local> <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
 <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 29 Aug 2014 12:24:06 -0400
Message-ID: <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: Ye Xianjin <advancedxy@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b8737626e85580501c71702
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b8737626e85580501c71702
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

There were several formatting and typographical errors in the SQL docs that
I've fixed in this PR <https://github.com/apache/spark/pull/2201>. Dunno if
we want to roll that into the release.


On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Okay I'll plan to add cdh4 binary as well for the final release!
>
> ---
> sent from my phone
> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
>
> > We just used CDH 4.7 for our production cluster. And I believe we won't
> > use CDH 5 in the next year.
> >
> > Sent from my iPhone
> >
> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia <matei=
.zaharia@gmail.com>
> wrote:
> > >
> > > Personally I'd actually consider putting CDH4 back if there are still
> > users on it. It's always better to be inclusive, and the convenience of=
 a
> > one-click download is high. Do we have a sense on what % of CDH users
> still
> > use CDH4?
> > >
> > > Matei
> > >
> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com)
> wrote:
> > >
> > > (Copying my reply since I don't know if it goes to the mailing list)
> > >
> > > Great, thanks for explaining the reasoning. You're saying these aren'=
t
> > > going into the final release? I think that moots any issue surroundin=
g
> > > distributing them then.
> > >
> > > This is all I know of from the ASF:
> > > https://community.apache.org/projectIndependence.html I don't read it
> > > as expressly forbidding this kind of thing although you can see how i=
t
> > > bumps up against the spirit. There's not a bright line -- what about
> > > Tomcat providing binaries compiled for Windows for example? does that
> > > favor an OS vendor?
> > >
> > > From this technical ASF perspective only the releases matter -- do
> > > what you want with snapshots and RCs. The only issue there is maybe
> > > releasing something different than was in the RC; is that at all
> > > confusing? Just needs a note.
> > >
> > > I think this theoretical issue doesn't exist if these binaries aren't
> > > released, so I see no reason to not proceed.
> > >
> > > The rest is a different question about whether you want to spend time
> > > maintaining this profile and candidate. The vendor already manages
> > > their build I think and -- and I don't know -- may even prefer not to
> > > have a different special build floating around. There's also the
> > > theoretical argument that this turns off other vendors from adopting
> > > Spark if it's perceived to be too connected to other vendors. I'd lik=
e
> > > to maximize Spark's distribution and there's some argument you do thi=
s
> > > by not making vendor profiles. But as I say a different question to
> > > just think about over time...
> > >
> > > (oh and PS for my part I think it's a good thing that CDH4 binaries
> > > were removed. I wasn't arguing for resurrecting them)
> > >
> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.com=
>
> > wrote:
> > >> Hey Sean,
> > >>
> > >> The reason there are no longer CDH-specific builds is that all newer
> > >> versions of CDH and HDP work with builds for the upstream Hadoop
> > >> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4) an=
d
> > >> the Hadoop-without-Hive (also 2.4) build.
> > >>
> > >> For MapR - we can't officially post those artifacts on ASF web space
> > >> when we make the final release, we can only link to them as being
> > >> hosted by MapR specifically since they use non-compatible licenses.
> > >> However, I felt that providing these during a testing period was
> > >> alright, with the goal of increasing test coverage. I couldn't find
> > >> any policy against posting these on personal web space during RC
> > >> voting. However, we can remove them if there is one.
> > >>
> > >> Dropping CDH4 was more because it is now pretty old, but we can add =
it
> > >> back if people want. The binary packaging is a slightly separate
> > >> question from release votes, so I can always add more binary package=
s
> > >> whenever. And on this, my main concern is covering the most popular
> > >> Hadoop versions to lower the bar for users to build and test Spark.
> > >>
> > >> - Patrick
> > >>
> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don'=
t
> > >>> fail any more than usual.
> > >>>
> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in anoth=
er
> > >>> project and have encountered no problems.
> > >>>
> > >>>
> > >>> I notice that the 1.1.0 release removes the CDH4-specific build, bu=
t
> > >>> adds two MapR-specific builds. Compare with
> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
> > >>> commented on the commit:
> > >>>
> >
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede71=
3a73cc
> > >>>
> > >>> I'm in favor of removing all vendor-specific builds. This change
> > >>> *looks* a bit funny as there was no JIRA (?) and appears to swap on=
e
> > >>> vendor for another. Of course there's nothing untoward going on, bu=
t
> > >>> what was the reasoning? It's best avoided, and MapR already
> > >>> distributes Spark just fine, no?
> > >>>
> > >>> This is a gray area with ASF projects. I mention it as well because
> it
> > >>> came up with Apache Flink recently
> > >>> (
> >
> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mb=
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gm=
ail.com%3E
> > )
> > >>> Another vendor rightly noted this could look like favoritism. They
> > >>> changed to remove vendor releases.
> > >>>
> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell <
> pwendell@gmail.com>
> > wrote:
> > >>>> Please vote on releasing the following candidate as Apache Spark
> > version 1.1.0!
> > >>>>
> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> > >>>>
> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711a=
ebb329ca28046396af1e34395a0df92b5327
> > >>>>
> > >>>> The release files, including signatures, digests, etc. can be foun=
d
> > at:
> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> > >>>>
> > >>>> Release artifacts are signed with the following key:
> > >>>> https://people.apache.org/keys/committer/pwendell.asc
> > >>>>
> > >>>> The staging repository for this release can be found at:
> > >>>>
> > https://repository.apache.org/content/repositories/orgapachespark-1029/
> > >>>>
> > >>>> The documentation corresponding to this release can be found at:
> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> > >>>>
> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
> > >>>>
> > >>>> The vote is open until Monday, September 01, at 03:11 UTC and pass=
es
> > if
> > >>>> a majority of at least 3 +1 PMC votes are cast.
> > >>>>
> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> > >>>> [ ] -1 Do not release this package because ...
> > >>>>
> > >>>> To learn more about Apache Spark, please see
> > >>>> http://spark.apache.org/
> > >>>>
> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
> > >>>> LZ4 compression issue:
> > https://issues.apache.org/jira/browse/SPARK-3277
> > >>>>
> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> > >>>> This vote is happening very late into the QA period compared with
> > >>>> previous votes, so -1 votes should only occur for significant
> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will not blo=
ck
> > >>>> this release.
> > >>>>
> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
> > >>>> 1. The default value of "spark.io.compression.codec" is now "snapp=
y"
> > >>>> --> Old behavior can be restored by switching to "lzf"
> > >>>>
> > >>>> 2. PySpark now performs external spilling during aggregations.
> > >>>> --> Old behavior can be restored by setting "spark.shuffle.spill" =
to
> > "false".
> > >>>>
> > >>>>
> ---------------------------------------------------------------------
> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> >
>

--047d7b8737626e85580501c71702--

From dev-return-9166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 16:56:10 2014
Return-Path: <dev-return-9166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E4D6611453
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 16:56:09 +0000 (UTC)
Received: (qmail 14580 invoked by uid 500); 29 Aug 2014 16:56:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14532 invoked by uid 500); 29 Aug 2014 16:56:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14514 invoked by uid 99); 29 Aug 2014 16:56:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:56:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 16:56:04 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so1988370obc.33
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 09:55:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=FNYAF/6FVTpvFzWU1gl5LVHJQ/+21M9dywXmqGr5r0c=;
        b=bjww4ZhMWLOHTVKQbxN7sEc9Gs3k91g+l5SyU+rbcxLfbStiN772uO3fh12neqTAjm
         jtHZl0VqU87Ap87iEDbN82JqlKJA7JhqKLipso7hWoiiz4wReGs4dkDl7mCVD6aQvG5x
         zdg1pURlnymIZR0Bybp3fto5NQHZ0Znj5CNammAD8jixKfL5FV5yU4RSwR39RRxnwmD0
         UhoRy2uM3yY/ZLY75qMh+IbAVNYTwZZ0wb5bm9JYzX5NKCrHk341pq29hGODfDfUW30/
         FWqwTzE3+ooOYNSgBob56EZHX9OkPNf4QyCRWGRBFw8WwBfKn2HuHVYnYgh6ntAfAoKt
         +TVg==
MIME-Version: 1.0
X-Received: by 10.182.191.39 with SMTP id gv7mr11358716obc.14.1409331344085;
 Fri, 29 Aug 2014 09:55:44 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Fri, 29 Aug 2014 09:55:44 -0700 (PDT)
In-Reply-To: <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<etPan.5400202e.579478fe.470a@mbp-3.local>
	<BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
	<CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
	<CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
Date: Fri, 29 Aug 2014 09:55:44 -0700
Message-ID: <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Ye Xianjin <advancedxy@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nicholas,

Thanks for this, we can merge in doc changes outside of the actual
release timeline, so we'll make sure to loop those changes in before
we publish the final 1.1 docs.

- Patrick

On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> There were several formatting and typographical errors in the SQL docs th=
at
> I've fixed in this PR. Dunno if we want to roll that into the release.
>
>
> On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>>
>> Okay I'll plan to add cdh4 binary as well for the final release!
>>
>> ---
>> sent from my phone
>> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
>>
>> > We just used CDH 4.7 for our production cluster. And I believe we won'=
t
>> > use CDH 5 in the next year.
>> >
>> > Sent from my iPhone
>> >
>> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia <mate=
i.zaharia@gmail.com>
>> > > wrote:
>> > >
>> > > Personally I'd actually consider putting CDH4 back if there are stil=
l
>> > users on it. It's always better to be inclusive, and the convenience o=
f
>> > a
>> > one-click download is high. Do we have a sense on what % of CDH users
>> > still
>> > use CDH4?
>> > >
>> > > Matei
>> > >
>> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com)
>> > > wrote:
>> > >
>> > > (Copying my reply since I don't know if it goes to the mailing list)
>> > >
>> > > Great, thanks for explaining the reasoning. You're saying these aren=
't
>> > > going into the final release? I think that moots any issue surroundi=
ng
>> > > distributing them then.
>> > >
>> > > This is all I know of from the ASF:
>> > > https://community.apache.org/projectIndependence.html I don't read i=
t
>> > > as expressly forbidding this kind of thing although you can see how =
it
>> > > bumps up against the spirit. There's not a bright line -- what about
>> > > Tomcat providing binaries compiled for Windows for example? does tha=
t
>> > > favor an OS vendor?
>> > >
>> > > From this technical ASF perspective only the releases matter -- do
>> > > what you want with snapshots and RCs. The only issue there is maybe
>> > > releasing something different than was in the RC; is that at all
>> > > confusing? Just needs a note.
>> > >
>> > > I think this theoretical issue doesn't exist if these binaries aren'=
t
>> > > released, so I see no reason to not proceed.
>> > >
>> > > The rest is a different question about whether you want to spend tim=
e
>> > > maintaining this profile and candidate. The vendor already manages
>> > > their build I think and -- and I don't know -- may even prefer not t=
o
>> > > have a different special build floating around. There's also the
>> > > theoretical argument that this turns off other vendors from adopting
>> > > Spark if it's perceived to be too connected to other vendors. I'd li=
ke
>> > > to maximize Spark's distribution and there's some argument you do th=
is
>> > > by not making vendor profiles. But as I say a different question to
>> > > just think about over time...
>> > >
>> > > (oh and PS for my part I think it's a good thing that CDH4 binaries
>> > > were removed. I wasn't arguing for resurrecting them)
>> > >
>> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <pwendell@gmail.co=
m>
>> > wrote:
>> > >> Hey Sean,
>> > >>
>> > >> The reason there are no longer CDH-specific builds is that all newe=
r
>> > >> versions of CDH and HDP work with builds for the upstream Hadoop
>> > >> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4) a=
nd
>> > >> the Hadoop-without-Hive (also 2.4) build.
>> > >>
>> > >> For MapR - we can't officially post those artifacts on ASF web spac=
e
>> > >> when we make the final release, we can only link to them as being
>> > >> hosted by MapR specifically since they use non-compatible licenses.
>> > >> However, I felt that providing these during a testing period was
>> > >> alright, with the goal of increasing test coverage. I couldn't find
>> > >> any policy against posting these on personal web space during RC
>> > >> voting. However, we can remove them if there is one.
>> > >>
>> > >> Dropping CDH4 was more because it is now pretty old, but we can add
>> > >> it
>> > >> back if people want. The binary packaging is a slightly separate
>> > >> question from release votes, so I can always add more binary packag=
es
>> > >> whenever. And on this, my main concern is covering the most popular
>> > >> Hadoop versions to lower the bar for users to build and test Spark.
>> > >>
>> > >> - Patrick
>> > >>
>> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com>
>> > wrote:
>> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
>> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests... don=
't
>> > >>> fail any more than usual.
>> > >>>
>> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in
>> > >>> another
>> > >>> project and have encountered no problems.
>> > >>>
>> > >>>
>> > >>> I notice that the 1.1.0 release removes the CDH4-specific build, b=
ut
>> > >>> adds two MapR-specific builds. Compare with
>> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
>> > >>> commented on the commit:
>> > >>>
>> >
>> > https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ed=
e713a73cc
>> > >>>
>> > >>> I'm in favor of removing all vendor-specific builds. This change
>> > >>> *looks* a bit funny as there was no JIRA (?) and appears to swap o=
ne
>> > >>> vendor for another. Of course there's nothing untoward going on, b=
ut
>> > >>> what was the reasoning? It's best avoided, and MapR already
>> > >>> distributes Spark just fine, no?
>> > >>>
>> > >>> This is a gray area with ASF projects. I mention it as well becaus=
e
>> > >>> it
>> > >>> came up with Apache Flink recently
>> > >>> (
>> >
>> > http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408=
.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail=
.gmail.com%3E
>> > )
>> > >>> Another vendor rightly noted this could look like favoritism. They
>> > >>> changed to remove vendor releases.
>> > >>>
>> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
>> > >>>> <pwendell@gmail.com>
>> > wrote:
>> > >>>> Please vote on releasing the following candidate as Apache Spark
>> > version 1.1.0!
>> > >>>>
>> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>> > >>>>
>> >
>> > https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D7=
11aebb329ca28046396af1e34395a0df92b5327
>> > >>>>
>> > >>>> The release files, including signatures, digests, etc. can be fou=
nd
>> > at:
>> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>> > >>>>
>> > >>>> Release artifacts are signed with the following key:
>> > >>>> https://people.apache.org/keys/committer/pwendell.asc
>> > >>>>
>> > >>>> The staging repository for this release can be found at:
>> > >>>>
>> > https://repository.apache.org/content/repositories/orgapachespark-1029=
/
>> > >>>>
>> > >>>> The documentation corresponding to this release can be found at:
>> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>> > >>>>
>> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
>> > >>>>
>> > >>>> The vote is open until Monday, September 01, at 03:11 UTC and
>> > >>>> passes
>> > if
>> > >>>> a majority of at least 3 +1 PMC votes are cast.
>> > >>>>
>> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
>> > >>>> [ ] -1 Do not release this package because ...
>> > >>>>
>> > >>>> To learn more about Apache Spark, please see
>> > >>>> http://spark.apache.org/
>> > >>>>
>> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
>> > >>>> LZ4 compression issue:
>> > https://issues.apache.org/jira/browse/SPARK-3277
>> > >>>>
>> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> > >>>> This vote is happening very late into the QA period compared with
>> > >>>> previous votes, so -1 votes should only occur for significant
>> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will not
>> > >>>> block
>> > >>>> this release.
>> > >>>>
>> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
>> > >>>> 1. The default value of "spark.io.compression.codec" is now
>> > >>>> "snappy"
>> > >>>> --> Old behavior can be restored by switching to "lzf"
>> > >>>>
>> > >>>> 2. PySpark now performs external spilling during aggregations.
>> > >>>> --> Old behavior can be restored by setting "spark.shuffle.spill"
>> > >>>> to
>> > "false".
>> > >>>>
>> > >>>>
>> > >>>> -----------------------------------------------------------------=
----
>> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
>> > >
>> > > --------------------------------------------------------------------=
-
>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > For additional commands, e-mail: dev-help@spark.apache.org
>> > >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 17:04:10 2014
Return-Path: <dev-return-9167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 23B6C114B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 17:04:10 +0000 (UTC)
Received: (qmail 39199 invoked by uid 500); 29 Aug 2014 17:04:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39124 invoked by uid 500); 29 Aug 2014 17:04:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39111 invoked by uid 99); 29 Aug 2014 17:04:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:04:09 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=FORGED_YAHOO_RCVD,FREEMAIL_REPLY,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zlgonzalez@yahoo.com designates 98.139.213.163 as permitted sender)
Received: from [98.139.213.163] (HELO nm1-vm1.bullet.mail.bf1.yahoo.com) (98.139.213.163)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:04:01 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=IyVM0nLbiTSFXk/4OboiHEn2ho3jaXx4nqdYSHllkeV587VcQzu/GyN8RLZ4ImhnXv3YQrR0c14zXnjhn9XbXo2fqa1vWVFVqSPLfRhCdJxf3Ogz9PAGDGqTQsTjKZo9A+rWCp2LjiWaFiHV16HOZnmw0kfvDATkuMyjFv3JfdR0gobPXLMSQhJ1GeZMOdqjhYUaU/0iFCP6g1a6QIzZjUJv8fHfofdtkqEEEAC0bA/KCF5ohVyXD/UrCJpJeSJsEzveaOQ9+zgh4sf/WKUNZEFichYqgoTLNqT3lbnyMFhZ5bNW5KjYjpvB/1AfeaM8Z+yMOAr+WmSZ76lLNXt/kA==;
Received: from [98.139.212.151] by nm1.bullet.mail.bf1.yahoo.com with NNFMP; 29 Aug 2014 17:03:40 -0000
Received: from [98.139.211.163] by tm8.bullet.mail.bf1.yahoo.com with NNFMP; 29 Aug 2014 17:03:40 -0000
Received: from [127.0.0.1] by smtp220.mail.bf1.yahoo.com with NNFMP; 29 Aug 2014 17:03:40 -0000
X-Yahoo-Newman-Id: 742945.81786.bm@smtp220.mail.bf1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: FiiPuggVM1melxtyJZjhf9gfFC3Cqt4vq8NMIYuaXh0mLNH
 Zlv9UyMnk1CeRNz0gG_NtAAYYo1JdU.AiTK3KEHL1sOKrIgxaNyPzUUMwGFP
 m7cJbHq.2TrJEnsFUJcUFPDt9cleXxHnuQEMHlS4.G96kLafJI0vjH8ZRQgC
 j9ETh9uEEQPQyMvVqFefCBkaQMFZnLp31p3swZcRhGenl7HkoDD641nqOTlR
 iQXy9rpLOWRxZ.8_tOb7d7QxwPLAHeB5S6qIYWY7GhXKeQGtygm77EF7IB6K
 rA9GrkjuKmCH0xK8YsOUFmCYbpqCX4Fp7YE0gfQYlCdBrULLe83R1MrSEoIc
 ob7QS4FnyDAjvruS3V56Bg6swN3fWRCaqvV1Ndg8Q2ya0PnabKJf_MictNE1
 AvLSTg7lPgQ_eQ5.guoi8uCE2m7psEFPyVg_sF3ksZh6f2hK8qrt84MFInwi
 OJvBdl1WJl0.ESic2hgbkMTxbWRKj4k0kRedKwKuf7pHXZyzEWXA79vI7gYW
 RkXbGtLbmRnvnyhoJDoTS7I6nIA--
X-Yahoo-SMTP: PcDvtRuswBD3HctS900Qj4PHM0_codE-
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Jira tickets for starter tasks
From: Ron's Yahoo! <zlgonzalez@yahoo.com.INVALID>
In-Reply-To: <etPan.53ffb3b8.2443a858.104@joshs-mbp>
Date: Fri, 29 Aug 2014 10:03:25 -0700
Cc: Bill Bejeck <bbejeck@gmail.com>,
 dev@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <19E8DB9B-E581-4B8A-938B-ED55B78FD016@yahoo.com>
References: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com> <etPan.53ffb3b8.2443a858.104@joshs-mbp>
To: Josh Rosen <rosenville@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Josh,
  Can you add me as well?

Thanks,
Ron

On Aug 28, 2014, at 3:56 PM, Josh Rosen <rosenville@gmail.com> wrote:

> A JIRA admin needs to add you to the =91=92Contributors=94 role group =
in order to allow you to assign issues to yourself.  I=92ve added this =
email address to that group, so you should be set!
>=20
> - Josh
>=20
>=20
> On August 28, 2014 at 3:52:57 PM, Bill Bejeck (bbejeck@gmail.com) =
wrote:
>=20
> Hi, =20
>=20
> How do I get a starter task jira ticket assigned to myself? Or do I =
just do =20
> the work and issue a pull request with the associated jira number? =20
>=20
> Thanks, =20
> Bill =20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 17:06:23 2014
Return-Path: <dev-return-9168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47930114CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 17:06:23 +0000 (UTC)
Received: (qmail 42507 invoked by uid 500); 29 Aug 2014 17:06:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42440 invoked by uid 500); 29 Aug 2014 17:06:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42427 invoked by uid 99); 29 Aug 2014 17:06:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:06:22 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:06:15 +0000
Received: by mail-pa0-f41.google.com with SMTP id lj1so6859834pab.28
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 10:05:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=aMmlkDSkJM1xqbQEllGtUWps1bSUodOlJCtYEXLiguI=;
        b=PEf6ivcd8hoYzgu1jScUPL3tEEq1nw+r0u41Hsx5fcfGLsovTuMlTsDHiULvWXYbPO
         vGgF3wcmI0zUFfFnOnd1EAsfYkL2lYeW0XHDi9/fd7S2YPKYI/N8HFkbDIwW5Wu4BqJ0
         SADaJldz8W/PSQ5pNhPzyjvaqD2X9KlAKTelEtbZuHxb0Bp8wdsMKPYcuUeFMT93ubu+
         tpylO1BvTBPbwpIGl+tpLxN113NXWzlpX+Ed/96RPqE+pPJ2+ixDdmCq6UcZz/1wI7Hf
         e9BDDqv3xFhy5WxShwUQDZ3vmNsis/Z2w4Y4eAuRGoTwTxFJmh6omxI/WaKcGdok7xky
         FJpQ==
X-Received: by 10.68.134.201 with SMTP id pm9mr17183936pbb.143.1409331955401;
        Fri, 29 Aug 2014 10:05:55 -0700 (PDT)
Received: from joshs-mbp (50-1-84-111.dsl.dynamic.sonic.net. [50.1.84.111])
        by mx.google.com with ESMTPSA id v10sm477604pbs.10.2014.08.29.10.05.53
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Fri, 29 Aug 2014 10:05:54 -0700 (PDT)
Date: Fri, 29 Aug 2014 10:05:52 -0700
From: Josh Rosen <rosenville@gmail.com>
To: =?utf-8?Q?Ron's_Yahoo=21?= <zlgonzalez@yahoo.com>
Cc: dev@spark.apache.org
Message-ID: <etPan.5400b2f1.8edbdab.104@joshs-mbp>
In-Reply-To: <19E8DB9B-E581-4B8A-938B-ED55B78FD016@yahoo.com>
References: <CAF7WS+qAKzzDshDqNy=jrRQWvHVVcupEL3zBtE69CRuuZh2izw@mail.gmail.com>
 <etPan.53ffb3b8.2443a858.104@joshs-mbp>
 <19E8DB9B-E581-4B8A-938B-ED55B78FD016@yahoo.com>
Subject: Re: Jira tickets for starter tasks
X-Mailer: Airmail Beta (250)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5400b2f1_79838cb2_104"
X-Virus-Checked: Checked by ClamAV on apache.org

--5400b2f1_79838cb2_104
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Added you; you should be set=21

If anyone else wants me to add them, please email me off-list so that we =
don=E2=80=99t end up flooding the dev list with replies. Thanks=21


On August 29, 2014 at 10:03:41 AM, Ron's Yahoo=21 (zlgonzalez=40yahoo.com=
) wrote:

Hi Josh, =20
Can you add me as well=3F =20

Thanks, =20
Ron =20

On Aug 28, 2014, at 3:56 PM, Josh Rosen <rosenville=40gmail.com> wrote: =20

> A JIRA admin needs to add you to the =E2=80=98=E2=80=99Contributors=E2=80=
=9D role group in order to allow you to assign issues to yourself. I=E2=80=
=99ve added this email address to that group, so you should be set=21 =20
> =20
> - Josh =20
> =20
> =20
> On August 28, 2014 at 3:52:57 PM, Bill Bejeck (bbejeck=40gmail.com) wro=
te: =20
> =20
> Hi, =20
> =20
> How do I get a starter task jira ticket assigned to myself=3F Or do I j=
ust do =20
> the work and issue a pull request with the associated jira number=3F =20
> =20
> Thanks, =20
> Bill =20


--5400b2f1_79838cb2_104--


From dev-return-9169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 17:19:39 2014
Return-Path: <dev-return-9169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E16011584
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 17:19:39 +0000 (UTC)
Received: (qmail 86614 invoked by uid 500); 29 Aug 2014 17:19:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86545 invoked by uid 500); 29 Aug 2014 17:19:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86533 invoked by uid 99); 29 Aug 2014 17:19:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:19:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:19:34 +0000
Received: by mail-wi0-f177.google.com with SMTP id cc10so2921352wib.4
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 10:19:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=80ZxUn6zhGd1GWaKZTGu29ULOkLshaY2X1F9N3wPy+M=;
        b=Z4Ncsj/IyxMIEdiezmTaw4O6Neh/1SwE9CLvtcuC/NJQFzxfWiHs/h/wVF0QL6wHwt
         uFpGEP349+ak+a+ywbXJjJCxYKkH0QpsmM7vh/BZXAY7UMypLJLw99bvSjrydFuFkl4W
         x3xXLs85Y29d5mYzhYK0iNWj7KsmCEJDJlZohIjpHiRwa2usCzluqWZQqwon5f4QHvwH
         hjM7ejafRHmqIFApb37/PtrlhcDY/aX7Oilww92JTV1dK1YzyCLLaKscN/ReLXEgpZeq
         7JJYIP2RvC60iv9aAzjVpgiynbzr0zOLNWqQIsbpPKfSyh99yZ61rjqIEV+3Q0okxMK/
         nI7g==
X-Received: by 10.194.71.210 with SMTP id x18mr14572207wju.6.1409332752898;
 Fri, 29 Aug 2014 10:19:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 29 Aug 2014 10:18:32 -0700 (PDT)
In-Reply-To: <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
 <etPan.5400202e.579478fe.470a@mbp-3.local> <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
 <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
 <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com> <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 29 Aug 2014 13:18:32 -0400
Message-ID: <CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: Ye Xianjin <advancedxy@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd07761b5f2d0501c7dae7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd07761b5f2d0501c7dae7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

[Let me know if I should be posting these comments in a different thread.]

Should the default Spark version in spark-ec2
<https://github.com/apache/spark/blob/e1535ad3c6f7400f2b7915ea91da9c6051055=
7ba/ec2/spark_ec2.py#L86>
be updated for this release?

Nick
=E2=80=8B


On Fri, Aug 29, 2014 at 12:55 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey Nicholas,
>
> Thanks for this, we can merge in doc changes outside of the actual
> release timeline, so we'll make sure to loop those changes in before
> we publish the final 1.1 docs.
>
> - Patrick
>
> On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > There were several formatting and typographical errors in the SQL docs
> that
> > I've fixed in this PR. Dunno if we want to roll that into the release.
> >
> >
> > On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >>
> >> Okay I'll plan to add cdh4 binary as well for the final release!
> >>
> >> ---
> >> sent from my phone
> >> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
> >>
> >> > We just used CDH 4.7 for our production cluster. And I believe we
> won't
> >> > use CDH 5 in the next year.
> >> >
> >> > Sent from my iPhone
> >> >
> >> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia <ma=
tei.zaharia@gmail.com>
> >> > > wrote:
> >> > >
> >> > > Personally I'd actually consider putting CDH4 back if there are
> still
> >> > users on it. It's always better to be inclusive, and the convenience
> of
> >> > a
> >> > one-click download is high. Do we have a sense on what % of CDH user=
s
> >> > still
> >> > use CDH4?
> >> > >
> >> > > Matei
> >> > >
> >> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com)
> >> > > wrote:
> >> > >
> >> > > (Copying my reply since I don't know if it goes to the mailing lis=
t)
> >> > >
> >> > > Great, thanks for explaining the reasoning. You're saying these
> aren't
> >> > > going into the final release? I think that moots any issue
> surrounding
> >> > > distributing them then.
> >> > >
> >> > > This is all I know of from the ASF:
> >> > > https://community.apache.org/projectIndependence.html I don't read
> it
> >> > > as expressly forbidding this kind of thing although you can see ho=
w
> it
> >> > > bumps up against the spirit. There's not a bright line -- what abo=
ut
> >> > > Tomcat providing binaries compiled for Windows for example? does
> that
> >> > > favor an OS vendor?
> >> > >
> >> > > From this technical ASF perspective only the releases matter -- do
> >> > > what you want with snapshots and RCs. The only issue there is mayb=
e
> >> > > releasing something different than was in the RC; is that at all
> >> > > confusing? Just needs a note.
> >> > >
> >> > > I think this theoretical issue doesn't exist if these binaries
> aren't
> >> > > released, so I see no reason to not proceed.
> >> > >
> >> > > The rest is a different question about whether you want to spend
> time
> >> > > maintaining this profile and candidate. The vendor already manages
> >> > > their build I think and -- and I don't know -- may even prefer not
> to
> >> > > have a different special build floating around. There's also the
> >> > > theoretical argument that this turns off other vendors from adopti=
ng
> >> > > Spark if it's perceived to be too connected to other vendors. I'd
> like
> >> > > to maximize Spark's distribution and there's some argument you do
> this
> >> > > by not making vendor profiles. But as I say a different question t=
o
> >> > > just think about over time...
> >> > >
> >> > > (oh and PS for my part I think it's a good thing that CDH4 binarie=
s
> >> > > were removed. I wasn't arguing for resurrecting them)
> >> > >
> >> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell <
> pwendell@gmail.com>
> >> > wrote:
> >> > >> Hey Sean,
> >> > >>
> >> > >> The reason there are no longer CDH-specific builds is that all
> newer
> >> > >> versions of CDH and HDP work with builds for the upstream Hadoop
> >> > >> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4)
> and
> >> > >> the Hadoop-without-Hive (also 2.4) build.
> >> > >>
> >> > >> For MapR - we can't officially post those artifacts on ASF web
> space
> >> > >> when we make the final release, we can only link to them as being
> >> > >> hosted by MapR specifically since they use non-compatible license=
s.
> >> > >> However, I felt that providing these during a testing period was
> >> > >> alright, with the goal of increasing test coverage. I couldn't fi=
nd
> >> > >> any policy against posting these on personal web space during RC
> >> > >> voting. However, we can remove them if there is one.
> >> > >>
> >> > >> Dropping CDH4 was more because it is now pretty old, but we can a=
dd
> >> > >> it
> >> > >> back if people want. The binary packaging is a slightly separate
> >> > >> question from release votes, so I can always add more binary
> packages
> >> > >> whenever. And on this, my main concern is covering the most popul=
ar
> >> > >> Hadoop versions to lower the bar for users to build and test Spar=
k.
> >> > >>
> >> > >> - Patrick
> >> > >>
> >> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com>
> >> > wrote:
> >> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
> >> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests...
> don't
> >> > >>> fail any more than usual.
> >> > >>>
> >> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in
> >> > >>> another
> >> > >>> project and have encountered no problems.
> >> > >>>
> >> > >>>
> >> > >>> I notice that the 1.1.0 release removes the CDH4-specific build,
> but
> >> > >>> adds two MapR-specific builds. Compare with
> >> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
> >> > >>> commented on the commit:
> >> > >>>
> >> >
> >> >
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede71=
3a73cc
> >> > >>>
> >> > >>> I'm in favor of removing all vendor-specific builds. This change
> >> > >>> *looks* a bit funny as there was no JIRA (?) and appears to swap
> one
> >> > >>> vendor for another. Of course there's nothing untoward going on,
> but
> >> > >>> what was the reasoning? It's best avoided, and MapR already
> >> > >>> distributes Spark just fine, no?
> >> > >>>
> >> > >>> This is a gray area with ASF projects. I mention it as well
> because
> >> > >>> it
> >> > >>> came up with Apache Flink recently
> >> > >>> (
> >> >
> >> >
> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mb=
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gm=
ail.com%3E
> >> > )
> >> > >>> Another vendor rightly noted this could look like favoritism. Th=
ey
> >> > >>> changed to remove vendor releases.
> >> > >>>
> >> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
> >> > >>>> <pwendell@gmail.com>
> >> > wrote:
> >> > >>>> Please vote on releasing the following candidate as Apache Spar=
k
> >> > version 1.1.0!
> >> > >>>>
> >> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> >> > >>>>
> >> >
> >> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711a=
ebb329ca28046396af1e34395a0df92b5327
> >> > >>>>
> >> > >>>> The release files, including signatures, digests, etc. can be
> found
> >> > at:
> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> >> > >>>>
> >> > >>>> Release artifacts are signed with the following key:
> >> > >>>> https://people.apache.org/keys/committer/pwendell.asc
> >> > >>>>
> >> > >>>> The staging repository for this release can be found at:
> >> > >>>>
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1029/
> >> > >>>>
> >> > >>>> The documentation corresponding to this release can be found at=
:
> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> >> > >>>>
> >> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
> >> > >>>>
> >> > >>>> The vote is open until Monday, September 01, at 03:11 UTC and
> >> > >>>> passes
> >> > if
> >> > >>>> a majority of at least 3 +1 PMC votes are cast.
> >> > >>>>
> >> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> >> > >>>> [ ] -1 Do not release this package because ...
> >> > >>>>
> >> > >>>> To learn more about Apache Spark, please see
> >> > >>>> http://spark.apache.org/
> >> > >>>>
> >> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
> >> > >>>> LZ4 compression issue:
> >> > https://issues.apache.org/jira/browse/SPARK-3277
> >> > >>>>
> >> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> >> > >>>> This vote is happening very late into the QA period compared wi=
th
> >> > >>>> previous votes, so -1 votes should only occur for significant
> >> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will not
> >> > >>>> block
> >> > >>>> this release.
> >> > >>>>
> >> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
> >> > >>>> 1. The default value of "spark.io.compression.codec" is now
> >> > >>>> "snappy"
> >> > >>>> --> Old behavior can be restored by switching to "lzf"
> >> > >>>>
> >> > >>>> 2. PySpark now performs external spilling during aggregations.
> >> > >>>> --> Old behavior can be restored by setting "spark.shuffle.spil=
l"
> >> > >>>> to
> >> > "false".
> >> > >>>>
> >> > >>>>
> >> > >>>>
> ---------------------------------------------------------------------
> >> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> > >
> >> > >
> ---------------------------------------------------------------------
> >> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > > For additional commands, e-mail: dev-help@spark.apache.org
> >> > >
> >> >
> >
> >
>

--047d7bfd07761b5f2d0501c7dae7--

From dev-return-9170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 17:27:42 2014
Return-Path: <dev-return-9170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D6D6115D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 17:27:42 +0000 (UTC)
Received: (qmail 2262 invoked by uid 500); 29 Aug 2014 17:27:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2197 invoked by uid 500); 29 Aug 2014 17:27:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2186 invoked by uid 99); 29 Aug 2014 17:27:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:27:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 17:27:15 +0000
Received: by mail-lb0-f180.google.com with SMTP id w7so2999686lbi.11
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 10:27:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=MycKtg1zrfA6mpeEOzDH5FyrBeBRhznMpE4dDPtyNW4=;
        b=StLp29jgsMz7dMJz3HQCtx5opkQB6husUn5vSdZOeSOzKyZzp0+qeEatDpxSSPFjfC
         nqR0mTCb4mhPxVeqoT26SvzGhbJ2LP3upasyG6Zdl1iAVNx03UG1VHlohaHykOm+xNnd
         UB+QmK/v39tRwPdR79bRiT712l0zyPIZAd1/9TcJxFg1ymmC05BKDnjYhNMjDvjjSdc+
         j5WNZFMmP+76J5qoKb91j0X4+tEj/kZCeRsIyj8pRfLCVQhCerhGkv0Is/4bUrQzS1nV
         PLUVuVQ8fw/qtNPoyUQWPazMsBfBLzbdxtWLPCyTt5YLQY52XV0P5m33NtSkfc+hCZqL
         YqSQ==
X-Gm-Message-State: ALoCoQnehzzH3lHDDF9/DN33f9gtGMGVMrItYxBG5lZZGGZJbo/Dk+gdD5Cq26SnwMqsnwxf4HwV
X-Received: by 10.112.4.70 with SMTP id i6mr12153168lbi.54.1409333234245; Fri,
 29 Aug 2014 10:27:14 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.70.200 with HTTP; Fri, 29 Aug 2014 10:26:54 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 29 Aug 2014 10:26:54 -0700
Message-ID: <CACdU-dRjdRtEC5pgZmpEy1E7LEngSwsdo0UG0ou+pzmWbcG2AQ@mail.gmail.com>
Subject: new jenkins plugin installed and ready for use
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae94ed6cdcc348b0501c7f61e
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae94ed6cdcc348b0501c7f61e
Content-Type: text/plain; charset=UTF-8

i have always found the 'Rebuild' plugin super useful:
https://wiki.jenkins-ci.org/display/JENKINS/Rebuild+Plugin

this is installed and enables.  enjoy!

shane

--14dae94ed6cdcc348b0501c7f61e--

From dev-return-9171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 18:18:36 2014
Return-Path: <dev-return-9171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A51DF117F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 18:18:36 +0000 (UTC)
Received: (qmail 24609 invoked by uid 500); 29 Aug 2014 18:18:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24536 invoked by uid 500); 29 Aug 2014 18:18:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24523 invoked by uid 99); 29 Aug 2014 18:18:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:18:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:18:07 +0000
Received: by mail-oa0-f53.google.com with SMTP id eb12so2079265oac.12
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 11:18:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=Y5DVhklobFyTYGy7EbdLhNoWB0dKKKn8cbpwNOpwl0Y=;
        b=czRDXznoFDTf0wIbZgXovnS/ald3RM7mHFBK0MnMohAU0MGJ25x+YzjSAtgn76CMJU
         fElJLs+2JiFpAdwX6HXdsNovyBIrAXnhpjschmLlZmPiWvXajelyFxwjZVULsiENo6KI
         0Dt0ujjOd+98lq6umfYro+VL/bFBPU7wjE/mkzPYlTD20SayGyxyAyY2vRKnofL+FYOH
         3t2U1/uazZPlkBMwLFZRxMCoDyZKyVfaUyw/38LA+M0QDgnic5f7yYmMVP00Ydf2SvBt
         ZqoLKuofmDSKLskJf0Av4BRxNALKetiTOrNrJ9aI8S/JD94p074q3qCPI8ziQKWvCRLh
         b/GQ==
MIME-Version: 1.0
X-Received: by 10.182.142.67 with SMTP id ru3mr12137526obb.15.1409336285831;
 Fri, 29 Aug 2014 11:18:05 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Fri, 29 Aug 2014 11:18:05 -0700 (PDT)
In-Reply-To: <CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<etPan.5400202e.579478fe.470a@mbp-3.local>
	<BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
	<CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
	<CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
	<CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
	<CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com>
Date: Fri, 29 Aug 2014 11:18:05 -0700
Message-ID: <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Ye Xianjin <advancedxy@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Oh darn - I missed this update. GRR, unfortunately I think this means
I'll need to cut a new RC. Thanks for catching this Nick.

On Fri, Aug 29, 2014 at 10:18 AM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> [Let me know if I should be posting these comments in a different thread.=
]
>
> Should the default Spark version in spark-ec2 be updated for this release=
?
>
> Nick
>
>
>
> On Fri, Aug 29, 2014 at 12:55 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>>
>> Hey Nicholas,
>>
>> Thanks for this, we can merge in doc changes outside of the actual
>> release timeline, so we'll make sure to loop those changes in before
>> we publish the final 1.1 docs.
>>
>> - Patrick
>>
>> On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
>> <nicholas.chammas@gmail.com> wrote:
>> > There were several formatting and typographical errors in the SQL docs
>> > that
>> > I've fixed in this PR. Dunno if we want to roll that into the release.
>> >
>> >
>> > On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >>
>> >> Okay I'll plan to add cdh4 binary as well for the final release!
>> >>
>> >> ---
>> >> sent from my phone
>> >> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
>> >>
>> >> > We just used CDH 4.7 for our production cluster. And I believe we
>> >> > won't
>> >> > use CDH 5 in the next year.
>> >> >
>> >> > Sent from my iPhone
>> >> >
>> >> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia <m=
atei.zaharia@gmail.com>
>> >> > > wrote:
>> >> > >
>> >> > > Personally I'd actually consider putting CDH4 back if there are
>> >> > > still
>> >> > users on it. It's always better to be inclusive, and the convenienc=
e
>> >> > of
>> >> > a
>> >> > one-click download is high. Do we have a sense on what % of CDH use=
rs
>> >> > still
>> >> > use CDH4?
>> >> > >
>> >> > > Matei
>> >> > >
>> >> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.com)
>> >> > > wrote:
>> >> > >
>> >> > > (Copying my reply since I don't know if it goes to the mailing
>> >> > > list)
>> >> > >
>> >> > > Great, thanks for explaining the reasoning. You're saying these
>> >> > > aren't
>> >> > > going into the final release? I think that moots any issue
>> >> > > surrounding
>> >> > > distributing them then.
>> >> > >
>> >> > > This is all I know of from the ASF:
>> >> > > https://community.apache.org/projectIndependence.html I don't rea=
d
>> >> > > it
>> >> > > as expressly forbidding this kind of thing although you can see h=
ow
>> >> > > it
>> >> > > bumps up against the spirit. There's not a bright line -- what
>> >> > > about
>> >> > > Tomcat providing binaries compiled for Windows for example? does
>> >> > > that
>> >> > > favor an OS vendor?
>> >> > >
>> >> > > From this technical ASF perspective only the releases matter -- d=
o
>> >> > > what you want with snapshots and RCs. The only issue there is may=
be
>> >> > > releasing something different than was in the RC; is that at all
>> >> > > confusing? Just needs a note.
>> >> > >
>> >> > > I think this theoretical issue doesn't exist if these binaries
>> >> > > aren't
>> >> > > released, so I see no reason to not proceed.
>> >> > >
>> >> > > The rest is a different question about whether you want to spend
>> >> > > time
>> >> > > maintaining this profile and candidate. The vendor already manage=
s
>> >> > > their build I think and -- and I don't know -- may even prefer no=
t
>> >> > > to
>> >> > > have a different special build floating around. There's also the
>> >> > > theoretical argument that this turns off other vendors from
>> >> > > adopting
>> >> > > Spark if it's perceived to be too connected to other vendors. I'd
>> >> > > like
>> >> > > to maximize Spark's distribution and there's some argument you do
>> >> > > this
>> >> > > by not making vendor profiles. But as I say a different question =
to
>> >> > > just think about over time...
>> >> > >
>> >> > > (oh and PS for my part I think it's a good thing that CDH4 binari=
es
>> >> > > were removed. I wasn't arguing for resurrecting them)
>> >> > >
>> >> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell
>> >> > >> <pwendell@gmail.com>
>> >> > wrote:
>> >> > >> Hey Sean,
>> >> > >>
>> >> > >> The reason there are no longer CDH-specific builds is that all
>> >> > >> newer
>> >> > >> versions of CDH and HDP work with builds for the upstream Hadoop
>> >> > >> projects. I dropped CDH4 in favor of a newer Hadoop version (2.4=
)
>> >> > >> and
>> >> > >> the Hadoop-without-Hive (also 2.4) build.
>> >> > >>
>> >> > >> For MapR - we can't officially post those artifacts on ASF web
>> >> > >> space
>> >> > >> when we make the final release, we can only link to them as bein=
g
>> >> > >> hosted by MapR specifically since they use non-compatible
>> >> > >> licenses.
>> >> > >> However, I felt that providing these during a testing period was
>> >> > >> alright, with the goal of increasing test coverage. I couldn't
>> >> > >> find
>> >> > >> any policy against posting these on personal web space during RC
>> >> > >> voting. However, we can remove them if there is one.
>> >> > >>
>> >> > >> Dropping CDH4 was more because it is now pretty old, but we can
>> >> > >> add
>> >> > >> it
>> >> > >> back if people want. The binary packaging is a slightly separate
>> >> > >> question from release votes, so I can always add more binary
>> >> > >> packages
>> >> > >> whenever. And on this, my main concern is covering the most
>> >> > >> popular
>> >> > >> Hadoop versions to lower the bar for users to build and test
>> >> > >> Spark.
>> >> > >>
>> >> > >> - Patrick
>> >> > >>
>> >> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <sowen@cloudera.com=
>
>> >> > wrote:
>> >> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
>> >> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests...
>> >> > >>> don't
>> >> > >>> fail any more than usual.
>> >> > >>>
>> >> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in
>> >> > >>> another
>> >> > >>> project and have encountered no problems.
>> >> > >>>
>> >> > >>>
>> >> > >>> I notice that the 1.1.0 release removes the CDH4-specific build=
,
>> >> > >>> but
>> >> > >>> adds two MapR-specific builds. Compare with
>> >> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I
>> >> > >>> commented on the commit:
>> >> > >>>
>> >> >
>> >> >
>> >> > https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d0=
3ede713a73cc
>> >> > >>>
>> >> > >>> I'm in favor of removing all vendor-specific builds. This chang=
e
>> >> > >>> *looks* a bit funny as there was no JIRA (?) and appears to swa=
p
>> >> > >>> one
>> >> > >>> vendor for another. Of course there's nothing untoward going on=
,
>> >> > >>> but
>> >> > >>> what was the reasoning? It's best avoided, and MapR already
>> >> > >>> distributes Spark just fine, no?
>> >> > >>>
>> >> > >>> This is a gray area with ASF projects. I mention it as well
>> >> > >>> because
>> >> > >>> it
>> >> > >>> came up with Apache Flink recently
>> >> > >>> (
>> >> >
>> >> >
>> >> > http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201=
408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40m=
ail.gmail.com%3E
>> >> > )
>> >> > >>> Another vendor rightly noted this could look like favoritism.
>> >> > >>> They
>> >> > >>> changed to remove vendor releases.
>> >> > >>>
>> >> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
>> >> > >>>> <pwendell@gmail.com>
>> >> > wrote:
>> >> > >>>> Please vote on releasing the following candidate as Apache Spa=
rk
>> >> > version 1.1.0!
>> >> > >>>>
>> >> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>> >> > >>>>
>> >> >
>> >> >
>> >> > https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=
=3D711aebb329ca28046396af1e34395a0df92b5327
>> >> > >>>>
>> >> > >>>> The release files, including signatures, digests, etc. can be
>> >> > >>>> found
>> >> > at:
>> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>> >> > >>>>
>> >> > >>>> Release artifacts are signed with the following key:
>> >> > >>>> https://people.apache.org/keys/committer/pwendell.asc
>> >> > >>>>
>> >> > >>>> The staging repository for this release can be found at:
>> >> > >>>>
>> >> >
>> >> > https://repository.apache.org/content/repositories/orgapachespark-1=
029/
>> >> > >>>>
>> >> > >>>> The documentation corresponding to this release can be found a=
t:
>> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>> >> > >>>>
>> >> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
>> >> > >>>>
>> >> > >>>> The vote is open until Monday, September 01, at 03:11 UTC and
>> >> > >>>> passes
>> >> > if
>> >> > >>>> a majority of at least 3 +1 PMC votes are cast.
>> >> > >>>>
>> >> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
>> >> > >>>> [ ] -1 Do not release this package because ...
>> >> > >>>>
>> >> > >>>> To learn more about Apache Spark, please see
>> >> > >>>> http://spark.apache.org/
>> >> > >>>>
>> >> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
>> >> > >>>> LZ4 compression issue:
>> >> > https://issues.apache.org/jira/browse/SPARK-3277
>> >> > >>>>
>> >> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> >> > >>>> This vote is happening very late into the QA period compared
>> >> > >>>> with
>> >> > >>>> previous votes, so -1 votes should only occur for significant
>> >> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will not
>> >> > >>>> block
>> >> > >>>> this release.
>> >> > >>>>
>> >> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
>> >> > >>>> 1. The default value of "spark.io.compression.codec" is now
>> >> > >>>> "snappy"
>> >> > >>>> --> Old behavior can be restored by switching to "lzf"
>> >> > >>>>
>> >> > >>>> 2. PySpark now performs external spilling during aggregations.
>> >> > >>>> --> Old behavior can be restored by setting
>> >> > >>>> "spark.shuffle.spill"
>> >> > >>>> to
>> >> > "false".
>> >> > >>>>
>> >> > >>>>
>> >> > >>>>
>> >> > >>>> --------------------------------------------------------------=
-------
>> >> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> > >
>> >> > >
>> >> > > -----------------------------------------------------------------=
----
>> >> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> > > For additional commands, e-mail: dev-help@spark.apache.org
>> >> > >
>> >> >
>> >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 18:38:49 2014
Return-Path: <dev-return-9172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A38611189C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 18:38:49 +0000 (UTC)
Received: (qmail 68752 invoked by uid 500); 29 Aug 2014 18:38:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68683 invoked by uid 500); 29 Aug 2014 18:38:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68671 invoked by uid 99); 29 Aug 2014 18:38:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:38:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.192.173 as permitted sender)
Received: from [209.85.192.173] (HELO mail-pd0-f173.google.com) (209.85.192.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:38:21 +0000
Received: by mail-pd0-f173.google.com with SMTP id p10so984106pdj.18
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 11:38:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=OBbVmGshG7eemRXxKvpM6ILuYRFyHqHoXB6rlez49eY=;
        b=rumYWcXTg1YtVc4+ereSzhpahSmGWHG5HH/PvYCVJ4p8z9zH5/IHi+5w8l+48bvBlR
         8foarCIbKgEqhDCHb6WkexZ962zNDAG6PZYJE59qj6I+oJuIQY2JjALikgSqmdvls3c5
         YbnHNnGivQCqG2iQKtTdUT6VLNpTHWNBEMqC6tnlfBf5zX+pcDHBrx1sX5256XIMAm8V
         TfXqy458rvwR8lO44IpSLsPhtMwaegACpmG88iiyYf1RM6BAGBJhnCwPo19wXC/zCa7K
         rruJMI+TAOOifV2H7mjsYpbd8mVbWbeZ7Ffwpl+Njlve4gey0xbZeA6B5NxsSnX94NCp
         ibFw==
X-Received: by 10.70.36.239 with SMTP id t15mr17875983pdj.83.1409337499130;
        Fri, 29 Aug 2014 11:38:19 -0700 (PDT)
Received: from [192.168.56.93] ([183.129.208.18])
        by mx.google.com with ESMTPSA id mj8sm2319843pab.19.2014.08.29.11.38.17
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 29 Aug 2014 11:38:18 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Compie error with XML elements
From: Yi Tian <tianyi.asiainfo@gmail.com>
In-Reply-To: <CAMQ+LQNhpAd_dP69bxphxv2Bq59tc4ePS6yz1yT_dQd8x6y19g@mail.gmail.com>
Date: Sat, 30 Aug 2014 02:38:13 +0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: 7bit
Message-Id: <614A85E8-CF17-4F91-8767-D73D40426868@gmail.com>
References: <CAMQ+LQNhpAd_dP69bxphxv2Bq59tc4ePS6yz1yT_dQd8x6y19g@mail.gmail.com>
To: Devl Devel <devl.development@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, Devl!

I got the same problem.

You can try to upgrade your scala plugins to  0.41.2

It works on my mac.

On Aug 12, 2014, at 15:19, Devl Devel <devl.development@gmail.com> wrote:

> When compiling the master checkout of spark. The Intellij compile fails
> with:
> 
>    Error:(45, 8) not found: value $scope
>      <div class="row-fluid">
>       ^
> which is caused by HTML elements in classes like HistoryPage.scala:
> 
>    val content =
>      <div class="row-fluid">
>        <div class="span12">...
> 
> How can I compile these classes that have html node elements in them?
> 
> Thanks in advance.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 18:44:46 2014
Return-Path: <dev-return-9173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92F50118D1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 18:44:46 +0000 (UTC)
Received: (qmail 90691 invoked by uid 500); 29 Aug 2014 18:44:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90630 invoked by uid 500); 29 Aug 2014 18:44:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90618 invoked by uid 99); 29 Aug 2014 18:44:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:44:45 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:44:18 +0000
Received: by mail-wg0-f52.google.com with SMTP id m15so2523572wgh.23
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 11:44:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=OJvOLkNfzoWVKaX/FpPGkFqItkLvpTo0USER8bM2l/U=;
        b=vlZL/I1RwsdekYKzMh3Uk9nb5wlcbxzL1tCwRlrr/X3Dm8IcJ2RjzfpVHLNz3wpnvR
         P73rUp95UdgevOQnm4i3TZa4PK40kuyBHWHRNV5OMn45viPq2/sk6kRN8K4HQ01bcsp+
         JJSXQFG57Xqy8DZPWDTbnzqZMhi+fzud1uYXfLWLjdT/nGP8LEP9mk9gNVKDxA4MK8oM
         Wsg/LDQAupnen7TRafphfMYNrem6i9KsdpN0rCI1ZNpDS0plow7YSsT0OdH+ojRbNR4d
         I5zfAWY8gUrlzaNB2qqGgR5sDLkRI/Nk80K0WpMl6SgElww09xPPwtVBb1vP6MRWvyaG
         EeRw==
X-Received: by 10.194.59.18 with SMTP id v18mr15839757wjq.64.1409337858180;
 Fri, 29 Aug 2014 11:44:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 29 Aug 2014 11:43:38 -0700 (PDT)
In-Reply-To: <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
 <etPan.5400202e.579478fe.470a@mbp-3.local> <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
 <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
 <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
 <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
 <CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com> <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 29 Aug 2014 14:43:38 -0400
Message-ID: <CAOhmDzfEuT_unicEUfhUKL1s1WcPdAAc7D92mYuuRaVx=wPncA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: Ye Xianjin <advancedxy@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bacb0a667ca6f0501c90afa
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb0a667ca6f0501c90afa
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sounds good. As an FYI, we had this problem with the 1.0.2 release
<https://issues.apache.org/jira/browse/SPARK-3242>. Is there perhaps some
kind of automated check we can make to catch this for us in the future?
Where would it go?


On Fri, Aug 29, 2014 at 2:18 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:

> Oh darn - I missed this update. GRR, unfortunately I think this means
> I'll need to cut a new RC. Thanks for catching this Nick.
>
> On Fri, Aug 29, 2014 at 10:18 AM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > [Let me know if I should be posting these comments in a different
> thread.]
> >
> > Should the default Spark version in spark-ec2 be updated for this
> release?
> >
> > Nick
> >
> >
> >
> > On Fri, Aug 29, 2014 at 12:55 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >>
> >> Hey Nicholas,
> >>
> >> Thanks for this, we can merge in doc changes outside of the actual
> >> release timeline, so we'll make sure to loop those changes in before
> >> we publish the final 1.1 docs.
> >>
> >> - Patrick
> >>
> >> On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
> >> <nicholas.chammas@gmail.com> wrote:
> >> > There were several formatting and typographical errors in the SQL do=
cs
> >> > that
> >> > I've fixed in this PR. Dunno if we want to roll that into the releas=
e.
> >> >
> >> >
> >> > On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.co=
m
> >
> >> > wrote:
> >> >>
> >> >> Okay I'll plan to add cdh4 binary as well for the final release!
> >> >>
> >> >> ---
> >> >> sent from my phone
> >> >> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
> >> >>
> >> >> > We just used CDH 4.7 for our production cluster. And I believe we
> >> >> > won't
> >> >> > use CDH 5 in the next year.
> >> >> >
> >> >> > Sent from my iPhone
> >> >> >
> >> >> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia =
<matei.zaharia@gmail.com>
> >> >> > > wrote:
> >> >> > >
> >> >> > > Personally I'd actually consider putting CDH4 back if there are
> >> >> > > still
> >> >> > users on it. It's always better to be inclusive, and the
> convenience
> >> >> > of
> >> >> > a
> >> >> > one-click download is high. Do we have a sense on what % of CDH
> users
> >> >> > still
> >> >> > use CDH4?
> >> >> > >
> >> >> > > Matei
> >> >> > >
> >> >> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.co=
m
> )
> >> >> > > wrote:
> >> >> > >
> >> >> > > (Copying my reply since I don't know if it goes to the mailing
> >> >> > > list)
> >> >> > >
> >> >> > > Great, thanks for explaining the reasoning. You're saying these
> >> >> > > aren't
> >> >> > > going into the final release? I think that moots any issue
> >> >> > > surrounding
> >> >> > > distributing them then.
> >> >> > >
> >> >> > > This is all I know of from the ASF:
> >> >> > > https://community.apache.org/projectIndependence.html I don't
> read
> >> >> > > it
> >> >> > > as expressly forbidding this kind of thing although you can see
> how
> >> >> > > it
> >> >> > > bumps up against the spirit. There's not a bright line -- what
> >> >> > > about
> >> >> > > Tomcat providing binaries compiled for Windows for example? doe=
s
> >> >> > > that
> >> >> > > favor an OS vendor?
> >> >> > >
> >> >> > > From this technical ASF perspective only the releases matter --
> do
> >> >> > > what you want with snapshots and RCs. The only issue there is
> maybe
> >> >> > > releasing something different than was in the RC; is that at al=
l
> >> >> > > confusing? Just needs a note.
> >> >> > >
> >> >> > > I think this theoretical issue doesn't exist if these binaries
> >> >> > > aren't
> >> >> > > released, so I see no reason to not proceed.
> >> >> > >
> >> >> > > The rest is a different question about whether you want to spen=
d
> >> >> > > time
> >> >> > > maintaining this profile and candidate. The vendor already
> manages
> >> >> > > their build I think and -- and I don't know -- may even prefer
> not
> >> >> > > to
> >> >> > > have a different special build floating around. There's also th=
e
> >> >> > > theoretical argument that this turns off other vendors from
> >> >> > > adopting
> >> >> > > Spark if it's perceived to be too connected to other vendors. I=
'd
> >> >> > > like
> >> >> > > to maximize Spark's distribution and there's some argument you =
do
> >> >> > > this
> >> >> > > by not making vendor profiles. But as I say a different questio=
n
> to
> >> >> > > just think about over time...
> >> >> > >
> >> >> > > (oh and PS for my part I think it's a good thing that CDH4
> binaries
> >> >> > > were removed. I wasn't arguing for resurrecting them)
> >> >> > >
> >> >> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell
> >> >> > >> <pwendell@gmail.com>
> >> >> > wrote:
> >> >> > >> Hey Sean,
> >> >> > >>
> >> >> > >> The reason there are no longer CDH-specific builds is that all
> >> >> > >> newer
> >> >> > >> versions of CDH and HDP work with builds for the upstream Hado=
op
> >> >> > >> projects. I dropped CDH4 in favor of a newer Hadoop version
> (2.4)
> >> >> > >> and
> >> >> > >> the Hadoop-without-Hive (also 2.4) build.
> >> >> > >>
> >> >> > >> For MapR - we can't officially post those artifacts on ASF web
> >> >> > >> space
> >> >> > >> when we make the final release, we can only link to them as
> being
> >> >> > >> hosted by MapR specifically since they use non-compatible
> >> >> > >> licenses.
> >> >> > >> However, I felt that providing these during a testing period w=
as
> >> >> > >> alright, with the goal of increasing test coverage. I couldn't
> >> >> > >> find
> >> >> > >> any policy against posting these on personal web space during =
RC
> >> >> > >> voting. However, we can remove them if there is one.
> >> >> > >>
> >> >> > >> Dropping CDH4 was more because it is now pretty old, but we ca=
n
> >> >> > >> add
> >> >> > >> it
> >> >> > >> back if people want. The binary packaging is a slightly separa=
te
> >> >> > >> question from release votes, so I can always add more binary
> >> >> > >> packages
> >> >> > >> whenever. And on this, my main concern is covering the most
> >> >> > >> popular
> >> >> > >> Hadoop versions to lower the bar for users to build and test
> >> >> > >> Spark.
> >> >> > >>
> >> >> > >> - Patrick
> >> >> > >>
> >> >> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <
> sowen@cloudera.com>
> >> >> > wrote:
> >> >> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
> >> >> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests..=
.
> >> >> > >>> don't
> >> >> > >>> fail any more than usual.
> >> >> > >>>
> >> >> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in
> >> >> > >>> another
> >> >> > >>> project and have encountered no problems.
> >> >> > >>>
> >> >> > >>>
> >> >> > >>> I notice that the 1.1.0 release removes the CDH4-specific
> build,
> >> >> > >>> but
> >> >> > >>> adds two MapR-specific builds. Compare with
> >> >> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/
> I
> >> >> > >>> commented on the commit:
> >> >> > >>>
> >> >> >
> >> >> >
> >> >> >
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede71=
3a73cc
> >> >> > >>>
> >> >> > >>> I'm in favor of removing all vendor-specific builds. This
> change
> >> >> > >>> *looks* a bit funny as there was no JIRA (?) and appears to
> swap
> >> >> > >>> one
> >> >> > >>> vendor for another. Of course there's nothing untoward going
> on,
> >> >> > >>> but
> >> >> > >>> what was the reasoning? It's best avoided, and MapR already
> >> >> > >>> distributes Spark just fine, no?
> >> >> > >>>
> >> >> > >>> This is a gray area with ASF projects. I mention it as well
> >> >> > >>> because
> >> >> > >>> it
> >> >> > >>> came up with Apache Flink recently
> >> >> > >>> (
> >> >> >
> >> >> >
> >> >> >
> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mb=
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gm=
ail.com%3E
> >> >> > )
> >> >> > >>> Another vendor rightly noted this could look like favoritism.
> >> >> > >>> They
> >> >> > >>> changed to remove vendor releases.
> >> >> > >>>
> >> >> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
> >> >> > >>>> <pwendell@gmail.com>
> >> >> > wrote:
> >> >> > >>>> Please vote on releasing the following candidate as Apache
> Spark
> >> >> > version 1.1.0!
> >> >> > >>>>
> >> >> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> >> >> > >>>>
> >> >> >
> >> >> >
> >> >> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711a=
ebb329ca28046396af1e34395a0df92b5327
> >> >> > >>>>
> >> >> > >>>> The release files, including signatures, digests, etc. can b=
e
> >> >> > >>>> found
> >> >> > at:
> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> >> >> > >>>>
> >> >> > >>>> Release artifacts are signed with the following key:
> >> >> > >>>> https://people.apache.org/keys/committer/pwendell.asc
> >> >> > >>>>
> >> >> > >>>> The staging repository for this release can be found at:
> >> >> > >>>>
> >> >> >
> >> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1029/
> >> >> > >>>>
> >> >> > >>>> The documentation corresponding to this release can be found
> at:
> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> >> >> > >>>>
> >> >> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
> >> >> > >>>>
> >> >> > >>>> The vote is open until Monday, September 01, at 03:11 UTC an=
d
> >> >> > >>>> passes
> >> >> > if
> >> >> > >>>> a majority of at least 3 +1 PMC votes are cast.
> >> >> > >>>>
> >> >> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> >> >> > >>>> [ ] -1 Do not release this package because ...
> >> >> > >>>>
> >> >> > >>>> To learn more about Apache Spark, please see
> >> >> > >>>> http://spark.apache.org/
> >> >> > >>>>
> >> >> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
> >> >> > >>>> LZ4 compression issue:
> >> >> > https://issues.apache.org/jira/browse/SPARK-3277
> >> >> > >>>>
> >> >> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> >> >> > >>>> This vote is happening very late into the QA period compared
> >> >> > >>>> with
> >> >> > >>>> previous votes, so -1 votes should only occur for significan=
t
> >> >> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will n=
ot
> >> >> > >>>> block
> >> >> > >>>> this release.
> >> >> > >>>>
> >> >> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
> >> >> > >>>> 1. The default value of "spark.io.compression.codec" is now
> >> >> > >>>> "snappy"
> >> >> > >>>> --> Old behavior can be restored by switching to "lzf"
> >> >> > >>>>
> >> >> > >>>> 2. PySpark now performs external spilling during aggregation=
s.
> >> >> > >>>> --> Old behavior can be restored by setting
> >> >> > >>>> "spark.shuffle.spill"
> >> >> > >>>> to
> >> >> > "false".
> >> >> > >>>>
> >> >> > >>>>
> >> >> > >>>>
> >> >> > >>>>
> ---------------------------------------------------------------------
> >> >> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> > >
> >> >> > >
> >> >> > >
> ---------------------------------------------------------------------
> >> >> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> > > For additional commands, e-mail: dev-help@spark.apache.org
> >> >> > >
> >> >> >
> >> >
> >> >
> >
> >
>

--047d7bacb0a667ca6f0501c90afa--

From dev-return-9174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 18:53:24 2014
Return-Path: <dev-return-9174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 040F711926
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 18:53:24 +0000 (UTC)
Received: (qmail 16580 invoked by uid 500); 29 Aug 2014 18:53:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16507 invoked by uid 500); 29 Aug 2014 18:53:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16488 invoked by uid 99); 29 Aug 2014 18:53:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:53:22 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:52:56 +0000
Received: by mail-qc0-f181.google.com with SMTP id i17so2812207qcy.26
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 11:52:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ij2Eh/InT5SBV5Aq95PuV+R/Mn2xPdAI50ILyq6SMeE=;
        b=KzmHmeARCJEobYGq1bAoi3ak96K11ccYHJPyse54aLX421kJoEgy2URnw9wd6SjI93
         xUeiiF0C1RLKXNDkKfbthZVGFwFa3GFr3VqNXa9Ne/E0KFMIj+vjiZgqZJhd9GfBi5Do
         R0iNc9PaTanSE2vWv9Ppl2XA38TPj7b9LjotsB2Dqw6xFwjzI6ikCu69jDXabpYbN74Z
         WILqAurA+XUL80g2vtqnRMos7BzynpCvaQhOpITvhZM8+jN/tNJNU6NiZ7j3PhQuNJbq
         3YM8Jrg0hBw6X/RYfIqm65iwd8vDAfbNWKZt9Oj77ze+Fw5hn0lX6IpHA3pnjHyU0n/6
         ihzg==
X-Received: by 10.224.37.134 with SMTP id x6mr20842868qad.39.1409338375199;
 Fri, 29 Aug 2014 11:52:55 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.92.210 with HTTP; Fri, 29 Aug 2014 11:52:35 -0700 (PDT)
In-Reply-To: <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
 <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
 <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
 <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
 <etPan.5400202e.579478fe.470a@mbp-3.local> <BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
 <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
 <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
 <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
 <CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com> <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Fri, 29 Aug 2014 11:52:35 -0700
Message-ID: <CAA_qdLo__HQ0YR8yL4_T_bAJP=_4bLgQBBBD7QtMBitVbp2S6w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Ye Xianjin <advancedxy@gmail.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1f4a238df4e0501c929ba
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f4a238df4e0501c929ba
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Just noticed one thing: although --with-hive is deprecated by -Phive,
make-distribution.sh still relies on $SPARK_HIVE (which was controlled by
--with-hive) to determine whether to include datanucleus jar files. This
means we have to do something like SPARK_HIVE=3Dtrue ./make-distribution.sh
... to enable Hive support. Otherwise datanucleus jars are not included in
lib/.

This issue is similar to SPARK-3234
<https://issues.apache.org/jira/browse/SPARK-3234>, both
SPARK_HADOOP_VERSION and SPARK_HIVE are controlled by some deprecated
command line options.
=E2=80=8B


On Fri, Aug 29, 2014 at 11:18 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Oh darn - I missed this update. GRR, unfortunately I think this means
> I'll need to cut a new RC. Thanks for catching this Nick.
>
> On Fri, Aug 29, 2014 at 10:18 AM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > [Let me know if I should be posting these comments in a different
> thread.]
> >
> > Should the default Spark version in spark-ec2 be updated for this
> release?
> >
> > Nick
> >
> >
> >
> > On Fri, Aug 29, 2014 at 12:55 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >>
> >> Hey Nicholas,
> >>
> >> Thanks for this, we can merge in doc changes outside of the actual
> >> release timeline, so we'll make sure to loop those changes in before
> >> we publish the final 1.1 docs.
> >>
> >> - Patrick
> >>
> >> On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
> >> <nicholas.chammas@gmail.com> wrote:
> >> > There were several formatting and typographical errors in the SQL do=
cs
> >> > that
> >> > I've fixed in this PR. Dunno if we want to roll that into the releas=
e.
> >> >
> >> >
> >> > On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.co=
m
> >
> >> > wrote:
> >> >>
> >> >> Okay I'll plan to add cdh4 binary as well for the final release!
> >> >>
> >> >> ---
> >> >> sent from my phone
> >> >> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote:
> >> >>
> >> >> > We just used CDH 4.7 for our production cluster. And I believe we
> >> >> > won't
> >> >> > use CDH 5 in the next year.
> >> >> >
> >> >> > Sent from my iPhone
> >> >> >
> >> >> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia =
<matei.zaharia@gmail.com>
> >> >> > > wrote:
> >> >> > >
> >> >> > > Personally I'd actually consider putting CDH4 back if there are
> >> >> > > still
> >> >> > users on it. It's always better to be inclusive, and the
> convenience
> >> >> > of
> >> >> > a
> >> >> > one-click download is high. Do we have a sense on what % of CDH
> users
> >> >> > still
> >> >> > use CDH4?
> >> >> > >
> >> >> > > Matei
> >> >> > >
> >> >> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.co=
m
> )
> >> >> > > wrote:
> >> >> > >
> >> >> > > (Copying my reply since I don't know if it goes to the mailing
> >> >> > > list)
> >> >> > >
> >> >> > > Great, thanks for explaining the reasoning. You're saying these
> >> >> > > aren't
> >> >> > > going into the final release? I think that moots any issue
> >> >> > > surrounding
> >> >> > > distributing them then.
> >> >> > >
> >> >> > > This is all I know of from the ASF:
> >> >> > > https://community.apache.org/projectIndependence.html I don't
> read
> >> >> > > it
> >> >> > > as expressly forbidding this kind of thing although you can see
> how
> >> >> > > it
> >> >> > > bumps up against the spirit. There's not a bright line -- what
> >> >> > > about
> >> >> > > Tomcat providing binaries compiled for Windows for example? doe=
s
> >> >> > > that
> >> >> > > favor an OS vendor?
> >> >> > >
> >> >> > > From this technical ASF perspective only the releases matter --
> do
> >> >> > > what you want with snapshots and RCs. The only issue there is
> maybe
> >> >> > > releasing something different than was in the RC; is that at al=
l
> >> >> > > confusing? Just needs a note.
> >> >> > >
> >> >> > > I think this theoretical issue doesn't exist if these binaries
> >> >> > > aren't
> >> >> > > released, so I see no reason to not proceed.
> >> >> > >
> >> >> > > The rest is a different question about whether you want to spen=
d
> >> >> > > time
> >> >> > > maintaining this profile and candidate. The vendor already
> manages
> >> >> > > their build I think and -- and I don't know -- may even prefer
> not
> >> >> > > to
> >> >> > > have a different special build floating around. There's also th=
e
> >> >> > > theoretical argument that this turns off other vendors from
> >> >> > > adopting
> >> >> > > Spark if it's perceived to be too connected to other vendors. I=
'd
> >> >> > > like
> >> >> > > to maximize Spark's distribution and there's some argument you =
do
> >> >> > > this
> >> >> > > by not making vendor profiles. But as I say a different questio=
n
> to
> >> >> > > just think about over time...
> >> >> > >
> >> >> > > (oh and PS for my part I think it's a good thing that CDH4
> binaries
> >> >> > > were removed. I wasn't arguing for resurrecting them)
> >> >> > >
> >> >> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell
> >> >> > >> <pwendell@gmail.com>
> >> >> > wrote:
> >> >> > >> Hey Sean,
> >> >> > >>
> >> >> > >> The reason there are no longer CDH-specific builds is that all
> >> >> > >> newer
> >> >> > >> versions of CDH and HDP work with builds for the upstream Hado=
op
> >> >> > >> projects. I dropped CDH4 in favor of a newer Hadoop version
> (2.4)
> >> >> > >> and
> >> >> > >> the Hadoop-without-Hive (also 2.4) build.
> >> >> > >>
> >> >> > >> For MapR - we can't officially post those artifacts on ASF web
> >> >> > >> space
> >> >> > >> when we make the final release, we can only link to them as
> being
> >> >> > >> hosted by MapR specifically since they use non-compatible
> >> >> > >> licenses.
> >> >> > >> However, I felt that providing these during a testing period w=
as
> >> >> > >> alright, with the goal of increasing test coverage. I couldn't
> >> >> > >> find
> >> >> > >> any policy against posting these on personal web space during =
RC
> >> >> > >> voting. However, we can remove them if there is one.
> >> >> > >>
> >> >> > >> Dropping CDH4 was more because it is now pretty old, but we ca=
n
> >> >> > >> add
> >> >> > >> it
> >> >> > >> back if people want. The binary packaging is a slightly separa=
te
> >> >> > >> question from release votes, so I can always add more binary
> >> >> > >> packages
> >> >> > >> whenever. And on this, my main concern is covering the most
> >> >> > >> popular
> >> >> > >> Hadoop versions to lower the bar for users to build and test
> >> >> > >> Spark.
> >> >> > >>
> >> >> > >> - Patrick
> >> >> > >>
> >> >> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <
> sowen@cloudera.com>
> >> >> > wrote:
> >> >> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
> >> >> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests..=
.
> >> >> > >>> don't
> >> >> > >>> fail any more than usual.
> >> >> > >>>
> >> >> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time in
> >> >> > >>> another
> >> >> > >>> project and have encountered no problems.
> >> >> > >>>
> >> >> > >>>
> >> >> > >>> I notice that the 1.1.0 release removes the CDH4-specific
> build,
> >> >> > >>> but
> >> >> > >>> adds two MapR-specific builds. Compare with
> >> >> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/
> I
> >> >> > >>> commented on the commit:
> >> >> > >>>
> >> >> >
> >> >> >
> >> >> >
> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede71=
3a73cc
> >> >> > >>>
> >> >> > >>> I'm in favor of removing all vendor-specific builds. This
> change
> >> >> > >>> *looks* a bit funny as there was no JIRA (?) and appears to
> swap
> >> >> > >>> one
> >> >> > >>> vendor for another. Of course there's nothing untoward going
> on,
> >> >> > >>> but
> >> >> > >>> what was the reasoning? It's best avoided, and MapR already
> >> >> > >>> distributes Spark just fine, no?
> >> >> > >>>
> >> >> > >>> This is a gray area with ASF projects. I mention it as well
> >> >> > >>> because
> >> >> > >>> it
> >> >> > >>> came up with Apache Flink recently
> >> >> > >>> (
> >> >> >
> >> >> >
> >> >> >
> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mb=
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gm=
ail.com%3E
> >> >> > )
> >> >> > >>> Another vendor rightly noted this could look like favoritism.
> >> >> > >>> They
> >> >> > >>> changed to remove vendor releases.
> >> >> > >>>
> >> >> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
> >> >> > >>>> <pwendell@gmail.com>
> >> >> > wrote:
> >> >> > >>>> Please vote on releasing the following candidate as Apache
> Spark
> >> >> > version 1.1.0!
> >> >> > >>>>
> >> >> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
> >> >> > >>>>
> >> >> >
> >> >> >
> >> >> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711a=
ebb329ca28046396af1e34395a0df92b5327
> >> >> > >>>>
> >> >> > >>>> The release files, including signatures, digests, etc. can b=
e
> >> >> > >>>> found
> >> >> > at:
> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
> >> >> > >>>>
> >> >> > >>>> Release artifacts are signed with the following key:
> >> >> > >>>> https://people.apache.org/keys/committer/pwendell.asc
> >> >> > >>>>
> >> >> > >>>> The staging repository for this release can be found at:
> >> >> > >>>>
> >> >> >
> >> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1029/
> >> >> > >>>>
> >> >> > >>>> The documentation corresponding to this release can be found
> at:
> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
> >> >> > >>>>
> >> >> > >>>> Please vote on releasing this package as Apache Spark 1.1.0!
> >> >> > >>>>
> >> >> > >>>> The vote is open until Monday, September 01, at 03:11 UTC an=
d
> >> >> > >>>> passes
> >> >> > if
> >> >> > >>>> a majority of at least 3 +1 PMC votes are cast.
> >> >> > >>>>
> >> >> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> >> >> > >>>> [ ] -1 Do not release this package because ...
> >> >> > >>>>
> >> >> > >>>> To learn more about Apache Spark, please see
> >> >> > >>>> http://spark.apache.org/
> >> >> > >>>>
> >> >> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
> >> >> > >>>> LZ4 compression issue:
> >> >> > https://issues.apache.org/jira/browse/SPARK-3277
> >> >> > >>>>
> >> >> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> >> >> > >>>> This vote is happening very late into the QA period compared
> >> >> > >>>> with
> >> >> > >>>> previous votes, so -1 votes should only occur for significan=
t
> >> >> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will n=
ot
> >> >> > >>>> block
> >> >> > >>>> this release.
> >> >> > >>>>
> >> >> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
> >> >> > >>>> 1. The default value of "spark.io.compression.codec" is now
> >> >> > >>>> "snappy"
> >> >> > >>>> --> Old behavior can be restored by switching to "lzf"
> >> >> > >>>>
> >> >> > >>>> 2. PySpark now performs external spilling during aggregation=
s.
> >> >> > >>>> --> Old behavior can be restored by setting
> >> >> > >>>> "spark.shuffle.spill"
> >> >> > >>>> to
> >> >> > "false".
> >> >> > >>>>
> >> >> > >>>>
> >> >> > >>>>
> >> >> > >>>>
> ---------------------------------------------------------------------
> >> >> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> > >
> >> >> > >
> >> >> > >
> ---------------------------------------------------------------------
> >> >> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> > > For additional commands, e-mail: dev-help@spark.apache.org
> >> >> > >
> >> >> >
> >> >
> >> >
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c1f4a238df4e0501c929ba--

From dev-return-9175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 18:53:46 2014
Return-Path: <dev-return-9175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31C881192B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 18:53:46 +0000 (UTC)
Received: (qmail 19042 invoked by uid 500); 29 Aug 2014 18:53:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18982 invoked by uid 500); 29 Aug 2014 18:53:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18969 invoked by uid 99); 29 Aug 2014 18:53:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:53:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.51 as permitted sender)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 18:53:19 +0000
Received: by mail-qg0-f51.google.com with SMTP id i50so2724755qgf.10
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 11:53:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type
         :content-transfer-encoding;
        bh=sq7uDZMBTMx4BbrVnfHU/i+EeWdGffQhSSbIAhjfTPQ=;
        b=dItv9+YCg2YQHb4eouXfi9sAd0io9+0YB3ngw/qFKoEV6hf2jSeoA4wno1nSCPV7Hp
         S0gbIzmo1s7XDzvKmIZSnUXBi7FqT+da1AQEU/Q5hZQRP83v/91pA9kBjT1s35s5gXI/
         e9i0G0EBarrCxzU/D8YYdQ3MJytlgdWS1Bv6p8uyX5P3XqaESc6B2O821KOYBEuOjDie
         AHoHkpRVS7r2GwYeMg8+TjCSLI80P8RqCVb9GlMRp0LowtfKMsvwLg6D9xS3yPfEPLAi
         N/SULXaB9etHGTTCiXuLC4DXN6dhD8MZhwIM73mKAIV5PigZbvWtrMw93yXbDZj4y/Wq
         srRw==
X-Gm-Message-State: ALoCoQlGV8ZrnzeI7YXAOq+fUXnOIKwkjndsR1SP+ONWt7Q/6wVMoLftPlCr3f9CyGCosPcslgux
MIME-Version: 1.0
X-Received: by 10.140.109.75 with SMTP id k69mr19691304qgf.96.1409338398054;
 Fri, 29 Aug 2014 11:53:18 -0700 (PDT)
Received: by 10.229.154.201 with HTTP; Fri, 29 Aug 2014 11:53:17 -0700 (PDT)
In-Reply-To: <CAOhmDzfEuT_unicEUfhUKL1s1WcPdAAc7D92mYuuRaVx=wPncA@mail.gmail.com>
References: <CABPQxsuRHqxQ7SyqSDc59Df9vaqGWJ+E+E8nf310pcLE=QnXWg@mail.gmail.com>
	<CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<etPan.5400202e.579478fe.470a@mbp-3.local>
	<BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
	<CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
	<CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
	<CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
	<CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com>
	<CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
	<CAOhmDzfEuT_unicEUfhUKL1s1WcPdAAc7D92mYuuRaVx=wPncA@mail.gmail.com>
Date: Fri, 29 Aug 2014 11:53:17 -0700
Message-ID: <CAAOnQ7uHfaSmAj4eWx-pbcgmDXgm=mFbZrBczB8mFESCX9ZobQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Ye Xianjin <advancedxy@gmail.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

In our internal projects we use this bit of code in the maven pom to
create a properties file with build information (sorry for the messy
indentation). Then we have code that reads this property file
somewhere and provides that info. This should make it easier to not
have to change version numbers in Scala/Java/Python code ever again.
:-)

Shouldn't be hard to do something like that in sbt (actually should be
much easier).


      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-antrun-plugin</artifactId>
        <version>1.6</version>
        <executions>
          <execution>
            <id>build-info</id>
            <phase>compile</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <target>
                <taskdef
                        resource=3D"net/sf/antcontrib/antcontrib.properties=
"
                        classpathref=3D"maven.plugin.classpath"/>
                    <if>
                      <not>
                        <isset property=3D"build.hash"/>
                      </not>
                      <then>
                        <exec executable=3D"git"
                              outputproperty=3D"build.hash">
                          <arg line=3D"rev-parse HEAD"/>
                        </exec>
                      </then>
                    </if>
                    <echo>buildRevision: ${build.hash}</echo>
                    <echo file=3D"${build.info}"
                        message=3D"version=3D${project.version}${line.separ=
ator}" />
                    <echo file=3D"${build.info}" append=3D"true"
                        message=3D"hash=3D${build.hash}${line.separator}" /=
>
                    <echo file=3D"${build.info}" append=3D"true" />
              </target>
            </configuration>
          </execution>
        </executions>
        <dependencies>
          <dependency>
            <groupId>ant-contrib</groupId>
            <artifactId>ant-contrib</artifactId>
            <version>1.0b3</version>
            <exclusions>
              <exclusion>
                <groupId>ant</groupId>
                <artifactId>ant</artifactId>
              </exclusion>
            </exclusions>
          </dependency>
        </dependencies>
      </plugin>
    </plugins>

On Fri, Aug 29, 2014 at 11:43 AM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Sounds good. As an FYI, we had this problem with the 1.0.2 release
> <https://issues.apache.org/jira/browse/SPARK-3242>. Is there perhaps some
> kind of automated check we can make to catch this for us in the future?
> Where would it go?
>
>
> On Fri, Aug 29, 2014 at 2:18 PM, Patrick Wendell <pwendell@gmail.com> wro=
te:
>
>> Oh darn - I missed this update. GRR, unfortunately I think this means
>> I'll need to cut a new RC. Thanks for catching this Nick.
>>
>> On Fri, Aug 29, 2014 at 10:18 AM, Nicholas Chammas
>> <nicholas.chammas@gmail.com> wrote:
>> > [Let me know if I should be posting these comments in a different
>> thread.]
>> >
>> > Should the default Spark version in spark-ec2 be updated for this
>> release?
>> >
>> > Nick
>> >
>> >
>> >
>> > On Fri, Aug 29, 2014 at 12:55 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >>
>> >> Hey Nicholas,
>> >>
>> >> Thanks for this, we can merge in doc changes outside of the actual
>> >> release timeline, so we'll make sure to loop those changes in before
>> >> we publish the final 1.1 docs.
>> >>
>> >> - Patrick
>> >>
>> >> On Fri, Aug 29, 2014 at 9:24 AM, Nicholas Chammas
>> >> <nicholas.chammas@gmail.com> wrote:
>> >> > There were several formatting and typographical errors in the SQL d=
ocs
>> >> > that
>> >> > I've fixed in this PR. Dunno if we want to roll that into the relea=
se.
>> >> >
>> >> >
>> >> > On Fri, Aug 29, 2014 at 12:17 PM, Patrick Wendell <pwendell@gmail.c=
om
>> >
>> >> > wrote:
>> >> >>
>> >> >> Okay I'll plan to add cdh4 binary as well for the final release!
>> >> >>
>> >> >> ---
>> >> >> sent from my phone
>> >> >> On Aug 29, 2014 8:26 AM, "Ye Xianjin" <advancedxy@gmail.com> wrote=
:
>> >> >>
>> >> >> > We just used CDH 4.7 for our production cluster. And I believe w=
e
>> >> >> > won't
>> >> >> > use CDH 5 in the next year.
>> >> >> >
>> >> >> > Sent from my iPhone
>> >> >> >
>> >> >> > > On 2014=E5=B9=B48=E6=9C=8829=E6=97=A5, at 14:39, Matei Zaharia=
 <matei.zaharia@gmail.com>
>> >> >> > > wrote:
>> >> >> > >
>> >> >> > > Personally I'd actually consider putting CDH4 back if there ar=
e
>> >> >> > > still
>> >> >> > users on it. It's always better to be inclusive, and the
>> convenience
>> >> >> > of
>> >> >> > a
>> >> >> > one-click download is high. Do we have a sense on what % of CDH
>> users
>> >> >> > still
>> >> >> > use CDH4?
>> >> >> > >
>> >> >> > > Matei
>> >> >> > >
>> >> >> > > On August 28, 2014 at 11:31:13 PM, Sean Owen (sowen@cloudera.c=
om
>> )
>> >> >> > > wrote:
>> >> >> > >
>> >> >> > > (Copying my reply since I don't know if it goes to the mailing
>> >> >> > > list)
>> >> >> > >
>> >> >> > > Great, thanks for explaining the reasoning. You're saying thes=
e
>> >> >> > > aren't
>> >> >> > > going into the final release? I think that moots any issue
>> >> >> > > surrounding
>> >> >> > > distributing them then.
>> >> >> > >
>> >> >> > > This is all I know of from the ASF:
>> >> >> > > https://community.apache.org/projectIndependence.html I don't
>> read
>> >> >> > > it
>> >> >> > > as expressly forbidding this kind of thing although you can se=
e
>> how
>> >> >> > > it
>> >> >> > > bumps up against the spirit. There's not a bright line -- what
>> >> >> > > about
>> >> >> > > Tomcat providing binaries compiled for Windows for example? do=
es
>> >> >> > > that
>> >> >> > > favor an OS vendor?
>> >> >> > >
>> >> >> > > From this technical ASF perspective only the releases matter -=
-
>> do
>> >> >> > > what you want with snapshots and RCs. The only issue there is
>> maybe
>> >> >> > > releasing something different than was in the RC; is that at a=
ll
>> >> >> > > confusing? Just needs a note.
>> >> >> > >
>> >> >> > > I think this theoretical issue doesn't exist if these binaries
>> >> >> > > aren't
>> >> >> > > released, so I see no reason to not proceed.
>> >> >> > >
>> >> >> > > The rest is a different question about whether you want to spe=
nd
>> >> >> > > time
>> >> >> > > maintaining this profile and candidate. The vendor already
>> manages
>> >> >> > > their build I think and -- and I don't know -- may even prefer
>> not
>> >> >> > > to
>> >> >> > > have a different special build floating around. There's also t=
he
>> >> >> > > theoretical argument that this turns off other vendors from
>> >> >> > > adopting
>> >> >> > > Spark if it's perceived to be too connected to other vendors. =
I'd
>> >> >> > > like
>> >> >> > > to maximize Spark's distribution and there's some argument you=
 do
>> >> >> > > this
>> >> >> > > by not making vendor profiles. But as I say a different questi=
on
>> to
>> >> >> > > just think about over time...
>> >> >> > >
>> >> >> > > (oh and PS for my part I think it's a good thing that CDH4
>> binaries
>> >> >> > > were removed. I wasn't arguing for resurrecting them)
>> >> >> > >
>> >> >> > >> On Fri, Aug 29, 2014 at 7:26 AM, Patrick Wendell
>> >> >> > >> <pwendell@gmail.com>
>> >> >> > wrote:
>> >> >> > >> Hey Sean,
>> >> >> > >>
>> >> >> > >> The reason there are no longer CDH-specific builds is that al=
l
>> >> >> > >> newer
>> >> >> > >> versions of CDH and HDP work with builds for the upstream Had=
oop
>> >> >> > >> projects. I dropped CDH4 in favor of a newer Hadoop version
>> (2.4)
>> >> >> > >> and
>> >> >> > >> the Hadoop-without-Hive (also 2.4) build.
>> >> >> > >>
>> >> >> > >> For MapR - we can't officially post those artifacts on ASF we=
b
>> >> >> > >> space
>> >> >> > >> when we make the final release, we can only link to them as
>> being
>> >> >> > >> hosted by MapR specifically since they use non-compatible
>> >> >> > >> licenses.
>> >> >> > >> However, I felt that providing these during a testing period =
was
>> >> >> > >> alright, with the goal of increasing test coverage. I couldn'=
t
>> >> >> > >> find
>> >> >> > >> any policy against posting these on personal web space during=
 RC
>> >> >> > >> voting. However, we can remove them if there is one.
>> >> >> > >>
>> >> >> > >> Dropping CDH4 was more because it is now pretty old, but we c=
an
>> >> >> > >> add
>> >> >> > >> it
>> >> >> > >> back if people want. The binary packaging is a slightly separ=
ate
>> >> >> > >> question from release votes, so I can always add more binary
>> >> >> > >> packages
>> >> >> > >> whenever. And on this, my main concern is covering the most
>> >> >> > >> popular
>> >> >> > >> Hadoop versions to lower the bar for users to build and test
>> >> >> > >> Spark.
>> >> >> > >>
>> >> >> > >> - Patrick
>> >> >> > >>
>> >> >> > >>> On Thu, Aug 28, 2014 at 11:04 PM, Sean Owen <
>> sowen@cloudera.com>
>> >> >> > wrote:
>> >> >> > >>> +1 I tested the source and Hadoop 2.4 release. Checksums and
>> >> >> > >>> signatures are OK. Compiles fine with Java 8 on OS X. Tests.=
..
>> >> >> > >>> don't
>> >> >> > >>> fail any more than usual.
>> >> >> > >>>
>> >> >> > >>> FWIW I've also been using the 1.1.0-SNAPSHOT for some time i=
n
>> >> >> > >>> another
>> >> >> > >>> project and have encountered no problems.
>> >> >> > >>>
>> >> >> > >>>
>> >> >> > >>> I notice that the 1.1.0 release removes the CDH4-specific
>> build,
>> >> >> > >>> but
>> >> >> > >>> adds two MapR-specific builds. Compare with
>> >> >> > >>> https://dist.apache.org/repos/dist/release/spark/spark-1.0.2=
/
>> I
>> >> >> > >>> commented on the commit:
>> >> >> > >>>
>> >> >> >
>> >> >> >
>> >> >> >
>> https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede7=
13a73cc
>> >> >> > >>>
>> >> >> > >>> I'm in favor of removing all vendor-specific builds. This
>> change
>> >> >> > >>> *looks* a bit funny as there was no JIRA (?) and appears to
>> swap
>> >> >> > >>> one
>> >> >> > >>> vendor for another. Of course there's nothing untoward going
>> on,
>> >> >> > >>> but
>> >> >> > >>> what was the reasoning? It's best avoided, and MapR already
>> >> >> > >>> distributes Spark just fine, no?
>> >> >> > >>>
>> >> >> > >>> This is a gray area with ASF projects. I mention it as well
>> >> >> > >>> because
>> >> >> > >>> it
>> >> >> > >>> came up with Apache Flink recently
>> >> >> > >>> (
>> >> >> >
>> >> >> >
>> >> >> >
>> http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.m=
box/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.g=
mail.com%3E
>> >> >> > )
>> >> >> > >>> Another vendor rightly noted this could look like favoritism=
.
>> >> >> > >>> They
>> >> >> > >>> changed to remove vendor releases.
>> >> >> > >>>
>> >> >> > >>>> On Fri, Aug 29, 2014 at 3:14 AM, Patrick Wendell
>> >> >> > >>>> <pwendell@gmail.com>
>> >> >> > wrote:
>> >> >> > >>>> Please vote on releasing the following candidate as Apache
>> Spark
>> >> >> > version 1.1.0!
>> >> >> > >>>>
>> >> >> > >>>> The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
>> >> >> > >>>>
>> >> >> >
>> >> >> >
>> >> >> >
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D711=
aebb329ca28046396af1e34395a0df92b5327
>> >> >> > >>>>
>> >> >> > >>>> The release files, including signatures, digests, etc. can =
be
>> >> >> > >>>> found
>> >> >> > at:
>> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2/
>> >> >> > >>>>
>> >> >> > >>>> Release artifacts are signed with the following key:
>> >> >> > >>>> https://people.apache.org/keys/committer/pwendell.asc
>> >> >> > >>>>
>> >> >> > >>>> The staging repository for this release can be found at:
>> >> >> > >>>>
>> >> >> >
>> >> >> >
>> https://repository.apache.org/content/repositories/orgapachespark-1029/
>> >> >> > >>>>
>> >> >> > >>>> The documentation corresponding to this release can be foun=
d
>> at:
>> >> >> > >>>> http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/
>> >> >> > >>>>
>> >> >> > >>>> Please vote on releasing this package as Apache Spark 1.1.0=
!
>> >> >> > >>>>
>> >> >> > >>>> The vote is open until Monday, September 01, at 03:11 UTC a=
nd
>> >> >> > >>>> passes
>> >> >> > if
>> >> >> > >>>> a majority of at least 3 +1 PMC votes are cast.
>> >> >> > >>>>
>> >> >> > >>>> [ ] +1 Release this package as Apache Spark 1.1.0
>> >> >> > >>>> [ ] -1 Do not release this package because ...
>> >> >> > >>>>
>> >> >> > >>>> To learn more about Apache Spark, please see
>> >> >> > >>>> http://spark.apache.org/
>> >> >> > >>>>
>> >> >> > >>>> =3D=3D Regressions fixed since RC1 =3D=3D
>> >> >> > >>>> LZ4 compression issue:
>> >> >> > https://issues.apache.org/jira/browse/SPARK-3277
>> >> >> > >>>>
>> >> >> > >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> >> >> > >>>> This vote is happening very late into the QA period compare=
d
>> >> >> > >>>> with
>> >> >> > >>>> previous votes, so -1 votes should only occur for significa=
nt
>> >> >> > >>>> regressions from 1.0.2. Bugs already present in 1.0.X will =
not
>> >> >> > >>>> block
>> >> >> > >>>> this release.
>> >> >> > >>>>
>> >> >> > >>>> =3D=3D What default changes should I be aware of? =3D=3D
>> >> >> > >>>> 1. The default value of "spark.io.compression.codec" is now
>> >> >> > >>>> "snappy"
>> >> >> > >>>> --> Old behavior can be restored by switching to "lzf"
>> >> >> > >>>>
>> >> >> > >>>> 2. PySpark now performs external spilling during aggregatio=
ns.
>> >> >> > >>>> --> Old behavior can be restored by setting
>> >> >> > >>>> "spark.shuffle.spill"
>> >> >> > >>>> to
>> >> >> > "false".
>> >> >> > >>>>
>> >> >> > >>>>
>> >> >> > >>>>
>> >> >> > >>>>
>> ---------------------------------------------------------------------
>> >> >> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >> > >
>> >> >> > >
>> >> >> > >
>> ---------------------------------------------------------------------
>> >> >> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> > > For additional commands, e-mail: dev-help@spark.apache.org
>> >> >> > >
>> >> >> >
>> >> >
>> >> >
>> >
>> >
>>



--=20
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 20:33:44 2014
Return-Path: <dev-return-9176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B653211C9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 20:33:44 +0000 (UTC)
Received: (qmail 56334 invoked by uid 500); 29 Aug 2014 20:33:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56273 invoked by uid 500); 29 Aug 2014 20:33:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56254 invoked by uid 99); 29 Aug 2014 20:33:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 20:33:43 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of freeman.jeremy@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 20:33:17 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <freeman.jeremy@gmail.com>)
	id 1XNSrE-0006Ek-4a
	for dev@spark.incubator.apache.org; Fri, 29 Aug 2014 13:33:16 -0700
Date: Fri, 29 Aug 2014 13:33:16 -0700 (PDT)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409344396103-8143.post@n3.nabble.com>
In-Reply-To: <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
References: <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com> <CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com> <CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com> <etPan.5400202e.579478fe.470a@mbp-3.local> <BE021340-0174-4147-8CFB-57E898B79840@gmail.com> <CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com> <CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com> <CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com> <CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com> <CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

+1. Validated several custom analysis pipelines on a private cluster in
standalone mode. Tested new PySpark support for arbitrary Hadoop input
formats, works great!

-- Jeremy



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-1-0-RC2-tp8107p8143.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 21:54:04 2014
Return-Path: <dev-return-9177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72B5F11FBA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 21:54:04 +0000 (UTC)
Received: (qmail 60406 invoked by uid 500); 29 Aug 2014 21:54:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60349 invoked by uid 500); 29 Aug 2014 21:54:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 97487 invoked by uid 99); 29 Aug 2014 19:21:18 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of sudershan.malpani@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Fri, 29 Aug 2014 12:20:53 -0700 (PDT)
From: smalpani <sudershan.malpani@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409340053924-8142.post@n3.nabble.com>
Subject: Need to check approach for continuing development on Spark
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

We are developing an app in Spring in which we are using Cassandra and
calling datastax api's from Java to query it. The internal library is
responsible for calling cassandra and other data sources like RDS. We are
calling several client API's from Spark provided by the client-jar to
perform certain operations on that data like:
1. Reading data from S3 and inserting in cassandra by providing the objects
through API and then internally the API will store in cassandra.
2. Taking the data from cassandra through API as objects and then processing
on that data to generate metrics and saving it in cassandra through APi's
only.
3. Then internally through those API's only calculating aggregates and
separating data in bands etc.

The whole project is driven by Spring. Please let me know if we are
approaching towards it fine.




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Need-to-check-approach-for-continuing-development-on-Spark-tp8142.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Aug 29 22:20:37 2014
Return-Path: <dev-return-9178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63E30110C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Aug 2014 22:20:37 +0000 (UTC)
Received: (qmail 16755 invoked by uid 500); 29 Aug 2014 22:20:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16678 invoked by uid 500); 29 Aug 2014 22:20:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16664 invoked by uid 99); 29 Aug 2014 22:20:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 22:20:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Aug 2014 22:20:10 +0000
Received: by mail-ob0-f181.google.com with SMTP id vb8so2329829obc.12
        for <dev@spark.apache.org>; Fri, 29 Aug 2014 15:20:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Kt8q8p8KgSNskihQWrWEOqeBqvBlQnyxFCTXCuWfrZI=;
        b=v+02aD3nH7j+x3BFQiCvMRKcclOg2X1ufoofNXckkF6iGbS2paS1YqeqirjYmzPo1B
         pjo9uOdtgSQQ55iKVccHhbowtXwgy+5I5oSDBAXu7uWfVOG6Ur1KBc8U7ORmyHeFqhdV
         zVCsdL4Q6UuCYVbpTE09RGDkjPbAiH8Q8ZoLWhA2BrTUayb+ErCxFHT2ADWz7Wi9QXa6
         CirgI33fVr1XWes7ZDVqpIFd0dZ5GWb4VmW/WNQCKXGwO0AD89QBut4adfHYXtHoCacw
         OXlJ9L49oJMEE8bYMdaYSjIJhquJGZQBqPf/jg1jC3gZgd+ID0LLb8QLcgR+M5lwtA15
         WlSA==
MIME-Version: 1.0
X-Received: by 10.182.44.135 with SMTP id e7mr13170382obm.18.1409350808789;
 Fri, 29 Aug 2014 15:20:08 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Fri, 29 Aug 2014 15:20:08 -0700 (PDT)
In-Reply-To: <614A85E8-CF17-4F91-8767-D73D40426868@gmail.com>
References: <CAMQ+LQNhpAd_dP69bxphxv2Bq59tc4ePS6yz1yT_dQd8x6y19g@mail.gmail.com>
	<614A85E8-CF17-4F91-8767-D73D40426868@gmail.com>
Date: Fri, 29 Aug 2014 15:20:08 -0700
Message-ID: <CABPQxsuBi51zhppp71yCKL-Ooo6mHC8zdy-56TRcQnKUXwm2Mw@mail.gmail.com>
Subject: Re: Compie error with XML elements
From: Patrick Wendell <pwendell@gmail.com>
To: Yi Tian <tianyi.asiainfo@gmail.com>
Cc: Devl Devel <devl.development@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

In some cases IntelliJ's Scala compiler can't compile valid Scala
source files. Hopefully they fix (or have fixed) this in a newer
version.

- Patrick

On Fri, Aug 29, 2014 at 11:38 AM, Yi Tian <tianyi.asiainfo@gmail.com> wrote:
> Hi, Devl!
>
> I got the same problem.
>
> You can try to upgrade your scala plugins to  0.41.2
>
> It works on my mac.
>
> On Aug 12, 2014, at 15:19, Devl Devel <devl.development@gmail.com> wrote:
>
>> When compiling the master checkout of spark. The Intellij compile fails
>> with:
>>
>>    Error:(45, 8) not found: value $scope
>>      <div class="row-fluid">
>>       ^
>> which is caused by HTML elements in classes like HistoryPage.scala:
>>
>>    val content =
>>      <div class="row-fluid">
>>        <div class="span12">...
>>
>> How can I compile these classes that have html node elements in them?
>>
>> Thanks in advance.
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 30 07:42:46 2014
Return-Path: <dev-return-9179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D88011AE1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 30 Aug 2014 07:42:46 +0000 (UTC)
Received: (qmail 78155 invoked by uid 500); 30 Aug 2014 07:42:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78087 invoked by uid 500); 30 Aug 2014 07:42:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78075 invoked by uid 99); 30 Aug 2014 07:42:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 07:42:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 07:42:40 +0000
Received: by mail-we0-f176.google.com with SMTP id q59so3125470wes.35
        for <dev@spark.apache.org>; Sat, 30 Aug 2014 00:42:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=/z7OiYjsPHGxhkuBFaFHB5GaKlHbSBVRJb6XH2XScBs=;
        b=Djp5zB2IYc51YALYw0JEEPT+IPGDB+UmLc0TZIA+7GhVcZIwXPB8djE6Xb99DoirdH
         n+DaKAQX+ikdTMhurR5x4yCeBC6Mjtws/TZYgcy8Xv+RYpNRXNyvmbWQfN6ck9hH3duL
         +nJ50m6cg0VR90uo8ZqUHyo1EpoQE34D1RmO6zNdup1F/yFXxBit9ceN2V5Y90LTNVhz
         PSKT0uhS2ofCuVHr4Lr79sBycSFtDLyNqwkelRGiV6x/OC36FG8gYBG+yBciIFnFgHRy
         Qn7vOM6x7xY/WuDbQ11Y34Taj9rYzMWbClrf+xTgALrnp+ONZZjE7LYGv08kXFtLSpYd
         VMfg==
X-Received: by 10.194.119.98 with SMTP id kt2mr910747wjb.96.1409384539792;
 Sat, 30 Aug 2014 00:42:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sat, 30 Aug 2014 00:41:39 -0700 (PDT)
In-Reply-To: <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 30 Aug 2014 03:41:39 -0400
Message-ID: <CAOhmDzeAO2QojA5pFGJ_4FiSjswG2KmVhhwZM=cczZ0GYFEZKA@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Patrick Wendell <pwendell@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01227b86d8ab290501d3e87b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01227b86d8ab290501d3e87b
Content-Type: text/plain; charset=UTF-8

On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> it's actually precedurally difficult for us to close pull requests


Just an FYI: Seems like the GitHub-sanctioned work-around to having
issues-only permissions is to have a second, issues-only repository
<https://help.github.com/articles/issues-only-access-permissions>. Not a
very attractive work-around...

Nick

--089e01227b86d8ab290501d3e87b--

From dev-return-9180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 30 22:03:37 2014
Return-Path: <dev-return-9180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E59E115B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 30 Aug 2014 22:03:37 +0000 (UTC)
Received: (qmail 34536 invoked by uid 500); 30 Aug 2014 22:03:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34471 invoked by uid 500); 30 Aug 2014 22:03:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34459 invoked by uid 99); 30 Aug 2014 22:03:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 22:03:36 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 22:03:31 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so2844864obc.33
        for <dev@spark.apache.org>; Sat, 30 Aug 2014 15:03:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=trPasbuG3D12n3intwcftUyQ5F+UBBIwRazVt++EmCI=;
        b=C8dc4rCkYsXQIHqOVgy/vESCiESOFWZFJmIc0eCEVyCx1di0gKH0hjze9hmXiy4seY
         FebmFUm0wIiC0X5UhXTWAJTJhDTXSRrchsJm9KvjR4U/kppFrVP5hx0Thwo6RPxJvjvi
         bfmi+gTfBoshlIo/scGQjh/w+8SLu3Y5cZzoWRBvjUTTlKieDwmEhBf2Zz9dOMTlU38b
         Mt/STLwOVq+iJ0DF+hgwMVEF/yq2LahjqGrOo/GG1m9mU05vmwtwEm9ljWuATd53DlFm
         MVGu5C07r2SxbGfmU59/sH2q/i3TI0WHVCP4hnes5nVGvkNLTYz5bZ1PhikTI7v2tW7u
         JDPA==
MIME-Version: 1.0
X-Received: by 10.60.133.103 with SMTP id pb7mr18405969oeb.48.1409436191275;
 Sat, 30 Aug 2014 15:03:11 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Sat, 30 Aug 2014 15:03:11 -0700 (PDT)
In-Reply-To: <1409344396103-8143.post@n3.nabble.com>
References: <CAMAsSdJBBa0gaCwd7GM-sc8K7KwBbt=iM0NqVadsgOaM0i408g@mail.gmail.com>
	<CABPQxsvt4_Yd_xgiV8QyA8rS=sLnANTSdJQXJKL8dB2qH1cWLg@mail.gmail.com>
	<CAMAsSdKKJQWzva6PPqrjM-RwFcT+rN3knAx7UcCe9jk3Y6LHGw@mail.gmail.com>
	<etPan.5400202e.579478fe.470a@mbp-3.local>
	<BE021340-0174-4147-8CFB-57E898B79840@gmail.com>
	<CABPQxsvHSTguEMX970kO-MfVvqHoYCWBsJSc5bntK6VQm6qPGg@mail.gmail.com>
	<CAOhmDzfz9vAdX8YFOGXJeJeTNRjzkXY2G=tXV9DXtBXEC+UK0w@mail.gmail.com>
	<CABPQxsv0arrMqpeUm2qrtXdmc2hGyXQ2_MtYymJt+a3ccy+ttw@mail.gmail.com>
	<CAOhmDzfeCTyboEUHQxakyfLRaUYbD3DqQEkJnCWV_=hVVMNJoA@mail.gmail.com>
	<CABPQxssRCV8b3jiqppcA5j7i2XfV_uDSm-HOxm9pN2DxeerTJg@mail.gmail.com>
	<1409344396103-8143.post@n3.nabble.com>
Date: Sat, 30 Aug 2014 15:03:11 -0700
Message-ID: <CABPQxstmvoV3v3uf05G_VLH1ig_izvOtzAB1N6Jnxga311zKTw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks to Nick Chammas and Cheng Lian who pointed out two issues with
the release candidate. I'll cancel this in favor of RC3.

On Fri, Aug 29, 2014 at 1:33 PM, Jeremy Freeman
<freeman.jeremy@gmail.com> wrote:
> +1. Validated several custom analysis pipelines on a private cluster in
> standalone mode. Tested new PySpark support for arbitrary Hadoop input
> formats, works great!
>
> -- Jeremy
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-1-0-RC2-tp8107p8143.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Aug 30 22:08:20 2014
Return-Path: <dev-return-9181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81CEF115C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 30 Aug 2014 22:08:20 +0000 (UTC)
Received: (qmail 37551 invoked by uid 500); 30 Aug 2014 22:08:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37471 invoked by uid 500); 30 Aug 2014 22:08:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37458 invoked by uid 99); 30 Aug 2014 22:08:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 22:08:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 30 Aug 2014 22:08:14 +0000
Received: by mail-ob0-f169.google.com with SMTP id wp4so2856097obc.14
        for <dev@spark.apache.org>; Sat, 30 Aug 2014 15:07:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Psk9dR8BJTWdINUIexOxnHFUpYJ0LMhaEEysRGzwQbQ=;
        b=JUJqROMaVU33VQmT/589ryW6KfVaZhWWLZcW8V21rZq2Zmrtd4wHNAwp3Y3TMFZRq7
         4qKs8WgFQyEUbRwTvKEGmoFCYpRupUKcRuXvcCsipkPsMA4OLVGIh1pkh5WPv+Snmp/V
         lia6A+ZR0azHFq+VB5ysp6M1RNt4vBETByHo1YGbiPnSXpKtFFv85W6vuvlYSEefGYhu
         6/GZtdTy+iPhAs5DnastTdHiXYz6gypJdDCUwO12/T+x9BGQv6JaWgmHB0a5L53l8yUx
         Q0MKX18Mc4iLZVfcQVXw8rmvy5jRdmqpHbBgWzEn9t8oTFE0cLEw1q//YhYN8ODudvfp
         b+ag==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr17746221obb.34.1409436473389;
 Sat, 30 Aug 2014 15:07:53 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Sat, 30 Aug 2014 15:07:52 -0700 (PDT)
Date: Sat, 30 Aug 2014 15:07:52 -0700
Message-ID: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.1.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc3 (commit b2d0493b):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b2d0493b223c5f98a593bb6d7372706cc02bebad

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1030/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Tuesday, September 02, at 23:07 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== Regressions fixed since RC1 ==
- Build issue for SQL support: https://issues.apache.org/jira/browse/SPARK-3234
- EC2 script version bump to 1.1.0.

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of "spark.io.compression.codec" is now "snappy"
--> Old behavior can be restored by switching to "lzf"

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting "spark.shuffle.spill" to "false".

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 02:39:32 2014
Return-Path: <dev-return-9182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3158B11894
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 02:39:32 +0000 (UTC)
Received: (qmail 78069 invoked by uid 500); 31 Aug 2014 02:39:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77995 invoked by uid 500); 31 Aug 2014 02:39:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77981 invoked by uid 99); 31 Aug 2014 02:39:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 02:39:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 02:39:03 +0000
Received: by mail-pd0-f182.google.com with SMTP id fp1so3262321pdb.13
        for <dev@spark.apache.org>; Sat, 30 Aug 2014 19:39:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:subject:message-id:date:to:mime-version;
        bh=KvFySBHR1PLCDymSeIcs4yDHveXLin85xzV8/0/aSpM=;
        b=SsqX0LGnjub2mHnRdXUPxC1wKOg/MbDTF/fgaYIyazyfkwsRwO2L8qTVGGQrmm34RF
         u6d6vzB5RPIdtLIE1wkrBBibAUxJvr2lt+Z/0AlPt0UFFUvusSx2Y1SaC4JHuva7NcMP
         ShT6Xb4Y14H9NtC9+EKWegGOOlwogxi448uDDM2dV1PC+Bus4m/YqJLiso1tlH9n3Y4S
         ETzvei4K/2VWFUVT1EDTU2D1CZXPfkftpN7BxpBr3LvHg9+uF0Mea1YILSz+6tNARsps
         PeD10/R/04oSLyDHhp0BQ6p26k8obBDw0yIiEcuEk4CV5MdWLOVNFVn/tjaIGSc6UVKi
         P8Gw==
X-Received: by 10.70.131.12 with SMTP id oi12mr28561346pdb.116.1409452741292;
        Sat, 30 Aug 2014 19:39:01 -0700 (PDT)
Received: from [192.168.56.93] ([183.129.208.18])
        by mx.google.com with ESMTPSA id m1sm6145747pdh.18.2014.08.30.19.38.54
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 30 Aug 2014 19:39:00 -0700 (PDT)
From: Yi Tian <tianyi.asiainfo@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_8CC12D10-6146-4BFD-B3CF-8743312E60CB"
Subject: [SPARK-3324] make yarn module as a unified maven jar project
Message-Id: <3189BFB1-B38A-4564-8005-33F39C18515E@gmail.com>
Date: Sun, 31 Aug 2014 10:38:46 +0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_8CC12D10-6146-4BFD-B3CF-8743312E60CB
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hi everyone!

I found the YARN module has nonstandard path structure like:

${SPARK_HOME}
  |--yarn
     |--alpha (contains yarn api support for 0.23 and 2.0.x)
     |--stable (contains yarn api support for 2.2 and later)
     |     |--pom.xml (spark-yarn)
     |--common (Common codes not depending on specific version of =
Hadoop)
     |--pom.xml (yarn-parent)

When we use maven to compile yarn module, maven will import 'alpha' or =
'stable' module according to profile setting.
And the submodule like 'stable' use the build propertie defined in =
yarn/pom.xml to import common codes to sourcePath.
It will cause IntelliJ can't directly recognize sources in common =
directory as sourcePath.

I thought we should change the yarn module to a unified maven jar =
project,=20
and add specify different version of yarn api via maven profile setting.

I created a JIRA ticket: =
https://issues.apache.org/jira/browse/SPARK-3324

Any advice will be appreciated .





--Apple-Mail=_8CC12D10-6146-4BFD-B3CF-8743312E60CB--

From dev-return-9183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 03:03:56 2014
Return-Path: <dev-return-9183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39DEA118CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 03:03:56 +0000 (UTC)
Received: (qmail 87009 invoked by uid 500); 31 Aug 2014 03:03:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86932 invoked by uid 500); 31 Aug 2014 03:03:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86913 invoked by uid 99); 31 Aug 2014 03:03:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 03:03:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 03:03:50 +0000
Received: by mail-qa0-f54.google.com with SMTP id x12so3653601qac.41
        for <dev@spark.apache.org>; Sat, 30 Aug 2014 20:03:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=rBlqszvwUfAhe2SJHU/suq8xcYK9CLIJZHlYoM5rZ9M=;
        b=Mhp6zrh701wxpw9WagSqNwU3u7HecMe0Bv+NC9FxoxcHbLBXt+qiital5BTljc9R/L
         ikhsg8/NE0eevtr587qVtr5hy1nvP0CP5eZYLJdefEzcdybpbSXZFydWPvsca/3lc4Wt
         Nkv9LO2ovKRyGKV+77mW1ZlAIOv1BdOMr8/YSPbIVQhSZvLWXSwqRBR8qI6gnCD7IojI
         zKfs1QMB0lq7FXscl7P9WipM4ca1ghY80SC7nw8AvHIUqVu0bqcgfLzfJG+KxwuYSBPy
         PcgEH3fYr88k3D+nxTuL711P3ATczLVibzx6Ok6XGBLJulD7ZrCVsbyBKjWj/a8xzoVe
         2w+Q==
X-Gm-Message-State: ALoCoQnjSalYvG5FeJsMxhcw4x0Ag0GFFjNzXdkpPURCQXigHFn0qKCHUwKPdQYlVEWPUVnMcUy4
X-Received: by 10.140.96.180 with SMTP id k49mr31082761qge.78.1409454208938;
 Sat, 30 Aug 2014 20:03:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.41.34 with HTTP; Sat, 30 Aug 2014 20:03:08 -0700 (PDT)
In-Reply-To: <5CF7038F-2C17-4024-A02D-64092B917714@gmail.com>
References: <EBBB1012-E630-40A5-9A72-404A93D05590@gmail.com>
 <CAPh_B=bqG4zmw3gh9=1OffvptwB0yn9aVNvSiooRNkxHGF8h4A@mail.gmail.com>
 <6AAB6579-DE62-4CD9-88F8-A015A66464DE@gmail.com> <CAPh_B=Y6SH8MGgbZmCsNnmg=eiV_by09rb8o-adFPvPoyTSs_Q@mail.gmail.com>
 <80D8F494-FD9C-44E4-81D9-6C189308D158@gmail.com> <CAPh_B=YGWy+R1fHHGUVBHg2Aj4TmgLmNjHO=JUvxjrhYgFiwaQ@mail.gmail.com>
 <5CF7038F-2C17-4024-A02D-64092B917714@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 30 Aug 2014 20:03:08 -0700
Message-ID: <CAPh_B=ZVhgeEh_VNxbfp=AK1h6-Y_hEPFPTe5F4_EGtM3d1b+g@mail.gmail.com>
Subject: Fwd: Partitioning strategy changed in Spark 1.0.x?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ac3d0737a530501e42178
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ac3d0737a530501e42178
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sending the response back to the dev list so this is indexable and
searchable by others.

---------- Forwarded message ----------
From: Milos Nikolic <milos.nikolic83@gmail.com>
Date: Sat, Aug 30, 2014 at 5:50 PM
Subject: Re: Partitioning strategy changed in Spark 1.0.x?
To: Reynold Xin <rxin@databricks.com>


Thank you, your insights were very helpful, and we managed to find a
solution that works for us.

Best,
Milos


On Aug 27, 2014, at 11:20 PM, Reynold Xin <rxin@databricks.com> wrote:

I don't think you can ever expect the mapping from data to physical nodes
in Spark, even in Spark 0.9. That is because the scheduler needs to be
fault-tolerant. What if the node is busy or the node is down?

What happens is the partitioning of data is deterministic, i.e. certain
data is always hashed into certain partitions (given the same partition
count). And if you don't run foreach twice, but instead simply zip the two
RDDs that are both hash partitioned using the same partitioner, then the
scheduler will not create extra stages.

e.g.

    // Let's say I have 10 nodes
    val partitioner =3D new HashPartitioner(10)

    // Create RDD
    val rdd =3D sc.parallelize(0 until 10).map(k =3D> (k, computeValue(k)))

    // Partition twice using the same partitioner
    val p1 =3D rdd.partitionBy(partitioner)
    val p2 =3D rdd.partitionBy(partitioner)
    p1.zip(p2)       <--- this should work




On Wed, Aug 27, 2014 at 1:50 PM, Milos Nikolic <milos.nikolic83@gmail.com>
wrote:

> Sure.
>
> Suppose we have two SQL relations, expressed as two RDDs, and we want to
> do a hash join between them. First, we would partition each RDD on the jo=
in
> key =E2=80=94 that will collocate partitions with the same join key on on=
e node.
> Then, I would zip corresponding partitions from two relations and do a
> local join on each node.
>
> This approach makes sense only if Spark always places key X on node Y for
> both RDDs, which is not true now. And I have no idea how to circumvent th=
is
> issue with the recent changes in hashing you mentioned.
>
> Milos
>
> On Aug 27, 2014, at 10:05 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> Can you elaborate your problem?
>
> I am not sure if I understand what you mean by "on one node, I get two
> different sets of keys"
>
>
> On Tue, Aug 26, 2014 at 2:16 AM, Milos Nikolic <milos.nikolic83@gmail.com=
>
> wrote:
>
>> Hi Reynold,
>>
>> The problem still exists even with more elements. On one node, I get two
>> different
>> sets of keys -- I want these local sets to be the same to be able to zip
>> local partitions
>> together later on (rather than RDD.join them, which involves shuffling).
>>
>> With this recent change in hashing, RDD.zip seems not to be useful
>> anymore
>> as I cannot guarantee anymore that local partitions from two RDDs will
>> share
>> the same set of keys on one node.
>>
>> Do you have any ideas on how to resolve this problem?
>>
>> Thanks,
>> Milos
>>
>>
>>
>> On Aug 26, 2014, at 10:04 AM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> It is better to use a larger number of elements rather than just 10 for
>> this test.
>>
>> Can you try larger? Like 1000 or 10000?
>>
>> IIRC, the hash function changed to murmur hash:
>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apac=
he/spark/util/collection/AppendOnlyMap.scala#L205
>>
>>
>>
>>
>>
>>
>> On Tue, Aug 26, 2014 at 1:01 AM, Milos Nikolic <milos.nikolic83@gmail.co=
m
>> > wrote:
>>
>>> Hi guys,
>>>
>>> I=E2=80=99ve noticed some changes in the behavior of partitioning under=
 Spark
>>> 1.0.x.
>>> I=E2=80=99d appreciate if someone could explain what has changed in the=
 meantime.
>>>
>>> Here is a small example. I want to create two RDD[(K, V)] objects and
>>> then
>>> collocate partitions with the same K on one node. When the same
>>> partitioner
>>> for two RDDs is used, partitions with the same K end up being on
>>> different nodes.
>>>
>>>     // Let's say I have 10 nodes
>>>     val partitioner =3D new HashPartitioner(10)
>>>
>>>     // Create RDD
>>>     val rdd =3D sc.parallelize(0 until 10).map(k =3D> (k, computeValue(=
k)))
>>>
>>>     // Partition twice using the same partitioner
>>>     rdd.partitionBy(partitioner).foreach { case (k, v) =3D>
>>> println("Dummy1 -> k =3D " + k) }
>>>     rdd.partitionBy(partitioner).foreach { case (k, v) =3D>
>>> println("Dummy2 -> k =3D " + k) }
>>>
>>> The output on one node is:
>>>     Dummy1 -> k =3D 2
>>>     Dummy2 -> k =3D 7
>>>
>>> I was expecting to see the same keys on each node. That was happening
>>> under Spark 0.9.2, but not under Spark 1.0.x.
>>>
>>> Anyone has an idea what has changed in the meantime? Or how to get
>>> corresponding partitions on one node?
>>>
>>> Thanks in advance,
>>> Milos
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>
>>
>
>

--001a113ac3d0737a530501e42178--

From dev-return-9184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 08:20:11 2014
Return-Path: <dev-return-9184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6F9C11C8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 08:20:10 +0000 (UTC)
Received: (qmail 73670 invoked by uid 500); 31 Aug 2014 08:20:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73602 invoked by uid 500); 31 Aug 2014 08:20:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73591 invoked by uid 99); 31 Aug 2014 08:20:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 08:20:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 08:20:02 +0000
Received: by mail-ig0-f177.google.com with SMTP id r10so4500190igi.4
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 01:19:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CUHOuLalrapg2DnSiKwC6e4CyT/27XKXFpmLpwBhh4k=;
        b=icuM+ZO5b1q5TO8zDghIOXUvC+ZwvCC4qzRiXQhdLcFmmlKZRdqMhk7PR9RCaK8r4r
         a2BEhR+XiTnjy1JlP/xRaMi9goLoyp0VVrYJEpOht+mMMO9Qftdh+t2IRi1KlM5tOt0I
         r4hSrd4HD6y4uCuZDBys/e3B8Mb5l7gZXqUJXSTvHd0j1oQrUJVY06vzjYVvUb3Ri7+T
         zNMGy+60MYVbi40KilTx7KlS0UwE7dJQaT3M4c9TdtWIVpmyti4/LJ5bCI5B8MawPZt6
         AOEePZqVswRSKfxMM4C12eQD53XyeVO2gCi/d0NyJaVToCaHArVvW4eyxqvryUU03NMP
         +5xA==
X-Gm-Message-State: ALoCoQljY7gwkniNxLD2Uj/6nx70+cYN7qryZa3QJZEStMGFNNOMv0EaXnKf90Kz3H6uNGcqk7ND
X-Received: by 10.42.82.6 with SMTP id b6mr813911icl.51.1409473182302; Sun, 31
 Aug 2014 01:19:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Sun, 31 Aug 2014 01:19:22 -0700 (PDT)
In-Reply-To: <3189BFB1-B38A-4564-8005-33F39C18515E@gmail.com>
References: <3189BFB1-B38A-4564-8005-33F39C18515E@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 31 Aug 2014 09:19:22 +0100
Message-ID: <CAMAsSdLhYDn4=r3FUt-c1zyLR6yEuCc8BPSDepuW_qVorhZTxA@mail.gmail.com>
Subject: Re: [SPARK-3324] make yarn module as a unified maven jar project
To: Yi Tian <tianyi.asiainfo@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This isn't possible since the two versions of YARN are mutually
incompatible at compile-time. However see my comments about how this
could be restructured to be a little more standard, and so that
IntelliJ would parse it out of the box.

Still I imagine it is not worth it if YARN alpha will go away at some
point and IntelliJ can easily be told where the extra src/ is.

On Sun, Aug 31, 2014 at 3:38 AM, Yi Tian <tianyi.asiainfo@gmail.com> wrote:
> Hi everyone!
>
> I found the YARN module has nonstandard path structure like:
>
> ${SPARK_HOME}
>   |--yarn
>      |--alpha (contains yarn api support for 0.23 and 2.0.x)
>      |--stable (contains yarn api support for 2.2 and later)
>      |     |--pom.xml (spark-yarn)
>      |--common (Common codes not depending on specific version of Hadoop)
>      |--pom.xml (yarn-parent)
>
> When we use maven to compile yarn module, maven will import 'alpha' or 'stable' module according to profile setting.
> And the submodule like 'stable' use the build propertie defined in yarn/pom.xml to import common codes to sourcePath.
> It will cause IntelliJ can't directly recognize sources in common directory as sourcePath.
>
> I thought we should change the yarn module to a unified maven jar project,
> and add specify different version of yarn api via maven profile setting.
>
> I created a JIRA ticket: https://issues.apache.org/jira/browse/SPARK-3324
>
> Any advice will be appreciated .
>
>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 12:33:43 2014
Return-Path: <dev-return-9185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B20F11045
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 12:33:43 +0000 (UTC)
Received: (qmail 24998 invoked by uid 500); 31 Aug 2014 12:33:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24925 invoked by uid 500); 31 Aug 2014 12:33:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24905 invoked by uid 99); 31 Aug 2014 12:33:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 12:33:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 12:33:15 +0000
Received: by mail-pa0-f45.google.com with SMTP id bj1so9948098pad.4
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 05:33:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=xM45fCYXVOy2w/Z2GiYlEs4ex7nu972S/KM9cTraji4=;
        b=mIZCbwnkK+jWkz4ykwDKN52bZ2WKdDzD29Sp98IOvA446bM0Jul3DyF8BdCYIrOiPy
         qEZJ4B25Xo2BmgegEG5+VAEcTb2ObE1R3t9Jhn7o90pO1DCgJfwhq3NFRimOB6ZL81My
         lwiySOPfGx5OpgiF21S8OqBF43zcDNBFAlZlANxqFm1z574/2n5sX2PiYFyIYLn9/XYt
         y8jI0uJcmk9HWxf4eHjDf7HSH89/yVwWPQ5BOD5rM9XID0bpnJJz36u0xFdhy4p4lpoH
         PqQ86KuLPDaZQyX6x6i5yu1So/+Hj/GQhBlVZWxwegs7gFO0trWLF9kWPgiXcQFPhSU1
         ax+A==
X-Received: by 10.66.251.195 with SMTP id zm3mr1216587pac.78.1409488393760;
        Sun, 31 Aug 2014 05:33:13 -0700 (PDT)
Received: from [172.20.10.2] ([211.140.4.153])
        by mx.google.com with ESMTPSA id c3sm7859514pdj.6.2014.08.31.05.33.11
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 31 Aug 2014 05:33:13 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [SPARK-3324] make yarn module as a unified maven jar project
From: Yi Tian <tianyi.asiainfo@gmail.com>
In-Reply-To: <CAMAsSdLhYDn4=r3FUt-c1zyLR6yEuCc8BPSDepuW_qVorhZTxA@mail.gmail.com>
Date: Sun, 31 Aug 2014 20:32:57 +0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <B58B331A-F0AC-43FA-80EB-DDDA22C3E5DE@gmail.com>
References: <3189BFB1-B38A-4564-8005-33F39C18515E@gmail.com> <CAMAsSdLhYDn4=r3FUt-c1zyLR6yEuCc8BPSDepuW_qVorhZTxA@mail.gmail.com>
To: Sean Owen <sowen@cloudera.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Sean

Before compile-time, maven could dynamically add either stable or alpha =
source to the yarn/ project.

So there are no incompatible at the compile-time.

Here are an example:

yarn/pom.xml

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>build-helper-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>add-scala-sources</id>
            <phase>generate-sources</phase>
            <goals>
              <goal>add-source</goal>
            </goals>
            <configuration>
              <sources>
                <source>common/src/main/scala</source>
                <source>${yarn.api}/src/main/scala</source>
              </sources>
            </configuration>
          </execution>
        </executions>
      </plugin>


On Aug 31, 2014, at 16:19, Sean Owen <sowen@cloudera.com> wrote:

> This isn't possible since the two versions of YARN are mutually
> incompatible at compile-time. However see my comments about how this
> could be restructured to be a little more standard, and so that
> IntelliJ would parse it out of the box.
>=20
> Still I imagine it is not worth it if YARN alpha will go away at some
> point and IntelliJ can easily be told where the extra src/ is.
>=20
> On Sun, Aug 31, 2014 at 3:38 AM, Yi Tian <tianyi.asiainfo@gmail.com> =
wrote:
>> Hi everyone!
>>=20
>> I found the YARN module has nonstandard path structure like:
>>=20
>> ${SPARK_HOME}
>>  |--yarn
>>     |--alpha (contains yarn api support for 0.23 and 2.0.x)
>>     |--stable (contains yarn api support for 2.2 and later)
>>     |     |--pom.xml (spark-yarn)
>>     |--common (Common codes not depending on specific version of =
Hadoop)
>>     |--pom.xml (yarn-parent)
>>=20
>> When we use maven to compile yarn module, maven will import 'alpha' =
or 'stable' module according to profile setting.
>> And the submodule like 'stable' use the build propertie defined in =
yarn/pom.xml to import common codes to sourcePath.
>> It will cause IntelliJ can't directly recognize sources in common =
directory as sourcePath.
>>=20
>> I thought we should change the yarn module to a unified maven jar =
project,
>> and add specify different version of yarn api via maven profile =
setting.
>>=20
>> I created a JIRA ticket: =
https://issues.apache.org/jira/browse/SPARK-3324
>>=20
>> Any advice will be appreciated .
>>=20
>>=20
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 14:30:16 2014
Return-Path: <dev-return-9186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38A911124E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 14:30:16 +0000 (UTC)
Received: (qmail 44948 invoked by uid 500); 31 Aug 2014 14:30:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44890 invoked by uid 500); 31 Aug 2014 14:30:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44878 invoked by uid 99); 31 Aug 2014 14:30:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 14:30:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 14:29:49 +0000
Received: by mail-ie0-f177.google.com with SMTP id tp5so4735429ieb.8
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 07:29:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6RR0nQXRliQc0HwET6mrSiBBCJbzWjBUUJg35ESEwwg=;
        b=WQPoDbkiMM7op+6CEdf+Tpe9nkp9EWKzp+Skai0DKVQF+RKLnv59C+evUFFq27t0uH
         xnU6W6I9EqXpn/3QyEB9YK6QygRtTPtgIlWPTLSX8fdNugM9JW4zhJkSFxzz6uohMfqg
         dcHWJi7XRXqNkPteP/jKJCjcAgtNTqF9Cdkg7/DmccOYFYCsnF6PWDDtsMTGkgIQA3NE
         XL+Jx2FIIdBmp60EJa+6mZK3+xG+oBTCaNvkOc/w6k9wf1NbAhyuQ1I10dH1obQn90i3
         dDMPjxxL4RjnYDDW1m0r9Dr5qFJRb+9C4mRdGY+N/0ocwKTjGQoU1LmW64oUwpLSelw3
         rUzQ==
X-Gm-Message-State: ALoCoQkXXNwyQr/svnlzq6Vs8odbXrKrGOxSGflf/JokxUqbKBe/BP0V7z12Y+T938Hf1tOGuDzj
X-Received: by 10.50.66.197 with SMTP id h5mr16077309igt.34.1409495388106;
 Sun, 31 Aug 2014 07:29:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Sun, 31 Aug 2014 07:29:26 -0700 (PDT)
In-Reply-To: <B58B331A-F0AC-43FA-80EB-DDDA22C3E5DE@gmail.com>
References: <3189BFB1-B38A-4564-8005-33F39C18515E@gmail.com>
 <CAMAsSdLhYDn4=r3FUt-c1zyLR6yEuCc8BPSDepuW_qVorhZTxA@mail.gmail.com> <B58B331A-F0AC-43FA-80EB-DDDA22C3E5DE@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 31 Aug 2014 15:29:26 +0100
Message-ID: <CAMAsSd+KXiH6rC+MMJyvDbSoNdHRyL-fW5VHn3DVfuQOzHw8bw@mail.gmail.com>
Subject: Re: [SPARK-3324] make yarn module as a unified maven jar project
To: Yi Tian <tianyi.asiainfo@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, alpha and stable need to stay in two separate modules. I think
this is a little less standard than simply having three modules:
common, stable, alpha.

On Sun, Aug 31, 2014 at 1:32 PM, Yi Tian <tianyi.asiainfo@gmail.com> wrote:
> Hi Sean
>
> Before compile-time, maven could dynamically add either stable or alpha source to the yarn/ project.
>
> So there are no incompatible at the compile-time.
>
> Here are an example:
>
> yarn/pom.xml
>
>       <plugin>
>         <groupId>org.codehaus.mojo</groupId>
>         <artifactId>build-helper-maven-plugin</artifactId>
>         <executions>
>           <execution>
>             <id>add-scala-sources</id>
>             <phase>generate-sources</phase>
>             <goals>
>               <goal>add-source</goal>
>             </goals>
>             <configuration>
>               <sources>
>                 <source>common/src/main/scala</source>
>                 <source>${yarn.api}/src/main/scala</source>
>               </sources>
>             </configuration>
>           </execution>
>         </executions>
>       </plugin>
>
>
> On Aug 31, 2014, at 16:19, Sean Owen <sowen@cloudera.com> wrote:
>
>> This isn't possible since the two versions of YARN are mutually
>> incompatible at compile-time. However see my comments about how this
>> could be restructured to be a little more standard, and so that
>> IntelliJ would parse it out of the box.
>>
>> Still I imagine it is not worth it if YARN alpha will go away at some
>> point and IntelliJ can easily be told where the extra src/ is.
>>
>> On Sun, Aug 31, 2014 at 3:38 AM, Yi Tian <tianyi.asiainfo@gmail.com> wrote:
>>> Hi everyone!
>>>
>>> I found the YARN module has nonstandard path structure like:
>>>
>>> ${SPARK_HOME}
>>>  |--yarn
>>>     |--alpha (contains yarn api support for 0.23 and 2.0.x)
>>>     |--stable (contains yarn api support for 2.2 and later)
>>>     |     |--pom.xml (spark-yarn)
>>>     |--common (Common codes not depending on specific version of Hadoop)
>>>     |--pom.xml (yarn-parent)
>>>
>>> When we use maven to compile yarn module, maven will import 'alpha' or 'stable' module according to profile setting.
>>> And the submodule like 'stable' use the build propertie defined in yarn/pom.xml to import common codes to sourcePath.
>>> It will cause IntelliJ can't directly recognize sources in common directory as sourcePath.
>>>
>>> I thought we should change the yarn module to a unified maven jar project,
>>> and add specify different version of yarn api via maven profile setting.
>>>
>>> I created a JIRA ticket: https://issues.apache.org/jira/browse/SPARK-3324
>>>
>>> Any advice will be appreciated .
>>>
>>>
>>>
>>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 16:02:27 2014
Return-Path: <dev-return-9187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB31711332
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 16:02:27 +0000 (UTC)
Received: (qmail 41032 invoked by uid 500); 31 Aug 2014 16:02:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40962 invoked by uid 500); 31 Aug 2014 16:02:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40946 invoked by uid 99); 31 Aug 2014 16:02:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 16:02:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 16:02:22 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so11771506igd.5
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 09:02:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Hpc/Jv8MtpBRq4tLKdQ4Izk1LJBzXxJaj6srnBV3UZY=;
        b=Z3lzOSn8h258kgJvt5xJdoeitLqFzrg8gQrvDreVRpHmIxhL6fmjMEfFQiVfZYNtX3
         v5gPZSRPBl/Nvd0tgxCp0YRCuMIkz1cU22iGB+Y62bGhIvuCWGlgkiJv60Q5UREZECNt
         FO+94pAqqIsMShXs9G9Fl2h2bS/OQWs5qub/wc9KAvOywf3Vvnwbj/9BUawe04nah/dF
         P6u3EOtGgNmq+xnCI01kpEGLC1NqBojAtpon635KE1HQALTeEJWJ+V1rxwE1OycEbdcm
         zjIK63DRMl+V/i5hNYSj8XD2xLUKttLDz1xx49oorYO3EnE6n/XalLwPIlvmMq+otOcK
         5TXA==
X-Gm-Message-State: ALoCoQl80eEQDRLzkC2zlpygRS7hZDPqZlW8y9yYT7F9toixvYrquKY1ZUmjZU5x/pn0Po8Bqf0n
X-Received: by 10.50.66.197 with SMTP id h5mr16448688igt.34.1409500922073;
 Sun, 31 Aug 2014 09:02:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Sun, 31 Aug 2014 09:01:41 -0700 (PDT)
In-Reply-To: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 31 Aug 2014 17:01:41 +0100
Message-ID: <CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

All the signatures are correct. The licensing all looks fine. The
source builds fine.

Now, let me ask about unit tests, since I had a more detailed look,
which I should have done before.


dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me
locally on 1.1.0-rc3. Does anyone else see that? It may be my env.
Although I still see the Hive failure on Debian too:

[info] - SET commands semantics for a HiveContext *** FAILED ***
[info]   Expected Array("spark.sql.key.usedfortestonly=test.val.0",
"spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"),
but got Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0",
"spark.sql.key.usedfortestonly=test.val.0") (HiveQuerySuite.scala:541)


Python lint checks fail for files in python/build/py4j. These aren't
Spark files and are only present in this location in the release. The
check should simply be updated later to ignore this. Not a blocker.


Evidently, the SBT tests pass, usually, in master:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/SparkPullRequestBuilder/
But Maven tests have not passed in master for a long time:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/

I can reproduce this with Maven for 1.1.0-rc3. It feels funny to ship
with a repeatable Maven build failure, since Maven is the build of
record for release. Whatever is being tested is probably OK since SBT
passes, so it need not block release. I'll look for a fix as well.

A simple "sbt test" always fails for me, and that just may be because
the build is now only meaningful with further configuration. SBT tests
are mostly passing if not consistently for all profiles:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/  These also sort of
feel funny, although nothing seems like an outright blocker.

I guess I'll add a non-binding +0 -- none of these are necessarily a
blocker but adds up to feeling a bit iffy about the state of tests in
the context of a release.

On Sat, Aug 30, 2014 at 11:07 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.1.0!
>
> The tag to be voted on is v1.1.0-rc3 (commit b2d0493b):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b2d0493b223c5f98a593bb6d7372706cc02bebad
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1030/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Tuesday, September 02, at 23:07 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == Regressions fixed since RC1 ==
> - Build issue for SQL support: https://issues.apache.org/jira/browse/SPARK-3234
> - EC2 script version bump to 1.1.0.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to "false".
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 17:11:52 2014
Return-Path: <dev-return-9188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4A0311414
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 17:11:52 +0000 (UTC)
Received: (qmail 36513 invoked by uid 500); 31 Aug 2014 17:11:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36439 invoked by uid 500); 31 Aug 2014 17:11:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36428 invoked by uid 99); 31 Aug 2014 17:11:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 17:11:51 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 17:11:47 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s7VHBNWp028769;
	Sun, 31 Aug 2014 13:11:23 -0400
Date: Sun, 31 Aug 2014 13:11:21 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org
Message-ID: <1549011493.48644371.1409505081207.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com> <CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.7]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF31 (Mac)/8.0.6_GA_5922)
Thread-Topic: Release Apache Spark 1.1.0 (RC3)
Thread-Index: niBKpfcejU9tWN3gdRkENy/8cg4r3A==
X-Virus-Checked: Checked by ClamAV on apache.org

----- Original Message -----

> dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me
> locally on 1.1.0-rc3. Does anyone else see that? It may be my env.
> Although I still see the Hive failure on Debian too:
> 
> [info] - SET commands semantics for a HiveContext *** FAILED ***
> [info]   Expected Array("spark.sql.key.usedfortestonly=test.val.0",
> "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"),
> but got
> Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0",
> "spark.sql.key.usedfortestonly=test.val.0") (HiveQuerySuite.scala:541)

I've seen this error before.  (In particular, I've seen it on my OS X machine using Oracle JDK 8 but not on Fedora using OpenJDK.)  I've also seen similar errors in topic branches (but not on master) that seem to indicate that tests depend on sets of pairs arriving from Hive in a particular order; it seems that this isn't a safe assumption.

I just submitted a (trivial) PR to fix this spurious failure:  https://github.com/apache/spark/pull/2220


best,
wb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 17:19:31 2014
Return-Path: <dev-return-9189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F73211427
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 17:19:31 +0000 (UTC)
Received: (qmail 46376 invoked by uid 500); 31 Aug 2014 17:19:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46308 invoked by uid 500); 31 Aug 2014 17:19:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46281 invoked by uid 99); 31 Aug 2014 17:19:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 17:19:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 17:19:04 +0000
Received: by mail-ie0-f178.google.com with SMTP id at1so4944315iec.23
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 10:19:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=tLYsPKjc3J+nVjVBFKZAEX33PZKCsXWjq2LRxOmP19A=;
        b=aemlLScW6LcQpsVYS5LP89EofRVLdpd7HqOhgt5RS1bzbKT6nbY178qlhnaBr/ukZZ
         o+AedcSEnmJ3IqmrgMDh1PH2OFFb1qeI+ATlvl5M0v3axJbpq/vYfGqx0H8Yfzi7sd/d
         g94ExKYh3iAYHMmVnP26MvIVdjYEkRau9bwQai3vAOXdbscqb2KgB8iIyVeoh8G91709
         Dza3vHMv7iWK/krzmlqFc9DaVlA9mqSy9Zy8p4a9SEWuODEaZAtM4hd7iPrKH07vcUW6
         LPgpNUsNLcsZqHo3PNXvVUxEkpwpnSoKD/6IIOenCbJzBUcSI/EIqeCHl1tTMfQEypEW
         +X+A==
X-Gm-Message-State: ALoCoQkAei0MvytFiWcOFpXJF16PdS1R4akgF9UyC3BZJkD165MbtEG95s+IQ0mboknLZEpjob1r
X-Received: by 10.43.57.203 with SMTP id wh11mr2142863icb.54.1409505543080;
 Sun, 31 Aug 2014 10:19:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.40.72 with HTTP; Sun, 31 Aug 2014 10:18:42 -0700 (PDT)
In-Reply-To: <1549011493.48644371.1409505081207.JavaMail.zimbra@redhat.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
 <CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com> <1549011493.48644371.1409505081207.JavaMail.zimbra@redhat.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 31 Aug 2014 18:18:42 +0100
Message-ID: <CAMAsSdL732rYb1ARy=anxUQRJoFBAndKD1otkW69RRDLXYDZwQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
To: Will Benton <willb@redhat.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Fantastic. As it happens, I just fixed up Mahout's tests for Java 8
and observed a lot of the same type of failure.

I'm about to submit PRs for the two issues I identified. AFAICT these
3 then cover the failures I mentioned:

https://issues.apache.org/jira/browse/SPARK-3329
https://issues.apache.org/jira/browse/SPARK-3330
https://issues.apache.org/jira/browse/SPARK-3331

I'd argue that none necessarily block a release, since they just
represent a problem with test-only code in Java 8, with the test-only
context of Jenkins and multiple profiles, and with a trivial
configuration in a style check for Python. Should be fixed but none
indicate a bug in the release.

On Sun, Aug 31, 2014 at 6:11 PM, Will Benton <willb@redhat.com> wrote:
> ----- Original Message -----
>
>> dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me
>> locally on 1.1.0-rc3. Does anyone else see that? It may be my env.
>> Although I still see the Hive failure on Debian too:
>>
>> [info] - SET commands semantics for a HiveContext *** FAILED ***
>> [info]   Expected Array("spark.sql.key.usedfortestonly=3Dtest.val.0",
>> "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=3Dtest.val.0=
test.val.0"),
>> but got
>> Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=3Dtest=
.val.0test.val.0",
>> "spark.sql.key.usedfortestonly=3Dtest.val.0") (HiveQuerySuite.scala:541)
>
> I've seen this error before.  (In particular, I've seen it on my OS X mac=
hine using Oracle JDK 8 but not on Fedora using OpenJDK.)  I've also seen s=
imilar errors in topic branches (but not on master) that seem to indicate t=
hat tests depend on sets of pairs arriving from Hive in a particular order;=
 it seems that this isn't a safe assumption.
>
> I just submitted a (trivial) PR to fix this spurious failure:  https://gi=
thub.com/apache/spark/pull/2220
>
>
> best,
> wb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 18:35:32 2014
Return-Path: <dev-return-9190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4536711559
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 18:35:32 +0000 (UTC)
Received: (qmail 22247 invoked by uid 500); 31 Aug 2014 18:35:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22168 invoked by uid 500); 31 Aug 2014 18:35:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22148 invoked by uid 99); 31 Aug 2014 18:35:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 18:35:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 18:35:27 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so2941006oia.15
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 11:35:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=uZ2BMb+PVXF49KAGwZhYm3AxPY89f6x3lkR/IHv8hyw=;
        b=UIH3rMIXvP61huThQxwOTtCSxGBh+O4kn7zW6oxVpo8bHxBYSXmzt8doNIv2VM22di
         swC3c5oJPUSehQwbf3IG2LAcIZQd+WFfzYdho3dzEwE8kAbtuKzv9bMXbC6Vwc539XwA
         xJjVcjUX2YSrSM7Yml3318a1Ntft9swG/E11WoCI/Buwe5fciiR463XYGH6WNes9O5gJ
         8g5m45mfZfhHKfANfNBoNn4l+elkVWkVj4dPfALh3cLeZfx2TC7tK8wSkD+Px4cWS5AZ
         q53psn3SbKqtQyru2s3W5gJLTIzw8WHnckYAxNBZb6QZakAAWMwhicrkXg84qMdwMTC1
         9yPw==
MIME-Version: 1.0
X-Received: by 10.60.155.205 with SMTP id vy13mr2496686oeb.65.1409510106443;
 Sun, 31 Aug 2014 11:35:06 -0700 (PDT)
Received: by 10.202.56.197 with HTTP; Sun, 31 Aug 2014 11:35:06 -0700 (PDT)
In-Reply-To: <CAMAsSdL732rYb1ARy=anxUQRJoFBAndKD1otkW69RRDLXYDZwQ@mail.gmail.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
	<CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com>
	<1549011493.48644371.1409505081207.JavaMail.zimbra@redhat.com>
	<CAMAsSdL732rYb1ARy=anxUQRJoFBAndKD1otkW69RRDLXYDZwQ@mail.gmail.com>
Date: Sun, 31 Aug 2014 11:35:06 -0700
Message-ID: <CABPQxstJVGRBJev90xEaydUrzhxtO0Jxj6rPuE6cS105a6HEQw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Will Benton <willb@redhat.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

For my part I'm +1 on this, though Sean it would be great separately
to fix the test environment.

For those who voted on rc2, this is almost identical, so feel free to
+1 unless you think there are issues with the two minor bug fixes.

On Sun, Aug 31, 2014 at 10:18 AM, Sean Owen <sowen@cloudera.com> wrote:
> Fantastic. As it happens, I just fixed up Mahout's tests for Java 8
> and observed a lot of the same type of failure.
>
> I'm about to submit PRs for the two issues I identified. AFAICT these
> 3 then cover the failures I mentioned:
>
> https://issues.apache.org/jira/browse/SPARK-3329
> https://issues.apache.org/jira/browse/SPARK-3330
> https://issues.apache.org/jira/browse/SPARK-3331
>
> I'd argue that none necessarily block a release, since they just
> represent a problem with test-only code in Java 8, with the test-only
> context of Jenkins and multiple profiles, and with a trivial
> configuration in a style check for Python. Should be fixed but none
> indicate a bug in the release.
>
> On Sun, Aug 31, 2014 at 6:11 PM, Will Benton <willb@redhat.com> wrote:
>> ----- Original Message -----
>>
>>> dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me
>>> locally on 1.1.0-rc3. Does anyone else see that? It may be my env.
>>> Although I still see the Hive failure on Debian too:
>>>
>>> [info] - SET commands semantics for a HiveContext *** FAILED ***
>>> [info]   Expected Array("spark.sql.key.usedfortestonly=3Dtest.val.0",
>>> "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=3Dtest.val.=
0test.val.0"),
>>> but got
>>> Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=3Dtes=
t.val.0test.val.0",
>>> "spark.sql.key.usedfortestonly=3Dtest.val.0") (HiveQuerySuite.scala:541=
)
>>
>> I've seen this error before.  (In particular, I've seen it on my OS X ma=
chine using Oracle JDK 8 but not on Fedora using OpenJDK.)  I've also seen =
similar errors in topic branches (but not on master) that seem to indicate =
that tests depend on sets of pairs arriving from Hive in a particular order=
; it seems that this isn't a safe assumption.
>>
>> I just submitted a (trivial) PR to fix this spurious failure:  https://g=
ithub.com/apache/spark/pull/2220
>>
>>
>> best,
>> wb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 18:58:06 2014
Return-Path: <dev-return-9191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 678AF115A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 18:58:06 +0000 (UTC)
Received: (qmail 41408 invoked by uid 500); 31 Aug 2014 18:58:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41334 invoked by uid 500); 31 Aug 2014 18:58:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41322 invoked by uid 99); 31 Aug 2014 18:58:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 18:58:05 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 18:57:40 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XOAJn-0000TW-4w
	for dev@spark.incubator.apache.org; Sun, 31 Aug 2014 11:57:39 -0700
Date: Sun, 31 Aug 2014 11:57:39 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409511459144-8157.post@n3.nabble.com>
In-Reply-To: <CAA_qdLp9FWi-ot6PzV-HWkP9bsZfbjHE48C7kjo15XLtCgKEyA@mail.gmail.com>
References: <1409064875366-8035.post@n3.nabble.com> <1409066984856-8039.post@n3.nabble.com> <CAA_qdLp9FWi-ot6PzV-HWkP9bsZfbjHE48C7kjo15XLtCgKEyA@mail.gmail.com>
Subject: Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Cheng, thank you very much for helping me to finally find out the secret
of this magic...

actually we defined this external table with
    SID STRING
    REQUEST_ID STRING
    TIMES_DQ TIMESTAMP
    TOTAL_PRICE FLOAT
    ...

using "desc table ext_fullorders" it is only shown as
[# col_name             data_type               comment             ]
...
[times_dq               string                  from deserializer   ]
[total_price            string                  from deserializer   ]
...
because, as you said, CSVSerde sets all field object inspectors to
javaStringObjectInspector
and therefore there are comments "from deserializer"

but in StorageDescriptor, are the real user defined types,
using "desc extended table ext_fullorders" we can see his
sd:StorageDescriptor
is:
FieldSchema(name:times_dq, type:timestamp, comment:null),
FieldSchema(name:total_price, type:float, comment:null)

and Spark HiveContext reads the schema info from this StorageDescriptor
https://github.com/apache/spark/blob/7e191fe29bb09a8560cd75d453c4f7f662dff406/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L316

so, in the SchemaRDD, the fields in Row were filled with strings (via
fillObject, all of values were retrieved from CSVSerDe with
javaStringObjectInspector)

but Spark considers that some of them are float or timestamp (schema info
were got from sd:StorageDescriptor)

crazy...

and sorry for update on the weekend...

a little more about how i fand this problem and why it is a trouble for us.

we use the new spark thrift server, to query normal managed hive table, it
works fine

but when we try to access the external tables with custom SerDe such as this
CSVSerDe, then we will get this ClassCastException, such as:
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Float

the reason is
https://github.com/apache/spark/blob/d94a44d7caaf3fe7559d9ad7b10872fa16cf81ca/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/server/SparkSQLOperationManager.scala#L104-L105

here Spark's thrift server try to get a float value from SparkRow, because
in the schema info (sd:StorageDescriptor) this column is float, but actually
in SparkRow, this field was filled with string value...



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/HiveContext-schemaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8035p8157.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-9192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 21:15:31 2014
Return-Path: <dev-return-9192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4F8E1190D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 21:15:31 +0000 (UTC)
Received: (qmail 5780 invoked by uid 500); 31 Aug 2014 21:15:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5720 invoked by uid 500); 31 Aug 2014 21:15:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5706 invoked by uid 99); 31 Aug 2014 21:15:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 21:15:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 21:15:24 +0000
Received: by mail-wi0-f169.google.com with SMTP id n3so1688742wiv.4
        for <dev@spark.apache.org>; Sun, 31 Aug 2014 14:15:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=pyDu9UbAmIjbJa1biWKKNsi+eS4IUYzrIG4AysomCcs=;
        b=rBdFaWfU7g1JC4hgdAoSB25tuuEZVAUrwK8W7qblcspwE589eMI2RDbAIjCtgFKkQt
         dMNQFEJvdXAihP8nOYQjr6pL3U5EYqNp4DIvgSHVwTdD/KZaxUkFp0uXw2EVVL/FyHo1
         EMjj6Ro5CepTLVRHDkssB4jk3FmqZnoMAKxtiRAh5zG7IIP1bMpoErzp70zM0eQZOixo
         LaBjEVjOy3xx3GsuXP75czUYDT02e12iqDTCTY9oBpXMgfyFyK5Ya0KllkW7bYvmmETT
         ZNjUc4Aq/g9gBfW8iPKThUzSxea6tkSIYjhn4XAhXKcnTa9upi6PJOrh4GntPwUUkScn
         uV7w==
X-Received: by 10.180.211.233 with SMTP id nf9mr16996525wic.33.1409519703373;
 Sun, 31 Aug 2014 14:15:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sun, 31 Aug 2014 14:14:23 -0700 (PDT)
In-Reply-To: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 31 Aug 2014 17:14:23 -0400
Message-ID: <CAOhmDzcoPjs3byXpuUf585VQSu4xs5rYAXcqy6zh=wBpENsnLg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c338d0393ee70501f361fe
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c338d0393ee70501f361fe
Content-Type: text/plain; charset=UTF-8

-1: I believe I've found a regression from 1.0.2. The report is captured in
SPARK-3333 <https://issues.apache.org/jira/browse/SPARK-3333>.


On Sat, Aug 30, 2014 at 6:07 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.1.0!
>
> The tag to be voted on is v1.1.0-rc3 (commit b2d0493b):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b2d0493b223c5f98a593bb6d7372706cc02bebad
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1030/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.1.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.1.0!
>
> The vote is open until Tuesday, September 02, at 23:07 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == Regressions fixed since RC1 ==
> - Build issue for SQL support:
> https://issues.apache.org/jira/browse/SPARK-3234
> - EC2 script version bump to 1.1.0.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.0.X will not block
> this release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.io.compression.codec" is now "snappy"
> --> Old behavior can be restored by switching to "lzf"
>
> 2. PySpark now performs external spilling during aggregations.
> --> Old behavior can be restored by setting "spark.shuffle.spill" to
> "false".
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c338d0393ee70501f361fe--

From dev-return-9193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Aug 31 22:38:34 2014
Return-Path: <dev-return-9193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4670111A27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 31 Aug 2014 22:38:34 +0000 (UTC)
Received: (qmail 93434 invoked by uid 500); 31 Aug 2014 22:38:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93370 invoked by uid 500); 31 Aug 2014 22:38:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93359 invoked by uid 99); 31 Aug 2014 22:38:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 22:38:33 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of teng.qiu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 31 Aug 2014 22:38:08 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <teng.qiu@gmail.com>)
	id 1XODl8-000229-Mo
	for dev@spark.incubator.apache.org; Sun, 31 Aug 2014 15:38:06 -0700
Date: Sun, 31 Aug 2014 15:38:06 -0700 (PDT)
From: chutium <teng.qiu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1409524686697-8159.post@n3.nabble.com>
In-Reply-To: <CABPQxstJVGRBJev90xEaydUrzhxtO0Jxj6rPuE6cS105a6HEQw@mail.gmail.com>
References: <CABPQxstG4PmQ1hKF2T3d_0GCZWqw15_gx9A2yh4utt9X=cPgZg@mail.gmail.com> <CAMAsSdKpzGdGTYOyzKxnTq-U=hStDPPSGhXxvrhSBAvp2XrgFQ@mail.gmail.com> <1549011493.48644371.1409505081207.JavaMail.zimbra@redhat.com> <CAMAsSdL732rYb1ARy=anxUQRJoFBAndKD1otkW69RRDLXYDZwQ@mail.gmail.com> <CABPQxstJVGRBJev90xEaydUrzhxtO0Jxj6rPuE6cS105a6HEQw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.1.0 (RC3)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

has anyone tried to build it on hadoop.version=2.0.0-mr1-cdh4.3.0 or
hadoop.version=1.0.3-mapr-3.0.3 ?

see comments in
https://issues.apache.org/jira/browse/SPARK-3124
https://github.com/apache/spark/pull/2035

i built spark snapshot on hadoop.version=1.0.3-mapr-3.0.3
and the ticket creator built on hadoop.version=2.0.0-mr1-cdh4.3.0

both hadoop version do not work

on 1.0.3-mapr3.0.3

when i try to start spark-shell

i got:

14/08/23 23:29:46 INFO SecurityManager: Changing view acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: Changing modify acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(client09, );
users with modify permissions: Set(client09, )
14/08/23 23:29:50 INFO Slf4jLogger: Slf4jLogger started
14/08/23 23:29:50 INFO Remoting: Starting remoting
14/08/23 23:29:50 ERROR ActorSystemImpl: Uncaught fatal error from thread
[spark-akka.actor.default-dispatcher-2] shutting down ActorSystem [spark]
java.lang.VerifyError: (class:
org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker
signature:
(Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;)
Wrong return type in function
        at
akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282)
        at
akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:78)
        at scala.util.Try$.apply(Try.scala:161)
...
...
...


it seems this netty jar conflict affects not only SQL component and some
test-case



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-1-0-RC3-tp8147p8159.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


