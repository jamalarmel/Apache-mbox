From dev-return-10995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  1 01:10:44 2015
Return-Path: <dev-return-10995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6457810EA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 Jan 2015 01:10:44 +0000 (UTC)
Received: (qmail 49982 invoked by uid 500); 1 Jan 2015 01:10:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49901 invoked by uid 500); 1 Jan 2015 01:10:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49887 invoked by uid 99); 1 Jan 2015 01:10:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 Jan 2015 01:10:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 Jan 2015 01:10:36 +0000
Received: by mail-pd0-f178.google.com with SMTP id r10so21537910pdi.23
        for <dev@spark.apache.org>; Wed, 31 Dec 2014 17:08:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=nCTlxs3MAO/luqNfKtbew8K2VF10YUvCQaf/vyh+UYc=;
        b=Eg2ZeHmt6xUxiUwtWZKR4zgm8f5vG9vPNCnJHnrKCtCiMc6+IsWDTAYmf9KaoZLCCG
         wwOQfof83A25SaAMyU17xSMG4dQD3gcLR6r/mziecvOsI4xL+md24JAzmLseXivyrtLg
         B+Zin9jMsXYf1DZcu60XJR3z/qqC1WytTPRs2Hht0sERjFopEpRbdmW8pHjwe4CdU2az
         3zcbo15FzxX8WEFsc5U09pbLVe9XBHuPWUJietmNGABL++MGJzIFTGy6TEHnDFTVRz8/
         WEq5Wt3q4REObOP/C4E1elZlZTvjvXKJ4XukESuXbfjnGIkOFboqV5YhjbzEo+Ya1YOW
         TIlw==
MIME-Version: 1.0
X-Received: by 10.70.94.136 with SMTP id dc8mr98231576pdb.37.1420074480684;
 Wed, 31 Dec 2014 17:08:00 -0800 (PST)
Received: by 10.70.41.80 with HTTP; Wed, 31 Dec 2014 17:07:56 -0800 (PST)
Date: Wed, 31 Dec 2014 17:07:56 -0800
Message-ID: <CAOEPXP53-uzohxozRtkneJK0TgyieJUKVJcrNH98Odc-dGS-DQ@mail.gmail.com>
Subject: Today's Jenkins failures in the Spark Maven builds
From: Josh Rosen <rosenville@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2d7d4f9d7ee050b8cda2e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d7d4f9d7ee050b8cda2e
Content-Type: text/plain; charset=UTF-8

If you've been following AMPLab Jenkins today, you'll notice that there's
been a huge number of Spark test failures in the maintenance branches and
Maven builds.

My best guess as to what's causing this is that I pushed a backport to all
maintenance branches at a moment where Jenkins was otherwise idle, causing
many builds to kick off at almost the exact same time and eventually fail
due to port contention issues in SparkSubmit tests (we didn't disable the
web UI when making external calls to ./spark-submit).  I pushed a hotfix to
address this.  When the first wave of Jenkins builds failed, the next wave
kicked off more-or-less in lockstep since there's only ever one active
build for the master builds and the problem was hit again, this time
failing a DriverSuite test (which has a port contention problem that needs
a separate fix; I'll hotfix this soon).

I believe that this flakiness is due to the lockstep synchronization of the
first wave of builds (e.g. a bunch of builds that ran DriverSuite and
SparkSubmitSuite within a minute or two of each other), and not changes in
recent patches.  If the problem persists after further web UI disabling
hotfixes, then I'll investigate the recent changes in more detail.

Thanks,
Josh

--001a11c2d7d4f9d7ee050b8cda2e--

From dev-return-10996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  1 01:51:19 2015
Return-Path: <dev-return-10996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9A3B10F6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 Jan 2015 01:51:19 +0000 (UTC)
Received: (qmail 74660 invoked by uid 500); 1 Jan 2015 01:51:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74568 invoked by uid 500); 1 Jan 2015 01:51:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74553 invoked by uid 99); 1 Jan 2015 01:51:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 Jan 2015 01:51:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 Jan 2015 01:50:51 +0000
Received: by mail-ob0-f181.google.com with SMTP id gq1so49944692obb.12
        for <dev@spark.apache.org>; Wed, 31 Dec 2014 17:49:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=XK91m7zPKGuSZAzLkA2X4qnU3uOtBaktMKPU/PhQDiI=;
        b=tdKWOvg84V+UZtguzPrdgPt2OZIvuT/MOPVCqA7O7Od2MiS4f/5v0QOv2Q9xqR/hMf
         og2iHRY1O3ESMBWnigdNBiWCZHvMw23jgz4Wp2rIuLq7INYOkJbequGqYlJR6rKPJ/s/
         Z5rOADONdOHfdLOClWmP4bjQEBY8FKqgY5Kh/blTcEhzyX50bZmujZmdWi+Eqq4xFkWy
         +VvT/8ctG9J+HaDCY782F2Gj2KVvDybUbYYv/i40Q17gFO44GY5ED7rEAlRi8VIfAL3+
         YijU5AXfTz4o5qVriobFQPsFJyq8d/Dg4pe6sYPBiqK15C27OweBun9pKG5UiUNA65dI
         D5VA==
X-Received: by 10.202.93.135 with SMTP id r129mr38348574oib.53.1420076959574;
 Wed, 31 Dec 2014 17:49:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Wed, 31 Dec 2014 17:48:59 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Wed, 31 Dec 2014 17:48:59 -0800
Message-ID: <CAJc_sy+MZ_qW45jg8FVc7EDTD-2kgUqS9T=Pp2JTboD_oocxUw@mail.gmail.com>
Subject: Spark driver main thread hanging after SQL insert
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d4a96bab1a8050b8d6ef3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d4a96bab1a8050b8d6ef3
Content-Type: text/plain; charset=UTF-8

Here's what the console shows:

15/01/01 01:12:29 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 58.0,
whose tasks have all completed, from pool
15/01/01 01:12:29 INFO scheduler.DAGScheduler: Stage 58 (runJob at
ParquetTableOperations.scala:326) finished in 5493.549 s
15/01/01 01:12:29 INFO scheduler.DAGScheduler: Job 41 finished: runJob at
ParquetTableOperations.scala:326, took 5493.747061 s

It is now 01:40:03, so the driver has been hanging for the last 28 minutes.
The web UI on the other hand shows that all tasks completed successfully,
and the output directory has been populated--although the _SUCCESS file is
missing.

It is worth noting that my code started this job as its own thread. The
actual code looks like the following snippet, modulo some simplifications.

  def save_to_parquet(allowExisting : Boolean = false) = {
    val threads = tables.map(table => {
      val thread = new Thread {
        override def run {
          table.insertInto(t.table_name)
        }
      }
      thread.start
      thread
    })
    threads.foreach(_.join)
  }

As far as I can see the insertInto call never returns. Any idea why?

Alex

--001a113d4a96bab1a8050b8d6ef3--

From dev-return-10997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 00:54:46 2015
Return-Path: <dev-return-10997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 998CFCF7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 00:54:46 +0000 (UTC)
Received: (qmail 92011 invoked by uid 500); 2 Jan 2015 00:54:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91935 invoked by uid 500); 2 Jan 2015 00:54:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91923 invoked by uid 99); 2 Jan 2015 00:54:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 00:54:43 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of manojkumarsivaraj334@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 00:54:39 +0000
Received: by mail-oi0-f45.google.com with SMTP id x69so39204714oia.4
        for <dev@spark.apache.org>; Thu, 01 Jan 2015 16:54:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=+LXtFdF9Sro5eJAeaFiRPyV/AZ3ydqO102u0TiVM8vo=;
        b=pLMGDLxHme2TJP+Lyr2Fsxz4QwwcgPqKSc3K1FNZ13yalfCc/h+af9wBtNbDq3NXFv
         X1nKr9/Up2ax5JbJa3msqB9v5N/LiMGMRDQ6NPj0+23AgfnOi5SFdNMiI9W4W5lkD2Md
         7FAFOIkC/6sVwXPdQOSEL9Xdcn/BLTJ5EZr4isgOTBdV5Av7pk2b0TKKEY/x8bjq5bJ7
         EnDhnGDFqNB7vNsZWQLhnyiHG5jGbwMnkd8J+s4KG9DA3oIUdIE6bo2ubgZsC0QWstdw
         VelnPC3krb4//4l2p/dLZRxcfRmXGlAQyoba+unl8TmabXGd9Lf2hugUTi4RK18VphN1
         jvMg==
MIME-Version: 1.0
X-Received: by 10.202.196.206 with SMTP id u197mr41050611oif.21.1420160058773;
 Thu, 01 Jan 2015 16:54:18 -0800 (PST)
Received: by 10.202.216.8 with HTTP; Thu, 1 Jan 2015 16:54:18 -0800 (PST)
Date: Fri, 2 Jan 2015 06:24:18 +0530
Message-ID: <CAFQAd-naOrmmNoUrq_nygEwnka8sLiAvzR6kena9srhcm8T61A@mail.gmail.com>
Subject: Highly interested in contributing to spark
From: Manoj Kumar <manojkumarsivaraj334@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11352e12d3d50e050ba0c7db
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11352e12d3d50e050ba0c7db
Content-Type: text/plain; charset=UTF-8

Hello,

I am Manoj (https://github.com/MechCoder), an undergraduate student highly
interested in Machine Learning. I have contributed to SymPy and
scikit-learn as part of Google Summer of Code projects and my bachelor's
thesis. I have a few quick (non-technical) questions before I dive into the
issue tracker.

Are the ones marked trivial easy to fix ones, that I could try before
attempting slightly more ambitious ones? Also I would like to know if
Apache Spark takes part in Google Summer of Code projects under the Apache
Software Foundation. It would be really great if it does!

Looking forward!

-- 
Godspeed,
Manoj Kumar,
Mech Undergrad
http://manojbits.wordpress.com

--001a11352e12d3d50e050ba0c7db--

From dev-return-10998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 04:28:08 2015
Return-Path: <dev-return-10998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59FC710256
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 04:28:08 +0000 (UTC)
Received: (qmail 8823 invoked by uid 500); 2 Jan 2015 04:28:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8741 invoked by uid 500); 2 Jan 2015 04:28:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8730 invoked by uid 99); 2 Jan 2015 04:28:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 04:28:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 04:27:59 +0000
Received: by mail-qc0-f176.google.com with SMTP id i17so12883800qcy.21
        for <dev@spark.apache.org>; Thu, 01 Jan 2015 20:25:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=adtGb1Vj5ibB6wLP+dnvOSk73xc1+L68cbIlPWqDJYM=;
        b=htTdXpy68cemgy2ULMtKENxk7URKRhczx5mx7TJxHDeXNtGzU5VVTzYfj6Zp88xHIm
         GAw6X3NolrpQtrjf7L3mo57TPv00P51UZHd6eNZD6GE/DzBWF5rvwAVjg5gq22WpHwaF
         NPBTXoQ+coLSHrmvdGBVi2DOPysEdOxPOiL2VJzselsKmuZRBkxDbrsH2pC/leO9JjkR
         uYEFgfUmBvbzgsfJNsdoXDKqLEqsl29Tbqo7G0F3oYsvB8Uzv8i+Cmzs0prcuz1C/ogw
         k6JiZn982A/L0kZCyxejvG8YO773kePorBxfGNH4yCWs+gL2ocgYxGYGRBgdTe8lxCVW
         eYHw==
X-Gm-Message-State: ALoCoQmCJawj6ALk8pkkywSZ/9+EJ+/frXHPel6I1phBWM3IkiLsfDZP8QFbNLruzWupqttaYbyV
X-Received: by 10.229.93.132 with SMTP id v4mr121504390qcm.27.1420172747154;
 Thu, 01 Jan 2015 20:25:47 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 1 Jan 2015 20:25:26 -0800 (PST)
In-Reply-To: <CAFQAd-naOrmmNoUrq_nygEwnka8sLiAvzR6kena9srhcm8T61A@mail.gmail.com>
References: <CAFQAd-naOrmmNoUrq_nygEwnka8sLiAvzR6kena9srhcm8T61A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 1 Jan 2015 20:25:26 -0800
Message-ID: <CAPh_B=b=B--JxWa+UeaBqJjbMkmSBU81K0TBP=6AjPOibgrhvw@mail.gmail.com>
Subject: Re: Highly interested in contributing to spark
To: Manoj Kumar <manojkumarsivaraj334@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11331da61d3feb050ba3bcec
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11331da61d3feb050ba3bcec
Content-Type: text/plain; charset=UTF-8

Hi Manoj,

Thanks for the email.

Yes - you should start with the starter task before attempting larger ones.
Last year I signed up as a mentor for GSoC, but no student signed up. I
don't think I'd have time to be a mentor this year, but others might.


On Thu, Jan 1, 2015 at 4:54 PM, Manoj Kumar <manojkumarsivaraj334@gmail.com>
wrote:

> Hello,
>
> I am Manoj (https://github.com/MechCoder), an undergraduate student highly
> interested in Machine Learning. I have contributed to SymPy and
> scikit-learn as part of Google Summer of Code projects and my bachelor's
> thesis. I have a few quick (non-technical) questions before I dive into the
> issue tracker.
>
> Are the ones marked trivial easy to fix ones, that I could try before
> attempting slightly more ambitious ones? Also I would like to know if
> Apache Spark takes part in Google Summer of Code projects under the Apache
> Software Foundation. It would be really great if it does!
>
> Looking forward!
>
> --
> Godspeed,
> Manoj Kumar,
> Mech Undergrad
> http://manojbits.wordpress.com
>

--001a11331da61d3feb050ba3bcec--

From dev-return-10999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 06:20:16 2015
Return-Path: <dev-return-10999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC1E810401
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 06:20:13 +0000 (UTC)
Received: (qmail 72775 invoked by uid 500); 2 Jan 2015 06:20:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72700 invoked by uid 500); 2 Jan 2015 06:20:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72680 invoked by uid 99); 2 Jan 2015 06:20:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 06:20:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 06:19:39 +0000
Received: by mail-qa0-f52.google.com with SMTP id v10so9048428qac.25
        for <dev@spark.apache.org>; Thu, 01 Jan 2015 22:19:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=7s4qSCt4VioGVCpLW2qLF9pQit6H/DxgnM5HVgw0yro=;
        b=aLiUuEAQfHSVCzuakEuTV1e8IIcYMTLsaXGxHt/aKG2/Re3Aa9hIRjz+DshLuW+zNT
         31UAU2dpXOeXqh9nFnwUVK2LZCNZnv05lgry5TYV91FQl9XHeuewq/W3MfbEuFUaYDpe
         FzFs3N7SribIeORx7TyZUJCV1OZnUH/F3Rs6q+oBcoC8GFNwJRLJ6pgQWSQZK69GnvfM
         7OywQqgHf0BOyn7j0KInfWXXL9lTIe/jMuxb2jUWFOCfbk8Q/0MWRdQXGQwj9KErbp85
         PsNdvqlhY7G11x3I8aTqGHL63GmPJR1YGpyUWJGHstD4tkr/NAY557x2BnHnaOZZ1SOp
         hn1w==
X-Received: by 10.229.248.69 with SMTP id mf5mr121889554qcb.29.1420179577789;
        Thu, 01 Jan 2015 22:19:37 -0800 (PST)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id f77sm42408400qgd.49.2015.01.01.22.19.36
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 01 Jan 2015 22:19:36 -0800 (PST)
Date: Thu, 01 Jan 2015 22:19:36 -0800 (PST)
X-Google-Original-Date: Fri, 02 Jan 2015 06:19:35 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1420179575975.75626b71@Nodemailer>
In-Reply-To: <CAPh_B=b=B--JxWa+UeaBqJjbMkmSBU81K0TBP=6AjPOibgrhvw@mail.gmail.com>
References: <CAPh_B=b=B--JxWa+UeaBqJjbMkmSBU81K0TBP=6AjPOibgrhvw@mail.gmail.com>
X-Orchestra-Oid: 76597DA0-8850-4142-AF53-6ABBEBBF2CA8
X-Orchestra-Sig: 4c8566bef7fb8776862f10ffe2a2923a653d207b
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1489145784129371627
X-Orchestra-Thrid-Sig: 00fa8a13900fb3a1385d7445bc54d56cceab8230
X-Orchestra-Account: 4a1a7f97f51e9bdc641c80f258a3a4d7ec280474
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: "Reynold Xin" <rxin@databricks.com>
Cc: "Manoj Kumar" <manojkumarsivaraj334@gmail.com>, dev@spark.apache.org
Subject: Re: Highly interested in contributing to spark
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1420179576674"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1420179576674
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

I'm sure Spark will sign up for GSoC again this year - and id be surprised =
if there was not some interest now for projects :)


If I have the time at that point in the year I'd be happy to mentor a =
project in MLlib but will have to see how my schedule is at that point!




Manoj perhaps some of the locality sensitive hashing stuff you did for =
scikit-learn could find its way to Spark or spark-projects.


=E2=80=94
Sent from Mailbox

On Fri, Jan 2, 2015 at 6:28 AM, Reynold Xin <rxin@databricks.com> wrote:

> Hi Manoj,
> Thanks for the email.
> Yes - you should start with the starter task before attempting larger =
ones.
> Last year I signed up as a mentor for GSoC, but no student signed up. I
> don't think I'd have time to be a mentor this year, but others might.
> On Thu, Jan 1, 2015 at 4:54 PM, Manoj Kumar <manojkumarsivaraj334@gmail.=
com>
> wrote:
>> Hello,
>>
>> I am Manoj (https://github.com/MechCoder), an undergraduate student =
highly
>> interested in Machine Learning. I have contributed to SymPy and
>> scikit-learn as part of Google Summer of Code projects and my =
bachelor's
>> thesis. I have a few quick (non-technical) questions before I dive into =
the
>> issue tracker.
>>
>> Are the ones marked trivial easy to fix ones, that I could try before
>> attempting slightly more ambitious ones=3F Also I would like to know if
>> Apache Spark takes part in Google Summer of Code projects under the =
Apache
>> Software Foundation. It would be really great if it does!
>>
>> Looking forward!
>>
>> --
>> Godspeed,
>> Manoj Kumar,
>> Mech Undergrad
>> http://manojbits.wordpress.com
>>
------Nodemailer-0.5.0-?=_1-1420179576674--

From dev-return-11000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 06:22:30 2015
Return-Path: <dev-return-11000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50EDC10405
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 06:22:30 +0000 (UTC)
Received: (qmail 74779 invoked by uid 500); 2 Jan 2015 06:22:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74696 invoked by uid 500); 2 Jan 2015 06:22:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74684 invoked by uid 99); 2 Jan 2015 06:22:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 06:22:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 06:22:03 +0000
Received: by mail-qg0-f41.google.com with SMTP id e89so11089355qgf.14
        for <dev@spark.apache.org>; Thu, 01 Jan 2015 22:21:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=uShEcK6CCAKUF2VgHFY1AJ5lPyp0B5sme2kseOpwcKs=;
        b=NPV6cqgfZK93wj1bzeyDgVEgKjkE2DAFpI7oXIEbVdo30k9Yy13CqC+I+gKZznFs0T
         Xekx08YGY0lEZsvWjks7hCtkP76xpiPa/tJzNYz2gqTVZbN1PXf/72HBQg+uV0Y6E7b3
         LVRxjqUePlzB59pCC4a93+eqKMynhvm0mSwF1actvRWQtHrVEaJEzvGsOmNTa/LveuHN
         Pi1ZYW1uO+QF4R1856KU1jK2l2/c2hkM9yYjCIbxq9UTZ38iQB0CawHlSBU15YlMjPQ3
         YaanfA0CJMnSW887W7/LMA8NWlL84RMsRfHoo2ebdoXomyWcmQh3RBJwoFIEeK+zDljh
         XjIg==
X-Received: by 10.229.213.131 with SMTP id gw3mr69269749qcb.23.1420179676120;
        Thu, 01 Jan 2015 22:21:16 -0800 (PST)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id n5sm42524430qat.13.2015.01.01.22.21.14
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 01 Jan 2015 22:21:15 -0800 (PST)
Date: Thu, 01 Jan 2015 22:21:15 -0800 (PST)
X-Google-Original-Date: Fri, 02 Jan 2015 06:21:14 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1420179674620.8f3b9ed6@Nodemailer>
In-Reply-To: <1420179575975.75626b71@Nodemailer>
References: <1420179575975.75626b71@Nodemailer>
X-Orchestra-Oid: 178964DB-FFA3-4A65-871E-2ACEE28C96EC
X-Orchestra-Sig: 3dced2e7d466701be634e191c5cd92022cdffadc
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1489145784129371627
X-Orchestra-Thrid-Sig: 00fa8a13900fb3a1385d7445bc54d56cceab8230
X-Orchestra-Account: 7e6562f34505fbeb62ea9eeb7529258ee4b14e00
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: "Reynold Xin" <rxin@databricks.com>
Cc: dev@spark.apache.org, "Manoj Kumar" <manojkumarsivaraj334@gmail.com>
Subject: Re: Highly interested in contributing to spark
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1420179675409"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1420179675409
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Oh actually I was confused with another project, yours was not LSH sorry!






=E2=80=94
Sent from Mailbox

On Fri, Jan 2, 2015 at 8:19 AM, Nick Pentreath <nick.pentreath@gmail.com>
wrote:

> I'm sure Spark will sign up for GSoC again this year - and id be =
surprised if there was not some interest now for projects :)
> If I have the time at that point in the year I'd be happy to mentor a =
project in MLlib but will have to see how my schedule is at that point!
> Manoj perhaps some of the locality sensitive hashing stuff you did for =
scikit-learn could find its way to Spark or spark-projects.
> =E2=80=94
> Sent from Mailbox
> On Fri, Jan 2, 2015 at 6:28 AM, Reynold Xin <rxin@databricks.com> wrote:
>> Hi Manoj,
>> Thanks for the email.
>> Yes - you should start with the starter task before attempting larger =
ones.
>> Last year I signed up as a mentor for GSoC, but no student signed up. I
>> don't think I'd have time to be a mentor this year, but others might.
>> On Thu, Jan 1, 2015 at 4:54 PM, Manoj Kumar <manojkumarsivaraj334@gmail.=
com>
>> wrote:
>>> Hello,
>>>
>>> I am Manoj (https://github.com/MechCoder), an undergraduate student =
highly
>>> interested in Machine Learning. I have contributed to SymPy and
>>> scikit-learn as part of Google Summer of Code projects and my =
bachelor's
>>> thesis. I have a few quick (non-technical) questions before I dive into=
 the
>>> issue tracker.
>>>
>>> Are the ones marked trivial easy to fix ones, that I could try before
>>> attempting slightly more ambitious ones=3F Also I would like to know =
if
>>> Apache Spark takes part in Google Summer of Code projects under the =
Apache
>>> Software Foundation. It would be really great if it does!
>>>
>>> Looking forward!
>>>
>>> --
>>> Godspeed,
>>> Manoj Kumar,
>>> Mech Undergrad
>>> http://manojbits.wordpress.com
>>>
------Nodemailer-0.5.0-?=_1-1420179675409--

From dev-return-11001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 17:15:37 2015
Return-Path: <dev-return-11001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A596C10719
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 17:15:37 +0000 (UTC)
Received: (qmail 98283 invoked by uid 500); 2 Jan 2015 17:15:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98198 invoked by uid 500); 2 Jan 2015 17:15:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98185 invoked by uid 99); 2 Jan 2015 17:15:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 17:15:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 17:15:29 +0000
Received: by mail-oi0-f52.google.com with SMTP id a3so10319824oib.11
        for <dev@spark.apache.org>; Fri, 02 Jan 2015 09:13:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BNe1SZvRrg7tQNk1oRmmqH0tia8j6frMvUMvsIpT50M=;
        b=XWMnZY2mHZTi1Z7SqjwwuAEsENuXAde5gyuiH9gKGstBlqX5JqLEC/Ew/DPGhDqHum
         wYOCZhF4aGQppikBVIkkRwGPiKPIgwhSVC+/zZGrDdcbO95iJMXOyacghumAkGVdlGow
         /wsJffMi/KoWg5y0C+o/vC6RchZ+kodw4MY4qu6X3Kxi4b+Nkrjyx0fSvciK19PKVl65
         sSkYo9k8o68K5p7gXCwrnv8PNBJiDrbnaO8oyZXa+TpdjDJdIS4pUdaOBa3reT76LFsb
         XSekfSiLKD0fT9xNNcVkUO+97hLmbdQAZee3aWWy9bhX1OShi8q9SUMKogP11gNbRiai
         +G3A==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr43202020oit.100.1420218818950;
 Fri, 02 Jan 2015 09:13:38 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Fri, 2 Jan 2015 09:13:38 -0800 (PST)
In-Reply-To: <CAJc_sy+MZ_qW45jg8FVc7EDTD-2kgUqS9T=Pp2JTboD_oocxUw@mail.gmail.com>
References: <CAJc_sy+MZ_qW45jg8FVc7EDTD-2kgUqS9T=Pp2JTboD_oocxUw@mail.gmail.com>
Date: Fri, 2 Jan 2015 12:13:38 -0500
Message-ID: <CABPQxsv_grks3-DrAHdDhAMag46qySMHodFexEcQcvFyc9je4Q@mail.gmail.com>
Subject: Re: Spark driver main thread hanging after SQL insert
From: Patrick Wendell <pwendell@gmail.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Alessandro,

Can you create a JIRA for this rather than reporting it on the dev
list? That's where we track issues like this. Thanks!.

- Patrick

On Wed, Dec 31, 2014 at 8:48 PM, Alessandro Baretta
<alexbaretta@gmail.com> wrote:
> Here's what the console shows:
>
> 15/01/01 01:12:29 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 58.0,
> whose tasks have all completed, from pool
> 15/01/01 01:12:29 INFO scheduler.DAGScheduler: Stage 58 (runJob at
> ParquetTableOperations.scala:326) finished in 5493.549 s
> 15/01/01 01:12:29 INFO scheduler.DAGScheduler: Job 41 finished: runJob at
> ParquetTableOperations.scala:326, took 5493.747061 s
>
> It is now 01:40:03, so the driver has been hanging for the last 28 minutes.
> The web UI on the other hand shows that all tasks completed successfully,
> and the output directory has been populated--although the _SUCCESS file is
> missing.
>
> It is worth noting that my code started this job as its own thread. The
> actual code looks like the following snippet, modulo some simplifications.
>
>   def save_to_parquet(allowExisting : Boolean = false) = {
>     val threads = tables.map(table => {
>       val thread = new Thread {
>         override def run {
>           table.insertInto(t.table_name)
>         }
>       }
>       thread.start
>       thread
>     })
>     threads.foreach(_.join)
>   }
>
> As far as I can see the insertInto call never returns. Any idea why?
>
> Alex

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 17:17:25 2015
Return-Path: <dev-return-11002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC09E10722
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 17:17:24 +0000 (UTC)
Received: (qmail 2897 invoked by uid 500); 2 Jan 2015 17:17:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2820 invoked by uid 500); 2 Jan 2015 17:17:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2785 invoked by uid 99); 2 Jan 2015 17:17:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 17:17:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 17:16:54 +0000
Received: by mail-ob0-f171.google.com with SMTP id uz6so53743614obc.2
        for <dev@spark.apache.org>; Fri, 02 Jan 2015 09:16:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qCb0Zbg+/ApGl5Cc8JTfe2lhQcL+u6nd1LLWtu0K4bI=;
        b=pueqhYDAMgXYp7Lt85UXrygeN9CWMZfqMjSyjeDCishLOH/l7BLoPFgsjibS4gmjGb
         0+tkAVNlDGaK4JSvrP0jedB5DGkET4C9qCkTnhhU3VjkUTu0Eb1/N1MGvL1QBBaJ2h6F
         KIqpR05fqi0+L+zjlOJpmI20YYWrw3YA1c09y43Ile4klCkqUyiadH2TiZcLCX1Wq0Vn
         TDSpX+5UxsuDLGMCB808butTJyufAbAIbPkl20FXn9YE3xqDZ7rJ3NnqP9neKwfhBpxB
         bXw7/rv+GRCZvvZZJtmVsfRfMMDbBhtn/kYSUw4aVLjtxLwDLjCwKpB8aAUFybHt5VM/
         gwvg==
MIME-Version: 1.0
X-Received: by 10.182.28.196 with SMTP id d4mr46117734obh.66.1420219013057;
 Fri, 02 Jan 2015 09:16:53 -0800 (PST)
Received: by 10.76.111.148 with HTTP; Fri, 2 Jan 2015 09:16:53 -0800 (PST)
Received: by 10.76.111.148 with HTTP; Fri, 2 Jan 2015 09:16:53 -0800 (PST)
In-Reply-To: <CABPQxsv_grks3-DrAHdDhAMag46qySMHodFexEcQcvFyc9je4Q@mail.gmail.com>
References: <CAJc_sy+MZ_qW45jg8FVc7EDTD-2kgUqS9T=Pp2JTboD_oocxUw@mail.gmail.com>
	<CABPQxsv_grks3-DrAHdDhAMag46qySMHodFexEcQcvFyc9je4Q@mail.gmail.com>
Date: Fri, 2 Jan 2015 09:16:53 -0800
Message-ID: <CAJc_syL-PVEWmwDAGh2dOzXKUDv=5REjcMmTkb38_4DEyo42=g@mail.gmail.com>
Subject: Re: Spark driver main thread hanging after SQL insert
From: Alessandro Baretta <alexbaretta@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c29d46c6d157050bae81a8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29d46c6d157050bae81a8
Content-Type: text/plain; charset=UTF-8

Patrick,

Sure. I was interested in knowing if anyone experienced a similar issue and
whether there was any known workaround. Anyway will report on JIRA.

Alex
On Jan 2, 2015 9:13 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Hi Alessandro,
>
> Can you create a JIRA for this rather than reporting it on the dev
> list? That's where we track issues like this. Thanks!.
>
> - Patrick
>
> On Wed, Dec 31, 2014 at 8:48 PM, Alessandro Baretta
> <alexbaretta@gmail.com> wrote:
> > Here's what the console shows:
> >
> > 15/01/01 01:12:29 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 58.0,
> > whose tasks have all completed, from pool
> > 15/01/01 01:12:29 INFO scheduler.DAGScheduler: Stage 58 (runJob at
> > ParquetTableOperations.scala:326) finished in 5493.549 s
> > 15/01/01 01:12:29 INFO scheduler.DAGScheduler: Job 41 finished: runJob at
> > ParquetTableOperations.scala:326, took 5493.747061 s
> >
> > It is now 01:40:03, so the driver has been hanging for the last 28
> minutes.
> > The web UI on the other hand shows that all tasks completed successfully,
> > and the output directory has been populated--although the _SUCCESS file
> is
> > missing.
> >
> > It is worth noting that my code started this job as its own thread. The
> > actual code looks like the following snippet, modulo some
> simplifications.
> >
> >   def save_to_parquet(allowExisting : Boolean = false) = {
> >     val threads = tables.map(table => {
> >       val thread = new Thread {
> >         override def run {
> >           table.insertInto(t.table_name)
> >         }
> >       }
> >       thread.start
> >       thread
> >     })
> >     threads.foreach(_.join)
> >   }
> >
> > As far as I can see the insertInto call never returns. Any idea why?
> >
> > Alex
>

--001a11c29d46c6d157050bae81a8--

From dev-return-11003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 18:13:35 2015
Return-Path: <dev-return-11003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E312108EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 18:13:35 +0000 (UTC)
Received: (qmail 98300 invoked by uid 500); 2 Jan 2015 18:13:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98113 invoked by uid 500); 2 Jan 2015 18:13:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98087 invoked by uid 99); 2 Jan 2015 18:13:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 18:13:32 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of manojkumarsivaraj334@gmail.com designates 209.85.218.65 as permitted sender)
Received: from [209.85.218.65] (HELO mail-oi0-f65.google.com) (209.85.218.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 18:13:07 +0000
Received: by mail-oi0-f65.google.com with SMTP id v63so11774764oia.0
        for <dev@spark.apache.org>; Fri, 02 Jan 2015 10:13:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=b+FlhIhkgGs1DuOebLjEax4gaGorBKYnp6wYtEzqVlo=;
        b=oNtSbrBtlR99UjohBLCA+flNcx9vf4d+JEOl1DqnaRZ2qArkPg/ZMe91rDUJQ2BvjE
         j7FvAVz6VRzKeWptHPnP/xMVX2YmqBdtFDt6dfOTG1yeKS0N0TIlHYfToyJ8ipKVpKOu
         oUEnqztqjtbgJtls2aYhymF4on3IwNHXFDqx2KgUbFUJi9g17M5okk63qyk7ZoM5T1s5
         OuGDFw2u9Ju6+iXajr2wJ46fQHZINdgJKBvG1VpQIGOL62GBdjkfvIpVLu6h8AM2uKzx
         k0OujClLWHzrCj58K3beuauh+1waXFEwWG89NrPat7tbSCZfLW0ywqgXVzb381OWm8T+
         gK9g==
MIME-Version: 1.0
X-Received: by 10.60.173.211 with SMTP id bm19mr45338411oec.66.1420222385779;
 Fri, 02 Jan 2015 10:13:05 -0800 (PST)
Received: by 10.202.216.8 with HTTP; Fri, 2 Jan 2015 10:13:05 -0800 (PST)
In-Reply-To: <1420179674620.8f3b9ed6@Nodemailer>
References: <1420179575975.75626b71@Nodemailer>
	<1420179674620.8f3b9ed6@Nodemailer>
Date: Fri, 2 Jan 2015 23:43:05 +0530
Message-ID: <CAFQAd-m8YmqCRQnHBsqCc=V+EHWmWVTFeuJTL_-RUk6WXTktLA@mail.gmail.com>
Subject: Re: Highly interested in contributing to spark
From: Manoj Kumar <manojkumarsivaraj334@gmail.com>
To: Nick Pentreath <nick.pentreath@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011764ebce7927050baf4aab
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011764ebce7927050baf4aab
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hello,

Thanks for your quick comments and encouragement.

I tried building Spark from source using build/sbt assembly

It however fails at this point

downloading
https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-li=
brary-2.10.4.jar
with SSL certificate errors. I understand that it is due to this problem (
http://apache-spark-user-list.1001560.n3.nabble.com/sbt-sbt-assembly-fails-=
with-ssl-certificate-error-td3046.html
)

but I'm not sure why it still it uses https when this PR
https://github.com/apache/spark/pull/209 has fixed it. Any help would be
greatful.






On Fri, Jan 2, 2015 at 11:51 AM, Nick Pentreath <nick.pentreath@gmail.com>
wrote:

> Oh actually I was confused with another project, yours was not LSH sorry!
>
>
>
> =E2=80=94
> Sent from Mailbox <https://www.dropbox.com/mailbox>
>
>
> On Fri, Jan 2, 2015 at 8:19 AM, Nick Pentreath <nick.pentreath@gmail.com>
> wrote:
>
>> I'm sure Spark will sign up for GSoC again this year - and id be
>> surprised if there was not some interest now for projects :)
>>
>> If I have the time at that point in the year I'd be happy to mentor a
>> project in MLlib but will have to see how my schedule is at that point!
>>
>> Manoj perhaps some of the locality sensitive hashing stuff you did for
>> scikit-learn could find its way to Spark or spark-projects.
>>
>> =E2=80=94
>> Sent from Mailbox <https://www.dropbox.com/mailbox>
>>
>>
>> On Fri, Jan 2, 2015 at 6:28 AM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Hi Manoj,
>>>
>>> Thanks for the email.
>>>
>>> Yes - you should start with the starter task before attempting larger
>>> ones.
>>> Last year I signed up as a mentor for GSoC, but no student signed up. I
>>> don't think I'd have time to be a mentor this year, but others might.
>>>
>>>
>>> On Thu, Jan 1, 2015 at 4:54 PM, Manoj Kumar <
>>> manojkumarsivaraj334@gmail.com>
>>> wrote:
>>>
>>> > Hello,
>>> >
>>> > I am Manoj (https://github.com/MechCoder), an undergraduate student
>>> highly
>>> > interested in Machine Learning. I have contributed to SymPy and
>>> > scikit-learn as part of Google Summer of Code projects and my
>>> bachelor's
>>> > thesis. I have a few quick (non-technical) questions before I dive
>>> into the
>>> > issue tracker.
>>> >
>>> > Are the ones marked trivial easy to fix ones, that I could try before
>>> > attempting slightly more ambitious ones? Also I would like to know if
>>> > Apache Spark takes part in Google Summer of Code projects under the
>>> Apache
>>> > Software Foundation. It would be really great if it does!
>>> >
>>> > Looking forward!
>>> >
>>> > --
>>> > Godspeed,
>>> > Manoj Kumar,
>>> > Mech Undergrad
>>> > http://manojbits.wordpress.com
>>> >
>>>
>>
>>
>


--=20
Godspeed,
Manoj Kumar,
Intern, Telecom ParisTech
Mech Undergrad
http://manojbits.wordpress.com

--089e011764ebce7927050baf4aab--

From dev-return-11004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  2 19:25:04 2015
Return-Path: <dev-return-11004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B60910C33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 Jan 2015 19:25:04 +0000 (UTC)
Received: (qmail 58715 invoked by uid 500); 2 Jan 2015 19:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58642 invoked by uid 500); 2 Jan 2015 19:25:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58605 invoked by uid 99); 2 Jan 2015 19:25:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 19:25:00 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail01.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 Jan 2015 19:24:35 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=10712; q=dns/txt; s=SM2048Apr2013K;
  t=1420226694; x=1420313094;
  h=from:to:cc:date:subject:message-id:references:
   in-reply-to:mime-version:content-transfer-encoding;
  bh=afnflmAmKikFnmGazd4RxWYI8hl7ctxkJ2xdan/t0oo=;
  b=p01SuQk/vgprETeOtbWOqG4kqR7qGf/bUSxqhnDwLdILWq7OPFUhBrdh
   Mfb2kd3uMGp1rs2c6bjKg+sP3MzYlwaC4Knk2mSU6Gxhx9slttihOKIPk
   X0WMYAWQ/guOfbu/k/fB/JnN07nTPUIyVRSQKNCOewt4gBIWvq7iqASZY
   9ohLfcc5HAqaA+QpeVUyMwtA4EdAW/KY8HWii92x1SEzxXg1wyaOviDPK
   8u8dn6iPHLeMhwGfyJc3qvmELX0gf2YkrhoJCxL8S5/8Q8ZWlnh1Atrxe
   TkcfYKPWDKUTjhkHxq3qzB9iwyXurHZ9YG5jo7+Dz3RkIzjuByh6d48Rh
   g==;
X-IronPort-AV: E=McAfee;i="5600,1067,7668"; a="188046512"
X-IronPort-AV: E=Sophos;i="5.07,685,1413259200"; 
   d="scan'208";a="188046512"
X-OSD_Exception: TRUE
Received: from kdcpexcasht04.cof.ds.capitalone.com ([10.37.194.14])
  by komail01.kdc.capitalone.com with ESMTP; 02 Jan 2015 14:23:32 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 KDCPEXCASHT04.cof.ds.capitalone.com ([10.37.194.14]) with mapi; Fri, 2 Jan
 2015 14:23:32 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: Manoj Kumar <manojkumarsivaraj334@gmail.com>, Nick Pentreath
	<nick.pentreath@gmail.com>
CC: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Date: Fri, 2 Jan 2015 14:23:31 -0500
Subject: Re: Highly interested in contributing to spark
Thread-Topic: Highly interested in contributing to spark
Thread-Index: AdAmwZgw4mxEoJCiRKaBXNXzzeiAFg==
Message-ID: <D0CC5A11.907F%ilya.ganelin@capitalone.com>
References: <1420179575975.75626b71@Nodemailer>
 <1420179674620.8f3b9ed6@Nodemailer>
 <CAFQAd-m8YmqCRQnHBsqCc=V+EHWmWVTFeuJTL_-RUk6WXTktLA@mail.gmail.com>
In-Reply-To: <CAFQAd-m8YmqCRQnHBsqCc=V+EHWmWVTFeuJTL_-RUk6WXTktLA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.3.140616
acceptlanguage: en-US
MIME-Version: 1.0
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I might be seeing a similar error - I=B9m trying to build behind a proxy. I
was able to build until recently, but now when I run mvn clean package, I
get the following errors:

I would love to know what=B9s going on here.

Exception in thread "pool-1-thread-1" Exception in thread "main"
java.lang.ExceptionInInitializerError
java.lang.ExceptionInInitializerError
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:186)
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:186)
	at =

java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:175)
	at =

java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:175)

	at javax.crypto.KeyAgreement.getInstance(Unknown Source)
	at com.ibm.jsse2.lb.h(lb.java:129)
	at javax.crypto.KeyAgreement.getInstance(Unknown Source)
	at com.ibm.jsse2.lb.h(lb.java:129)
	at com.ibm.jsse2.lb.a(lb.java:165)
	at com.ibm.jsse2.l$c_.a(l$c_.java:18)
	at com.ibm.jsse2.lb.a(lb.java:165)
	at com.ibm.jsse2.l$c_.a(l$c_.java:18)	at com.ibm.jsse2.l.a(l.java:172)
	at com.ibm.jsse2.m.a(m.java:38)
	at com.ibm.jsse2.l.a(l.java:172)

	at com.ibm.jsse2.m.a(m.java:38)
	at com.ibm.jsse2.m.h(m.java:21)
	at com.ibm.jsse2.m.h(m.java:21)
	at com.ibm.jsse2.qc.a(qc.java:110)
	at com.ibm.jsse2.qc.<init>(qc.java:822)
	at com.ibm.jsse2.qc.a(qc.java:110)
	at com.ibm.jsse2.qc.<init>(qc.java:822)
	at =

com.ibm.jsse2.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:1
0)	at =

com.ibm.jsse2.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:1
0)

	at =

org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSock
etFactory.createLayeredSocket(SSLConnectionSocketFactory.java:274)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.conn.HttpClientConnec
tionOperator.upgrade(HttpClientConnectionOperator.java:167)
	at =

org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSock
etFactory.createLayeredSocket(SSLConnectionSocketFactory.java:274)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.conn.HttpClientConnec
tionOperator.upgrade(HttpClientConnectionOperator.java:167)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClien
tConnectionManager.upgrade(PoolingHttpClientConnectionManager.java:329)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.establishRoute(MainClientExec.java:392)	at
org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClien
tConnectionManager.upgrade(PoolingHttpClientConnectionManager.java:329)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.establishRoute(MainClientExec.java:392)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.execute(MainClientExec.java:218)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExe
c.execute(ProtocolExec.java:194)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.execute(MainClientExec.java:218)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExe
c.execute(ProtocolExec.java:194)

	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.e
xecute(RetryExec.java:85)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExe
c.execute(RedirectExec.java:108)	at
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.e
xecute(RetryExec.java:85)

	at =

org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpCl
ient.doExecute(InternalHttpClient.java:186)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpC
lient.execute(CloseableHttpClient.java:82)
	at =

org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(Abstr
actHttpClientWagon.java:756)	at
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExe
c.execute(RedirectExec.java:108)

	at =

org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData
(AbstractHttpClientWagon.java:854)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpCl
ient.doExecute(InternalHttpClient.java:186)
	at =

org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpC
lient.execute(CloseableHttpClient.java:82)
	at =

org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(Abstr
actHttpClientWagon.java:756)	at
org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
	at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)
	at org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
	at =

org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData
(AbstractHttpClientWagon.java:854)

	at org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
	at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)	at
org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(Wag
onRepositoryConnector.java:660)

	at =

org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableEr
rorForwarder.java:67)	at
org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
	at =

org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(Wag
onRepositoryConnector.java:660)
	at =

org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableEr
rorForwarder.java:67)
	at =

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1
177)
	at =

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1
177)
	at =

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:
642)

	at java.lang.Thread.run(Thread.java:857)
Caused by: 	at =

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:
642)
	at java.lang.Thread.run(Thread.java:857)
Caused by: java.lang.SecurityException: Cannot set up certs for trusted CAs
java.lang.SecurityException: Cannot set up certs for trusted CAs
	at javax.crypto.b.<clinit>(Unknown Source)
	at javax.crypto.b.<clinit>(Unknown Source)
	... 30 more
Caused by: java.lang.SecurityException: Cannot locate policy or framework
files!
	... 30 more
	at javax.crypto.b.c(Unknown Source)
	at javax.crypto.b.access$600(Unknown Source)Caused by:
java.lang.SecurityException: Cannot locate policy or framework files!

	at javax.crypto.b$0.run(Unknown Source)
	at javax.crypto.b.c(Unknown Source)	at
java.security.AccessController.doPrivileged(AccessController.java:333)
	... 31 more
	at javax.crypto.b.access$600(Unknown Source)

	at javax.crypto.b$0.run(Unknown Source)
	at java.security.AccessController.doPrivileged(AccessController.java:333)
	... 31 more


On 1/2/15, 1:13 PM, "Manoj Kumar" <manojkumarsivaraj334@gmail.com> wrote:

>Hello,
>
>Thanks for your quick comments and encouragement.
>
>I tried building Spark from source using build/sbt assembly
>
>It however fails at this point
>
>downloading
>https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-l
>ibrary-2.10.4.jar
>with SSL certificate errors. I understand that it is due to this problem (
>http://apache-spark-user-list.1001560.n3.nabble.com/sbt-sbt-assembly-fails
>-with-ssl-certificate-error-td3046.html
>)
>
>but I'm not sure why it still it uses https when this PR
>https://github.com/apache/spark/pull/209 has fixed it. Any help would be
>greatful.
>
>
>
>
>
>
>On Fri, Jan 2, 2015 at 11:51 AM, Nick Pentreath <nick.pentreath@gmail.com>
>wrote:
>
>> Oh actually I was confused with another project, yours was not LSH
>>sorry!
>>
>>
>>
>> =8B
>> Sent from Mailbox <https://www.dropbox.com/mailbox>
>>
>>
>> On Fri, Jan 2, 2015 at 8:19 AM, Nick Pentreath
>><nick.pentreath@gmail.com>
>> wrote:
>>
>>> I'm sure Spark will sign up for GSoC again this year - and id be
>>> surprised if there was not some interest now for projects :)
>>>
>>> If I have the time at that point in the year I'd be happy to mentor a
>>> project in MLlib but will have to see how my schedule is at that point!
>>>
>>> Manoj perhaps some of the locality sensitive hashing stuff you did for
>>> scikit-learn could find its way to Spark or spark-projects.
>>>
>>> =8B
>>> Sent from Mailbox <https://www.dropbox.com/mailbox>
>>>
>>>
>>> On Fri, Jan 2, 2015 at 6:28 AM, Reynold Xin <rxin@databricks.com>
>>>wrote:
>>>
>>>> Hi Manoj,
>>>>
>>>> Thanks for the email.
>>>>
>>>> Yes - you should start with the starter task before attempting larger
>>>> ones.
>>>> Last year I signed up as a mentor for GSoC, but no student signed up.
>>>>I
>>>> don't think I'd have time to be a mentor this year, but others might.
>>>>
>>>>
>>>> On Thu, Jan 1, 2015 at 4:54 PM, Manoj Kumar <
>>>> manojkumarsivaraj334@gmail.com>
>>>> wrote:
>>>>
>>>> > Hello,
>>>> >
>>>> > I am Manoj (https://github.com/MechCoder), an undergraduate student
>>>> highly
>>>> > interested in Machine Learning. I have contributed to SymPy and
>>>> > scikit-learn as part of Google Summer of Code projects and my
>>>> bachelor's
>>>> > thesis. I have a few quick (non-technical) questions before I dive
>>>> into the
>>>> > issue tracker.
>>>> >
>>>> > Are the ones marked trivial easy to fix ones, that I could try
>>>>before
>>>> > attempting slightly more ambitious ones? Also I would like to know
>>>>if
>>>> > Apache Spark takes part in Google Summer of Code projects under the
>>>> Apache
>>>> > Software Foundation. It would be really great if it does!
>>>> >
>>>> > Looking forward!
>>>> >
>>>> > --
>>>> > Godspeed,
>>>> > Manoj Kumar,
>>>> > Mech Undergrad
>>>> > http://manojbits.wordpress.com
>>>> >
>>>>
>>>
>>>
>>
>
>
>-- =

>Godspeed,
>Manoj Kumar,
>Intern, Telecom ParisTech
>Mech Undergrad
>http://manojbits.wordpress.com

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan  3 17:35:11 2015
Return-Path: <dev-return-11005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9D391093C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 Jan 2015 17:35:11 +0000 (UTC)
Received: (qmail 22243 invoked by uid 500); 3 Jan 2015 17:35:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22156 invoked by uid 500); 3 Jan 2015 17:35:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22144 invoked by uid 99); 3 Jan 2015 17:35:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 Jan 2015 17:35:09 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.126 as permitted sender)
Received: from [98.139.213.126] (HELO nm30-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 Jan 2015 17:35:02 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=ODXFzTT1W6LWx8r2fWDSU6X/MOHcNjShzfDsNhRMO5LWeQs8np2nLtECW7M/dKGxOj4ISroHyaNUuMuHdQKpe1Gtus1IdRAaiLT6WNb7TAem3pVK3QperwALMYdjiHyc+xheqUuOHPwAGBfZA/0FwJIWnZDCNOQ/uMNOJl3OINwHtbUp8kgWvZuWtSjn0hFBufD1n424C+avSq7dIeOsyMfUQZrB+ZbNJiSauz6+7jLqxBMZ5WxNjKr8fR8lpXO7iQYQOeVhEIaSjqJDy82NOMLnQbPkY8Gxs+qDijoRRydw/pq+nuCDOqa46uWTVLdLTGqrHCWBRoLZagTSFFcqpQ==;
Received: from [98.139.215.140] by nm30.bullet.mail.bf1.yahoo.com with NNFMP; 03 Jan 2015 17:34:41 -0000
Received: from [98.139.212.221] by tm11.bullet.mail.bf1.yahoo.com with NNFMP; 03 Jan 2015 17:34:41 -0000
Received: from [127.0.0.1] by omp1030.mail.bf1.yahoo.com with NNFMP; 03 Jan 2015 17:34:41 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 234563.43374.bm@omp1030.mail.bf1.yahoo.com
X-YMail-OSG: 6GhZI5gVM1mQH1NCn.2dn4OjLFEFZVZl.FIPsBsSJ_Uv9p7nmkNwX6r_AiMqZop
 1Kl.9vuNYApHB8uNuSTp1ZZd56gWDKooZk2FZ_nYgd.g_lTt5CheHWeB1StSRk6Re_6uBwaMz3sh
 7K.tFaqBoxiL9g1wVswHBGb9gmDKgibwwZd9EZAxLlD5p.9YS2DkpS3k6eabQcKkYxPXuLW9H21S
 SV5VzYoncGHvIw0jZDd6GgGjdyZlmcPRnd8ivvQCbe4zebJ1bXibQNePsGnEjMv.ZoJIjnvv2fvy
 KyY2UHys1JnQfOVpJPEQSNwHdY2SWkRJKJP8n9gRlym2CajSfCwFemAOSlKcroHBZkg6uX_CgNvh
 06QlBurQNDhSyrSGDhrsEYf_0HVJJyALmEXTEgloOXG3U.h.2Z.XCpAEgE6TBylPK3RVCmaDsJaj
 F1pEs6kYdH72mqV5_a65ylIL5poc3bFRuwRbOlPlC9Q3u4u1c28ZF6ZQx5ZxkckEvOIreIkybYhf
 tQtNMHcASSI5Mdr8Galb79LsxBLSjlaeNTbZx1CJEBWiS8nPQw9vbure818XNFp0Ei9yb_b6YQZz
 _tJWDkQdzDmWXFXhgsGwaJsYZfrQsIXxmzQEHfcY1Kv8_2UqdC.Jx98i3vGcQz.8xtpsB98JLRhX
 uJK1_Xc0LYF3hY6V5KHUXFoAyvLXY83E-
Received: by 66.196.81.111; Sat, 03 Jan 2015 17:34:40 +0000 
Date: Sat, 3 Jan 2015 17:34:39 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1739804011.798912.1420306479826.JavaMail.yahoo@jws10666.mail.bf1.yahoo.com>
Subject: GraphX rmatGraph hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The following single line just hangs, when executed in either Spark Shell or standalone:

org.apache.spark.graphx.util.GraphGenerators.rmatGraph(sc, 4, 8)

It just outputs "0 edges" and then locks up.
The only other information I've found via Google is:

http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408617621830-12570.post@n3.nabble.com%3E

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan  3 21:32:05 2015
Return-Path: <dev-return-11006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E8FD1727E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 Jan 2015 21:32:05 +0000 (UTC)
Received: (qmail 94261 invoked by uid 500); 3 Jan 2015 21:32:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94172 invoked by uid 500); 3 Jan 2015 21:32:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 73936 invoked by uid 99); 3 Jan 2015 21:04:51 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of xhudik@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Sat, 3 Jan 2015 14:04:23 -0700 (MST)
From: xhudik <xhudik@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420319063364-9996.post@n3.nabble.com>
In-Reply-To: <1739804011.798912.1420306479826.JavaMail.yahoo@jws10666.mail.bf1.yahoo.com>
References: <1739804011.798912.1420306479826.JavaMail.yahoo@jws10666.mail.bf1.yahoo.com>
Subject: Re: GraphX rmatGraph hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Michael,

yes, I can confirm the behavior.
It get stuck (loop?) and eat all resources, command top gives:
14013 ll     20   0 2998136 489992  19804 S 100.2 12.10  13:29.39 java

You might create an issue/bug in jira
(https://issues.apache.org/jira/browse/SPARK)


best, tomas



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/GraphX-rmatGraph-hangs-tp9995p9996.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan  4 12:57:47 2015
Return-Path: <dev-return-11007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21EF517FD9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 Jan 2015 12:57:47 +0000 (UTC)
Received: (qmail 71911 invoked by uid 500); 4 Jan 2015 12:57:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71834 invoked by uid 500); 4 Jan 2015 12:57:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71822 invoked by uid 99); 4 Jan 2015 12:57:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 Jan 2015 12:57:45 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.162 as permitted sender)
Received: from [98.139.213.162] (HELO nm19-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.162)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 Jan 2015 12:57:39 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=UsYK//YG7EhTyrXBmSQQSimN5PxQRN9bzpnXb5EyMdLMgwix8Y5tPpFrYsdt2xIJTtiOeEETs8MmXbxbIHP82rNj2fcGkCJzkSwaLTZBd9V5z1fL+AHa7PvSLeiQtNmVTnpw4bpdmEYhzuw7Zn13vNxAbANRQ0mc538SFII66i8oAUyIAbULojIF8vb4jIKRq3AvoYucpx2CeX11y5FmGwgWK8LKZRiZuSbVnRPU3PiMB2TmqI93RCW0AWpvuOjoi5+OPQIUbGcyVx6ZrbBp49Z4OJ1fJr6nhA9ZO40Fk78gFvUewSjQjeJUI9r82pPchjYSBieEgH0vmolPS/JUNA==;
Received: from [66.196.81.171] by nm19.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jan 2015 12:56:14 -0000
Received: from [98.139.212.215] by tm17.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jan 2015 12:56:14 -0000
Received: from [127.0.0.1] by omp1024.mail.bf1.yahoo.com with NNFMP; 04 Jan 2015 12:56:14 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 894983.33530.bm@omp1024.mail.bf1.yahoo.com
X-YMail-OSG: RLWXcPYVM1ltvzjfWJl9Yx0OFjA81TdDR0Oz7XmqLmT7nHeZ_5fipJZFa22uxJT
 Tk4Pv9ixeGwFZ5XOTV2Sb0Fv100fV.rp5spUCMl5zcKpYZvBjJl52QkZ4HZAToZgfX56ZjSoPmPH
 dIIlmgPSvdnQj4c93BlBjW47cBtOmbetZW8zrSHZRpz_9e0MYZaB3u8ze8_hDc.WlHljdjS2_JF3
 bKnghGZDVrTnOjudNeQiROGv9Q1J7bYaLNL75ccYHBxwoX9HTkbTOx1yP7R8ZQawJzOdQxH_cJ_q
 vHbnUSRT_yZOwlo3qrQp14gRMOpr4EKhMACb40LMuR.kbnMqqQ2Vfhy3HArq1F_lSZU56wwTwtJQ
 Oh4oabjpNdjK7kXeUgTJtKs.x3m5u8Nxy6zD_FZNouNPmB771H9mevbldQDqQgN8klBYctPtAJ0b
 1xrNqs0hKBiFQ4Gl5DppglqTUO6WS9CehOAHgHMWePOF3aA4NUMMmRnVHGHw9Q2fK_sKw3AvVh7L
 aJsEHtknv2tY88xnuNpJHNgDheYI.BfZRQ2e1BWUQNNrbrwvrUkyxR3yC2Q8YzcQxDBQekFKeSgA
 PHpsvpdWi59wb2QSG7J1cALkEl15wp684nyFON60wxwGIjFt9dIQLqMUhHKeAafIGOXBJaGYg1Sm
 J7qd916.Fs1DstQqMvYyZjgzporJiMKcXoTbKU7GL63TxFiVaDU1tYmV4rSsRQEPiimStiJIO1Mb
 rkNAcVkHM.0eqa6oOr2OyWnt39j18Yy86
Received: by 66.196.80.121; Sun, 04 Jan 2015 12:56:14 +0000 
Date: Sun, 4 Jan 2015 12:56:13 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: xhudik <xhudik@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <883128502.4245146.1420376173093.JavaMail.yahoo@jws106122.mail.bf1.yahoo.com>
In-Reply-To: <1420319063364-9996.post@n3.nabble.com>
References: <1420319063364-9996.post@n3.nabble.com>
Subject: Re: GraphX rmatGraph hangs
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you. I created 
https://issues.apache.org/jira/browse/SPARK-5064


----- Original Message -----
From: xhudik <xhudik@gmail.com>
To: dev@spark.apache.org
Cc: 
Sent: Saturday, January 3, 2015 2:04 PM
Subject: Re: GraphX rmatGraph hangs

Hi Michael,

yes, I can confirm the behavior.
It get stuck (loop?) and eat all resources, command top gives:
14013 ll     20   0 2998136 489992  19804 S 100.2 12.10  13:29.39 java

You might create an issue/bug in jira
(https://issues.apache.org/jira/browse/SPARK)


best, tomas



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/GraphX-rmatGraph-hangs-tp9995p9996.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 02:28:34 2015
Return-Path: <dev-return-11008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6567110550
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 02:28:34 +0000 (UTC)
Received: (qmail 11335 invoked by uid 500); 5 Jan 2015 02:28:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11264 invoked by uid 500); 5 Jan 2015 02:28:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8638 invoked by uid 99); 5 Jan 2015 02:28:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 02:28:31 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alcaid1801@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 02:28:05 +0000
Received: by mail-wi0-f176.google.com with SMTP id ex7so2391460wid.15;
        Sun, 04 Jan 2015 18:28:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=pEPxRnh+JCTttiDKNDST4seFGqB/hS49A8f0BQnjHnc=;
        b=EkwGSNmp9DLYRUFsUx5K0hOrRYOmRpO4l2f8pH2080pZpWDfzMnkn7EGso7YtbBeEU
         Rs1qsH9rLmiBcwnpK+iOZ95hSEdlLLLvz3pgkR02z4VInXY3k3VSzCQcIdkInhiTifQ6
         yqCvj7QxB8nS4U0F1Sz5YRhkLycKOsXZgayqk2irCa9H9hr8evRJihjCpQ9VSS/dL6SC
         sT/bQ5rR45tQf41p3gNNvU5G4RkEqLsz4DAvSPEmSSNjXcWQux1wwxLxREtMp5xYDwF3
         x9lcY1lv5ylsud/6ydyYOM39hcDDo20kdez9qH64Tp6d6zvDsIi2o5toH5am+1CZfuXO
         bpDg==
MIME-Version: 1.0
X-Received: by 10.180.37.142 with SMTP id y14mr20948634wij.47.1420424884881;
 Sun, 04 Jan 2015 18:28:04 -0800 (PST)
Received: by 10.217.123.69 with HTTP; Sun, 4 Jan 2015 18:28:04 -0800 (PST)
Date: Mon, 5 Jan 2015 10:28:04 +0800
Message-ID: <CACdk1M6NwYQ-EHfRZPq9Dm7yij4i==gAAneH0P2Fsm06FDLn-g@mail.gmail.com>
Subject: Using graphx to calculate average distance of a big graph
From: James <alcaid1801@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f647387b18e1a050bde700b
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f647387b18e1a050bde700b
Content-Type: text/plain; charset=UTF-8

Recently we want to use spark to calculate the average shortest path
distance between each reachable pair of nodes in a very big graph.

Is there any one ever try this? We hope to discuss about the problem.

--e89a8f647387b18e1a050bde700b--

From dev-return-11009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 10:26:32 2015
Return-Path: <dev-return-11009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5166810FF5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 10:26:32 +0000 (UTC)
Received: (qmail 30564 invoked by uid 500); 5 Jan 2015 10:26:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30485 invoked by uid 500); 5 Jan 2015 10:26:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30465 invoked by uid 99); 5 Jan 2015 10:26:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 10:26:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 10:26:04 +0000
Received: by mail-wi0-f171.google.com with SMTP id bs8so2941669wib.10
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 02:25:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=w995/eepelFdFwZuZ3PZ3Or2B5/+AAA7i+u14awL/lc=;
        b=1IqBqAKQVFABJ+puZqB8Ode46GfoP6TqhbORcRdjFAdVqgf6LU4AEnJzgczr36k9ke
         fVsosBLGE9oo9rUtOnMpvWmfhvDCHYpMru60GBJLMaJognSAMV11KBM7pkj/myDv1ugi
         iOuM4vchhsAbbIVMV92x2PzAzu5AY8yHN85mrxo1Mh5EPRLErgz9QZEBiPEERXIe/goE
         UnW5zw0nZjeDNo5DKyTV5KVDu8JMgK7nAscICx5hJgPhAeQDfHoHwhK8lMHU//KysL8C
         7BeLbIdImksnXzxfolS31q9EIFRsXbTDK9mJf+wPDMqDBAbkTvrxIrsHZL75MoShX8di
         2yMw==
X-Received: by 10.194.85.161 with SMTP id i1mr177236563wjz.35.1420453518474;
 Mon, 05 Jan 2015 02:25:18 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.25.5 with HTTP; Mon, 5 Jan 2015 02:24:48 -0800 (PST)
In-Reply-To: <1419949963646-9968.post@n3.nabble.com>
References: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
 <1419949963646-9968.post@n3.nabble.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Mon, 5 Jan 2015 11:24:48 +0100
Message-ID: <CAMc-71kQw3LDGG0kkU_PNdpHwVck6k8-q+1+6y9NY2Cq6FFOTQ@mail.gmail.com>
Subject: Re: Registering custom metrics
To: eshioji <eshioji@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e010d852e63829b050be51bd1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e010d852e63829b050be51bd1
Content-Type: text/plain; charset=UTF-8

Hi,

Yes, I managed to create a register custom metrics by creating an
 implementation  of org.apache.spark.metrics.source.Source and registering
it to the metrics subsystem.
Source is [Spark] private, so you need to create it under a org.apache.spark
package. In my case, I'm dealing with Spark Streaming metrics, and I
created my CustomStreamingSource under org.apache.spark.streaming as I also
needed access to some [Streaming] private components.

Then, you register your new metric Source on the Spark's metric system,
like so:

SparkEnv.get.metricsSystem.registerSource(customStreamingSource)

And it will get reported to the metrics Sync active on your system. By
default, you can access them through the metric endpoint:
http://<driver-host>:<ui-port>/metrics/json

I hope this helps.

-kr, Gerard.






On Tue, Dec 30, 2014 at 3:32 PM, eshioji <eshioji@gmail.com> wrote:

> Hi,
>
> Did you find a way to do this / working on this?
> Am trying to find a way to do this as well, but haven't been able to find a
> way.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-custom-metrics-tp9030p9968.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e010d852e63829b050be51bd1--

From dev-return-11010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 13:26:02 2015
Return-Path: <dev-return-11010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 07253C655
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 13:26:02 +0000 (UTC)
Received: (qmail 66464 invoked by uid 500); 5 Jan 2015 13:26:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66402 invoked by uid 500); 5 Jan 2015 13:26:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66367 invoked by uid 99); 5 Jan 2015 13:26:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 13:26:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 13:25:34 +0000
Received: by mail-we0-f182.google.com with SMTP id w62so7841309wes.27
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 05:23:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=e6oVehzJQeZqclzbrYCGeAB4R5ihhd7lAUHSCWbsV38=;
        b=UbQUy+GxOfeF/jKAOQ0fGap614LcxMNq3Bklnz5rZwuUASe8O3j1E6mewTqhInSeyZ
         GGwThQl912CqVGh+59SfWY3e1PFel5n2P4skyldQEk980Ky9U16FbXTS2aI2YB1eP9fx
         LdRtvTVt23KyGolQpXlvcpSAtqFy/rHibRpbXn/JcSOJl1iNnkWNRRSMIgZLg/F+qrQl
         6zuSax/jVWNJIVjakkuzC0himxV7AyyntSrJJpUR2AADZINNr/CGAxhOQltcmm5fHOiP
         AdrakPSAmhnOETgNU07C5Jo59bZzlxc2ddLg3UXxfUo6jTyKVu3VDSgaC1IYkFzFjTFm
         0eVA==
X-Received: by 10.194.71.203 with SMTP id x11mr181005650wju.131.1420464198263;
 Mon, 05 Jan 2015 05:23:18 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.25.5 with HTTP; Mon, 5 Jan 2015 05:22:47 -0800 (PST)
In-Reply-To: <CAFx0iW_rO43PkS6n1xKzu45zdDhvB2HDzTyBFYVZVBP-py1+YA@mail.gmail.com>
References: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
 <59D086C1-3E76-4754-9770-64273DAF2E54@gmail.com> <CAMc-71nGbEkoCV9ZWGusxCe-XDpyi=i6poDzTrN6BgtZxb2wmA@mail.gmail.com>
 <CAFx0iW_rO43PkS6n1xKzu45zdDhvB2HDzTyBFYVZVBP-py1+YA@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Mon, 5 Jan 2015 14:22:47 +0100
Message-ID: <CAMc-71mMqjt_s_uy8iSFf0s58a5TxBWnCN_VOHkY-eByzGhoLQ@mail.gmail.com>
Subject: Re: Tuning Spark Streaming jobs
To: Timothy Chen <tnachen@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Tathagata Das <tathagata.das1565@gmail.com>
Content-Type: multipart/alternative; boundary=047d7bfd0bd2f42c27050be797f8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0bd2f42c27050be797f8
Content-Type: text/plain; charset=UTF-8

Hi Tim,

First of all, let m wish you a happy and fulfilling New Year.
Sorry for the delay in my response. I was out for the xmas break.

I've added my thoughts to the ticket from the perspective of a streaming
Job.
@TD: What do you think?

-kr, Gerard.

On Tue, Dec 23, 2014 at 8:02 PM, Timothy Chen <tnachen@gmail.com> wrote:

> Hi Gerard,
>
> SPARK-4286 is the ticket I am working on, which besides supporting shuffle
> service it also supports the executor scaling callbacks (kill/request
> total) for coarse grain mode.
>
> I created SPARK-4940 to discuss more about the distribution problem, and
> let's bring our discussions there.
>
> Tim
>
>
>
> On Dec 22, 2014, at 11:16 AM, Gerard Maas <gerard.maas@gmail.com> wrote:
>
> Hi Tim,
>
> That would be awesome. We have seen some really disparate Mesos
> allocations for our Spark Streaming jobs. (like (7,4,1) over 3 executors
> for 4 kafka consumer instead of the ideal (3,3,3,3))
> For network dependent consumers, achieving an even deployment would
>  provide a reliable and reproducible streaming job execution from the
> performance point of view.
> We're deploying in coarse grain mode. Not sure Spark Streaming would work
> well in fine-grained given the added latency to acquire a worker.
>
> You mention that you're changing the Mesos scheduler. Is there a Jira
> where this job is taking place?
>
> -kr, Gerard.
>
>
> On Mon, Dec 22, 2014 at 6:01 PM, Timothy Chen <tnachen@gmail.com> wrote:
>
>> Hi Gerard,
>>
>> Really nice guide!
>>
>> I'm particularly interested in the Mesos scheduling side to more evenly
>> distribute cores across cluster.
>>
>> I wonder if you are using coarse grain mode or fine grain mode?
>>
>> I'm making changes to the spark mesos scheduler and I think we can
>> propose a best way to achieve what you mentioned.
>>
>> Tim
>>
>> Sent from my iPhone
>>
>> > On Dec 22, 2014, at 8:33 AM, Gerard Maas <gerard.maas@gmail.com> wrote:
>> >
>> > Hi,
>> >
>> > After facing issues with the performance of some of our Spark Streaming
>> > jobs, we invested quite some effort figuring out the factors that affect
>> > the performance characteristics of a Streaming job. We  defined an
>> > empirical model that helps us reason about Streaming jobs and applied
>> it to
>> > tune the jobs in order to maximize throughput.
>> >
>> > We have summarized our findings in a blog post with the intention of
>> > collecting feedback and hoping that it is useful to other Spark
>> Streaming
>> > users facing similar issues.
>> >
>> > http://www.virdata.com/tuning-spark/
>> >
>> > Your feedback is welcome.
>> >
>> > With kind regards,
>> >
>> > Gerard.
>> > Data Processing Team Lead
>> > Virdata.com
>> > @maasg
>>
>
>

--047d7bfd0bd2f42c27050be797f8--

From dev-return-11011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 14:43:57 2015
Return-Path: <dev-return-11011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0425C8DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 14:43:56 +0000 (UTC)
Received: (qmail 13896 invoked by uid 500); 5 Jan 2015 14:43:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13827 invoked by uid 500); 5 Jan 2015 14:43:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13810 invoked by uid 99); 5 Jan 2015 14:43:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 14:43:52 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of gen.tang86@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 14:43:48 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 74201F8D1AC
	for <dev@spark.apache.org>; Mon,  5 Jan 2015 06:42:57 -0800 (PST)
Date: Mon, 5 Jan 2015 07:42:56 -0700 (MST)
From: tgbaggio <gen.tang86@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420468976814-10001.post@n3.nabble.com>
Subject: python converter in HBaseConverter.scala(spark/examples)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, 

In  HBaseConverter.scala
<https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala> 
, the python converter HBaseResultToStringConverter return only the value of
first column in the result. In my opinion, it limits the utility of this
converter, because it returns only one value per row and moreover it loses
the other information of record, such as column:cell, timestamp. 

Therefore, I would like to propose some modifications about
HBaseResultToStringConverter which will be able to return all records in the
hbase with more complete information: I have already written some code in
pythonConverters.scala
<https://github.com/GenTang/spark_hbase/blob/master/src/main/scala/examples/pythonConverters.scala>  
and it works

Is it OK to modify the code in HBaseConverters.scala, please?
Thanks a lot in advance.

Cheers
Gen




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 14:57:16 2015
Return-Path: <dev-return-11012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 537EBC96E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 14:57:16 +0000 (UTC)
Received: (qmail 41564 invoked by uid 500); 5 Jan 2015 14:57:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41482 invoked by uid 500); 5 Jan 2015 14:57:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41458 invoked by uid 99); 5 Jan 2015 14:57:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 14:57:14 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of eshioji@gmail.com designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 14:56:49 +0000
Received: by mail-la0-f53.google.com with SMTP id gm9so17981314lab.40
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 06:56:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=o4oAck7V4+VPOwXh2H8tL7jLZcrXaIiM65wHk3KHruA=;
        b=YmAThNzw1wsZMItBRNy8oZka9tMKrElbgMpq7aRQtzUxorWLRZqyvzfL4cUy/b/0W4
         hhPQDxdIQTJvkkKcGEQ9io9N3Fja2yWg+6O7sC7BMTGnTZWWIxcPpqwqyfUi7KcTjyJ+
         Xzhbnv3ePlbJM2oenZ9tl5HcNfAKZiJY/PP0pOXRpbSDhnH8cej1/JVMdc7OtFqwbZAs
         g0xXS7qHom1AJI4WA6GvHQxZewaoWNzj/gwv45VCZVdMVQcmGumlpelnr92va+UP/JLt
         E3P6sef0WYpeb+AaiRKano1X64qReb+JjZTH5eWnLXsp2gbHi6yQBIf3wgYT2i1vmQoD
         PEdQ==
MIME-Version: 1.0
X-Received: by 10.152.27.228 with SMTP id w4mr92776265lag.75.1420469762603;
 Mon, 05 Jan 2015 06:56:02 -0800 (PST)
Received: by 10.112.176.36 with HTTP; Mon, 5 Jan 2015 06:56:02 -0800 (PST)
In-Reply-To: <CAMc-71kQw3LDGG0kkU_PNdpHwVck6k8-q+1+6y9NY2Cq6FFOTQ@mail.gmail.com>
References: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
	<1419949963646-9968.post@n3.nabble.com>
	<CAMc-71kQw3LDGG0kkU_PNdpHwVck6k8-q+1+6y9NY2Cq6FFOTQ@mail.gmail.com>
Date: Mon, 5 Jan 2015 14:56:02 +0000
Message-ID: <CAE50=dqZdtJviURKqpZXBQqxz1w0XRYZGYK-996X5Ru3c=Kctg@mail.gmail.com>
Subject: Re: Registering custom metrics
From: Enno Shioji <eshioji@gmail.com>
To: Gerard Maas <gerard.maas@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160a3b69d406b050be8e3b9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a3b69d406b050be8e3b9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Gerard,

Thanks for the answer! I had a good look at it, but I couldn't figure out
whether one can use that to emit metrics from your application code.

Suppose I wanted to monitor the rate of bytes I produce, like so:

    stream
        .map { input =3D>
          val bytes =3D produce(input)
          // metricRegistry.meter("some.metrics").mark(bytes.length)
          bytes
        }
        .saveAsTextFile("text")

Is there a way to achieve this with the MetricSystem?


=E1=90=A7

On Mon, Jan 5, 2015 at 10:24 AM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Hi,
>
> Yes, I managed to create a register custom metrics by creating an
>  implementation  of org.apache.spark.metrics.source.Source and
> registering it to the metrics subsystem.
> Source is [Spark] private, so you need to create it under a org.apache.sp=
ark
> package. In my case, I'm dealing with Spark Streaming metrics, and I
> created my CustomStreamingSource under org.apache.spark.streaming as I
> also needed access to some [Streaming] private components.
>
> Then, you register your new metric Source on the Spark's metric system,
> like so:
>
> SparkEnv.get.metricsSystem.registerSource(customStreamingSource)
>
> And it will get reported to the metrics Sync active on your system. By
> default, you can access them through the metric endpoint:
> http://<driver-host>:<ui-port>/metrics/json
>
> I hope this helps.
>
> -kr, Gerard.
>
>
>
>
>
>
> On Tue, Dec 30, 2014 at 3:32 PM, eshioji <eshioji@gmail.com> wrote:
>
>> Hi,
>>
>> Did you find a way to do this / working on this?
>> Am trying to find a way to do this as well, but haven't been able to fin=
d
>> a
>> way.
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-cu=
stom-metrics-tp9030p9968.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--089e0160a3b69d406b050be8e3b9--

From dev-return-11013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 15:37:05 2015
Return-Path: <dev-return-11013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D247CB2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 15:37:05 +0000 (UTC)
Received: (qmail 30450 invoked by uid 500); 5 Jan 2015 15:37:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30376 invoked by uid 500); 5 Jan 2015 15:37:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30360 invoked by uid 99); 5 Jan 2015 15:37:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 15:37:02 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.49 as permitted sender)
Received: from [209.85.213.49] (HELO mail-yh0-f49.google.com) (209.85.213.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 15:36:58 +0000
Received: by mail-yh0-f49.google.com with SMTP id f10so10612813yha.22
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 07:36:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DS/0O4S/SDViwT52VjD1K323FonvwnFF5ScL0omxGEc=;
        b=Gdc8D8YDDEIpNPwARAgwK/6wE1ZiMLgpZBjHIqPk4Bcs1dy+GF5s6ZkA5sXGh+iu5+
         rV4BBNIqo5k54LdTN+D1nYLh2IwbNRdxxZVH3++RlvqUxZsTgWsaoHecWg56QEGMt578
         jhN2JIHccDQF5MkiUddXKc8MzyJX3/Z1a4YGGSFO2gRGr7lKOI2lR8ToL7tSfNe8j5wP
         Zh4On66N4U82Uze0owyd85yT4Sl5eXdSCew4PK033yPVejm6TY7+4BWpu5+zqLF7+Y0j
         CgK/KrMWSxxra8nrx9FqDuGpNoQx+lrjOZzzwG4Mxq25qX57Dkmj7CsizIL+g4WO6w8a
         1TUQ==
MIME-Version: 1.0
X-Received: by 10.170.141.6 with SMTP id i6mr66950703ykc.122.1420472197657;
 Mon, 05 Jan 2015 07:36:37 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Mon, 5 Jan 2015 07:36:37 -0800 (PST)
In-Reply-To: <1420468976814-10001.post@n3.nabble.com>
References: <1420468976814-10001.post@n3.nabble.com>
Date: Mon, 5 Jan 2015 07:36:37 -0800
Message-ID: <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
Subject: Re: python converter in HBaseConverter.scala(spark/examples)
From: Ted Yu <yuzhihong@gmail.com>
To: tgbaggio <gen.tang86@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139f3cac13aec050be97440
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139f3cac13aec050be97440
Content-Type: text/plain; charset=UTF-8

In my opinion this would be useful - there was another thread where returning
only the value of first column in the result was mentioned.

Please create a SPARK JIRA and a pull request.

Cheers

On Mon, Jan 5, 2015 at 6:42 AM, tgbaggio <gen.tang86@gmail.com> wrote:

> Hi,
>
> In  HBaseConverter.scala
> <
> https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala
> >
> , the python converter HBaseResultToStringConverter return only the value
> of
> first column in the result. In my opinion, it limits the utility of this
> converter, because it returns only one value per row and moreover it loses
> the other information of record, such as column:cell, timestamp.
>
> Therefore, I would like to propose some modifications about
> HBaseResultToStringConverter which will be able to return all records in
> the
> hbase with more complete information: I have already written some code in
> pythonConverters.scala
> <
> https://github.com/GenTang/spark_hbase/blob/master/src/main/scala/examples/pythonConverters.scala
> >
> and it works
>
> Is it OK to modify the code in HBaseConverters.scala, please?
> Thanks a lot in advance.
>
> Cheers
> Gen
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1139f3cac13aec050be97440--

From dev-return-11014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 15:55:51 2015
Return-Path: <dev-return-11014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CCD54CBDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 15:55:51 +0000 (UTC)
Received: (qmail 74245 invoked by uid 500); 5 Jan 2015 15:55:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74164 invoked by uid 500); 5 Jan 2015 15:55:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74152 invoked by uid 99); 5 Jan 2015 15:55:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 15:55:50 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 15:55:46 +0000
Received: by mail-qc0-f180.google.com with SMTP id i8so15218415qcq.25
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 07:54:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=PacaCojOY9cBFkY9y7lFfkAo7plImdEGV7xBdq0RRiE=;
        b=IJcdSTjRKdGMejxookG5vP1onoVYzjckjzTzATSQP6m7p4gq0dHHZl7dFU60GaAwQY
         DmGtxSyCyqbmcfcbeMHCLrBPy+U8cCcaIuKXm34Ep8eLDqDgCb5PWtJQ7J+FvKJO1Vjm
         0NJRYRu0fTrRdjRMnhJ+0uwDDRBkZQkBDTtua+ffxVd89dN3b6i5epZiQF886zfrTUwC
         4upB5RgpL77sqLyCGyAp++T3DeOohAmiqejob8jFE5QuBcpdgLkjQabvellx3vD57s3K
         qpRff2rqfyYi16uYkNoxCwwXduDcghoHWcwYsEsbZ0faSFXG5TletrcDE9F856NuaFxY
         H47Q==
X-Received: by 10.229.181.5 with SMTP id bw5mr40495803qcb.15.1420473280544;
        Mon, 05 Jan 2015 07:54:40 -0800 (PST)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id h16sm3811909qge.3.2015.01.05.07.54.39
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 05 Jan 2015 07:54:39 -0800 (PST)
Date: Mon, 05 Jan 2015 07:54:39 -0800 (PST)
X-Google-Original-Date: Mon, 05 Jan 2015 15:54:38 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1420473278868.1043109b@Nodemailer>
In-Reply-To: <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
References: <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
X-Orchestra-Oid: AA55BA1A-946B-4A14-8D13-B986A22A267F
X-Orchestra-Sig: 70e9a48a0c707e4187967b3534b09e1af58b0fec
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1489469742583243020
X-Orchestra-Thrid-Sig: f15c068ded4dd0e24eeb2580ae31ea8325073ddb
X-Orchestra-Account: 71203659a86b12fe4dd2ca6642b88db3996da251
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: "Ted Yu" <yuzhihong@gmail.com>
Cc: "tgbaggio" <gen.tang86@gmail.com>, dev@spark.apache.org
Subject: Re: python converter in HBaseConverter.scala(spark/examples)
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1420473279602"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1420473279602
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hey=C2=A0


These converters are actually just intended to be examples of how to set up=
 a custom converter for a specific input format. The converter interface is=
 there to provide flexibility where needed, although with the new SparkSQL =
data store interface the intention is that most common use cases can be =
handled using that approach rather than custom converters.




The intention is not to have specific converters living in Spark core, =
which is why these are in the examples project.




Having said that, if you wish to expand the example converter for others =
reference do feel free to submit a PR.




Ideally though, I would think that various custom converters would be part =
of external projects that can be listed with=C2=A0http://spark-packages.=
org/=C2=A0I see your project is already listed there.


=E2=80=94
Sent from Mailbox

On Mon, Jan 5, 2015 at 5:37 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> In my opinion this would be useful - there was another thread where =
returning
> only the value of first column in the result was mentioned.
> Please create a SPARK JIRA and a pull request.
> Cheers
> On Mon, Jan 5, 2015 at 6:42 AM, tgbaggio <gen.tang86@gmail.com> wrote:
>> Hi,
>>
>> In  HBaseConverter.scala
>> <
>> https://github.com/apache/spark/blob/master/examples/src/main/scala/org/=
apache/spark/examples/pythonconverters/HBaseConverters.scala
>> >
>> , the python converter HBaseResultToStringConverter return only the =
value
>> of
>> first column in the result. In my opinion, it limits the utility of =
this
>> converter, because it returns only one value per row and moreover it =
loses
>> the other information of record, such as column:cell, timestamp.
>>
>> Therefore, I would like to propose some modifications about
>> HBaseResultToStringConverter which will be able to return all records =
in
>> the
>> hbase with more complete information: I have already written some code =
in
>> pythonConverters.scala
>> <
>> https://github.com/GenTang/spark=5Fhbase/blob/master/src/main/scala/exam=
ples/pythonConverters.scala
>> >
>> and it works
>>
>> Is it OK to modify the code in HBaseConverters.scala, please=3F
>> Thanks a lot in advance.
>>
>> Cheers
>> Gen
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.=
com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
------Nodemailer-0.5.0-?=_1-1420473279602--

From dev-return-11015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 16:23:25 2015
Return-Path: <dev-return-11015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74B55CD92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 16:23:25 +0000 (UTC)
Received: (qmail 49306 invoked by uid 500); 5 Jan 2015 16:23:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49223 invoked by uid 500); 5 Jan 2015 16:23:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49173 invoked by uid 99); 5 Jan 2015 16:23:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 16:23:23 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 16:22:58 +0000
Received: from 172.18.9.243 (EHLO dfweml704-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BAQ76327;
	Mon, 05 Jan 2015 10:22:56 -0600 (CST)
Received: from SJCEML702-CHM.china.huawei.com (10.212.94.48) by
 dfweml704-chm.china.huawei.com (10.193.5.141) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Mon, 5 Jan 2015 08:22:55 -0800
Received: from SJCEML701-CHM.china.huawei.com ([169.254.3.253]) by
 SJCEML702-CHM.china.huawei.com ([169.254.4.46]) with mapi id 14.03.0158.001;
 Mon, 5 Jan 2015 08:22:50 -0800
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: Ted Yu <yuzhihong@gmail.com>, tgbaggio <gen.tang86@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: python converter in HBaseConverter.scala(spark/examples)
Thread-Topic: python converter in HBaseConverter.scala(spark/examples)
Thread-Index: AQHQKPYN2iKqUMPxl0um2mE7sgjXZZyyLviA//+GRKA=
Date: Mon, 5 Jan 2015 16:22:50 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D1AC60C@SJCEML701-CHM.china.huawei.com>
References: <1420468976814-10001.post@n3.nabble.com>
 <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
In-Reply-To: <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.57]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

V2UgYXJlIHBsYW5uaW5nIHRvIHN1cHBvcnQgSEJhc2UgYXMgYSAibmF0aXZlIiBkYXRhIHNvdXJj
ZSB0byBTcGFyayBTUUwgaW4gMS4zIChTUEFSSy0zODgwKS4gDQpNb3JlIGRldGFpbHMgd2lsbCBj
b21lIHNvb24uDQoNCg0KLS0tLS1PcmlnaW5hbCBNZXNzYWdlLS0tLS0NCkZyb206IFRlZCBZdSBb
bWFpbHRvOnl1emhpaG9uZ0BnbWFpbC5jb21dIA0KU2VudDogTW9uZGF5LCBKYW51YXJ5IDA1LCAy
MDE1IDc6MzcgQU0NClRvOiB0Z2JhZ2dpbw0KQ2M6IGRldkBzcGFyay5hcGFjaGUub3JnDQpTdWJq
ZWN0OiBSZTogcHl0aG9uIGNvbnZlcnRlciBpbiBIQmFzZUNvbnZlcnRlci5zY2FsYShzcGFyay9l
eGFtcGxlcykNCg0KSW4gbXkgb3BpbmlvbiB0aGlzIHdvdWxkIGJlIHVzZWZ1bCAtIHRoZXJlIHdh
cyBhbm90aGVyIHRocmVhZCB3aGVyZSByZXR1cm5pbmcgb25seSB0aGUgdmFsdWUgb2YgZmlyc3Qg
Y29sdW1uIGluIHRoZSByZXN1bHQgd2FzIG1lbnRpb25lZC4NCg0KUGxlYXNlIGNyZWF0ZSBhIFNQ
QVJLIEpJUkEgYW5kIGEgcHVsbCByZXF1ZXN0Lg0KDQpDaGVlcnMNCg0KT24gTW9uLCBKYW4gNSwg
MjAxNSBhdCA2OjQyIEFNLCB0Z2JhZ2dpbyA8Z2VuLnRhbmc4NkBnbWFpbC5jb20+IHdyb3RlOg0K
DQo+IEhpLA0KPg0KPiBJbiAgSEJhc2VDb252ZXJ0ZXIuc2NhbGENCj4gPA0KPiBodHRwczovL2dp
dGh1Yi5jb20vYXBhY2hlL3NwYXJrL2Jsb2IvbWFzdGVyL2V4YW1wbGVzL3NyYy9tYWluL3NjYWxh
L29yDQo+IGcvYXBhY2hlL3NwYXJrL2V4YW1wbGVzL3B5dGhvbmNvbnZlcnRlcnMvSEJhc2VDb252
ZXJ0ZXJzLnNjYWxhDQo+ID4NCj4gLCB0aGUgcHl0aG9uIGNvbnZlcnRlciBIQmFzZVJlc3VsdFRv
U3RyaW5nQ29udmVydGVyIHJldHVybiBvbmx5IHRoZSANCj4gdmFsdWUgb2YgZmlyc3QgY29sdW1u
IGluIHRoZSByZXN1bHQuIEluIG15IG9waW5pb24sIGl0IGxpbWl0cyB0aGUgDQo+IHV0aWxpdHkg
b2YgdGhpcyBjb252ZXJ0ZXIsIGJlY2F1c2UgaXQgcmV0dXJucyBvbmx5IG9uZSB2YWx1ZSBwZXIg
cm93IA0KPiBhbmQgbW9yZW92ZXIgaXQgbG9zZXMgdGhlIG90aGVyIGluZm9ybWF0aW9uIG9mIHJl
Y29yZCwgc3VjaCBhcyANCj4gY29sdW1uOmNlbGwsIHRpbWVzdGFtcC4NCj4NCj4gVGhlcmVmb3Jl
LCBJIHdvdWxkIGxpa2UgdG8gcHJvcG9zZSBzb21lIG1vZGlmaWNhdGlvbnMgYWJvdXQgDQo+IEhC
YXNlUmVzdWx0VG9TdHJpbmdDb252ZXJ0ZXIgd2hpY2ggd2lsbCBiZSBhYmxlIHRvIHJldHVybiBh
bGwgcmVjb3JkcyANCj4gaW4gdGhlIGhiYXNlIHdpdGggbW9yZSBjb21wbGV0ZSBpbmZvcm1hdGlv
bjogSSBoYXZlIGFscmVhZHkgd3JpdHRlbiANCj4gc29tZSBjb2RlIGluIHB5dGhvbkNvbnZlcnRl
cnMuc2NhbGEgPCANCj4gaHR0cHM6Ly9naXRodWIuY29tL0dlblRhbmcvc3BhcmtfaGJhc2UvYmxv
Yi9tYXN0ZXIvc3JjL21haW4vc2NhbGEvZXhhbQ0KPiBwbGVzL3B5dGhvbkNvbnZlcnRlcnMuc2Nh
bGENCj4gPg0KPiBhbmQgaXQgd29ya3MNCj4NCj4gSXMgaXQgT0sgdG8gbW9kaWZ5IHRoZSBjb2Rl
IGluIEhCYXNlQ29udmVydGVycy5zY2FsYSwgcGxlYXNlPw0KPiBUaGFua3MgYSBsb3QgaW4gYWR2
YW5jZS4NCj4NCj4gQ2hlZXJzDQo+IEdlbg0KPg0KPg0KPg0KPg0KPiAtLQ0KPiBWaWV3IHRoaXMg
bWVzc2FnZSBpbiBjb250ZXh0Og0KPiBodHRwOi8vYXBhY2hlLXNwYXJrLWRldmVsb3BlcnMtbGlz
dC4xMDAxNTUxLm4zLm5hYmJsZS5jb20vcHl0aG9uLWNvbnZlDQo+IHJ0ZXItaW4tSEJhc2VDb252
ZXJ0ZXItc2NhbGEtc3BhcmstZXhhbXBsZXMtdHAxMDAwMS5odG1sDQo+IFNlbnQgZnJvbSB0aGUg
QXBhY2hlIFNwYXJrIERldmVsb3BlcnMgTGlzdCBtYWlsaW5nIGxpc3QgYXJjaGl2ZSBhdCANCj4g
TmFiYmxlLmNvbS4NCj4NCj4gLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tDQo+IFRvIHVuc3Vic2NyaWJlLCBlLW1haWw6
IGRldi11bnN1YnNjcmliZUBzcGFyay5hcGFjaGUub3JnIEZvciANCj4gYWRkaXRpb25hbCBjb21t
YW5kcywgZS1tYWlsOiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQo+DQo+DQo=
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 17:08:51 2015
Return-Path: <dev-return-11016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A00E3CF8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 17:08:51 +0000 (UTC)
Received: (qmail 60176 invoked by uid 500); 5 Jan 2015 17:08:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60098 invoked by uid 500); 5 Jan 2015 17:08:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60086 invoked by uid 99); 5 Jan 2015 17:08:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 17:08:49 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.43 as permitted sender)
Received: from [209.85.213.43] (HELO mail-yh0-f43.google.com) (209.85.213.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 17:08:24 +0000
Received: by mail-yh0-f43.google.com with SMTP id z6so10698134yhz.16
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 09:06:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8MY5MhBmgLRB7QS1KgO87VQBJEPF0h4m5Wb/nwYAfho=;
        b=Bm/sTLKcqb0/SKfzaGaMoG6iZVczHdRIAkV7eJtw7mH8MRlCrb5OSt7pOzKlx2Imzo
         EEcZ7AqJR6BcNcd8/UqqHWbfLJLmhCBAKmF+zTgw3ZIXrdx3anEYCVRcWSWQq9ohjVOI
         RtVCyPuEcG55jlREzYIUYNJk/msIucgJVGe7IFmO5s64lZxyT9k8IQpXc3WuzNKU16UI
         ysa5arVsKO0kIcKe3nkAnKGBPwJcJ6wXsIxt5UEUfjkWyz41Vrzx23wNjr6o+fXh8xZ5
         GwjM0/VOV5fScmW66ZtrXJqvIRPB5xPWtQXrc8rvXwuSu0u27dtCi7gtwTvsS4NJ4pPj
         B1Og==
MIME-Version: 1.0
X-Received: by 10.236.20.226 with SMTP id p62mr58623483yhp.97.1420477613166;
 Mon, 05 Jan 2015 09:06:53 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Mon, 5 Jan 2015 09:06:53 -0800 (PST)
In-Reply-To: <1420473278868.1043109b@Nodemailer>
References: <CALte62xJMi8MLU7-uGEcGJB8YKRZ7yK94M-euGhuaQV=7DoZ9w@mail.gmail.com>
	<1420473278868.1043109b@Nodemailer>
Date: Mon, 5 Jan 2015 09:06:53 -0800
Message-ID: <CALte62xO_s8Vey7WahKfowgNQzftYceNOQon7SDYu3LmEgWydw@mail.gmail.com>
Subject: Re: python converter in HBaseConverter.scala(spark/examples)
From: Ted Yu <yuzhihong@gmail.com>
To: Nick Pentreath <nick.pentreath@gmail.com>
Cc: tgbaggio <gen.tang86@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1de828b724e050beab7d5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1de828b724e050beab7d5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

HBaseConverter is in Spark source tree. Therefore I think it makes sense
for this improvement to be accepted so that the example is more useful.

Cheers

On Mon, Jan 5, 2015 at 7:54 AM, Nick Pentreath <nick.pentreath@gmail.com>
wrote:

> Hey
>
> These converters are actually just intended to be examples of how to set
> up a custom converter for a specific input format. The converter interfac=
e
> is there to provide flexibility where needed, although with the new
> SparkSQL data store interface the intention is that most common use cases
> can be handled using that approach rather than custom converters.
>
> The intention is not to have specific converters living in Spark core,
> which is why these are in the examples project.
>
> Having said that, if you wish to expand the example converter for others
> reference do feel free to submit a PR.
>
> Ideally though, I would think that various custom converters would be par=
t
> of external projects that can be listed with http://spark-packages.org/ I
> see your project is already listed there.
>
> =E2=80=94
> Sent from Mailbox <https://www.dropbox.com/mailbox>
>
>
> On Mon, Jan 5, 2015 at 5:37 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> In my opinion this would be useful - there was another thread where
>> returning
>> only the value of first column in the result was mentioned.
>>
>> Please create a SPARK JIRA and a pull request.
>>
>> Cheers
>>
>> On Mon, Jan 5, 2015 at 6:42 AM, tgbaggio <gen.tang86@gmail.com> wrote:
>>
>> > Hi,
>> >
>> > In HBaseConverter.scala
>> > <
>> >
>> https://github.com/apache/spark/blob/master/examples/src/main/scala/org/=
apache/spark/examples/pythonconverters/HBaseConverters.scala
>> > >
>> > , the python converter HBaseResultToStringConverter return only the
>> value
>> > of
>> > first column in the result. In my opinion, it limits the utility of
>> this
>> > converter, because it returns only one value per row and moreover it
>> loses
>> > the other information of record, such as column:cell, timestamp.
>> >
>> > Therefore, I would like to propose some modifications about
>> > HBaseResultToStringConverter which will be able to return all records
>> in
>> > the
>> > hbase with more complete information: I have already written some code
>> in
>> > pythonConverters.scala
>> > <
>> >
>> https://github.com/GenTang/spark_hbase/blob/master/src/main/scala/exampl=
es/pythonConverters.scala
>> > >
>> > and it works
>> >
>> > Is it OK to modify the code in HBaseConverters.scala, please?
>> > Thanks a lot in advance.
>> >
>> > Cheers
>> > Gen
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/python-convert=
er-in-HBaseConverter-scala-spark-examples-tp10001.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>

--001a11c1de828b724e050beab7d5--

From dev-return-11017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 18:04:29 2015
Return-Path: <dev-return-11017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A667910333
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 18:04:29 +0000 (UTC)
Received: (qmail 1980 invoked by uid 500); 5 Jan 2015 18:04:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1902 invoked by uid 500); 5 Jan 2015 18:04:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1868 invoked by uid 99); 5 Jan 2015 18:04:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 18:04:26 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 18:04:01 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so15997650qcv.29
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 10:03:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=PZHHh1UV09Dt2FpgiLeX/9rZcENlCEwFo715g3BOqB4=;
        b=VLL8h11ewpQxRsj+VdNDLVW0Q/EPgm4FSYFtwiZY5U1FkM+5/wYOzGsOxwLPxA2URU
         PT65r0SuRcpiHRjkHWcSuXcenlCjkckPtff5Sx9X6bCwIc5UkYQnz6LMSLw+2vPP3Ld8
         YkbJjIHhpchhsFzOPjFy+tyizsK7YXkNXp1RYKXkBmtHJvoTAj9OAu+kA/2BR4EXqPCo
         rz9tHJ9qeRvHMWhXkuelcp4rUMiGIyx711oH4habvDw/zWrzPm7bjKinCZqUMkcrCU43
         k17M3tZ48poRdVij5wMK0JAoQk//7CkBvu9lLS7PRa5zxvQpbPiJ2Qr63F+9aijhgTr8
         ygVg==
X-Received: by 10.140.51.77 with SMTP id t71mr136914540qga.88.1420481039635;
        Mon, 05 Jan 2015 10:03:59 -0800 (PST)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id z61sm32560405qge.21.2015.01.05.10.03.58
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 05 Jan 2015 10:03:58 -0800 (PST)
Date: Mon, 05 Jan 2015 10:03:58 -0800 (PST)
X-Google-Original-Date: Mon, 05 Jan 2015 18:03:57 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1420481037971.cbfa2f28@Nodemailer>
In-Reply-To: <CALte62xO_s8Vey7WahKfowgNQzftYceNOQon7SDYu3LmEgWydw@mail.gmail.com>
References: <CALte62xO_s8Vey7WahKfowgNQzftYceNOQon7SDYu3LmEgWydw@mail.gmail.com>
X-Orchestra-Oid: 113BB672-840E-42A9-AD5E-107F7DDB8466
X-Orchestra-Sig: e554422d1a193b82a9c5c89dc21db40e266dec30
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1489469742583243020
X-Orchestra-Thrid-Sig: f15c068ded4dd0e24eeb2580ae31ea8325073ddb
X-Orchestra-Account: b10da9d3320567374d9f9766daaefab3d43eba00
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: "Ted Yu" <yuzhihong@gmail.com>
Cc: dev@spark.apache.org, "tgbaggio" <gen.tang86@gmail.com>
Subject: Re: python converter in HBaseConverter.scala(spark/examples)
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1420481038702"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1420481038702
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Absolutely; as I mentioned by all means submit a PR - I just wanted to =
point out that any specific converter is not =22officially=22 supported, =
although the interface is of course.


I'm happy to review a PR just ping me when ready.


=E2=80=94
Sent from Mailbox

On Mon, Jan 5, 2015 at 7:06 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> HBaseConverter is in Spark source tree. Therefore I think it makes sense
> for this improvement to be accepted so that the example is more useful.
> Cheers
> On Mon, Jan 5, 2015 at 7:54 AM, Nick Pentreath <nick.pentreath@gmail.=
com>
> wrote:
>> Hey
>>
>> These converters are actually just intended to be examples of how to =
set
>> up a custom converter for a specific input format. The converter =
interface
>> is there to provide flexibility where needed, although with the new
>> SparkSQL data store interface the intention is that most common use =
cases
>> can be handled using that approach rather than custom converters.
>>
>> The intention is not to have specific converters living in Spark core,
>> which is why these are in the examples project.
>>
>> Having said that, if you wish to expand the example converter for =
others
>> reference do feel free to submit a PR.
>>
>> Ideally though, I would think that various custom converters would be =
part
>> of external projects that can be listed with http://spark-packages.org/ =
I
>> see your project is already listed there.
>>
>> =E2=80=94
>> Sent from Mailbox <https://www.dropbox.com/mailbox>
>>
>>
>> On Mon, Jan 5, 2015 at 5:37 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>>> In my opinion this would be useful - there was another thread where
>>> returning
>>> only the value of first column in the result was mentioned.
>>>
>>> Please create a SPARK JIRA and a pull request.
>>>
>>> Cheers
>>>
>>> On Mon, Jan 5, 2015 at 6:42 AM, tgbaggio <gen.tang86@gmail.com> wrote:
>>>
>>> > Hi,
>>> >
>>> > In HBaseConverter.scala
>>> > <
>>> >
>>> https://github.com/apache/spark/blob/master/examples/src/main/scala/org=
/apache/spark/examples/pythonconverters/HBaseConverters.scala
>>> > >
>>> > , the python converter HBaseResultToStringConverter return only the
>>> value
>>> > of
>>> > first column in the result. In my opinion, it limits the utility of
>>> this
>>> > converter, because it returns only one value per row and moreover it
>>> loses
>>> > the other information of record, such as column:cell, timestamp.
>>> >
>>> > Therefore, I would like to propose some modifications about
>>> > HBaseResultToStringConverter which will be able to return all =
records
>>> in
>>> > the
>>> > hbase with more complete information: I have already written some =
code
>>> in
>>> > pythonConverters.scala
>>> > <
>>> >
>>> https://github.com/GenTang/spark=5Fhbase/blob/master/src/main/scala/exa=
mples/pythonConverters.scala
>>> > >
>>> > and it works
>>> >
>>> > Is it OK to modify the code in HBaseConverters.scala, please=3F
>>> > Thanks a lot in advance.
>>> >
>>> > Cheers
>>> > Gen
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> >
>>> http://apache-spark-developers-list.1001551.n3.nabble.=
com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html
>>> > Sent from the Apache Spark Developers List mailing list archive at
>>> > Nabble.com.
>>> >
>>> > ---------------------------------------------------------------------=

>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>> >
>>> >
>>>
>>
>>
------Nodemailer-0.5.0-?=_1-1420481038702--

From dev-return-11018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 18:38:48 2015
Return-Path: <dev-return-11018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF0151059D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 18:38:47 +0000 (UTC)
Received: (qmail 1579 invoked by uid 500); 5 Jan 2015 18:38:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1505 invoked by uid 500); 5 Jan 2015 18:38:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1493 invoked by uid 99); 5 Jan 2015 18:38:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 18:38:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 18:38:42 +0000
Received: by mail-la0-f53.google.com with SMTP id gm9so19169788lab.12
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 10:37:36 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=52I/t9n5Ft+tSj8JlkCjsrUJR6ZHDDMAH51UfbYL8qA=;
        b=ZE4JuKi+krGWaA3pB8IOCUi6vrPqKK+4dSsCXxFVxABnfR7T7GSH9yGwJsqkmv94zm
         kMViV7yNc+QgH1TI3WdVEDsjEVbo5z8WFu4tdNNJz3M4vndKqvfTQI+BpwohDAAsrK4z
         lLkHuyuFE0LGWIMqX6MzHci+dU4OJwG2LUQ3pGUiaEwffw1/Ua6TVrmnG/LzYYntB9mJ
         alTBsRpXUewOmXU0xcBjuiYFA2ttFa6AIBRFKmSlYSWs/oXO9CoGJewZSmybwctMEvEe
         l0ftoM+KA295eI5IoR71jx+4F7MI3dIvOkwzzJPBytsJYf47l/nYpJUPHuQYKb3a7uPd
         TZBA==
X-Gm-Message-State: ALoCoQkgRQm0ODBc8x9Y4jKht0+cTVyodHxitwYjjG5NAcS/4E2jgBIRCYv4rksnaXfBuaUACyjG
X-Received: by 10.112.170.69 with SMTP id ak5mr72877440lbc.71.1420483055874;
 Mon, 05 Jan 2015 10:37:35 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Mon, 5 Jan 2015 10:37:15 -0800 (PST)
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 5 Jan 2015 10:37:15 -0800
Message-ID: <CACdU-dTwCz42WHgKkueJYS6RPgk=N1YZcyRb=_bMiEB6YF7EBQ@mail.gmail.com>
Subject: jenkins redirect down (but jenkins is up!), lots of potential
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c24060f47f8f050bebfb8f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24060f47f8f050bebfb8f
Content-Type: text/plain; charset=UTF-8

UC Berkeley had some major maintenance done this past weekend, and long
story short, not everything came back.  our primary webserver's NFS is down
and that means we're not serving websites, meaning that the redirect to
jenkins is failing.

jenkins is still up, and building some jobs, but we will probably see pull
request builder failures, and other transient issues.  SCM-polling builds
should be fine.

there is no ETA on when this will be fixed, but once our
amplab.cs.berkeley.edu/jenkins redir is working, i will let everyone know.
 i'm trying to get more status updates as they come.

i'm really sorry about the inconvenience.

shane

--001a11c24060f47f8f050bebfb8f--

From dev-return-11019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 19:37:05 2015
Return-Path: <dev-return-11019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8ABB0108C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 19:37:05 +0000 (UTC)
Received: (qmail 81911 invoked by uid 500); 5 Jan 2015 19:37:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81843 invoked by uid 500); 5 Jan 2015 19:37:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79130 invoked by uid 99); 5 Jan 2015 19:37:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 19:37:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 19:36:35 +0000
Received: by mail-pd0-f179.google.com with SMTP id fp1so28892807pdb.10;
        Mon, 05 Jan 2015 11:36:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:subject:date:references:to:message-id
         :mime-version;
        bh=c5LZYmcNIDB/yTVIfXzyHnc8j5kuAhB3Ic3NNBSYrxc=;
        b=P4gShkyNsT9OZzQ1eYd6uOHQPCIIXPjn9BFaVyA6aVS61kA145kNchqecx6iW//dX6
         yw5yrSpC1fpXZ0QHvBa8qvzd97LWXGpAUCYw6lx5Rhg9akSDXNclyD+W8c0zfCUNPD8e
         8xrPveQY8ReRq8pv4u/v1UjXIxEHw9lWdY1rFpuo4a7YSVHzZaM4C29ctr4ecPYY+7h8
         T6bqPWf8prGYkdh67KLvL1sQhxB6z7AZgg8/aQZbVlBuacogsQOoMxkvz06TAsInFCoI
         gSOgtTkP5M3ALaq46zgB/Fufvh35v8TwGDqjX7KC+//ZjuVD5hLbo/W2POi26qJUIQ33
         1URg==
X-Received: by 10.67.13.12 with SMTP id eu12mr151852679pad.157.1420486593674;
        Mon, 05 Jan 2015 11:36:33 -0800 (PST)
Received: from [192.168.1.100] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id pv3sm54910442pbb.56.2015.01.05.11.36.32
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 05 Jan 2015 11:36:32 -0800 (PST)
From: Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_C7BF7164-82E0-4760-9430-0B2AA4BEA498"
Subject: Fwd: ApacheCon North America 2015 Call For Papers
Date: Mon, 5 Jan 2015 11:36:31 -0800
References: <54AACC99.20001@rcbowen.com>
To: user <user@spark.apache.org>,
 dev <dev@spark.apache.org>
Message-Id: <EB05826C-0297-4E9D-95B0-C412220E6323@gmail.com>
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_C7BF7164-82E0-4760-9430-0B2AA4BEA498
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

FYI, ApacheCon North America call for papers is up.

Matei

> Begin forwarded message:
>=20
> Date: January 5, 2015 at 9:40:41 AM PST
> From: Rich Bowen <rbowen@rcbowen.com>
> Reply-To: dev <dev@community.apache.org>
> To: dev <dev@community.apache.org>
> Subject: ApacheCon North America 2015 Call For Papers
>=20
> Fellow ASF enthusiasts,
>=20
> We now have less than a month remaining in the Call For Papers for =
ApacheCon North America 2015, and so far the submissions are on the =
paltry side. Please consider submitting papers for consideration for =
this event.
>=20
> Details about the event are available at =
http://events.linuxfoundation.org/events/apachecon-north-america
>=20
> The call for papers is at =
http://events.linuxfoundation.org//events/apachecon-north-america/program/=
cfp
>=20
> Please help us out by getting this message out to your user@ and dev@ =
community on the projects that you're involved in, so that these =
projects can be represented in Austin.
>=20
> If you are interested in chairing a content track, and taking on the =
task of wrangling your community together to create a compelling story =
about your technology space, please join the comdev mailing list - =
dev-subscribe@community.apache.org - and speak up there.
>=20
> (Message is Bcc'ed committers@, and Reply-to set to dev@community, if =
you want to discuss this topic further there.)
>=20
> Thanks!
>=20
> --=20
> Rich Bowen - rbowen@rcbowen.com - @rbowen
> http://apachecon.com/ - @apachecon


--Apple-Mail=_C7BF7164-82E0-4760-9430-0B2AA4BEA498--

From dev-return-11020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 20:25:40 2015
Return-Path: <dev-return-11020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56AF310B57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 20:25:40 +0000 (UTC)
Received: (qmail 93490 invoked by uid 500); 5 Jan 2015 20:25:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93416 invoked by uid 500); 5 Jan 2015 20:25:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93404 invoked by uid 99); 5 Jan 2015 20:25:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 20:25:38 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of olivier.toupin@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 20:25:33 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id C8F5AF969A9
	for <dev@spark.apache.org>; Mon,  5 Jan 2015 12:25:13 -0800 (PST)
Date: Mon, 5 Jan 2015 13:25:12 -0700 (MST)
From: Olivier Toupin <olivier.toupin@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420489512983-10010.post@n3.nabble.com>
Subject: Spark UI history job duration is wrong
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello,

I'm using Spark 1.2.0 and when running an application, if I go into the UI
and then in the job tab ("/jobs/") the jobs duration are relevant and the
posted durations looks ok.

However when I open the history ("history/app-<xyz>/jobs/") for that job,
the duration are wrong showing milliseconds instead of the relevant job
time. The submitted time for each job (except maybe the first) is different
also.

The stage tab is unaffected and show the correct duration for each stages in
both mode.

Should I open a bug?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-UI-history-job-duration-is-wrong-tp10010.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 20:37:44 2015
Return-Path: <dev-return-11021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9805210BBE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 20:37:44 +0000 (UTC)
Received: (qmail 17201 invoked by uid 500); 5 Jan 2015 20:37:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17127 invoked by uid 500); 5 Jan 2015 20:37:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17105 invoked by uid 99); 5 Jan 2015 20:37:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 20:37:43 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 20:37:38 +0000
Received: by mail-ob0-f172.google.com with SMTP id va8so62695566obc.3
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 12:37:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ZmqVOCYEfU4KKqBloTfo7LtzUD0NH1ydrwywTmpUQJk=;
        b=qdQXhvFodUQw/sNgMEem7PXrKn0KBguXaL5kMPwtxD3o1odO6iy4Cxpp6uKBXvRPmt
         qzKHg2WLcLGjArHp3p7SZommHKPvkVXbOiL2NvC+oILIiuHG86XMR2Sp3tYADM4HtRtL
         Aonz94ANxfB4ZlRnk8I1bxgRZOQA2J18uusUvnQJ+8JAYhCycWL9x2ZZMU08aDU0+ruH
         y0CoilUSJWfrggZuLhIHoDZ6Cujbv9GgfcZL6OahCjxqFKiuQ+WYPKv+6o9r0P/xNACf
         t43VFA0wqzn9/pc3E5n6JCyJKNMNNq9eaMKutJKgIwE/aGRgq1tW7pTiBWkaTZtu3fbv
         pueg==
MIME-Version: 1.0
X-Received: by 10.182.71.73 with SMTP id s9mr55618721obu.15.1420490237958;
 Mon, 05 Jan 2015 12:37:17 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 5 Jan 2015 12:37:17 -0800 (PST)
In-Reply-To: <1420489512983-10010.post@n3.nabble.com>
References: <1420489512983-10010.post@n3.nabble.com>
Date: Mon, 5 Jan 2015 12:37:17 -0800
Message-ID: <CABPQxsuUXNr4KH9TjpoCu5wOEcx_H3a_ybJv0u0G87hAtUK5Rg@mail.gmail.com>
Subject: Re: Spark UI history job duration is wrong
From: Patrick Wendell <pwendell@gmail.com>
To: Olivier Toupin <olivier.toupin@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for reporting this - it definitely sounds like a bug. Please
open a JIRA for it. My guess is that we define the start or end time
of the job based on the current time instead of looking at data
encoded in the underlying event stream. That would cause it to not
work properly when loading from historical data.

- Patrick

On Mon, Jan 5, 2015 at 12:25 PM, Olivier Toupin
<olivier.toupin@gmail.com> wrote:
> Hello,
>
> I'm using Spark 1.2.0 and when running an application, if I go into the UI
> and then in the job tab ("/jobs/") the jobs duration are relevant and the
> posted durations looks ok.
>
> However when I open the history ("history/app-<xyz>/jobs/") for that job,
> the duration are wrong showing milliseconds instead of the relevant job
> time. The submitted time for each job (except maybe the first) is different
> also.
>
> The stage tab is unaffected and show the correct duration for each stages in
> both mode.
>
> Should I open a bug?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-UI-history-job-duration-is-wrong-tp10010.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 21:58:21 2015
Return-Path: <dev-return-11022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69B3B10FA4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 21:58:21 +0000 (UTC)
Received: (qmail 8965 invoked by uid 500); 5 Jan 2015 21:58:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8896 invoked by uid 500); 5 Jan 2015 21:58:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7334 invoked by uid 99); 5 Jan 2015 21:58:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 21:58:15 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 21:57:50 +0000
Received: by mail-ie0-f174.google.com with SMTP id at20so20397187iec.33
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 13:57:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type:content-transfer-encoding;
        bh=Ud/41X8KwaTUkpzNp58lFC0E+cILjrw/jNYhiYhBu2Q=;
        b=QgU+rGiSm00FsGnletdvmzPtA7s0rI5FuxSpYcBpnuXQHa0M38MVFMFzpkAJDOloZV
         J/ywlDR8DijG18OZ443yZo/cBiwF+NpJLX3KpiO4Qg6XKoJyeZi6OyZEcdYIvbCY8jWZ
         YgzNHuZiwCj+p7UPAxEftknmwpbgu+fFshj0wmPutZ3U2Hcpl8GXWQhUuNPHoTC5jzRO
         z08rxbPNpu6omUe+WRZ2IJrekdbChbkqHyhGO3Uh8Zl4tesCt8/FdG2qBbd6MwS0od6D
         iJ8lI7wHFF9zSGWMlJTy9/PS8ynBii3zkYfHR7ZkE7jskbhxhCMQI94/GuHK1bpxGBQR
         EsCg==
X-Received: by 10.42.12.147 with SMTP id y19mr69216856icy.80.1420495064206;
 Mon, 05 Jan 2015 13:57:44 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.12.162 with HTTP; Mon, 5 Jan 2015 13:57:14 -0800 (PST)
In-Reply-To: <1418929497786.4ba3e417@Nodemailer>
References: <1418929497786.4ba3e417@Nodemailer>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Mon, 5 Jan 2015 13:57:14 -0800
Message-ID: <CAMwrk0=4iPBGzKjO6vfftuMgJ+zVJTDez+X36BKyi3KSQym4UQ@mail.gmail.com>
Subject: Re: Spark Streaming Data flow graph
To: francois.garillot@typesafe.com
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Fran=C3=A7ois,

Well, at a high-level here is what I thought about the diagram.

- ReceiverSupervisor handles only one Receiver.
- BlockGenerator is part of ReceiverSupervisor not ReceivedBlockHandler
- The blocks are inserted in BlockManager and if activated,
WriteAheadLogManager in parallel, not through BlockManager as the
diagram seems to imply
- It would be good to have a clean visual separation of what runs in
Executor (better term than Worker) and what is in Driver ... Driver
stuff on left and Executor stuff on right, or vice versa.

More importantly, the word of caution is that all the internal stuff
like ReceiverBlockHandler, Supervisor, etc are subject to change any
time as we keep refactoring stuff. So highlighting these internal
details too much too publicly may lead to future confusion.

TD

On Thu, Dec 18, 2014 at 11:04 AM,  <francois.garillot@typesafe.com> wrote:
> I=E2=80=99ve been trying to produce an updated box diagram to refresh :
> http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tatha=
gatadassparkmeetup20130617/26
>
>
> =E2=80=A6 after the SPARK-3129, and other switches (a surprising number o=
f comments still mention NetworkReceiver).
>
>
> Here=E2=80=99s what I have so far:
> https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf?dl=3D0
>
>
> This is not supposed to respect any particular convention (ER, ORM, =E2=
=80=A6). Data flow up to right before RDD creation is in bold arrows, metad=
ata flow is in normal width arrows.
>
>
> This diagram is still very much a WIP (see below : todo), but I wanted to=
 share it to ask:
> - what=E2=80=99s wrong ?
> - what are the glaring omissions ?
> - how can I make this better (i.e. what should I add first to the Todo-li=
st below) ?
>
>
> I=E2=80=99ll be happy to share this (including sources) with whoever asks=
 for it.
>
>
> Todo :
> - mark private/public classes
> - mark queues in Receiver, ReceivedBlockHandler, BlockManager
> - mark type of info on transport : e.g. Actor message, ReceivedBlockInfo
>
>
>
> =E2=80=94
> Fran=C3=A7ois Garillot

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 22:27:25 2015
Return-Path: <dev-return-11023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92436173B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 22:27:25 +0000 (UTC)
Received: (qmail 95570 invoked by uid 500); 5 Jan 2015 22:27:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95488 invoked by uid 500); 5 Jan 2015 22:27:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95472 invoked by uid 99); 5 Jan 2015 22:27:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 22:27:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 22:27:19 +0000
Received: by mail-pd0-f170.google.com with SMTP id v10so29163036pde.29
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 14:25:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+cRKX/0aeXBEChEfXR7yQHmIrSI3yXgoE6rtbCHCYn0=;
        b=yEf4I8HflAFOw0aGO+deCQmYonGhqsvvU1MB5lKO0M+A/e++6HADFTeKNaO6sDVm/P
         kEHBbwQfBXOSfmLkd1FhbLRPkiTC+UnDpFTS1GuVj5mJlXQaLbTanodsd2p4sXP8WY+e
         7zo2Rd52wRolEmMrg76PjHd8ZeAJPLPKr0tSGSuFKKPZrWoXquiXCXbgc0r1DHTpJaSx
         /ZaQTPiXXiEnmkuOQ73Ep7Cuc/n6eF7GSyztlhgxFh53zumCAusZcK9CDziaNjVUiNLy
         mkIN/obTfTaEIvN4B+3Gtc/1YiPO5Kj3WKIL9Zjp7l0oFC2tqoXkQ5XUzUBXzRcV85st
         pLYw==
MIME-Version: 1.0
X-Received: by 10.66.65.108 with SMTP id w12mr149695637pas.115.1420496729185;
 Mon, 05 Jan 2015 14:25:29 -0800 (PST)
Received: by 10.70.41.80 with HTTP; Mon, 5 Jan 2015 14:25:29 -0800 (PST)
In-Reply-To: <CACdU-dTwCz42WHgKkueJYS6RPgk=N1YZcyRb=_bMiEB6YF7EBQ@mail.gmail.com>
References: <CACdU-dTwCz42WHgKkueJYS6RPgk=N1YZcyRb=_bMiEB6YF7EBQ@mail.gmail.com>
Date: Mon, 5 Jan 2015 14:25:29 -0800
Message-ID: <CAOEPXP51kFdo4axWAU9HMqRpVDGU5J6iny09vRGYovHMj69pHw@mail.gmail.com>
Subject: Re: jenkins redirect down (but jenkins is up!), lots of potential
From: Josh Rosen <rosenville@gmail.com>
To: shane knapp <sknapp@berkeley.edu>
Cc: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134b85af2a2b8050bef2aee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134b85af2a2b8050bef2aee
Content-Type: text/plain; charset=UTF-8

The pull request builder and SCM-polling builds appear to be working fine,
but the links in pull request comments won't work because the AMP Lab
webserver is still down.  In the meantime, though, you can continue to
access Jenkins through https://hadrian.ist.berkeley.edu/jenkins/

On Mon, Jan 5, 2015 at 10:37 AM, shane knapp <sknapp@berkeley.edu> wrote:

> UC Berkeley had some major maintenance done this past weekend, and long
> story short, not everything came back.  our primary webserver's NFS is down
> and that means we're not serving websites, meaning that the redirect to
> jenkins is failing.
>
> jenkins is still up, and building some jobs, but we will probably see pull
> request builder failures, and other transient issues.  SCM-polling builds
> should be fine.
>
> there is no ETA on when this will be fixed, but once our
> amplab.cs.berkeley.edu/jenkins redir is working, i will let everyone know.
>  i'm trying to get more status updates as they come.
>
> i'm really sorry about the inconvenience.
>
> shane
>

--001a1134b85af2a2b8050bef2aee--

From dev-return-11024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan  5 23:43:36 2015
Return-Path: <dev-return-11024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 271B0176B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 Jan 2015 23:43:36 +0000 (UTC)
Received: (qmail 78513 invoked by uid 500); 5 Jan 2015 23:43:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78444 invoked by uid 500); 5 Jan 2015 23:43:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76519 invoked by uid 99); 5 Jan 2015 23:43:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 23:43:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 Jan 2015 23:43:04 +0000
Received: by mail-qg0-f53.google.com with SMTP id l89so15911820qgf.40
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 15:41:12 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=BQetRxnMbemCvkyy4ivBmfHnlzve5BomAhdMsXAXjLM=;
        b=hSj6vjVALBlpde/+RKdCHBmy4D6LR4YJDrjeO6JtDHllNbznUsHtpcNUf6owF9X+k2
         Qu3RyI0ccgqZPW6C74v/TO98ACQ3EZWppbs2zQokAtmet5s15uxas0mzbWyXy94f/gHA
         60Xag/rArUC5asrpCooyCpeoZOoID20MuGHQEtSgGYTboGzOEYHiXT3ieq14jfsjuuyl
         mvamRURmQPcAw6hK8okEf7MWNbm3Oxv1YdPZxCS23PR1i5r6tQ9VEpv9pu3FaluOfiEi
         efnAJBwPcH9cdCo7Nh7Vk8WBULnqOmJXexRlHTjr1koJOhkyCwoPPVPAk//6OH8f05fU
         0HHQ==
X-Gm-Message-State: ALoCoQm2kLTylFtVA9oqputVG+C2zAVMYVAtS5XC4gJc9N4I2GTIgvprrQn6EJAjX6eGkemoWzD4
X-Received: by 10.224.88.129 with SMTP id a1mr69269552qam.92.1420501272158;
        Mon, 05 Jan 2015 15:41:12 -0800 (PST)
Received: from mail-qc0-f175.google.com (mail-qc0-f175.google.com. [209.85.216.175])
        by mx.google.com with ESMTPSA id v65sm11080519qgd.20.2015.01.05.15.41.11
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 05 Jan 2015 15:41:11 -0800 (PST)
Received: by mail-qc0-f175.google.com with SMTP id p6so148405qcv.20
        for <dev@spark.apache.org>; Mon, 05 Jan 2015 15:41:10 -0800 (PST)
X-Received: by 10.224.20.1 with SMTP id d1mr69943011qab.58.1420501270725; Mon,
 05 Jan 2015 15:41:10 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.89.11 with HTTP; Mon, 5 Jan 2015 15:40:50 -0800 (PST)
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 5 Jan 2015 15:40:50 -0800
Message-ID: <CA+-p3AG5SmS5KVfddbZmvZwMu10ONiq5R7MY3aX4LOgwEqZoZg@mail.gmail.com>
Subject: Maintainer for Mesos
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1e294a52c36050bf039b0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1e294a52c36050bf039b0
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

I'm interested in having a committer look at a PR [1] for Mesos, but
there's not an entry for Mesos in the maintainers specialties on the wiki
[2].  Which Spark committers have expertise in the Mesos features?

Thanks!
Andrew


[1] https://github.com/apache/spark/pull/3074
[2]
https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers

--001a11c1e294a52c36050bf039b0--

From dev-return-11025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan  6 08:38:39 2015
Return-Path: <dev-return-11025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8E3B10698
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 Jan 2015 08:38:39 +0000 (UTC)
Received: (qmail 90541 invoked by uid 500); 6 Jan 2015 08:38:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90439 invoked by uid 500); 6 Jan 2015 08:38:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88507 invoked by uid 99); 6 Jan 2015 08:38:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 08:38:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda.perera@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 08:38:32 +0000
Received: by mail-ob0-f177.google.com with SMTP id va2so63598966obc.8;
        Tue, 06 Jan 2015 00:38:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=jkr7ysOSW4e0eRQsqOEw1QllVfy6Bh+fRI3CzPJFB+g=;
        b=i6m81pOC6p8fjDbxzwjvlMOJcryKhAIaNmWe2oHJ6yQPwnVmqxtQSXb4tW1nsc45NW
         DS1aEjdnzvPP542bW0zLFGdpAjcgGPyHHRkP/f02ROvurn8fpKiGFcBhS6o9IS/d/gDr
         ZtbKbmVAenO9ffN+u5G+quFk5SnwrGQf4DAPz+nvslEaSj/hDxEPDTCg3ptV2UNfs4Vt
         h3zu49yK59rL2bej6fQwNaZpBQdjaOMIYBWIx+0amOALXgHOvyhR4lPwS3HXlP27+0Qv
         GpbrWse+qsXGqGlqIhYw2efFfS6H6FMFv/mDvWDyBqJOaZH6onApf+p8KMFaiOW/rM1j
         SN1g==
MIME-Version: 1.0
X-Received: by 10.60.83.34 with SMTP id n2mr57676915oey.61.1420533492255; Tue,
 06 Jan 2015 00:38:12 -0800 (PST)
Received: by 10.202.105.72 with HTTP; Tue, 6 Jan 2015 00:38:12 -0800 (PST)
Date: Tue, 6 Jan 2015 14:08:12 +0530
Message-ID: <CANCoaU7npWV5jo2fTDkwhG=yN0_2h3Z0SqQCDCvDwCZ0Wb3qiA@mail.gmail.com>
Subject: Guava 11 dependency issue in Spark 1.2.0
From: Niranda Perera <niranda.perera@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0112c362329448050bf7ba1c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112c362329448050bf7ba1c
Content-Type: text/plain; charset=UTF-8

Hi,

I have been running a simple Spark app on a local spark cluster and I came
across this error.

Exception in thread "main" java.lang.NoSuchMethodError:
com.google.common.hash.HashFunction.hashInt(I)Lcom/google/common/hash/HashCode;
    at org.apache.spark.util.collection.OpenHashSet.org
$apache$spark$util$collection$OpenHashSet$$hashcode(OpenHashSet.scala:261)
    at
org.apache.spark.util.collection.OpenHashSet$mcI$sp.getPos$mcI$sp(OpenHashSet.scala:165)
    at
org.apache.spark.util.collection.OpenHashSet$mcI$sp.contains$mcI$sp(OpenHashSet.scala:102)
    at
org.apache.spark.util.SizeEstimator$$anonfun$visitArray$2.apply$mcVI$sp(SizeEstimator.scala:214)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at
org.apache.spark.util.SizeEstimator$.visitArray(SizeEstimator.scala:210)
    at
org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:169)
    at
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:161)
    at
org.apache.spark.util.SizeEstimator$.estimate(SizeEstimator.scala:155)
    at
org.apache.spark.util.collection.SizeTracker$class.takeSample(SizeTracker.scala:78)
    at
org.apache.spark.util.collection.SizeTracker$class.afterUpdate(SizeTracker.scala:70)
    at
org.apache.spark.util.collection.SizeTrackingVector.$plus$eq(SizeTrackingVector.scala:31)
    at
org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
    at
org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:136)
    at
org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:114)
    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:787)
    at
org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
    at
org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:992)
    at
org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:98)
    at
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:84)
    at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
    at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
    at
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:945)
    at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:695)
    at
com.databricks.spark.avro.AvroRelation.buildScan$lzycompute(AvroRelation.scala:45)
    at
com.databricks.spark.avro.AvroRelation.buildScan(AvroRelation.scala:44)
    at
org.apache.spark.sql.sources.DataSourceStrategy$.apply(DataSourceStrategy.scala:56)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
    at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:418)
    at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:416)
    at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:422)
    at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:422)
    at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
    at
org.apache.spark.sql.api.java.JavaSchemaRDD.collect(JavaSchemaRDD.scala:114)


While looking into this I found out that Guava was downgraded to version 11
in this PR.
https://github.com/apache/spark/pull/1610

In this PR OpenHashSet.scala:261 line hashInt has been changed to hashLong.
But when I actually run my app,  "java.lang.NoSuchMethodError:
com.google.common.hash.HashFunction.hashInt" error occurs,
which is understandable because hashInt is not available before Guava 12.

So, I''m wondering why this occurs?

Cheers
-- 
Niranda Perera

--089e0112c362329448050bf7ba1c--

From dev-return-11026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan  6 09:26:01 2015
Return-Path: <dev-return-11026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 521D710852
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 Jan 2015 09:26:01 +0000 (UTC)
Received: (qmail 86707 invoked by uid 500); 6 Jan 2015 09:26:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86634 invoked by uid 500); 6 Jan 2015 09:26:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86618 invoked by uid 99); 6 Jan 2015 09:25:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 09:25:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 09:25:32 +0000
Received: by mail-pa0-f43.google.com with SMTP id kx10so30587676pab.16
        for <dev@spark.apache.org>; Tue, 06 Jan 2015 01:25:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:organization:user-agent:mime-version:to
         :subject:content-type:content-transfer-encoding;
        bh=gy4IEfrK7gFTqQN2lfZ0nAWqsBJKyfZ3Odwuq6vbDX4=;
        b=FFzmBTfcRMDiqZH1HbzvyZX50olIXSnkXaBlanZeUHWVZ9qZEO2NUITQM4HXI/N0Hc
         BTOOySEAnhMFzYFwAZdqwM+e4bDEZZ8UL4f3DxK1TNS3y5qBAVpkwuvUd2qkyZNWlbNH
         s9nP8ZV2gky/0gpYEQsOYh64Kmg+wtn9Njqt86zKNIekpHdkLK4GxONV6flyjVwDpeYb
         F2MCGFrk70iOXbkPB2cMhgbVBzaBq4L8QY9IfUZx1WdPEPqT64Jj2pRDSOscSP97AzIc
         ecnZsmDPZGlC8rOvPGe5G3iU4CWo6BIsNMTrgRTyG/7MM8mHd+WTsx1ozMm53zghZpbS
         +5QA==
X-Received: by 10.68.233.170 with SMTP id tx10mr68522733pbc.89.1420536330815;
        Tue, 06 Jan 2015 01:25:30 -0800 (PST)
Received: from [192.168.99.18] ([104.143.36.185])
        by mx.google.com with ESMTPSA id og12sm56445820pdb.43.2015.01.06.01.25.29
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 06 Jan 2015 01:25:30 -0800 (PST)
Message-ID: <54ABAA06.2060908@gmail.com>
Date: Tue, 06 Jan 2015 17:25:26 +0800
From: Yi Tian <tianyi.asiainfo@gmail.com>
Organization: Asiainfo.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: [SPARK-5100][SQL] Spark Thrift server monitor page
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, all

I have create a JIRA ticket about adding a monitor page for Thrift server.

https://issues.apache.org/jira/browse/SPARK-5100

Anyone could review the design doc, and give some advises?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan  6 15:15:14 2015
Return-Path: <dev-return-11027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9DA310324
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 Jan 2015 15:15:14 +0000 (UTC)
Received: (qmail 28009 invoked by uid 500); 6 Jan 2015 15:15:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27933 invoked by uid 500); 6 Jan 2015 15:15:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27770 invoked by uid 99); 6 Jan 2015 15:15:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 15:15:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of francois.garillot@typesafe.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 15:14:40 +0000
Received: by mail-wi0-f173.google.com with SMTP id r20so5555257wiv.6
        for <dev@spark.apache.org>; Tue, 06 Jan 2015 07:14:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=typesafe.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=eqIkJodYK0ewu1fM6tFwIhBtZHvUlLNQ9S579CyjnkE=;
        b=FLs7DlaZuPyv/mjOeHoTtMze/kzC67ipQLW9wRrrtdnVUs9j7WVL+h4JUJZvLS1kjB
         sKssD+6Y02lR/LRsLqHK7pr5MfiWkOKPzaXK0Xs1+KzBaWEB4xQ1SrH0zBX7B2Uwar5K
         t0RtaRueY0FW4zKAu7NzXzJJtFEVU0IAYihjI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=eqIkJodYK0ewu1fM6tFwIhBtZHvUlLNQ9S579CyjnkE=;
        b=TVSDjQtKWwOPw2g7gYDmXhn/gnh2xk+zOQYZMywTYlYtRXIHh/Whme48S19VhSo83t
         C2IicxgQ2n8iG4ZS6LUnpJy/shkig+u33wWVLuphB8YQ7T+mikfUe3FBw0bxzUh3Qf1S
         kpujKeks8IZhGlN1Ry9TXGDTCl9z3kSMHspLYqRt1SIF/Uik+1wxG9eRaeAACI79wGEh
         I3RXM+8jyIa2wodQpH6un5MHDb6t7JWluvo7NqpLfGRna4z/qzUbdRkKuGayS96TFN/2
         s8mUNJ0Z6TFKpvWK8GaWlN0VPxEMjw81PdI1tR+Ee4zZQDiUz0FgDDXpfzRy5Zmwx0Id
         L0xg==
X-Gm-Message-State: ALoCoQlwpFDvQOAVsfzWBwvEc7A2gg5IaGvUgyUhT8IL1/tHQxvlUf3AESePVzP1UpLucoqDkpnR
X-Received: by 10.194.62.76 with SMTP id w12mr193796436wjr.5.1420557279102;
 Tue, 06 Jan 2015 07:14:39 -0800 (PST)
MIME-Version: 1.0
Received: by 10.180.36.135 with HTTP; Tue, 6 Jan 2015 07:14:08 -0800 (PST)
In-Reply-To: <CAMwrk0=4iPBGzKjO6vfftuMgJ+zVJTDez+X36BKyi3KSQym4UQ@mail.gmail.com>
References: <1418929497786.4ba3e417@Nodemailer> <CAMwrk0=4iPBGzKjO6vfftuMgJ+zVJTDez+X36BKyi3KSQym4UQ@mail.gmail.com>
From: =?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois.garillot@typesafe.com>
Date: Tue, 6 Jan 2015 16:14:08 +0100
Message-ID: <CAEg_Zo=_SM5hqTJ3LPrYydFx0Es0Y2g6OL3eDqtsqaX5vDrdYg@mail.gmail.com>
Subject: Re: Spark Streaming Data flow graph
To: Tathagata Das <tathagata.das1565@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba977a401237e050bfd44e8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba977a401237e050bfd44e8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks a LOT for your answer ! I've updated the diagram, at the same
address :
https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf?dl=3D0

I've addressed your more straightforward remarks directly in the diagram. A
couple questions:

- the location of instances (Executor, Master, Driver) is now marked, I
hope I didn't make too many mistakes there, did I ?

- Given that the communication between instances and their members (e.g.
ReceiverSupervisor / ReceivedBlockHandler) is willingly omitted, have I
forgotten any communication channels ?

- I've represented some queues / buffers using a red trapezoid. I'm thus
starting an inventory of queues or buffers, and I'm interested in adding
the 'implicit' ones as well (e.g. jobSets in JobScheduler, which is indexed
by time in ms). I'd be happy with pointers on where to look : ideally I'm
trying to see any place in the data flow where data is sitting idle for any
length of time, waiting to be chunked somehow (whether it's at the RDD or
block level doesn't really matter to me, I'm interested in all types of
'chunking').

Naturally, this is intended to be a developer document exclusively (hence
in particular why I'm not publicising this on the user ML).


On Mon, Jan 5, 2015 at 10:57 PM, Tathagata Das <tathagata.das1565@gmail.com=
>
wrote:

> Hey Fran=C3=A7ois,
>
> Well, at a high-level here is what I thought about the diagram.
>
> - ReceiverSupervisor handles only one Receiver.
> - BlockGenerator is part of ReceiverSupervisor not ReceivedBlockHandler
> - The blocks are inserted in BlockManager and if activated,
> WriteAheadLogManager in parallel, not through BlockManager as the
> diagram seems to imply
> - It would be good to have a clean visual separation of what runs in
> Executor (better term than Worker) and what is in Driver ... Driver
> stuff on left and Executor stuff on right, or vice versa.
>
> More importantly, the word of caution is that all the internal stuff
> like ReceiverBlockHandler, Supervisor, etc are subject to change any
> time as we keep refactoring stuff. So highlighting these internal
> details too much too publicly may lead to future confusion.
>
> TD
>
> On Thu, Dec 18, 2014 at 11:04 AM,  <francois.garillot@typesafe.com> wrote=
:
> > I=E2=80=99ve been trying to produce an updated box diagram to refresh :
> >
> http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tatha=
gatadassparkmeetup20130617/26
> >
> >
> > =E2=80=A6 after the SPARK-3129, and other switches (a surprising number=
 of
> comments still mention NetworkReceiver).
> >
> >
> > Here=E2=80=99s what I have so far:
> > https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf?dl=3D0
> >
> >
> > This is not supposed to respect any particular convention (ER, ORM, =E2=
=80=A6).
> Data flow up to right before RDD creation is in bold arrows, metadata flo=
w
> is in normal width arrows.
> >
> >
> > This diagram is still very much a WIP (see below : todo), but I wanted
> to share it to ask:
> > - what=E2=80=99s wrong ?
> > - what are the glaring omissions ?
> > - how can I make this better (i.e. what should I add first to the
> Todo-list below) ?
> >
> >
> > I=E2=80=99ll be happy to share this (including sources) with whoever as=
ks for it.
> >
> >
> > Todo :
> > - mark private/public classes
> > - mark queues in Receiver, ReceivedBlockHandler, BlockManager
> > - mark type of info on transport : e.g. Actor message, ReceivedBlockInf=
o
> >
> >
> >
> > =E2=80=94
> > Fran=C3=A7ois Garillot
>



--=20
Fran=C3=A7ois Garillot

--047d7ba977a401237e050bfd44e8--

From dev-return-11028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan  6 17:26:33 2015
Return-Path: <dev-return-11028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83D4310B81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 Jan 2015 17:26:33 +0000 (UTC)
Received: (qmail 24711 invoked by uid 500); 6 Jan 2015 17:26:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24634 invoked by uid 500); 6 Jan 2015 17:26:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24622 invoked by uid 99); 6 Jan 2015 17:26:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 17:26:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.173 as permitted sender)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 Jan 2015 17:26:05 +0000
Received: by mail-lb0-f173.google.com with SMTP id z12so19912940lbi.18
        for <dev@spark.apache.org>; Tue, 06 Jan 2015 09:26:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=XXRK5zWmU6ihVHB/PDO4mvP0uqnyJ2EtWfAA55JDWPI=;
        b=U3cbSD3Vi/RZPvcauNM8AiBe4vVevy/zHbs2gz5/NDXNSopi1YDDEo5R3bnLmwc+pb
         p9NmYlFH1szf6u3aSZFSPFtYqVOzT8Tqe9J2zOPr0+fEGaPb8YD+tbt4L6O6sIlwzvCu
         NaW67ZoxpafpWo3CpKsHWM75oDPxObze6sOz3KQqwfJK3l9vWEdzUqfGQIUEcjYi5Jjm
         NZeKKH5BCuCOS2ETGFUD0A43CgQllmI54GtKwQVCFxrEcUw3lEjvuz3AMAQca5xIVT/x
         FSW1NaY/zJYyPVWv5/cxZnFJjKEgA5IZAQxhHMNN3SybnT0qOJLAcMbO0MqdkZXwtI6Z
         aiAw==
X-Gm-Message-State: ALoCoQl9BuvACzxiczac+ctUDyR8bKZIFGskHeeSrVcSx9gguOy5KVbQkV+9EruXjECE+dYG41WN
X-Received: by 10.153.8.132 with SMTP id dk4mr97484499lad.56.1420565164127;
 Tue, 06 Jan 2015 09:26:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Tue, 6 Jan 2015 09:25:43 -0800 (PST)
In-Reply-To: <CAOEPXP51kFdo4axWAU9HMqRpVDGU5J6iny09vRGYovHMj69pHw@mail.gmail.com>
References: <CACdU-dTwCz42WHgKkueJYS6RPgk=N1YZcyRb=_bMiEB6YF7EBQ@mail.gmail.com>
 <CAOEPXP51kFdo4axWAU9HMqRpVDGU5J6iny09vRGYovHMj69pHw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Tue, 6 Jan 2015 09:25:43 -0800
Message-ID: <CACdU-dRBCatTDrpM8X1RVJ9uOTfYO6CVNN3-whSPazuabVVW7Q@mail.gmail.com>
Subject: Re: jenkins redirect down (but jenkins is up!), lots of potential
To: Josh Rosen <rosenville@gmail.com>
Cc: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11348212fd0a6e050bff1941
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11348212fd0a6e050bff1941
Content-Type: text/plain; charset=UTF-8

the regular url is working now, thanks for your patience.

On Mon, Jan 5, 2015 at 2:25 PM, Josh Rosen <rosenville@gmail.com> wrote:

> The pull request builder and SCM-polling builds appear to be working fine,
> but the links in pull request comments won't work because the AMP Lab
> webserver is still down.  In the meantime, though, you can continue to
> access Jenkins through https://hadrian.ist.berkeley.edu/jenkins/
>
> On Mon, Jan 5, 2015 at 10:37 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> UC Berkeley had some major maintenance done this past weekend, and long
>> story short, not everything came back.  our primary webserver's NFS is
>> down
>> and that means we're not serving websites, meaning that the redirect to
>> jenkins is failing.
>>
>> jenkins is still up, and building some jobs, but we will probably see pull
>> request builder failures, and other transient issues.  SCM-polling builds
>> should be fine.
>>
>> there is no ETA on when this will be fixed, but once our
>> amplab.cs.berkeley.edu/jenkins redir is working, i will let everyone
>> know.
>>  i'm trying to get more status updates as they come.
>>
>> i'm really sorry about the inconvenience.
>>
>> shane
>>
>
>

--001a11348212fd0a6e050bff1941--

From dev-return-11029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 03:49:46 2015
Return-Path: <dev-return-11029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6247C1086C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 03:49:46 +0000 (UTC)
Received: (qmail 64212 invoked by uid 500); 7 Jan 2015 03:49:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64130 invoked by uid 500); 7 Jan 2015 03:49:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64117 invoked by uid 99); 7 Jan 2015 03:49:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 03:49:46 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [208.65.144.72] (HELO p01c11o149.mxlogic.net) (208.65.144.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 03:49:39 +0000
Received: from unknown [203.199.118.206] (EHLO VSHINMSHTCAS02.vshodc.lntinfotech.com)
	by p01c11o149.mxlogic.net(mxl_mta-8.2.0-3)
	with ESMTP id ebcaca45.2b2cac20c940.72220.00-568.187532.p01c11o149.mxlogic.net (envelope-from <jeniba.johnson@lntinfotech.com>);
	Tue, 06 Jan 2015 20:49:18 -0700 (MST)
X-MXL-Hash: 54acacbe3cb0a023-c0cdd43a99074a042e439a86cc03d5acd86908e8
Received: from unknown [203.199.118.206] (EHLO VSHINMSHTCAS02.vshodc.lntinfotech.com)
	by p01c11o149.mxlogic.net(mxl_mta-8.2.0-3) over TLS secured channel
	with ESMTP id a1caca45.0.72157.00-343.186465.p01c11o149.mxlogic.net (envelope-from <jeniba.johnson@lntinfotech.com>);
	Tue, 06 Jan 2015 20:46:36 -0700 (MST)
X-MXL-Hash: 54acac1c27ee904f-dee567edf5fcd80d8add3de676a2eab7453d1662
Received: from vshinmsmbx01.vshodc.lntinfotech.com ([172.17.24.117]) by
 VSHINMSHTCAS02.vshodc.lntinfotech.com ([172.17.24.114]) with mapi; Wed, 7 Jan
 2015 09:18:27 +0530
From: Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>
To: "Hari Shreedharan (hshreedharan@cloudera.com)" <hshreedharan@cloudera.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Date: Wed, 7 Jan 2015 09:16:48 +0530
Subject: Reading Data  Using TextFileStream
Thread-Topic: Reading Data  Using TextFileStream
Thread-Index: AdAqLJAGrEaJjtIlRyabXT4OjP//bQ==
Message-ID: <FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DB@VSHINMSMBX01.vshodc.lntinfotech.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DBVSHINMSMBX01v_"
MIME-Version: 1.0
X-AnalysisOut: [v=2.1 cv=WrY/MLvv c=1 sm=1 tr=0 a=u6+EFOFP01MjBJ/gkmoqjg==]
X-AnalysisOut: [:117 a=u6+EFOFP01MjBJ/gkmoqjg==:17 a=BLceEmwcHowA:10 a=tP0]
X-AnalysisOut: [R5P8-AAAA:8 a=YlVTAMxIAAAA:8 a=YNv0rlydsVwA:10 a=sBLtT7Um3]
X-AnalysisOut: [ZwyLn7_AQIA:9 a=CjuIK1q_8ugA:10 a=yMhMjlubAAAA:8 a=SSmOFEA]
X-AnalysisOut: [CAAAA:8 a=KVtNuOdTUsWHqk5JGsUA:9 a=6X2lThY6k2eDKS9j:21 a=g]
X-AnalysisOut: [KO2Hq4RSVkA:10 a=UiCQ7L4-1S4A:10 a=hTZeC7Yk6K0A:10 a=frz4A]
X-AnalysisOut: [uCg-hUA:10]
X-Spam: [F=0.5000000000; CM=0.500; MH=0.500(2015010636); S=0.200(2014051901)]
X-MAIL-FROM: <jeniba.johnson@lntinfotech.com>
X-SOURCE-IP: [203.199.118.206]
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DBVSHINMSMBX01v_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable


Hi Hari,

Iam trying to read data from a file which is stored in HDFS. Using Flume th=
e data is tailed and stored in HDFS.
Now I want to read this data using TextFileStream. Using the below mentione=
d code Iam not able to fetch the
Data  from a file which is stored in HDFS. Can anyone help me with this iss=
ue.

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import com.google.common.collect.Lists;

import java.util.Arrays;
import java.util.List;
import java.util.regex.Pattern;

public final class Test1 {
  public static void main(String[] args) throws Exception {

    SparkConf sparkConf =3D new SparkConf().setAppName("JavaWordCount");
    JavaStreamingContext ssc =3D new JavaStreamingContext("local[4]","JavaW=
ordCount",  new Duration(20000));

    JavaDStream<String> textStream =3D ssc.textFileStream("user/huser/user/=
huser/flume");//Data Directory Path in HDFS


    JavaDStream<String> suspectedStream =3D textStream.flatMap(new FlatMapF=
unction<String,String>()
     {
                            public Iterable<String> call(String line) throw=
s Exception {

                            //return Arrays.asList(line.toString().toString=
());
                           return  Lists.newArrayList(line.toString().toStr=
ing());
                             }
     });


    suspectedStream.foreach(new Function<JavaRDD<String>,Void>(){

        public Void call(JavaRDD<String> rdd) throws Exception {
        List<String> output =3D rdd.collect();
        System.out.println("Sentences Collected from Flume " + output);
               return  null;
        }
        });

    suspectedStream.print();

    System.out.println("Welcome TO Flume Streaming");
    ssc.start();
    ssc.awaitTermination();
  }

}

The command I use is:
./bin/spark-submit --verbose --jars lib/spark-examples-1.1.0-hadoop1.0.4.ja=
r,lib/mysql.jar --master local[*] --deploy-mode client --class xyz.Test1 bi=
n/filestream3.jar





Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential =
or privileged information for the intended recipient(s). Unintended recipie=
nts are prohibited from taking action on the basis of information in this e=
-mail and using or disseminating the information, and must notify the sende=
r and delete it from their system. L&T Infotech will not accept responsibil=
ity or liability for the accuracy or completeness of, or the presence of an=
y virus or disabling code in this e-mail"

--_000_FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DBVSHINMSMBX01v_--

From dev-return-11030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 05:59:13 2015
Return-Path: <dev-return-11030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D0A210B3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 05:59:13 +0000 (UTC)
Received: (qmail 26865 invoked by uid 500); 7 Jan 2015 05:59:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26807 invoked by uid 500); 7 Jan 2015 05:59:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26795 invoked by uid 99); 7 Jan 2015 05:59:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 05:59:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 05:58:44 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so2568841pdi.3
        for <dev@spark.apache.org>; Tue, 06 Jan 2015 21:57:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=sTrxXiUB9iNHd7Cowy0keFAWkyB2jZxRCfuUgxmGdXk=;
        b=ZacOy4Ucpj1be6RvgdAAYAbwGKX97dWMVAZwt3yHy0evguZYfR3+HsjHOv9ZoARZoV
         fmgtwp0YJnojK649V/Q3dMQasCEmejPAIWEwPkvqLXQ32WL1JWzmdzIwvpHFq3oa+btu
         yaInW9Bc0qu2+G4AId4gLdnURfLOVDLVvAeP1iwL9XRgcPw9HUleTQnBiPviAjUJRbRd
         +AQmQ48d9IyI3bOyn2uKtqJpDqlZTIAX03Wjzp2ZKYTr6Lh9YBxE88mGEnsPsNHam6bF
         8O7QWl7ruo1TpXOUXRT9WqjCPAj5nbuvHNNndZAtwgy0qFaLjkYciHxDOmmulAcKr57s
         TN2Q==
X-Received: by 10.70.54.37 with SMTP id g5mr2024321pdp.71.1420610232858;
        Tue, 06 Jan 2015 21:57:12 -0800 (PST)
Received: from [192.168.99.2] ([198.176.48.184])
        by mx.google.com with ESMTPSA id oa8sm584637pdb.84.2015.01.06.21.57.10
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 06 Jan 2015 21:57:12 -0800 (PST)
Message-ID: <54ACCAB4.1080504@gmail.com>
Date: Wed, 07 Jan 2015 13:57:08 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Yi Tian <tianyi.asiainfo@gmail.com>, dev@spark.apache.org
Subject: Re: [SPARK-5100][SQL] Spark Thrift server monitor page
References: <54ABAA06.2060908@gmail.com>
In-Reply-To: <54ABAA06.2060908@gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Talked with Yi offline, personally I think this feature is pretty 
useful, and the design makes sense, and he's already got a running 
prototype.

Yi, would you mind to open a PR for this? Thanks!

Cheng

On 1/6/15 5:25 PM, Yi Tian wrote:
> Hi, all
>
> I have create a JIRA ticket about adding a monitor page for Thrift 
> server.
>
> https://issues.apache.org/jira/browse/SPARK-5100
>
> Anyone could review the design doc, and give some advises?
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 06:42:50 2015
Return-Path: <dev-return-11031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 84F0210BF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 06:42:50 +0000 (UTC)
Received: (qmail 83527 invoked by uid 500); 7 Jan 2015 06:42:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83454 invoked by uid 500); 7 Jan 2015 06:42:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83443 invoked by uid 99); 7 Jan 2015 06:42:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 06:42:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 06:42:24 +0000
Received: by mail-lb0-f169.google.com with SMTP id p9so569074lbv.0
        for <dev@spark.apache.org>; Tue, 06 Jan 2015 22:41:18 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=V+Sx7G5hHEVEtZYf8z5mcX1pYvDVExcDWxbN250pgcA=;
        b=AcgHJOcLGf87rz08X0kASlO/iWFp1bDmG5yKzaJbvoQSgpoNzDJaxj713ygisNGmlO
         Bf/ObbEeKLViY07ajyek1J+K6vDYhjTBlt+eUnPh+Olno8sHCvkhu80lI0dk7wfO0c6r
         gEWWcyJaMIQEQCxOWfB2MnKjPyWFar+Uc97C2TRq2mD1++oYkHNScc3/L2m2yEGyDnMO
         WN2V6w7gTx8RobZNIsQeG6s9UR0T8AmdeF0DYrnJ6RPn/Htb6bqalTB+bQynk1jsZiYe
         F1Yo/YGrGNNXMJFRxGjGfwlQCR0WxemsB+1QV/aKKVjGExQ7Yt5oqDTZImqkPV/ijei2
         fkjQ==
X-Gm-Message-State: ALoCoQlllVaV5PnPGyjUdFYAaP47Bz32RAgMSphjsvTPhllQHcKsCNfB7Uthy8hHqTHQp02hIbmY
MIME-Version: 1.0
X-Received: by 10.152.21.134 with SMTP id v6mr1500542lae.13.1420612878129;
 Tue, 06 Jan 2015 22:41:18 -0800 (PST)
Received: by 10.152.114.10 with HTTP; Tue, 6 Jan 2015 22:41:18 -0800 (PST)
In-Reply-To: <FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DB@VSHINMSMBX01.vshodc.lntinfotech.com>
References: <FCD65A279D2E8F418B46C97F0FF7A00C1E361E39DB@VSHINMSMBX01.vshodc.lntinfotech.com>
Date: Wed, 7 Jan 2015 12:11:18 +0530
Message-ID: <CAHUQ+_YbZf99v-JceFkBfhj_V7MvKfqAd+JGiz9s8rcH7sw7vw@mail.gmail.com>
Subject: Re: Reading Data Using TextFileStream
From: Akhil Das <akhil@sigmoidanalytics.com>
To: Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>
Cc: "Hari Shreedharan (hshreedharan@cloudera.com)" <hshreedharan@cloudera.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b74ef6f1d6050c0a35f2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b74ef6f1d6050c0a35f2
Content-Type: text/plain; charset=UTF-8

I think you need to start your streaming job, then put the files there to
get them read. textFileStream doesn't read the existing files i believe.

Also are you sure the path is not the following? (no missing / in the
beginning?)

JavaDStream<String> textStream = ssc.textFileStream("/user/
huser/user/huser/flume");


Thanks
Best Regards

On Wed, Jan 7, 2015 at 9:16 AM, Jeniba Johnson <
Jeniba.Johnson@lntinfotech.com> wrote:

>
> Hi Hari,
>
> Iam trying to read data from a file which is stored in HDFS. Using Flume
> the data is tailed and stored in HDFS.
> Now I want to read this data using TextFileStream. Using the below
> mentioned code Iam not able to fetch the
> Data  from a file which is stored in HDFS. Can anyone help me with this
> issue.
>
> import org.apache.spark.SparkConf;
> import org.apache.spark.api.java.JavaRDD;
> import org.apache.spark.api.java.function.FlatMapFunction;
> import org.apache.spark.api.java.function.Function;
> import org.apache.spark.streaming.Duration;
> import org.apache.spark.streaming.api.java.JavaDStream;
> import org.apache.spark.streaming.api.java.JavaStreamingContext;
>
> import com.google.common.collect.Lists;
>
> import java.util.Arrays;
> import java.util.List;
> import java.util.regex.Pattern;
>
> public final class Test1 {
>   public static void main(String[] args) throws Exception {
>
>     SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount");
>     JavaStreamingContext ssc = new
> JavaStreamingContext("local[4]","JavaWordCount",  new Duration(20000));
>
>     JavaDStream<String> textStream =
> ssc.textFileStream("user/huser/user/huser/flume");//Data Directory Path in
> HDFS
>
>
>     JavaDStream<String> suspectedStream = textStream.flatMap(new
> FlatMapFunction<String,String>()
>      {
>                             public Iterable<String> call(String line)
> throws Exception {
>
>                             //return
> Arrays.asList(line.toString().toString());
>                            return
> Lists.newArrayList(line.toString().toString());
>                              }
>      });
>
>
>     suspectedStream.foreach(new Function<JavaRDD<String>,Void>(){
>
>         public Void call(JavaRDD<String> rdd) throws Exception {
>         List<String> output = rdd.collect();
>         System.out.println("Sentences Collected from Flume " + output);
>                return  null;
>         }
>         });
>
>     suspectedStream.print();
>
>     System.out.println("Welcome TO Flume Streaming");
>     ssc.start();
>     ssc.awaitTermination();
>   }
>
> }
>
> The command I use is:
> ./bin/spark-submit --verbose --jars
> lib/spark-examples-1.1.0-hadoop1.0.4.jar,lib/mysql.jar --master local[*]
> --deploy-mode client --class xyz.Test1 bin/filestream3.jar
>
>
>
>
>
> Regards,
> Jeniba Johnson
>
>
> ________________________________
> The contents of this e-mail and any attachment(s) may contain confidential
> or privileged information for the intended recipient(s). Unintended
> recipients are prohibited from taking action on the basis of information in
> this e-mail and using or disseminating the information, and must notify the
> sender and delete it from their system. L&T Infotech will not accept
> responsibility or liability for the accuracy or completeness of, or the
> presence of any virus or disabling code in this e-mail"
>

--089e0158b74ef6f1d6050c0a35f2--

From dev-return-11032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 08:29:03 2015
Return-Path: <dev-return-11032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50ADA10F03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 08:29:03 +0000 (UTC)
Received: (qmail 44100 invoked by uid 500); 7 Jan 2015 08:29:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44018 invoked by uid 500); 7 Jan 2015 08:29:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44007 invoked by uid 99); 7 Jan 2015 08:29:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 08:29:00 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jongwook@nyu.edu does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 08:28:35 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 4CA08FBFCB5
	for <dev@spark.apache.org>; Wed,  7 Jan 2015 00:27:33 -0800 (PST)
Date: Wed, 7 Jan 2015 01:27:32 -0700 (MST)
From: Jong Wook Kim <jongwook@nyu.edu>
To: dev@spark.apache.org
Message-ID: <1420619252793-10023.post@n3.nabble.com>
In-Reply-To: <54986C12.3030806@ccri.com>
References: <54945BD9.7050109@ccri.com> <CAMAsSdKfJYRTgY2j=Th0KCMG_DqPvxeTxyudVht4=hpYiRmXgg@mail.gmail.com> <54986C12.3030806@ccri.com>
Subject: Re: spark-yarn_2.10 1.2.0 artifacts
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think that's the case. 

spark-yarn contains `org.apache.spark.deploy.yarn` package, whereas
spark-network-yarn contains `org.apache.spark.network.yarn`, and they do
different things.

The former contains codes for deploying Spark applications to YARN cluster,
and called when running `spark-submit --master yarn...` and the latter
contains a shuffle service that runs in NM process.

While spark-yarn is not a programming interface and normally used only by
Spark toolchain, I need it as a library dependency, for programatically
launching spark application on YARN. 

spark-yarn.jar does appear in '/yarn/stable/target` when building Spark
1.2.0 from source code and I'm manually adding that to my maven repository.
But I'd like to know why it is excluded from the maven repo.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-yarn-2-10-1-2-0-artifacts-tp9853p10023.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 08:33:45 2015
Return-Path: <dev-return-11033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C58F10F1B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 08:33:45 +0000 (UTC)
Received: (qmail 51634 invoked by uid 500); 7 Jan 2015 08:33:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51545 invoked by uid 500); 7 Jan 2015 08:33:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51528 invoked by uid 99); 7 Jan 2015 08:33:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 08:33:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 08:33:40 +0000
Received: by mail-ob0-f177.google.com with SMTP id va2so2038347obc.8
        for <dev@spark.apache.org>; Wed, 07 Jan 2015 00:31:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KkUMb8Sc44EQhMaKWtKJiJ5rWHrWBNB/36f8nOKLz70=;
        b=vT62VhufIP+liZb+fviCBEbwz8e8WR2fsMXknzQP/NNwGoHF46dF5e60zQvS/jYbub
         VlCGFFWXBoooFXORWQqR18viKy3n2nyoPE9NIIqNyGhSyeejyUBOmcAx0be0L80AFVGv
         jzwhfN95oUEmIs7WAabUYSEd9RofYwRkAk8RmCNmXMJaeyEgNwGjIzc2A3P9N+lVEfIM
         KgKBZxwKAgzvj+Kqe60vW8vitUAXtZbYUp8S7yEc0LQUmuBfl8pQ0nM4MbvzhWgpWht0
         FEKDW+AfNocobdw87/k3OpJF7NOSul6SKGh7gBXcCGAa+68nKoIcc3d4jYqhTtnAN2hu
         Oh2w==
MIME-Version: 1.0
X-Received: by 10.60.133.141 with SMTP id pc13mr1175768oeb.68.1420619509420;
 Wed, 07 Jan 2015 00:31:49 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 7 Jan 2015 00:31:49 -0800 (PST)
In-Reply-To: <CA+-p3AF4Vsu0OEE8VX1jUWk-dUWs_5HPp1nFTFRc28qEeMyJwA@mail.gmail.com>
References: <CA+-p3AF4Vsu0OEE8VX1jUWk-dUWs_5HPp1nFTFRc28qEeMyJwA@mail.gmail.com>
Date: Wed, 7 Jan 2015 00:31:49 -0800
Message-ID: <CABPQxsu7BpD5di-27fBoDhCdxfr4aEZXz=2DimSU2Ee6fDioUQ@mail.gmail.com>
Subject: Re: Hang on Executor classloader lookup for the remote REPL URL classloader
From: Patrick Wendell <pwendell@gmail.com>
To: Andrew Ash <andrew@andrewash.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

So the executors in Spark will fetch classes from the driver node for
classes defined in the repl from an HTTP server on the driver. Is this
happening in the context of a repl session? Also, is it deterministic
or does it happen only periodically?

The reason all of the other threads are hanging is that there is a
global lock around classloading, so they all queue up.

Could you attach the full stack trace from the driver? Is it possible
that something in the network is blocking the transfer of bytes
between these two processes? Based on the stack trace it looks like it
sent an HTTP request and is waiting on the result back from the
driver.

One thing to check is to verify that the TCP connection between them
used for the repl class server is still alive from the vantage point
of both the executor and driver nodes. Another thing to try would be
to temporarily open up any firewalls that are on the nodes or in the
network and see if this makes the problem go away (to isolate it to an
exogenous-to-Spark network issue).

- Patrick

On Wed, Aug 20, 2014 at 11:35 PM, Andrew Ash <andrew@andrewash.com> wrote:
> Hi Spark devs,
>
> I'm seeing a stacktrace where the classloader that reads from the REPL is
> hung, and blocking all progress on that executor.  Below is that hung
> thread's stacktrace, and also the stacktrace of another hung thread.
>
> I thought maybe there was an issue with the REPL's JVM on the other side,
> but didn't see anything useful in that stacktrace either.
>
> Any ideas what I should be looking for?
>
> Thanks!
> Andrew
>
>
> "Executor task launch worker-0" daemon prio=10 tid=0x00007f780c208000
> nid=0x6ae9 runnable [0x00007f78c2eeb000]
>    java.lang.Thread.State: RUNNABLE
>         at java.net.SocketInputStream.socketRead0(Native Method)
>         at java.net.SocketInputStream.read(SocketInputStream.java:152)
>         at java.net.SocketInputStream.read(SocketInputStream.java:122)
>         at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
>         at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
>         at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
>         - locked <0x00007f7e13ea9560> (a java.io.BufferedInputStream)
>         at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
>         at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
>         at
> sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
>         - locked <0x00007f7e13e9eeb0> (a
> sun.net.www.protocol.http.HttpURLConnection)
>         at java.net.URL.openStream(URL.java:1037)
>         at
> org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:86)
>         at
> org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:63)
>         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>         - locked <0x00007f7fc9018980> (a
> org.apache.spark.repl.ExecutorClassLoader)
>         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>         at java.lang.Class.forName0(Native Method)
>         at java.lang.Class.forName(Class.java:270)
>         at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
>         at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:82)
>         at
> org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
>         at
> org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
>         at
> org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
>         at
> org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
>         at
> org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
>         at
> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
>         at
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
>         at
> org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
>         at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
>         at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
>
> And the other threads are stuck on the Class.forName0() method too:
>
> "Executor task launch worker-4" daemon prio=10 tid=0x00007f780c20f000
> nid=0x6aed waiting for monitor entry [0x00007f78c2ae8000]
>    java.lang.Thread.State: BLOCKED (on object monitor)
>         at java.lang.Class.forName0(Native Method)
>         at java.lang.Class.forName(Class.java:270)
>         at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
>         at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:79)
>         at
> org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
>         at
> org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
>         at
> org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
>         at
> org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
>         at
> org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
>         at
> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
>         at
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
>         at
> org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
>         at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
>         at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>
> asdf

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 15:15:25 2015
Return-Path: <dev-return-11034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFDE017BE9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 15:15:25 +0000 (UTC)
Received: (qmail 15163 invoked by uid 500); 7 Jan 2015 15:15:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15095 invoked by uid 500); 7 Jan 2015 15:15:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12444 invoked by uid 99); 7 Jan 2015 15:15:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 15:15:22 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gen.tang86@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 15:15:17 +0000
Received: by mail-wg0-f51.google.com with SMTP id x12so1329781wgg.24;
        Wed, 07 Jan 2015 07:14:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ewgl7cua+I2SlZppr31SHp5435O9NLG1/6h5jq4QQsQ=;
        b=WOS8gOz/w3UYjBEvLZCC7ibzTf35teCPpgkWtqUc+MJinu1qREWjFqYCcWcYv7ENHR
         R0ZIG5ZL1WH11neYNUj1G3WwH3YlXeA6+BMQ4p1pvpbB2k9So65BcCiSXRXpoZvGXdcI
         INOYRCRqA6U/M1OwnfA5kQS9N2LqTgeZBXZkI0kgLzCXxF92qaMSC8kqjvM9rylUktMl
         Ps7Kt/BlcqmPOTwjApUDoUAGKvaX4nNFhFWcNWQLkhAp3RgoiKZJNXeAni53J7KZunyz
         5CQNYePO6UHS/t/2ER3B8h5/+DHokSiqNhuHBqPZpb9JJNCUUOjvfwBnuGfyiAqXwkGT
         OGCA==
MIME-Version: 1.0
X-Received: by 10.194.185.243 with SMTP id ff19mr7103337wjc.126.1420643650579;
 Wed, 07 Jan 2015 07:14:10 -0800 (PST)
Received: by 10.180.102.35 with HTTP; Wed, 7 Jan 2015 07:14:10 -0800 (PST)
Date: Wed, 7 Jan 2015 16:14:10 +0100
Message-ID: <CACRMEukv1iH9c9nbL2=iDG9BzFRXuE3QYn2GLUjZckXY8x0zXA@mail.gmail.com>
Subject: Spark on teradata?
From: gen tang <gen.tang86@gmail.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb03f1225347d050c1160d4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03f1225347d050c1160d4
Content-Type: text/plain; charset=UTF-8

Hi,

I have a stupid question:
Is it possible to use spark on Teradata data warehouse, please? I read some
news on internet which say yes. However, I didn't find any example about
this issue

Thanks in advance.

Cheers
Gen

--047d7bb03f1225347d050c1160d4--

From dev-return-11035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 23:06:12 2015
Return-Path: <dev-return-11035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69AC6101C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 23:06:12 +0000 (UTC)
Received: (qmail 51489 invoked by uid 500); 7 Jan 2015 23:06:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51413 invoked by uid 500); 7 Jan 2015 23:06:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51401 invoked by uid 99); 7 Jan 2015 23:06:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 23:06:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 23:06:04 +0000
Received: by mail-oi0-f54.google.com with SMTP id u20so5026783oif.13
        for <dev@spark.apache.org>; Wed, 07 Jan 2015 15:04:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=fCnsVZV+311TzOtpqz+g0W2akyXiMyh0Gi5twceKmfk=;
        b=RQ5pTlipCuVMx3FAyfnwUJC997MIyYrl8WvLqEvKwIh02f5Ho8XDz4qghM8F3aqQX5
         5iPCxug2G67IJMAosU+c6Gey+/fMqpLO+edLLLyygBdAHlmdc+buW3k1Qwb/dx0U4l4x
         1wCuiHFDKSe0SA/gwrqQ9KJroSv7XlUaHkTry7MJtZqJwGJZi6FRDHD//oeqE81Ubro+
         ixzib/pjjcatjavtXUjGv1ruVkgOrh+LBnzKafGvX9DHi/ESmb/CklZiLorvs2ofB0bA
         Q+rvG/aFC7Kq6nkp4m8IgZ/EKu/xCSJykSE76MjgSabL6v32agAbYCY7MBRuILM5lU5u
         Ykwg==
X-Received: by 10.60.52.101 with SMTP id s5mr3673665oeo.33.1420671899120; Wed,
 07 Jan 2015 15:04:59 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Wed, 7 Jan 2015 15:04:38 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Wed, 7 Jan 2015 15:04:38 -0800
Message-ID: <CAJc_sy+pc9pTMS-avfSfGM8jyFU6Q_evta5Zg7kAgQmKTq3d6w@mail.gmail.com>
Subject: Missing Catalyst API docs
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11330d54e3bce6050c17f315
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11330d54e3bce6050c17f315
Content-Type: text/plain; charset=UTF-8

Gents,

It looks like some of the Catalyst classes' API docs are missing. For
instance, the Expression class, referred to by the SchemaRDD docs seems to
be missing. (See here:
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD
)

Is this intended or is it due to a failure in the doc creation process?

Alex

--001a11330d54e3bce6050c17f315--

From dev-return-11036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan  7 23:15:22 2015
Return-Path: <dev-return-11036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 376D210242
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  7 Jan 2015 23:15:22 +0000 (UTC)
Received: (qmail 70379 invoked by uid 500); 7 Jan 2015 23:15:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70237 invoked by uid 500); 7 Jan 2015 23:15:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70220 invoked by uid 99); 7 Jan 2015 23:15:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 23:15:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 Jan 2015 23:14:53 +0000
Received: by mail-qa0-f42.google.com with SMTP id n8so4953612qaq.1
        for <dev@spark.apache.org>; Wed, 07 Jan 2015 15:13:46 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=tr3I7wwfB4QOSnymunPzh/k/onzmeKl96J2BjqCV7CA=;
        b=XxS3wUxm+THGC6P5QgXqb0spleJwsqBMZxPAd5aEFiCcwkvZkBh2SVJY6wTViybnFi
         KJA/wYo0LWZGcepiOU3Twn9YZA4ca7EXXu6Dy3d29PO/cn1KUplGoiBGpjQUtWpZG5Af
         rExAI9ttl8O9FGquvvU//iKyVm0YnHwjTROVSZ/EQNr9p2JUljRNt1wlC2xrySZ3QXAB
         iIWMJSoltfVqQq5TLT+uv2pfTVP86knbttF0p1p6eXOR/zTCT2HfrbIF874claiHbaLZ
         tzPXsUWieWuzzV3zFat25DOprQoJD8I3vWXVVHe4d1KFcbn86uo6KSs1ZstA/7scF2vQ
         wnAA==
X-Gm-Message-State: ALoCoQnOjqrMeulV/rK++N4worlSFu0jtDW8bOna2l5LsO2al+vyOfMB2F06vCkICgP8MnMZEkjc
X-Received: by 10.224.51.11 with SMTP id b11mr9256531qag.43.1420672426096;
 Wed, 07 Jan 2015 15:13:46 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 7 Jan 2015 15:13:24 -0800 (PST)
In-Reply-To: <CAJc_sy+pc9pTMS-avfSfGM8jyFU6Q_evta5Zg7kAgQmKTq3d6w@mail.gmail.com>
References: <CAJc_sy+pc9pTMS-avfSfGM8jyFU6Q_evta5Zg7kAgQmKTq3d6w@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 7 Jan 2015 15:13:24 -0800
Message-ID: <CAPh_B=ZbJ-iyE=amJPJhYbXrWY_xAnCUUj+A4Ad4CfHi4K5_uA@mail.gmail.com>
Subject: Re: Missing Catalyst API docs
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdcad4e4cd1ed050c181314
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdcad4e4cd1ed050c181314
Content-Type: text/plain; charset=UTF-8

I'm in the middle of revamping the SchemaRDD public API and in 1.3, we will
have a public, documented version of the expression library. The Catalyst
expression library will remain hidden.

You can track it with this ticket:
https://issues.apache.org/jira/browse/SPARK-5097



On Wed, Jan 7, 2015 at 3:04 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Gents,
>
> It looks like some of the Catalyst classes' API docs are missing. For
> instance, the Expression class, referred to by the SchemaRDD docs seems to
> be missing. (See here:
>
> http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD
> )
>
> Is this intended or is it due to a failure in the doc creation process?
>
> Alex
>

--047d7bdcad4e4cd1ed050c181314--

From dev-return-11037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 02:02:09 2015
Return-Path: <dev-return-11037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5AB7910898
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 02:02:09 +0000 (UTC)
Received: (qmail 84089 invoked by uid 500); 8 Jan 2015 02:02:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84010 invoked by uid 500); 8 Jan 2015 02:02:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83988 invoked by uid 99); 8 Jan 2015 02:02:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 02:02:08 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xuelincao2014@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 02:02:03 +0000
Received: by mail-qg0-f47.google.com with SMTP id q108so333851qgd.6
        for <dev@spark.apache.org>; Wed, 07 Jan 2015 18:01:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=NhNpZnO/Xw0h3rDmR20KWY7s/hScfP58C/MfMzqsr3M=;
        b=DY7aUZGYT8mt8bLdSYyZ95/oGI9qiLE8r2FfQbRow843oDU1RoVKNT98q9qARJM6we
         p1UqsCcGehXJe0mPmC7JvtIWHMMj330f5zn6iIRNVUtRzxPbB8l3alun3yfx0oerPE93
         8+9zoABg+4XFFA7pd+aVN4JIyctxS5k5bJA5r8CX1PLrZX9i6rEPgKlvJWmFXNaotKzj
         vxp6bgdua9iDeewbwmD1PB1/arAn6c2YvaCeKPEGyaaeXSPfTNClGgfAjbDuwslklwLh
         Dvw3yFqrtUwNCq7Qfc/cWZKLays+9w4ImyyXfQoKabil/MLMd2b143cfYjQjXcVKx0AH
         U8Vg==
MIME-Version: 1.0
X-Received: by 10.224.93.6 with SMTP id t6mr10432958qam.93.1420682502873; Wed,
 07 Jan 2015 18:01:42 -0800 (PST)
Received: by 10.140.81.42 with HTTP; Wed, 7 Jan 2015 18:01:42 -0800 (PST)
In-Reply-To: <CABjPPTTt43W7zoUHCFx3bXOKXS7K_QOgiBMz3S10HmU1MMJd2g@mail.gmail.com>
References: <CABjPPTTt43W7zoUHCFx3bXOKXS7K_QOgiBMz3S10HmU1MMJd2g@mail.gmail.com>
Date: Thu, 8 Jan 2015 10:01:42 +0800
Message-ID: <CABjPPTTLQ-j55+pDhvjCSiobcSL+5-JMfXdUCfNVvAfi5Y67Ag@mail.gmail.com>
Subject: Fwd: When will spark support "push" style shuffle?
From: =?UTF-8?B?5pu56Zuq5p6X?= <xuelincao2014@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01495028ec2952050c1a6b59
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01495028ec2952050c1a6b59
Content-Type: text/plain; charset=UTF-8

Hi,

      I've heard a lot of complain about spark's "pull" style shuffle. Is
there any plan to support "push" style shuffle in the near future?

      Currently, the shuffle phase must be completed before the next stage
starts. While, it is said, in Impala, the shuffled data is "streamed" to
the next stage handler, which greatly saves time. Will spark support this
mechanism one day?

Thanks

--089e01495028ec2952050c1a6b59--

From dev-return-11038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 05:06:31 2015
Return-Path: <dev-return-11038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43A3410C67
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 05:06:31 +0000 (UTC)
Received: (qmail 35178 invoked by uid 500); 8 Jan 2015 05:06:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35099 invoked by uid 500); 8 Jan 2015 05:06:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35082 invoked by uid 99); 8 Jan 2015 05:06:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 05:06:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 05:06:04 +0000
Received: by mail-oi0-f52.google.com with SMTP id a3so5985536oib.11
        for <dev@spark.apache.org>; Wed, 07 Jan 2015 21:06:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=aYYglYsgkxM48MzkyBRX5R1kXUO/xiYWhM/9YJh+9+w=;
        b=PdKfqS+VfVinVdqxXp7bXvN9LdpBHdbaiHoT5a2lIGxeq8BzGOdISV+TAPhlAadJnD
         6NujwdbaxAhbe+ZBXCmIgmhi5Eaoo5D1zhohL+CSmPEclGDX4nULVuEVmW1siX7QfbP/
         k2xDsFTmxnNx3ZqwPHd902mdzlf1mC7G1MfTezQrHopoE+UUA2hfqoGhvw73nTuzUnmI
         JHbeChIlyTxNagCV6vHps73G8F1mXHki2GO38RaP8R6y0PTUkB/JE7V3bYBJmCQ3Dmcc
         nhdWS935YdTFMl1asbaesga9Iu1RUBtjGKPESCuvq6rzYSMBTeh50ZONVmsPzvRAobUP
         mRXw==
MIME-Version: 1.0
X-Received: by 10.182.33.138 with SMTP id r10mr4400687obi.67.1420693563107;
 Wed, 07 Jan 2015 21:06:03 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 7 Jan 2015 21:06:03 -0800 (PST)
In-Reply-To: <CABjPPTTLQ-j55+pDhvjCSiobcSL+5-JMfXdUCfNVvAfi5Y67Ag@mail.gmail.com>
References: <CABjPPTTt43W7zoUHCFx3bXOKXS7K_QOgiBMz3S10HmU1MMJd2g@mail.gmail.com>
	<CABjPPTTLQ-j55+pDhvjCSiobcSL+5-JMfXdUCfNVvAfi5Y67Ag@mail.gmail.com>
Date: Wed, 7 Jan 2015 21:06:03 -0800
Message-ID: <CABPQxsvKHfmYe7HWcCsecp6CLOjk-cgCFQBTOtQ5+M1mm7_snQ@mail.gmail.com>
Subject: Re: When will spark support "push" style shuffle?
From: Patrick Wendell <pwendell@gmail.com>
To: =?UTF-8?B?5pu56Zuq5p6X?= <xuelincao2014@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

This question is conflating a few different concepts. I think the main
question is whether Spark will have a shuffle implementation that
streams data rather than persisting it to disk/cache as a buffer.
Spark currently decouples the shuffle write from the read using
disk/OS cache as a buffer. The two benefits of this approach this are
that it allows intra-query fault tolerance and it makes it easier to
elastically scale and reschedule work within a job. We consider these
to be design requirements (think about jobs that run for several hours
on hundreds of machines). Impala, and similar systems like dremel and
f1, not offer fault tolerance within a query at present. They also
require gang scheduling the entire set of resources that will exist
for the duration of a query.

A secondary question is whether our shuffle should have a barrier or
not. Spark's shuffle currently has a hard barrier between map and
reduce stages. We haven't seen really strong evidence that removing
the barrier is a net win. It can help the performance of a single job
(modestly), but in the a multi-tenant workload, it leads to poor
utilization since you have a lot of reduce tasks that are taking up
slots waiting for mappers to finish. Many large scale users of
Map/Reduce disable this feature in production clusters for that
reason. Thus, we haven't seen compelling evidence for removing the
barrier at this point, given the complexity of doing so.

It is possible that future versions of Spark will support push-based
shuffles, potentially in a mode that remove some of Spark's fault
tolerance properties. But there are many other things we can still
optimize about the shuffle that would likely come before this.

- Patrick

On Wed, Jan 7, 2015 at 6:01 PM, =E6=9B=B9=E9=9B=AA=E6=9E=97 <xuelincao2014@=
gmail.com> wrote:
> Hi,
>
>       I've heard a lot of complain about spark's "pull" style shuffle. Is
> there any plan to support "push" style shuffle in the near future?
>
>       Currently, the shuffle phase must be completed before the next stag=
e
> starts. While, it is said, in Impala, the shuffled data is "streamed" to
> the next stage handler, which greatly saves time. Will spark support this
> mechanism one day?
>
> Thanks

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 05:40:55 2015
Return-Path: <dev-return-11039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48FD310CE2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 05:40:55 +0000 (UTC)
Received: (qmail 86749 invoked by uid 500); 8 Jan 2015 05:40:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86671 invoked by uid 500); 8 Jan 2015 05:40:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 70911 invoked by uid 99); 8 Jan 2015 05:22:05 -0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prakash.indu@gmail.com designates 209.85.214.179 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=GHSv+rnrbx+XLyL7CKrgHnvpgr9a+CE493Ni2bPQUkU=;
        b=c7jO6lemp8YaJPwnAbpSX61VR+4qd4awSwjB0P0i0ni7GALwUAIv+yx+W12vxH1fnu
         DO4pFHlLqyUB/GvnJmahIHaBV7vGJs5qfrule0ec4+FQxJeVU81dnjE/UkSd+Z7pe/OA
         kxkkPS2rqXixb796RV8wxNqLg2U/p0psY0Fov59tNHwG+JFwD4s2oPS6IOhhzzUf/Psw
         B6BJ/MlCPWn03ustr/AJcFxIT+QjUEy//9IkDIQ/CeVmurHT9foR4Yt7XqDDFgQZcarw
         5wsH7Q4g3vRUT/Vy+y2GGvJnqDhOPcXlTeXcGx5eEPunnp4Ih6hjDvv7QXg6tXWAJInE
         8tjw==
MIME-Version: 1.0
X-Received: by 10.60.45.202 with SMTP id p10mr4460177oem.60.1420694453650;
 Wed, 07 Jan 2015 21:20:53 -0800 (PST)
Date: Wed, 7 Jan 2015 21:20:53 -0800
Message-ID: <CAH+oPYh1h1eTmfes_ur0ycEkJZhAGbgnEmXO53fcGbfAPFVUyw@mail.gmail.com>
Subject: Need Help to display output of the the command on UI
From: Indu Chaube <prakash.indu@gmail.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1da643e8cbe050c1d3483
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1da643e8cbe050c1d3483
Content-Type: text/plain; charset=UTF-8

Hi Folk,
I need to print output of the below command on Web UI

 val conf=new SparkConf().setMaster("local")
    val sc=new SparkContext(conf)
    val file1=sc.textFile("/var/log/dpkg.log")

    //Applying filter onto the data
    val data1=file1.filter(line => line.contains("installed"))
    <b style="color:red">{data1.count() } </b>

But it is printing on console. Any Idea how to do it. That will be great
help.


Thanks,
Indu

--001a11c1da643e8cbe050c1d3483--

From dev-return-11040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 06:26:44 2015
Return-Path: <dev-return-11040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1DAB610DCE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 06:26:44 +0000 (UTC)
Received: (qmail 31068 invoked by uid 500); 8 Jan 2015 06:26:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30997 invoked by uid 500); 8 Jan 2015 06:26:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30979 invoked by uid 99); 8 Jan 2015 06:26:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 06:26:42 +0000
X-ASF-Spam-Status: No, hits=4.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of xuelincao2014@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 06:26:15 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 2ADEAFDB4CD
	for <dev@spark.apache.org>; Wed,  7 Jan 2015 22:25:45 -0800 (PST)
Date: Wed, 7 Jan 2015 23:25:43 -0700 (MST)
From: "Xuelin Cao.2015" <xuelincao2014@gmail.com>
To: dev@spark.apache.org
Message-ID: <CABjPPTQAhDtBbWfz=oUyKcOdqnhZYT1jmqJeBB8VhhJTMe=fSw@mail.gmail.com>
In-Reply-To: <CABPQxsvKHfmYe7HWcCsecp6CLOjk-cgCFQBTOtQ5+M1mm7_snQ@mail.gmail.com>
References: <CABjPPTTLQ-j55+pDhvjCSiobcSL+5-JMfXdUCfNVvAfi5Y67Ag@mail.gmail.com> <CABPQxsvKHfmYe7HWcCsecp6CLOjk-cgCFQBTOtQ5+M1mm7_snQ@mail.gmail.com>
Subject: Re: When will spark support "push" style shuffle?
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_153097_760180446.1420698343949"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_153097_760180446.1420698343949
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Got it. The explain makes sense. Thank you.


On Thu, Jan 8, 2015 at 1:06 PM, Patrick Wendell [via Apache Spark
Developers List] <ml-node+s1001551n10029h8@n3.nabble.com> wrote:

> This question is conflating a few different concepts. I think the main
> question is whether Spark will have a shuffle implementation that
> streams data rather than persisting it to disk/cache as a buffer.
> Spark currently decouples the shuffle write from the read using
> disk/OS cache as a buffer. The two benefits of this approach this are
> that it allows intra-query fault tolerance and it makes it easier to
> elastically scale and reschedule work within a job. We consider these
> to be design requirements (think about jobs that run for several hours
> on hundreds of machines). Impala, and similar systems like dremel and
> f1, not offer fault tolerance within a query at present. They also
> require gang scheduling the entire set of resources that will exist
> for the duration of a query.
>
> A secondary question is whether our shuffle should have a barrier or
> not. Spark's shuffle currently has a hard barrier between map and
> reduce stages. We haven't seen really strong evidence that removing
> the barrier is a net win. It can help the performance of a single job
> (modestly), but in the a multi-tenant workload, it leads to poor
> utilization since you have a lot of reduce tasks that are taking up
> slots waiting for mappers to finish. Many large scale users of
> Map/Reduce disable this feature in production clusters for that
> reason. Thus, we haven't seen compelling evidence for removing the
> barrier at this point, given the complexity of doing so.
>
> It is possible that future versions of Spark will support push-based
> shuffles, potentially in a mode that remove some of Spark's fault
> tolerance properties. But there are many other things we can still
> optimize about the shuffle that would likely come before this.
>
> - Patrick
>
> On Wed, Jan 7, 2015 at 6:01 PM, =E6=9B=B9=E9=9B=AA=E6=9E=97 <[hidden emai=
l]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10029&i=3D0>> wrote:
>
> > Hi,
> >
> >       I've heard a lot of complain about spark's "pull" style shuffle.
> Is
> > there any plan to support "push" style shuffle in the near future?
> >
> >       Currently, the shuffle phase must be completed before the next
> stage
> > starts. While, it is said, in Impala, the shuffled data is "streamed" t=
o
> > the next stage handler, which greatly saves time. Will spark support
> this
> > mechanism one day?
> >
> > Thanks
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10029&i=3D1>
> For additional commands, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10029&i=3D2>
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Fwd-When-will-s=
park-support-push-style-shuffle-tp10028p10029.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h40@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3DeHVlbGluY2FvMjAxNEBn=
bWFpbC5jb218MXwtOTc3NDY2MzAy>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subscri=
bers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instan=
t_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Fwd-When-will-spark-support-push-style-shuffle-tp10028p10031.h=
tml
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_153097_760180446.1420698343949--

From dev-return-11041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 10:02:38 2015
Return-Path: <dev-return-11041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F5621772C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 10:02:38 +0000 (UTC)
Received: (qmail 76982 invoked by uid 500); 8 Jan 2015 10:02:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76903 invoked by uid 500); 8 Jan 2015 10:02:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76848 invoked by uid 99); 8 Jan 2015 10:02:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:02:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.123 as permitted sender)
Received: from [77.75.72.123] (HELO mxf1.seznam.cz) (77.75.72.123)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:02:31 +0000
Received: from email.seznam.cz
	by email-smtpc13a.go.seznam.cz (email-smtpc13a.go.seznam.cz [192.168.92.56])
	id 34ef65d0b3f6831637776092;
	Thu, 08 Jan 2015 11:00:09 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1420711209; bh=eCsFeK/hc2h+tVMC0ykNFEL+SbhIvLH3Qh/tJ/HxDSc=;
	h=Received:From:To:Subject:Date:Message-Id:Mime-Version:X-Mailer:
	 Content-Type;
	b=UEPo4kj+nYL6fCk02XhiTITT/0SNmEBOmroPRrbiLhPgFJIvNhUhsEqbml5hl5BHY
	 NAIjeRjnsFQx6EybgpDhGx8UtxXJzbVrs+1Ie6GjMwVK5OU8tT/s801uM8kCxMyYsp
	 Ei6cpx6Rzb72LnjcJYmuytxwYhf6XkHXNVtGXhlw=
Received: from 85-207-13-216.static.bluetone.cz
	(85-207-13-216.static.bluetone.cz [85.207.13.216])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Thu, 08 Jan 2015 11:00:08 +0100 (CET)
From: "Jakub Dubovsky" <spark.dubovsky.jakub@seznam.cz>
To: <dev@spark.apache.org>
Subject: Spark development with IntelliJ
Date: Thu, 08 Jan 2015 11:00:08 +0100 (CET)
Message-Id: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_2f039af642b5ab322af91e1a=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_2f039af642b5ab322af91e1a=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi devs,=0A=
=0A=
=C2=A0 I'd like to ask if anybody has experience with using intellij 14 to=
 step =0A=
into spark code. Whatever I try I get compilation error:=0A=
=0A=
Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/=
=0A=
paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar=0A=
=0A=
=C2=A0 Project is set up by Patrick's instruction [1] and packaged by mvn =
-=0A=
DskipTests clean install. Compilation works fine. Then I just created =
=0A=
breakpoint in test code and run debug with the error.=0A=
=0A=
=C2=A0 Thanks for any hints=0A=
=0A=
=C2=A0 Jakub=0A=
=0A=
[1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+=
=0A=
Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA=0A=
--=_2f039af642b5ab322af91e1a=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-11042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 10:11:41 2015
Return-Path: <dev-return-11042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB55C17765
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 10:11:41 +0000 (UTC)
Received: (qmail 89045 invoked by uid 500); 8 Jan 2015 10:11:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88970 invoked by uid 500); 8 Jan 2015 10:11:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88953 invoked by uid 99); 8 Jan 2015 10:11:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:11:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of petar.zecevic@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:11:13 +0000
Received: by mail-wi0-f175.google.com with SMTP id l15so2188960wiw.2
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 02:08:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=L5aEXnjwF8EG7hXozEcM1aJ1amYoS7lsZ2D5uS5lbyU=;
        b=QWZOeFCyHdu31r1KdW5hj75EbKvz8T2HbF5Xku26xvSu8vI+CKAYagSje4VJL2ira0
         SQMIADg9ug5QqfiYVAMflvwILChohRNBj02nzJGI8FOYcn/Zi33UtFt+HEpIJAVIUl5x
         D6MPup3/dOCvvdtnUmwaOPOiDlf6bLmnd9b1jkFOLc94fAKj2/qkctccGW+9WtJUX3aa
         pnfqWZqG6HiYIQ+d2Hfx6OJNgWCbyX6unY+laF/V7o208kDuVHB1/RvNQGpgeurio5+0
         jFTIae9SxmYytfxSSUKlfnq6b18mdBrMRvZ+V2MecfN0dE+uVv/Yy0GSZoDCr/knC/MQ
         4jdA==
X-Received: by 10.194.206.70 with SMTP id lm6mr4967580wjc.30.1420711737039;
        Thu, 08 Jan 2015 02:08:57 -0800 (PST)
Received: from [192.168.0.39] (93-138-149-150.adsl.net.t-com.hr. [93.138.149.150])
        by mx.google.com with ESMTPSA id dc1sm21189709wib.18.2015.01.08.02.08.55
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 08 Jan 2015 02:08:56 -0800 (PST)
Message-ID: <54AE5803.5010905@gmail.com>
Date: Thu, 08 Jan 2015 11:12:19 +0100
From: Petar Zecevic <petar.zecevic@gmail.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Spark development with IntelliJ
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
In-Reply-To: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


This helped me:

http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea


On 8.1.2015. 11:00, Jakub Dubovsky wrote:
> Hi devs,
>
>    I'd like to ask if anybody has experience with using intellij 14 to step
> into spark code. Whatever I try I get compilation error:
>
> Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/
> paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar
>
>    Project is set up by Patrick's instruction [1] and packaged by mvn -
> DskipTests clean install. Compilation works fine. Then I just created
> breakpoint in test code and run debug with the error.
>
>    Thanks for any hints
>
>    Jakub
>
> [1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+
> Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 10:29:18 2015
Return-Path: <dev-return-11043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1A1C917829
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 10:29:18 +0000 (UTC)
Received: (qmail 23311 invoked by uid 500); 8 Jan 2015 10:29:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23229 invoked by uid 500); 8 Jan 2015 10:29:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23209 invoked by uid 99); 8 Jan 2015 10:29:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:29:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 10:28:49 +0000
Received: by mail-qa0-f51.google.com with SMTP id i13so6617822qae.10
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 02:27:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=A+vIN8Qm0nIXwyUn6LfVHlVrtD7/1smFOeWKbpk3uIc=;
        b=W9hh0TI2Oo1Gn5VUFDhpKbLwY0vxsY2sSZfoYyR5Cf0aOnoV2gl42mfsfI1QDm8ug7
         nn7+Xs7Jdzf2+bA4c5iUXKkRqS5jysMli2f9Z2ZRgvHEGI9QbWl+CZokGjKAey7Rj/Up
         UylI2r92wr9ZncqSEAKGppYu0K/eLlb8Oa+jVB4Gr9uqkp4XFbHiMxNtWnS8e2QrSpt5
         LoUQSDRbjRaVg8AOPx+nDjxnARDIW86rzYHeL7dUIucyVN6owKmD1CNSooUYafBodJGf
         Lh//zKawnT3vZuw9tL0yD27iDzSz4buxMR035wLzu4ohAy42tH0BhpbnjcyxenVQlu2r
         oipg==
X-Gm-Message-State: ALoCoQk9Rqxz8JchpdkyZ1quGij1kDhytL6niIUvrfa35G4Yu54f4u/LxqdKGUqg2qIeovSEdM/3
X-Received: by 10.140.82.136 with SMTP id h8mr12927687qgd.75.1420712837261;
 Thu, 08 Jan 2015 02:27:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.41.7 with HTTP; Thu, 8 Jan 2015 02:26:56 -0800 (PST)
In-Reply-To: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 8 Jan 2015 10:26:56 +0000
Message-ID: <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
To: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, I hit this too. IntelliJ picks this up from the build but then
it can't run its own scalac with this plugin added.

Go to Preferences > Build, Execution, Deployment > Scala Compiler and
clear the "Additional compiler options" field. It will work then
although the option will come back when the project reimports.

Right now I don't know of a better fix.

There's another recent open question about updating IntelliJ docs:
https://issues.apache.org/jira/browse/SPARK-5136  Should this stuff go
in the site docs, or wiki? I vote for wiki I suppose and make the site
docs point to the wiki. I'd be happy to make wiki edits if I can get
permission, or propose this text along with other new text on the
JIRA.

On Thu, Jan 8, 2015 at 10:00 AM, Jakub Dubovsky
<spark.dubovsky.jakub@seznam.cz> wrote:
> Hi devs,
>
>   I'd like to ask if anybody has experience with using intellij 14 to step
> into spark code. Whatever I try I get compilation error:
>
> Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/
> paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar
>
>   Project is set up by Patrick's instruction [1] and packaged by mvn -
> DskipTests clean install. Compilation works fine. Then I just created
> breakpoint in test code and run debug with the error.
>
>   Thanks for any hints
>
>   Jakub
>
> [1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+
> Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 11:34:06 2015
Return-Path: <dev-return-11044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CDD617A33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 11:34:06 +0000 (UTC)
Received: (qmail 48688 invoked by uid 500); 8 Jan 2015 11:34:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48611 invoked by uid 500); 8 Jan 2015 11:34:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48566 invoked by uid 99); 8 Jan 2015 11:34:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 11:34:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.43 as permitted sender)
Received: from [77.75.72.43] (HELO smtp1.seznam.cz) (77.75.72.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 11:33:37 +0000
Received: from email.seznam.cz
	by email-smtpc6a.go.seznam.cz (email-smtpc6a.go.seznam.cz [192.168.92.42])
	id 4fa4dd00c8bd3bc64c3cd842;
	Thu, 08 Jan 2015 12:33:33 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1420716813; bh=jkWx+hZuoov2PsOg4QMzRb6POWNq2frbefOICzRviFE=;
	h=Received:From:To:Cc:Subject:Date:Message-Id:References:
	 Mime-Version:X-Mailer:Content-Type;
	b=fVwexg2vcmQPk9C3OuXP58Q+7GuO/VVWD8eKoHhrb5Cn010DYtAmuLpCF0toQq3YT
	 zqKkvUHv8r2QLGUGu72YxfzEVZzRDMebigEoUpjc8sTLZ15SH2EwnCHi5mIJcVi1OL
	 pPFnZsuBTgvlNVaANrGQsJ4vlmtHeWCvgz9c+GiY=
Received: from 85-207-13-216.static.bluetone.cz
	(85-207-13-216.static.bluetone.cz [85.207.13.216])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Thu, 08 Jan 2015 12:33:32 +0100 (CET)
From: "Jakub Dubovsky" <spark.dubovsky.jakub@seznam.cz>
To: "Sean Owen" <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Subject: Re: Spark development with IntelliJ
Date: Thu, 08 Jan 2015 12:33:32 +0100 (CET)
Message-Id: <3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_58aeb18b3f2ef36f7b4afad4=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_58aeb18b3f2ef36f7b4afad4=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Thanks that helped.=0A=
=0A=
I vote for wiki as well. More fine graned documentation should be on wiki =
=0A=
and linked,=0A=
=0A=
Jakub=0A=
=0A=
=0A=
---------- P=C5=AFvodn=C3=AD zpr=C3=A1va ----------=0A=
Od: Sean Owen <sowen@cloudera.com>=0A=
Komu: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>=0A=
Datum: 8. 1. 2015 11:29:22=0A=
P=C5=99edm=C4=9Bt: Re: Spark development with IntelliJ=0A=
=0A=
"Yeah, I hit this too. IntelliJ picks this up from the build but then=
=0A=
it can't run its own scalac with this plugin added.=0A=
=0A=
Go to Preferences > Build, Execution, Deployment > Scala Compiler and=
=0A=
clear the "Additional compiler options" field. It will work then=0A=
although the option will come back when the project reimports.=0A=
=0A=
Right now I don't know of a better fix.=0A=
=0A=
There's another recent open question about updating IntelliJ docs:=0A=
https://issues.apache.org/jira/browse/SPARK-5136 Should this stuff go=
=0A=
in the site docs, or wiki? I vote for wiki I suppose and make the site=
=0A=
docs point to the wiki. I'd be happy to make wiki edits if I can get=0A=
permission, or propose this text along with other new text on the=0A=
JIRA.=0A=
=0A=
On Thu, Jan 8, 2015 at 10:00 AM, Jakub Dubovsky=0A=
<spark.dubovsky.jakub@seznam.cz> wrote:=0A=
> Hi devs,=0A=
>=0A=
> I'd like to ask if anybody has experience with using intellij 14 to step=
=0A=
> into spark code. Whatever I try I get compilation error:=0A=
>=0A=
> Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/=
=0A=
> paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar=0A=
>=0A=
> Project is set up by Patrick's instruction [1] and packaged by mvn -=
=0A=
> DskipTests clean install. Compilation works fine. Then I just created=
=0A=
> breakpoint in test code and run debug with the error.=0A=
>=0A=
> Thanks for any hints=0A=
>=0A=
> Jakub=0A=
>=0A=
> [1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+=
=0A=
> Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA=0A=
=0A=
---------------------------------------------------------------------=
=0A=
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org=0A=
For additional commands, e-mail: dev-help@spark.apache.org"=
--=_58aeb18b3f2ef36f7b4afad4=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-11045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 13:42:18 2015
Return-Path: <dev-return-11045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B45C17D67
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 13:42:18 +0000 (UTC)
Received: (qmail 80027 invoked by uid 500); 8 Jan 2015 13:42:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79950 invoked by uid 500); 8 Jan 2015 13:42:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79939 invoked by uid 99); 8 Jan 2015 13:42:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 13:42:16 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [129.184.85.11] (HELO odin2.bull.net) (129.184.85.11)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 13:41:51 +0000
Received: from BUMSG2WM.fr.ad.bull.net (bumsg2wm.fr.ad.bull.net [10.192.1.16])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by odin2.bull.net (Bull S.A.) with ESMTP id 131C82039F
	for <dev@spark.apache.org>; Thu,  8 Jan 2015 14:40:50 +0100 (CET)
Received: from BUMSG3WM.fr.ad.bull.net ([10.192.1.139]) by
 BUMSG2WM.fr.ad.bull.net ([10.192.1.16]) with mapi id 14.03.0210.002; Thu, 8
 Jan 2015 14:40:49 +0100
From: Tony Reix <tony.reix@bull.net>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Results of tests
Thread-Topic: Results of tests
Thread-Index: AdArSLaLLhxvOb+fRIS2Z8OrGN8LPw==
Date: Thu, 8 Jan 2015 13:40:49 +0000
Message-ID: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
Accept-Language: fr-FR, en-US
Content-Language: fr-FR
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.192.1.123]
Content-Type: multipart/alternative;
	boundary="_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A16DBUMSG3WMfradbul_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A16DBUMSG3WMfradbul_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when ru=
nning on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can fin=
d the results of the tests of Spark, for each version and for the different=
 versions, in order to have a reference to compare my results with. I canno=
t find them on Spark web-site.
Thx
Tony


--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A16DBUMSG3WMfradbul_--

From dev-return-11046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 15:12:20 2015
Return-Path: <dev-return-11046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9297C0CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 15:12:20 +0000 (UTC)
Received: (qmail 53032 invoked by uid 500); 8 Jan 2015 15:12:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52953 invoked by uid 500); 8 Jan 2015 15:12:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52930 invoked by uid 99); 8 Jan 2015 15:12:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 15:12:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.52 as permitted sender)
Received: from [209.85.213.52] (HELO mail-yh0-f52.google.com) (209.85.213.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 15:12:14 +0000
Received: by mail-yh0-f52.google.com with SMTP id z6so1431164yhz.11
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 07:11:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SPYV6eP1gB0JIQg9Dq7boCUDEkZsKPT1qN90mj4jTeE=;
        b=DcQAVvhB1CPLOGuDQSg++SJqslxw9jJDR7mQIErNFwEwVkyr1g167YXoI5KX3YfNz9
         LEmf8dJiT4ZmYXJxvvwnv8GJeqAwqhmyi27UZox6BrX+1gyu98tMdN2eWw/5Z7i0nce2
         +8CrZkN/tzrjLV58w0PqmKLQsQWhuKeTJq4jwGjQqAI9ugCWyQ7dBwRFN38+xZGENJFj
         esIfnN+zi938Vm3JSshnG9PPGwEk+4xf5Y2GxUHBq1GSQLE9Le803H9RbKqhuE6JeclP
         nWrq80yChb5wtCLo4VkgfO5POANoOP4HE5FopzAox04UcQFQNVv30edf0Q8iu8sHjf+x
         5EPQ==
MIME-Version: 1.0
X-Received: by 10.236.206.8 with SMTP id k8mr6731220yho.23.1420729914076; Thu,
 08 Jan 2015 07:11:54 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Thu, 8 Jan 2015 07:11:53 -0800 (PST)
In-Reply-To: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
Date: Thu, 8 Jan 2015 07:11:54 -0800
Message-ID: <CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
Subject: Re: Results of tests
From: Ted Yu <yuzhihong@gmail.com>
To: Tony Reix <tony.reix@bull.net>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011616a8d9b676050c2575cf
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011616a8d9b676050c2575cf
Content-Type: text/plain; charset=UTF-8

Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/

On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net> wrote:

> Hi,
> I'm checking that Spark works fine on a new environment (PPC64 hardware).
> I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when
> running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can
> find the results of the tests of Spark, for each version and for the
> different versions, in order to have a reference to compare my results
> with. I cannot find them on Spark web-site.
> Thx
> Tony
>
>

--089e011616a8d9b676050c2575cf--

From dev-return-11047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:00:42 2015
Return-Path: <dev-return-11047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38270C406
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:00:42 +0000 (UTC)
Received: (qmail 91071 invoked by uid 500); 8 Jan 2015 16:00:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91001 invoked by uid 500); 8 Jan 2015 16:00:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90914 invoked by uid 99); 8 Jan 2015 16:00:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:00:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of devl.development@gmail.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:00:30 +0000
Received: by mail-qg0-f49.google.com with SMTP id f51so2770730qge.8
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 07:58:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=J/HODQxsf6W0y+pbzXUnTmVe9a4xwSuKzUBj/dWCuHg=;
        b=jqyRrW91kGpQgOPBe+qlsCu9BaJzTzDYjVRD4h0uWKiINlbLgKvF6EIw/9oO8ANQYF
         GwnybbN2gvqf79sy0bgDsuv1achysbSwKPM/Vzzj6VvBA477wFLzyL6txEHYIq5zWqZB
         A5KBjld433wKn4xtrKDJnbaCYG5rY/9lIJLKJJPT0f2Fkjs9cdNT45pHk0rBWXABpekz
         RlORI14Q7A7K05BM6mXe75elUJfpKObsv23CjvbP/BXCztEEy/WF6Ql60rG6L/6/FmTL
         0w7QVqAXMKj4sCgVF18hGq3hf+zGiNs6AgGEFjbkKyajrczAvE26n6uX8FL/557RNQUe
         x0dQ==
MIME-Version: 1.0
X-Received: by 10.224.136.7 with SMTP id p7mr6383697qat.65.1420732719216; Thu,
 08 Jan 2015 07:58:39 -0800 (PST)
Received: by 10.140.102.111 with HTTP; Thu, 8 Jan 2015 07:58:39 -0800 (PST)
Date: Thu, 8 Jan 2015 15:58:39 +0000
Message-ID: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
Subject: K-Means And Class Tags
From: Devl Devel <devl.development@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c299a40cbe6f050c261dfb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c299a40cbe6f050c261dfb
Content-Type: text/plain; charset=UTF-8

Hi All,

I'm trying a simple K-Means example as per the website:

val parsedData = data.map(s => Vectors.dense(s.split(',').map(_.toDouble)))

but I'm trying to write a Java based validation method first so that
missing values are omitted or replaced with 0.

public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
        JavaRDD<Vector> words = data.flatMap(new FlatMapFunction<String,
Vector>() {
            public Iterable<Vector> call(String s) {
                String[] split = s.split(",");
                ArrayList<Vector> add = new ArrayList<Vector>();
                if (split.length != 2) {
                    add.add(Vectors.dense(0, 0));
                } else
                {
                    add.add(Vectors.dense(Double.parseDouble(split[0]),
               Double.parseDouble(split[1])));
                }

                return add;
            }
        });

        return words.rdd();
}

When I then call from scala:

val parsedData=dc.prepareKMeans(data);
val p=parsedData.collect();

I get Exception in thread "main" java.lang.ClassCastException:
[Ljava.lang.Object; cannot be cast to
[Lorg.apache.spark.mllib.linalg.Vector;

Why is the class tag is object rather than vector?

1) How do I get this working correctly using the Java validation example
above or
2) How can I modify val parsedData = data.map(s =>
Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split size <2 I
ignore the line? or
3) Is there a better way to do input validation first?

Using spark and mlib:
libraryDependencies += "org.apache.spark" % "spark-core_2.10" %  "1.2.0"
libraryDependencies += "org.apache.spark" % "spark-mllib_2.10" % "1.2.0"

Many thanks in advance
Dev

--001a11c299a40cbe6f050c261dfb--

From dev-return-11048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:05:43 2015
Return-Path: <dev-return-11048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F15ACC44B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:05:43 +0000 (UTC)
Received: (qmail 6831 invoked by uid 500); 8 Jan 2015 16:05:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6753 invoked by uid 500); 8 Jan 2015 16:05:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6741 invoked by uid 99); 8 Jan 2015 16:05:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:05:42 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [129.184.85.11] (HELO odin2.bull.net) (129.184.85.11)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:05:17 +0000
Received: from BUMSG2WM.fr.ad.bull.net (bumsg2wm.fr.ad.bull.net [10.192.1.16])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by odin2.bull.net (Bull S.A.) with ESMTP id 4915B203D3;
	Thu,  8 Jan 2015 17:05:16 +0100 (CET)
Received: from BUMSG3WM.fr.ad.bull.net ([10.192.1.139]) by
 BUMSG2WM.fr.ad.bull.net ([10.192.1.16]) with mapi id 14.03.0210.002; Thu, 8
 Jan 2015 17:05:16 +0100
From: Tony Reix <tony.reix@bull.net>
To: Ted Yu <yuzhihong@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE:Results of tests
Thread-Topic: Results of tests
Thread-Index: AdArSLaLLhxvOb+fRIS2Z8OrGN8LPwABFdkAAAPHSXI=
Date: Thu, 8 Jan 2015 16:05:15 +0000
Message-ID: <5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>,<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
In-Reply-To: <CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
Accept-Language: fr-FR, en-US
Content-Language: fr-FR
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.192.1.123]
Content-Type: multipart/alternative;
	boundary="_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A233BUMSG3WMfradbul_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A233BUMSG3WMfradbul_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Thanks !

I've been able to see that there are 3745 tests for version 1.2.0 with prof=
ile Hadoop 2.4  .
However, on my side, the maximum tests I've seen are 3485... About 300 test=
s are missing on my side.
Which Maven option has been used for producing the report file used for bui=
lding the page:
     https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-=
with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/te=
stReport/
  ? (I'm not authorized to look at the "configuration" part)

Thx !

Tony

________________________________
De : Ted Yu [yuzhihong@gmail.com]
Envoy=E9 : jeudi 8 janvier 2015 16:11
=C0 : Tony Reix
Cc : dev@spark.apache.org
Objet : Re: Results of tests

Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/

On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto:tony.r=
eix@bull.net>> wrote:
Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when ru=
nning on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can fin=
d the results of the tests of Spark, for each version and for the different=
 versions, in order to have a reference to compare my results with. I canno=
t find them on Spark web-site.
Thx
Tony



--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A233BUMSG3WMfradbul_--

From dev-return-11049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:16:07 2015
Return-Path: <dev-return-11049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FAE8C53A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:16:07 +0000 (UTC)
Received: (qmail 36215 invoked by uid 500); 8 Jan 2015 16:16:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36144 invoked by uid 500); 8 Jan 2015 16:16:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36120 invoked by uid 99); 8 Jan 2015 16:16:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:16:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yana.kadiyska@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:15:58 +0000
Received: by mail-wg0-f50.google.com with SMTP id a1so3461402wgh.9
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 08:13:22 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:in-reply-to:references:date:message-id
         :subject:from:to:cc:content-type;
        bh=Jr1rLNBAM6mSFZrYGtBfDk83Bw5JjqXp7QIpVHqzSNs=;
        b=HT95oPlwdv4HAKOHnOZNz3+c1wOK8kk70Y4GoLa8uXUjpwO4oMMLXHJA/GCysBBh5A
         e4qavuRGJ2q7BfdV+SFgt9IQAEuYj5VYPMxJFa6CqhtIar7mGcDqESycg0GO6LAuaGad
         lBfUdiubeE08xdBuEL5r5qM6gY+VT7MWTlf962zK/mg4BhMn4bk0prjEVsdYP1LxUogZ
         IYUzbsdddPbEeyrbcIr6NdhFM0rJ89Jw4MzHMo7/grrDQgiJnxSxz2gnoP6yfS2pLhRO
         djG6iPdEfstkz66hky/03dkN/BiEuQDLQe6+9SLgDw19Qwwq29ruxPkmUAujuLNcaXTV
         uSxQ==
MIME-Version: 1.0
X-Received: by 10.194.58.19 with SMTP id m19mr21133946wjq.52.1420733602340;
 Thu, 08 Jan 2015 08:13:22 -0800 (PST)
Received: by 10.217.99.197 with HTTP; Thu, 8 Jan 2015 08:13:22 -0800 (PST)
Reply-To: yana.kadiyska@gmail.com
In-Reply-To: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
Date: Thu, 8 Jan 2015 11:13:22 -0500
Message-ID: <CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
Subject: Re: K-Means And Class Tags
From: Yana Kadiyska <yana.kadiyska@gmail.com>
To: Devl Devel <devl.development@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b86dd30b023da050c265142
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86dd30b023da050c265142
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

How about

data.map(s=3D>s.split(",")).filter(_.length>1).map(good_entry=3D>Vectors.de=
nse((Double.parseDouble(good_entry[0]),
Double.parseDouble(good_entry[1]))
=E2=80=8B
(full disclosure, I didn't actually run this). But after the first map you
should have an RDD[Array[String]], then you'd discard everything shorter
than 2, and convert the rest to dense vectors?...In fact if you're
expecting length exactly 2 might want to filter =3D=3D2...


On Thu, Jan 8, 2015 at 10:58 AM, Devl Devel <devl.development@gmail.com>
wrote:

> Hi All,
>
> I'm trying a simple K-Means example as per the website:
>
> val parsedData =3D data.map(s =3D> Vectors.dense(s.split(',').map(_.toDou=
ble)))
>
> but I'm trying to write a Java based validation method first so that
> missing values are omitted or replaced with 0.
>
> public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
>         JavaRDD<Vector> words =3D data.flatMap(new FlatMapFunction<String=
,
> Vector>() {
>             public Iterable<Vector> call(String s) {
>                 String[] split =3D s.split(",");
>                 ArrayList<Vector> add =3D new ArrayList<Vector>();
>                 if (split.length !=3D 2) {
>                     add.add(Vectors.dense(0, 0));
>                 } else
>                 {
>                     add.add(Vectors.dense(Double.parseDouble(split[0]),
>                Double.parseDouble(split[1])));
>                 }
>
>                 return add;
>             }
>         });
>
>         return words.rdd();
> }
>
> When I then call from scala:
>
> val parsedData=3Ddc.prepareKMeans(data);
> val p=3DparsedData.collect();
>
> I get Exception in thread "main" java.lang.ClassCastException:
> [Ljava.lang.Object; cannot be cast to
> [Lorg.apache.spark.mllib.linalg.Vector;
>
> Why is the class tag is object rather than vector?
>
> 1) How do I get this working correctly using the Java validation example
> above or
> 2) How can I modify val parsedData =3D data.map(s =3D>
> Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split size <2=
 I
> ignore the line? or
> 3) Is there a better way to do input validation first?
>
> Using spark and mlib:
> libraryDependencies +=3D "org.apache.spark" % "spark-core_2.10" %  "1.2.0=
"
> libraryDependencies +=3D "org.apache.spark" % "spark-mllib_2.10" % "1.2.0=
"
>
> Many thanks in advance
> Dev
>

--047d7b86dd30b023da050c265142--

From dev-return-11050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:32:25 2015
Return-Path: <dev-return-11050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F724C65D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:32:25 +0000 (UTC)
Received: (qmail 90097 invoked by uid 500); 8 Jan 2015 16:32:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89961 invoked by uid 500); 8 Jan 2015 16:32:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87989 invoked by uid 99); 8 Jan 2015 16:32:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:32:21 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of eshioji@gmail.com designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:31:56 +0000
Received: by mail-la0-f44.google.com with SMTP id gd6so10132688lab.3;
        Thu, 08 Jan 2015 08:30:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=JSXf1WXDRc6vefu/HsdzvdC64/RssN5NFxlkYSyQFB4=;
        b=fvlSzGs3g5RBovtU0V940tRpjJHBfIbPw2cmSsIm6ZwYvjx47fG4bWHKcU63rlciMF
         QbGD308js7Vx9BtIAxSRkaJln4TtfO6c58egNHPeq6r008mx10jekiCKcqYNRb/47qNs
         5TKYI2QjZY7MNQUAB29bPrReNM2RHjuGZHVHgs7U4w06rXiUta8hxYJbAvGAiEViL2kO
         LncL1KavpYcMufxK1UvQaFu8LsJ+c8nEIRxIC+Rv+VnlKWqO2anVli7NqzKHpw3KcSUk
         dfPCFrTOaubws6Db3eVSA4qZ7oe34npPTvj//50sLhRhXKVZ/4XHLJNJIMu6HQbbLfYq
         4IZA==
MIME-Version: 1.0
X-Received: by 10.152.27.8 with SMTP id p8mr15271504lag.69.1420734621873; Thu,
 08 Jan 2015 08:30:21 -0800 (PST)
Received: by 10.112.176.36 with HTTP; Thu, 8 Jan 2015 08:30:21 -0800 (PST)
In-Reply-To: <CAE50=dqZdtJviURKqpZXBQqxz1w0XRYZGYK-996X5Ru3c=Kctg@mail.gmail.com>
References: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
	<1419949963646-9968.post@n3.nabble.com>
	<CAMc-71kQw3LDGG0kkU_PNdpHwVck6k8-q+1+6y9NY2Cq6FFOTQ@mail.gmail.com>
	<CAE50=dqZdtJviURKqpZXBQqxz1w0XRYZGYK-996X5Ru3c=Kctg@mail.gmail.com>
Date: Thu, 8 Jan 2015 16:30:21 +0000
Message-ID: <CAE50=dq+6tdx9VNVM3ctBMWPLDPbUAacO3aN3L8x38zg=xb6VQ@mail.gmail.com>
Subject: Re: Registering custom metrics
From: Enno Shioji <eshioji@gmail.com>
To: Gerard Maas <gerard.maas@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160bdb274feaa050c268e17
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bdb274feaa050c268e17
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

FYI I found this approach by Ooyala.

/** Instrumentation for Spark based on accumulators.
  *
  * Usage:
  * val instrumentation =3D new SparkInstrumentation("example.metrics")
  * val numReqs =3D sc.accumulator(0L)
  * instrumentation.source.registerDailyAccumulator(numReqs, "numReqs")
  * instrumentation.register()
  *
  * Will create and report the following metrics:
  * - Gauge with total number of requests (daily)
  * - Meter with rate of requests
  *
  * @param prefix prefix for all metrics that will be reported by this
Instrumentation
  */

https://gist.github.com/ibuenros/9b94736c2bad2f4b8e23
=E1=90=A7

On Mon, Jan 5, 2015 at 2:56 PM, Enno Shioji <eshioji@gmail.com> wrote:

> Hi Gerard,
>
> Thanks for the answer! I had a good look at it, but I couldn't figure out
> whether one can use that to emit metrics from your application code.
>
> Suppose I wanted to monitor the rate of bytes I produce, like so:
>
>     stream
>         .map { input =3D>
>           val bytes =3D produce(input)
>           // metricRegistry.meter("some.metrics").mark(bytes.length)
>           bytes
>         }
>         .saveAsTextFile("text")
>
> Is there a way to achieve this with the MetricSystem?
>
>
> =E1=90=A7
>
> On Mon, Jan 5, 2015 at 10:24 AM, Gerard Maas <gerard.maas@gmail.com>
> wrote:
>
>> Hi,
>>
>> Yes, I managed to create a register custom metrics by creating an
>>  implementation  of org.apache.spark.metrics.source.Source and
>> registering it to the metrics subsystem.
>> Source is [Spark] private, so you need to create it under a org.apache.s=
park
>> package. In my case, I'm dealing with Spark Streaming metrics, and I
>> created my CustomStreamingSource under org.apache.spark.streaming as I
>> also needed access to some [Streaming] private components.
>>
>> Then, you register your new metric Source on the Spark's metric system,
>> like so:
>>
>> SparkEnv.get.metricsSystem.registerSource(customStreamingSource)
>>
>> And it will get reported to the metrics Sync active on your system. By
>> default, you can access them through the metric endpoint:
>> http://<driver-host>:<ui-port>/metrics/json
>>
>> I hope this helps.
>>
>> -kr, Gerard.
>>
>>
>>
>>
>>
>>
>> On Tue, Dec 30, 2014 at 3:32 PM, eshioji <eshioji@gmail.com> wrote:
>>
>>> Hi,
>>>
>>> Did you find a way to do this / working on this?
>>> Am trying to find a way to do this as well, but haven't been able to
>>> find a
>>> way.
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-c=
ustom-metrics-tp9030p9968.html
>>> Sent from the Apache Spark Developers List mailing list archive at
>>> Nabble.com.
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>
>

--089e0160bdb274feaa050c268e17--

From dev-return-11051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:39:11 2015
Return-Path: <dev-return-11051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B986DC6C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:39:11 +0000 (UTC)
Received: (qmail 12709 invoked by uid 500); 8 Jan 2015 16:39:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12624 invoked by uid 500); 8 Jan 2015 16:39:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12605 invoked by uid 99); 8 Jan 2015 16:39:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:39:10 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of xhudik@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:38:44 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id A9A30FE7A3A
	for <dev@spark.apache.org>; Thu,  8 Jan 2015 08:38:43 -0800 (PST)
Date: Thu, 8 Jan 2015 09:38:42 -0700 (MST)
From: xhudik <xhudik@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420735122219-10042.post@n3.nabble.com>
In-Reply-To: <CACRMEukv1iH9c9nbL2=iDG9BzFRXuE3QYn2GLUjZckXY8x0zXA@mail.gmail.com>
References: <CACRMEukv1iH9c9nbL2=iDG9BzFRXuE3QYn2GLUjZckXY8x0zXA@mail.gmail.com>
Subject: Re: Spark on teradata?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think this makes sense. TD database is standard RDBMS (even parallel)
while Spark is used for non-relational issues. 
What could make sense is to deploy Spark on Teradata Aster. Aster is a
database cluster that might call external programs via STREAM operator. 
That said Spark/Scala app can be can be called and process some data. The
deployment itself should be easy the potential benefit - hard to say...


hope this helps, Tomas



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-on-teradata-tp10025p10042.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:46:10 2015
Return-Path: <dev-return-11052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 027FBC779
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:46:10 +0000 (UTC)
Received: (qmail 42987 invoked by uid 500); 8 Jan 2015 16:46:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42917 invoked by uid 500); 8 Jan 2015 16:46:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40174 invoked by uid 99); 8 Jan 2015 16:46:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:46:06 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:45:41 +0000
Received: by mail-we0-f171.google.com with SMTP id u56so3487859wes.2;
        Thu, 08 Jan 2015 08:43:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=sM7ALokGKHvt+KxeF7ukziYb6a8/JsCtTnUBoRJm+aQ=;
        b=rzb7655jJgaqOj5MNaGjetVxt2LP/Qmf8G3NWEPxnwD8tODZzhVO/dK4BV0oQd53or
         k5yMGWidhd0XwKE0hm4u+OdG6LwkawGByk72bA+Acgv3Xc96E3Zo6dsZldW1PFAmiESh
         RtRuBdNKvvFD9vOkZr45g12ifZ2ougRRtt2NbblVQqZHoCnYRb05R0ePPXBX4gzEhYWR
         zSNACGWW5udrJYnVXXPQSFz2IxuRsZgKvkF785IbbuuvYyyLFeQ8GcUsT3BsFTg0VaB9
         un2JGEhCFHrZkm+FdvYyL4UlK6GiwxHAUr/aAALPmjaZvLpuNp/fXzDpLAjqBYhQl4Vu
         Oh1Q==
X-Received: by 10.194.236.1 with SMTP id uq1mr21521966wjc.28.1420735405010;
 Thu, 08 Jan 2015 08:43:25 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.25.5 with HTTP; Thu, 8 Jan 2015 08:42:54 -0800 (PST)
In-Reply-To: <CAE50=dq+6tdx9VNVM3ctBMWPLDPbUAacO3aN3L8x38zg=xb6VQ@mail.gmail.com>
References: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
 <1419949963646-9968.post@n3.nabble.com> <CAMc-71kQw3LDGG0kkU_PNdpHwVck6k8-q+1+6y9NY2Cq6FFOTQ@mail.gmail.com>
 <CAE50=dqZdtJviURKqpZXBQqxz1w0XRYZGYK-996X5Ru3c=Kctg@mail.gmail.com> <CAE50=dq+6tdx9VNVM3ctBMWPLDPbUAacO3aN3L8x38zg=xb6VQ@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Thu, 8 Jan 2015 17:42:54 +0100
Message-ID: <CAMc-71nT1_98Z19A4R-j13X5OoYP1E_W+KmeJx3nE+HVrrWLUw@mail.gmail.com>
Subject: Re: Registering custom metrics
To: Enno Shioji <eshioji@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013d14a222b4b0050c26bde9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d14a222b4b0050c26bde9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Very interesting approach. Thanks for sharing it!

On Thu, Jan 8, 2015 at 5:30 PM, Enno Shioji <eshioji@gmail.com> wrote:

> FYI I found this approach by Ooyala.
>
> /** Instrumentation for Spark based on accumulators.
>   *
>   * Usage:
>   * val instrumentation =3D new SparkInstrumentation("example.metrics")
>   * val numReqs =3D sc.accumulator(0L)
>   * instrumentation.source.registerDailyAccumulator(numReqs, "numReqs")
>   * instrumentation.register()
>   *
>   * Will create and report the following metrics:
>   * - Gauge with total number of requests (daily)
>   * - Meter with rate of requests
>   *
>   * @param prefix prefix for all metrics that will be reported by this In=
strumentation
>   */
>
> https://gist.github.com/ibuenros/9b94736c2bad2f4b8e23
> =E1=90=A7
>
> On Mon, Jan 5, 2015 at 2:56 PM, Enno Shioji <eshioji@gmail.com> wrote:
>
>> Hi Gerard,
>>
>> Thanks for the answer! I had a good look at it, but I couldn't figure ou=
t
>> whether one can use that to emit metrics from your application code.
>>
>> Suppose I wanted to monitor the rate of bytes I produce, like so:
>>
>>     stream
>>         .map { input =3D>
>>           val bytes =3D produce(input)
>>           // metricRegistry.meter("some.metrics").mark(bytes.length)
>>           bytes
>>         }
>>         .saveAsTextFile("text")
>>
>> Is there a way to achieve this with the MetricSystem?
>>
>>
>> =E1=90=A7
>>
>> On Mon, Jan 5, 2015 at 10:24 AM, Gerard Maas <gerard.maas@gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> Yes, I managed to create a register custom metrics by creating an
>>>  implementation  of org.apache.spark.metrics.source.Source and
>>> registering it to the metrics subsystem.
>>> Source is [Spark] private, so you need to create it under a org.apache.=
spark
>>> package. In my case, I'm dealing with Spark Streaming metrics, and I
>>> created my CustomStreamingSource under org.apache.spark.streaming as I
>>> also needed access to some [Streaming] private components.
>>>
>>> Then, you register your new metric Source on the Spark's metric system,
>>> like so:
>>>
>>> SparkEnv.get.metricsSystem.registerSource(customStreamingSource)
>>>
>>> And it will get reported to the metrics Sync active on your system. By
>>> default, you can access them through the metric endpoint:
>>> http://<driver-host>:<ui-port>/metrics/json
>>>
>>> I hope this helps.
>>>
>>> -kr, Gerard.
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Tue, Dec 30, 2014 at 3:32 PM, eshioji <eshioji@gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> Did you find a way to do this / working on this?
>>>> Am trying to find a way to do this as well, but haven't been able to
>>>> find a
>>>> way.
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-=
custom-metrics-tp9030p9968.html
>>>> Sent from the Apache Spark Developers List mailing list archive at
>>>> Nabble.com.
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>>
>>>
>>
>

--089e013d14a222b4b0050c26bde9--

From dev-return-11053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 16:46:22 2015
Return-Path: <dev-return-11053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9CC4C77A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 16:46:22 +0000 (UTC)
Received: (qmail 44562 invoked by uid 500); 8 Jan 2015 16:46:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44487 invoked by uid 500); 8 Jan 2015 16:46:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44469 invoked by uid 99); 8 Jan 2015 16:46:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:46:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.44 as permitted sender)
Received: from [209.85.213.44] (HELO mail-yh0-f44.google.com) (209.85.213.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 16:46:16 +0000
Received: by mail-yh0-f44.google.com with SMTP id c41so1704609yho.3
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 08:43:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=XL78iHv9Zc2gBKbu+4bVKbo+EwPkzQ9XtQxeBcQ/VCg=;
        b=io3Re1H6e4tjZdRpz1BPBP94YZUieVWrsWjlOpynQNFPefGh64kycYa2zZZmJH78XQ
         KEzf58kF2Wff27KfgIM3vbcxbCz2jwMYb0k5E1zDVOUsHTuP+R6HAJg51HWrp+gWmgKW
         /uYKQu6kYOZOAeR8Wt1ClCG4jhnGX8YgJPruKtfKP1nu338LAtU1ei0gcUKEv82Io0dH
         Qw8/TNdT9w/McRDcbQ0732wnXkPh07v776odGTJd9186zigrA/aYxZgZ5Yj9Ac6dyTIv
         fX8KT5IVcV7niUajmVzbL5qqH56/n3BEMFhdDKZztqChuujxM0pReM3XJA58V98jHWAq
         Dv6g==
MIME-Version: 1.0
X-Received: by 10.236.32.8 with SMTP id n8mr7482625yha.74.1420735420476; Thu,
 08 Jan 2015 08:43:40 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Thu, 8 Jan 2015 08:43:40 -0800 (PST)
In-Reply-To: <5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
	<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
Date: Thu, 8 Jan 2015 08:43:40 -0800
Message-ID: <CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
Subject: Re: Results of tests
From: Ted Yu <yuzhihong@gmail.com>
To: Tony Reix <tony.reix@bull.net>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1c1340eb2bc050c26be54
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c1340eb2bc050c26be54
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here it is:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3=
.0.5/bin/mvn
-DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Phadoop-2.4
-Pyarn -Phive clean package


You can find the above in
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-=
YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/console=
Full


Cheers


On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net> wrote:

>  Thanks !
>
> I've been able to see that there are 3745 tests for version 1.2.0 with
> profile Hadoop 2.4  .
> However, on my side, the maximum tests I've seen are 3485... About 300
> tests are missing on my side.
> Which Maven option has been used for producing the report file used for
> building the page:
>
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
>   ? (I'm not authorized to look at the "configuration" part)
>
> Thx !
>
> Tony
>
>  ------------------------------
> *De :* Ted Yu [yuzhihong@gmail.com]
> *Envoy=C3=A9 :* jeudi 8 janvier 2015 16:11
> *=C3=80 :* Tony Reix
> *Cc :* dev@spark.apache.org
> *Objet :* Re: Results of tests
>
>   Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark=
/
>
> On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net> wrote:
>
>> Hi,
>> I'm checking that Spark works fine on a new environment (PPC64 hardware)=
.
>> I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when
>> running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I ca=
n
>> find the results of the tests of Spark, for each version and for the
>> different versions, in order to have a reference to compare my results
>> with. I cannot find them on Spark web-site.
>> Thx
>> Tony
>>
>>
>

--001a11c1c1340eb2bc050c26be54--

From dev-return-11054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 17:47:29 2015
Return-Path: <dev-return-11054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 88740CC21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 17:47:29 +0000 (UTC)
Received: (qmail 59980 invoked by uid 500); 8 Jan 2015 17:47:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59897 invoked by uid 500); 8 Jan 2015 17:47:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59882 invoked by uid 99); 8 Jan 2015 17:47:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 17:47:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 17:47:24 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so3763005wes.1
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 09:46:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zDMFFPD4HVElPJbHADivr07oG/+lt2oZmo6O8i39H2w=;
        b=ylxqSt+Qd+rx6UZL8N6x9bHVbmKijrGkuBkwL2jVBa1wxfdFGI04lwH1PHe7rUxABm
         b8Q77unbwJfmbrsbmtgLLFxof9lsQoUQsG1o4vzvEsgJbKgg5s/XV9a7y7jAmPm/URWN
         RVkCs2E3VVP28+GiyS1uCGOPGrpfUUW2GwEaaNhPDa73e4tEEYjyhUHQwGeq1a6ubZgQ
         ZdHVsgGK2j+KfFBF+Sy7/pCH4Y3/o7b2lamAhAdYGojuJ5AVDWJaCMD4ZtgWSJI1A2jA
         Js80wvFSZCfcjwkZNZtVlVwVJSZdkAJwCNdSg3aBvNpdn8vMvKQnpkeEaslQVJImCM26
         UuFw==
MIME-Version: 1.0
X-Received: by 10.180.103.40 with SMTP id ft8mr61606767wib.68.1420739178031;
 Thu, 08 Jan 2015 09:46:18 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Thu, 8 Jan 2015 09:46:17 -0800 (PST)
In-Reply-To: <CA+-p3AG5SmS5KVfddbZmvZwMu10ONiq5R7MY3aX4LOgwEqZoZg@mail.gmail.com>
References: <CA+-p3AG5SmS5KVfddbZmvZwMu10ONiq5R7MY3aX4LOgwEqZoZg@mail.gmail.com>
Date: Thu, 8 Jan 2015 12:46:17 -0500
Message-ID: <CADtDQQ+MQE2peTsN_E6tfhxfuhHxkWuRR6R=E3O4ha4Ks_stbw@mail.gmail.com>
Subject: Re: Maintainer for Mesos
From: RJ Nowling <rnowling@gmail.com>
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d044282ac06704f050c279ee5
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044282ac06704f050c279ee5
Content-Type: text/plain; charset=UTF-8

Hi Andrew,

Patrick Wendell and Andrew Or have committed previous patches related to
Mesos. Maybe they would be good committers to look at it?

RJ

On Mon, Jan 5, 2015 at 6:40 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi Spark devs,
>
> I'm interested in having a committer look at a PR [1] for Mesos, but
> there's not an entry for Mesos in the maintainers specialties on the wiki
> [2].  Which Spark committers have expertise in the Mesos features?
>
> Thanks!
> Andrew
>
>
> [1] https://github.com/apache/spark/pull/3074
> [2]
>
> https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers
>

--f46d044282ac06704f050c279ee5--

From dev-return-11055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 18:40:14 2015
Return-Path: <dev-return-11055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87D02CF3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 18:40:14 +0000 (UTC)
Received: (qmail 32082 invoked by uid 500); 8 Jan 2015 18:40:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31997 invoked by uid 500); 8 Jan 2015 18:40:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31965 invoked by uid 99); 8 Jan 2015 18:40:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 18:40:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 18:39:46 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so3829062qgd.5
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 10:39:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=um5fuKaYLBfalMLUerf8xlWCmBdR+V/9uUN0a+cfACM=;
        b=HQ3FugLTYePuaLMUbTABjiuzbZDgwdo+3Vn9aWQv9AW9EkBSMPjcuNpa/G/3LQ+TS1
         hoaTEjXca8nXht4ivSwfrNJo/YkVIZ4vwqw3U2C9HIbiR3DOuwF9gReXxnjkfplX+aXI
         T6tZAlj+0xQLCKEtuCmCL7LMlniFLOz/K6fpmgvf1SeDnYmLuBnIHq5D1HOs5JZSeFXy
         mznuX+1Fr/rzxoiEL+X3g4GT/0dsCcamSdkqXCN+IXJl7apne8+6pVTsVTY9Z0sUdTBD
         /nWOXHlafGm2MoZ+g+yZPirMQ1rOcQNfNqaR45ClB4eRnQguNQCNeBvB09J1zVlHqA5X
         qHyg==
X-Gm-Message-State: ALoCoQmGS8QC/C/sacar2hfXRxXvmJXU70tVS+S/fC0C0bjymT0rAhQZsR730+JTxEC9zH5nSUh7
X-Received: by 10.140.48.197 with SMTP id o63mr17467338qga.81.1420742364190;
 Thu, 08 Jan 2015 10:39:24 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 8 Jan 2015 10:39:04 -0800 (PST)
In-Reply-To: <CACRMEukv1iH9c9nbL2=iDG9BzFRXuE3QYn2GLUjZckXY8x0zXA@mail.gmail.com>
References: <CACRMEukv1iH9c9nbL2=iDG9BzFRXuE3QYn2GLUjZckXY8x0zXA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 8 Jan 2015 10:39:04 -0800
Message-ID: <CAPh_B=YaeG065ShaX5Wv9=YVS520YKd_302Yb54RXawmfbcOLg@mail.gmail.com>
Subject: Re: Spark on teradata?
To: gen tang <gen.tang86@gmail.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11350ee6ef7046050c285be1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11350ee6ef7046050c285be1
Content-Type: text/plain; charset=UTF-8

Depending on your use cases. If the use case is to extract small amount of
data out of teradata, then you can use the JdbcRDD and soon a jdbc input
source based on the new Spark SQL external data source API.



On Wed, Jan 7, 2015 at 7:14 AM, gen tang <gen.tang86@gmail.com> wrote:

> Hi,
>
> I have a stupid question:
> Is it possible to use spark on Teradata data warehouse, please? I read
> some news on internet which say yes. However, I didn't find any example
> about this issue
>
> Thanks in advance.
>
> Cheers
> Gen
>
>

--001a11350ee6ef7046050c285be1--

From dev-return-11056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 20:48:53 2015
Return-Path: <dev-return-11056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E594F10785
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 20:48:53 +0000 (UTC)
Received: (qmail 41405 invoked by uid 500); 8 Jan 2015 20:48:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41333 invoked by uid 500); 8 Jan 2015 20:48:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41322 invoked by uid 99); 8 Jan 2015 20:48:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 20:48:52 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of devl.development@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 20:48:47 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 1ADB6FED1AA
	for <dev@spark.apache.org>; Thu,  8 Jan 2015 12:47:58 -0800 (PST)
Date: Thu, 8 Jan 2015 13:47:56 -0700 (MST)
From: "devl.development" <devl.development@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420750076524-10047.post@n3.nabble.com>
In-Reply-To: <CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com> <CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
Subject: Re: K-Means And Class Tags
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for the suggestion, can anyone offer any advice on the ClassCast
Exception going from Java to Scala? Why does going from JavaRDD.rdd() and
then a collect() result in this exception?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/K-Means-And-Class-Tags-tp10038p10047.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 20:50:07 2015
Return-Path: <dev-return-11057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D94110795
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 20:50:07 +0000 (UTC)
Received: (qmail 45513 invoked by uid 500); 8 Jan 2015 20:50:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45443 invoked by uid 500); 8 Jan 2015 20:50:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45423 invoked by uid 99); 8 Jan 2015 20:50:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 20:50:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of devl.development@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 20:49:34 +0000
Received: by mail-qg0-f45.google.com with SMTP id z107so4727563qgd.4
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 12:48:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/yvA8cPO2E3AZDJchZwqcCZIpHse23LbTmR7QCjvZ0s=;
        b=oIfSstEpZTMeMyajErGU8GWSYCdz4W0p/aI99urwWLNMhNFSjVveNRiz+Y8ZkOiPWB
         zonIKjvw8DyDK88dA/NdZ+qRR0Ltn9tHTkyj/5uCuzHH0E9Q+CnXeD/d4TUNlPgvz9UA
         oYBzKpH8YkX8J0haAxRaOEC8XSUOaeZIk60+V8/wjvnYbWywHR7LFTvNub5bfMx5qvfS
         azSGvucYrFE2A6fSyNCHoNr2dofxsgOGsD+EUiwxxSLzatJpijIhMa/WLtxZrrX02olG
         Db8TM3j/kIUpOMFKaH0ucgXCD+3y++WjRUcYepjunu4HWJRUTXMc1K/8AII7q/32CYI0
         NhSw==
MIME-Version: 1.0
X-Received: by 10.229.140.72 with SMTP id h8mr19707510qcu.25.1420750128200;
 Thu, 08 Jan 2015 12:48:48 -0800 (PST)
Received: by 10.140.102.111 with HTTP; Thu, 8 Jan 2015 12:48:48 -0800 (PST)
In-Reply-To: <CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
	<CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
Date: Thu, 8 Jan 2015 20:48:48 +0000
Message-ID: <CAMQ+LQPnhw3eKRxDAQzk5iFYFn2FXxvw+ygb+D-EBRVmU+hXYA@mail.gmail.com>
Subject: Re: K-Means And Class Tags
From: Devl Devel <devl.development@gmail.com>
To: yana.kadiyska@gmail.com
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ee40b4c23f050c2a2a5d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ee40b4c23f050c2a2a5d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the suggestion, can anyone offer any advice on the ClassCast
Exception going from Java to Scala? Why does JavaRDD.rdd() and then a
collect() result in this exception?

On Thu, Jan 8, 2015 at 4:13 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
wrote:

> How about
>
> data.map(s=3D>s.split(",")).filter(_.length>1).map(good_entry=3D>Vectors.=
dense((Double.parseDouble(good_entry[0]),
> Double.parseDouble(good_entry[1]))
> =E2=80=8B
> (full disclosure, I didn't actually run this). But after the first map yo=
u
> should have an RDD[Array[String]], then you'd discard everything shorter
> than 2, and convert the rest to dense vectors?...In fact if you're
> expecting length exactly 2 might want to filter =3D=3D2...
>
>
> On Thu, Jan 8, 2015 at 10:58 AM, Devl Devel <devl.development@gmail.com>
> wrote:
>
>> Hi All,
>>
>> I'm trying a simple K-Means example as per the website:
>>
>> val parsedData =3D data.map(s =3D>
>> Vectors.dense(s.split(',').map(_.toDouble)))
>>
>> but I'm trying to write a Java based validation method first so that
>> missing values are omitted or replaced with 0.
>>
>> public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
>>         JavaRDD<Vector> words =3D data.flatMap(new FlatMapFunction<Strin=
g,
>> Vector>() {
>>             public Iterable<Vector> call(String s) {
>>                 String[] split =3D s.split(",");
>>                 ArrayList<Vector> add =3D new ArrayList<Vector>();
>>                 if (split.length !=3D 2) {
>>                     add.add(Vectors.dense(0, 0));
>>                 } else
>>                 {
>>                     add.add(Vectors.dense(Double.parseDouble(split[0]),
>>                Double.parseDouble(split[1])));
>>                 }
>>
>>                 return add;
>>             }
>>         });
>>
>>         return words.rdd();
>> }
>>
>> When I then call from scala:
>>
>> val parsedData=3Ddc.prepareKMeans(data);
>> val p=3DparsedData.collect();
>>
>> I get Exception in thread "main" java.lang.ClassCastException:
>> [Ljava.lang.Object; cannot be cast to
>> [Lorg.apache.spark.mllib.linalg.Vector;
>>
>> Why is the class tag is object rather than vector?
>>
>> 1) How do I get this working correctly using the Java validation example
>> above or
>> 2) How can I modify val parsedData =3D data.map(s =3D>
>> Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split size <=
2
>> I
>> ignore the line? or
>> 3) Is there a better way to do input validation first?
>>
>> Using spark and mlib:
>> libraryDependencies +=3D "org.apache.spark" % "spark-core_2.10" %  "1.2.=
0"
>> libraryDependencies +=3D "org.apache.spark" % "spark-mllib_2.10" % "1.2.=
0"
>>
>> Many thanks in advance
>> Dev
>>
>
>

--001a1132ee40b4c23f050c2a2a5d--

From dev-return-11058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 21:36:37 2015
Return-Path: <dev-return-11058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A4741098D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 21:36:37 +0000 (UTC)
Received: (qmail 47646 invoked by uid 500); 8 Jan 2015 21:36:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47572 invoked by uid 500); 8 Jan 2015 21:36:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47551 invoked by uid 99); 8 Jan 2015 21:36:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 21:36:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 21:36:10 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so10297293obc.0
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 13:35:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=phXxngzZjpyjCUJ2gADgLwDfUWeaQm+pK/ZmsD8vL2E=;
        b=nBl9Phkby7IXMULm2jMWd88cDuR+VARfYAK7XIxQDRsTv4xReSY7NsvvQYoUSu43Fi
         dAG3TkV7aFvr3EbNgQHXq/2BPZCIxb5IIFtLYvEfZ2Yl5UxmmB88qZnHqy9GoCCSdXpk
         49xLRd4c4wJbxe8qg6QPD0YcJdqAvf+wXnUqvUCPRpe4KEzOQYgChvQSUJtyAYXhVgOc
         nTt+Qed9r5P7z66B2PxbEcLKKYWvrX59jBwjr81XQdb7QQPvs4qJ7SP4kZ1uz5cfieTP
         GKZJGq84hJ9i1o/DkLKFYA4N3CGI6EiySFoI4r+iCgaShkuWNezmKxo37tc60IfwP7JQ
         /JjQ==
X-Gm-Message-State: ALoCoQm1F8STcpnq8/+bEGu2jFiHtYIsG0I2tpEY2DIPfln7NohTaaGSnvCLfF6KIMMHODCw84QZ
MIME-Version: 1.0
X-Received: by 10.182.251.138 with SMTP id zk10mr7228784obc.72.1420752947522;
 Thu, 08 Jan 2015 13:35:47 -0800 (PST)
Received: by 10.60.9.197 with HTTP; Thu, 8 Jan 2015 13:35:47 -0800 (PST)
In-Reply-To: <CAMQ+LQPnhw3eKRxDAQzk5iFYFn2FXxvw+ygb+D-EBRVmU+hXYA@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
	<CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
	<CAMQ+LQPnhw3eKRxDAQzk5iFYFn2FXxvw+ygb+D-EBRVmU+hXYA@mail.gmail.com>
Date: Thu, 8 Jan 2015 13:35:47 -0800
Message-ID: <CAF7ADNqLu=w+Emc_6q0O0d_6QauSFmH22a2qNj41nPr6q+RDbw@mail.gmail.com>
Subject: Re: K-Means And Class Tags
From: Joseph Bradley <joseph@databricks.com>
To: Devl Devel <devl.development@gmail.com>
Cc: yana.kadiyska@gmail.com, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1dadac0482a050c2ad250
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1dadac0482a050c2ad250
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I believe you're running into an erasure issue which we found in
DecisionTree too.  Check out:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache=
/spark/mllib/tree/RandomForest.scala#L134

That retags RDDs which were created from Java to prevent the exception
you're running into.

Hope this helps!
Joseph

On Thu, Jan 8, 2015 at 12:48 PM, Devl Devel <devl.development@gmail.com>
wrote:

> Thanks for the suggestion, can anyone offer any advice on the ClassCast
> Exception going from Java to Scala? Why does JavaRDD.rdd() and then a
> collect() result in this exception?
>
> On Thu, Jan 8, 2015 at 4:13 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
> wrote:
>
> > How about
> >
> >
> data.map(s=3D>s.split(",")).filter(_.length>1).map(good_entry=3D>Vectors.=
dense((Double.parseDouble(good_entry[0]),
> > Double.parseDouble(good_entry[1]))
> > =E2=80=8B
> > (full disclosure, I didn't actually run this). But after the first map
> you
> > should have an RDD[Array[String]], then you'd discard everything shorte=
r
> > than 2, and convert the rest to dense vectors?...In fact if you're
> > expecting length exactly 2 might want to filter =3D=3D2...
> >
> >
> > On Thu, Jan 8, 2015 at 10:58 AM, Devl Devel <devl.development@gmail.com=
>
> > wrote:
> >
> >> Hi All,
> >>
> >> I'm trying a simple K-Means example as per the website:
> >>
> >> val parsedData =3D data.map(s =3D>
> >> Vectors.dense(s.split(',').map(_.toDouble)))
> >>
> >> but I'm trying to write a Java based validation method first so that
> >> missing values are omitted or replaced with 0.
> >>
> >> public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
> >>         JavaRDD<Vector> words =3D data.flatMap(new FlatMapFunction<Str=
ing,
> >> Vector>() {
> >>             public Iterable<Vector> call(String s) {
> >>                 String[] split =3D s.split(",");
> >>                 ArrayList<Vector> add =3D new ArrayList<Vector>();
> >>                 if (split.length !=3D 2) {
> >>                     add.add(Vectors.dense(0, 0));
> >>                 } else
> >>                 {
> >>                     add.add(Vectors.dense(Double.parseDouble(split[0])=
,
> >>                Double.parseDouble(split[1])));
> >>                 }
> >>
> >>                 return add;
> >>             }
> >>         });
> >>
> >>         return words.rdd();
> >> }
> >>
> >> When I then call from scala:
> >>
> >> val parsedData=3Ddc.prepareKMeans(data);
> >> val p=3DparsedData.collect();
> >>
> >> I get Exception in thread "main" java.lang.ClassCastException:
> >> [Ljava.lang.Object; cannot be cast to
> >> [Lorg.apache.spark.mllib.linalg.Vector;
> >>
> >> Why is the class tag is object rather than vector?
> >>
> >> 1) How do I get this working correctly using the Java validation examp=
le
> >> above or
> >> 2) How can I modify val parsedData =3D data.map(s =3D>
> >> Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split size
> <2
> >> I
> >> ignore the line? or
> >> 3) Is there a better way to do input validation first?
> >>
> >> Using spark and mlib:
> >> libraryDependencies +=3D "org.apache.spark" % "spark-core_2.10" %  "1.=
2.0"
> >> libraryDependencies +=3D "org.apache.spark" % "spark-mllib_2.10" % "1.=
2.0"
> >>
> >> Many thanks in advance
> >> Dev
> >>
> >
> >
>

--001a11c1dadac0482a050c2ad250--

From dev-return-11059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 22:38:56 2015
Return-Path: <dev-return-11059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B40F510BCF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 22:38:56 +0000 (UTC)
Received: (qmail 85807 invoked by uid 500); 8 Jan 2015 22:38:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85724 invoked by uid 500); 8 Jan 2015 22:38:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85700 invoked by uid 99); 8 Jan 2015 22:38:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 22:38:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of bbejeck@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 22:38:51 +0000
Received: by mail-ig0-f169.google.com with SMTP id z20so4888687igj.0
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 14:38:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/K5jVdHi/yoVPYpTORQYXnGfjKv9q+fw+bh3XvJzU6E=;
        b=qaV8Dwj0Y/l69D1wzHPmJn+KP71VUWC0+gmIEadQfDjySIx+SRZ2BtqMfcHngYfEgw
         57S3w4DNctRXx9GjFVVqdgbvY68tFxs2cR/8Us8JEvVHQ/kVNB82sAyfaKnHBWmyI6rt
         bM59AGqk1X+agwFBBUo2jQCyf7AcjkY5gaKKQGsgYsGoFvQdP0qtnrVDUoZ9m+qWcWka
         DNANrHvSgpY+B3cW3zgn5wEYYwNEL/dm3QAF3fMfNCyNkQOjqM/L4uvcrITDmwLf7l7m
         o9B4yYWrD66t/JBtpoOtAUuxQdb+uwSSV82K6AzbrRpKQ/W5kziZ/W42EESdgN5gbADi
         erQA==
MIME-Version: 1.0
X-Received: by 10.50.17.99 with SMTP id n3mr30596013igd.21.1420756710965; Thu,
 08 Jan 2015 14:38:30 -0800 (PST)
Received: by 10.64.93.67 with HTTP; Thu, 8 Jan 2015 14:38:30 -0800 (PST)
In-Reply-To: <3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
	<3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
Date: Thu, 8 Jan 2015 17:38:30 -0500
Message-ID: <CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
From: Bill Bejeck <bbejeck@gmail.com>
To: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
Cc: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122827811cfd1050c2bb31f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122827811cfd1050c2bb31f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I was having the same issue and that helped.  But now I get the following
compilation error when trying to run a test from within Intellij (v 14)

/Users/bbejeck/dev/github_clones/bbejeck-spark/sql/catalyst/src/main/scala/=
org/apache/spark/sql/catalyst/dsl/package.scala
Error:(308, 109) polymorphic expression cannot be instantiated to expected
type;
 found   : [T(in method
apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)=
]
 required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in
method functionToUdfBuilder)]
  implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
ScalaUdfBuilder[T] =3D ScalaUdfBuilder(func)

Any thoughts?

                                ^

On Thu, Jan 8, 2015 at 6:33 AM, Jakub Dubovsky <
spark.dubovsky.jakub@seznam.cz> wrote:

> Thanks that helped.
>
> I vote for wiki as well. More fine graned documentation should be on wiki
> and linked,
>
> Jakub
>
>
> ---------- P=C5=AFvodn=C3=AD zpr=C3=A1va ----------
> Od: Sean Owen <sowen@cloudera.com>
> Komu: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
> Datum: 8. 1. 2015 11:29:22
> P=C5=99edm=C4=9Bt: Re: Spark development with IntelliJ
>
> "Yeah, I hit this too. IntelliJ picks this up from the build but then
> it can't run its own scalac with this plugin added.
>
> Go to Preferences > Build, Execution, Deployment > Scala Compiler and
> clear the "Additional compiler options" field. It will work then
> although the option will come back when the project reimports.
>
> Right now I don't know of a better fix.
>
> There's another recent open question about updating IntelliJ docs:
> https://issues.apache.org/jira/browse/SPARK-5136 Should this stuff go
> in the site docs, or wiki? I vote for wiki I suppose and make the site
> docs point to the wiki. I'd be happy to make wiki edits if I can get
> permission, or propose this text along with other new text on the
> JIRA.
>
> On Thu, Jan 8, 2015 at 10:00 AM, Jakub Dubovsky
> <spark.dubovsky.jakub@seznam.cz> wrote:
> > Hi devs,
> >
> > I'd like to ask if anybody has experience with using intellij 14 to ste=
p
> > into spark code. Whatever I try I get compilation error:
> >
> > Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros=
/
> > paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar
> >
> > Project is set up by Patrick's instruction [1] and packaged by mvn -
> > DskipTests clean install. Compilation works fine. Then I just created
> > breakpoint in test code and run debug with the error.
> >
> > Thanks for any hints
> >
> > Jakub
> >
> > [1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+
> > Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org"
>

--089e0122827811cfd1050c2bb31f--

From dev-return-11060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 22:43:49 2015
Return-Path: <dev-return-11060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F3AFA10C0B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 22:43:48 +0000 (UTC)
Received: (qmail 3017 invoked by uid 500); 8 Jan 2015 22:43:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2946 invoked by uid 500); 8 Jan 2015 22:43:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2933 invoked by uid 99); 8 Jan 2015 22:43:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 22:43:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of bbejeck@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 22:43:42 +0000
Received: by mail-ie0-f178.google.com with SMTP id vy18so12107134iec.9
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 14:43:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Wr22hUNBSdwFp3tabVSyomMs03ZiZf3IsJWL3V6emIc=;
        b=QdUldqddqG+9JdRL6SYSk5t2slRjkoMG+J/Q98gjtBLsuE5OUR7TN4xkdM+2pavucB
         gR4ALs7ga2su51KqE3Fm8NAKRSmQ7cPHD46vEKGjV2x0NUKJrd5eUBlM2odd6auWM7Z1
         uEMq6GQxO2NSaFYBmxHCEJA4jq0FCVwc4NjlN/npZy5pAGlDH2jON5qF4dVm5MrJncTU
         TXKe/onO1oV4ZjXBG+vYAjnDtKvhOyqDZwLe7SRCt2cpVLMYq2/uaGKn4tVfn2Gqf2/5
         mKS/URtr78yeC4kDUlYmm9J5XcrbihAv8MzU3jNKr16LXlcZSEnZMLVTDHG0J4qSepGC
         oKRw==
MIME-Version: 1.0
X-Received: by 10.50.50.177 with SMTP id d17mr30708229igo.0.1420757001927;
 Thu, 08 Jan 2015 14:43:21 -0800 (PST)
Received: by 10.64.93.67 with HTTP; Thu, 8 Jan 2015 14:43:21 -0800 (PST)
Date: Thu, 8 Jan 2015 17:43:21 -0500
Message-ID: <CAF7WS+qWZEgnsjBJBnJhGgbmRCnmRYDArY_-QVZe8pEEJNQnVg@mail.gmail.com>
Subject: PR #3872
From: Bill Bejeck <bbejeck@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdca6866980d1050c2bc4fb
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdca6866980d1050c2bc4fb
Content-Type: text/plain; charset=UTF-8

Could one of the admins take a look at PR 3872 (JIRA 3299) submitted on 1/1

--047d7bdca6866980d1050c2bc4fb--

From dev-return-11061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 23:19:54 2015
Return-Path: <dev-return-11061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01F8910D3B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 23:19:54 +0000 (UTC)
Received: (qmail 75649 invoked by uid 500); 8 Jan 2015 23:19:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75571 invoked by uid 500); 8 Jan 2015 23:19:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75560 invoked by uid 99); 8 Jan 2015 23:19:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 23:19:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 23:19:48 +0000
Received: by mail-wi0-f170.google.com with SMTP id bs8so6004580wib.1
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 15:17:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=FcXWL1VIjxD9ZONip+3G7oWo3cDi/GTGJnUskIpSs+c=;
        b=TkyaKzYjWLup7cTBz/2iY1wVp6dDVmzqCAXGCMcp4azTg4mpbHiGdZNHlUS7WTBrGN
         mbukzQkKrCeXeUfNxOWe8iAULP1YGD4mIRrB+PASRUGEFgEPmq8mwIpmaOJ6fe0iRwAc
         4kJEj4ix6yN7hfZpShNtSmu/JbfL+nvItiBkS8dfJAmJeeZWA6gLcfbPgtA0/q83P85n
         1lzkwDDWW3s83VXwrzCH/EwuGeMCjkmZfsRt7/kw6VOmozy1Hq2Wqw9sB4roimifuDk2
         WaitISiQoXbQMrFTzUmeglnQ+vsA7uru+qhQBI8Oc+M6Fc/11yrqh2uhv0vJFX0AmvIu
         d0/w==
X-Gm-Message-State: ALoCoQnkczXnFTeX2OFj0utDrDkKeDbiRLn8w4KUf1jLUF2dFG+h38qj/B+TesPu/KEKblZHeN0U
X-Received: by 10.194.90.229 with SMTP id bz5mr25502563wjb.63.1420759077480;
 Thu, 08 Jan 2015 15:17:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Thu, 8 Jan 2015 15:17:37 -0800 (PST)
In-Reply-To: <CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
 <3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz> <CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 8 Jan 2015 23:17:37 +0000
Message-ID: <CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
To: Bill Bejeck <bbejeck@gmail.com>
Cc: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I remember seeing this too, but it seemed to be transient. Try
compiling again. In my case I recall that IJ was still reimporting
some modules when I tried to build. I don't see this error in general.

On Thu, Jan 8, 2015 at 10:38 PM, Bill Bejeck <bbejeck@gmail.com> wrote:
> I was having the same issue and that helped.  But now I get the following
> compilation error when trying to run a test from within Intellij (v 14)
>
> /Users/bbejeck/dev/github_clones/bbejeck-spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
> Error:(308, 109) polymorphic expression cannot be instantiated to expected
> type;
>  found   : [T(in method
> apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)]
>  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in
> method functionToUdfBuilder)]
>   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
> ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
>
> Any thoughts?
>
> ^

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan  8 23:23:51 2015
Return-Path: <dev-return-11062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DC7910D83
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  8 Jan 2015 23:23:51 +0000 (UTC)
Received: (qmail 88480 invoked by uid 500); 8 Jan 2015 23:23:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88405 invoked by uid 500); 8 Jan 2015 23:23:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88392 invoked by uid 99); 8 Jan 2015 23:23:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 23:23:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 Jan 2015 23:23:47 +0000
Received: by mail-ie0-f180.google.com with SMTP id rp18so12272695iec.11
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 15:22:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=X5Jw6Z9MwQtBm7ydv6snFa9ABuQPc5ehAg1jO+I9bMw=;
        b=npTUx9pTtLaMqjYUTdCRVhMMAz8wSPTE7j91gwHdBaNJw1lNMc6zF8svK7pLAOcPc/
         xDMquHJqj5nMi0z7hcMIvYn41a8pr7jVnTEuQIAERJaCWFgQUfuge1K6rU++ioC0Jv71
         /FMOK84vnV3l7VH5V3GQ4hPhi6EO+vUj+nxud/e8V9S2dKqTS4pi5B4V1ykkvaON0OpJ
         jXJm75YkQKmp9ubeFlBVNyPOW5eINuIPzUARxWXuwWcpw/b/VAnkk3jZlGs4uWRKr4OH
         DZX1UYVLwdAA/GoOkpjiTsSXzQzO2XS7vqg0EgYDhEGiGWT/vmx2XtX9ZVNuup3I1GX2
         4d6Q==
X-Received: by 10.50.1.48 with SMTP id 16mr12828137igj.45.1420759361530; Thu,
 08 Jan 2015 15:22:41 -0800 (PST)
MIME-Version: 1.0
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
 <3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz> <CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
 <CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 08 Jan 2015 23:22:40 +0000
Message-ID: <CAOhmDzcqjwW=u_f9pTuBeYYhoFc7S+6tZgA0RoMJtBOgONPZaQ@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
To: Sean Owen <sowen@cloudera.com>, Bill Bejeck <bbejeck@gmail.com>
Cc: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc12b00e33bf050c2c5128
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc12b00e33bf050c2c5128
Content-Type: text/plain; charset=UTF-8

Side question: Should this section
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup>
in
the wiki link to Useful Developer Tools
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools>?

On Thu Jan 08 2015 at 6:19:55 PM Sean Owen <sowen@cloudera.com> wrote:

> I remember seeing this too, but it seemed to be transient. Try
> compiling again. In my case I recall that IJ was still reimporting
> some modules when I tried to build. I don't see this error in general.
>
> On Thu, Jan 8, 2015 at 10:38 PM, Bill Bejeck <bbejeck@gmail.com> wrote:
> > I was having the same issue and that helped.  But now I get the following
> > compilation error when trying to run a test from within Intellij (v 14)
> >
> > /Users/bbejeck/dev/github_clones/bbejeck-spark/sql/
> catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
> > Error:(308, 109) polymorphic expression cannot be instantiated to
> expected
> > type;
> >  found   : [T(in method
> > apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method
> apply)]
> >  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(
> in
> > method functionToUdfBuilder)]
> >   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
> > ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
> >
> > Any thoughts?
> >
> > ^
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bdc12b00e33bf050c2c5128--

From dev-return-11063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 02:15:45 2015
Return-Path: <dev-return-11063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EBB2175EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 02:15:45 +0000 (UTC)
Received: (qmail 54333 invoked by uid 500); 9 Jan 2015 02:15:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54262 invoked by uid 500); 9 Jan 2015 02:15:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54229 invoked by uid 99); 9 Jan 2015 02:15:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:15:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of bbejeck@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:15:38 +0000
Received: by mail-ig0-f171.google.com with SMTP id z20so5932991igj.4
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 18:14:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8JLP5db7U+I22irumnTHLJpXoysYOQwdV15ZGu6Y/Xk=;
        b=TAOhWpVY09i9uqxmTToQFL+gL1YoHgTWqnliivWn+NoQcIWvY4NuX3yBBqHyHAd/VJ
         9Ed+lNDImXdcqfVv/1uzkV5QXrgsisP4sS7/aH59iIRVv8fHDO8IHF7W6FZZoKz21pXP
         jrkx40ovmle8eujEqJpb2+4dDI7QsnetQ8ijlHm4/oabs8t1SFhgOxGOtkl4wlXLBfQC
         sVxJBdvfxnkZDMmYdOVLt+u6xRBZN6cAgYr1aplmQsY0QkkU7sxAJeXUilt9BFZ56aXH
         P8fdfL8Mfiv3UX4AXjNs5aueNFIp7ZPNeGZ3Zw+NJR78Kdi5EwTLvpoBcDKr/Kmy4qvP
         yd9A==
MIME-Version: 1.0
X-Received: by 10.50.138.226 with SMTP id qt2mr228013igb.1.1420769672847; Thu,
 08 Jan 2015 18:14:32 -0800 (PST)
Received: by 10.64.93.67 with HTTP; Thu, 8 Jan 2015 18:14:32 -0800 (PST)
In-Reply-To: <CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
	<3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
	<CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
	<CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
Date: Thu, 8 Jan 2015 21:14:32 -0500
Message-ID: <CAF7WS+r8N7s0LgKWK4U-=0HiJCuMV3vvFvOBxafMd4ZW88utww@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
From: Bill Bejeck <bbejeck@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122a208a869cd050c2eb71e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122a208a869cd050c2eb71e
Content-Type: text/plain; charset=UTF-8

That worked, thx

On Thu, Jan 8, 2015 at 6:17 PM, Sean Owen <sowen@cloudera.com> wrote:

> I remember seeing this too, but it seemed to be transient. Try
> compiling again. In my case I recall that IJ was still reimporting
> some modules when I tried to build. I don't see this error in general.
>
> On Thu, Jan 8, 2015 at 10:38 PM, Bill Bejeck <bbejeck@gmail.com> wrote:
> > I was having the same issue and that helped.  But now I get the following
> > compilation error when trying to run a test from within Intellij (v 14)
> >
> >
> /Users/bbejeck/dev/github_clones/bbejeck-spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
> > Error:(308, 109) polymorphic expression cannot be instantiated to
> expected
> > type;
> >  found   : [T(in method
> > apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method
> apply)]
> >  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in
> > method functionToUdfBuilder)]
> >   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
> > ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
> >
> > Any thoughts?
> >
> > ^
>

--089e0122a208a869cd050c2eb71e--

From dev-return-11064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 02:35:07 2015
Return-Path: <dev-return-11064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5AB3517688
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 02:35:07 +0000 (UTC)
Received: (qmail 85094 invoked by uid 500); 9 Jan 2015 02:35:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85011 invoked by uid 500); 9 Jan 2015 02:35:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84992 invoked by uid 99); 9 Jan 2015 02:35:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:35:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:34:55 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so6128608qgd.5
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 18:33:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:subject:mime-version:content-type;
        bh=sstSCt2xOztNNr1z7YsW1wYCf7SNcND97kV3tBjkJls=;
        b=Sle3LiUaSmS9IHxZFONVY3hFEeV49EHRvdtpJWvmcq/HMBXwO91g9hIFso3VuGi9qa
         aJprQ+zCsVldASINmJmRWa4bgjI+TKNoppOhd1R72aDhrYo4eXo2XaDcWJNZKYaDVppI
         6XKmy+DAVYxy6nTYUx2cTKS2VOrhKXqW1xBOEG4cL4uYZyFRj91nAd824zzGEMVdbfY5
         AGhjnj1ckwH33/uMcVejiLHNJ8PqqvLUcF6LzAVW+EdcQ56XrJpRLU+2hDOqPW7TPI/K
         ZxnlAbu1iFoTUU2EDhSFFTfUoUBDyZ/of/hyYl9p5IyeVnXSvdlTftzO4oe01tTg5QLU
         xciw==
X-Received: by 10.140.44.69 with SMTP id f63mr20719068qga.3.1420770830322;
        Thu, 08 Jan 2015 18:33:50 -0800 (PST)
Received: from [192.168.2.16] (bas3-montreal42-1168076984.dsl.bell.ca. [69.159.112.184])
        by mx.google.com with ESMTPSA id g20sm5786145qar.17.2015.01.08.18.33.49
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Thu, 08 Jan 2015 18:33:49 -0800 (PST)
Date: Thu, 8 Jan 2015 21:33:48 -0500
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: tdas@databricks.com
Message-ID: <105FCC9386BD420B9E4EDA0E8563CBC1@gmail.com>
Subject: missing document of several messages in actor-based receiver?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54af3e0c_70a64e2a_19a"
X-Virus-Checked: Checked by ClamAV on apache.org

--54af3e0c_70a64e2a_19a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi, TD and other streaming developers,

When I look at the implementation of actor-based receiver (ActorReceiver.=
scala), I found that there are several messages which are not mentioned i=
n the document =20

case props: Props =3D>
val worker =3D context.actorOf(props)
logInfo(=22Started receiver worker at:=22 + worker.path)
sender =21 worker

case (props: Props, name: String) =3D>
val worker =3D context.actorOf(props, name)
logInfo(=22Started receiver worker at:=22 + worker.path)
sender =21 worker

case =5F: PossiblyHarmful =3D> hiccups.incrementAndGet()

case =5F: Statistics =3D>
val workers =3D context.children
sender =21 Statistics(n.get, workers.size, hiccups.get, workers.mkString(=
=22=5Cn=E2=80=9D))

Is it hided with intention or incomplete document, or I missed something=3F=

And the handler of these messages are =E2=80=9Cbuggy=22=3F e.g. when we s=
tart a new worker, we didn=E2=80=99t increase n (counter of children), an=
d n and hiccups are unnecessarily set to AtomicInteger =3F

Best,

-- =20
Nan Zhu
http://codingcat.me


--54af3e0c_70a64e2a_19a--


From dev-return-11065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 02:49:02 2015
Return-Path: <dev-return-11065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 895FA1772F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 02:49:02 +0000 (UTC)
Received: (qmail 13756 invoked by uid 500); 9 Jan 2015 02:49:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13670 invoked by uid 500); 9 Jan 2015 02:49:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12525 invoked by uid 99); 9 Jan 2015 02:49:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:49:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lewis.mcgibbney@gmail.com designates 209.85.217.174 as permitted sender)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 02:48:55 +0000
Received: by mail-lb0-f174.google.com with SMTP id 10so6685423lbg.5;
        Thu, 08 Jan 2015 18:47:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=pk5UsQFG99D+G9mrlEJ9TA7GDu8FEaacWPAw0WaqpmY=;
        b=sIJjjCdCc4xIslSWLuniIGEDmgZllLPzBlKUQTjo/fWrTP+NYLueNWXR60/3bRB+D9
         UHW32xLs7sDMo/YLyMX0Mzr0fRBIleE16VpLEnzZotbpIEsyPHXHfapnCFiRuSOq73Wx
         H0Xwn87KEQpV4tPXTedlZZyqAO5dIjAIz+GqXctkgjQMTWG6i+U4brErqBpZO0LmTUS1
         ncwFavWm05nlbi6p1xvsxvmYfeLL7BVJLYF4AbfMhLW1JzO1QLrM77CAl+IfxkL1pBT5
         JNRGULFEsrYoS9WvOWA0IJtDBfM66twsIWZcZVsMS7iWvOpHs2NlMSJ4qUqY6QE0dKWD
         j16A==
MIME-Version: 1.0
X-Received: by 10.112.134.74 with SMTP id pi10mr18416960lbb.67.1420771669730;
 Thu, 08 Jan 2015 18:47:49 -0800 (PST)
Received: by 10.112.44.225 with HTTP; Thu, 8 Jan 2015 18:47:49 -0800 (PST)
Date: Thu, 8 Jan 2015 18:47:49 -0800
Message-ID: <CAGaRif31WWtgNS8=7K7aHZ=UCcXNz90y8-1bvX8_BarPongLFA@mail.gmail.com>
Subject: [ANNOUNCE] Apache Science and Healthcare Track @ApacheCon NA 2015
From: Lewis John Mcgibbney <lewis.mcgibbney@gmail.com>
To: user@ctakes.apache.org, dev@ctakes.apache.org, user@tika.apache.org, 
	user@spark.apache.org, dev@spark.apache.org, user@sis.apache.org, 
	dev@sis.apache.org, user@taverna.incubator.apache.org, 
	dev@taverna.incubator.apache.org, user@airavata.apache.org, 
	Airavata Dev <dev@airavata.apache.org>, user@jena.apache.org, dev@jena.apache.org, 
	user@clerezza.apache.org, dev@clerezza.apache.org, user@marmotta.apache.org, 
	dev@marmotta.apache.org, user@stanbol.apache.org, dev@stanbol.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a8ff0ae6b80050c2f2e9e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8ff0ae6b80050c2f2e9e
Content-Type: text/plain; charset=UTF-8

Hi Folks,

Apologies for cross posting :(

As some of you may already know, @ApacheCon NA 2015 is happening in Austin,
TX April 13th-16th.

This email is specifically written to attract all folks interested in
Science and Healthcare... this is an official call to arms! I am aware that
there are many Science and Healthcare-type people also lingering in the
Apache Semantic Web communities so this one is for all of you folks as well.

Over a number of years the Science track has been emerging as an attractive
and exciting, at times mind blowing non-traditional track running alongside
the resident HTTP server, Big Data, etc tracks. The Semantic Web Track is
another such emerging track which has proved popular. This year we want to
really get the message out there about how much Apache technology is
actually being used in Science and Healthcare. This is not *only* aimed at
attracting members of the communities below
<http://wiki.apache.org/apachecon/ACNA2015ContentCommittee#Target_Projects>
but also at potentially attracting a brand new breed of conference
participants to ApacheCon <https://wiki.apache.org/apachecon/ApacheCon> and
the Foundation e.g. Scientists who love Apache. We are looking for
exciting, invigorating, obscure, half-baked, funky, academic, practical and
impractical stories, use cases, experiments and down right successes alike
from within the Science domain. The only thing they need to have in common
is that they consume, contribute towards, advocate, disseminate or even
commercialize Apache technology within the Scientific domain and would be
relevant to that audience. It is fully open to interest whether this track
be combined with the proposed *healthcare track*... if there is interest to
do this then we can rename this track to Science and Healthcare. In essence
one could argue that they are one and the same however I digress [image: :)]

What I would like those of you that are interested to do, is to merely
check out the scope and intent of the Apache in Science content curation
which is currently ongoing and to potentially register your interest.

https://wiki.apache.org/apachecon/ACNA2015ContentCommittee#Apache_in_Science

I would love to see the Science and Healthcare track be THE BIGGEST track
@ApacheCon, and although we have some way to go, I'm sure many previous
track participants will tell you this is not to missed.

We are looking for content from a wide variety of Scientific use cases all
related to Apache technology.
Thanks in advance and I look forward to seeing you in Austin.
Lewis

-- 
*Lewis*

--047d7b3a8ff0ae6b80050c2f2e9e--

From dev-return-11066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 06:27:17 2015
Return-Path: <dev-return-11066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C5AAB17B6E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 06:27:17 +0000 (UTC)
Received: (qmail 85440 invoked by uid 500); 9 Jan 2015 06:27:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85365 invoked by uid 500); 9 Jan 2015 06:27:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85353 invoked by uid 99); 9 Jan 2015 06:27:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 06:27:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 06:26:52 +0000
Received: by mail-oi0-f43.google.com with SMTP id i138so10736731oig.2
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 22:25:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Og8gSMDJtGm71i0nPKPwahZbx4WGmg0VUSKIcpPaeGc=;
        b=IAIDiGHS5Byo52B/FlmHH76AFF45f6I+WW+cANn+BxrXbD055pfqn6QQf4Ob7DaGMg
         EdicMoizqJrUf8Ez/PwCCA5bTDnsSjMo4L5RhxNLnOBKab/xy0W+d6ekWeuNt3lup5TN
         3K7j9l8w80GGJHmivdttQ4DCYLN59KA6AR+2/6XdtDPhXZENL4WWAXHMd08Bo1AgmH3Q
         qtOGnHec1HC8gQ2IYop66uit8qATZjL0Mlj+kdMq0oL+cIFpALaf27DNyfCEZqYcifZE
         +iapSoLIBwkBIRBcem13W2T+ii6g5G4LxyZG88eHGC0q4z3JNhfQXWWxCmPBlK7K1vFQ
         CZyg==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr7738950oie.78.1420784720508;
 Thu, 08 Jan 2015 22:25:20 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Thu, 8 Jan 2015 22:25:20 -0800 (PST)
In-Reply-To: <CAOhmDzcqjwW=u_f9pTuBeYYhoFc7S+6tZgA0RoMJtBOgONPZaQ@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
	<3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
	<CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
	<CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
	<CAOhmDzcqjwW=u_f9pTuBeYYhoFc7S+6tZgA0RoMJtBOgONPZaQ@mail.gmail.com>
Date: Thu, 8 Jan 2015 22:25:20 -0800
Message-ID: <CABPQxsv52BgekS6gpqcrzBNkkyA2GdwWvBvWY4h5NftZ-+wOjw@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Bill Bejeck <bbejeck@gmail.com>, 
	Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Nick - yes. Do you mind moving it? I should have put it in the
"Contributing to Spark" page.

On Thu, Jan 8, 2015 at 3:22 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Side question: Should this section
> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup>
> in
> the wiki link to Useful Developer Tools
> <https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools>?
>
> On Thu Jan 08 2015 at 6:19:55 PM Sean Owen <sowen@cloudera.com> wrote:
>
>> I remember seeing this too, but it seemed to be transient. Try
>> compiling again. In my case I recall that IJ was still reimporting
>> some modules when I tried to build. I don't see this error in general.
>>
>> On Thu, Jan 8, 2015 at 10:38 PM, Bill Bejeck <bbejeck@gmail.com> wrote:
>> > I was having the same issue and that helped.  But now I get the following
>> > compilation error when trying to run a test from within Intellij (v 14)
>> >
>> > /Users/bbejeck/dev/github_clones/bbejeck-spark/sql/
>> catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
>> > Error:(308, 109) polymorphic expression cannot be instantiated to
>> expected
>> > type;
>> >  found   : [T(in method
>> > apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method
>> apply)]
>> >  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(
>> in
>> > method functionToUdfBuilder)]
>> >   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
>> > ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
>> >
>> > Any thoughts?
>> >
>> > ^
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 06:59:47 2015
Return-Path: <dev-return-11067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1960F17C27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 06:59:47 +0000 (UTC)
Received: (qmail 68737 invoked by uid 500); 9 Jan 2015 06:59:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68655 invoked by uid 500); 9 Jan 2015 06:59:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68639 invoked by uid 99); 9 Jan 2015 06:59:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 06:59:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 06:59:20 +0000
Received: by mail-oi0-f49.google.com with SMTP id a141so10749336oig.8
        for <dev@spark.apache.org>; Thu, 08 Jan 2015 22:58:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=r3j/naEw+dfwFVExYA+XpFF0Ao0Zsq/YiHXn70D9ICU=;
        b=gEkcN+G3pm77CthsX8cwky/ULqH03vuIwAA/2lloySpgLzfRYje5x3tAUglK+okb4u
         KsrSneoftqvtrFpdHz+rrYszz/ph2SK+MFDIJ5SjCmTqdUoLFhUx3XIvMx2zzp3nBOAI
         AK4spQlhaoKY15rqgC4kALL/l0nglkDHdwvWaZwvtvJN/rkLToEuRak3iVHtZ9qYBnja
         ZTKF5Ofjs8LrjwlB3r4P62S17pTVS5Xrgv49t1DrYCgdwJR+3tvx/4hJIXsKbHAWezJW
         XhzCaRQtOP2329cniZG6hXGDELWGAYgKRgcISYbrgZMdCKCGR6T3H9BxyDZM+/dOrXWy
         UI3A==
MIME-Version: 1.0
X-Received: by 10.182.33.138 with SMTP id r10mr8271870obi.67.1420786714300;
 Thu, 08 Jan 2015 22:58:34 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Thu, 8 Jan 2015 22:58:34 -0800 (PST)
In-Reply-To: <CABPQxsv52BgekS6gpqcrzBNkkyA2GdwWvBvWY4h5NftZ-+wOjw@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
	<3es1.3c1ob.1Cb9ZfjeQj4.1KhciC@seznam.cz>
	<CAF7WS+qn3s1jGyv9NUrcBk30VG6+YR2brz3rMy8-G-5VooBJiw@mail.gmail.com>
	<CAMAsSdKe1py7zo87AOurobi_hR80XykqYWY_Nroh9WwCO4QmjA@mail.gmail.com>
	<CAOhmDzcqjwW=u_f9pTuBeYYhoFc7S+6tZgA0RoMJtBOgONPZaQ@mail.gmail.com>
	<CABPQxsv52BgekS6gpqcrzBNkkyA2GdwWvBvWY4h5NftZ-+wOjw@mail.gmail.com>
Date: Thu, 8 Jan 2015 22:58:34 -0800
Message-ID: <CABPQxsv+rc7OA1neqrPcynKuGyoi7TEG2m6pAvcabndsOioG4Q@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Bill Bejeck <bbejeck@gmail.com>, 
	Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Actually I went ahead and did it.

On Thu, Jan 8, 2015 at 10:25 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Nick - yes. Do you mind moving it? I should have put it in the
> "Contributing to Spark" page.
>
> On Thu, Jan 8, 2015 at 3:22 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
>> Side question: Should this section
>> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup>
>> in
>> the wiki link to Useful Developer Tools
>> <https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools>?
>>
>> On Thu Jan 08 2015 at 6:19:55 PM Sean Owen <sowen@cloudera.com> wrote:
>>
>>> I remember seeing this too, but it seemed to be transient. Try
>>> compiling again. In my case I recall that IJ was still reimporting
>>> some modules when I tried to build. I don't see this error in general.
>>>
>>> On Thu, Jan 8, 2015 at 10:38 PM, Bill Bejeck <bbejeck@gmail.com> wrote:
>>> > I was having the same issue and that helped.  But now I get the following
>>> > compilation error when trying to run a test from within Intellij (v 14)
>>> >
>>> > /Users/bbejeck/dev/github_clones/bbejeck-spark/sql/
>>> catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
>>> > Error:(308, 109) polymorphic expression cannot be instantiated to
>>> expected
>>> > type;
>>> >  found   : [T(in method
>>> > apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method
>>> apply)]
>>> >  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(
>>> in
>>> > method functionToUdfBuilder)]
>>> >   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
>>> > ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
>>> >
>>> > Any thoughts?
>>> >
>>> > ^
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 09:17:43 2015
Return-Path: <dev-return-11068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D035317F28
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 09:17:43 +0000 (UTC)
Received: (qmail 85267 invoked by uid 500); 9 Jan 2015 09:17:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85187 invoked by uid 500); 9 Jan 2015 09:17:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85174 invoked by uid 99); 9 Jan 2015 09:17:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 09:17:43 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [129.184.85.11] (HELO odin2.bull.net) (129.184.85.11)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 09:17:39 +0000
Received: from BUMSG1WM.fr.ad.bull.net (bumsg1wm.fr.ad.bull.net [10.192.1.15])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by odin2.bull.net (Bull S.A.) with ESMTP id 78353203F9;
	Fri,  9 Jan 2015 10:15:46 +0100 (CET)
Received: from BUMSG3WM.fr.ad.bull.net ([10.192.1.139]) by
 BUMSG1WM.fr.ad.bull.net ([10.192.1.15]) with mapi id 14.03.0210.002; Fri, 9
 Jan 2015 10:15:46 +0100
From: Tony Reix <tony.reix@bull.net>
To: Ted Yu <yuzhihong@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Pascal Oliva
	<pascal.oliva@bull.net>
Subject: RE:Results of tests
Thread-Topic: Results of tests
Thread-Index: AdArSLaLLhxvOb+fRIS2Z8OrGN8LPwABFdkAAAPHSXL///tpAIABIfMr
Date: Fri, 9 Jan 2015 09:15:45 +0000
Message-ID: <5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
	<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>,<CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
In-Reply-To: <CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
Accept-Language: fr-FR, en-US
Content-Language: fr-FR
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.192.1.123]
Content-Type: multipart/alternative;
	boundary="_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6BUMSG3WMfradbul_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6BUMSG3WMfradbul_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Hi Ted

Thanks for the info.
However, I'm still unable to understand how the page:
   https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/
has been built.
This page contains details I do not find in the page you indicated to me:
   https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull

As an example, I'm still unable to find these details:
org.apache.spark<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spar=
k-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3Dcentos/testReport/org.apache.spark/>       12 mn   0
        1
        247
        248

org.apache.spark.api.python<https://amplab.cs.berkeley.edu/jenkins/view/Spa=
rk/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoo=
p-2.4,label=3Dcentos/testReport/org.apache.spark.api.python/> 20 ms   0
        0
        2
        2

org.apache.spark.bagel<https://amplab.cs.berkeley.edu/jenkins/view/Spark/jo=
b/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4=
,label=3Dcentos/testReport/org.apache.spark.bagel/>   7.7 s   0
        0
        4
        4

org.apache.spark.broadcast<https://amplab.cs.berkeley.edu/jenkins/view/Spar=
k/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop=
-2.4,label=3Dcentos/testReport/org.apache.spark.broadcast/>   43 s    0
        0
        17
        17

org.apache.spark.deploy<https://amplab.cs.berkeley.edu/jenkins/view/Spark/j=
ob/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.=
4,label=3Dcentos/testReport/org.apache.spark.deploy/> 16 s    0
        0
        29
        29

org.apache.spark.deploy.worker<https://amplab.cs.berkeley.edu/jenkins/view/=
Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dha=
doop-2.4,label=3Dcentos/testReport/org.apache.spark.deploy.worker/>   0.55 =
s  0
        0
        12
        12

........


Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests and 0 f=
ailures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ). When u=
sing IBM JVM, there are only 2566 tests and 5 failures (in same component: =
Streaming).

On my PPC64BE (BE =3D Big-Endian)environment, the tests block after 2 hundr=
eds of tests.
Is Spark independent of Little/Big-Endian stuff ?

On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests only (l=
ike on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...

So, I need to learn more about how your Jenkins environment extracts detail=
s about the results.
Moreover, which JVM is used ?

Do you plan to use IBM JVM in order to check that Spark and IBM JVM are com=
patible ? (they already do not look to be compatible 100% ...).

Thanks

Tony

IBM Coop Architect & Technical Leader
Office : +33 (0) 4 76 29 72 67
1 rue de Provence - 38432 =C9chirolles - France
www.atos.net<http://www.atos.net/>
________________________________
De : Ted Yu [yuzhihong@gmail.com]
Envoy=E9 : jeudi 8 janvier 2015 17:43
=C0 : Tony Reix
Cc : dev@spark.apache.org
Objet : Re: Results of tests

Here it is:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3=
.0.5/bin/mvn -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Ph=
adoop-2.4 -Pyarn -Phive clean package


You can find the above in https://amplab.cs.berkeley.edu/jenkins/view/Spark=
/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-=
2.4,label=3Dcentos/consoleFull


Cheers

On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net<mailto:tony.r=
eix@bull.net>> wrote:
Thanks !

I've been able to see that there are 3745 tests for version 1.2.0 with prof=
ile Hadoop 2.4  .
However, on my side, the maximum tests I've seen are 3485... About 300 test=
s are missing on my side.
Which Maven option has been used for producing the report file used for bui=
lding the page:
     https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-=
with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/te=
stReport/
  ? (I'm not authorized to look at the "configuration" part)

Thx !

Tony

________________________________
De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
Envoy=E9 : jeudi 8 janvier 2015 16:11
=C0 : Tony Reix
Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
Objet : Re: Results of tests

Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/

On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto:tony.r=
eix@bull.net>> wrote:
Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when ru=
nning on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can fin=
d the results of the tests of Spark, for each version and for the different=
 versions, in order to have a reference to compare my results with. I canno=
t find them on Spark web-site.
Thx
Tony




--_000_5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6BUMSG3WMfradbul_--

From dev-return-11069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 09:32:55 2015
Return-Path: <dev-return-11069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7922517F6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 09:32:55 +0000 (UTC)
Received: (qmail 15938 invoked by uid 500); 9 Jan 2015 09:32:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15850 invoked by uid 500); 9 Jan 2015 09:32:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 6240 invoked by uid 99); 9 Jan 2015 09:29:04 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=5DIm3IQaDlbV0wv9lhDyfioLNoX1NsXJZoAQCW80ddg=;
        b=TRsrDjLXWlaVB3bo5Z3RgH167VcGaVNRWk0Z8eyBJsDpYY/7kVhwMqf+ODy8FGQNcD
         WgmdL65yzZSc+CFwxXi0daZtbYgMC2dNRwZi4UJy5h6fxzdUxjrin6uTrAJgG1cKGotV
         8vMk3bn55OmzlRjv+QtULqa6+ierrpgLpjhC/dZ+2AASp7fSpIZ3kUD1eMs2gn0B66in
         ppgJ/rsDh6nlkus1r+yn5rVNC6AwG8j8t2TIMnm51xOjEa4OA+KoUw1kefbxpSjz7ofw
         /j2REH04IxKQ+8NBQVcBgK7vUQNLhfPGrrfM6oRqprmogzA/EXU7+I6Al5+BHFgWkGn3
         fHcQ==
X-Gm-Message-State: ALoCoQkKJoUPdsF64loRBMG4tmLNGFvOUOwXB4DjGTRfk5jyGHNKuipvGWMYo8u01TTBh558RUrn
X-Received: by 10.194.58.37 with SMTP id n5mr104237wjq.14.1420795562093; Fri,
 09 Jan 2015 01:26:02 -0800 (PST)
MIME-Version: 1.0
In-Reply-To: <105FCC9386BD420B9E4EDA0E8563CBC1@gmail.com>
References: <105FCC9386BD420B9E4EDA0E8563CBC1@gmail.com>
From: Tathagata Das <tdas@databricks.com>
Date: Fri, 9 Jan 2015 01:25:31 -0800
Message-ID: <CA+AHuKn+xX6a2_4iWPWq00q8av9=tyA3PmMFuVuBaqUgZt+QCg@mail.gmail.com>
Subject: Re: missing document of several messages in actor-based receiver?
To: Nan Zhu <zhunanmcgill@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7ba9801ec704a0050c34bedd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9801ec704a0050c34bedd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It was not really mean to be hidden. So its essentially the case of the
documentation being insufficient. This code has not gotten much attention
for a while, so it could have a bugs. If you find any and submit a fix for
them, I am happy to take a look!

TD

On Thu, Jan 8, 2015 at 6:33 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

>  Hi, TD and other streaming developers,
>
> When I look at the implementation of actor-based receiver
> (ActorReceiver.scala), I found that there are several messages which are
> not mentioned in the document
>
> case props: Props =3D>
>   val worker =3D context.actorOf(props)
>   logInfo("Started receiver worker at:" + worker.path)
>   sender ! worker
>
> case (props: Props, name: String) =3D>
>   val worker =3D context.actorOf(props, name)
>   logInfo("Started receiver worker at:" + worker.path)
>   sender ! worker
>
> case _: PossiblyHarmful =3D> hiccups.incrementAndGet()
>
> case _: Statistics =3D>
>   val workers =3D context.children
>   sender ! Statistics(n.get, workers.size, hiccups.get, workers.mkString(=
"\n*=E2=80=9D*))
>
>
> Is it hided with intention or incomplete document, or I missed something?
>
> And the handler of these messages are =E2=80=9Cbuggy"? e.g. when we start=
 a new worker, we didn=E2=80=99t increase n (counter of children), and n an=
d hiccups are unnecessarily set to AtomicInteger ?
>
> Best,
>
> --
> Nan Zhu
> http://codingcat.me
>

--047d7ba9801ec704a0050c34bedd--

From dev-return-11070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 10:44:34 2015
Return-Path: <dev-return-11070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24A8EC2E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 10:44:34 +0000 (UTC)
Received: (qmail 54488 invoked by uid 500); 9 Jan 2015 10:44:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54417 invoked by uid 500); 9 Jan 2015 10:44:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54403 invoked by uid 99); 9 Jan 2015 10:44:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 10:44:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of devl.development@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 10:44:30 +0000
Received: by mail-qg0-f45.google.com with SMTP id z107so7767332qgd.4
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 02:41:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ubaWFUCbpqAz++MvOHEZIyavnv2AUK6Gu2ytAR6cX2I=;
        b=AbduI++Tde9dL9Z2npCyottRmQGDZyj/61ukik06DXXSmV32V5KOOW479j68ZYyk1U
         EUlFZgxLOdSzIodOiqxQENfciGIxVpE2W0tDSPkWHpHICLqZAbToaGYVMnxExM083Xpk
         iWrSffYbV+enJtL3CcecTZisYNKipCbIND8kN6oZCNW+DjUYNPOq3kjTZk94miFFslTH
         FHnePLeh43wc0sMtoawDIneGKKFk2ACqSHCBuf4YKCjQZFQPUV90qNa6UunkxK+WH8uX
         iuZTfvoYAqufbqesQuFD8zSkROZefc/Z/+ruqSw1+zK2strMQtdOhdyj9ZcNooTlSmEJ
         oCBw==
MIME-Version: 1.0
X-Received: by 10.224.167.134 with SMTP id q6mr14504385qay.72.1420800114241;
 Fri, 09 Jan 2015 02:41:54 -0800 (PST)
Received: by 10.140.102.111 with HTTP; Fri, 9 Jan 2015 02:41:54 -0800 (PST)
In-Reply-To: <CAF7ADNqLu=w+Emc_6q0O0d_6QauSFmH22a2qNj41nPr6q+RDbw@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
	<CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
	<CAMQ+LQPnhw3eKRxDAQzk5iFYFn2FXxvw+ygb+D-EBRVmU+hXYA@mail.gmail.com>
	<CAF7ADNqLu=w+Emc_6q0O0d_6QauSFmH22a2qNj41nPr6q+RDbw@mail.gmail.com>
Date: Fri, 9 Jan 2015 10:41:54 +0000
Message-ID: <CAMQ+LQNWVaR4O9-HLGBShqQNq=BhS3raMpce9XGNXeDQyEM3kA@mail.gmail.com>
Subject: Re: K-Means And Class Tags
From: Devl Devel <devl.development@gmail.com>
To: Joseph Bradley <joseph@databricks.com>, Yana Kadiyska <yana.kadiyska@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c9be1b37e9050c35ce22
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c9be1b37e9050c35ce22
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Joseph

Thanks for the suggestion, however retag is a private method and when I
call in Scala:

val retaggedInput =3D parsedData.retag(classOf[Vector])

I get:

Symbol retag is inaccessible from this place

However I can do this from Java, and it works in Scala:

return words.rdd().retag(Vector.class);

Dev



On Thu, Jan 8, 2015 at 9:35 PM, Joseph Bradley <joseph@databricks.com>
wrote:

> I believe you're running into an erasure issue which we found in
> DecisionTree too.  Check out:
>
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/tree/RandomForest.scala#L134
>
> That retags RDDs which were created from Java to prevent the exception
> you're running into.
>
> Hope this helps!
> Joseph
>
> On Thu, Jan 8, 2015 at 12:48 PM, Devl Devel <devl.development@gmail.com>
> wrote:
>
>> Thanks for the suggestion, can anyone offer any advice on the ClassCast
>> Exception going from Java to Scala? Why does JavaRDD.rdd() and then a
>> collect() result in this exception?
>>
>> On Thu, Jan 8, 2015 at 4:13 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
>> wrote:
>>
>> > How about
>> >
>> >
>> data.map(s=3D>s.split(",")).filter(_.length>1).map(good_entry=3D>Vectors=
.dense((Double.parseDouble(good_entry[0]),
>> > Double.parseDouble(good_entry[1]))
>> > =E2=80=8B
>> > (full disclosure, I didn't actually run this). But after the first map
>> you
>> > should have an RDD[Array[String]], then you'd discard everything short=
er
>> > than 2, and convert the rest to dense vectors?...In fact if you're
>> > expecting length exactly 2 might want to filter =3D=3D2...
>> >
>> >
>> > On Thu, Jan 8, 2015 at 10:58 AM, Devl Devel <devl.development@gmail.co=
m
>> >
>> > wrote:
>> >
>> >> Hi All,
>> >>
>> >> I'm trying a simple K-Means example as per the website:
>> >>
>> >> val parsedData =3D data.map(s =3D>
>> >> Vectors.dense(s.split(',').map(_.toDouble)))
>> >>
>> >> but I'm trying to write a Java based validation method first so that
>> >> missing values are omitted or replaced with 0.
>> >>
>> >> public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
>> >>         JavaRDD<Vector> words =3D data.flatMap(new
>> FlatMapFunction<String,
>> >> Vector>() {
>> >>             public Iterable<Vector> call(String s) {
>> >>                 String[] split =3D s.split(",");
>> >>                 ArrayList<Vector> add =3D new ArrayList<Vector>();
>> >>                 if (split.length !=3D 2) {
>> >>                     add.add(Vectors.dense(0, 0));
>> >>                 } else
>> >>                 {
>> >>                     add.add(Vectors.dense(Double.parseDouble(split[0]=
),
>> >>                Double.parseDouble(split[1])));
>> >>                 }
>> >>
>> >>                 return add;
>> >>             }
>> >>         });
>> >>
>> >>         return words.rdd();
>> >> }
>> >>
>> >> When I then call from scala:
>> >>
>> >> val parsedData=3Ddc.prepareKMeans(data);
>> >> val p=3DparsedData.collect();
>> >>
>> >> I get Exception in thread "main" java.lang.ClassCastException:
>> >> [Ljava.lang.Object; cannot be cast to
>> >> [Lorg.apache.spark.mllib.linalg.Vector;
>> >>
>> >> Why is the class tag is object rather than vector?
>> >>
>> >> 1) How do I get this working correctly using the Java validation
>> example
>> >> above or
>> >> 2) How can I modify val parsedData =3D data.map(s =3D>
>> >> Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split siz=
e
>> <2
>> >> I
>> >> ignore the line? or
>> >> 3) Is there a better way to do input validation first?
>> >>
>> >> Using spark and mlib:
>> >> libraryDependencies +=3D "org.apache.spark" % "spark-core_2.10" %
>> "1.2.0"
>> >> libraryDependencies +=3D "org.apache.spark" % "spark-mllib_2.10" %
>> "1.2.0"
>> >>
>> >> Many thanks in advance
>> >> Dev
>> >>
>> >
>> >
>>
>
>

--089e0149c9be1b37e9050c35ce22--

From dev-return-11071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 11:55:48 2015
Return-Path: <dev-return-11071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CFCD2C4E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 11:55:48 +0000 (UTC)
Received: (qmail 1213 invoked by uid 500); 9 Jan 2015 11:55:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1139 invoked by uid 500); 9 Jan 2015 11:55:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1128 invoked by uid 99); 9 Jan 2015 11:55:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 11:55:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 11:55:43 +0000
Received: by mail-wg0-f49.google.com with SMTP id n12so7685326wgh.8
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 03:53:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=NJw1ggH7lTOTymCRwMxMck3YOHMo8GJyauzjA33xUWI=;
        b=GEFzXE8kMff21wxUwhCXK1DtMLwKs5yAyIlWCtmjmmtCcra5Lp7Kqiu79+0FAmQ/PK
         RKT3wols+jaSP6A42iGaZ8X3JAtyoFawVU5uhXVn2rnOGTrnr252ZSOfOwcIMtYW8kPi
         DWLZ1s0VblHaO73MeO4RoTLRYpk9o5jNHa+pKTQvUJv5sBXHY91O0XAfHd773+DelMGH
         8xteKgh7VY5jYFQ5Kq3ll/lHtcHcH1WDxbf6RebR69HUIzzmvuBcAJ0nz/mU4w5wdmcx
         MlMXTjJ5MBpllatE5b7QoMooQQZlqr9DeT8k+WwGiigZTlpwVPqaAogGet80Z/WxjOSu
         S+Eg==
X-Gm-Message-State: ALoCoQnhx+xu4TL7VHKBGa+uA5KjlB80/qK32mfRKPu8bsDp98V8kWtaz78E+YpvQz1VmJR+Wln9
X-Received: by 10.180.7.201 with SMTP id l9mr4239748wia.80.1420804387803; Fri,
 09 Jan 2015 03:53:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 9 Jan 2015 03:52:47 -0800 (PST)
In-Reply-To: <5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
 <CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
 <5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
 <CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com> <5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 9 Jan 2015 11:52:47 +0000
Message-ID: <CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
Subject: Re: Results of tests
To: Tony Reix <tony.reix@bull.net>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Tony, the number of tests run could vary depending on how the
build is configured. For example, YARN-related tests would only run
when the yarn profile is turned on. Java 8 tests would only run under
Java 8.

Although I don't know that there's any reason to believe the IBM JVM
has a problem with Spark, I see this issue that is potentially related
to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
don't know if that was a Spark issue. Certainly, would be good for you
to investigate if you are interested in resolving it.

The Jenkins output shows you exactly what tests were run and how --
have a look at the logs.

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-=
YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/console=
Full

On Fri, Jan 9, 2015 at 9:15 AM, Tony Reix <tony.reix@bull.net> wrote:
> Hi Ted
>
> Thanks for the info.
> However, I'm still unable to understand how the page:
>    https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-=
with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/te=
stReport/
> has been built.
> This page contains details I do not find in the page you indicated to me:
>    https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-=
with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/co=
nsoleFull
>
> As an example, I'm still unable to find these details:
> org.apache.spark<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Sp=
ark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,lab=
el=3Dcentos/testReport/org.apache.spark/>       12 mn   0
>         1
>         247
>         248
>
> org.apache.spark.api.python<https://amplab.cs.berkeley.edu/jenkins/view/S=
park/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhad=
oop-2.4,label=3Dcentos/testReport/org.apache.spark.api.python/> 20 ms   0
>         0
>         2
>         2
>
> org.apache.spark.bagel<https://amplab.cs.berkeley.edu/jenkins/view/Spark/=
job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2=
.4,label=3Dcentos/testReport/org.apache.spark.bagel/>   7.7 s   0
>         0
>         4
>         4
>
> org.apache.spark.broadcast<https://amplab.cs.berkeley.edu/jenkins/view/Sp=
ark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhado=
op-2.4,label=3Dcentos/testReport/org.apache.spark.broadcast/>   43 s    0
>         0
>         17
>         17
>
> org.apache.spark.deploy<https://amplab.cs.berkeley.edu/jenkins/view/Spark=
/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-=
2.4,label=3Dcentos/testReport/org.apache.spark.deploy/> 16 s    0
>         0
>         29
>         29
>
> org.apache.spark.deploy.worker<https://amplab.cs.berkeley.edu/jenkins/vie=
w/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3D=
hadoop-2.4,label=3Dcentos/testReport/org.apache.spark.deploy.worker/>   0.5=
5 s  0
>         0
>         12
>         12
>
> ........
>
>
> Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests and 0=
 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ). When=
 using IBM JVM, there are only 2566 tests and 5 failures (in same component=
: Streaming).
>
> On my PPC64BE (BE =3D Big-Endian)environment, the tests block after 2 hun=
dreds of tests.
> Is Spark independent of Little/Big-Endian stuff ?
>
> On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests only =
(like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
>
> So, I need to learn more about how your Jenkins environment extracts deta=
ils about the results.
> Moreover, which JVM is used ?
>
> Do you plan to use IBM JVM in order to check that Spark and IBM JVM are c=
ompatible ? (they already do not look to be compatible 100% ...).
>
> Thanks
>
> Tony
>
> IBM Coop Architect & Technical Leader
> Office : +33 (0) 4 76 29 72 67
> 1 rue de Provence - 38432 =C3=89chirolles - France
> www.atos.net<http://www.atos.net/>
> ________________________________
> De : Ted Yu [yuzhihong@gmail.com]
> Envoy=C3=A9 : jeudi 8 janvier 2015 17:43
> =C3=80 : Tony Reix
> Cc : dev@spark.apache.org
> Objet : Re: Results of tests
>
> Here it is:
>
> [centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven=
_3.0.5/bin/mvn -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -=
Phadoop-2.4 -Pyarn -Phive clean package
>
>
> You can find the above in https://amplab.cs.berkeley.edu/jenkins/view/Spa=
rk/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoo=
p-2.4,label=3Dcentos/consoleFull
>
>
> Cheers
>
> On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net<mailto:tony=
.reix@bull.net>> wrote:
> Thanks !
>
> I've been able to see that there are 3745 tests for version 1.2.0 with pr=
ofile Hadoop 2.4  .
> However, on my side, the maximum tests I've seen are 3485... About 300 te=
sts are missing on my side.
> Which Maven option has been used for producing the report file used for b=
uilding the page:
>      https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Mave=
n-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/=
testReport/
>   ? (I'm not authorized to look at the "configuration" part)
>
> Thx !
>
> Tony
>
> ________________________________
> De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
> Envoy=C3=A9 : jeudi 8 janvier 2015 16:11
> =C3=80 : Tony Reix
> Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
> Objet : Re: Results of tests
>
> Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/
>
> On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto:tony=
.reix@bull.net>> wrote:
> Hi,
> I'm checking that Spark works fine on a new environment (PPC64 hardware).
> I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when =
running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can f=
ind the results of the tests of Spark, for each version and for the differe=
nt versions, in order to have a reference to compare my results with. I can=
not find them on Spark web-site.
> Thx
> Tony
>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 12:38:50 2015
Return-Path: <dev-return-11072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFC3FC64D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 12:38:50 +0000 (UTC)
Received: (qmail 68294 invoked by uid 500); 9 Jan 2015 12:38:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68208 invoked by uid 500); 9 Jan 2015 12:38:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68196 invoked by uid 99); 9 Jan 2015 12:38:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 12:38:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 12:38:44 +0000
Received: by mail-qc0-f175.google.com with SMTP id p6so8327505qcv.6
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 04:38:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=gQD1tlpvyKDAvNLxf9vApfjZnYMFAMio3NPao9T7vVo=;
        b=rniNb+xUDsxt0rqdi1T8y0m2CxpyVf1+EVy73c77qiqzaeoyyQA9iOcrd3DZxJ699o
         ON/4XvDxGhKpxIRK6ZfTA3SmCEC8AAtNsuulfKTOK/c0ZNA5VpJp2SO72n4KbDGE9kec
         hCLKXBocu/oyZhy+/a7rY9bSPX247D2WZYoWipizL+aT1UJPKdNZamchXPecT0KAI3l6
         vFnDU/1fjZaIN8hhfThPx/EkmbSe+TJ1PRReaMG8qHzxslmoE18hZuqeouMZrU/QFKHo
         xslHQdr7xVKWK/kX1WkzpmoM2+WH6YBlNWKrnQIeFFbW3a6eWHdSIGiTm2ZDoF9G/PBo
         l0Ug==
X-Received: by 10.224.114.81 with SMTP id d17mr15603722qaq.27.1420807103891;
        Fri, 09 Jan 2015 04:38:23 -0800 (PST)
Received: from [192.168.2.16] (bas3-montreal42-1168076984.dsl.bell.ca. [69.159.112.184])
        by mx.google.com with ESMTPSA id i91sm6815181qgd.25.2015.01.09.04.38.23
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Fri, 09 Jan 2015 04:38:23 -0800 (PST)
Date: Fri, 9 Jan 2015 07:38:22 -0500
From: Nan Zhu <zhunanmcgill@gmail.com>
To: Tathagata Das <tdas@databricks.com>
Cc: dev@spark.apache.org
Message-ID: <8BD22CD8D2F148BDAF988F8EFF31627E@gmail.com>
In-Reply-To: <CA+AHuKn+xX6a2_4iWPWq00q8av9=tyA3PmMFuVuBaqUgZt+QCg@mail.gmail.com>
References: <105FCC9386BD420B9E4EDA0E8563CBC1@gmail.com>
 <CA+AHuKn+xX6a2_4iWPWq00q8av9=tyA3PmMFuVuBaqUgZt+QCg@mail.gmail.com>
Subject: Re: missing document of several messages in actor-based
 receiver?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54afcbbe_579478fe_19a"
X-Virus-Checked: Checked by ClamAV on apache.org

--54afcbbe_579478fe_19a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Thanks, TD, =20

I just created 2 JIRAs to track these, =20

https://issues.apache.org/jira/browse/SPARK-5174

https://issues.apache.org/jira/browse/SPARK-5175

Can you help to me assign these two JIRAs to me, and I=E2=80=99d like to =
submit the PRs

Best, =20

-- =20
Nan Zhu
http://codingcat.me


On =46riday, January 9, 2015 at 4:25 AM, Tathagata Das wrote:

> It was not really mean to be hidden. So its essentially the case of the=
 documentation being insufficient. This code has not gotten much attentio=
n for a while, so it could have a bugs. If you find any and submit a fix =
for them, I am happy to take a look=21
> =20
> TD
> =20
> On Thu, Jan 8, 2015 at 6:33 PM, Nan Zhu <zhunanmcgill=40gmail.com (mail=
to:zhunanmcgill=40gmail.com)> wrote:
> > Hi, TD and other streaming developers,
> > =20
> > When I look at the implementation of actor-based receiver (ActorRecei=
ver.scala), I found that there are several messages which are not mention=
ed in the document =20
> > =20
> > case props: Props =3D>
> > val worker =3D context.actorOf(props)
> > logInfo(=22Started receiver worker at:=22 + worker.path)
> > sender =21 worker
> > =20
> > case (props: Props, name: String) =3D>
> > val worker =3D context.actorOf(props, name)
> > logInfo(=22Started receiver worker at:=22 + worker.path)
> > sender =21 worker
> > =20
> > case =5F: PossiblyHarmful =3D> hiccups.incrementAndGet()
> > =20
> > case =5F: Statistics =3D>
> > val workers =3D context.children
> > sender =21 Statistics(n.get, workers.size, hiccups.get, workers.mkStr=
ing(=22=5Cn=E2=80=9D))
> > =20
> > Is it hided with intention or incomplete document, or I missed someth=
ing=3F
> > And the handler of these messages are =E2=80=9Cbuggy=22=3F e.g. when =
we start a new worker, we didn=E2=80=99t increase n (counter of children)=
, and n and hiccups are unnecessarily set to AtomicInteger =3F
> > =20
> > Best,
> > =20
> > -- =20
> > Nan Zhu
> > http://codingcat.me
> > =20
> > =20
> =20
> =20
> =20


--54afcbbe_579478fe_19a--


From dev-return-11073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 12:46:29 2015
Return-Path: <dev-return-11073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBE02C66F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 12:46:28 +0000 (UTC)
Received: (qmail 78993 invoked by uid 500); 9 Jan 2015 12:46:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78906 invoked by uid 500); 9 Jan 2015 12:46:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78895 invoked by uid 99); 9 Jan 2015 12:46:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 12:46:28 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 12:46:24 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Fri, 9 Jan 2015
 07:45:41 -0500
Message-ID: <54AFCD68.9010908@flytxt.com>
Date: Fri, 9 Jan 2015 18:15:28 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Python to Java object conversion of numpy array
Content-Type: multipart/alternative;
	boundary="------------010900010708090707010603"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------010900010708090707010603
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi,
I am trying to send a numpy array as an argument to a function predict() 
in a class in spark/python/pyspark/mllib/clustering.py which is passed 
to the function callMLlibFunc(name, *args)  in 
spark/python/pyspark/mllib/common.py.

Now the value is passed to the function  _py2java(sc, obj) .Here I am 
getting an exception

Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.api.python.SerDe.loads.
: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
	at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)


Why common._py2java(sc, obj) is not handling numpy array type?

Please help..


-- 

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_


--------------010900010708090707010603--

From dev-return-11074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 16:37:20 2015
Return-Path: <dev-return-11074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C91FE100A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 16:37:20 +0000 (UTC)
Received: (qmail 59535 invoked by uid 500); 9 Jan 2015 16:37:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59457 invoked by uid 500); 9 Jan 2015 16:37:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59444 invoked by uid 99); 9 Jan 2015 16:37:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 16:37:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.175] (HELO mail-lb0-f175.google.com) (209.85.217.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 16:37:14 +0000
Received: by mail-lb0-f175.google.com with SMTP id z11so8966542lbi.6
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 08:35:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZhnkTLlteGuEOPQBKHp1RE/9Y7htsLd062oONfpwBuE=;
        b=QqFwY78zXqBWJcO8dFUdmT6B1oZ76KcRaRazUNzia470eKZJX2SGnjn3xDy/gNS+xL
         nvQa8E1ld6SEdQ0aS8AWzCBXSoeVZcfw7Qpj40Y/jX7odSOsJTVBYsbzRpwmThGt75Jg
         oMPitV+SWYbOV1B4hMArx/6hOpUiyVMfnQgaj+efHCBAf6WlctkwLZApnh9mC+eL6dlz
         qQyEif0Ta7hdQGPiPV5hMCZDISrK1KJM5j8lGM1ye9y/x918elQ5P1TKqYW6q2TkbzLZ
         4ijSD/KFvpr0JM6o59Tn/wmVMveWgUEx8lQYOKqOsdI313HpDVvJdjqAZu1AO7Kuqfkz
         CkLg==
X-Gm-Message-State: ALoCoQkymMIbFJ7uR0TQUZGiTZFLmZJO2zYUcT02hZPGjoi0AZ7pLV2KCrn6Ejc9IYSj0ub3cq7y
X-Received: by 10.152.36.1 with SMTP id m1mr22420284laj.95.1420821302825; Fri,
 09 Jan 2015 08:35:02 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Fri, 9 Jan 2015 08:34:42 -0800 (PST)
In-Reply-To: <CAF7WS+qWZEgnsjBJBnJhGgbmRCnmRYDArY_-QVZe8pEEJNQnVg@mail.gmail.com>
References: <CAF7WS+qWZEgnsjBJBnJhGgbmRCnmRYDArY_-QVZe8pEEJNQnVg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 9 Jan 2015 08:34:42 -0800
Message-ID: <CAAswR-7Cq11mrRkFfArZcRqNwfYbu7P1LxCQJeMoTsc+G2FxrQ@mail.gmail.com>
Subject: Re: PR #3872
To: Bill Bejeck <bbejeck@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b7cc0b6976050c3abd97
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b7cc0b6976050c3abd97
Content-Type: text/plain; charset=UTF-8

I will look at it this weekend.

On Thu, Jan 8, 2015 at 2:43 PM, Bill Bejeck <bbejeck@gmail.com> wrote:

> Could one of the admins take a look at PR 3872 (JIRA 3299) submitted on 1/1
>

--089e0158b7cc0b6976050c3abd97--

From dev-return-11075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 18:08:34 2015
Return-Path: <dev-return-11075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D56C410717
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 18:08:34 +0000 (UTC)
Received: (qmail 36865 invoked by uid 500); 9 Jan 2015 18:08:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36787 invoked by uid 500); 9 Jan 2015 18:08:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36759 invoked by uid 99); 9 Jan 2015 18:08:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 18:08:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 18:08:09 +0000
Received: by mail-lb0-f174.google.com with SMTP id 10so9980719lbg.5
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 10:07:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=TO3xY3MzjQqE8SJqhvIqpHZAo8nENmixRsjLhe9llRo=;
        b=Qvz8gbevDvja5HzFod7ZrQ7mAXbkdfnUNIFFbyA33q8hwthXUaN8C/xNsXfLgmGKQE
         RoRirQHMBnZu0tmd9P+jLk3atGN3PSHqbkJUWIKKWyK76kITmpoE6MYK24ZG4HVQWfZP
         qgXVgB7qClA97LarD5PDT56lQ/2UQxzxTlVoPFdFRdlU2aDHyOmk9fNBI+qQBO0+JlDm
         hDv72Gk6/stwlZAHXEKOra8IjmvXe+tjUtKvH+mnjZk+rubSnHNITfxkI0AS8dCaz9Hs
         Gs9inxn3PMytcwoF6Dvaeyjb+hVm9Rz5kRfW2jUhFmimmBM+nKG4vH5tidHHSTC4w4Ek
         EfHQ==
X-Gm-Message-State: ALoCoQkJ3CjlKdw9NDK3rMU/f+UyJ1DNw0Ewbc8lz6lTBVCOkNwFoxyevmqphHjQJTFlKdpLdZMx
MIME-Version: 1.0
X-Received: by 10.152.3.195 with SMTP id e3mr23122441lae.8.1420826866980; Fri,
 09 Jan 2015 10:07:46 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Fri, 9 Jan 2015 10:07:46 -0800 (PST)
In-Reply-To: <54AFCD68.9010908@flytxt.com>
References: <54AFCD68.9010908@flytxt.com>
Date: Fri, 9 Jan 2015 10:07:46 -0800
Message-ID: <CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
Subject: Re: Python to Java object conversion of numpy array
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Meethu,

The Java API accepts only Vector, so you should convert the numpy array into
pyspark.mllib.linalg.DenseVector.

BTW, which class are you using? the KMeansModel.predict() accept numpy.array,
it will do the conversion for you.

Davies

On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
> Hi,
> I am trying to send a numpy array as an argument to a function predict() in
> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
> function callMLlibFunc(name, *args)  in
> spark/python/pyspark/mllib/common.py.
>
> Now the value is passed to the function  _py2java(sc, obj) .Here I am
> getting an exception
>
> Py4JJavaError: An error occurred while calling
> z:org.apache.spark.mllib.api.python.SerDe.loads.
> : net.razorvine.pickle.PickleException: expected zero arguments for
> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>         at
> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>         at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>         at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>         at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>         at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>
>
> Why common._py2java(sc, obj) is not handling numpy array type?
>
> Please help..
>
>
> --
>
> Regards,
>
> *Meethu Mathew*
>
> *Engineer*
>
> *Flytxt*
>
> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
> <http://www.twitter.com/flytxt> | _Connect on Linkedin
> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 18:49:00 2015
Return-Path: <dev-return-11076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62462109A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 18:49:00 +0000 (UTC)
Received: (qmail 82960 invoked by uid 500); 9 Jan 2015 18:49:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82887 invoked by uid 500); 9 Jan 2015 18:49:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82694 invoked by uid 99); 9 Jan 2015 18:48:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 18:48:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 18:48:32 +0000
Received: by mail-yh0-f41.google.com with SMTP id a41so4886272yho.0
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 10:47:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=veaDJUQGFPF9LN0pjYt0Wx/G/7A7mCSs17GdxIZzAWU=;
        b=xUJ1whRQWzvv988vexnyRW4/ZdCvhC5kiQf3mQ+Xg21Y2h7WUBTNBIx8EXfK6pp+iM
         aC/psExBatRMFYiIo0/q8FDA4RxBGP+JPTucVyV2UcswL52gi3qFdG2/vIFvcuUH5dl/
         wyDcQwo3H81oj2buonXpEbfe59oD3X/kSKkiHM+RMNwx/NSdfJZGomfO+MOCs3a/hPVY
         LDwOfW9DFrfp1BLKxoRNxsuSqkqtOySB2Ma987K37T07+JAUWCKnKm/RYVwhCBcroegV
         sez3fdkoixbZNo1S/DhUvKhx5d6atNZ2LKS1hTK1UMED3R02GbWkf//x5K4u4mDIOW5b
         39yw==
MIME-Version: 1.0
X-Received: by 10.236.209.5 with SMTP id r5mr8187400yho.97.1420829265798; Fri,
 09 Jan 2015 10:47:45 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Fri, 9 Jan 2015 10:47:45 -0800 (PST)
In-Reply-To: <CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
	<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
	<CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
	<CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
Date: Fri, 9 Jan 2015 10:47:45 -0800
Message-ID: <CALte62wcnByQMp9zd0ju-=tkKW0=n3auiY1-r5gVhksgO_Nm4Q@mail.gmail.com>
Subject: Re: Results of tests
From: Ted Yu <yuzhihong@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Tony Reix <tony.reix@bull.net>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2028aacaf94050c3c97a3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2028aacaf94050c3c97a3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

For a build which uses JUnit, we would see a summary such as the following =
(
https://builds.apache.org/job/HBase-TRUNK/6007/console):

Tests run: 2199, Failures: 0, Errors: 0, Skipped: 25


In https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull
, I don't see such statistics.


Looks like scalatest-maven-plugin can be enhanced :-)


On Fri, Jan 9, 2015 at 3:52 AM, Sean Owen <sowen@cloudera.com> wrote:

> Hey Tony, the number of tests run could vary depending on how the
> build is configured. For example, YARN-related tests would only run
> when the yarn profile is turned on. Java 8 tests would only run under
> Java 8.
>
> Although I don't know that there's any reason to believe the IBM JVM
> has a problem with Spark, I see this issue that is potentially related
> to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
> don't know if that was a Spark issue. Certainly, would be good for you
> to investigate if you are interested in resolving it.
>
> The Jenkins output shows you exactly what tests were run and how --
> have a look at the logs.
>
>
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
>
> On Fri, Jan 9, 2015 at 9:15 AM, Tony Reix <tony.reix@bull.net> wrote:
> > Hi Ted
> >
> > Thanks for the info.
> > However, I'm still unable to understand how the page:
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
> > has been built.
> > This page contains details I do not find in the page you indicated to m=
e:
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> >
> > As an example, I'm still unable to find these details:
> > org.apache.spark<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark/>
>      12 mn   0
> >         1
> >         247
> >         248
> >
> > org.apache.spark.api.python<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.api.python/>
> 20 ms   0
> >         0
> >         2
> >         2
> >
> > org.apache.spark.bagel<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.bagel/>
>  7.7 s   0
> >         0
> >         4
> >         4
> >
> > org.apache.spark.broadcast<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.broadcast/>
>  43 s    0
> >         0
> >         17
> >         17
> >
> > org.apache.spark.deploy<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.deploy/>
> 16 s    0
> >         0
> >         29
> >         29
> >
> > org.apache.spark.deploy.worker<
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.deploy.worker/>
>  0.55 s  0
> >         0
> >         12
> >         12
> >
> > ........
> >
> >
> > Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests and
> 0 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ).
> When using IBM JVM, there are only 2566 tests and 5 failures (in same
> component: Streaming).
> >
> > On my PPC64BE (BE =3D Big-Endian)environment, the tests block after 2
> hundreds of tests.
> > Is Spark independent of Little/Big-Endian stuff ?
> >
> > On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests onl=
y
> (like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
> >
> > So, I need to learn more about how your Jenkins environment extracts
> details about the results.
> > Moreover, which JVM is used ?
> >
> > Do you plan to use IBM JVM in order to check that Spark and IBM JVM are
> compatible ? (they already do not look to be compatible 100% ...).
> >
> > Thanks
> >
> > Tony
> >
> > IBM Coop Architect & Technical Leader
> > Office : +33 (0) 4 76 29 72 67
> > 1 rue de Provence - 38432 =C3=89chirolles - France
> > www.atos.net<http://www.atos.net/>
> > ________________________________
> > De : Ted Yu [yuzhihong@gmail.com]
> > Envoy=C3=A9 : jeudi 8 janvier 2015 17:43
> > =C3=80 : Tony Reix
> > Cc : dev@spark.apache.org
> > Objet : Re: Results of tests
> >
> > Here it is:
> >
> > [centos] $
> /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/=
mvn
> -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Phadoop-2.4 -=
Pyarn
> -Phive clean package
> >
> >
> > You can find the above in
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> >
> >
> > Cheers
> >
> > On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net<mailto:
> tony.reix@bull.net>> wrote:
> > Thanks !
> >
> > I've been able to see that there are 3745 tests for version 1.2.0 with
> profile Hadoop 2.4  .
> > However, on my side, the maximum tests I've seen are 3485... About 300
> tests are missing on my side.
> > Which Maven option has been used for producing the report file used for
> building the page:
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
> >   ? (I'm not authorized to look at the "configuration" part)
> >
> > Thx !
> >
> > Tony
> >
> > ________________________________
> > De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
> > Envoy=C3=A9 : jeudi 8 janvier 2015 16:11
> > =C3=80 : Tony Reix
> > Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
> > Objet : Re: Results of tests
> >
> > Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark=
/
> >
> > On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto:
> tony.reix@bull.net>> wrote:
> > Hi,
> > I'm checking that Spark works fine on a new environment (PPC64 hardware=
).
> > I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even whe=
n
> running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can
> find the results of the tests of Spark, for each version and for the
> different versions, in order to have a reference to compare my results
> with. I cannot find them on Spark web-site.
> > Thx
> > Tony
> >
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c2028aacaf94050c3c97a3--

From dev-return-11077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 19:48:47 2015
Return-Path: <dev-return-11077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 681B610C1D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 19:48:47 +0000 (UTC)
Received: (qmail 50391 invoked by uid 500); 9 Jan 2015 19:48:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50315 invoked by uid 500); 9 Jan 2015 19:48:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50298 invoked by uid 99); 9 Jan 2015 19:48:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 19:48:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.215.42 as permitted sender)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 19:48:21 +0000
Received: by mail-la0-f42.google.com with SMTP id gd6so16294076lab.1
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 11:46:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=D9rpvxgaOtvOeud8loFf7ftREbJjNGTcNNnrLF3F5g8=;
        b=vTKppUnVrR0cQSG+KrPX92g4r/CPTrEoFv9ZqK3PHZp62PeenBGb9DIBKTJP+5yYWd
         mdGv3+0M6Jhc4pMhUFO9+hj95FF+M9yxIFti+L3uRQH2VxhZWjv4p1HYM7QUa4ZBjqUQ
         YycVmnk7zxs11Sm6nqcRd3mPGafHMvTfWYKVglpWMGXlZxIdx0ig0PuoZPdEmu8v7V5X
         qGBZsusVLewGt6gERQN0U4e+5xEQRMNBn2prxXqNe2OYcYi5axW9Zdmokqw5cd+ZZhrr
         KI0Ko68Fy+YGizHhZPUMZZFX1p13iknCvjlm3MNgsff+itJdOczP4zWOreR7OEn5a6WL
         YLUQ==
MIME-Version: 1.0
X-Received: by 10.112.162.226 with SMTP id yd2mr23418102lbb.1.1420832764917;
 Fri, 09 Jan 2015 11:46:04 -0800 (PST)
Received: by 10.112.126.3 with HTTP; Fri, 9 Jan 2015 11:46:04 -0800 (PST)
In-Reply-To: <CALte62wcnByQMp9zd0ju-=tkKW0=n3auiY1-r5gVhksgO_Nm4Q@mail.gmail.com>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
	<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
	<CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
	<CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
	<CALte62wcnByQMp9zd0ju-=tkKW0=n3auiY1-r5gVhksgO_Nm4Q@mail.gmail.com>
Date: Fri, 9 Jan 2015 11:46:04 -0800
Message-ID: <CAOEPXP5+8ttBKLhYb1Fixt8zK-sJy5rhxP77yXxQuTPCj7dZDA@mail.gmail.com>
Subject: Re: Results of tests
From: Josh Rosen <rosenville@gmail.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Tony Reix <tony.reix@bull.net>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0112c86c3cfe53050c3d6819
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112c86c3cfe53050c3d6819
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The "Test Result" pages for Jenkins builds shows some nice statistics for
the test run, including individual test times:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-=
YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testRep=
ort/

Currently this only covers the Java / Scala tests, but we might be able to
integrate the PySpark tests here, too (I think it's just a matter of
getting the Python test runner to generate the correct test result XML
output).

On Fri, Jan 9, 2015 at 10:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:

> For a build which uses JUnit, we would see a summary such as the followin=
g
> (
> https://builds.apache.org/job/HBase-TRUNK/6007/console):
>
> Tests run: 2199, Failures: 0, Errors: 0, Skipped: 25
>
>
> In
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> , I don't see such statistics.
>
>
> Looks like scalatest-maven-plugin can be enhanced :-)
>
>
> On Fri, Jan 9, 2015 at 3:52 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Hey Tony, the number of tests run could vary depending on how the
> > build is configured. For example, YARN-related tests would only run
> > when the yarn profile is turned on. Java 8 tests would only run under
> > Java 8.
> >
> > Although I don't know that there's any reason to believe the IBM JVM
> > has a problem with Spark, I see this issue that is potentially related
> > to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
> > don't know if that was a Spark issue. Certainly, would be good for you
> > to investigate if you are interested in resolving it.
> >
> > The Jenkins output shows you exactly what tests were run and how --
> > have a look at the logs.
> >
> >
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> >
> > On Fri, Jan 9, 2015 at 9:15 AM, Tony Reix <tony.reix@bull.net> wrote:
> > > Hi Ted
> > >
> > > Thanks for the info.
> > > However, I'm still unable to understand how the page:
> > >
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
> > > has been built.
> > > This page contains details I do not find in the page you indicated to
> me:
> > >
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> > >
> > > As an example, I'm still unable to find these details:
> > > org.apache.spark<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark/
> >
> >      12 mn   0
> > >         1
> > >         247
> > >         248
> > >
> > > org.apache.spark.api.python<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.api.python/
> >
> > 20 ms   0
> > >         0
> > >         2
> > >         2
> > >
> > > org.apache.spark.bagel<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.bagel/
> >
> >  7.7 s   0
> > >         0
> > >         4
> > >         4
> > >
> > > org.apache.spark.broadcast<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.broadcast/
> >
> >  43 s    0
> > >         0
> > >         17
> > >         17
> > >
> > > org.apache.spark.deploy<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.deploy/
> >
> > 16 s    0
> > >         0
> > >         29
> > >         29
> > >
> > > org.apache.spark.deploy.worker<
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/org.apache.spark.deploy.worker/
> >
> >  0.55 s  0
> > >         0
> > >         12
> > >         12
> > >
> > > ........
> > >
> > >
> > > Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests a=
nd
> > 0 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ).
> > When using IBM JVM, there are only 2566 tests and 5 failures (in same
> > component: Streaming).
> > >
> > > On my PPC64BE (BE =3D Big-Endian)environment, the tests block after 2
> > hundreds of tests.
> > > Is Spark independent of Little/Big-Endian stuff ?
> > >
> > > On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests o=
nly
> > (like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
> > >
> > > So, I need to learn more about how your Jenkins environment extracts
> > details about the results.
> > > Moreover, which JVM is used ?
> > >
> > > Do you plan to use IBM JVM in order to check that Spark and IBM JVM a=
re
> > compatible ? (they already do not look to be compatible 100% ...).
> > >
> > > Thanks
> > >
> > > Tony
> > >
> > > IBM Coop Architect & Technical Leader
> > > Office : +33 (0) 4 76 29 72 67
> > > 1 rue de Provence - 38432 =C3=89chirolles - France
> > > www.atos.net<http://www.atos.net/>
> > > ________________________________
> > > De : Ted Yu [yuzhihong@gmail.com]
> > > Envoy=C3=A9 : jeudi 8 janvier 2015 17:43
> > > =C3=80 : Tony Reix
> > > Cc : dev@spark.apache.org
> > > Objet : Re: Results of tests
> > >
> > > Here it is:
> > >
> > > [centos] $
> >
> /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/=
mvn
> > -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Phadoop-2.4
> -Pyarn
> > -Phive clean package
> > >
> > >
> > > You can find the above in
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/conso=
leFull
> > >
> > >
> > > Cheers
> > >
> > > On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net<mailto:
> > tony.reix@bull.net>> wrote:
> > > Thanks !
> > >
> > > I've been able to see that there are 3745 tests for version 1.2.0 wit=
h
> > profile Hadoop 2.4  .
> > > However, on my side, the maximum tests I've seen are 3485... About 30=
0
> > tests are missing on my side.
> > > Which Maven option has been used for producing the report file used f=
or
> > building the page:
> > >
> >
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
> > >   ? (I'm not authorized to look at the "configuration" part)
> > >
> > > Thx !
> > >
> > > Tony
> > >
> > > ________________________________
> > > De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
> > > Envoy=C3=A9 : jeudi 8 janvier 2015 16:11
> > > =C3=80 : Tony Reix
> > > Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
> > > Objet : Re: Results of tests
> > >
> > > Please take a look at
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/
> > >
> > > On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto:
> > tony.reix@bull.net>> wrote:
> > > Hi,
> > > I'm checking that Spark works fine on a new environment (PPC64
> hardware).
> > > I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even
> when
> > running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I c=
an
> > find the results of the tests of Spark, for each version and for the
> > different versions, in order to have a reference to compare my results
> > with. I cannot find them on Spark web-site.
> > > Thx
> > > Tony
> > >
> > >
> > >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--089e0112c86c3cfe53050c3d6819--

From dev-return-11078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 20:16:46 2015
Return-Path: <dev-return-11078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B85FA10D6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 20:16:46 +0000 (UTC)
Received: (qmail 20645 invoked by uid 500); 9 Jan 2015 20:16:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20569 invoked by uid 500); 9 Jan 2015 20:16:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20557 invoked by uid 99); 9 Jan 2015 20:16:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:16:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:16:20 +0000
Received: by mail-ig0-f180.google.com with SMTP id h15so3541965igd.1
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 12:15:33 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=uWlONh7emGEetRTHujwN0Yd2spvCJUgRpm1HHVoIGdI=;
        b=gzHw1FKWiecYyQFhOKNakf2saSNDeT9NvXfuvZVqwk+SqBuhOUO33VuRQERkqe8p/z
         HJpZpKtVxcp5zKxKl7S/Nm36XgG3sxvfDP1dqewspKiOQMpmX/57S8NeSMqmKn0PY1xu
         uE/8ELy1/F+rTVjfVAdJeKwTsEm/nMxdBF40Tk9m3Vt9P471Fmhz0Bor4GCL2kNeEjYV
         yjwiSGt2R6Ql3urMUuPTmtf2KnTiZ3JyNFEtB4ma4EmUbGpAfroUxT5p5RUQPeWxqpdN
         wZSk3w/MyqCMQFT2QDqqGK4RIQbv+cqbZKJusFAxuh7TI1xJw3nPb7A/yBmpkOCj/IhG
         8+YQ==
X-Received: by 10.107.160.143 with SMTP id j137mr16979333ioe.43.1420834533393;
 Fri, 09 Jan 2015 12:15:33 -0800 (PST)
MIME-Version: 1.0
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
 <CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
 <5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
 <CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
 <5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
 <CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
 <CALte62wcnByQMp9zd0ju-=tkKW0=n3auiY1-r5gVhksgO_Nm4Q@mail.gmail.com> <CAOEPXP5+8ttBKLhYb1Fixt8zK-sJy5rhxP77yXxQuTPCj7dZDA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 09 Jan 2015 20:15:32 +0000
Message-ID: <CAOhmDzcfhA2dQJFUaZho=Bzi0GzUDS=1tO_tj-D-VHM2ZbYDww@mail.gmail.com>
Subject: Re: Results of tests
To: Josh Rosen <rosenville@gmail.com>, Ted Yu <yuzhihong@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Tony Reix <tony.reix@bull.net>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114043d6a5d909050c3dd160
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114043d6a5d909050c3dd160
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Just created: "Integrate Python unit tests into Jenkins"

https://issues.apache.org/jira/browse/SPARK-5178

Nick


On Fri Jan 09 2015 at 2:48:48 PM Josh Rosen <rosenville@gmail.com> wrote:

> The "Test Result" pages for Jenkins builds shows some nice statistics for
> the test run, including individual test times:
>
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/
>
> Currently this only covers the Java / Scala tests, but we might be able t=
o
> integrate the PySpark tests here, too (I think it's just a matter of
> getting the Python test runner to generate the correct test result XML
> output).
>
> On Fri, Jan 9, 2015 at 10:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > For a build which uses JUnit, we would see a summary such as the
> following
> > (
> > https://builds.apache.org/job/HBase-TRUNK/6007/console):
> >
> > Tests run: 2199, Failures: 0, Errors: 0, Skipped: 25
> >
> >
> > In
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/consoleFull
> > , I don't see such statistics.
> >
> >
> > Looks like scalatest-maven-plugin can be enhanced :-)
> >
> >
> > On Fri, Jan 9, 2015 at 3:52 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> > > Hey Tony, the number of tests run could vary depending on how the
> > > build is configured. For example, YARN-related tests would only run
> > > when the yarn profile is turned on. Java 8 tests would only run under
> > > Java 8.
> > >
> > > Although I don't know that there's any reason to believe the IBM JVM
> > > has a problem with Spark, I see this issue that is potentially relate=
d
> > > to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
> > > don't know if that was a Spark issue. Certainly, would be good for yo=
u
> > > to investigate if you are interested in resolving it.
> > >
> > > The Jenkins output shows you exactly what tests were run and how --
> > > have a look at the logs.
> > >
> > >
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/consoleFull
> > >
> > > On Fri, Jan 9, 2015 at 9:15 AM, Tony Reix <tony.reix@bull.net> wrote:
> > > > Hi Ted
> > > >
> > > > Thanks for the info.
> > > > However, I'm still unable to understand how the page:
> > > >
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/
> > > > has been built.
> > > > This page contains details I do not find in the page you indicated =
to
> > me:
> > > >
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/consoleFull
> > > >
> > > > As an example, I'm still unable to find these details:
> > > > org.apache.spark<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark/
> > >
> > >      12 mn   0
> > > >         1
> > > >         247
> > > >         248
> > > >
> > > > org.apache.spark.api.python<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark.api.python/
> > >
> > > 20 ms   0
> > > >         0
> > > >         2
> > > >         2
> > > >
> > > > org.apache.spark.bagel<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark.bagel/
> > >
> > >  7.7 s   0
> > > >         0
> > > >         4
> > > >         4
> > > >
> > > > org.apache.spark.broadcast<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark.broadcast/
> > >
> > >  43 s    0
> > > >         0
> > > >         17
> > > >         17
> > > >
> > > > org.apache.spark.deploy<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark.deploy/
> > >
> > > 16 s    0
> > > >         0
> > > >         29
> > > >         29
> > > >
> > > > org.apache.spark.deploy.worker<
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/org.apache.spark.deploy.worker/
> > >
> > >  0.55 s  0
> > > >         0
> > > >         12
> > > >         12
> > > >
> > > > ........
> > > >
> > > >
> > > > Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests
> and
> > > 0 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 =
).
> > > When using IBM JVM, there are only 2566 tests and 5 failures (in same
> > > component: Streaming).
> > > >
> > > > On my PPC64BE (BE =3D Big-Endian)environment, the tests block after=
 2
> > > hundreds of tests.
> > > > Is Spark independent of Little/Big-Endian stuff ?
> > > >
> > > > On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests
> only
> > > (like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
> > > >
> > > > So, I need to learn more about how your Jenkins environment extract=
s
> > > details about the results.
> > > > Moreover, which JVM is used ?
> > > >
> > > > Do you plan to use IBM JVM in order to check that Spark and IBM JVM
> are
> > > compatible ? (they already do not look to be compatible 100% ...).
> > > >
> > > > Thanks
> > > >
> > > > Tony
> > > >
> > > > IBM Coop Architect & Technical Leader
> > > > Office : +33 (0) 4 76 29 72 67
> > > > 1 rue de Provence - 38432 =C3=89chirolles - France
> > > > www.atos.net<http://www.atos.net/>
> > > > ________________________________
> > > > De : Ted Yu [yuzhihong@gmail.com]
> > > > Envoy=C3=A9 : jeudi 8 janvier 2015 17:43
> > > > =C3=80 : Tony Reix
> > > > Cc : dev@spark.apache.org
> > > > Objet : Re: Results of tests
> > > >
> > > > Here it is:
> > > >
> > > > [centos] $
> > >
> > /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Mav
> en_3.0.5/bin/mvn
> > > -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Phadoop-2=
.4
> > -Pyarn
> > > -Phive clean package
> > > >
> > > >
> > > > You can find the above in
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/consoleFull
> > > >
> > > >
> > > > Cheers
> > > >
> > > > On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net
> <mailto:
> > > tony.reix@bull.net>> wrote:
> > > > Thanks !
> > > >
> > > > I've been able to see that there are 3745 tests for version 1.2.0
> with
> > > profile Hadoop 2.4  .
> > > > However, on my side, the maximum tests I've seen are 3485... About
> 300
> > > tests are missing on my side.
> > > > Which Maven option has been used for producing the report file used
> for
> > > building the page:
> > > >
> > >
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-
> 1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=
=3D
> centos/testReport/
> > > >   ? (I'm not authorized to look at the "configuration" part)
> > > >
> > > > Thx !
> > > >
> > > > Tony
> > > >
> > > > ________________________________
> > > > De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
> > > > Envoy=C3=A9 : jeudi 8 janvier 2015 16:11
> > > > =C3=80 : Tony Reix
> > > > Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
> > > > Objet : Re: Results of tests
> > > >
> > > > Please take a look at
> > https://amplab.cs.berkeley.edu/jenkins/view/Spark/
> > > >
> > > > On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net
> <mailto:
> > > tony.reix@bull.net>> wrote:
> > > > Hi,
> > > > I'm checking that Spark works fine on a new environment (PPC64
> > hardware).
> > > > I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even
> > when
> > > running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I
> can
> > > find the results of the tests of Spark, for each version and for the
> > > different versions, in order to have a reference to compare my result=
s
> > > with. I cannot find them on Spark web-site.
> > > > Thx
> > > > Tony
> > > >
> > > >
> > > >
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > >
> >
>

--001a114043d6a5d909050c3dd160--

From dev-return-11079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 20:17:58 2015
Return-Path: <dev-return-11079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B19C10D7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 20:17:58 +0000 (UTC)
Received: (qmail 26684 invoked by uid 500); 9 Jan 2015 20:17:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26602 invoked by uid 500); 9 Jan 2015 20:17:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26590 invoked by uid 99); 9 Jan 2015 20:17:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:17:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.174 as permitted sender)
Received: from [209.85.160.174] (HELO mail-yk0-f174.google.com) (209.85.160.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:17:54 +0000
Received: by mail-yk0-f174.google.com with SMTP id 10so4889219ykt.5
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 12:16:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=8+44VXmWukUM0QYsUIWxX2DRLAp/dRUze4Rdf8/Ib3A=;
        b=lK7qThPn6MkvJhgqVGyQmzMEenWBP6pOZYwBLC5l4MMpyamRtQ0h/xPgO7jOsenFf8
         0ZC1dsOvc7uFM6qPhDDq6pvptRI6YFHAHdDSAJgPKHkwmom2BlNhirSz03z9kWsferC7
         JwunSe9hdbkaoH/WEl7heAszviPk8uROW53045sMGD2ipOMW6id1HgtF/+H7j2hVBY6f
         rkFpFf1wVPr51anASaLMCBHYwgPyPlN69Qb/pcHqQplyZo+5V4FhJ59O67PIoKrPqn9F
         bWhUk8GWDmFzQPWsVwWlgUBnrRUwmhB9ntuqhtCIZ75c8pSunTwGLxh83Wq10OuoO8dN
         HZEA==
X-Received: by 10.170.156.193 with SMTP id x184mr14068412ykc.120.1420834608936;
 Fri, 09 Jan 2015 12:16:48 -0800 (PST)
MIME-Version: 1.0
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Fri, 09 Jan 2015 20:16:46 +0000
Message-ID: <CANeJXFN6DTTfAZX24NhGf4hH55TuMZnqK8-BVjq+5spLiVfy1w@mail.gmail.com>
Subject: Present/Future of monitoring spark jobs, "MetricsSystem" vs. Web UI, etc.
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a50ba267d62050c3dd6ef
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a50ba267d62050c3dd6ef
Content-Type: text/plain; charset=UTF-8

I've long wished the web UI gave me a better sense of how the metrics it
reports are changing over time, so I was intrigued to stumble across the
MetricsSystem
<https://github.com/apache/spark/blob/b6aa557300275b835cce7baa7bc8a80eb5425cbb/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala>
infrastructure the other day.

I've set up a very basic Graphite instance and had dummy Spark jobs report
to it, but that process was a little bumpy (and the docs sparse
<https://spark.apache.org/docs/latest/monitoring.html#metrics>) so I wanted
to come up for air and ask a few questions about the present/future plans
for monitoring Spark jobs.

In rough order of increasing scope:

   - Do most people monitor their Spark jobs in realtime by repeatedly
   refreshing the web UI (cf. SPARK-5106
   <https://issues.apache.org/jira/browse/SPARK-5106>), or is there a
   better way?
   - Does anyone use or rely on the GraphiteSink? Quick googling turned up
   no evidence of anyone using it.
      - Likewise the other Sinks? GangliaSink?
   - Do people have custom Sink subclasses and dashboards that they've
   built to monitor Spark jobs, as was suggested by the appearance of a
   mysterious Ooyala "DatadogSink" gist
   <https://gist.github.com/ibuenros/9b94736c2bad2f4b8e23#file-sparkutils-scala-L336>
   in the recent thread on this list about custom metrics
   <http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-custom-metrics-tp9030p10041.html>
   ?
   - What is the longer-term plan for how people should monitor / diagnose
   problems at runtime?
      - Will the official Spark web UI remain the main way that the average
      user will monitor their jobs?
      - Or, will SPARK-3644
      <https://issues.apache.org/jira/browse/SPARK-3644> usher in an era of
      many external implementations of Spark web UIs, so that the average user
      will take one of those "off the shelf" that they like best (because its
      graphs are prettier or it emphasizes / pivots around certain metrics that
      others do not)?
      - Is the MetricsSystem infrastructure redundant with the REST API
      discussed in SPARK-3644
      <https://issues.apache.org/jira/browse/SPARK-3644>?
         - Would more robust versions of each start to be redundant in the
         future?
         - I feel like the answers are "somewhat yes" and "yes", and would
         like to hear other perspectives.

Basically, I want to live in a world where:

   - I can see all of the stats currently exposed on the Web UI,
   - as well as others that aren't there yet,
      - number of records assigned to each task,
      - number of records completed by each task in realtime,
      - gc stats in realtime,
      - # of spill events,
      - size of spill events,
   - and all kinds of derivates of the above,
      - latencies/histograms for everything
         - records per second per task,
         - records per second per executor,
         - top N slowest/worst of any metric,
         - avg spill size,
         - etc.
      - over time,
   - at scale <https://issues.apache.org/jira/browse/SPARK-2017>


Are we going to get to this world by improving the web UI that ships with
Spark? I am pessimistic of that approach:

   - It may be impossible to do in a way that satisfies all stakeholders'
   aesthetic sensibilities and preferences for what stats/views are important.
   - It would be a monumental undertaking relative to the amount of
   attention that seems to have been directed at improving the web UI in the
   last few quarters.

OTOH, if the space of derivative stats and slices thereof that we want to
support is as complex as the outline I gave above suggests it might be,
then Graphite (or some equivalent) could be well suited to the task.
However, this is at odds with the relative obscurity that the MetricsSystem
seems to reside in and my impression that it is not something that core
developers think about or are focused on.

Finally, while the existence of SPARK-3644 (and Josh et al's great work on
it thus far) implies that the REST API / "let 1000 [web UIs] bloom" vision
is at least nominally being pursued, it seems like it's still a long way
from fostering a world where my dream use-cases above are realized, and
it's not clear from the outside whether fulfilling that vision is a
priority.

So I'm interested to hear peoples' thoughts on the above questions and what
the plan is / should be going forward. Having learned a lot about how Spark
works, the process of figuring out "Why My Spark Jobs Are Failing" still
feels daunting (at best) using the tools I've come across; we need to do a
better job of empowering people to figure these things out.

--001a113a50ba267d62050c3dd6ef--

From dev-return-11080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 20:20:53 2015
Return-Path: <dev-return-11080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5982F10DC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 20:20:53 +0000 (UTC)
Received: (qmail 43898 invoked by uid 500); 9 Jan 2015 20:20:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43821 invoked by uid 500); 9 Jan 2015 20:20:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43801 invoked by uid 99); 9 Jan 2015 20:20:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:20:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.177 as permitted sender)
Received: from [209.85.160.177] (HELO mail-yk0-f177.google.com) (209.85.160.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:20:27 +0000
Received: by mail-yk0-f177.google.com with SMTP id 9so4896494ykp.8
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 12:18:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=e2EQSvmGFccvEGmwudcAfzoJFp+HDwSGrMnxUuRYErA=;
        b=nQrpjuoc3ehCZTK8HJtPIlF0HZxq22eRxo3EA2M107GLKvErO6mrjC7UMwFmxEelkI
         QEZMpH09cWzbvL1DPq9hylYNsf2Dej6gQrtFcVBhpJVBM5kJFgzW+Ji98EgfzeD7QGlO
         GiWvgM7UrD35tkksd0THCn0jBKmGLURqoO1y0rf5CIHTw9Esu5wIKcN67UHH2uMUwQv7
         uAMvN7rm7gkrOxt4toLyvNuzZCqh1p0ZKsHvT2gu0zNrUKBQymGpQS0PXeoCeeO324w0
         lCQVLrNCUIWqMMMpIC+75yaBfwN79u3zktVRfVJPtKYWIV9+3zbJjb+UjlhYUJ+TUfue
         tM3A==
MIME-Version: 1.0
X-Received: by 10.236.223.8 with SMTP id u8mr12652838yhp.150.1420834690395;
 Fri, 09 Jan 2015 12:18:10 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Fri, 9 Jan 2015 12:18:10 -0800 (PST)
In-Reply-To: <CAOEPXP5+8ttBKLhYb1Fixt8zK-sJy5rhxP77yXxQuTPCj7dZDA@mail.gmail.com>
References: <5EFAF879C767CF40BD00D800C20AE80E7CF0A16D@BUMSG3WM.fr.ad.bull.net>
	<CALte62wmLFcfHiX5AkiS00eFNjHhNuhdSrGFqw=QXY1wTv6QPw@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A233@BUMSG3WM.fr.ad.bull.net>
	<CALte62zA=iNhGCNnsauRGofnVum5tnb00GyCe1frqynfXfo7kA@mail.gmail.com>
	<5EFAF879C767CF40BD00D800C20AE80E7CF0A3C6@BUMSG3WM.fr.ad.bull.net>
	<CAMAsSd+D=Jbtox4VLAhLom9oappm+WWwzKpFFWDQGg3R=WV_og@mail.gmail.com>
	<CALte62wcnByQMp9zd0ju-=tkKW0=n3auiY1-r5gVhksgO_Nm4Q@mail.gmail.com>
	<CAOEPXP5+8ttBKLhYb1Fixt8zK-sJy5rhxP77yXxQuTPCj7dZDA@mail.gmail.com>
Date: Fri, 9 Jan 2015 12:18:10 -0800
Message-ID: <CALte62yLwcb1Td7djHbt7503bvc_2wG661DOV1Oov7QjCtE9ug@mail.gmail.com>
Subject: Re: Results of tests
From: Ted Yu <yuzhihong@gmail.com>
To: Josh Rosen <rosenville@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Tony Reix <tony.reix@bull.net>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134b6ce017507050c3ddb1b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134b6ce017507050c3ddb1b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I noticed that org.apache.spark.sql.hive.execution has a lot of tests
skipped.

Is there plan to enable these tests on Jenkins (so that there is no
regression across releases) ?

Cheers

On Fri, Jan 9, 2015 at 11:46 AM, Josh Rosen <rosenville@gmail.com> wrote:

> The "Test Result" pages for Jenkins builds shows some nice statistics for
> the test run, including individual test times:
>
>
> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wit=
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/testR=
eport/
>
> Currently this only covers the Java / Scala tests, but we might be able t=
o
> integrate the PySpark tests here, too (I think it's just a matter of
> getting the Python test runner to generate the correct test result XML
> output).
>
> On Fri, Jan 9, 2015 at 10:47 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> For a build which uses JUnit, we would see a summary such as the
>> following (
>> https://builds.apache.org/job/HBase-TRUNK/6007/console):
>>
>> Tests run: 2199, Failures: 0, Errors: 0, Skipped: 25
>>
>>
>> In
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull
>> , I don't see such statistics.
>>
>>
>> Looks like scalatest-maven-plugin can be enhanced :-)
>>
>>
>> On Fri, Jan 9, 2015 at 3:52 AM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> > Hey Tony, the number of tests run could vary depending on how the
>> > build is configured. For example, YARN-related tests would only run
>> > when the yarn profile is turned on. Java 8 tests would only run under
>> > Java 8.
>> >
>> > Although I don't know that there's any reason to believe the IBM JVM
>> > has a problem with Spark, I see this issue that is potentially related
>> > to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
>> > don't know if that was a Spark issue. Certainly, would be good for you
>> > to investigate if you are interested in resolving it.
>> >
>> > The Jenkins output shows you exactly what tests were run and how --
>> > have a look at the logs.
>> >
>> >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull
>> >
>> > On Fri, Jan 9, 2015 at 9:15 AM, Tony Reix <tony.reix@bull.net> wrote:
>> > > Hi Ted
>> > >
>> > > Thanks for the info.
>> > > However, I'm still unable to understand how the page:
>> > >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/
>> > > has been built.
>> > > This page contains details I do not find in the page you indicated t=
o
>> me:
>> > >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull
>> > >
>> > > As an example, I'm still unable to find these details:
>> > > org.apache.spark<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark/
>> >
>> >      12 mn   0
>> > >         1
>> > >         247
>> > >         248
>> > >
>> > > org.apache.spark.api.python<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark.api.python/
>> >
>> > 20 ms   0
>> > >         0
>> > >         2
>> > >         2
>> > >
>> > > org.apache.spark.bagel<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark.bagel/
>> >
>> >  7.7 s   0
>> > >         0
>> > >         4
>> > >         4
>> > >
>> > > org.apache.spark.broadcast<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark.broadcast/
>> >
>> >  43 s    0
>> > >         0
>> > >         17
>> > >         17
>> > >
>> > > org.apache.spark.deploy<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark.deploy/
>> >
>> > 16 s    0
>> > >         0
>> > >         29
>> > >         29
>> > >
>> > > org.apache.spark.deploy.worker<
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/org.apache.spark.deploy.worker/
>> >
>> >  0.55 s  0
>> > >         0
>> > >         12
>> > >         12
>> > >
>> > > ........
>> > >
>> > >
>> > > Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests
>> and
>> > 0 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 )=
.
>> > When using IBM JVM, there are only 2566 tests and 5 failures (in same
>> > component: Streaming).
>> > >
>> > > On my PPC64BE (BE =3D Big-Endian)environment, the tests block after =
2
>> > hundreds of tests.
>> > > Is Spark independent of Little/Big-Endian stuff ?
>> > >
>> > > On my PPC64LE (LE =3D Little-Endian) environment, I have 3485 tests =
only
>> > (like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
>> > >
>> > > So, I need to learn more about how your Jenkins environment extracts
>> > details about the results.
>> > > Moreover, which JVM is used ?
>> > >
>> > > Do you plan to use IBM JVM in order to check that Spark and IBM JVM
>> are
>> > compatible ? (they already do not look to be compatible 100% ...).
>> > >
>> > > Thanks
>> > >
>> > > Tony
>> > >
>> > > IBM Coop Architect & Technical Leader
>> > > Office : +33 (0) 4 76 29 72 67
>> > > 1 rue de Provence - 38432 =C3=89chirolles - France
>> > > www.atos.net<http://www.atos.net/>
>> > > ________________________________
>> > > De : Ted Yu [yuzhihong@gmail.com]
>> > > Envoy=C3=A9 : jeudi 8 janvier 2015 17:43
>> > > =C3=80 : Tony Reix
>> > > Cc : dev@spark.apache.org
>> > > Objet : Re: Results of tests
>> > >
>> > > Here it is:
>> > >
>> > > [centos] $
>> >
>> /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin=
/mvn
>> > -DHADOOP_PROFILE=3Dhadoop-2.4 -Dlabel=3Dcentos -DskipTests -Phadoop-2.=
4
>> -Pyarn
>> > -Phive clean package
>> > >
>> > >
>> > > You can find the above in
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/cons=
oleFull
>> > >
>> > >
>> > > Cheers
>> > >
>> > > On Thu, Jan 8, 2015 at 8:05 AM, Tony Reix <tony.reix@bull.net<mailto=
:
>> > tony.reix@bull.net>> wrote:
>> > > Thanks !
>> > >
>> > > I've been able to see that there are 3745 tests for version 1.2.0 wi=
th
>> > profile Hadoop 2.4  .
>> > > However, on my side, the maximum tests I've seen are 3485... About 3=
00
>> > tests are missing on my side.
>> > > Which Maven option has been used for producing the report file used
>> for
>> > building the page:
>> > >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-wi=
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=3Dhadoop-2.4,label=3Dcentos/test=
Report/
>> > >   ? (I'm not authorized to look at the "configuration" part)
>> > >
>> > > Thx !
>> > >
>> > > Tony
>> > >
>> > > ________________________________
>> > > De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
>> > > Envoy=C3=A9 : jeudi 8 janvier 2015 16:11
>> > > =C3=80 : Tony Reix
>> > > Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
>> > > Objet : Re: Results of tests
>> > >
>> > > Please take a look at
>> https://amplab.cs.berkeley.edu/jenkins/view/Spark/
>> > >
>> > > On Thu, Jan 8, 2015 at 5:40 AM, Tony Reix <tony.reix@bull.net<mailto=
:
>> > tony.reix@bull.net>> wrote:
>> > > Hi,
>> > > I'm checking that Spark works fine on a new environment (PPC64
>> hardware).
>> > > I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even
>> when
>> > running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I
>> can
>> > find the results of the tests of Spark, for each version and for the
>> > different versions, in order to have a reference to compare my results
>> > with. I cannot find them on Spark web-site.
>> > > Thx
>> > > Tony
>> > >
>> > >
>> > >
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>

--001a1134b6ce017507050c3ddb1b--

From dev-return-11081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 20:23:18 2015
Return-Path: <dev-return-11081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4DDA10DF8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 20:23:18 +0000 (UTC)
Received: (qmail 53123 invoked by uid 500); 9 Jan 2015 20:23:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53054 invoked by uid 500); 9 Jan 2015 20:23:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52763 invoked by uid 99); 9 Jan 2015 20:23:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:23:18 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ogeagla@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 20:23:12 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 1C9171008B9A
	for <dev@spark.apache.org>; Fri,  9 Jan 2015 12:21:53 -0800 (PST)
Date: Fri, 9 Jan 2015 13:21:52 -0700 (MST)
From: ogeagla <ogeagla@gmail.com>
To: dev@spark.apache.org
Message-ID: <1420834912389-10073.post@n3.nabble.com>
Subject: Re-use scaling means and variances from StandardScalerModel
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello,

I would like to re-use the means and variances computed by the fit function
in the StandardScaler, as I persist them and my use case requires consisted
scaling of data based on some initial data set.  The StandardScalerModel's
constructor takes means and variances, but is private[mllib]. 
Forking/compiling Spark or copy/pasting the class into my project are both
options, but  I'd like to stay away from them.  Any chance there is interest
in a PR to allow this re-use via removal of private from the the
constructor?  Or perhaps an alternative solution exists?  

Thanks,
Octavian



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-use-scaling-means-and-variances-from-StandardScalerModel-tp10073.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan  9 21:17:21 2015
Return-Path: <dev-return-11082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87CFE172D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  9 Jan 2015 21:17:21 +0000 (UTC)
Received: (qmail 80221 invoked by uid 500); 9 Jan 2015 21:17:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80129 invoked by uid 500); 9 Jan 2015 21:17:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80048 invoked by uid 99); 9 Jan 2015 21:17:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 21:17:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 Jan 2015 21:16:54 +0000
Received: by mail-qg0-f41.google.com with SMTP id e89so10840315qgf.0
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 13:14:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=46ZaCPOQzz29CO5zl84ieT5BK7xnlEZF5Hihdn4AIL4=;
        b=FQGSlCekuFwXVhNGFXOJL71vDQwspQMJoULa5k6AP0/NaAR5sTJBQicToSj7ugxA31
         4NRWS2vSrPbL0ZlLIfjIa3f243vTkHB3UL3Wa6zcUswJtFUAh0J9ejND904JhMmJzdum
         y7IKB8lNzYy2brwNOhgDxVc30vwXnkyAdE9KW9wBXXA9nOuv9MSS7H5uWD5l4PsgiwjF
         mWRhOEZIUlxTpLD8XjbceAuebfFyavzdrDhYX6g3jj5vbPhs7wqamxXhBy371zvolY7O
         aBgk6k+pPRxl1+R75nzXfc9cXT90eeg1TM83oj7u4KxU6RIVW1lFrphkZnjkw59cPcY1
         R6tg==
X-Received: by 10.224.32.69 with SMTP id b5mr30260497qad.53.1420838078116;
        Fri, 09 Jan 2015 13:14:38 -0800 (PST)
Received: from [142.157.43.103] (wpa043103.Wireless.McGill.CA. [142.157.43.103])
        by mx.google.com with ESMTPSA id l91sm8027447qge.34.2015.01.09.13.14.37
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Fri, 09 Jan 2015 13:14:37 -0800 (PST)
Date: Fri, 9 Jan 2015 16:14:32 -0500
From: Nan Zhu <zhunanmcgill@gmail.com>
To: Tathagata Das <tdas@databricks.com>
Cc: dev@spark.apache.org
Message-ID: <483F78BC828741A183FE2DE8BCB493DB@gmail.com>
In-Reply-To: <8BD22CD8D2F148BDAF988F8EFF31627E@gmail.com>
References: <105FCC9386BD420B9E4EDA0E8563CBC1@gmail.com>
 <CA+AHuKn+xX6a2_4iWPWq00q8av9=tyA3PmMFuVuBaqUgZt+QCg@mail.gmail.com>
 <8BD22CD8D2F148BDAF988F8EFF31627E@gmail.com>
Subject: Re: missing document of several messages in actor-based
 receiver?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54b044bc_1cf10fd8_19a"
X-Virus-Checked: Checked by ClamAV on apache.org

--54b044bc_1cf10fd8_19a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi, =20

I have created the PR for these two issues

Best, =20

-- =20
Nan Zhu
http://codingcat.me


On =46riday, January 9, 2015 at 7:38 AM, Nan Zhu wrote:

> Thanks, TD, =20
> =20
> I just created 2 JIRAs to track these, =20
> =20
> https://issues.apache.org/jira/browse/SPARK-5174
> =20
> https://issues.apache.org/jira/browse/SPARK-5175
> =20
> Can you help to me assign these two JIRAs to me, and I=E2=80=99d like t=
o submit the PRs
> =20
> Best, =20
> =20
> -- =20
> Nan Zhu
> http://codingcat.me
> =20
> =20
> On =46riday, January 9, 2015 at 4:25 AM, Tathagata Das wrote:
> =20
> > It was not really mean to be hidden. So its essentially the case of t=
he documentation being insufficient. This code has not gotten much attent=
ion for a while, so it could have a bugs. If you find any and submit a fi=
x for them, I am happy to take a look=21
> > =20
> > TD
> > =20
> > On Thu, Jan 8, 2015 at 6:33 PM, Nan Zhu <zhunanmcgill=40gmail.com (ma=
ilto:zhunanmcgill=40gmail.com)> wrote:
> > > Hi, TD and other streaming developers,
> > > =20
> > > When I look at the implementation of actor-based receiver (ActorRec=
eiver.scala), I found that there are several messages which are not menti=
oned in the document =20
> > > =20
> > > case props: Props =3D>
> > > val worker =3D context.actorOf(props)
> > > logInfo(=22Started receiver worker at:=22 + worker.path)
> > > sender =21 worker
> > > =20
> > > case (props: Props, name: String) =3D>
> > > val worker =3D context.actorOf(props, name)
> > > logInfo(=22Started receiver worker at:=22 + worker.path)
> > > sender =21 worker
> > > =20
> > > case =5F: PossiblyHarmful =3D> hiccups.incrementAndGet()
> > > =20
> > > case =5F: Statistics =3D>
> > > val workers =3D context.children
> > > sender =21 Statistics(n.get, workers.size, hiccups.get, workers.mkS=
tring(=22=5Cn=E2=80=9D))
> > > =20
> > > Is it hided with intention or incomplete document, or I missed some=
thing=3F
> > > And the handler of these messages are =E2=80=9Cbuggy=22=3F e.g. whe=
n we start a new worker, we didn=E2=80=99t increase n (counter of childre=
n), and n and hiccups are unnecessarily set to AtomicInteger =3F
> > > =20
> > > Best,
> > > =20
> > > -- =20
> > > Nan Zhu
> > > http://codingcat.me
> > > =20
> > > =20
> > =20
> > =20
> > =20
> =20


--54b044bc_1cf10fd8_19a--


From dev-return-11083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 10 06:16:07 2015
Return-Path: <dev-return-11083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE3071009D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 10 Jan 2015 06:16:07 +0000 (UTC)
Received: (qmail 25020 invoked by uid 500); 10 Jan 2015 06:16:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24948 invoked by uid 500); 10 Jan 2015 06:16:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24936 invoked by uid 99); 10 Jan 2015 06:16:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 Jan 2015 06:16:06 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 Jan 2015 06:15:41 +0000
Received: by mail-ig0-f172.google.com with SMTP id hl2so4925334igb.5
        for <dev@spark.apache.org>; Fri, 09 Jan 2015 22:14:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=O9pNN5sfyAB09Zp6CxKIRHYBbq/bmXFciWyHkKsbWfM=;
        b=LqB+flw1+eIDuy/p1oTGqbez/MyENAVWm2EztcRpi56svTOWe5bdAhJFPZQsD/dTay
         RID37kOnwV8hrLfgfXDZIGRcXkhPJ/SQoXPopzJ+UMT5b0Lcu97JivEDIKA4UAOAEGbE
         9pmbamC794IDcugi9GSZ8t+hkLNdC8LMgaCaRkty8LVccdjvMxucG19QR25HO4utLiHz
         8DM2hjScxl20JQSCgL6Kn3hYz6PNBExEdYO8SZY3dT3Ne7bfugz+bch0zjmNNrMFio8y
         Cq+WUykrdIMvTqdDqK0DUUFEJ2QuHe7IPjmR+RSgc2fYf3gPHHc9Zje7WLZ4ZM2Z932X
         c8QA==
MIME-Version: 1.0
X-Received: by 10.107.135.163 with SMTP id r35mr18473374ioi.25.1420870495251;
 Fri, 09 Jan 2015 22:14:55 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Fri, 9 Jan 2015 22:14:55 -0800 (PST)
In-Reply-To: <1420834912389-10073.post@n3.nabble.com>
References: <1420834912389-10073.post@n3.nabble.com>
Date: Fri, 9 Jan 2015 22:14:55 -0800
Message-ID: <CAJgQjQ-hJJP65YXjGj7a2FHLae8i9c3x+w1e5Sg1S4jS9Dv2yA@mail.gmail.com>
Subject: Re: Re-use scaling means and variances from StandardScalerModel
From: Xiangrui Meng <mengxr@gmail.com>
To: ogeagla <ogeagla@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Feel free to create a JIRA for this issue. We might need to discuss
what to put in the public constructors. In the meanwhile, you can use
Java serialization to save/load the model:

sc.parallelize(Seq(model), 1).saveAsObjectFile("/tmp/model")
val model = sc.objectFile[StandardScalerModel]("/tmp/model").first()

-Xiangrui

On Fri, Jan 9, 2015 at 12:21 PM, ogeagla <ogeagla@gmail.com> wrote:
> Hello,
>
> I would like to re-use the means and variances computed by the fit function
> in the StandardScaler, as I persist them and my use case requires consisted
> scaling of data based on some initial data set.  The StandardScalerModel's
> constructor takes means and variances, but is private[mllib].
> Forking/compiling Spark or copy/pasting the class into my project are both
> options, but  I'd like to stay away from them.  Any chance there is interest
> in a PR to allow this re-use via removal of private from the the
> constructor?  Or perhaps an alternative solution exists?
>
> Thanks,
> Octavian
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-use-scaling-means-and-variances-from-StandardScalerModel-tp10073.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 11 00:41:30 2015
Return-Path: <dev-return-11084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C1AFC7BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 Jan 2015 00:41:30 +0000 (UTC)
Received: (qmail 64754 invoked by uid 500); 11 Jan 2015 00:41:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64667 invoked by uid 500); 11 Jan 2015 00:41:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64650 invoked by uid 99); 11 Jan 2015 00:41:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 00:41:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 00:41:24 +0000
Received: by mail-ob0-f182.google.com with SMTP id wo20so18022025obc.13
        for <dev@spark.apache.org>; Sat, 10 Jan 2015 16:41:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=3ol00BbdRX3nkx0VN99NKorm38hIB6189MGvAQym2U8=;
        b=Z/4Vrk7uXNnT75tt4yfObvlw7uMKg7oZZCbzmgQIv9AIbXt/Yl67k+/rFYNmGOaNXb
         kscN/3mdPDgZoVh8tB2H40i5bSxDZnuFn62munjNzvsgR20YI9LMisbGBsvOsxgj/brz
         FkvfdTHv10RhGr73VaGDCv371jBuME2dNxcgFShrdqf56F5rHIzy7GqjRC42Z7JHvdVB
         uDbT/ceMHwVKZQwFvLfjTrpMviX5HIRRO8JdhKab9rkAsGztbZkXGrI7A8uqtbTOTdis
         KyRlRfwsNq+POO6lNzB6XQKfhoMooRAzaQZlNAYsYZM87fx2rveVoaTlj75ZEzZq4psF
         fdmg==
X-Received: by 10.202.135.78 with SMTP id j75mr12786879oid.106.1420936863905;
 Sat, 10 Jan 2015 16:41:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Sat, 10 Jan 2015 16:40:43 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Sat, 10 Jan 2015 16:40:43 -0800
Message-ID: <CAJc_syLZJ2VJC7H6gxLBbpTfQtsXrt2HJvhGGhyA6VBSjW55Sw@mail.gmail.com>
Subject: Job priority
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ef56a05802f050c55a593
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ef56a05802f050c55a593
Content-Type: text/plain; charset=UTF-8

Is it possible to specify a priority level for a job, such that the active
jobs might be scheduled in order of priority?

Alex

--001a113ef56a05802f050c55a593--

From dev-return-11085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 11 15:39:02 2015
Return-Path: <dev-return-11085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B85A1083E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 Jan 2015 15:39:02 +0000 (UTC)
Received: (qmail 11017 invoked by uid 500); 11 Jan 2015 15:39:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10932 invoked by uid 500); 11 Jan 2015 15:39:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10048 invoked by uid 99); 11 Jan 2015 15:38:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 15:38:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 15:38:55 +0000
Received: by mail-ob0-f171.google.com with SMTP id uz6so19269322obc.2;
        Sun, 11 Jan 2015 07:37:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=DUNtZ/wSvDBy1u7WeROOpjDeIhYskaoCsYaWe0vjbM4=;
        b=toBhdS5fruXKra77I4Nf627qNFRoZfMpDy3gdNwxSvLezuzkxoVXGfT5ecnH8ATmaM
         mevh051FLfLiK5NLR/EkZQBOBRyAvof6W7qv2Sz1t3I7XLPrhNHdzlm07/UkRJBOS0k0
         6S336B0rRs0mDHfBbN6FwSzLcaYfYzvPVKbZzaj7LSdQybzIv/Rp8sXRlMby+JrWmIga
         32cEq0y3dCWI6eHa+IsN9OWcRo47XqgFVF0FqZ0ePolp3DU5iTACNlk7CSLvfKVIK5HM
         ggaovcU6LioPMR1NzN2h/XM70VWRzPRVoio1lOhm2351wWupRToSPHOLHN3C9ig/64tm
         A+bA==
X-Received: by 10.202.224.198 with SMTP id x189mr14222319oig.62.1420990624700;
 Sun, 11 Jan 2015 07:37:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Sun, 11 Jan 2015 07:36:44 -0800 (PST)
In-Reply-To: <CAKWX9VVSNJtvugnYU21Ms=G2fq6OWEwP+QmhCZmkMV9CXpoXtA@mail.gmail.com>
References: <CAJc_syLZJ2VJC7H6gxLBbpTfQtsXrt2HJvhGGhyA6VBSjW55Sw@mail.gmail.com>
 <CAAsvFP=rdV-JG0Yv950TY9979kGAZ3d4+28GLotNJHq+-mHfGg@mail.gmail.com>
 <CAJc_syKPRdrAN_bR4hvBEvrBEDm+WB0432cuo0R2-VdLg8b78g@mail.gmail.com>
 <CAKWX9VWpW_xueXQ7TcNG4FgTc6_wKUzZ=SdZeoJH6s4aH1oR7A@mail.gmail.com>
 <CAJc_syJJvmYYF+H5zFOGUcWMmLoBGM1wC9A6Mu4F1jDSt54idg@mail.gmail.com> <CAKWX9VVSNJtvugnYU21Ms=G2fq6OWEwP+QmhCZmkMV9CXpoXtA@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Sun, 11 Jan 2015 07:36:44 -0800
Message-ID: <CAJc_syJWQkEzUdcXMWrAtoZt7aOKPtOGJ5ALV_CG0x1nfL=WVw@mail.gmail.com>
Subject: Re: Job priority
To: Cody Koeninger <cody@koeninger.org>
Cc: Mark Hamstra <mark@clearstorydata.com>, 
	"user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d34306a21d5050c6229c5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d34306a21d5050c6229c5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Cody,

While I might be able to improve the scheduling of my jobs by using a few
different pools with weights equal to, say, 1, 1e3 and 1e6, effectively
getting a small handful of priority classes. Still, this is really not
quite what I am describing. This is why my original post was on the dev
list. Let me then ask if there is any interest in having priority queue job
scheduling in Spark. This is something I might be able to pull off.

Alex

On Sun, Jan 11, 2015 at 6:21 AM, Cody Koeninger <cody@koeninger.org> wrote:

> If you set up a number of pools equal to the number of different priority
> levels you want, make the relative weights of those pools very different,
> and submit a job to the pool representing its priority, I think youll get
> behavior equivalent to a priority queue. Try it and see.
>
> If I'm misunderstandng what youre trying to do, then I don't know.
>
>
> On Sunday, January 11, 2015, Alessandro Baretta <alexbaretta@gmail.com>
> wrote:
>
>> Cody,
>>
>> Maybe I'm not getting this, but it doesn't look like this page is
>> describing a priority queue scheduling policy. What this section discuss=
es
>> is how resources are shared between queues. A weight-1000 pool will get
>> 1000 times more resources allocated to it than a priority 1 queue. Great=
,
>> but not what I want. I want to be able to define an Ordering on make my
>> tasks representing their priority, and have Spark allocate all resources=
 to
>> the job that has the highest priority.
>>
>> Alex
>>
>> On Sat, Jan 10, 2015 at 10:11 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>>
>>>
>>> http://spark.apache.org/docs/latest/job-scheduling.html#configuring-poo=
l-properties
>>>
>>> "Setting a high weight such as 1000 also makes it possible to implement
>>> *priority* between pools=E2=80=94in essence, the weight-1000 pool will =
always
>>> get to launch tasks first whenever it has jobs active."
>>>
>>> On Sat, Jan 10, 2015 at 11:57 PM, Alessandro Baretta <
>>> alexbaretta@gmail.com> wrote:
>>>
>>>> Mark,
>>>>
>>>> Thanks, but I don't see how this documentation solves my problem. You
>>>> are referring me to documentation of fair scheduling; whereas, I am as=
king
>>>> about as unfair a scheduling policy as can be: a priority queue.
>>>>
>>>> Alex
>>>>
>>>> On Sat, Jan 10, 2015 at 5:00 PM, Mark Hamstra <mark@clearstorydata.com=
>
>>>> wrote:
>>>>
>>>>> -dev, +user
>>>>>
>>>>> http://spark.apache.org/docs/latest/job-scheduling.html
>>>>>
>>>>>
>>>>> On Sat, Jan 10, 2015 at 4:40 PM, Alessandro Baretta <
>>>>> alexbaretta@gmail.com> wrote:
>>>>>
>>>>>> Is it possible to specify a priority level for a job, such that the
>>>>>> active
>>>>>> jobs might be scheduled in order of priority?
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>

--001a113d34306a21d5050c6229c5--

From dev-return-11086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 11 18:07:27 2015
Return-Path: <dev-return-11086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78D6510B52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 Jan 2015 18:07:27 +0000 (UTC)
Received: (qmail 86506 invoked by uid 500); 11 Jan 2015 18:07:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86428 invoked by uid 500); 11 Jan 2015 18:07:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86417 invoked by uid 99); 11 Jan 2015 18:07:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 18:07:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 Jan 2015 18:07:02 +0000
Received: by mail-wi0-f178.google.com with SMTP id em10so10665576wid.5
        for <dev@spark.apache.org>; Sun, 11 Jan 2015 10:07:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wHRcbE6HRQ56RucikIYe/nEEMlt0stD7Ex7UCg6+sls=;
        b=CWigOfJEdZ3yQoRKR+0FGdY/3SocMYhCbvVWE7CyzoEaRF2EOdL21SoggROIFigfkK
         YGg+NlSuMzO9B0W+SBeLXb2rLZcurrTgPIN75Q8AWh24nnAGjBXn7dbWAZiN9jbYwWnR
         Rqsh2hc5X26i4VAoFyeYwOAUXJaq0t53wJn5qkPFMfTsmsBvub/M8BAycJzRvfKKV/6V
         yIilELZnEpxncNw+rzp1VobSQWl0zYLlH1q7ppM1XIeilNvMZA8shIRYqGQKPd8RszTm
         0/Ay9G34+qq+xnrptsUWehO2iNgg093W4U8Cn5WEHFJi6Rapes8T7pZFSYZOR45YxLQj
         4IZw==
X-Gm-Message-State: ALoCoQn1zTxRO9PkuXAPCQ7p+T1mfAih1yidjJlabSOVKv++4BL4s+9sfdBJ111v9WWCEBaQ3NFn
MIME-Version: 1.0
X-Received: by 10.180.126.99 with SMTP id mx3mr23704688wib.66.1420999620602;
 Sun, 11 Jan 2015 10:07:00 -0800 (PST)
Received: by 10.216.114.148 with HTTP; Sun, 11 Jan 2015 10:07:00 -0800 (PST)
In-Reply-To: <CAJc_syJWQkEzUdcXMWrAtoZt7aOKPtOGJ5ALV_CG0x1nfL=WVw@mail.gmail.com>
References: <CAJc_syLZJ2VJC7H6gxLBbpTfQtsXrt2HJvhGGhyA6VBSjW55Sw@mail.gmail.com>
	<CAAsvFP=rdV-JG0Yv950TY9979kGAZ3d4+28GLotNJHq+-mHfGg@mail.gmail.com>
	<CAJc_syKPRdrAN_bR4hvBEvrBEDm+WB0432cuo0R2-VdLg8b78g@mail.gmail.com>
	<CAKWX9VWpW_xueXQ7TcNG4FgTc6_wKUzZ=SdZeoJH6s4aH1oR7A@mail.gmail.com>
	<CAJc_syJJvmYYF+H5zFOGUcWMmLoBGM1wC9A6Mu4F1jDSt54idg@mail.gmail.com>
	<CAKWX9VVSNJtvugnYU21Ms=G2fq6OWEwP+QmhCZmkMV9CXpoXtA@mail.gmail.com>
	<CAJc_syJWQkEzUdcXMWrAtoZt7aOKPtOGJ5ALV_CG0x1nfL=WVw@mail.gmail.com>
Date: Sun, 11 Jan 2015 10:07:00 -0800
Message-ID: <CAAsvFP=N54jM-VbuxKqi0hWyt9eyB4TdZ11XR1UD3OLAk21Wbg@mail.gmail.com>
Subject: Re: Job priority
From: Mark Hamstra <mark@clearstorydata.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: Cody Koeninger <cody@koeninger.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f8389d19cc1f6050c644100
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f8389d19cc1f6050c644100
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes, if you are asking about developing a new priority queue job scheduling
feature and not just about how job scheduling currently works in Spark, the
that's a dev list issue.  The current job scheduling priority is at the
granularity of pools containing jobs, not the jobs themselves; so if you
require strictly job-level priority queuing, that would require a new
development effort -- and one that I expect will involve a lot of tricky
corner cases.

Sorry for misreading the nature of your initial inquiry.

On Sun, Jan 11, 2015 at 7:36 AM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Cody,
>
> While I might be able to improve the scheduling of my jobs by using a few
> different pools with weights equal to, say, 1, 1e3 and 1e6, effectively
> getting a small handful of priority classes. Still, this is really not
> quite what I am describing. This is why my original post was on the dev
> list. Let me then ask if there is any interest in having priority queue j=
ob
> scheduling in Spark. This is something I might be able to pull off.
>
> Alex
>
> On Sun, Jan 11, 2015 at 6:21 AM, Cody Koeninger <cody@koeninger.org>
> wrote:
>
>> If you set up a number of pools equal to the number of different priorit=
y
>> levels you want, make the relative weights of those pools very different=
,
>> and submit a job to the pool representing its priority, I think youll ge=
t
>> behavior equivalent to a priority queue. Try it and see.
>>
>> If I'm misunderstandng what youre trying to do, then I don't know.
>>
>>
>> On Sunday, January 11, 2015, Alessandro Baretta <alexbaretta@gmail.com>
>> wrote:
>>
>>> Cody,
>>>
>>> Maybe I'm not getting this, but it doesn't look like this page is
>>> describing a priority queue scheduling policy. What this section discus=
ses
>>> is how resources are shared between queues. A weight-1000 pool will get
>>> 1000 times more resources allocated to it than a priority 1 queue. Grea=
t,
>>> but not what I want. I want to be able to define an Ordering on make my
>>> tasks representing their priority, and have Spark allocate all resource=
s to
>>> the job that has the highest priority.
>>>
>>> Alex
>>>
>>> On Sat, Jan 10, 2015 at 10:11 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>>
>>>>
>>>> http://spark.apache.org/docs/latest/job-scheduling.html#configuring-po=
ol-properties
>>>>
>>>> "Setting a high weight such as 1000 also makes it possible to
>>>> implement *priority* between pools=E2=80=94in essence, the weight-1000=
 pool
>>>> will always get to launch tasks first whenever it has jobs active."
>>>>
>>>> On Sat, Jan 10, 2015 at 11:57 PM, Alessandro Baretta <
>>>> alexbaretta@gmail.com> wrote:
>>>>
>>>>> Mark,
>>>>>
>>>>> Thanks, but I don't see how this documentation solves my problem. You
>>>>> are referring me to documentation of fair scheduling; whereas, I am a=
sking
>>>>> about as unfair a scheduling policy as can be: a priority queue.
>>>>>
>>>>> Alex
>>>>>
>>>>> On Sat, Jan 10, 2015 at 5:00 PM, Mark Hamstra <mark@clearstorydata.co=
m
>>>>> > wrote:
>>>>>
>>>>>> -dev, +user
>>>>>>
>>>>>> http://spark.apache.org/docs/latest/job-scheduling.html
>>>>>>
>>>>>>
>>>>>> On Sat, Jan 10, 2015 at 4:40 PM, Alessandro Baretta <
>>>>>> alexbaretta@gmail.com> wrote:
>>>>>>
>>>>>>> Is it possible to specify a priority level for a job, such that the
>>>>>>> active
>>>>>>> jobs might be scheduled in order of priority?
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>

--e89a8f8389d19cc1f6050c644100--

From dev-return-11087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 04:35:35 2015
Return-Path: <dev-return-11087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5EB8B179A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 04:35:35 +0000 (UTC)
Received: (qmail 60671 invoked by uid 500); 12 Jan 2015 04:35:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60575 invoked by uid 500); 12 Jan 2015 04:35:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60551 invoked by uid 99); 12 Jan 2015 04:35:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 04:35:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 04:35:30 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so20655142obc.0
        for <dev@spark.apache.org>; Sun, 11 Jan 2015 20:34:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=y+2VDG4hG+L0cN05sY8ruBJzo6OkiSqWbGOoOAd7fog=;
        b=FKQQFNtMlKffPmjSEHuw35nO4xUfzABrbOG+KTqfOELloPfF3jzl6vjbG1T5h4LS7z
         2b401mC57AX4kMTUhr2jBTbwpqKDCHMhWCHM09lHp0ndnOQf/cTQeAYLZj0nwfTrDTGt
         sDDCB9mUnv3VhJA1le9Iawo0Uir2XsABcW/qYuHblLYl7qGTX5Ia0zeTHD/wRkgs7xHs
         9zym6mPoXolaWIATXwEeCycn8u9qVWg4HbwPgPPpT2Cr00ffUy6eQ4UjUqwk9XwrZDk4
         3qzUkQjlZE5d7uMKikP2Gh3PugYctYetlyQjLCDDu+VOMhlc4fl6G9sAfaVJtLU4PjoB
         NGzQ==
MIME-Version: 1.0
X-Received: by 10.60.81.226 with SMTP id d2mr16021596oey.83.1421037265026;
 Sun, 11 Jan 2015 20:34:25 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Sun, 11 Jan 2015 20:34:24 -0800 (PST)
In-Reply-To: <CAAsvFP=N54jM-VbuxKqi0hWyt9eyB4TdZ11XR1UD3OLAk21Wbg@mail.gmail.com>
References: <CAJc_syLZJ2VJC7H6gxLBbpTfQtsXrt2HJvhGGhyA6VBSjW55Sw@mail.gmail.com>
	<CAAsvFP=rdV-JG0Yv950TY9979kGAZ3d4+28GLotNJHq+-mHfGg@mail.gmail.com>
	<CAJc_syKPRdrAN_bR4hvBEvrBEDm+WB0432cuo0R2-VdLg8b78g@mail.gmail.com>
	<CAKWX9VWpW_xueXQ7TcNG4FgTc6_wKUzZ=SdZeoJH6s4aH1oR7A@mail.gmail.com>
	<CAJc_syJJvmYYF+H5zFOGUcWMmLoBGM1wC9A6Mu4F1jDSt54idg@mail.gmail.com>
	<CAKWX9VVSNJtvugnYU21Ms=G2fq6OWEwP+QmhCZmkMV9CXpoXtA@mail.gmail.com>
	<CAJc_syJWQkEzUdcXMWrAtoZt7aOKPtOGJ5ALV_CG0x1nfL=WVw@mail.gmail.com>
	<CAAsvFP=N54jM-VbuxKqi0hWyt9eyB4TdZ11XR1UD3OLAk21Wbg@mail.gmail.com>
Date: Sun, 11 Jan 2015 20:34:24 -0800
Message-ID: <CABPQxstq+147+msug7iDS+-_KLsM9vLPa6RuxeeXUa=HQ-fwsQ@mail.gmail.com>
Subject: Re: Job priority
From: Patrick Wendell <pwendell@gmail.com>
To: Mark Hamstra <mark@clearstorydata.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, Cody Koeninger <cody@koeninger.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Priority scheduling isn't something we've supported in Spark and we've
opted to support FIFO and Fair scheduling and asked users to try and
fit these to the needs of their applications.

In practice from what I've seen of priority schedulers, such as the
linux CPU scheduler, is that strict priority scheduling is never used
in practice because of priority starvation and other issues. So you
have this second tier of heuristics that exist to deal with issues
like starvation, priority inversion, etc, and these become very
complex over time.

That said, I looked a this a bit with @kayousterhout and I don't think
it would be very hard to implement a simple priority scheduler in the
current architecture. My main concern would be additional complexity
that would develop over time, based on looking at previous
implementations in the wild.

Alessandro, would you be able to open a JIRA and list some of your
requirements there? That way we could hear whether other people have
similar needs.

- Patrick

On Sun, Jan 11, 2015 at 10:07 AM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Yes, if you are asking about developing a new priority queue job scheduling
> feature and not just about how job scheduling currently works in Spark, the
> that's a dev list issue.  The current job scheduling priority is at the
> granularity of pools containing jobs, not the jobs themselves; so if you
> require strictly job-level priority queuing, that would require a new
> development effort -- and one that I expect will involve a lot of tricky
> corner cases.
>
> Sorry for misreading the nature of your initial inquiry.
>
> On Sun, Jan 11, 2015 at 7:36 AM, Alessandro Baretta <alexbaretta@gmail.com>
> wrote:
>
>> Cody,
>>
>> While I might be able to improve the scheduling of my jobs by using a few
>> different pools with weights equal to, say, 1, 1e3 and 1e6, effectively
>> getting a small handful of priority classes. Still, this is really not
>> quite what I am describing. This is why my original post was on the dev
>> list. Let me then ask if there is any interest in having priority queue job
>> scheduling in Spark. This is something I might be able to pull off.
>>
>> Alex
>>
>> On Sun, Jan 11, 2015 at 6:21 AM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>>
>>> If you set up a number of pools equal to the number of different priority
>>> levels you want, make the relative weights of those pools very different,
>>> and submit a job to the pool representing its priority, I think youll get
>>> behavior equivalent to a priority queue. Try it and see.
>>>
>>> If I'm misunderstandng what youre trying to do, then I don't know.
>>>
>>>
>>> On Sunday, January 11, 2015, Alessandro Baretta <alexbaretta@gmail.com>
>>> wrote:
>>>
>>>> Cody,
>>>>
>>>> Maybe I'm not getting this, but it doesn't look like this page is
>>>> describing a priority queue scheduling policy. What this section discusses
>>>> is how resources are shared between queues. A weight-1000 pool will get
>>>> 1000 times more resources allocated to it than a priority 1 queue. Great,
>>>> but not what I want. I want to be able to define an Ordering on make my
>>>> tasks representing their priority, and have Spark allocate all resources to
>>>> the job that has the highest priority.
>>>>
>>>> Alex
>>>>
>>>> On Sat, Jan 10, 2015 at 10:11 PM, Cody Koeninger <cody@koeninger.org>
>>>> wrote:
>>>>
>>>>>
>>>>> http://spark.apache.org/docs/latest/job-scheduling.html#configuring-pool-properties
>>>>>
>>>>> "Setting a high weight such as 1000 also makes it possible to
>>>>> implement *priority* between pools--in essence, the weight-1000 pool
>>>>> will always get to launch tasks first whenever it has jobs active."
>>>>>
>>>>> On Sat, Jan 10, 2015 at 11:57 PM, Alessandro Baretta <
>>>>> alexbaretta@gmail.com> wrote:
>>>>>
>>>>>> Mark,
>>>>>>
>>>>>> Thanks, but I don't see how this documentation solves my problem. You
>>>>>> are referring me to documentation of fair scheduling; whereas, I am asking
>>>>>> about as unfair a scheduling policy as can be: a priority queue.
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>> On Sat, Jan 10, 2015 at 5:00 PM, Mark Hamstra <mark@clearstorydata.com
>>>>>> > wrote:
>>>>>>
>>>>>>> -dev, +user
>>>>>>>
>>>>>>> http://spark.apache.org/docs/latest/job-scheduling.html
>>>>>>>
>>>>>>>
>>>>>>> On Sat, Jan 10, 2015 at 4:40 PM, Alessandro Baretta <
>>>>>>> alexbaretta@gmail.com> wrote:
>>>>>>>
>>>>>>>> Is it possible to specify a priority level for a job, such that the
>>>>>>>> active
>>>>>>>> jobs might be scheduled in order of priority?
>>>>>>>>
>>>>>>>> Alex
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 04:37:17 2015
Return-Path: <dev-return-11088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6EA4C179AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 04:37:17 +0000 (UTC)
Received: (qmail 63084 invoked by uid 500); 12 Jan 2015 04:37:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63009 invoked by uid 500); 12 Jan 2015 04:37:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62996 invoked by uid 99); 12 Jan 2015 04:37:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 04:37:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 04:37:13 +0000
Received: by mail-ob0-f179.google.com with SMTP id va2so20660404obc.10
        for <dev@spark.apache.org>; Sun, 11 Jan 2015 20:36:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=GFKCOrYJGS/rBIhR9VV1XZDLsfUn886b2xyh7DX7uKg=;
        b=rD9Ce+RBA1fyZ+7HnrXpgkNxFkI/DWZ8tYgGImLfHztXthEP3Mlu8UQsc9TCQjs74C
         305HOZnqhwq1KM6ySSgHOXcxtSe62qkzXsZdKEwWgMnBh33RBMpy6l6GrIOKkgngFcR8
         HZ/IgMdU1tfE1cuFc3PXIMS/ytIZ2ZznoWZA2kMQsf885h6TAgRw/6Zny7C41tHIyWP3
         5nkB/Mas0jL902B1MVrnBda0S9rhK5LM41PSrPnwk8AwG7z8IqxEI6crH4lM+jiejhA5
         IZdx4C4GBgBtvCLyIpu4aKP1kSxfR6cGcDzzxZsOTNfElNwWJseVtdxHXmLSwqXFNoDq
         USHQ==
X-Received: by 10.202.93.135 with SMTP id r129mr15157937oib.53.1421037367605;
 Sun, 11 Jan 2015 20:36:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Sun, 11 Jan 2015 20:35:47 -0800 (PST)
In-Reply-To: <CABPQxstq+147+msug7iDS+-_KLsM9vLPa6RuxeeXUa=HQ-fwsQ@mail.gmail.com>
References: <CAJc_syLZJ2VJC7H6gxLBbpTfQtsXrt2HJvhGGhyA6VBSjW55Sw@mail.gmail.com>
 <CAAsvFP=rdV-JG0Yv950TY9979kGAZ3d4+28GLotNJHq+-mHfGg@mail.gmail.com>
 <CAJc_syKPRdrAN_bR4hvBEvrBEDm+WB0432cuo0R2-VdLg8b78g@mail.gmail.com>
 <CAKWX9VWpW_xueXQ7TcNG4FgTc6_wKUzZ=SdZeoJH6s4aH1oR7A@mail.gmail.com>
 <CAJc_syJJvmYYF+H5zFOGUcWMmLoBGM1wC9A6Mu4F1jDSt54idg@mail.gmail.com>
 <CAKWX9VVSNJtvugnYU21Ms=G2fq6OWEwP+QmhCZmkMV9CXpoXtA@mail.gmail.com>
 <CAJc_syJWQkEzUdcXMWrAtoZt7aOKPtOGJ5ALV_CG0x1nfL=WVw@mail.gmail.com>
 <CAAsvFP=N54jM-VbuxKqi0hWyt9eyB4TdZ11XR1UD3OLAk21Wbg@mail.gmail.com> <CABPQxstq+147+msug7iDS+-_KLsM9vLPa6RuxeeXUa=HQ-fwsQ@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Sun, 11 Jan 2015 20:35:47 -0800
Message-ID: <CAJc_syLECGtk_JGczoWyJa=gySFOU4_r+=C9pVyE56wzJHemRg@mail.gmail.com>
Subject: Re: Job priority
To: Patrick Wendell <pwendell@gmail.com>
Cc: Mark Hamstra <mark@clearstorydata.com>, Cody Koeninger <cody@koeninger.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d4a96824053050c6d0bd7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d4a96824053050c6d0bd7
Content-Type: text/plain; charset=UTF-8

Ok, will do.

Thanks for providing some context on this topic.

Alex

On Sun, Jan 11, 2015 at 8:34 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Priority scheduling isn't something we've supported in Spark and we've
> opted to support FIFO and Fair scheduling and asked users to try and
> fit these to the needs of their applications.
>
> In practice from what I've seen of priority schedulers, such as the
> linux CPU scheduler, is that strict priority scheduling is never used
> in practice because of priority starvation and other issues. So you
> have this second tier of heuristics that exist to deal with issues
> like starvation, priority inversion, etc, and these become very
> complex over time.
>
> That said, I looked a this a bit with @kayousterhout and I don't think
> it would be very hard to implement a simple priority scheduler in the
> current architecture. My main concern would be additional complexity
> that would develop over time, based on looking at previous
> implementations in the wild.
>
> Alessandro, would you be able to open a JIRA and list some of your
> requirements there? That way we could hear whether other people have
> similar needs.
>
> - Patrick
>
> On Sun, Jan 11, 2015 at 10:07 AM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > Yes, if you are asking about developing a new priority queue job
> scheduling
> > feature and not just about how job scheduling currently works in Spark,
> the
> > that's a dev list issue.  The current job scheduling priority is at the
> > granularity of pools containing jobs, not the jobs themselves; so if you
> > require strictly job-level priority queuing, that would require a new
> > development effort -- and one that I expect will involve a lot of tricky
> > corner cases.
> >
> > Sorry for misreading the nature of your initial inquiry.
> >
> > On Sun, Jan 11, 2015 at 7:36 AM, Alessandro Baretta <
> alexbaretta@gmail.com>
> > wrote:
> >
> >> Cody,
> >>
> >> While I might be able to improve the scheduling of my jobs by using a
> few
> >> different pools with weights equal to, say, 1, 1e3 and 1e6, effectively
> >> getting a small handful of priority classes. Still, this is really not
> >> quite what I am describing. This is why my original post was on the dev
> >> list. Let me then ask if there is any interest in having priority queue
> job
> >> scheduling in Spark. This is something I might be able to pull off.
> >>
> >> Alex
> >>
> >> On Sun, Jan 11, 2015 at 6:21 AM, Cody Koeninger <cody@koeninger.org>
> >> wrote:
> >>
> >>> If you set up a number of pools equal to the number of different
> priority
> >>> levels you want, make the relative weights of those pools very
> different,
> >>> and submit a job to the pool representing its priority, I think youll
> get
> >>> behavior equivalent to a priority queue. Try it and see.
> >>>
> >>> If I'm misunderstandng what youre trying to do, then I don't know.
> >>>
> >>>
> >>> On Sunday, January 11, 2015, Alessandro Baretta <alexbaretta@gmail.com
> >
> >>> wrote:
> >>>
> >>>> Cody,
> >>>>
> >>>> Maybe I'm not getting this, but it doesn't look like this page is
> >>>> describing a priority queue scheduling policy. What this section
> discusses
> >>>> is how resources are shared between queues. A weight-1000 pool will
> get
> >>>> 1000 times more resources allocated to it than a priority 1 queue.
> Great,
> >>>> but not what I want. I want to be able to define an Ordering on make
> my
> >>>> tasks representing their priority, and have Spark allocate all
> resources to
> >>>> the job that has the highest priority.
> >>>>
> >>>> Alex
> >>>>
> >>>> On Sat, Jan 10, 2015 at 10:11 PM, Cody Koeninger <cody@koeninger.org>
> >>>> wrote:
> >>>>
> >>>>>
> >>>>>
> http://spark.apache.org/docs/latest/job-scheduling.html#configuring-pool-properties
> >>>>>
> >>>>> "Setting a high weight such as 1000 also makes it possible to
> >>>>> implement *priority* between pools--in essence, the weight-1000 pool
> >>>>> will always get to launch tasks first whenever it has jobs active."
> >>>>>
> >>>>> On Sat, Jan 10, 2015 at 11:57 PM, Alessandro Baretta <
> >>>>> alexbaretta@gmail.com> wrote:
> >>>>>
> >>>>>> Mark,
> >>>>>>
> >>>>>> Thanks, but I don't see how this documentation solves my problem.
> You
> >>>>>> are referring me to documentation of fair scheduling; whereas, I am
> asking
> >>>>>> about as unfair a scheduling policy as can be: a priority queue.
> >>>>>>
> >>>>>> Alex
> >>>>>>
> >>>>>> On Sat, Jan 10, 2015 at 5:00 PM, Mark Hamstra <
> mark@clearstorydata.com
> >>>>>> > wrote:
> >>>>>>
> >>>>>>> -dev, +user
> >>>>>>>
> >>>>>>> http://spark.apache.org/docs/latest/job-scheduling.html
> >>>>>>>
> >>>>>>>
> >>>>>>> On Sat, Jan 10, 2015 at 4:40 PM, Alessandro Baretta <
> >>>>>>> alexbaretta@gmail.com> wrote:
> >>>>>>>
> >>>>>>>> Is it possible to specify a priority level for a job, such that
> the
> >>>>>>>> active
> >>>>>>>> jobs might be scheduled in order of priority?
> >>>>>>>>
> >>>>>>>> Alex
> >>>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>
>

--001a113d4a96824053050c6d0bd7--

From dev-return-11089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 05:28:57 2015
Return-Path: <dev-return-11089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 544B717A54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 05:28:57 +0000 (UTC)
Received: (qmail 6242 invoked by uid 500); 12 Jan 2015 05:28:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6169 invoked by uid 500); 12 Jan 2015 05:28:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4853 invoked by uid 99); 12 Jan 2015 05:28:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 05:28:50 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 05:28:46 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Mon, 12 Jan
 2015 00:28:24 -0500
Message-ID: <54B35B6E.1000606@flytxt.com>
Date: Mon, 12 Jan 2015 10:58:14 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Davies Liu <davies@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Python to Java object conversion of numpy array
References: <54AFCD68.9010908@flytxt.com> <CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
In-Reply-To: <CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
Content-Type: multipart/alternative;
	boundary="------------050803090508070500020903"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------050803090508070500020903
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi,
Thanks Davies .

I added a new class GaussianMixtureModel in clustering.py and the method 
predict in it and trying to pass numpy array from this method.I 
converted it to DenseVector and its solved now.

Similarly I tried passing a List  of more than one dimension to the 
function _py2java , but now the exception is

'list' object has no attribute '_get_object_id'

and when I give a tuple input (Vectors.dense([0.8786, 
-0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like

'numpy.ndarray' object has no attribute '_get_object_id'

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_

On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
> Hey Meethu,
>
> The Java API accepts only Vector, so you should convert the numpy array into
> pyspark.mllib.linalg.DenseVector.
>
> BTW, which class are you using? the KMeansModel.predict() accept numpy.array,
> it will do the conversion for you.
>
> Davies
>
> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
>> Hi,
>> I am trying to send a numpy array as an argument to a function predict() in
>> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
>> function callMLlibFunc(name, *args)  in
>> spark/python/pyspark/mllib/common.py.
>>
>> Now the value is passed to the function  _py2java(sc, obj) .Here I am
>> getting an exception
>>
>> Py4JJavaError: An error occurred while calling
>> z:org.apache.spark.mllib.api.python.SerDe.loads.
>> : net.razorvine.pickle.PickleException: expected zero arguments for
>> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>>          at
>> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>>          at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>>          at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>>          at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>>          at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>>
>>
>> Why common._py2java(sc, obj) is not handling numpy array type?
>>
>> Please help..
>>
>>
>> --
>>
>> Regards,
>>
>> *Meethu Mathew*
>>
>> *Engineer*
>>
>> *Flytxt*
>>
>> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
>> <http://www.twitter.com/flytxt> | _Connect on Linkedin
>> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>>


--------------050803090508070500020903--

From dev-return-11090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 06:17:38 2015
Return-Path: <dev-return-11090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE45F17B3D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 06:17:38 +0000 (UTC)
Received: (qmail 65201 invoked by uid 500); 12 Jan 2015 06:17:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65124 invoked by uid 500); 12 Jan 2015 06:17:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65108 invoked by uid 99); 12 Jan 2015 06:17:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 06:17:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.54] (HELO mail-la0-f54.google.com) (209.85.215.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 06:17:13 +0000
Received: by mail-la0-f54.google.com with SMTP id pv20so22373255lab.13
        for <dev@spark.apache.org>; Sun, 11 Jan 2015 22:16:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=48ZISCkk9fLNWgNRGj1p21Ud7EoEAOMJnQe08Y01dC0=;
        b=Y50rSlL2jAz9G8ufeM93V5DtyMLU2y2As9q82dEjn59TJe+Io0TnKSshlla4ME0pSM
         ORWV0BzDcnKkoErl26IZrWlqE4eU8rsf9DmG5Z8ttOpNUI0qwnqhu3LNOYdGid+KKfdA
         baQMVfDhFYtmuzq0BuVaWCqrbyx/EHxySMIAyEAL7+GoUtVu5NSqlP2dUaPsphtIBAZS
         cXG3XQhSvZHBcA/F6zBRNjqC8HbplJp9VvCG9oEAbbuIJ3b3Xd2aFsIjfUsAetvQc1Gc
         /jacpD++bFO1wQUNS4BSwUYbo+/Ml+wbpFqaV055ZAGLglKJK30/2td7e5G5IbuleMQ3
         lQBg==
X-Gm-Message-State: ALoCoQnxYOPs3l/VoqpxhNHphz4MPwKpdRBjLtdR8qn75K68wKv0ft5N8OmsA1/fbl9SR3+erq4X
MIME-Version: 1.0
X-Received: by 10.152.120.97 with SMTP id lb1mr34852091lab.76.1421043366128;
 Sun, 11 Jan 2015 22:16:06 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Sun, 11 Jan 2015 22:16:06 -0800 (PST)
In-Reply-To: <54B35B6E.1000606@flytxt.com>
References: <54AFCD68.9010908@flytxt.com>
	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
	<54B35B6E.1000606@flytxt.com>
Date: Sun, 11 Jan 2015 22:16:06 -0800
Message-ID: <CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
Subject: Re: Python to Java object conversion of numpy array
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Could you post a piece of code here?

On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
> Hi,
> Thanks Davies .
>
> I added a new class GaussianMixtureModel in clustering.py and the method
> predict in it and trying to pass numpy array from this method.I converted it
> to DenseVector and its solved now.
>
> Similarly I tried passing a List  of more than one dimension to the function
> _py2java , but now the exception is
>
> 'list' object has no attribute '_get_object_id'
>
> and when I give a tuple input (Vectors.dense([0.8786,
> -0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like
>
> 'numpy.ndarray' object has no attribute '_get_object_id'
>
> Regards,
>
>
>
> Meethu Mathew
>
> Engineer
>
> Flytxt
>
> www.flytxt.com | Visit our blog  |  Follow us | Connect on Linkedin
>
>
>
> On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
>
> Hey Meethu,
>
> The Java API accepts only Vector, so you should convert the numpy array into
> pyspark.mllib.linalg.DenseVector.
>
> BTW, which class are you using? the KMeansModel.predict() accept
> numpy.array,
> it will do the conversion for you.
>
> Davies
>
> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com>
> wrote:
>
> Hi,
> I am trying to send a numpy array as an argument to a function predict() in
> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
> function callMLlibFunc(name, *args)  in
> spark/python/pyspark/mllib/common.py.
>
> Now the value is passed to the function  _py2java(sc, obj) .Here I am
> getting an exception
>
> Py4JJavaError: An error occurred while calling
> z:org.apache.spark.mllib.api.python.SerDe.loads.
> : net.razorvine.pickle.PickleException: expected zero arguments for
> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>         at
> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>         at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>         at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>         at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>         at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>
>
> Why common._py2java(sc, obj) is not handling numpy array type?
>
> Please help..
>
>
> --
>
> Regards,
>
> *Meethu Mathew*
>
> *Engineer*
>
> *Flytxt*
>
> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
> <http://www.twitter.com/flytxt> | _Connect on Linkedin
> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 06:22:59 2015
Return-Path: <dev-return-11091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4799E17B4B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 06:22:59 +0000 (UTC)
Received: (qmail 72088 invoked by uid 500); 12 Jan 2015 06:23:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72012 invoked by uid 500); 12 Jan 2015 06:22:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72001 invoked by uid 99); 12 Jan 2015 06:22:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 06:22:59 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 06:22:55 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Mon, 12 Jan
 2015 01:21:29 -0500
Message-ID: <54B367DF.6000807@flytxt.com>
Date: Mon, 12 Jan 2015 11:51:19 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Davies Liu <davies@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Python to Java object conversion of numpy array
References: <54AFCD68.9010908@flytxt.com>	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>	<54B35B6E.1000606@flytxt.com> <CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
In-Reply-To: <CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
Content-Type: multipart/alternative;
	boundary="------------080009070009030604020404"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------080009070009030604020404
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi,

This is the code I am running.

mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))

membershipMatrix = callMLlibFunc("findPredict", 
rdd.map(_convert_to_vector), mu)

Regards,
Meethu
On Monday 12 January 2015 11:46 AM, Davies Liu wrote:
> Could you post a piece of code here?
>
> On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
>> Hi,
>> Thanks Davies .
>>
>> I added a new class GaussianMixtureModel in clustering.py and the method
>> predict in it and trying to pass numpy array from this method.I converted it
>> to DenseVector and its solved now.
>>
>> Similarly I tried passing a List  of more than one dimension to the function
>> _py2java , but now the exception is
>>
>> 'list' object has no attribute '_get_object_id'
>>
>> and when I give a tuple input (Vectors.dense([0.8786,
>> -0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like
>>
>> 'numpy.ndarray' object has no attribute '_get_object_id'
>>
>> Regards,
>>
>>
>>
>> Meethu Mathew
>>
>> Engineer
>>
>> Flytxt
>>
>> www.flytxt.com | Visit our blog  |  Follow us | Connect on Linkedin
>>
>>
>>
>> On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
>>
>> Hey Meethu,
>>
>> The Java API accepts only Vector, so you should convert the numpy array into
>> pyspark.mllib.linalg.DenseVector.
>>
>> BTW, which class are you using? the KMeansModel.predict() accept
>> numpy.array,
>> it will do the conversion for you.
>>
>> Davies
>>
>> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com>
>> wrote:
>>
>> Hi,
>> I am trying to send a numpy array as an argument to a function predict() in
>> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
>> function callMLlibFunc(name, *args)  in
>> spark/python/pyspark/mllib/common.py.
>>
>> Now the value is passed to the function  _py2java(sc, obj) .Here I am
>> getting an exception
>>
>> Py4JJavaError: An error occurred while calling
>> z:org.apache.spark.mllib.api.python.SerDe.loads.
>> : net.razorvine.pickle.PickleException: expected zero arguments for
>> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>>          at
>> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>>          at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>>          at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>>          at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>>          at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>>
>>
>> Why common._py2java(sc, obj) is not handling numpy array type?
>>
>> Please help..
>>
>>
>> --
>>
>> Regards,
>>
>> *Meethu Mathew*
>>
>> *Engineer*
>>
>> *Flytxt*
>>
>> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
>> <http://www.twitter.com/flytxt> | _Connect on Linkedin
>> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>>
>>


--------------080009070009030604020404--

From dev-return-11092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 13:35:34 2015
Return-Path: <dev-return-11092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72EFE1073B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 13:35:34 +0000 (UTC)
Received: (qmail 33223 invoked by uid 500); 12 Jan 2015 13:35:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33149 invoked by uid 500); 12 Jan 2015 13:35:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33066 invoked by uid 99); 12 Jan 2015 13:35:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 13:35:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 13:35:28 +0000
Received: by mail-qa0-f44.google.com with SMTP id w8so7585553qac.3
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 05:35:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=uUvWnI2+Hgvx8MXQmqMJoqpsKVNtnPqlGLiaJRTPxBQ=;
        b=KLZDtfrU6votXJSRfLiZ48H/mKIFWfhJLIsSAk+4Z3WW84Za7w87vpVvVcMAQduzCx
         GiBsJW20hYqKLSp8q2yR1WB/DJjLe95IGhTZsDCDbaOVuB4o6DL9KHEnGnNlasE2P+ny
         ZRtMWid3gG0PsNWagO4vPdsnsOxrlprdbFchmZhT5jAGocZPkBZS/mXka910OL4zz2aJ
         r+IaO8aLpfxVlxVURIU5Ak2LDo/NDX9GcCGORZnb6g0TwpVyHh2qDmExkhZayM7YxO9B
         6qnADmgW+wjGz7YPP+KSPXrezY+/nhUvO8S2SCegEVetZAh9B+wXq0bcYArUSaiZPhSQ
         YpvQ==
X-Received: by 10.229.92.196 with SMTP id s4mr28452464qcm.25.1421069708150;
 Mon, 12 Jan 2015 05:35:08 -0800 (PST)
MIME-Version: 1.0
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Date: Mon, 12 Jan 2015 13:35:07 +0000
Message-ID: <CAJOb8bu3HmGtyAaXqg8q9VHASa+ytL0AHefNXNgE4KQmqsgWJA@mail.gmail.com>
Subject: Discussion | SparkContext 's setJobGroup and clearJobGroup should
 return a new instance of SparkContext
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133141427cda4050c749332
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133141427cda4050c749332
Content-Type: text/plain; charset=UTF-8

Hi spark committers

I would like to discuss the possibility of changing the signature
of SparkContext 's setJobGroup and clearJobGroup functions to return a
replica of SparkContext with the job group set/unset instead of mutating
the original context. I am building a spark job server and I am assigning
job groups before passing control to user provided logic that uses spark
context to define and execute a job (very much like job-server). The issue
is that I can't reliably know when to clear the job group as user defined
code can use futures to submit multiple tasks in parallel. In fact, I am
even allowing users to return a future from their function on which spark
server can register callbacks to know when the user defined job is
complete. Now, if I set the job group before passing control to user
function and wait on future to complete so that I can clear the job group,
I can no longer use that SparkContext for any other job. This means I will
have to lock on the SparkContext which seems like a bad idea. Therefore, my
proposal would be to return new instance of SparkContext (a replica with
just job group set/unset) that can further be used in concurrent
environment safely. I am also happy mutating the original SparkContext just
not break backward compatibility as long as the returned SparkContext is
not affected by set/unset of job groups on original SparkContext.

Thoughts please?

Thanks,
Aniket

--001a1133141427cda4050c749332--

From dev-return-11093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 13:38:45 2015
Return-Path: <dev-return-11093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39B991077E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 13:38:45 +0000 (UTC)
Received: (qmail 38775 invoked by uid 500); 12 Jan 2015 13:38:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38701 invoked by uid 500); 12 Jan 2015 13:38:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38689 invoked by uid 99); 12 Jan 2015 13:38:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 13:38:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 13:38:19 +0000
Received: by mail-qg0-f41.google.com with SMTP id e89so17464470qgf.0
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 05:38:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=9LYCeCBYjY/UNw90WB+FdUi0UpaEG1/5qELUilovifU=;
        b=AmozZQunVOImYL2G6yDnkAKKEr0SWqAeVGGImfaRUsaAh/+bsqvHrxE6KvFHktUeHZ
         fw0ar1LdIAolQH7+B5dnCYXjYpvDngr4ZuSeeitcDZFvQtPHv6B0SiQPnWG8H+WzCnV8
         b8si6jhaskmDANXpb3KR6u3FWP0O6ooit/cp9da+dcdZPSeqfCtM0GmIvGe0Ocm1rBDu
         t90+zlmq8KCiZgPlDPEZ3uVWwYzm0R1GnjhqdFDOP+gV1lsVpjveHUVGFKxc29NMjzVE
         wbiPzokVHNGuQMzcQplE6Cme+bYc52MyvgJ3dWYAQEYOCH+VvwqC1hgiT0IFhrgibTMr
         xPEg==
X-Received: by 10.229.211.193 with SMTP id gp1mr49201621qcb.19.1421069898081;
 Mon, 12 Jan 2015 05:38:18 -0800 (PST)
MIME-Version: 1.0
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Date: Mon, 12 Jan 2015 13:38:17 +0000
Message-ID: <CAJOb8bthhaN5Wa=4KXX2hoWRTf0ofxPYL_FvUD_UXbhTd+kaDg@mail.gmail.com>
Subject: YARN | SPARK-5164 | Submitting jobs from windows to linux YARN
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134401c79fce6050c749eb8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134401c79fce6050c749eb8
Content-Type: text/plain; charset=UTF-8

Hi Spark YARN maintainers

Can anyone please look and comment on SPARK-5164? Basically, this stops
users from submitting jobs (or using spark shell) from a windows machine to
a a YARN cluster running on linux. I should be able to submit a pull
request for this provided the community agrees. This would be a great help
for windows users (like me).

Thanks,
Aniket

--001a1134401c79fce6050c749eb8--

From dev-return-11094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 14:12:39 2015
Return-Path: <dev-return-11094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E83010938
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 14:12:39 +0000 (UTC)
Received: (qmail 13959 invoked by uid 500); 12 Jan 2015 14:12:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13865 invoked by uid 500); 12 Jan 2015 14:12:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13851 invoked by uid 99); 12 Jan 2015 14:12:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 14:12:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of toth.zsolt.bme@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 14:12:33 +0000
Received: by mail-oi0-f42.google.com with SMTP id g201so2407472oib.1
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 06:12:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=dI/aCxBVyr9wS4TRrMnM4HKlwojZBpR9fClrUCP8pwQ=;
        b=Wso09wFnD8lySWA2I6K9p6eNHqjoSLBKEfPHcS/FFCwM1PYBtgX01fahG82W2ru1EL
         oh3lLXMGBRON70CEOeCX8/awbLYITn/IBAYSVSJi7wpX16PclQvYwg80/3CIzlYIimkf
         CiENETSJlvxBHO+m/K2W05WqInX7Og/wEy5g+DohgbrQyL5UbPtRqD8WLd1bP7Co3vzc
         J1iU5WR0d2GjXPRPC9mLmcioLxjS1dUE57stSS/pp6H+2x6a93g3y1opRDz8b565MRnl
         E3m0grp7X+2zU+rejkNR9y9Bu6N1Zwz3CTynkk+hmkDy1WsbRqnwxUCTugVlfPIHcP0Z
         +HKw==
MIME-Version: 1.0
X-Received: by 10.60.83.235 with SMTP id t11mr17341566oey.40.1421071933234;
 Mon, 12 Jan 2015 06:12:13 -0800 (PST)
Received: by 10.76.87.228 with HTTP; Mon, 12 Jan 2015 06:12:13 -0800 (PST)
In-Reply-To: <CAJOb8bthhaN5Wa=4KXX2hoWRTf0ofxPYL_FvUD_UXbhTd+kaDg@mail.gmail.com>
References: <CAJOb8bthhaN5Wa=4KXX2hoWRTf0ofxPYL_FvUD_UXbhTd+kaDg@mail.gmail.com>
Date: Mon, 12 Jan 2015 15:12:13 +0100
Message-ID: <CABVU3FFLF8CiS9j6WeqkaOQtL6G93WvZNGWJux=wAw2HjJFouA@mail.gmail.com>
Subject: Re: YARN | SPARK-5164 | Submitting jobs from windows to linux YARN
From: =?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>
To: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01182c88c7e69f050c7517e9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182c88c7e69f050c7517e9
Content-Type: text/plain; charset=UTF-8

Hi Aniket,

I think this is a duplicate of SPARK-1825, isn't it?

Zsolt

2015-01-12 14:38 GMT+01:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:

> Hi Spark YARN maintainers
>
> Can anyone please look and comment on SPARK-5164? Basically, this stops
> users from submitting jobs (or using spark shell) from a windows machine to
> a a YARN cluster running on linux. I should be able to submit a pull
> request for this provided the community agrees. This would be a great help
> for windows users (like me).
>
> Thanks,
> Aniket
>

--089e01182c88c7e69f050c7517e9--

From dev-return-11095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 14:21:53 2015
Return-Path: <dev-return-11095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A41110992
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 14:21:53 +0000 (UTC)
Received: (qmail 32805 invoked by uid 500); 12 Jan 2015 14:21:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32731 invoked by uid 500); 12 Jan 2015 14:21:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32714 invoked by uid 99); 12 Jan 2015 14:21:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 14:21:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 14:21:46 +0000
Received: by mail-qa0-f41.google.com with SMTP id bm13so7750846qab.0
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 06:19:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=c61CHARquMzR8zs+3BM2gZ9FCyY9q2EHvxyWN+BetUs=;
        b=tx4iIfwKJckhD4IpbOjh9wgioiFF4OcAhtVbRHHMULqmo1OpV7NnL5EsWbKh4KfQKt
         3UlDSSh1tIMoZuBCv23mcoXYgDI3TboS6jvOHbN8i4Htauk3LyIAh5GUn/I6HRCO441G
         cEaFqsnq/ShmfX7/9dtdUIzxVj+57Zhcaj6VIBI0bA9YlLGvyiMA+NE43d6Ux5WmfPGH
         7hzd7pjCwVuLkGLs5+JmqpFfFZnQUzKfoSIKeMmn05WLAvN4kUCLoT0V9JJRqV7/VTDv
         fBbPv0aS4xm8ajyquJpJ4/RvkLOXCFt2Oo67m1AdSE86v7bzwlZRIeSADYx8/+nPpKbt
         fnFw==
X-Received: by 10.224.34.137 with SMTP id l9mr9010004qad.57.1421072396168;
 Mon, 12 Jan 2015 06:19:56 -0800 (PST)
MIME-Version: 1.0
References: <CAJOb8bthhaN5Wa=4KXX2hoWRTf0ofxPYL_FvUD_UXbhTd+kaDg@mail.gmail.com>
 <CABVU3FFLF8CiS9j6WeqkaOQtL6G93WvZNGWJux=wAw2HjJFouA@mail.gmail.com>
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Date: Mon, 12 Jan 2015 14:19:55 +0000
Message-ID: <CAJOb8btf+89hyc+D3Uvjgpxd9-xUBpoTy7-K6eHOqgArZdb+xw@mail.gmail.com>
Subject: Re: YARN | SPARK-5164 | Submitting jobs from windows to linux YARN
To: =?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2952a5fb5c4050c753364
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2952a5fb5c4050c753364
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Ohh right. It is. I will mark my defect as duplicate and cross check my
notes with the fixes in the pull request. Thanks for pointing out Zsolt :)

On Mon, Jan 12, 2015, 7:42 PM Zsolt T=C3=B3th <toth.zsolt.bme@gmail.com> wr=
ote:

> Hi Aniket,
>
> I think this is a duplicate of SPARK-1825, isn't it?
>
> Zsolt
>
> 2015-01-12 14:38 GMT+01:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:
>
>> Hi Spark YARN maintainers
>>
>> Can anyone please look and comment on SPARK-5164? Basically, this stops
>> users from submitting jobs (or using spark shell) from a windows machine
>> to
>> a a YARN cluster running on linux. I should be able to submit a pull
>> request for this provided the community agrees. This would be a great he=
lp
>> for windows users (like me).
>>
>> Thanks,
>> Aniket
>>
>
>

--001a11c2952a5fb5c4050c753364--

From dev-return-11096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 16:38:48 2015
Return-Path: <dev-return-11096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF42710EED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 16:38:48 +0000 (UTC)
Received: (qmail 52227 invoked by uid 500); 12 Jan 2015 16:38:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52157 invoked by uid 500); 12 Jan 2015 16:38:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52146 invoked by uid 99); 12 Jan 2015 16:38:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 16:38:48 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of etander@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 16:38:23 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 99D671045C6B
	for <dev@spark.apache.org>; Mon, 12 Jan 2015 08:38:22 -0800 (PST)
Date: Mon, 12 Jan 2015 09:38:21 -0700 (MST)
From: preeze <etander@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421080701472-10088.post@n3.nabble.com>
Subject: Apache Spark client high availability
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Dear community,

I've been searching the internet for quite a while to find out what is the
best architecture to support HA for a spark client.

We run an application that connects to a standalone Spark cluster and caches
a big chuck of data for subsequent intensive calculations. To achieve HA
we'll need to run several instances of the application on different hosts.

Initially I explored the option to reuse (i.e. share) the same executors set
between SparkContext instances of all running applications. Found it
impossible.

So, every application, which creates an instance of SparkContext, has to
spawn its own executors. Externalizing and sharing executors' memory cache
with Tachyon is a semi-solution since each application's executors will keep
using their own set of CPU cores.

Spark-jobserver is another possibility. It manages SparkContext itself and
accepts job requests from multiple clients for the same context which is
brilliant. However, this becomes a new single point of failure.

Now I am exploring if it's possible to run the Spark cluster in YARN cluster
mode and connect to the driver from multiple clients.

Is there anything I am missing guys?
Any suggestion is highly appreciated!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-client-high-availability-tp10088.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 16:54:20 2015
Return-Path: <dev-return-11097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7784B10FF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 16:54:20 +0000 (UTC)
Received: (qmail 95397 invoked by uid 500); 12 Jan 2015 16:54:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95317 invoked by uid 500); 12 Jan 2015 16:54:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95306 invoked by uid 99); 12 Jan 2015 16:54:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 16:54:19 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 16:54:15 +0000
Received: by mail-lb0-f174.google.com with SMTP id 10so19214785lbg.5
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 08:52:49 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=tzgaNsbruNtYoIAfsLj5i9ihTY+KNlsKE3nkw8wbrV8=;
        b=lLwfQvghzcHWyyP2HmC1QQf+A7/9aTg609JAO0MWxani1FwwZUGP5pgiZP44v2WnkO
         10UBsao7Utjr0LaXEK27qRZfIgwUkDAXRGzoUuKoKbvukixFpWwXQFKKEXYu8c7EDKMP
         ecJxoKXsbXBe13ppl1Aa0xCGZBxlWkihBLw0Exn18b7ZEgoEmNTv/eVtP/1PCUqm7ENr
         su+hgcaKPipaT9mciAAVlRaT5LQ+MJSbpBiCXfskRkMqVHKWw2rqt8rCnWdYQQ6cL3SU
         OP6z4AqhZNDrB2j0LtHsOyI6BPg2J2PClX2JdD2I2PjJs/m7ZJ9QrUHYtVR5r/IEnirF
         sghQ==
X-Gm-Message-State: ALoCoQlP9mk8AI2v/RZVC3b7XCuKPF+5jkcjDLXdZ35pCXJoAudvOkBLweUg8ZvuEGGh/PWcSeet
MIME-Version: 1.0
X-Received: by 10.112.37.161 with SMTP id z1mr6438509lbj.87.1421081569248;
 Mon, 12 Jan 2015 08:52:49 -0800 (PST)
Received: by 10.152.114.10 with HTTP; Mon, 12 Jan 2015 08:52:49 -0800 (PST)
In-Reply-To: <1421080701472-10088.post@n3.nabble.com>
References: <1421080701472-10088.post@n3.nabble.com>
Date: Mon, 12 Jan 2015 22:22:49 +0530
Message-ID: <CAHUQ+_Z9Bd9=MO2D+V_f4X4WUeFhKs5TeSkKDZsdD_6zqR2PCw@mail.gmail.com>
Subject: Re: Apache Spark client high availability
From: Akhil Das <akhil@sigmoidanalytics.com>
To: preeze <etander@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134809421d64c050c775660
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134809421d64c050c775660
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

We usually run Spark in HA with the following stack:

-> Apache Mesos
-> Marathon - init/control system for starting, stopping, and maintaining
always-on applications.(Mainly SparkStreaming)
-> Chronos - general-purpose scheduler for Mesos, supports job dependency
graphs.
-> Spark Job Server - primarily for it's ability to reuse shared contexts
with multiple jobs

=E2=80=8BThis thread has a better discussion
http://apache-spark-user-list.1001560.n3.nabble.com/How-do-you-run-your-spa=
rk-app-td7935.html
=E2=80=8B


Thanks
Best Regards

On Mon, Jan 12, 2015 at 10:08 PM, preeze <etander@gmail.com> wrote:

> Dear community,
>
> I've been searching the internet for quite a while to find out what is th=
e
> best architecture to support HA for a spark client.
>
> We run an application that connects to a standalone Spark cluster and
> caches
> a big chuck of data for subsequent intensive calculations. To achieve HA
> we'll need to run several instances of the application on different hosts=
.
>
> Initially I explored the option to reuse (i.e. share) the same executors
> set
> between SparkContext instances of all running applications. Found it
> impossible.
>
> So, every application, which creates an instance of SparkContext, has to
> spawn its own executors. Externalizing and sharing executors' memory cach=
e
> with Tachyon is a semi-solution since each application's executors will
> keep
> using their own set of CPU cores.
>
> Spark-jobserver is another possibility. It manages SparkContext itself an=
d
> accepts job requests from multiple clients for the same context which is
> brilliant. However, this becomes a new single point of failure.
>
> Now I am exploring if it's possible to run the Spark cluster in YARN
> cluster
> mode and connect to the driver from multiple clients.
>
> Is there anything I am missing guys?
> Any suggestion is highly appreciated!
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-cl=
ient-high-availability-tp10088.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1134809421d64c050c775660--

From dev-return-11098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 17:15:21 2015
Return-Path: <dev-return-11098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7BECE17320
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 17:15:21 +0000 (UTC)
Received: (qmail 51905 invoked by uid 500); 12 Jan 2015 17:15:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51827 invoked by uid 500); 12 Jan 2015 17:15:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51816 invoked by uid 99); 12 Jan 2015 17:15:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 17:15:20 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of eerlands@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 17:14:54 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id t0CHCpnd008574;
	Mon, 12 Jan 2015 12:12:51 -0500
Date: Mon, 12 Jan 2015 12:12:51 -0500 (EST)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Cc: dev@spark.apache.org
Message-ID: <170732568.7568028.1421082771188.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAJOb8bu3HmGtyAaXqg8q9VHASa+ytL0AHefNXNgE4KQmqsgWJA@mail.gmail.com>
References: <CAJOb8bu3HmGtyAaXqg8q9VHASa+ytL0AHefNXNgE4KQmqsgWJA@mail.gmail.com>
Subject: Re: Discussion | SparkContext 's setJobGroup and clearJobGroup
 should return a new instance of SparkContext
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.6]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC38 (Linux)/8.0.6_GA_5922)
Thread-Topic: Discussion | SparkContext 's setJobGroup and clearJobGroup should return a new instance of SparkContext
Thread-Index: wGAA7+VUf/66e+eICyLZ2Z2mECg3fw==
X-Virus-Checked: Checked by ClamAV on apache.org

setJobGroup needs fixing:
https://issues.apache.org/jira/browse/SPARK-4514

I'm interested in any community input on what the semantics or design "ought" to be changed to.


----- Original Message -----
> Hi spark committers
> 
> I would like to discuss the possibility of changing the signature
> of SparkContext 's setJobGroup and clearJobGroup functions to return a
> replica of SparkContext with the job group set/unset instead of mutating
> the original context. I am building a spark job server and I am assigning
> job groups before passing control to user provided logic that uses spark
> context to define and execute a job (very much like job-server). The issue
> is that I can't reliably know when to clear the job group as user defined
> code can use futures to submit multiple tasks in parallel. In fact, I am
> even allowing users to return a future from their function on which spark
> server can register callbacks to know when the user defined job is
> complete. Now, if I set the job group before passing control to user
> function and wait on future to complete so that I can clear the job group,
> I can no longer use that SparkContext for any other job. This means I will
> have to lock on the SparkContext which seems like a bad idea. Therefore, my
> proposal would be to return new instance of SparkContext (a replica with
> just job group set/unset) that can further be used in concurrent
> environment safely. I am also happy mutating the original SparkContext just
> not break backward compatibility as long as the returned SparkContext is
> not affected by set/unset of job groups on original SparkContext.
> 
> Thoughts please?
> 
> Thanks,
> Aniket
> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 18:05:49 2015
Return-Path: <dev-return-11099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0FC417563
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 18:05:49 +0000 (UTC)
Received: (qmail 88837 invoked by uid 500); 12 Jan 2015 18:05:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88762 invoked by uid 500); 12 Jan 2015 18:05:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88751 invoked by uid 99); 12 Jan 2015 18:05:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 18:05:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 18:05:46 +0000
Received: by mail-lb0-f180.google.com with SMTP id l4so18963435lbv.11
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 10:05:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=KZPHlkDcFZpoEQn7rxfgYyyBVEvQ1eRH5VcqhJQ0Rlg=;
        b=lcp32uOtu+f5MFFqx2JICtKd6Miu8EcgZP6OFNYI0DrdOTidW07h7YKGBY1ZlFC/1O
         zy9/tPv3zd7aFt5qMubFpjTdvYfxstDXkuY92TvZFx/zYKISDP82bXZ6jkI9hLZg5HWg
         lV23TDTqsSoMgwdFDL1wAYTS4z/eqljNLUBPXUTDN9PuX67q3Lv7yGpKFieq8lvt04A3
         8bbqIe/MijE1rE2Pm66Ygwv8I3nj7QjXmCnOJgTe9FULGbXgEIcRT0+6aQXu3zVu1Tet
         CD9ohqw7f1zeGQDP3bWgxyGTo+1YTKaAsvNQs6OoEUTECiMImdO8H901Hf257+C148+c
         MBBw==
X-Gm-Message-State: ALoCoQlWQ7crPN9Ru7o8BFbBJxU/w8UWKin9lFt3oow3czFANje6y9oa+75Qzg799e9DTgNjI8Uy
MIME-Version: 1.0
X-Received: by 10.152.206.108 with SMTP id ln12mr37909116lac.3.1421085903847;
 Mon, 12 Jan 2015 10:05:03 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Mon, 12 Jan 2015 10:05:03 -0800 (PST)
In-Reply-To: <54B367DF.6000807@flytxt.com>
References: <54AFCD68.9010908@flytxt.com>
	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
	<54B35B6E.1000606@flytxt.com>
	<CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
	<54B367DF.6000807@flytxt.com>
Date: Mon, 12 Jan 2015 10:05:03 -0800
Message-ID: <CA+2Pv=gUaWx4e=UWZ53DxBBzC7eqGD3JyJgOZ6YiKC1YLOAd2Q@mail.gmail.com>
Subject: Re: Python to Java object conversion of numpy array
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Sun, Jan 11, 2015 at 10:21 PM, Meethu Mathew
<meethu.mathew@flytxt.com> wrote:
> Hi,
>
> This is the code I am running.
>
> mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))
>
> membershipMatrix = callMLlibFunc("findPredict", rdd.map(_convert_to_vector),
> mu)

What's the Java API looks like? all the arguments of findPredict
should be converted
into java objects, so what should `mu` be converted to?

> Regards,
> Meethu
> On Monday 12 January 2015 11:46 AM, Davies Liu wrote:
>
> Could you post a piece of code here?
>
> On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew <meethu.mathew@flytxt.com>
> wrote:
>
> Hi,
> Thanks Davies .
>
> I added a new class GaussianMixtureModel in clustering.py and the method
> predict in it and trying to pass numpy array from this method.I converted it
> to DenseVector and its solved now.
>
> Similarly I tried passing a List  of more than one dimension to the function
> _py2java , but now the exception is
>
> 'list' object has no attribute '_get_object_id'
>
> and when I give a tuple input (Vectors.dense([0.8786,
> -0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like
>
> 'numpy.ndarray' object has no attribute '_get_object_id'
>
> Regards,
>
>
>
> Meethu Mathew
>
> Engineer
>
> Flytxt
>
> www.flytxt.com | Visit our blog  |  Follow us | Connect on Linkedin
>
>
>
> On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
>
> Hey Meethu,
>
> The Java API accepts only Vector, so you should convert the numpy array into
> pyspark.mllib.linalg.DenseVector.
>
> BTW, which class are you using? the KMeansModel.predict() accept
> numpy.array,
> it will do the conversion for you.
>
> Davies
>
> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com>
> wrote:
>
> Hi,
> I am trying to send a numpy array as an argument to a function predict() in
> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
> function callMLlibFunc(name, *args)  in
> spark/python/pyspark/mllib/common.py.
>
> Now the value is passed to the function  _py2java(sc, obj) .Here I am
> getting an exception
>
> Py4JJavaError: An error occurred while calling
> z:org.apache.spark.mllib.api.python.SerDe.loads.
> : net.razorvine.pickle.PickleException: expected zero arguments for
> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>         at
> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>         at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>         at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>         at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>         at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>
>
> Why common._py2java(sc, obj) is not handling numpy array type?
>
> Please help..
>
>
> --
>
> Regards,
>
> *Meethu Mathew*
>
> *Engineer*
>
> *Flytxt*
>
> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
> <http://www.twitter.com/flytxt> | _Connect on Linkedin
> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 12 19:20:35 2015
Return-Path: <dev-return-11100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 04E22179C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 19:20:35 +0000 (UTC)
Received: (qmail 58983 invoked by uid 500); 12 Jan 2015 19:20:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58901 invoked by uid 500); 12 Jan 2015 19:20:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58890 invoked by uid 99); 12 Jan 2015 19:20:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 19:20:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ogeagla@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 19:20:27 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 3DE2D1049E4C
	for <dev@spark.apache.org>; Mon, 12 Jan 2015 11:19:37 -0800 (PST)
Date: Mon, 12 Jan 2015 12:19:36 -0700 (MST)
From: Octavian Geagla <ogeagla@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421090376007-10092.post@n3.nabble.com>
In-Reply-To: <CAJgQjQ-hJJP65YXjGj7a2FHLae8i9c3x+w1e5Sg1S4jS9Dv2yA@mail.gmail.com>
References: <1420834912389-10073.post@n3.nabble.com> <CAJgQjQ-hJJP65YXjGj7a2FHLae8i9c3x+w1e5Sg1S4jS9Dv2yA@mail.gmail.com>
Subject: Re: Re-use scaling means and variances from StandardScalerModel
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for the suggestions.  

I've opened this JIRA ticket:
https://issues.apache.org/jira/browse/SPARK-5207
Feel free to modify it, assign it to me, kick off a discussion, etc.  

I'd be more than happy to own this feature and PR.

Thanks,
-Octavian



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-use-scaling-means-and-variances-from-StandardScalerModel-tp10073p10092.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 04:15:45 2015
Return-Path: <dev-return-11101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC27717362
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 04:15:45 +0000 (UTC)
Received: (qmail 22569 invoked by uid 500); 13 Jan 2015 04:15:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22494 invoked by uid 500); 13 Jan 2015 04:15:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22483 invoked by uid 99); 13 Jan 2015 04:15:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 04:15:44 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 04:15:39 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Mon, 12 Jan
 2015 23:14:56 -0500
Message-ID: <54B49BB7.40703@flytxt.com>
Date: Tue, 13 Jan 2015 09:44:47 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Davies Liu <davies@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Python to Java object conversion of numpy array
References: <54AFCD68.9010908@flytxt.com>	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>	<54B35B6E.1000606@flytxt.com>	<CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>	<54B367DF.6000807@flytxt.com> <CA+2Pv=gUaWx4e=UWZ53DxBBzC7eqGD3JyJgOZ6YiKC1YLOAd2Q@mail.gmail.com>
In-Reply-To: <CA+2Pv=gUaWx4e=UWZ53DxBBzC7eqGD3JyJgOZ6YiKC1YLOAd2Q@mail.gmail.com>
Content-Type: multipart/alternative;
	boundary="------------060802040000010908000006"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060802040000010908000006
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi,

This is the function defined in PythonMLLibAPI.scala
def findPredict(
       data: JavaRDD[Vector],
       wt: Object,
       mu: Array[Object],
       si: Array[Object]):  RDD[Array[Double]]  = {
}

So the parameter mu should be converted to Array[object].

mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))

def _py2java(sc, obj):

     if isinstance(obj, RDD):
         ...
     elif isinstance(obj, SparkContext):
       ...
     elif isinstance(obj, dict):
        ...
     elif isinstance(obj, (list, tuple)):
         obj = ListConverter().convert(obj, sc._gateway._gateway_client)
     elif isinstance(obj, JavaObject):
         pass
     elif isinstance(obj, (int, long, float, bool, basestring)):
         pass
     else:
         bytes = bytearray(PickleSerializer().dumps(obj))
         obj = sc._jvm.SerDe.loads(bytes)
     return obj

Since its a tuple of Densevectors, in _py2java() its entering the 
isinstance(obj, (list, tuple)) condition and throwing exception(happens 
because the dimension of tuple >1). However the conversion occurs 
correctly if the Pickle conversion is done (last else part).

Hope its clear now.

Regards,
Meethu

On Monday 12 January 2015 11:35 PM, Davies Liu wrote:
> On Sun, Jan 11, 2015 at 10:21 PM, Meethu Mathew
> <meethu.mathew@flytxt.com> wrote:
>> Hi,
>>
>> This is the code I am running.
>>
>> mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))
>>
>> membershipMatrix = callMLlibFunc("findPredict", rdd.map(_convert_to_vector),
>> mu)
> What's the Java API looks like? all the arguments of findPredict
> should be converted
> into java objects, so what should `mu` be converted to?
>
>> Regards,
>> Meethu
>> On Monday 12 January 2015 11:46 AM, Davies Liu wrote:
>>
>> Could you post a piece of code here?
>>
>> On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew <meethu.mathew@flytxt.com>
>> wrote:
>>
>> Hi,
>> Thanks Davies .
>>
>> I added a new class GaussianMixtureModel in clustering.py and the method
>> predict in it and trying to pass numpy array from this method.I converted it
>> to DenseVector and its solved now.
>>
>> Similarly I tried passing a List  of more than one dimension to the function
>> _py2java , but now the exception is
>>
>> 'list' object has no attribute '_get_object_id'
>>
>> and when I give a tuple input (Vectors.dense([0.8786,
>> -0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like
>>
>> 'numpy.ndarray' object has no attribute '_get_object_id'
>>
>> Regards,
>>
>>
>>
>> Meethu Mathew
>>
>> Engineer
>>
>> Flytxt
>>
>> www.flytxt.com | Visit our blog  |  Follow us | Connect on Linkedin
>>
>>
>>
>> On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
>>
>> Hey Meethu,
>>
>> The Java API accepts only Vector, so you should convert the numpy array into
>> pyspark.mllib.linalg.DenseVector.
>>
>> BTW, which class are you using? the KMeansModel.predict() accept
>> numpy.array,
>> it will do the conversion for you.
>>
>> Davies
>>
>> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com>
>> wrote:
>>
>> Hi,
>> I am trying to send a numpy array as an argument to a function predict() in
>> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
>> function callMLlibFunc(name, *args)  in
>> spark/python/pyspark/mllib/common.py.
>>
>> Now the value is passed to the function  _py2java(sc, obj) .Here I am
>> getting an exception
>>
>> Py4JJavaError: An error occurred while calling
>> z:org.apache.spark.mllib.api.python.SerDe.loads.
>> : net.razorvine.pickle.PickleException: expected zero arguments for
>> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>>          at
>> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>>          at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>>          at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>>          at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>>          at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>>
>>
>> Why common._py2java(sc, obj) is not handling numpy array type?
>>
>> Please help..
>>
>>
>> --
>>
>> Regards,
>>
>> *Meethu Mathew*
>>
>> *Engineer*
>>
>> *Flytxt*
>>
>> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
>> <http://www.twitter.com/flytxt> | _Connect on Linkedin
>> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>>
>>
>>


--------------060802040000010908000006--

From dev-return-11102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 08:52:02 2015
Return-Path: <dev-return-11102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B40A717A7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 08:52:02 +0000 (UTC)
Received: (qmail 40721 invoked by uid 500); 13 Jan 2015 08:52:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40634 invoked by uid 500); 13 Jan 2015 08:52:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40617 invoked by uid 99); 13 Jan 2015 08:52:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 08:52:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of niranda.perera@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 08:51:36 +0000
Received: by mail-oi0-f46.google.com with SMTP id a3so1377548oib.5
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 00:51:35 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:cc:content-type;
        bh=7iSvyEEZOFMQ2C16uyXncXOGpi9+sfqhR6+HeOrZ7vA=;
        b=V7QchYr9f8uUMZ8654dLcZ8UoA38ezcNTSv+3sK1SOaEOg4Gis4p0c+JoWe9e5hOoG
         OQik3J0nu5VUtj9+NBB/LlaI4ngXF8b6ZEsFFiuUGgOk3vfno2LRPM+wH+rPCTcU1rL1
         Zwj8/6Pc5aSZwZ73/moHtoA6Y+zPXRxraZrj0fngmhlFwF8Lvx0zt35uPiaEf1GaH1Rz
         UE8MEnn2pwlFTezb53JA3y1W8yKljOtiHLKb2TcxkJnBcRLFCykz/vpHHQPnPd/g8lR9
         Ry+kY5GL1OehmRTh9avurMag6C9MEh7nwF7LAQ2gI5KxTsJbhYzfhtS++nX79LsDLSw6
         Tgtw==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr18951215oie.78.1421139095317;
 Tue, 13 Jan 2015 00:51:35 -0800 (PST)
Received: by 10.202.107.204 with HTTP; Tue, 13 Jan 2015 00:51:35 -0800 (PST)
Date: Tue, 13 Jan 2015 14:21:35 +0530
Message-ID: <CANCoaU60-EOr6YcpQS6cTR8uee_zT1ANRO7UC4atKGD59AEmoQ@mail.gmail.com>
Subject: create a SchemaRDD from a custom datasource
From: Niranda Perera <niranda.perera@gmail.com>
To: dev@spark.apache.org
Cc: mmalithh@gmail.com
Content-Type: multipart/alternative; boundary=001a113ce8e4f3f629050c84baab
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce8e4f3f629050c84baab
Content-Type: text/plain; charset=UTF-8

Hi,

We have a custom datasources API, which connects to various data sources
and exposes them out as a common API. We are now trying to implement the
Spark datasources API released in 1.2.0 to connect Spark for analytics.

Looking at the sources API, we figured out that we should extend a scan
class (table scan etc). While doing so, we would have to implement the
'schema' and 'buildScan' methods.

say, we can infer the schema of the underlying data and take data out as
Row elements. Is there any way we could create RDD[Row] (needed in the
buildScan method) using these Row elements?

Cheers
-- 
Niranda

--001a113ce8e4f3f629050c84baab--

From dev-return-11103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 09:00:28 2015
Return-Path: <dev-return-11103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B91CC17ACC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 09:00:28 +0000 (UTC)
Received: (qmail 72657 invoked by uid 500); 13 Jan 2015 09:00:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72583 invoked by uid 500); 13 Jan 2015 09:00:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72566 invoked by uid 99); 13 Jan 2015 09:00:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 09:00:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 09:00:21 +0000
Received: by mail-qa0-f54.google.com with SMTP id w8so316295qac.13
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 00:59:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Bab6maqI3cZU8vbe0bLV2ughyRHNFTxEgb3Y22mTSU8=;
        b=M0RLDwOeRdnWLnCwKEBF/1qEO3QSGNfN/zKyG0RVrVMXjAVQttcMB33+7apmqteSHH
         2C6vj2UinQUnANLhMJbn2tR3LZ4ZsgvCUzx3mBNFTEZk+FIsciNbKCSq+KNeKKrFQ2Sj
         OHGh8h2OAuDFCYfBwcPY4hAXFOWIeTuXXmWecaAasgXaEcbb93Oyo1X5uQgZEH/YFMzZ
         9tDj/WwhgANKF5OMIZL/2cV8Nolsg1T7mmXONKWfD2oHXcn4KLBDygRL7AlAkhw3bgKm
         yg1NTVS6KLA5B626w+EIP2b3UR6UN0JPMeXkQsoKVRLC8DEpvclLbTKTvx+ora1p2Nic
         ZOOw==
X-Gm-Message-State: ALoCoQlfqn+3ndNqoFNgA5368pNfV2bJra6gD6+o9uptnlk0kS830ZAmIvE1l/1UA4Y78RypqUM6
X-Received: by 10.224.14.84 with SMTP id f20mr963998qaa.43.1421139579892; Tue,
 13 Jan 2015 00:59:39 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 13 Jan 2015 00:59:19 -0800 (PST)
In-Reply-To: <CANCoaU60-EOr6YcpQS6cTR8uee_zT1ANRO7UC4atKGD59AEmoQ@mail.gmail.com>
References: <CANCoaU60-EOr6YcpQS6cTR8uee_zT1ANRO7UC4atKGD59AEmoQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 13 Jan 2015 00:59:19 -0800
Message-ID: <CAPh_B=aX4PXnwdmRqr97g-O5g=JLeHFR=DW0i=qTntauubHvLA@mail.gmail.com>
Subject: Re: create a SchemaRDD from a custom datasource
To: Niranda Perera <niranda.perera@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, mmalithh@gmail.com
Content-Type: multipart/alternative; boundary=047d7bdcabaad61d03050c84d75d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdcabaad61d03050c84d75d
Content-Type: text/plain; charset=UTF-8

Depends on what the other side is doing. You can create your own RDD
implementation by subclassing RDD, or it might work if you use
sc.parallelize(1 to n, n).mapPartitionsWithIndex( /* code to read the data
and return an iterator */ ) where n is the number of partitions.

On Tue, Jan 13, 2015 at 12:51 AM, Niranda Perera <niranda.perera@gmail.com>
wrote:

> Hi,
>
> We have a custom datasources API, which connects to various data sources
> and exposes them out as a common API. We are now trying to implement the
> Spark datasources API released in 1.2.0 to connect Spark for analytics.
>
> Looking at the sources API, we figured out that we should extend a scan
> class (table scan etc). While doing so, we would have to implement the
> 'schema' and 'buildScan' methods.
>
> say, we can infer the schema of the underlying data and take data out as
> Row elements. Is there any way we could create RDD[Row] (needed in the
> buildScan method) using these Row elements?
>
> Cheers
> --
> Niranda
>

--047d7bdcabaad61d03050c84d75d--

From dev-return-11104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 12:54:57 2015
Return-Path: <dev-return-11104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7598A10061
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 12:54:57 +0000 (UTC)
Received: (qmail 74285 invoked by uid 500); 13 Jan 2015 12:54:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74201 invoked by uid 500); 13 Jan 2015 12:54:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74189 invoked by uid 99); 13 Jan 2015 12:54:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 12:54:57 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 12:54:29 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Tue, 13 Jan
 2015 07:53:44 -0500
Message-ID: <54B5154E.8090506@flytxt.com>
Date: Tue, 13 Jan 2015 18:23:34 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Use of MapConverter, ListConverter in python to java object conversion
Content-Type: multipart/alternative;
	boundary="------------090706040508020107070601"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------090706040508020107070601
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi all,

In the python object to java conversion done in the method _py2java in 
spark/python/pyspark/mllib/common.py, why  we are doing individual 
conversion  using MpaConverter,ListConverter? The same can be acheived 
using

bytearray(PickleSerializer().dumps(obj))
obj = sc._jvm.SerDe.loads(bytes)

Is there any performance gain or something in using individual 
converters rather than PickleSerializer?

-- 

Regards,

*Meethu*

--------------090706040508020107070601--

From dev-return-11105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 16:00:20 2015
Return-Path: <dev-return-11105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 098EB10B65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 16:00:20 +0000 (UTC)
Received: (qmail 93555 invoked by uid 500); 13 Jan 2015 16:00:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93469 invoked by uid 500); 13 Jan 2015 16:00:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93385 invoked by uid 99); 13 Jan 2015 16:00:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 16:00:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of z.w.chan.jason@gmail.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 15:59:49 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so2871702qcv.1
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 07:59:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=bm0p8xWi9rV7LSAlub/ikcKABr2/pX54BZEv4+y8iHY=;
        b=NKcTJ8fJvfW8G2NBOupzwD+Fn2v3Yo3cCfScTDZb5vxL8rprHdbkSGoXktLDN6aBHO
         5ZL1FOlwXmETgjIg9c41w3rkAfKVTI+OYsWIxQymuHqVBmNQug3uEh0dXt/0E1SEl4pf
         d5eTgcvipvoJF8QDNeHQ7+UnuWtZdhCjgUa8d/hsYCVgi3Pf57PMcVbaiEPIy5t4Znr2
         LpX3rlbxNhHLf56ZghNJxV77wZKTJA02yDOngb0213E8b1EU/6l9kks0vAVJimR4C0BA
         4+l1F+Q/DXBEIZNGACQkhRcK/4kM/KKUmKWg4akzBXK8ZpOCwVdMWrzWJZ/1ahq177eE
         6GjQ==
MIME-Version: 1.0
X-Received: by 10.224.135.193 with SMTP id o1mr49557949qat.97.1421164743305;
 Tue, 13 Jan 2015 07:59:03 -0800 (PST)
Received: by 10.96.114.201 with HTTP; Tue, 13 Jan 2015 07:59:03 -0800 (PST)
Date: Tue, 13 Jan 2015 23:59:03 +0800
Message-ID: <CAMv5TiDKbZE0HvOnpkxAegKDKK0QV7mmHngEvapcwBq0EbgZoQ@mail.gmail.com>
Subject: Unable to find configuration file at location scalastyle-config.xml
From: Zhiwei Chan <z.w.chan.jason@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2b028b14d35050c8ab392
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b028b14d35050c8ab392
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi everyone,
  I am newly to spark, and try to package the spark-core for some
modification.  I use IDEA to package the spark-core_2.10 of spark 1.1.1.
When encounter the following error, I  check the website
http://www.scalastyle.org/maven.html, and its suggest configuration is to
modify the spark-parent.pom and add a
 ${basedir}to the <configLocation> and <outputFile> of
scalastyle-maven-plugin, but it doesn't work. I solve this error by cocpy
the config file to core directory, but, is there any other better solution=
=EF=BC=9F

[ERROR] Failed to execute goal
org.scalastyle:scalastyle-maven-plugin:0.4.0:check (default) on project
spark-core_2.10: Failed during scalastyle execution: Unable to find
configuration file at location scalastyle-config.xml -> [Help 1]
[ERROR]

--001a11c2b028b14d35050c8ab392--

From dev-return-11106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 16:50:39 2015
Return-Path: <dev-return-11106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9621C10E29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 16:50:39 +0000 (UTC)
Received: (qmail 53690 invoked by uid 500); 13 Jan 2015 16:50:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53619 invoked by uid 500); 13 Jan 2015 16:50:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53607 invoked by uid 99); 13 Jan 2015 16:50:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 16:50:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 16:50:35 +0000
Received: by mail-oi0-f46.google.com with SMTP id a3so3191672oib.5
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 08:50:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=PjdlUUzd2/tiior78kp6W8Ue2A4PnoEGKwEmHF9zUo8=;
        b=dTs6iXqSG8p03CoO/ixBNF0kbZD9if93tpbyv/8jOTTQeYW00wU7389v/5F6d0xa/U
         ZYpNyAnTEZackc2+CBi3OlufKmKMTwkjhqnWZUZrquHXcqGknBvd3McLboOvzOSIK3wX
         4g3GYoa3KRpt6WN/wiojOjXQtbOHzPytn9twU78VQJ2NQneVwpVQH9lmODdoWPklhlbu
         a+8DOwQI0oV9VGu8hmS5n4vnDhGRH0o9+4XWiunJAFd0w3VN8IC0SLVEhV1bMT18JUmW
         Ektg+BKNYWPZsq+zmnYAX1A/hnhd68OMKcZJtGBt6Rr3AnnbeH8i1Yhb3VmKCJjseKhl
         03dQ==
MIME-Version: 1.0
X-Received: by 10.182.215.163 with SMTP id oj3mr21610905obc.49.1421167814724;
 Tue, 13 Jan 2015 08:50:14 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 13 Jan 2015 08:50:14 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 13 Jan 2015 08:50:14 -0800 (PST)
In-Reply-To: <CA+MUDWEhR0qfKzD-rrtKgry_0if6vcu=1MEWq_dt-L+0qZv_Mg@mail.gmail.com>
References: <CA+MUDWEhR0qfKzD-rrtKgry_0if6vcu=1MEWq_dt-L+0qZv_Mg@mail.gmail.com>
Date: Tue, 13 Jan 2015 08:50:14 -0800
Message-ID: <CABPQxsvnC_vc2MfkPfEGSpeE5wQYLqw0MK3DAY7NnLTFHgDpOQ@mail.gmail.com>
Subject: Fwd: [ NOTICE ] Service Downtime Notification - R/W git repos
From: Patrick Wendell <pwendell@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c2cec357e5050c8b6a57
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c2cec357e5050c8b6a57
Content-Type: text/plain; charset=ISO-8859-1

FYI our git repo may be down for a few hours today.
---------- Forwarded message ----------
From: "Tony Stevenson" <pctony@apache.org>
Date: Jan 13, 2015 6:49 AM
Subject: [ NOTICE ] Service Downtime Notification - R/W git repos
To:
Cc:

Folks,

Please note than on Thursday 15th at 20:00 UTC the Infrastructure team
will be taking the read/write git repositories offline.  We expect
that this migration to last about 4 hours.

During the outage the service will be migrated from an old host to a
new one.   We intend to keep the URL the same for access to the repos
after the migration, but an alternate name is already in place in case
DNS updates take too long.   Please be aware it might take some hours
after the completion of the downtime for github to update and reflect
any changes.

The Infrastructure team have been trialling the new host for about a
week now, and [touch wood] have not had any problems with it.

The service is current;y available by accessing repos via:
https://git-wip-us.apache.org

If you have any questions please address them to infrastructure@apache.org




--
Cheers,
Tony

On behalf of the Apache Infrastructure Team

----------------------------------
Tony Stevenson

tony@pc-tony.com
pctony@apache.org

http://www.pc-tony.com

GPG - 1024D/51047D66
----------------------------------

--001a11c2c2cec357e5050c8b6a57--

From dev-return-11107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 18:14:57 2015
Return-Path: <dev-return-11107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E8A3174AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 18:14:57 +0000 (UTC)
Received: (qmail 70672 invoked by uid 500); 13 Jan 2015 18:14:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70608 invoked by uid 500); 13 Jan 2015 18:14:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70596 invoked by uid 99); 13 Jan 2015 18:14:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 18:14:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 18:14:32 +0000
Received: by mail-lb0-f176.google.com with SMTP id p9so3991208lbv.7
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 10:13:25 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=w3bogacSoL3QyRZnV0GUV6gb16c+3IqwED0xx5ejn5A=;
        b=d6/QWMNZlNRMBsLMqL1zNICyUGcnCtvGC5jQSZ5OTWk2z7Lxh73XwwFgarR2cbZ/Ms
         MfDoH66xYHurhAHUWFONn5tZq7NZk2YOsPHBS9P6xmfuXQ0xb2QRsP/qo/253fFBv4Xy
         1TGvPZSqNCZHI1mIs1zFZADTQvP8OnBFtWI4S8gUAa2infIdd4+0mh7qm1nD6rGYNOPa
         nqfF+480KyoPpd+5yEzEepKmyRvhdTdvR6mHPay5J7BeZF9nf9VbCVxXTCTCqouY539B
         BgfNuWZwUkb1WtMa+JgRRuKC5XfcfG56DvP89V+kAZnC9zd8WaNprf6dj4N6zEy6V8iY
         j7OA==
X-Gm-Message-State: ALoCoQmCbgGd/PhSldQS3F3n7TniuyQDZp2ZZMwcLGhsqTwdIC3JAex3NCEMDXfyEchSfXOFr+Bb
MIME-Version: 1.0
X-Received: by 10.112.132.67 with SMTP id os3mr43235907lbb.90.1421172805349;
 Tue, 13 Jan 2015 10:13:25 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Tue, 13 Jan 2015 10:13:25 -0800 (PST)
In-Reply-To: <54B49BB7.40703@flytxt.com>
References: <54AFCD68.9010908@flytxt.com>
	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
	<54B35B6E.1000606@flytxt.com>
	<CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
	<54B367DF.6000807@flytxt.com>
	<CA+2Pv=gUaWx4e=UWZ53DxBBzC7eqGD3JyJgOZ6YiKC1YLOAd2Q@mail.gmail.com>
	<54B49BB7.40703@flytxt.com>
Date: Tue, 13 Jan 2015 10:13:25 -0800
Message-ID: <CA+2Pv=h-mkzzu8==wRizDziXyea5g6FW81o4OXR3vfvf9fPmSA@mail.gmail.com>
Subject: Re: Python to Java object conversion of numpy array
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Jan 12, 2015 at 8:14 PM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
> Hi,
>
> This is the function defined in PythonMLLibAPI.scala
> def findPredict(
>       data: JavaRDD[Vector],
>       wt: Object,
>       mu: Array[Object],
>       si: Array[Object]):  RDD[Array[Double]]  = {
> }
>
> So the parameter mu should be converted to Array[object].
>
> mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))
>
> def _py2java(sc, obj):
>
>     if isinstance(obj, RDD):
>         ...
>     elif isinstance(obj, SparkContext):
>       ...
>     elif isinstance(obj, dict):
>        ...
>     elif isinstance(obj, (list, tuple)):
>         obj = ListConverter().convert(obj, sc._gateway._gateway_client)
>     elif isinstance(obj, JavaObject):
>         pass
>     elif isinstance(obj, (int, long, float, bool, basestring)):
>         pass
>     else:
>         bytes = bytearray(PickleSerializer().dumps(obj))
>         obj = sc._jvm.SerDe.loads(bytes)
>     return obj
>
> Since its a tuple of Densevectors, in _py2java() its entering the
> isinstance(obj, (list, tuple)) condition and throwing exception(happens
> because the dimension of tuple >1). However the conversion occurs correctly
> if the Pickle conversion is done (last else part).

I see, we should remove the special case for list and tuple, pickle should work
more reliably for them. I had tried to remove it, it did not break any tests.

Could you do it in your PR or I create a PR for it separately?

> Hope its clear now.
>
> Regards,
> Meethu
>
> On Monday 12 January 2015 11:35 PM, Davies Liu wrote:
>
> On Sun, Jan 11, 2015 at 10:21 PM, Meethu Mathew
> <meethu.mathew@flytxt.com> wrote:
>
> Hi,
>
> This is the code I am running.
>
> mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))
>
> membershipMatrix = callMLlibFunc("findPredict", rdd.map(_convert_to_vector),
> mu)
>
> What's the Java API looks like? all the arguments of findPredict
> should be converted
> into java objects, so what should `mu` be converted to?
>
> Regards,
> Meethu
> On Monday 12 January 2015 11:46 AM, Davies Liu wrote:
>
> Could you post a piece of code here?
>
> On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew <meethu.mathew@flytxt.com>
> wrote:
>
> Hi,
> Thanks Davies .
>
> I added a new class GaussianMixtureModel in clustering.py and the method
> predict in it and trying to pass numpy array from this method.I converted it
> to DenseVector and its solved now.
>
> Similarly I tried passing a List  of more than one dimension to the function
> _py2java , but now the exception is
>
> 'list' object has no attribute '_get_object_id'
>
> and when I give a tuple input (Vectors.dense([0.8786,
> -0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like
>
> 'numpy.ndarray' object has no attribute '_get_object_id'
>
> Regards,
>
>
>
> Meethu Mathew
>
> Engineer
>
> Flytxt
>
> www.flytxt.com | Visit our blog  |  Follow us | Connect on Linkedin
>
>
>
> On Friday 09 January 2015 11:37 PM, Davies Liu wrote:
>
> Hey Meethu,
>
> The Java API accepts only Vector, so you should convert the numpy array into
> pyspark.mllib.linalg.DenseVector.
>
> BTW, which class are you using? the KMeansModel.predict() accept
> numpy.array,
> it will do the conversion for you.
>
> Davies
>
> On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew <meethu.mathew@flytxt.com>
> wrote:
>
> Hi,
> I am trying to send a numpy array as an argument to a function predict() in
> a class in spark/python/pyspark/mllib/clustering.py which is passed to the
> function callMLlibFunc(name, *args)  in
> spark/python/pyspark/mllib/common.py.
>
> Now the value is passed to the function  _py2java(sc, obj) .Here I am
> getting an exception
>
> Py4JJavaError: An error occurred while calling
> z:org.apache.spark.mllib.api.python.SerDe.loads.
> : net.razorvine.pickle.PickleException: expected zero arguments for
> construction of ClassDict (for numpy.core.multiarray._reconstruct)
>         at
> net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
>         at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
>         at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
>         at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
>         at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
>
>
> Why common._py2java(sc, obj) is not handling numpy array type?
>
> Please help..
>
>
> --
>
> Regards,
>
> *Meethu Mathew*
>
> *Engineer*
>
> *Flytxt*
>
> www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
> <http://www.twitter.com/flytxt> | _Connect on Linkedin
> <http://www.linkedin.com/home?trk=hb_tab_home_top>_
>
>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 18:18:17 2015
Return-Path: <dev-return-11108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33208174F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 18:18:17 +0000 (UTC)
Received: (qmail 895 invoked by uid 500); 13 Jan 2015 18:18:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 820 invoked by uid 500); 13 Jan 2015 18:18:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 805 invoked by uid 99); 13 Jan 2015 18:18:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 18:18:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 18:17:53 +0000
Received: by mail-lb0-f178.google.com with SMTP id u14so3989303lbd.9
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 10:17:31 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=TAJHXboX7F7Jhk0zdqTU87GU7A86lxkvYKl2qn+i01I=;
        b=dt6yIr2PC86N10pKQZBul4bwO2D391M6oLeAtoxCWUtR0Do+/IqekalN3QtaIYKJWv
         8DO2kBI2yeFlZoqOXXmfYPeA2o/0ePJUim8XR0y5KRBjSA+IMGl+hi/z+6n+wuRx+cHM
         FCY8T9yyAFWiKE9LY18F+HoW1bLC52xCunBjvRdPOc1w7/AZXqJ1TiIMbnWMBjfY62s5
         /ELOkQExwQz1PCWCTfJxdxu5wNZiAgCie0yE36tQj83YPtarPkmLBHMxFxtYtAogTyr5
         CDqZNTdWGwv70kwpdWCTD325XuLs7TfTEGL0NpLRpP7fUlCcEFhEYTRAACA+9qH1DiLa
         64Dw==
X-Gm-Message-State: ALoCoQnePHPEM3sXkQlsG99n6rI1pEAdgcdPhtw19diSAbKMpR4YIhROXtnIEwCxa32qqOGSB34j
MIME-Version: 1.0
X-Received: by 10.112.73.66 with SMTP id j2mr43828784lbv.44.1421173051639;
 Tue, 13 Jan 2015 10:17:31 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Tue, 13 Jan 2015 10:17:31 -0800 (PST)
In-Reply-To: <54B5154E.8090506@flytxt.com>
References: <54B5154E.8090506@flytxt.com>
Date: Tue, 13 Jan 2015 10:17:31 -0800
Message-ID: <CA+2Pv=jwDwLt2Tt0Qs2+Z_ofd=0Edr95sVGM7QjwbTTcqm-wxA@mail.gmail.com>
Subject: Re: Use of MapConverter, ListConverter in python to java object conversion
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's not necessary, I will create a PR to remove them.

For larger dict/list/tuple, the pickle approach may have less RPC
calls, better performance.

Davies

On Tue, Jan 13, 2015 at 4:53 AM, Meethu Mathew <meethu.mathew@flytxt.com> wrote:
> Hi all,
>
> In the python object to java conversion done in the method _py2java in
> spark/python/pyspark/mllib/common.py, why  we are doing individual
> conversion  using MpaConverter,ListConverter? The same can be acheived using
>
> bytearray(PickleSerializer().dumps(obj))
> obj = sc._jvm.SerDe.loads(bytes)
>
> Is there any performance gain or something in using individual converters
> rather than PickleSerializer?
>
> --
>
> Regards,
>
> *Meethu*

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 19:10:40 2015
Return-Path: <dev-return-11109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B57F1773C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 19:10:40 +0000 (UTC)
Received: (qmail 84708 invoked by uid 500); 13 Jan 2015 19:10:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84641 invoked by uid 500); 13 Jan 2015 19:10:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84626 invoked by uid 99); 13 Jan 2015 19:10:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 19:10:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 19:10:36 +0000
Received: by mail-qc0-f182.google.com with SMTP id r5so3770416qcx.13
        for <dev@spark.apache.org>; Tue, 13 Jan 2015 11:09:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/SbGLqOcwYXe66v5k4aHkMIMwfLKMwRjNUsqStCUieo=;
        b=YUqmAhLW8yj01FMK6nOVIYGIHcUjEu7SgZD74wwtq0mdrL3roUPVxXn3Q7O/VJ4KT9
         7RDs2C39muouWPeNMjncCOrofU3PtvcJdLVKF7+T9MXeETKKUYUvuB5kWJWbrUNmRwbp
         b6Zc2R7lDyGADB3iz3/WoTB1vobgADFGAH7Juv18oDjh0lxoj6UQhhVGBPAwcy2fu8jQ
         vWc5XcHGenweUnnXpPP1VpDnPtoDEfcZeVgl6M8mK9qngF8z2X1+obTSwwN60SiUakhg
         3YGSDA33BwyMa+5uZ3UCoiwKVeCRNQdRrL52KuDfjcwCyJF/XRX5ktx6whZFRims1UWF
         6wxA==
X-Gm-Message-State: ALoCoQkI7p70VgVPzo46y7Ad6BD5mt7eRDTSmCciYSKUro0WysIaIJJuj0hTA1qZS09A+XTrV8wN
X-Received: by 10.224.92.205 with SMTP id s13mr62123419qam.52.1421176195397;
 Tue, 13 Jan 2015 11:09:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 13 Jan 2015 11:09:35 -0800 (PST)
In-Reply-To: <CANoJNxSGpTVTE6EeWp=0_Ab1DSWx9bqx0bAyRKbcEbhqeoc5Yw@mail.gmail.com>
References: <CANCoaU60-EOr6YcpQS6cTR8uee_zT1ANRO7UC4atKGD59AEmoQ@mail.gmail.com>
 <CAPh_B=aX4PXnwdmRqr97g-O5g=JLeHFR=DW0i=qTntauubHvLA@mail.gmail.com> <CANoJNxSGpTVTE6EeWp=0_Ab1DSWx9bqx0bAyRKbcEbhqeoc5Yw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 13 Jan 2015 11:09:35 -0800
Message-ID: <CAPh_B=aoRmN7xGqDnUTP2aRLiW=k0uC2iDCYLiaSH7NRO0tp5w@mail.gmail.com>
Subject: Re: create a SchemaRDD from a custom datasource
To: Malith Dhanushka <mmalithh@gmail.com>
Cc: Niranda Perera <niranda.perera@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e015375e44a5073050c8d5e81
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015375e44a5073050c8d5e81
Content-Type: text/plain; charset=UTF-8

If it is a small collection of them on the driver, you can just use
sc.parallelize to create an RDD.


On Tue, Jan 13, 2015 at 7:56 AM, Malith Dhanushka <mmalithh@gmail.com>
wrote:

> Hi Reynold,
>
> Thanks for the response. I am just wondering, lets say we have set of Row
> objects. Isn't there a straightforward way of creating RDD[Row] out of it
> without writing a custom RDD?
>
> ie - a utility method
>
> Thanks
> Malith
>
> On Tue, Jan 13, 2015 at 2:29 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Depends on what the other side is doing. You can create your own RDD
>> implementation by subclassing RDD, or it might work if you use
>> sc.parallelize(1 to n, n).mapPartitionsWithIndex( /* code to read the data
>> and return an iterator */ ) where n is the number of partitions.
>>
>> On Tue, Jan 13, 2015 at 12:51 AM, Niranda Perera <
>> niranda.perera@gmail.com> wrote:
>>
>>> Hi,
>>>
>>> We have a custom datasources API, which connects to various data sources
>>> and exposes them out as a common API. We are now trying to implement the
>>> Spark datasources API released in 1.2.0 to connect Spark for analytics.
>>>
>>> Looking at the sources API, we figured out that we should extend a scan
>>> class (table scan etc). While doing so, we would have to implement the
>>> 'schema' and 'buildScan' methods.
>>>
>>> say, we can infer the schema of the underlying data and take data out as
>>> Row elements. Is there any way we could create RDD[Row] (needed in the
>>> buildScan method) using these Row elements?
>>>
>>> Cheers
>>> --
>>> Niranda
>>>
>>
>>
> <Email-mmalithh@gmail.com>
>
>

--089e015375e44a5073050c8d5e81--

From dev-return-11110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 21:43:25 2015
Return-Path: <dev-return-11110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4522D10204
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 21:43:25 +0000 (UTC)
Received: (qmail 71080 invoked by uid 500); 13 Jan 2015 21:43:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71010 invoked by uid 500); 13 Jan 2015 21:43:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70998 invoked by uid 99); 13 Jan 2015 21:43:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 21:43:25 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of angellandros@yahoo.com designates 98.138.90.81 as permitted sender)
Received: from [98.138.90.81] (HELO nm18.bullet.mail.ne1.yahoo.com) (98.138.90.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 21:42:57 +0000
Received: from [98.138.101.130] by nm18.bullet.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:39:46 -0000
Received: from [98.138.226.165] by tm18.bullet.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:39:46 -0000
Received: from [127.0.0.1] by omp1066.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:39:46 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 503663.58428.bm@omp1066.mail.ne1.yahoo.com
X-YMail-OSG: qYk_1UEVM1mUyCt5OaUiZZ5sA7_It_X0lxJUeCDhLepMYqRbfcaxjmx.Uy5PCZ2
 znnwQCW08tUH6hRFEm2K6lraQa78AqNThjR3pAwbG1POGdbUwjqlxyyFyQVTd63aF2E0Swupo.Fk
 yY8iTPZB52w8.Km.sB5mSqR3DZ4sk1kScjNydSxPDrUOq7GWGCB4gc3SANlyln.oG7LjxQEg8AcU
 f_hxXA1zQlu_BRabf3Tnk5h4ctUC4zMllwaa.iOAR0Vrg6bgA7OKstQqDSIdEw6_6xuHz_ERbePV
 6oo0BV6QNuOVE4D6FutgZ1VbvtYj30Nwz8RkKeGfGqp1sG6kil0B6FNn9EUCwY2O3MqXeyxyHrFr
 wVR1HNRK1pXgsX4V1xPKVYk4oAmcPyic9GbviU5k7OKRMe5RJmV5zzeggqCD8kG44n1L.UD0MYQ1
 qujQj_muD39ldzCuciIk9lWaovrx24eyJrQzeuBG1bMg8kC7KbzqPrZXUH2DYH0ffz6RUGEdb343
 4JcvtcAbGqq_9
Received: by 98.138.105.253; Tue, 13 Jan 2015 21:39:46 +0000 
Date: Tue, 13 Jan 2015 21:39:45 +0000 (UTC)
From: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>
Reply-To: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1551808826.393105.1421185185848.JavaMail.yahoo@jws100161.mail.ne1.yahoo.com>
Subject: DBSCAN for MLlib
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_393104_1060004460.1421185185846"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_393104_1060004460.1421185185846
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Dear all,
I think MLlib needs more clustering algorithms and DBSCAN is my first candidate. I am starting to implement it. Any advice?
Muhammad-Ali
------=_Part_393104_1060004460.1421185185846--

From dev-return-11111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 21:50:41 2015
Return-Path: <dev-return-11111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49A3810264
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 21:50:41 +0000 (UTC)
Received: (qmail 4322 invoked by uid 500); 13 Jan 2015 21:50:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4254 invoked by uid 500); 13 Jan 2015 21:50:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4232 invoked by uid 99); 13 Jan 2015 21:50:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 21:50:41 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of angellandros@yahoo.com designates 98.138.91.254 as permitted sender)
Received: from [98.138.91.254] (HELO nm2-vm6.bullet.mail.ne1.yahoo.com) (98.138.91.254)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 21:50:33 +0000
Received: from [98.138.100.112] by nm2.bullet.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:49:09 -0000
Received: from [98.138.89.245] by tm103.bullet.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:49:09 -0000
Received: from [127.0.0.1] by omp1059.mail.ne1.yahoo.com with NNFMP; 13 Jan 2015 21:49:09 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 754893.49621.bm@omp1059.mail.ne1.yahoo.com
X-YMail-OSG: UIcisTgVM1kGgTsQS4MIMyajffsEiOl2jsqULllWhzB_b6.Fk_PfVubcqnpmuMS
 3qJX7ws2eFkLxuP5EMyRLqsSTLQr_e5l8VTEnvSAdJ5btflQH8CVfyBrJ_nJ1TRvmUNAB_kKeXDL
 h6rYaYSAKbQbTM48NJljGY_TC0fTvRs7pcPX4rPn4TnmuFAIf9BUyT5YWYiAm9UJhfF.VfQ1X0Gz
 TU2zDK6_ScSG5Ms75IL8Yp1M2wlu7jEIwA2V08syfuReCKiucLn0mWUMfRo3CDDmM6wfx7oehKX5
 wU4PRwMjrlX2vrzy.4PrqSCU7oxZZvF7lqLN4qFCRT_7fn60_ggu.q1SktxtJ87TQq71oF6yjZoH
 a0E9..7tcUA23O91MZ93.aVTZs7UH6uVrCqe51187x_dmaNiqJvC5ioe8ECzcY9ZrVhuEhQhuUbI
 scnk8cQxMTCyw5XpLd_pXlvogDGku0u6EDuS1W5ztRCycB4FMU.koyMQJY3e9RHE1bcD605EiAsY
 8lwKQnlIjYsv.8KfgI2JQo64iY_7azD8xo9VTS4Bcku0dPQlFkoyGiqLv.ZH9YxYVJzaTsTO5WXk-
Received: by 98.138.105.251; Tue, 13 Jan 2015 21:49:09 +0000 
Date: Tue, 13 Jan 2015 21:49:08 +0000 (UTC)
From: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>
Reply-To: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <32085278.400753.1421185748959.JavaMail.yahoo@jws100154.mail.ne1.yahoo.com>
In-Reply-To: <1551808826.393105.1421185185848.JavaMail.yahoo@jws100161.mail.ne1.yahoo.com>
References: <1551808826.393105.1421185185848.JavaMail.yahoo@jws100161.mail.ne1.yahoo.com>
Subject: Re: DBSCAN for MLlib
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_400752_1816021245.1421185748951"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_400752_1816021245.1421185748951
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I have to say, I have created a Jira task for it:
[SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRA

| =C2=A0 |
| =C2=A0 | =C2=A0 | =C2=A0 | =C2=A0 | =C2=A0 |
| [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRAMLlib is =
all k-means now, and I think we should add some new clustering algorithms t=
o it. First candidate is DBSCAN as I think.  |
|  |
| View on issues.apache.org | Preview by Yahoo |
|  |
| =C2=A0 |

 =C2=A0=20

     On Wednesday, January 14, 2015 1:09 AM, Muhammad Ali A'r=C3=A5by <ange=
llandros@yahoo.com> wrote:
  =20

 Dear all,
I think MLlib needs more clustering algorithms and DBSCAN is my first candi=
date. I am starting to implement it. Any advice?
Muhammad-Ali

   
------=_Part_400752_1816021245.1421185748951--

From dev-return-11112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 13 22:16:45 2015
Return-Path: <dev-return-11112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2BF310405
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 Jan 2015 22:16:45 +0000 (UTC)
Received: (qmail 974 invoked by uid 500); 13 Jan 2015 22:16:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 903 invoked by uid 500); 13 Jan 2015 22:16:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 824 invoked by uid 99); 13 Jan 2015 22:16:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 22:16:45 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 Jan 2015 22:16:41 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id D9FF4106DE9E
	for <dev@spark.apache.org>; Tue, 13 Jan 2015 14:16:01 -0800 (PST)
Date: Tue, 13 Jan 2015 15:15:59 -0700 (MST)
From: Madhu <madhu@madhu.com>
To: dev@spark.apache.org
Message-ID: <1421187359797-10104.post@n3.nabble.com>
Subject: VertexId type in GraphX
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Are there any plans to generalize the type of VertexId in GraphX?
Our keys are particularly long. We could use the hashCode() trick, but the
chance of collisions is not acceptable. Given our data volume, we have
encountered hashCode() collisions more than once.

I see this Jira, but it is specific to UUIDs:

https://issues.apache.org/jira/browse/SPARK-1153

Looking through the code, it seems doable, but I'm not aware of the
consequences.
Is it entirely a performance issue?

Support for an arbitrary type would be ideal, but arbitrarily long byte
arrays are a reasonable compromise, if that helps.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VertexId-type-in-GraphX-tp10104.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 13:34:16 2015
Return-Path: <dev-return-11113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F8DE10306
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 13:34:16 +0000 (UTC)
Received: (qmail 6859 invoked by uid 500); 14 Jan 2015 13:34:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6781 invoked by uid 500); 14 Jan 2015 13:34:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6768 invoked by uid 99); 14 Jan 2015 13:34:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 13:34:16 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [157.193.49.127] (HELO smtp3.ugent.be) (157.193.49.127)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 13:34:09 +0000
Received: from localhost (mcheck2.ugent.be [157.193.49.249])
	by smtp3.ugent.be (Postfix) with ESMTP id 4CE80C415
	for <dev@spark.apache.org>; Wed, 14 Jan 2015 14:33:46 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp3.ugent.be ([IPv6:::ffff:157.193.49.127])
	by localhost (mcheck2.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id QCNr0nVdSpdR for <dev@spark.apache.org>;
	Wed, 14 Jan 2015 14:33:45 +0100 (CET)
Received: from [157.193.44.243] (gast044c.ugent.be [157.193.44.243])
	(Authenticated sender: ehiggs)
	by smtp3.ugent.be (Postfix) with ESMTPSA id CF505C40E
	for <dev@spark.apache.org>; Wed, 14 Jan 2015 14:33:45 +0100 (CET)
Message-ID: <54B67039.9070408@ugent.be>
Date: Wed, 14 Jan 2015 14:33:45 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: SparkSpark-perf terasort WIP branch
Content-Type: multipart/alternative;
 boundary="------------050303000307070203080904"
X-Miltered: at jchkm3 with ID 54B67039.002 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 54B67039.002 from gast044c.ugent.be/gast044c.ugent.be/157.193.44.243/[157.193.44.243]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 54B67039.002 on smtp3.ugent.be : j-chkmail score : . : R=. U=. O=. B=0.000 -> S=0.000
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

--------------050303000307070203080904
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Hi all,
I'm trying to build the Spark-perf WIP code but there are some errors to 
do with Hadoop APIs. I presume this is because there is some Hadoop 
version set and it's referring to that. But I can't seem to find it.

The errors are as follows:

[info] Compiling 15 Scala sources and 2 Java sources to 
/home/ehiggs/src/spark-perf/spark-tests/target/scala-2.10/classes...
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:40: 
object task is not a member of package org.apache.hadoop.mapreduce
[error] import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
[error]                                    ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:132: 
not found: type TaskAttemptContextImpl
[error]             val context = new TaskAttemptContextImpl(
[error]                               ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:37: 
object TTConfig is not a member of package 
org.apache.hadoop.mapreduce.server.tasktracker
[error] import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig
[error]        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:91: 
not found: value TTConfig
[error]   var slotsPerHost : Int = conf.getInt(TTConfig.TT_MAP_SLOTS, 4)
[error]                                        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:7: 
value run is not a member of org.apache.spark.examples.terasort.TeraGen
[error]     tg.run(Array[String]("10M", "/tmp/terasort_in"))
[error]        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:9: 
value run is not a member of org.apache.spark.examples.terasort.TeraSort
[error]     ts.run(Array[String]("/tmp/terasort_in", "/tmp/terasort_out"))
[error]        ^
[error] 6 errors found
[error] (compile:compile) Compilation failed
[error] Total time: 13 s, completed 05-Jan-2015 12:21:47

I can build the same code if it's in the Spark tree using the following 
command:
mvn -Dhadoop.version=2.5.0 -DskipTests=true install

Is there a way I can convince spark-perf to build this code with the 
appropriate Hadoop library version? I tried to apply the following to 
spark-tests/project/SparkTestsBuild.scala but it didn't seem to work as 
I expected:

$ git diff project/SparkTestsBuild.scala
diff --git a/spark-tests/project/SparkTestsBuild.scala 
b/spark-tests/project/SparkTestsBuild.scala
index 4116326..4ed5f0c 100644
--- a/spark-tests/project/SparkTestsBuild.scala
+++ b/spark-tests/project/SparkTestsBuild.scala
@@ -16,7 +16,9 @@ object SparkTestsBuild extends Build {
          "org.scalatest" %% "scalatest" % "2.2.1" % "test",
          "com.google.guava" % "guava" % "14.0.1",
          "org.apache.spark" %% "spark-core" % "1.0.0" % "provided",
-        "org.json4s" %% "json4s-native" % "3.2.9"
+        "org.json4s" %% "json4s-native" % "3.2.9",
+        "org.apache.hadoop" % "hadoop-common" % "2.5.0",
+        "org.apache.hadoop" % "hadoop-mapreduce" % "2.5.0"
        ),
        test in assembly := {},
        outputPath in assembly := 
file("target/spark-perf-tests-assembly.jar"),
@@ -36,4 +38,4 @@ object SparkTestsBuild extends Build {
          case _ => MergeStrategy.first
        }
      ))
-}
\ No newline at end of file
+}


Yours,
Ewan

--------------050303000307070203080904--

From dev-return-11114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 19:27:14 2015
Return-Path: <dev-return-11114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2AB01795B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 19:27:14 +0000 (UTC)
Received: (qmail 44392 invoked by uid 500); 14 Jan 2015 19:27:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44308 invoked by uid 500); 14 Jan 2015 19:27:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44278 invoked by uid 99); 14 Jan 2015 19:27:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 19:27:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 19:27:09 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so10714486wes.1
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 11:26:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=R1VSmMe5Bg7hCso0Zwpv0rXAJB5YfLtHzPXahq3Bi8g=;
        b=AKVtW1Fh8iYkR5s1sczLL5lz9c29hY8irSQAONYcakYXhSYuZIvGJH6LpaixmvdDuU
         htMi0Skd3GwJk1fSeNkM4AX6nujuZ6r2Ma4vfDw7d5vj6Gpjn5pZ7KS+nJmfLJEHMOEd
         +XweoxDNZKP8+c70LcYLkiZY94OeP+sMjxmafr5V0oGzQwGBaAw3ALNDjPftos2Cz3iH
         DuVblkn9jTYxOUkELpKNxyhO6F0lqfPIUpx/AZkb10ywqpqZi/lsBHIQ5C+YwFrnGOyU
         sagAaPJXBCwWp8MYnhL7aGX781GIYaEQ4xNFPsbISZX4wx4VHg7GzPq7T66V5Oa8raN5
         J0Mg==
MIME-Version: 1.0
X-Received: by 10.180.73.108 with SMTP id k12mr11577168wiv.24.1421263608420;
 Wed, 14 Jan 2015 11:26:48 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Wed, 14 Jan 2015 11:26:48 -0800 (PST)
Date: Wed, 14 Jan 2015 14:26:48 -0500
Message-ID: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
Subject: Incorrect Maven Artifact Names
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043891d1832177050ca1b839
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043891d1832177050ca1b839
Content-Type: text/plain; charset=UTF-8

Hi all,

I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of the
RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
artifacts are  placed in com/google/guava and there is no org/apache/spark.

I saw that the pom.xml files had been modified to prevent the install
command and that the guava dependency was modified.  Could someone who is
more familiar with the Spark maven files comment on what might be causing
this oddity?

Thanks,
RJ

We build Spark like so:
$ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package

Then build a local Maven repo:

mvn -Phadoop-2.4 \
    -Dmesos.version=0.20.0 \
    -DskipTests install:install-file  \
    -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
    -DcreateChecksum=true \
    -DgeneratePom=true \
    -DartifactId=spark-assembly_2.1.0 \
    -DlocalRepositoryPath=../maven2

--f46d043891d1832177050ca1b839--

From dev-return-11115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 19:33:53 2015
Return-Path: <dev-return-11115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9D06179DC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 19:33:53 +0000 (UTC)
Received: (qmail 77937 invoked by uid 500); 14 Jan 2015 19:33:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77862 invoked by uid 500); 14 Jan 2015 19:33:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77850 invoked by uid 99); 14 Jan 2015 19:33:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 19:33:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 19:33:28 +0000
Received: by mail-wi0-f179.google.com with SMTP id ho1so4814582wib.0
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 11:33:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ohjx+b8swskJJSHZWcGYcgi9r5ToSjrKJtO0YgGoRwc=;
        b=YnfEGfgVhPfoGILKYTqLq9USVYFgRMTpEv8iHKCsSfcBW/rXa1EmwnTNIN119lHOfU
         TZ8+W+tfqr7u0uiYSPFL2TKdIz2pWEcDG1St1WsbHXScKluV47KveQdw4Y51A4RAG0aE
         yreR/+bwWnBlNPGlf6S7pjJF7aQn+G9k0NImLGZma6vKBOJ0rFa/hTtHRYlMLgFiU2wf
         RINpOkpIJTfgRNknWN+NqzR9CYos4hs7Yy7MdciNZGNAQA1+Oj+c6TTloTx99NrCFIjt
         6WNN57lcwrESCdX9LUrYnJaxCEp+I3R2tANxsELduTAX5rIvi1//p7bbFFFxhMatghn3
         +SIQ==
X-Gm-Message-State: ALoCoQmmw5b3uW0g3z/phn6V10taOe6Z1azkMoidaEgLCJEiLp41ccdnypFvAMhzFSyYiIAem8OZ
X-Received: by 10.180.91.36 with SMTP id cb4mr52212392wib.30.1421264006909;
 Wed, 14 Jan 2015 11:33:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Wed, 14 Jan 2015 11:33:06 -0800 (PST)
In-Reply-To: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 14 Jan 2015 19:33:06 +0000
Message-ID: <CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
To: RJ Nowling <rnowling@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Guava is shaded, although one class is left in its original package.
This shouldn't have anything to do with Spark's package or namespace
though. What are you saying is in com/google/guava?

You can un-skip the install plugin with -Dmaven.install.skip=false

On Wed, Jan 14, 2015 at 7:26 PM, RJ Nowling <rnowling@gmail.com> wrote:
> Hi all,
>
> I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of the
> RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
> artifacts are  placed in com/google/guava and there is no org/apache/spark.
>
> I saw that the pom.xml files had been modified to prevent the install
> command and that the guava dependency was modified.  Could someone who is
> more familiar with the Spark maven files comment on what might be causing
> this oddity?
>
> Thanks,
> RJ
>
> We build Spark like so:
> $ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package
>
> Then build a local Maven repo:
>
> mvn -Phadoop-2.4 \
>     -Dmesos.version=0.20.0 \
>     -DskipTests install:install-file  \
>     -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
>     -DcreateChecksum=true \
>     -DgeneratePom=true \
>     -DartifactId=spark-assembly_2.1.0 \
>     -DlocalRepositoryPath=../maven2

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 20:01:10 2015
Return-Path: <dev-return-11116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 882FA17B19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 20:01:10 +0000 (UTC)
Received: (qmail 62406 invoked by uid 500); 14 Jan 2015 20:01:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62333 invoked by uid 500); 14 Jan 2015 20:01:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62319 invoked by uid 99); 14 Jan 2015 20:01:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 20:01:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 20:00:45 +0000
Received: by mail-we0-f181.google.com with SMTP id q58so10828933wes.12
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 11:59:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=w3RE95niDj1Bq5iuTlr4Sn1r5ixIEPsMdfYSr6LzXio=;
        b=s9kCK4HSOPtOt6SJSiFuS20vtFYu7Y19GIEikBXzEe8V1rJRMfgfj96k6Bk2hC9xKM
         CfiTaNk9MiaqBLbDnpf+AQ7Djd4ByK1jI3mfAw5Kv/GgIe/Y4+CCrcpvoW04k+ACzbbT
         gPHRg3/cZSbLVfuKAtePJPu6Z4oAtU5rq/4QgqYa0meePx0MpPuJbxv8ibCMTfladJT1
         H5xXauQhQZXrOagA9Od8jI18BK90T2jDIm+xyKKZxXb/OM9Q9cDW3r3QVHRsSQ3zfEOs
         R2/CTLth/yiMcHSfDn7lW8P9ecu3YGd03ikwRAS7en6oJwzkI6rwWeedfNJ9iT7Hq5l0
         x2zg==
MIME-Version: 1.0
X-Received: by 10.180.72.178 with SMTP id e18mr18392146wiv.23.1421265553619;
 Wed, 14 Jan 2015 11:59:13 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Wed, 14 Jan 2015 11:59:13 -0800 (PST)
In-Reply-To: <CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
	<CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
Date: Wed, 14 Jan 2015 14:59:13 -0500
Message-ID: <CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
From: RJ Nowling <rnowling@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c7cb07482c4050ca22c5c
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c7cb07482c4050ca22c5c
Content-Type: text/plain; charset=UTF-8

Thanks, Sean.

Yes, Spark is incorrectly copying the spark assembly jar to
com/google/guava in the maven repository.  This is for the 1.2.0 release,
just to clarify.

I reverted the patches that shade Guava and removed the parts disabling the
install plugin and it seemed to fix the issue.

It seems that Spark poms are inheriting something from Guava.

RJ

On Wed, Jan 14, 2015 at 2:33 PM, Sean Owen <sowen@cloudera.com> wrote:

> Guava is shaded, although one class is left in its original package.
> This shouldn't have anything to do with Spark's package or namespace
> though. What are you saying is in com/google/guava?
>
> You can un-skip the install plugin with -Dmaven.install.skip=false
>
> On Wed, Jan 14, 2015 at 7:26 PM, RJ Nowling <rnowling@gmail.com> wrote:
> > Hi all,
> >
> > I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of
> the
> > RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
> > artifacts are  placed in com/google/guava and there is no
> org/apache/spark.
> >
> > I saw that the pom.xml files had been modified to prevent the install
> > command and that the guava dependency was modified.  Could someone who is
> > more familiar with the Spark maven files comment on what might be causing
> > this oddity?
> >
> > Thanks,
> > RJ
> >
> > We build Spark like so:
> > $ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package
> >
> > Then build a local Maven repo:
> >
> > mvn -Phadoop-2.4 \
> >     -Dmesos.version=0.20.0 \
> >     -DskipTests install:install-file  \
> >
>  -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
> >     -DcreateChecksum=true \
> >     -DgeneratePom=true \
> >     -DartifactId=spark-assembly_2.1.0 \
> >     -DlocalRepositoryPath=../maven2
>

--f46d043c7cb07482c4050ca22c5c--

From dev-return-11117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 21:08:33 2015
Return-Path: <dev-return-11117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83DD017DFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 21:08:33 +0000 (UTC)
Received: (qmail 93978 invoked by uid 500); 14 Jan 2015 21:08:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93904 invoked by uid 500); 14 Jan 2015 21:08:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93892 invoked by uid 99); 14 Jan 2015 21:08:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:08:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:08:09 +0000
Received: by mail-wi0-f170.google.com with SMTP id bs8so29122265wib.1
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 13:08:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=E0R9k/bRj3qhmounCoI0hRNvlzxuYUbbqXAhQmvAoDA=;
        b=fW1nO9ra0a+7vn+FdL4UsqHlHfXOV0lavcAXVNwHFfTpPultZ3+MxbEfpX/srq2QxH
         NbyUN0KlD6Q9qh4cY94khJKw009Z4GDZIbTkJvmNojtr7NA3QcruaXtE5l+C/Jy8gb3U
         PXodPgzgzlVXNFceYE9+3oiP5EaXeVA1Eo5HYatFqKuteoki2M9vKs872N3ruU3lRGJa
         3rw2UsCh3t6XnylZTmuDNz/IZqqbHVqThsD86DSvR/GPXWbpYWAuX1wDZHxKmecTucsv
         daD7/dhIyjvYQbO1neM8x2TU3Qw3yw1jNGWbfGzkVAPYCP8vImTqCCcu0vRvIFQBKyjH
         5Whw==
MIME-Version: 1.0
X-Received: by 10.180.90.81 with SMTP id bu17mr12043406wib.23.1421269688000;
 Wed, 14 Jan 2015 13:08:08 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Wed, 14 Jan 2015 13:08:07 -0800 (PST)
In-Reply-To: <CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
	<CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
	<CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
Date: Wed, 14 Jan 2015 16:08:07 -0500
Message-ID: <CADtDQQ+TxvyVaG0WTLfrD_8huc47KqkbfZGMoguTsUH+dJCqGw@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
From: RJ Nowling <rnowling@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0435c012e22909050ca32242
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0435c012e22909050ca32242
Content-Type: text/plain; charset=UTF-8

Hi Sean,

I confirmed that if I take the Spark 1.2.0 release (a428c446), undo the
guava PR [1], and use -Dmaven.install.skip=false with the workflow above,
the problem is fixed.

RJ


[1]
https://github.com/apache/spark/commit/c9f743957fa963bc1dbed7a44a346ffce1a45cf2#diff-6382f8428b13fa6082fa688178f3dbcc

On Wed, Jan 14, 2015 at 2:59 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Thanks, Sean.
>
> Yes, Spark is incorrectly copying the spark assembly jar to
> com/google/guava in the maven repository.  This is for the 1.2.0 release,
> just to clarify.
>
> I reverted the patches that shade Guava and removed the parts disabling
> the install plugin and it seemed to fix the issue.
>
> It seems that Spark poms are inheriting something from Guava.
>
> RJ
>
> On Wed, Jan 14, 2015 at 2:33 PM, Sean Owen <sowen@cloudera.com> wrote:
>
>> Guava is shaded, although one class is left in its original package.
>> This shouldn't have anything to do with Spark's package or namespace
>> though. What are you saying is in com/google/guava?
>>
>> You can un-skip the install plugin with -Dmaven.install.skip=false
>>
>> On Wed, Jan 14, 2015 at 7:26 PM, RJ Nowling <rnowling@gmail.com> wrote:
>> > Hi all,
>> >
>> > I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of
>> the
>> > RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
>> > artifacts are  placed in com/google/guava and there is no
>> org/apache/spark.
>> >
>> > I saw that the pom.xml files had been modified to prevent the install
>> > command and that the guava dependency was modified.  Could someone who
>> is
>> > more familiar with the Spark maven files comment on what might be
>> causing
>> > this oddity?
>> >
>> > Thanks,
>> > RJ
>> >
>> > We build Spark like so:
>> > $ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package
>> >
>> > Then build a local Maven repo:
>> >
>> > mvn -Phadoop-2.4 \
>> >     -Dmesos.version=0.20.0 \
>> >     -DskipTests install:install-file  \
>> >
>>  -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
>> >     -DcreateChecksum=true \
>> >     -DgeneratePom=true \
>> >     -DartifactId=spark-assembly_2.1.0 \
>> >     -DlocalRepositoryPath=../maven2
>>
>
>

--f46d0435c012e22909050ca32242--

From dev-return-11118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 21:16:57 2015
Return-Path: <dev-return-11118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 001BE17E50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 21:16:56 +0000 (UTC)
Received: (qmail 14999 invoked by uid 500); 14 Jan 2015 21:16:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14927 invoked by uid 500); 14 Jan 2015 21:16:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14915 invoked by uid 99); 14 Jan 2015 21:16:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:16:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:16:31 +0000
Received: by mail-qa0-f54.google.com with SMTP id w8so7675963qac.13
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 13:16:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=yud3X7bC/aAmuJjT/T/uhEQOvQdhYgfb5MI/ikOqR14=;
        b=UZXIRz4MWArH3WdN9ghNdMC/SEKDt79OsLQ8lg5HGTNZYqfWGHOkN+i4OhFK3AZ2pj
         +51JR4vCPQj7UN8VvrCYNEiE0LjkitG5jqlTlspEFfQkSo2COpePr/LBcfdUCui40vSK
         1XOv/+9ISeGJIMCh98t4hyepMtcTdiUvtAOZSOTvfXKr0gZXXEgr9eXcF+HNrJQAmYTq
         cf24NkLZmCyNxwkOkpsm08lAo21kO4p8OTRlc6akjcWTtcq8SIayb3W+aEqyn5DkfF7Y
         dZgF3PVwHIoPhidCk8yeCO//WZfEmNRj5RJO7nKI04NLAtxLDE416x4t7rn4kyMHdZzX
         OkWg==
X-Gm-Message-State: ALoCoQkdpFBtsaUNl06YostGkKUTkUvvKgDYhVtabFp4efEz6FkDWD+PGae+4Z9L4Z03iw50HYiR
MIME-Version: 1.0
X-Received: by 10.140.102.72 with SMTP id v66mr965189qge.31.1421270189628;
 Wed, 14 Jan 2015 13:16:29 -0800 (PST)
Received: by 10.229.155.2 with HTTP; Wed, 14 Jan 2015 13:16:29 -0800 (PST)
In-Reply-To: <CADtDQQ+TxvyVaG0WTLfrD_8huc47KqkbfZGMoguTsUH+dJCqGw@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
	<CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
	<CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
	<CADtDQQ+TxvyVaG0WTLfrD_8huc47KqkbfZGMoguTsUH+dJCqGw@mail.gmail.com>
Date: Wed, 14 Jan 2015 13:16:29 -0800
Message-ID: <CAAOnQ7upx9wyLWiuF2pmS3hbO447rJbS-gjTtTf_Mor6+vqTfA@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
From: Marcelo Vanzin <vanzin@cloudera.com>
To: RJ Nowling <rnowling@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi RJ,

I think I remember noticing in the past that some Guava metadata ends
up overwriting maven-generated metadata in the assembly's manifest.
That's probably something we should fix if that still affects the
build.

That being said, this is probably happening because you're using
"install-file" instead of "install". If you want a workaround that
doesn't require unshading things, you can change assembly.pom to (i)
not skip the install plugin and (ii) have "jar" as the packaging,
instead of pom.



On Wed, Jan 14, 2015 at 1:08 PM, RJ Nowling <rnowling@gmail.com> wrote:
> Hi Sean,
>
> I confirmed that if I take the Spark 1.2.0 release (a428c446), undo the
> guava PR [1], and use -Dmaven.install.skip=false with the workflow above,
> the problem is fixed.
>
> RJ
>
>
> [1]
> https://github.com/apache/spark/commit/c9f743957fa963bc1dbed7a44a346ffce1a45cf2#diff-6382f8428b13fa6082fa688178f3dbcc
>
> On Wed, Jan 14, 2015 at 2:59 PM, RJ Nowling <rnowling@gmail.com> wrote:
>
>> Thanks, Sean.
>>
>> Yes, Spark is incorrectly copying the spark assembly jar to
>> com/google/guava in the maven repository.  This is for the 1.2.0 release,
>> just to clarify.
>>
>> I reverted the patches that shade Guava and removed the parts disabling
>> the install plugin and it seemed to fix the issue.
>>
>> It seems that Spark poms are inheriting something from Guava.
>>
>> RJ
>>
>> On Wed, Jan 14, 2015 at 2:33 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>>> Guava is shaded, although one class is left in its original package.
>>> This shouldn't have anything to do with Spark's package or namespace
>>> though. What are you saying is in com/google/guava?
>>>
>>> You can un-skip the install plugin with -Dmaven.install.skip=false
>>>
>>> On Wed, Jan 14, 2015 at 7:26 PM, RJ Nowling <rnowling@gmail.com> wrote:
>>> > Hi all,
>>> >
>>> > I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of
>>> the
>>> > RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
>>> > artifacts are  placed in com/google/guava and there is no
>>> org/apache/spark.
>>> >
>>> > I saw that the pom.xml files had been modified to prevent the install
>>> > command and that the guava dependency was modified.  Could someone who
>>> is
>>> > more familiar with the Spark maven files comment on what might be
>>> causing
>>> > this oddity?
>>> >
>>> > Thanks,
>>> > RJ
>>> >
>>> > We build Spark like so:
>>> > $ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package
>>> >
>>> > Then build a local Maven repo:
>>> >
>>> > mvn -Phadoop-2.4 \
>>> >     -Dmesos.version=0.20.0 \
>>> >     -DskipTests install:install-file  \
>>> >
>>>  -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
>>> >     -DcreateChecksum=true \
>>> >     -DgeneratePom=true \
>>> >     -DartifactId=spark-assembly_2.1.0 \
>>> >     -DlocalRepositoryPath=../maven2
>>>
>>
>>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 21:25:43 2015
Return-Path: <dev-return-11119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44CF717EBD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 21:25:43 +0000 (UTC)
Received: (qmail 40155 invoked by uid 500); 14 Jan 2015 21:25:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40082 invoked by uid 500); 14 Jan 2015 21:25:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40071 invoked by uid 99); 14 Jan 2015 21:25:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:25:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:25:39 +0000
Received: by mail-ob0-f181.google.com with SMTP id gq1so10298835obb.12
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 13:24:58 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=IHMk9/WVMvtfp6R4Q/eOX4nx4ewdT4SP6rApuKgaQ3c=;
        b=Tv6E3X2EhFB8+w44XWloH6wUwbZaPACo62FS7UHEQYUE5VAd6g2+FqtzcKcF8KLeKp
         oX6ocaD4t+HdloZV2WUUrKxatxmhgEye+t+wQ78CKendEzTwoZs00QfjryxzSOKkz3Sn
         kZm0fJxtJqBbMfKfULORdzPmmivQGE32f5WLeD7BDtx7VK/otaoHl1K0YVxKju0d930l
         mQcwaviXYZWwl0FRJ8f9ag0ruP04Z4/ikqfmUdvdELyW1BOGTEFa5eHDgP0YduswaQSr
         QmoZ+q7j8kdNYxjwTsAdZpn1B/QWyB7j+AS59Df77PR5zaZH6Nd94o+Pur0Ec8J/smQe
         ok2A==
X-Gm-Message-State: ALoCoQkRuZtutwF/eAnzN5MCBHuLRQK2kCXcCVOtB9eouhWsIaLXmD6kk8glz1gbBfkDBC12ERbF
MIME-Version: 1.0
X-Received: by 10.182.58.81 with SMTP id o17mr3864390obq.82.1421270697957;
 Wed, 14 Jan 2015 13:24:57 -0800 (PST)
Received: by 10.60.4.133 with HTTP; Wed, 14 Jan 2015 13:24:57 -0800 (PST)
In-Reply-To: <CAMQ+LQNWVaR4O9-HLGBShqQNq=BhS3raMpce9XGNXeDQyEM3kA@mail.gmail.com>
References: <CAMQ+LQN_Cm4VdtKWEW7doAu+76MaSpTimbubgbNaOTEf53P7Vg@mail.gmail.com>
	<CAJ4HpHGWs90XPo5cbvDupPMk-Tm=nA_aX26nzUgBuLUoAww-4w@mail.gmail.com>
	<CAMQ+LQPnhw3eKRxDAQzk5iFYFn2FXxvw+ygb+D-EBRVmU+hXYA@mail.gmail.com>
	<CAF7ADNqLu=w+Emc_6q0O0d_6QauSFmH22a2qNj41nPr6q+RDbw@mail.gmail.com>
	<CAMQ+LQNWVaR4O9-HLGBShqQNq=BhS3raMpce9XGNXeDQyEM3kA@mail.gmail.com>
Date: Wed, 14 Jan 2015 13:24:57 -0800
Message-ID: <CAF7ADNp+Us-Gp4mcpWGXc1Q-=UbxsO2d_0m7yxOVyviTMhCp8w@mail.gmail.com>
Subject: Re: K-Means And Class Tags
From: Joseph Bradley <joseph@databricks.com>
To: Devl Devel <devl.development@gmail.com>
Cc: Yana Kadiyska <yana.kadiyska@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f83a83b1502b5050ca35f75
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f83a83b1502b5050ca35f75
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

(After asking around,) retag() is private[spark] in Scala, but Java ignores
the "private[X]," making retag (unintentionally) public in Java.

Currently, your solution of retagging from Java is the best hack I can
think of.  It may take a bit of engineering to create a proper fix for the
long-term.
Joseph

On Fri, Jan 9, 2015 at 2:41 AM, Devl Devel <devl.development@gmail.com>
wrote:

> Hi Joseph
>
> Thanks for the suggestion, however retag is a private method and when I
> call in Scala:
>
> val retaggedInput =3D parsedData.retag(classOf[Vector])
>
> I get:
>
> Symbol retag is inaccessible from this place
>
> However I can do this from Java, and it works in Scala:
>
> return words.rdd().retag(Vector.class);
>
> Dev
>
>
>
> On Thu, Jan 8, 2015 at 9:35 PM, Joseph Bradley <joseph@databricks.com>
> wrote:
>
>> I believe you're running into an erasure issue which we found in
>> DecisionTree too.  Check out:
>>
>> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apa=
che/spark/mllib/tree/RandomForest.scala#L134
>>
>> That retags RDDs which were created from Java to prevent the exception
>> you're running into.
>>
>> Hope this helps!
>> Joseph
>>
>> On Thu, Jan 8, 2015 at 12:48 PM, Devl Devel <devl.development@gmail.com>
>> wrote:
>>
>>> Thanks for the suggestion, can anyone offer any advice on the ClassCast
>>> Exception going from Java to Scala? Why does JavaRDD.rdd() and then a
>>> collect() result in this exception?
>>>
>>> On Thu, Jan 8, 2015 at 4:13 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
>>> wrote:
>>>
>>> > How about
>>> >
>>> >
>>> data.map(s=3D>s.split(",")).filter(_.length>1).map(good_entry=3D>Vector=
s.dense((Double.parseDouble(good_entry[0]),
>>> > Double.parseDouble(good_entry[1]))
>>> > =E2=80=8B
>>> > (full disclosure, I didn't actually run this). But after the first ma=
p
>>> you
>>> > should have an RDD[Array[String]], then you'd discard everything
>>> shorter
>>> > than 2, and convert the rest to dense vectors?...In fact if you're
>>> > expecting length exactly 2 might want to filter =3D=3D2...
>>> >
>>> >
>>> > On Thu, Jan 8, 2015 at 10:58 AM, Devl Devel <
>>> devl.development@gmail.com>
>>> > wrote:
>>> >
>>> >> Hi All,
>>> >>
>>> >> I'm trying a simple K-Means example as per the website:
>>> >>
>>> >> val parsedData =3D data.map(s =3D>
>>> >> Vectors.dense(s.split(',').map(_.toDouble)))
>>> >>
>>> >> but I'm trying to write a Java based validation method first so that
>>> >> missing values are omitted or replaced with 0.
>>> >>
>>> >> public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
>>> >>         JavaRDD<Vector> words =3D data.flatMap(new
>>> FlatMapFunction<String,
>>> >> Vector>() {
>>> >>             public Iterable<Vector> call(String s) {
>>> >>                 String[] split =3D s.split(",");
>>> >>                 ArrayList<Vector> add =3D new ArrayList<Vector>();
>>> >>                 if (split.length !=3D 2) {
>>> >>                     add.add(Vectors.dense(0, 0));
>>> >>                 } else
>>> >>                 {
>>> >>
>>>  add.add(Vectors.dense(Double.parseDouble(split[0]),
>>> >>                Double.parseDouble(split[1])));
>>> >>                 }
>>> >>
>>> >>                 return add;
>>> >>             }
>>> >>         });
>>> >>
>>> >>         return words.rdd();
>>> >> }
>>> >>
>>> >> When I then call from scala:
>>> >>
>>> >> val parsedData=3Ddc.prepareKMeans(data);
>>> >> val p=3DparsedData.collect();
>>> >>
>>> >> I get Exception in thread "main" java.lang.ClassCastException:
>>> >> [Ljava.lang.Object; cannot be cast to
>>> >> [Lorg.apache.spark.mllib.linalg.Vector;
>>> >>
>>> >> Why is the class tag is object rather than vector?
>>> >>
>>> >> 1) How do I get this working correctly using the Java validation
>>> example
>>> >> above or
>>> >> 2) How can I modify val parsedData =3D data.map(s =3D>
>>> >> Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split
>>> size <2
>>> >> I
>>> >> ignore the line? or
>>> >> 3) Is there a better way to do input validation first?
>>> >>
>>> >> Using spark and mlib:
>>> >> libraryDependencies +=3D "org.apache.spark" % "spark-core_2.10" %
>>> "1.2.0"
>>> >> libraryDependencies +=3D "org.apache.spark" % "spark-mllib_2.10" %
>>> "1.2.0"
>>> >>
>>> >> Many thanks in advance
>>> >> Dev
>>> >>
>>> >
>>> >
>>>
>>
>>
>

--e89a8f83a83b1502b5050ca35f75--

From dev-return-11120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 21:42:11 2015
Return-Path: <dev-return-11120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38AD917F42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 21:42:11 +0000 (UTC)
Received: (qmail 74851 invoked by uid 500); 14 Jan 2015 21:42:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74764 invoked by uid 500); 14 Jan 2015 21:42:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74750 invoked by uid 99); 14 Jan 2015 21:42:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:42:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:41:46 +0000
Received: by mail-wi0-f179.google.com with SMTP id ho1so5482981wib.0
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 13:40:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=XoAdw6isDnqGyAelnMIO4AjVhu6Oo7Twbj82xL8vAZk=;
        b=X0Vzwp77VibyVX8+SlxoiIccOlvXZSNhp+aXw5Op4XWQg/OTeTl50HF0GccY/4H3rG
         A791kUh3aPYcJTgs+QrSiMLkSvKagIXIFX8QqaI5GlksPS98iYVGy3yWbh3FT2Rb+cS+
         kLRcu4PoXjIzHJIZxRaDqR2ZkCg7r85lIdbmIHGyXjkUm/IWEIrD3+zHr57E0lI2ErEr
         qvpYk1pfimKiOR9ZFmANDHCzF09RgRtug1YNqJoMAgvveNsckX0BvxqO2D2ajlK3Pw1p
         1iISBBg4rM1odtpF8DVKW1ijRdkeYfwLST/xCkERKlv1Nuw++KKD36KoGIs8MJQry71p
         pAig==
MIME-Version: 1.0
X-Received: by 10.180.73.143 with SMTP id l15mr53446240wiv.24.1421271615289;
 Wed, 14 Jan 2015 13:40:15 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Wed, 14 Jan 2015 13:40:15 -0800 (PST)
In-Reply-To: <CAAOnQ7upx9wyLWiuF2pmS3hbO447rJbS-gjTtTf_Mor6+vqTfA@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
	<CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
	<CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
	<CADtDQQ+TxvyVaG0WTLfrD_8huc47KqkbfZGMoguTsUH+dJCqGw@mail.gmail.com>
	<CAAOnQ7upx9wyLWiuF2pmS3hbO447rJbS-gjTtTf_Mor6+vqTfA@mail.gmail.com>
Date: Wed, 14 Jan 2015 16:40:15 -0500
Message-ID: <CADtDQQKBQL+RRf+BnHGcPLqSGVO9X5kDoBaAZiZ=ZQGMeuAz8w@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
From: RJ Nowling <rnowling@gmail.com>
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c06bcc240e3050ca3952a
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c06bcc240e3050ca3952a
Content-Type: text/plain; charset=UTF-8

Thanks, Marcelo!

I'll look into "install" vs "install-file".

What is the difference between pom and jar packaging?

One of the challenges is that I have to satisfy Fedora / Red Hat packaging
guidelines, which makes life a little more interesting. :)  (e.g., RPMs
should resolve against other RPMs instead of external repositories.)

On Wed, Jan 14, 2015 at 4:16 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:

> Hi RJ,
>
> I think I remember noticing in the past that some Guava metadata ends
> up overwriting maven-generated metadata in the assembly's manifest.
> That's probably something we should fix if that still affects the
> build.
>
> That being said, this is probably happening because you're using
> "install-file" instead of "install". If you want a workaround that
> doesn't require unshading things, you can change assembly.pom to (i)
> not skip the install plugin and (ii) have "jar" as the packaging,
> instead of pom.
>
>
>
> On Wed, Jan 14, 2015 at 1:08 PM, RJ Nowling <rnowling@gmail.com> wrote:
> > Hi Sean,
> >
> > I confirmed that if I take the Spark 1.2.0 release (a428c446), undo the
> > guava PR [1], and use -Dmaven.install.skip=false with the workflow above,
> > the problem is fixed.
> >
> > RJ
> >
> >
> > [1]
> >
> https://github.com/apache/spark/commit/c9f743957fa963bc1dbed7a44a346ffce1a45cf2#diff-6382f8428b13fa6082fa688178f3dbcc
> >
> > On Wed, Jan 14, 2015 at 2:59 PM, RJ Nowling <rnowling@gmail.com> wrote:
> >
> >> Thanks, Sean.
> >>
> >> Yes, Spark is incorrectly copying the spark assembly jar to
> >> com/google/guava in the maven repository.  This is for the 1.2.0
> release,
> >> just to clarify.
> >>
> >> I reverted the patches that shade Guava and removed the parts disabling
> >> the install plugin and it seemed to fix the issue.
> >>
> >> It seems that Spark poms are inheriting something from Guava.
> >>
> >> RJ
> >>
> >> On Wed, Jan 14, 2015 at 2:33 PM, Sean Owen <sowen@cloudera.com> wrote:
> >>
> >>> Guava is shaded, although one class is left in its original package.
> >>> This shouldn't have anything to do with Spark's package or namespace
> >>> though. What are you saying is in com/google/guava?
> >>>
> >>> You can un-skip the install plugin with -Dmaven.install.skip=false
> >>>
> >>> On Wed, Jan 14, 2015 at 7:26 PM, RJ Nowling <rnowling@gmail.com>
> wrote:
> >>> > Hi all,
> >>> >
> >>> > I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part
> of
> >>> the
> >>> > RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
> >>> > artifacts are  placed in com/google/guava and there is no
> >>> org/apache/spark.
> >>> >
> >>> > I saw that the pom.xml files had been modified to prevent the install
> >>> > command and that the guava dependency was modified.  Could someone
> who
> >>> is
> >>> > more familiar with the Spark maven files comment on what might be
> >>> causing
> >>> > this oddity?
> >>> >
> >>> > Thanks,
> >>> > RJ
> >>> >
> >>> > We build Spark like so:
> >>> > $ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package
> >>> >
> >>> > Then build a local Maven repo:
> >>> >
> >>> > mvn -Phadoop-2.4 \
> >>> >     -Dmesos.version=0.20.0 \
> >>> >     -DskipTests install:install-file  \
> >>> >
> >>>
> -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
> >>> >     -DcreateChecksum=true \
> >>> >     -DgeneratePom=true \
> >>> >     -DartifactId=spark-assembly_2.1.0 \
> >>> >     -DlocalRepositoryPath=../maven2
> >>>
> >>
> >>
>
>
>
> --
> Marcelo
>

--f46d043c06bcc240e3050ca3952a--

From dev-return-11121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 21:46:26 2015
Return-Path: <dev-return-11121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C3B5117F7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 21:46:26 +0000 (UTC)
Received: (qmail 87350 invoked by uid 500); 14 Jan 2015 21:46:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87271 invoked by uid 500); 14 Jan 2015 21:46:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87260 invoked by uid 99); 14 Jan 2015 21:46:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:46:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 21:46:01 +0000
Received: by mail-qc0-f175.google.com with SMTP id p6so9499443qcv.6
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 13:43:44 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=+dRVeIpwhFa3+6ehccF6+48K9POdN8rqJaf/ZIZow74=;
        b=ehxnR8QUkmPaIqza/Z2mwovPtxlqp/cL/d8zBmV/OWv7gLr8cFlElbPKgcPUKooLtG
         XK4CaWwBqLfPbdHv80wd7ru3ZakPqHZZmhzF39R8JUATflP/1Jkdosm6gRRLXt2y5Ix6
         WYgs1hIGiitSK2lB/1cip9O/PIR0AB2tTtFfQr779O/CJAQb+8EY6jKALVILI9gGdFD0
         TfTWDCc3GS3vEuDCmcmqBDezxX1cKynwokL1VzxHuxa4wtSKVAiC0y7ABsbx7aCaTf+n
         9h5ynLA9r9XDzQP0Vl2AJoKKa8RCTo1Gs4MzW0jfI+RmuQlMDxPq3h5UCGgZt4ppBUJg
         20yw==
X-Gm-Message-State: ALoCoQlYtOA6SWf60vIvoz+4frU0RTvZv4cU2Gt4rZc9gychyJClk6ZggJ/0Si3SQ/mOTcBlSoQk
MIME-Version: 1.0
X-Received: by 10.140.102.72 with SMTP id v66mr1157496qge.31.1421271824479;
 Wed, 14 Jan 2015 13:43:44 -0800 (PST)
Received: by 10.229.155.2 with HTTP; Wed, 14 Jan 2015 13:43:44 -0800 (PST)
In-Reply-To: <CADtDQQKBQL+RRf+BnHGcPLqSGVO9X5kDoBaAZiZ=ZQGMeuAz8w@mail.gmail.com>
References: <CADtDQQJYkokCnZmW18dr2bUYJ4=px=nYmP0Zj7SoHDvDhO4SgQ@mail.gmail.com>
	<CAMAsSd+mxHsofkFkk4L7_o9Eo_Q=kQQ3AhKeiYC1+wS0RsLZqQ@mail.gmail.com>
	<CADtDQQJarkk4Z5FAfnVcXKLRQ7=5yL2m+sOPtEjXCJVMFKsVpQ@mail.gmail.com>
	<CADtDQQ+TxvyVaG0WTLfrD_8huc47KqkbfZGMoguTsUH+dJCqGw@mail.gmail.com>
	<CAAOnQ7upx9wyLWiuF2pmS3hbO447rJbS-gjTtTf_Mor6+vqTfA@mail.gmail.com>
	<CADtDQQKBQL+RRf+BnHGcPLqSGVO9X5kDoBaAZiZ=ZQGMeuAz8w@mail.gmail.com>
Date: Wed, 14 Jan 2015 13:43:44 -0800
Message-ID: <CAAOnQ7v7jdpo2+hvXGuoMgEEy8xH1eY0fYF+cfTGLovTzK72Kg@mail.gmail.com>
Subject: Re: Incorrect Maven Artifact Names
From: Marcelo Vanzin <vanzin@cloudera.com>
To: RJ Nowling <rnowling@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Jan 14, 2015 at 1:40 PM, RJ Nowling <rnowling@gmail.com> wrote:
> What is the difference between pom and jar packaging?

If you do an install on a "pom" packaging module, it will only install
the module's pom file in the target repository.

-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 14 23:56:06 2015
Return-Path: <dev-return-11122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 82FCB10652
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 Jan 2015 23:56:06 +0000 (UTC)
Received: (qmail 43355 invoked by uid 500); 14 Jan 2015 23:56:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43290 invoked by uid 500); 14 Jan 2015 23:56:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43273 invoked by uid 99); 14 Jan 2015 23:56:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 23:56:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 Jan 2015 23:55:40 +0000
Received: by mail-ig0-f172.google.com with SMTP id l13so12367258iga.5
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 15:55:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=Y70l2+lal8BIKZRxbkLI3Pyr8+WFvlmitFyky0oaf9c=;
        b=C+V13F52CP1ThBQOzTna9JEiX4DIt5Ttxd4C2y8hfGVQCHGNsTITKSjTQ46s/qre7h
         IwwSk07veF1eEH4ttAVKhLQErncZv7YDVbRqri98Y7SA0pc5i4H7AZHd8t8MiHvNnZtp
         LY1LY3fBzaF2qemvmmrHSWWiJTUPeQYPZd0YpcF9sMkXmsfH7YebK3RCFxEplGqw8S94
         bAsHHcX8u3w4oYhA16ESyMguHjvuZUMLXT6GYOFPQBNTasAs24aVm0KKezak4jwSjexE
         AEkhVlFI2YIMApSWDgdk/9TiXp9eC1qkAwXPH8sdHz5tRPVuwr97lM2pTJvXLdtdkl0V
         UYRw==
MIME-Version: 1.0
X-Received: by 10.50.78.202 with SMTP id d10mr7433860igx.30.1421279739002;
 Wed, 14 Jan 2015 15:55:39 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Wed, 14 Jan 2015 15:55:38 -0800 (PST)
In-Reply-To: <32085278.400753.1421185748959.JavaMail.yahoo@jws100154.mail.ne1.yahoo.com>
References: <1551808826.393105.1421185185848.JavaMail.yahoo@jws100161.mail.ne1.yahoo.com>
	<32085278.400753.1421185748959.JavaMail.yahoo@jws100154.mail.ne1.yahoo.com>
Date: Wed, 14 Jan 2015 15:55:38 -0800
Message-ID: <CAJgQjQ_-MuN=oDJHkjsSUsEyeH68aJhwXmzmrad6AFAATuftzQ@mail.gmail.com>
Subject: Re: DBSCAN for MLlib
From: Xiangrui Meng <mengxr@gmail.com>
To: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Please find my comments on the JRIA page. -Xiangrui

On Tue, Jan 13, 2015 at 1:49 PM, Muhammad Ali A'r=C3=A5by
<angellandros@yahoo.com.invalid> wrote:
> I have to say, I have created a Jira task for it:
> [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRA
>
> |   |
> |   |   |   |   |   |
> | [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRAMLlib i=
s all k-means now, and I think we should add some new clustering algorithms=
 to it. First candidate is DBSCAN as I think.  |
> |  |
> | View on issues.apache.org | Preview by Yahoo |
> |  |
> |   |
>
>
>
>      On Wednesday, January 14, 2015 1:09 AM, Muhammad Ali A'r=C3=A5by <an=
gellandros@yahoo.com> wrote:
>
>
>  Dear all,
> I think MLlib needs more clustering algorithms and DBSCAN is my first can=
didate. I am starting to implement it. Any advice?
> Muhammad-Ali
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 02:09:13 2015
Return-Path: <dev-return-11123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED0A410B20
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 02:09:12 +0000 (UTC)
Received: (qmail 94649 invoked by uid 500); 15 Jan 2015 02:09:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94575 invoked by uid 500); 15 Jan 2015 02:09:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94511 invoked by uid 99); 15 Jan 2015 02:09:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:09:13 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chris.a.mattmann@jpl.nasa.gov designates 128.149.139.106 as permitted sender)
Received: from [128.149.139.106] (HELO mail.jpl.nasa.gov) (128.149.139.106)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:09:08 +0000
Received: from mail.jpl.nasa.gov (ap-ehub-sp01.jpl.nasa.gov [128.149.137.148])
	by smtp.jpl.nasa.gov (Sentrion-MTA-4.3.1/Sentrion-MTA-4.3.1) with ESMTP id t0F28k0Z015166
	(using TLSv1/SSLv3 with cipher AES128-SHA (128 bits) verified NO)
	for <dev@spark.apache.org>; Wed, 14 Jan 2015 18:08:47 -0800
Received: from AP-EMBX-SP40.RES.AD.JPL ([169.254.7.214]) by
 ap-ehub-sp01.RES.AD.JPL ([169.254.3.89]) with mapi id 14.03.0210.002; Wed, 14
 Jan 2015 18:08:46 -0800
From: "Mattmann, Chris A (3980)" <chris.a.mattmann@jpl.nasa.gov>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: SciSpark: NASA AIST14 proposal
Thread-Topic: SciSpark: NASA AIST14 proposal
Thread-Index: AQHQMGgxmDC4xeTdNEqPK8DY/RXToA==
Date: Thu, 15 Jan 2015 02:08:45 +0000
Message-ID: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.7.141117
x-originating-ip: [128.149.137.113]
Content-Type: text/plain; charset="utf-8"
Content-ID: <3D55D1FEE3BD9047A5FDFD9B88A1D043@ad.jpl>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Source-Sender: chris.a.mattmann@jpl.nasa.gov
X-AUTH: Authorized
X-Virus-Checked: Checked by ClamAV on apache.org

SGkgU3BhcmsgRGV2cywNCg0KSnVzdCB3YW50ZWQgdG8gRllJIHRoYXQgSSB3YXMgZnVuZGVkIG9u
IGEgMiB5ZWFyIE5BU0EgcHJvcG9zYWwNCnRvIGJ1aWxkIG91dCB0aGUgY29uY2VwdCBvZiBhIHNj
aWVudGlmaWMgUkREIChjcmVhdGUgYnkgc3BhY2UvdGltZSwNCmFuZCBvdGhlciBvcGVyYXRpb25z
KSBmb3IgdXNlIGluIHNvbWUgbmVhdCBjbGltYXRlIHJlbGF0ZWQgTkFTQQ0KdXNlIGNhc2VzLg0K
DQpodHRwOi8vZXN0by5uYXNhLmdvdi9maWxlcy9zb2xpY2l0YXRpb25zL0FJU1RfMTQvUk9TRVMy
MDE0X0FJU1RfQTQxX2F3YXJkcy4NCmh0bWwNCg0KDQpJIHdpbGwga2VlcCBldmVyeW9uZSBwb3N0
ZWQgYW5kIHBsYW4gb24gaW50ZXJhY3Rpbmcgd2l0aCB0aGUgbGlzdA0Kb3ZlciBoZXJlIHRvIGdl
dCBpdCBkb25lLiBJIGV4cGVjdCB0aGF0IHdl4oCZbGwgc3RhcnQgd29yayBpbiBNYXJjaC4NCklu
IHRoZSBtZWFud2hpbGUgeW91IGd1eXMgY2FuIHNjb3BlIHRoZSBhYnN0cmFjdCBhdCB0aGUgbGlu
ayBwcm92aWRlZC4NCkhhcHB5DQp0byBjaGF0IGFib3V0IGl0IGlmIHlvdSBoYXZlIGFueSBxdWVz
dGlvbnMgdG9vLg0KDQpDaGVlcnMhDQoNCkNocmlzDQoNCisrKysrKysrKysrKysrKysrKysrKysr
KysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKw0KQ2hyaXMgTWF0dG1h
bm4sIFBoLkQuDQpDaGllZiBBcmNoaXRlY3QNCkluc3RydW1lbnQgU29mdHdhcmUgYW5kIFNjaWVu
Y2UgRGF0YSBTeXN0ZW1zIFNlY3Rpb24gKDM5OCkNCk5BU0EgSmV0IFByb3B1bHNpb24gTGFib3Jh
dG9yeSBQYXNhZGVuYSwgQ0EgOTExMDkgVVNBDQpPZmZpY2U6IDE2OC01MTksIE1haWxzdG9wOiAx
NjgtNTI3DQpFbWFpbDogY2hyaXMuYS5tYXR0bWFubkBuYXNhLmdvdg0KV1dXOiAgaHR0cDovL3N1
bnNldC51c2MuZWR1L35tYXR0bWFubi8NCisrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysr
KysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKw0KQWRqdW5jdCBBc3NvY2lhdGUgUHJv
ZmVzc29yLCBDb21wdXRlciBTY2llbmNlIERlcGFydG1lbnQNClVuaXZlcnNpdHkgb2YgU291dGhl
cm4gQ2FsaWZvcm5pYSwgTG9zIEFuZ2VsZXMsIENBIDkwMDg5IFVTQQ0KKysrKysrKysrKysrKysr
KysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrDQoNCg0K
DQoNCg==
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 02:20:50 2015
Return-Path: <dev-return-11124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 321EA10B9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 02:20:50 +0000 (UTC)
Received: (qmail 11351 invoked by uid 500); 15 Jan 2015 02:20:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11273 invoked by uid 500); 15 Jan 2015 02:20:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11260 invoked by uid 99); 15 Jan 2015 02:20:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:20:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:20:25 +0000
Received: by mail-qa0-f43.google.com with SMTP id v10so9419897qac.2
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 18:18:32 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=EHJTzF630yvya/Zgu53UiTa+QoyF65P3mWjb310cisI=;
        b=XVicOw56slT6zaX38IHY6ZEPN9HUDjk4EoJn8CccMuu2+OU9Kn0yIpeURg/PKJQqK1
         uPFfh+mMqxnyxzw837qLxrdRHQ4vUzILRK1RATXxngyO1TlsysN1Qg1OGQbrQqsJ9QCd
         hN1NHPak719NkxQUPICc07qMwUuFlH6AgADt+MvXR0fw3BPzW3/leCAplejVb7psvpot
         3c26X6kGlgToZLvtrDQThO3WOzmUBILsQG65popyTUemUAej2x0684zKAGZjnC/INyS1
         q3ca0Em0GCpLXpEllx9qrWHSOr9LN+vCKEEbbDEmXH0SyGSJJJ/WtNVMbE0CtfUZJovH
         gNcg==
X-Gm-Message-State: ALoCoQnVf5eSLYr5piNIyCN82iopuzyqDbVogTdcjCTJJqVFdJoGkVdOjnFKXqWxBjHVTnJYPg8z
X-Received: by 10.224.92.205 with SMTP id s13mr12626508qam.52.1421288312789;
 Wed, 14 Jan 2015 18:18:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 14 Jan 2015 18:18:11 -0800 (PST)
In-Reply-To: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
References: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 14 Jan 2015 18:18:11 -0800
Message-ID: <CAPh_B=a3Rq2Er9bwV0F+g0MLtDBXE4aQxDCVFmw=t=_znvwiig@mail.gmail.com>
Subject: Re: SciSpark: NASA AIST14 proposal
To: "Mattmann, Chris A (3980)" <chris.a.mattmann@jpl.nasa.gov>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e015375e401f45c050ca779f2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015375e401f45c050ca779f2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Chris,

This is really cool. Congratulations and thanks for sharing the news.


On Wed, Jan 14, 2015 at 6:08 PM, Mattmann, Chris A (3980) <
chris.a.mattmann@jpl.nasa.gov> wrote:

> Hi Spark Devs,
>
> Just wanted to FYI that I was funded on a 2 year NASA proposal
> to build out the concept of a scientific RDD (create by space/time,
> and other operations) for use in some neat climate related NASA
> use cases.
>
> http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_award=
s.
> html
>
>
> I will keep everyone posted and plan on interacting with the list
> over here to get it done. I expect that we=E2=80=99ll start work in March=
.
> In the meanwhile you guys can scope the abstract at the link provided.
> Happy
> to chat about it if you have any questions too.
>
> Cheers!
>
> Chris
>
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> Chris Mattmann, Ph.D.
> Chief Architect
> Instrument Software and Science Data Systems Section (398)
> NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
> Office: 168-519, Mailstop: 168-527
> Email: chris.a.mattmann@nasa.gov
> WWW:  http://sunset.usc.edu/~mattmann/
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> Adjunct Associate Professor, Computer Science Department
> University of Southern California, Los Angeles, CA 90089 USA
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
>
>
>
>

--089e015375e401f45c050ca779f2--

From dev-return-11125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 02:39:45 2015
Return-Path: <dev-return-11125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 22ACE10C3B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 02:39:45 +0000 (UTC)
Received: (qmail 51849 invoked by uid 500); 15 Jan 2015 02:39:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51761 invoked by uid 500); 15 Jan 2015 02:39:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51749 invoked by uid 99); 15 Jan 2015 02:39:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:39:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 02:39:41 +0000
Received: by mail-pd0-f178.google.com with SMTP id r10so13425437pdi.9
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 18:39:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=DysptHwuK92rs2pER43vccAroPtySpbwmUQ86tLcOPU=;
        b=dbmGwP6dcaOQJgG4JTD/zfK4z4m5wg4fBZ6/Wq329YHegeHOdShoj0bWtjoyDRUbg8
         AUv1L94kJZs0Y/Ie0DjsgBRTQRmlHa4ny+UwAQoSLQBjOIpcd542ips+Hy5/OZKaYD82
         dDoS3rHAuf23YPKfSKmZdX5OfxdlHLPPJr7ufe+4DYW+BanTE0dvHqtiCRjzi90+mQ2I
         exABOzsC3Usr4/PSCTbYT0SGci8QZ8eCT/nFUHvHWXDdo++DtqbX2gSsrxLn3+fRNYE9
         DUV72WPykOVbBT7YH9djR2StWxLpWL0RQNhrcb7odGWQOvW/ymUCmAt71e4T7u6TRG1B
         vaBw==
X-Received: by 10.66.254.68 with SMTP id ag4mr10528258pad.39.1421289560667;
        Wed, 14 Jan 2015 18:39:20 -0800 (PST)
Received: from [192.168.1.100] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id ob4sm120307pdb.48.2015.01.14.18.39.19
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 14 Jan 2015 18:39:19 -0800 (PST)
Content-Type: text/plain; charset=utf-8
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: SciSpark: NASA AIST14 proposal
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAPh_B=a3Rq2Er9bwV0F+g0MLtDBXE4aQxDCVFmw=t=_znvwiig@mail.gmail.com>
Date: Wed, 14 Jan 2015 18:39:18 -0800
Cc: "Mattmann, Chris A (3980)" <chris.a.mattmann@jpl.nasa.gov>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <D31556F3-1D3C-403F-9B10-2A68439CCA4E@gmail.com>
References: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov> <CAPh_B=a3Rq2Er9bwV0F+g0MLtDBXE4aQxDCVFmw=t=_znvwiig@mail.gmail.com>
To: Reynold Xin <rxin@databricks.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, very cool! You may also want to check out =
https://issues.apache.org/jira/browse/SPARK-5097 as something to build =
upon for these operations.

Matei

> On Jan 14, 2015, at 6:18 PM, Reynold Xin <rxin@databricks.com> wrote:
>=20
> Chris,
>=20
> This is really cool. Congratulations and thanks for sharing the news.
>=20
>=20
> On Wed, Jan 14, 2015 at 6:08 PM, Mattmann, Chris A (3980) <
> chris.a.mattmann@jpl.nasa.gov> wrote:
>=20
>> Hi Spark Devs,
>>=20
>> Just wanted to FYI that I was funded on a 2 year NASA proposal
>> to build out the concept of a scientific RDD (create by space/time,
>> and other operations) for use in some neat climate related NASA
>> use cases.
>>=20
>> =
http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_awards=
.
>> html
>>=20
>>=20
>> I will keep everyone posted and plan on interacting with the list
>> over here to get it done. I expect that we=E2=80=99ll start work in =
March.
>> In the meanwhile you guys can scope the abstract at the link =
provided.
>> Happy
>> to chat about it if you have any questions too.
>>=20
>> Cheers!
>>=20
>> Chris
>>=20
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> Chris Mattmann, Ph.D.
>> Chief Architect
>> Instrument Software and Science Data Systems Section (398)
>> NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
>> Office: 168-519, Mailstop: 168-527
>> Email: chris.a.mattmann@nasa.gov
>> WWW:  http://sunset.usc.edu/~mattmann/
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> Adjunct Associate Professor, Computer Science Department
>> University of Southern California, Los Angeles, CA 90089 USA
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>>=20
>>=20
>>=20
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 04:07:32 2015
Return-Path: <dev-return-11126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45BA510F8D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 04:07:32 +0000 (UTC)
Received: (qmail 1707 invoked by uid 500); 15 Jan 2015 04:07:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1587 invoked by uid 500); 15 Jan 2015 04:07:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1576 invoked by uid 99); 15 Jan 2015 04:07:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 04:07:31 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of aniket.bhatnagar@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 04:07:27 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id CC79E1091D8F
	for <dev@spark.apache.org>; Wed, 14 Jan 2015 20:07:07 -0800 (PST)
Date: Wed, 14 Jan 2015 21:07:06 -0700 (MST)
From: Aniket <aniket.bhatnagar@gmail.com>
To: dev@spark.apache.org
Message-ID: <CAJOb8btc6R5qntvK2Lr=ThOp3ysDzygeEzHKtzUuwG2fRoV68A@mail.gmail.com>
In-Reply-To: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
References: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
Subject: Re: SciSpark: NASA AIST14 proposal
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_102206_560181223.1421294826922"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_102206_560181223.1421294826922
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Chris

This is super cool. I was wondering if this would be an open source project
so that people can contribute or reuse?

Thanks,
Aniket

On Thu Jan 15 2015 at 07:39:29 Mattmann, Chris A (3980) [via Apache Spark
Developers List] <ml-node+s1001551n10115h10@n3.nabble.com> wrote:

> Hi Spark Devs,
>
> Just wanted to FYI that I was funded on a 2 year NASA proposal
> to build out the concept of a scientific RDD (create by space/time,
> and other operations) for use in some neat climate related NASA
> use cases.
>
> http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_award=
s.
>
> html
>
>
> I will keep everyone posted and plan on interacting with the list
> over here to get it done. I expect that we=E2=80=99ll start work in March=
.
> In the meanwhile you guys can scope the abstract at the link provided.
> Happy
> to chat about it if you have any questions too.
>
> Cheers!
>
> Chris
>
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> Chris Mattmann, Ph.D.
> Chief Architect
> Instrument Software and Science Data Systems Section (398)
> NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
> Office: 168-519, Mailstop: 168-527
> Email: [hidden email]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D0>
> WWW:  http://sunset.usc.edu/~mattmann/
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> Adjunct Associate Professor, Computer Science Department
> University of Southern California, Los Angeles, CA 90089 USA
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
>
>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D1>
> For additional commands, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D2>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/SciSpark-NASA-A=
IST14-proposal-tp10115.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h76@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3DYW5pa2V0LmJoYXRuYWdh=
ckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabb=
leNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_sub=
scribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_in=
stant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/SciSpark-NASA-AIST14-proposal-tp10115p10118.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_102206_560181223.1421294826922--

From dev-return-11127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 04:53:26 2015
Return-Path: <dev-return-11127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90894172F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 04:53:26 +0000 (UTC)
Received: (qmail 61613 invoked by uid 500); 15 Jan 2015 04:53:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61531 invoked by uid 500); 15 Jan 2015 04:53:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61519 invoked by uid 99); 15 Jan 2015 04:53:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 04:53:25 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 04:53:21 +0000
Received: by mail-wg0-f42.google.com with SMTP id k14so12701192wgh.1
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 20:51:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pPyZPUrAQF+hUHaiYqG8U+RVsEw1W8WBfuRbeRMbOLU=;
        b=EybI7kICtdjEB3fRF2i2J69JkTN3bLSKjV3JUpdGPFp6cL6HfegVIS3eVsQXAYUV7n
         w111IaRtWnvP9hL/WwajjLm6COkaG+LP4o3CT6evRFVm60WGxqnW6MZ38ol1HeivXWV3
         aLTizbpQYLimruQ5SpI2vG/s9C7G7OV91aLqnxSskyIL2496FFD1BhI3BwOHbgCXRl7m
         CDMQkzkPQRWRGAZEfsv3TQpF+NFZ3q2qg0uSWtcCVMhg39hH7hOHKPfDVO6KczpZKp1F
         GlTVsSDNZKHNsaxRAuijDAYBoo4hu/rdgjAreVO8hmPDRmkLe8ON8xjVh0zPNcxjlEae
         qsww==
MIME-Version: 1.0
X-Received: by 10.194.206.70 with SMTP id lm6mr14169919wjc.30.1421297490469;
 Wed, 14 Jan 2015 20:51:30 -0800 (PST)
Received: by 10.194.24.39 with HTTP; Wed, 14 Jan 2015 20:51:30 -0800 (PST)
In-Reply-To: <CAJOb8btc6R5qntvK2Lr=ThOp3ysDzygeEzHKtzUuwG2fRoV68A@mail.gmail.com>
References: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
	<CAJOb8btc6R5qntvK2Lr=ThOp3ysDzygeEzHKtzUuwG2fRoV68A@mail.gmail.com>
Date: Wed, 14 Jan 2015 23:51:30 -0500
Message-ID: <CADtDQQK_9pDiMF20AVn82=0QCbK6a+jRTUpbTE6-fcFNqVGOJQ@mail.gmail.com>
Subject: Re: SciSpark: NASA AIST14 proposal
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb70b660a2aca050ca99c96
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb70b660a2aca050ca99c96
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Congratulations, Chris!

I created a JIRA for "dimensional" RDDs that might be relevant:
https://issues.apache.org/jira/browse/SPARK-4727

Jeremy Freeman pointed me to his lab's work on for neuroscience that have
some related functionality :
http://thefreemanlab.com/thunder/

On Wed, Jan 14, 2015 at 11:07 PM, Aniket <aniket.bhatnagar@gmail.com> wrote=
:

> Hi Chris
>
> This is super cool. I was wondering if this would be an open source proje=
ct
> so that people can contribute or reuse?
>
> Thanks,
> Aniket
>
> On Thu Jan 15 2015 at 07:39:29 Mattmann, Chris A (3980) [via Apache Spark
> Developers List] <ml-node+s1001551n10115h10@n3.nabble.com> wrote:
>
> > Hi Spark Devs,
> >
> > Just wanted to FYI that I was funded on a 2 year NASA proposal
> > to build out the concept of a scientific RDD (create by space/time,
> > and other operations) for use in some neat climate related NASA
> > use cases.
> >
> >
> http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_award=
s
> .
> >
> > html
> >
> >
> > I will keep everyone posted and plan on interacting with the list
> > over here to get it done. I expect that we=E2=80=99ll start work in Mar=
ch.
> > In the meanwhile you guys can scope the abstract at the link provided.
> > Happy
> > to chat about it if you have any questions too.
> >
> > Cheers!
> >
> > Chris
> >
> > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > Chris Mattmann, Ph.D.
> > Chief Architect
> > Instrument Software and Science Data Systems Section (398)
> > NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
> > Office: 168-519, Mailstop: 168-527
> > Email: [hidden email]
> > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D0>
> > WWW:  http://sunset.usc.edu/~mattmann/
> > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > Adjunct Associate Professor, Computer Science Department
> > University of Southern California, Los Angeles, CA 90089 USA
> > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> >
> >
> >
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: [hidden email]
> > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D1>
> > For additional commands, e-mail: [hidden email]
> > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D2>
> >
> >
> > ------------------------------
> >  If you reply to this email, your message will be added to the discussi=
on
> > below:
> >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/SciSpark-NASA-A=
IST14-proposal-tp10115.html
> >  To start a new topic under Apache Spark Developers List, email
> > ml-node+s1001551n1h76@n3.nabble.com
> > To unsubscribe from Apache Spark Developers List, click here
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3DYW5pa2V0LmJoYXRuYWdhc=
kBnbWFpbC5jb218MXwxMzE3NTAzMzQz
> >
> > .
> > NAML
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=
=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNam=
espace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabbl=
eNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subs=
cribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_ins=
tant_email%21nabble%3Aemail.naml
> >
> >
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/SciSpark-NASA-A=
IST14-proposal-tp10115p10118.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.

--047d7bb70b660a2aca050ca99c96--

From dev-return-11128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 05:47:18 2015
Return-Path: <dev-return-11128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A54D917416
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 05:47:18 +0000 (UTC)
Received: (qmail 38420 invoked by uid 500); 15 Jan 2015 05:47:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38347 invoked by uid 500); 15 Jan 2015 05:47:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38336 invoked by uid 99); 15 Jan 2015 05:47:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 05:47:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 05:46:52 +0000
Received: by mail-qg0-f42.google.com with SMTP id q108so10376998qgd.1
        for <dev@spark.apache.org>; Wed, 14 Jan 2015 21:45:45 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=YgMMRLsha5V+9CykVkNMRc9qCOrO9HYrTejvzcJf1PU=;
        b=NNj63EQV0VZy80sqv62YkMFewaWz+CkQ2HcO9nUjhMZtWxUU5Gk3Tt3zKLEZimsJ7L
         MyJ9q+Me7aoq2TqPoLuUFj19ncv7SPn/0G/myMQxzdvn/dGiTkti97+YnAIDWbd+fIOj
         hmepSainBOOyvsVLpKPWceeCCMG5n0GfSEGJcXdxlTpM5smuQv7aTMitlNv1o6Q3PPQc
         F/uj1F0Cz/4+WJJ+pN6iFPBelp6xazh/nUgjz8RDZD5I+eJ/b+1ZBFfhapJW8fL+r9Zf
         fcvpmNLnccSqCMCHGA4AzKSzhalwOE72gYbJ4scktM2V0YB38gcrp0x1YOC4MMoaXO9l
         BXWQ==
X-Gm-Message-State: ALoCoQkvysjMUJGc5k6E2MGwouxebDtDAeCVwSh/6cYfwC/JfVir8WhcvXENTNSDMaOWM4vNyMvW
X-Received: by 10.140.43.195 with SMTP id e61mr3554486qga.13.1421300745048;
 Wed, 14 Jan 2015 21:45:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 14 Jan 2015 21:45:24 -0800 (PST)
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 14 Jan 2015 21:45:24 -0800
Message-ID: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
Subject: Spark SQL API changes and stabilization
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a74ea0735c1050caa5e3c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a74ea0735c1050caa5e3c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Spark devs,

Given the growing number of developers that are building on Spark SQL, we
would like to stabilize the API in 1.3 so users and developers can be
confident to build on it. This also gives us a chance to improve the API.

In particular, we are proposing the following major changes. This should
have no impact for most users (i.e. those running SQL through the JDBC
client or SQLContext.sql method).

1. Everything in sql.catalyst package is private to the project.

2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
SchemaRDD and logical plans in order to construct test cases. We have
received feedback from a lot of users that the DSL can be incredibly
powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it suitable=
 for not
only constructing test cases, but also in everyday data pipelines. The new
SchemaRDD API is inspired by the data frame concept in Pandas and R.

3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose one
set of APIs that will work for both Java and Scala. The current Java API
(sql.api.java) does not share any common ancestor with the Scala API. This
led to high maintenance burden for us as Spark developers and for library
developers. We propose to eliminate the Java specific API, and simply work
on the existing Scala API to make it also usable for Java. This will make
Java a first class citizen as Scala. This effectively means that all public
classes should be usable for both Scala and Java, including SQLContext,
HiveContext, SchemaRDD, data types, and the aforementioned DSL.


Again, this should have no impact on most users since the existing DSL is
rarely used by end users. However, library developers might need to change
the import statements because we are moving certain classes around. We will
keep you posted as patches are merged.

--001a113a74ea0735c1050caa5e3c--

From dev-return-11129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 09:39:30 2015
Return-Path: <dev-return-11129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 368E717B04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 09:39:30 +0000 (UTC)
Received: (qmail 61715 invoked by uid 500); 15 Jan 2015 09:39:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61640 invoked by uid 500); 15 Jan 2015 09:39:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61626 invoked by uid 99); 15 Jan 2015 09:39:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 09:39:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.217.178 as permitted sender)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 09:39:26 +0000
Received: by mail-lb0-f178.google.com with SMTP id u14so12262386lbd.9
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 01:39:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=/ERTnwXs1ykHfbzTvIyf6Pyh3PaSSgDeRPRLHR+XybY=;
        b=D3hToGphh8XMfO+NFx7BR1bLGkRxrYC82rU2Nd7iy1fK4vg931asmYNOrk4SjL+3yV
         1dLtagmMwfNMT9fdcO7QCrN1BKr17BDqD1j78VHOqMaV/i7CqICf/G0iwRr7b/x3WN1q
         bcUzPURGPhpX79WuCvtEkVCtMJNF3KpRd5P9ZzkwKpoFa6NiYRe+zQKWJj/+xY8b3Wwi
         Ltsu8Ub3EqLCWwiUBokyQ9VcM8ghgCVolx0E7QT8bXd7C1dXRiJzvVV60Voyu5KUiCX9
         aBEVO9wFIaQQ7034y15aPFQWj0qupfgbAyfZh0bTIQExziN5cO1DMji4wNIRxr3UyhEN
         VGYg==
X-Received: by 10.112.222.135 with SMTP id qm7mr8550448lbc.19.1421314745305;
 Thu, 15 Jan 2015 01:39:05 -0800 (PST)
MIME-Version: 1.0
References: <D0DC8B3C.1DA0DF%chris.a.mattmann@jpl.nasa.gov>
 <CAJOb8btc6R5qntvK2Lr=ThOp3ysDzygeEzHKtzUuwG2fRoV68A@mail.gmail.com> <CADtDQQK_9pDiMF20AVn82=0QCbK6a+jRTUpbTE6-fcFNqVGOJQ@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Thu, 15 Jan 2015 09:39:04 +0000
Message-ID: <CAKn3j0v8E6hrDW3Rj4_LgGXeacU6Wq-G1+2g=s7MMqWEjKxRqQ@mail.gmail.com>
Subject: Re: SciSpark: NASA AIST14 proposal
To: RJ Nowling <rnowling@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134d228821179050cada0ac
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d228821179050cada0ac
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hey Chris,

This sounds amazing!
You might have to check also with the Geotrellis
<https://github.com/geotrellis/geotrellis> team (Rob and Eugene for
instance) who have already covered quite interesting ground dealing with
tiles as RDD element.
Some algebra operations are there, but also thingies like Shortest Path
(within rasters).

A small, I've a student who is working on a implementation of LU/LC using
Spark (first using CA, then I hope extension using stochastic methods like
local random forest).

If you consider implementing a R-Tree (or perhaps the SD version) for OGIs
operation, I thought that IndexedRDD
<https://github.com/amplab/spark-indexedrdd> could be interesting to
consider (I've been asked to look at options to implement this kind of
distributed and resilient R-Tree, so I'll be happy to see how it'd perform
^^).

cheers and have fun!
andy


On Thu Jan 15 2015 at 5:53:27 AM RJ Nowling <rnowling@gmail.com> wrote:

> Congratulations, Chris!
>
> I created a JIRA for "dimensional" RDDs that might be relevant:
> https://issues.apache.org/jira/browse/SPARK-4727
>
> Jeremy Freeman pointed me to his lab's work on for neuroscience that have
> some related functionality :
> http://thefreemanlab.com/thunder/
>
> On Wed, Jan 14, 2015 at 11:07 PM, Aniket <aniket.bhatnagar@gmail.com>
> wrote:
>
> > Hi Chris
> >
> > This is super cool. I was wondering if this would be an open source
> project
> > so that people can contribute or reuse?
> >
> > Thanks,
> > Aniket
> >
> > On Thu Jan 15 2015 at 07:39:29 Mattmann, Chris A (3980) [via Apache Spa=
rk
> > Developers List] <ml-node+s1001551n10115h10@n3.nabble.com> wrote:
> >
> > > Hi Spark Devs,
> > >
> > > Just wanted to FYI that I was funded on a 2 year NASA proposal
> > > to build out the concept of a scientific RDD (create by space/time,
> > > and other operations) for use in some neat climate related NASA
> > > use cases.
> > >
> > >
> > http://esto.nasa.gov/files/solicitations/AIST_14/
> ROSES2014_AIST_A41_awards
> > .
> > >
> > > html
> > >
> > >
> > > I will keep everyone posted and plan on interacting with the list
> > > over here to get it done. I expect that we=E2=80=99ll start work in M=
arch.
> > > In the meanwhile you guys can scope the abstract at the link provided=
.
> > > Happy
> > > to chat about it if you have any questions too.
> > >
> > > Cheers!
> > >
> > > Chris
> > >
> > > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > > Chris Mattmann, Ph.D.
> > > Chief Architect
> > > Instrument Software and Science Data Systems Section (398)
> > > NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
> > > Office: 168-519, Mailstop: 168-527
> > > Email: [hidden email]
> > > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D0>
> > > WWW:  http://sunset.usc.edu/~mattmann/
> > > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > > Adjunct Associate Professor, Computer Science Department
> > > University of Southern California, Los Angeles, CA 90089 USA
> > > ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > >
> > >
> > >
> > >
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: [hidden email]
> > > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D1>
> > > For additional commands, e-mail: [hidden email]
> > > <http:///user/SendEmail.jtp?type=3Dnode&node=3D10115&i=3D2>
> > >
> > >
> > > ------------------------------
> > >  If you reply to this email, your message will be added to the
> discussion
> > > below:
> > >
> > >
> > http://apache-spark-developers-list.1001551.n3.nabble.com/SciSpark-NASA=
-
> AIST14-proposal-tp10115.html
> > >  To start a new topic under Apache Spark Developers List, email
> > > ml-node+s1001551n1h76@n3.nabble.com
> > > To unsubscribe from Apache Spark Developers List, click here
> > > <
> > http://apache-spark-developers-list.1001551.n3.nabble.com/template/
> NamlServlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3D
> YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz
> > >
> > > .
> > > NAML
> > > <
> > http://apache-spark-developers-list.1001551.n3.nabble.com/template/
> NamlServlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%
> 21nabble%3Aemail.naml&base=3Dnabble.naml.namespaces.
> BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-nabble.view.
> web.template.NabbleNamespace-nabble.view.web.template.
> NodeNamespace&breadcrumbs=3Dnotify_subscribers%21nabble%
> 3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_
> instant_email%21nabble%3Aemail.naml
> > >
> > >
> >
> >
> >
> >
> > --
> > View this message in context:
> > http://apache-spark-developers-list.1001551.n3.nabble.com/SciSpark-NASA=
-
> AIST14-proposal-tp10115p10118.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
>

--001a1134d228821179050cada0ac--

From dev-return-11130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 11:53:00 2015
Return-Path: <dev-return-11130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EB8417F5E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 11:53:00 +0000 (UTC)
Received: (qmail 10715 invoked by uid 500); 15 Jan 2015 11:53:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10646 invoked by uid 500); 15 Jan 2015 11:53:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10635 invoked by uid 99); 15 Jan 2015 11:53:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 11:53:00 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of etander@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 11:52:55 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5D1961099BB3
	for <dev@spark.apache.org>; Thu, 15 Jan 2015 03:52:05 -0800 (PST)
Date: Thu, 15 Jan 2015 04:52:04 -0700 (MST)
From: preeze <etander@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421322724150-10122.post@n3.nabble.com>
Subject: Spark client reconnect to driver in yarn-cluster deployment mode
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

>From the official spark documentation
(http://spark.apache.org/docs/1.2.0/running-on-yarn.html):

"In yarn-cluster mode, the Spark driver runs inside an application master
process which is managed by YARN on the cluster, and the client can go away
after initiating the application."

Is there any designed way that the client connects back to the driver (still
running in YARN) for collecting results at a later stage?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-client-reconnect-to-driver-in-yarn-cluster-deployment-mode-tp10122.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 13:05:38 2015
Return-Path: <dev-return-11131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C43410173
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 13:05:38 +0000 (UTC)
Received: (qmail 13338 invoked by uid 500); 15 Jan 2015 13:05:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13259 invoked by uid 500); 15 Jan 2015 13:05:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13247 invoked by uid 99); 15 Jan 2015 13:05:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 13:05:38 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 13:05:33 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 696D1109B3B5
	for <dev@spark.apache.org>; Thu, 15 Jan 2015 05:04:53 -0800 (PST)
Date: Thu, 15 Jan 2015 06:04:52 -0700 (MST)
From: PierreB <pierre.borckmans@realimpactanalytics.com>
To: dev@spark.apache.org
Message-ID: <1421327092260-10123.post@n3.nabble.com>
Subject: Spark 1.2.0: MissingRequirementError
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi guys,

A few people seem to have the same problem with Spark 1.2.0 so I figured I
would push it here.

see:
http://apache-spark-user-list.1001560.n3.nabble.com/MissingRequirementError-with-spark-td21149.html

In a nutshell, for sbt test to work, we now need to fork a JVM and also give
more memory to be able to run tests.

See
also:https://github.com/deanwampler/spark-workshop/blob/master/project/Build.scala

This all used to work fine until 1.2.0.

Could u have a look please?
Thanks

P.




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-2-0-MissingRequirementError-tp10123.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 15:42:00 2015
Return-Path: <dev-return-11132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 039971083C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 15:42:00 +0000 (UTC)
Received: (qmail 13409 invoked by uid 500); 15 Jan 2015 15:42:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13333 invoked by uid 500); 15 Jan 2015 15:42:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13316 invoked by uid 99); 15 Jan 2015 15:41:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 15:41:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 15:41:54 +0000
Received: by mail-oi0-f45.google.com with SMTP id x69so12902712oia.4
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 07:40:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=nX1Q8t6ONWLzl7rywqplXzrl4SeE1yJgGs9Rj4aZbNo=;
        b=URR/3KJOS/kmhA+UaKsHF6EAwzNxX8LwmgEAuGQsNDCNSlimxvH/Cm9V1Opt/CUxj4
         35uowr9VpRh3UjJx1pO5dZbmvU7iUsVHdXXG5y7gmYIG26BkkLexzAnpmthM/LZqM0wE
         zKm5CH+wqWoAQ/q1t/Hbba09PpwmHRgG6KtrAwyJR4iVsN16cwJAmxqMsed3EINb70GY
         BzkzOpUgpFpDOSktUFUCYh6zyNo9cjBs/7q91T5h2JMW8F4PEMp1JLB3YX+JdgoT1UOS
         mbmRNKuuFcg/aqYMAFti+In3t5sC51+46RZZnww36qucTW+YkVwZqiAdHLu7przUFT/A
         zWKw==
X-Received: by 10.60.98.240 with SMTP id el16mr6309115oeb.4.1421336403854;
 Thu, 15 Jan 2015 07:40:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Thu, 15 Jan 2015 07:39:43 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 15 Jan 2015 07:39:43 -0800
Message-ID: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
Subject: Join implementation in SparkSQL
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011832a6754c4b050cb2ab92
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011832a6754c4b050cb2ab92
Content-Type: text/plain; charset=UTF-8

Hello,

Where can I find docs about how joins are implemented in SparkSQL? In
particular, I'd like to know whether they are implemented according to
their relational algebra definition as filters on top of a cartesian
product.

Thanks,

Alex

--089e011832a6754c4b050cb2ab92--

From dev-return-11133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 15:54:35 2015
Return-Path: <dev-return-11133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83579108C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 15:54:35 +0000 (UTC)
Received: (qmail 50645 invoked by uid 500); 15 Jan 2015 15:54:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50568 invoked by uid 500); 15 Jan 2015 15:54:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50529 invoked by uid 99); 15 Jan 2015 15:54:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 15:54:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 15:54:10 +0000
Received: by mail-oi0-f54.google.com with SMTP id u20so12895506oif.13
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 07:54:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=7NcNN9rHobJV8l1mSoHii+Lvzy0BToJlPit+r/iAg4A=;
        b=EimjufuuYoN+OUSuvMOreshT+x2xpJlN0UAMybYVTfu2T8QNhvmr+9P4wdPc2IisMO
         zLb6G8aQgGxOa10XJm1GDm6GHGU2IZxs+BVJFPU0oSEip3P3BWivDwuh9WAX2cIschSr
         Nyh/vw9dI8bHoNNz7uUUlk1M6bCFEEGR0LpNd5pFk/7WgxABqEvhrnM8rj+MxYAo9req
         Y7772797rlVxTUejptT7oNEqPyqI5X7tU5Qkol0Uiilvd9JjX2C1kdECTanFhBUrSstv
         8RYmQK0yT2K+t92MqVTu84HJimDCoAaJmrvXxbfXY4eNB4heVvLqN1Llz/xI5Z2PgnxS
         ZaoA==
X-Received: by 10.202.216.9 with SMTP id p9mr6033779oig.94.1421337248693; Thu,
 15 Jan 2015 07:54:08 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Thu, 15 Jan 2015 07:53:48 -0800 (PST)
In-Reply-To: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 15 Jan 2015 07:53:48 -0800
Message-ID: <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d37ead085c9050cb2ddd9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d37ead085c9050cb2ddd9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Reynold,

Thanks for the heads up. In general, I strongly oppose the use of "private"
to restrict access to certain parts of the API, the reason being that I
might find the need to use some of the internals of a library from my own
project. I find that a @DeveloperAPI annotation serves the same purpose as
"private" without imposing unnecessary restrictions: it discourages people
from using the annotated API and reserves the right for the core developers
to change it suddenly in backwards incompatible ways.

In particular, I would like to express the desire that the APIs to
programmatically construct SchemaRDDs from an RDD[Row] and a StructType
remain public. All the SparkSQL data type objects should be exposed by the
API, and the jekyll build should not hide the docs as it does now.

Thanks.

Alex

On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com> wrote:

> Hi Spark devs,
>
> Given the growing number of developers that are building on Spark SQL, we
> would like to stabilize the API in 1.3 so users and developers can be
> confident to build on it. This also gives us a chance to improve the API.
>
> In particular, we are proposing the following major changes. This should
> have no impact for most users (i.e. those running SQL through the JDBC
> client or SQLContext.sql method).
>
> 1. Everything in sql.catalyst package is private to the project.
>
> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
> SchemaRDD and logical plans in order to construct test cases. We have
> received feedback from a lot of users that the DSL can be incredibly
> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it suitab=
le for not
> only constructing test cases, but also in everyday data pipelines. The ne=
w
> SchemaRDD API is inspired by the data frame concept in Pandas and R.
>
> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose on=
e
> set of APIs that will work for both Java and Scala. The current Java API
> (sql.api.java) does not share any common ancestor with the Scala API. Thi=
s
> led to high maintenance burden for us as Spark developers and for library
> developers. We propose to eliminate the Java specific API, and simply wor=
k
> on the existing Scala API to make it also usable for Java. This will make
> Java a first class citizen as Scala. This effectively means that all publ=
ic
> classes should be usable for both Scala and Java, including SQLContext,
> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
>
>
> Again, this should have no impact on most users since the existing DSL is
> rarely used by end users. However, library developers might need to chang=
e
> the import statements because we are moving certain classes around. We wi=
ll
> keep you posted as patches are merged.
>

--001a113d37ead085c9050cb2ddd9--

From dev-return-11134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 16:32:01 2015
Return-Path: <dev-return-11134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DC3210AB5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 16:32:01 +0000 (UTC)
Received: (qmail 62772 invoked by uid 500); 15 Jan 2015 16:32:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62704 invoked by uid 500); 15 Jan 2015 16:32:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62692 invoked by uid 99); 15 Jan 2015 16:32:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 16:32:01 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of drobin1437@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 16:31:56 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id D04E810A0F8F
	for <dev@spark.apache.org>; Thu, 15 Jan 2015 08:31:36 -0800 (PST)
Date: Thu, 15 Jan 2015 09:31:35 -0700 (MST)
From: David Robinson <drobin1437@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421339495591-10126.post@n3.nabble.com>
In-Reply-To: <CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com>
References: <CAKXMip3DT9sRwCu2VkWN5KkfSKu+az+_RtV6m-GVhuK52_55SA@mail.gmail.com> <D08139AE.44FB%brennon.york@capitalone.com> <CANjHi9q7rwGUVroDQF2+u=P+2ZBFevq4PjQxpykSvWZwD4wRZg@mail.gmail.com> <D08150FA.45D7%brennon.york@capitalone.com> <CAKXMip1PhE37UmezLM0EG+NZo20Pqdicii0AQE7AYJoXt0ceHQ@mail.gmail.com> <CAPh_B=YiOJiOJ8uSy4Hn4YKjTPgTkH+QZEW=WMoOihmpgKgh6g@mail.gmail.com> <CAKXMip0k5ZGO+QO79G53=b-Epz+-oBFr=BiMDV0hCgaMurWdKA@mail.gmail.com> <CANjHi9qNEtk1PGnUceLMo1xupaP8MhtFXVvF6bQVuXOrD5Ab+Q@mail.gmail.com> <CAKXMip21Yx3fKioQ+5OpJrqVybuOoopAjocsn2g0JMv=HktFCg@mail.gmail.com> <CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com>
Subject: Re: Implementing TinkerPop on top of GraphX
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am new to Spark and GraphX, however, I use Tinkerpop backed graphs and
think the idea of using Tinkerpop as the API for GraphX is a great idea and
hope you are still headed in that direction.  I noticed that Tinkerpop 3 is
moving into the Apache family:
http://wiki.apache.org/incubator/TinkerPopProposal  which might alleviate
concerns about having an API definition "outside" of Spark.

Thanks,




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Implementing-TinkerPop-on-top-of-GraphX-tp9169p10126.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 16:47:37 2015
Return-Path: <dev-return-11135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F10310B58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 16:47:37 +0000 (UTC)
Received: (qmail 16722 invoked by uid 500); 15 Jan 2015 16:47:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16644 invoked by uid 500); 15 Jan 2015 16:47:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16631 invoked by uid 99); 15 Jan 2015 16:47:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 16:47:37 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of devl.development@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 16:47:32 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id DFABB10A164B
	for <dev@spark.apache.org>; Thu, 15 Jan 2015 08:46:42 -0800 (PST)
Date: Thu, 15 Jan 2015 09:46:41 -0700 (MST)
From: "devl.development" <devl.development@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421340401649-10127.post@n3.nabble.com>
Subject: LinearRegressionWithSGD accuracy
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

>From what I gather, you use LinearRegressionWithSGD to predict y or the
response variable given a feature vector x.

In a simple example I used a perfectly linear dataset such that x=y
y,x
1,1
2,2
...

10000,10000

Using the out-of-box example from the website (with and without scaling):

 val data = sc.textFile(file)

    val parsedData = data.map { line =>
      val parts = line.split(',')
     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) //y
and x

    }
    val scaler = new StandardScaler(withMean = true, withStd = true)
      .fit(parsedData.map(x => x.features))
    val scaledData = parsedData
      .map(x =>
      LabeledPoint(x.label,
        scaler.transform(Vectors.dense(x.features.toArray))))

    // Building the model
    val numIterations = 100
    val model = LinearRegressionWithSGD.train(parsedData, numIterations)

    // Evaluate model on training examples and compute training error *
tried using both scaledData and parsedData
    val valuesAndPreds = scaledData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
    println("training Mean Squared Error = " + MSE)

Both scaled and unscaled attempts give:

training Mean Squared Error = NaN

I've even tried x, y+(sample noise from normal with mean 0 and stddev 1)
still comes up with the same thing.

Is this not supposed to work for x and y or 2 dimensional plots? Is there
something I'm missing or wrong in the code above? Or is there a limitation
in the method?

Thanks for any advice.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegressionWithSGD-accuracy-tp10127.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 17:43:39 2015
Return-Path: <dev-return-11136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D23810E6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 17:43:39 +0000 (UTC)
Received: (qmail 20603 invoked by uid 500); 15 Jan 2015 17:43:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20532 invoked by uid 500); 15 Jan 2015 17:43:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20520 invoked by uid 99); 15 Jan 2015 17:43:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 17:43:39 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [212.124.192.207] (HELO vicki.2020media.net.uk) (212.124.192.207)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 17:43:12 +0000
Received: from [86.7.249.6] (helo=[192.168.0.8])
	by vicki.2020media.net.uk with esmtpsa (TLSv1:AES128-SHA:128)
	(Exim 4.72)
	(envelope-from <robin.east@xense.co.uk>)
	id 1YBoRW-0003Nj-Tv; Thu, 15 Jan 2015 17:42:51 +0000
Content-Type: multipart/alternative; boundary="Apple-Mail=_60562FC0-2BEC-4933-9912-BFCC18D4BE25"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: LinearRegressionWithSGD accuracy
From: Robin East <robin.east@xense.co.uk>
In-Reply-To: <1421340401649-10127.post@n3.nabble.com>
Date: Thu, 15 Jan 2015 17:42:47 +0000
Cc: dev@spark.apache.org
Message-Id: <56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
References: <1421340401649-10127.post@n3.nabble.com>
To: "devl.development" <devl.development@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-2020-Relay: Sent using 2020MEDIA.net.uk relay with auth code: xense
 Send Abuse reports to abuse@2020media.net.uk
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_60562FC0-2BEC-4933-9912-BFCC18D4BE25
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

-dev, +user

You=92ll need to set the gradient descent step size to something small - =
a bit of trial and error shows that 0.00000001 works.

You=92ll need to create a LinearRegressionWithSGD instance and set the =
step size explicitly:

val lr =3D new LinearRegressionWithSGD()
lr.optimizer.setStepSize(0.00000001)
lr.optimizer.setNumIterations(100)
val model =3D lr.run(parsedData)

On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.com> =
wrote:

> =46rom what I gather, you use LinearRegressionWithSGD to predict y or =
the
> response variable given a feature vector x.
>=20
> In a simple example I used a perfectly linear dataset such that x=3Dy
> y,x
> 1,1
> 2,2
> ...
>=20
> 10000,10000
>=20
> Using the out-of-box example from the website (with and without =
scaling):
>=20
> val data =3D sc.textFile(file)
>=20
>    val parsedData =3D data.map { line =3D>
>      val parts =3D line.split(',')
>     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) =
//y
> and x
>=20
>    }
>    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D =
true)
>      .fit(parsedData.map(x =3D> x.features))
>    val scaledData =3D parsedData
>      .map(x =3D>
>      LabeledPoint(x.label,
>        scaler.transform(Vectors.dense(x.features.toArray))))
>=20
>    // Building the model
>    val numIterations =3D 100
>    val model =3D LinearRegressionWithSGD.train(parsedData, =
numIterations)
>=20
>    // Evaluate model on training examples and compute training error *
> tried using both scaledData and parsedData
>    val valuesAndPreds =3D scaledData.map { point =3D>
>      val prediction =3D model.predict(point.features)
>      (point.label, prediction)
>    }
>    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p), =
2)}.mean()
>    println("training Mean Squared Error =3D " + MSE)
>=20
> Both scaled and unscaled attempts give:
>=20
> training Mean Squared Error =3D NaN
>=20
> I've even tried x, y+(sample noise from normal with mean 0 and stddev =
1)
> still comes up with the same thing.
>=20
> Is this not supposed to work for x and y or 2 dimensional plots? Is =
there
> something I'm missing or wrong in the code above? Or is there a =
limitation
> in the method?
>=20
> Thanks for any advice.
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegression=
WithSGD-accuracy-tp10127.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


--Apple-Mail=_60562FC0-2BEC-4933-9912-BFCC18D4BE25--

From dev-return-11137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 18:34:57 2015
Return-Path: <dev-return-11137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B1244172CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 18:34:57 +0000 (UTC)
Received: (qmail 66550 invoked by uid 500); 15 Jan 2015 18:34:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66472 invoked by uid 500); 15 Jan 2015 18:34:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66461 invoked by uid 99); 15 Jan 2015 18:34:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:34:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:34:31 +0000
Received: by mail-qg0-f49.google.com with SMTP id f51so12963618qge.8
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 10:33:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=QHqT/VpPc5EPkijtPzXSP744rbRMoZgj2iHUJlQpWgg=;
        b=FQO/ChqN3uCiJKI1xuhVK8Mz3tgyg6Y/h75JhefMW4KC83FfrW483xEBFnQ83qwtrz
         BnmHhFDkt1ivodMpZYZlFV/hsnKRzCZ9zqVHViYxfRe8kPPTqmCAGhzTuP3AnqlWl5pg
         3Qp1wo2BlY2GlQqaQITW1MKBSkTPxEZ/6PIp8mzblF2yC3OgTKwvYxLJwVnx2pz30Uea
         8+54FG+6XKmVCxKn+FbT/u6OarCOVu+kiymzquhqzoOlMnkPGOpQ7v/D2dq9XkWTmV8b
         OzhpG6gHwaRvwEBXO3RuQYThnVzrRfgi1amZ9Y/oDFt4jK+lV7eHpqC/L3Y68u+EaBOz
         mJoA==
X-Gm-Message-State: ALoCoQmltME7nnpnthoY5MIZqc5XVD7P6K6S5HfGspJbDLfQXIqcPN3OAfB5VfGxavXWrSoZCNAI
X-Received: by 10.229.27.73 with SMTP id h9mr18632140qcc.3.1421346804494; Thu,
 15 Jan 2015 10:33:24 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 15 Jan 2015 10:33:04 -0800 (PST)
In-Reply-To: <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
 <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 15 Jan 2015 10:33:04 -0800
Message-ID: <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133c3fe629978050cb51713
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133c3fe629978050cb51713
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alex,

I didn't communicate properly. By "private", I simply meant the expectation
that it is not a public API. The plan is to still omit it from the
scaladoc/javadoc generation, but no language visibility modifier will be
applied on them.

After 1.3, you will likely no longer need to use things in sql.catalyst
package directly. Programmatically construct SchemaRDDs is going to be a
first class public API. Data types have already been moved out of the
sql.catalyst package and now lives in sql.types. They are becoming stable
public APIs. When the "data frame" patch is submitted, you will see a
public expression library also. There will be few reason for end users or
library developers to hook into things in sql.catalyst. For the bravest and
the most advanced, they can still use them, with the expectation that it is
subject to change.





On Thu, Jan 15, 2015 at 7:53 AM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Reynold,
>
> Thanks for the heads up. In general, I strongly oppose the use of
> "private" to restrict access to certain parts of the API, the reason bein=
g
> that I might find the need to use some of the internals of a library from
> my own project. I find that a @DeveloperAPI annotation serves the same
> purpose as "private" without imposing unnecessary restrictions: it
> discourages people from using the annotated API and reserves the right fo=
r
> the core developers to change it suddenly in backwards incompatible ways.
>
> In particular, I would like to express the desire that the APIs to
> programmatically construct SchemaRDDs from an RDD[Row] and a StructType
> remain public. All the SparkSQL data type objects should be exposed by th=
e
> API, and the jekyll build should not hide the docs as it does now.
>
> Thanks.
>
> Alex
>
> On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Hi Spark devs,
>>
>> Given the growing number of developers that are building on Spark SQL, w=
e
>> would like to stabilize the API in 1.3 so users and developers can be
>> confident to build on it. This also gives us a chance to improve the API=
.
>>
>> In particular, we are proposing the following major changes. This should
>> have no impact for most users (i.e. those running SQL through the JDBC
>> client or SQLContext.sql method).
>>
>> 1. Everything in sql.catalyst package is private to the project.
>>
>> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
>> SchemaRDD and logical plans in order to construct test cases. We have
>> received feedback from a lot of users that the DSL can be incredibly
>> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it suita=
ble for
>> not
>> only constructing test cases, but also in everyday data pipelines. The n=
ew
>> SchemaRDD API is inspired by the data frame concept in Pandas and R.
>>
>> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose o=
ne
>> set of APIs that will work for both Java and Scala. The current Java API
>> (sql.api.java) does not share any common ancestor with the Scala API. Th=
is
>> led to high maintenance burden for us as Spark developers and for librar=
y
>> developers. We propose to eliminate the Java specific API, and simply wo=
rk
>> on the existing Scala API to make it also usable for Java. This will mak=
e
>> Java a first class citizen as Scala. This effectively means that all
>> public
>> classes should be usable for both Scala and Java, including SQLContext,
>> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
>>
>>
>> Again, this should have no impact on most users since the existing DSL i=
s
>> rarely used by end users. However, library developers might need to chan=
ge
>> the import statements because we are moving certain classes around. We
>> will
>> keep you posted as patches are merged.
>>
>
>

--001a1133c3fe629978050cb51713--

From dev-return-11138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 18:37:28 2015
Return-Path: <dev-return-11138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B1367172D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 18:37:28 +0000 (UTC)
Received: (qmail 70627 invoked by uid 500); 15 Jan 2015 18:37:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70554 invoked by uid 500); 15 Jan 2015 18:37:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70543 invoked by uid 99); 15 Jan 2015 18:37:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:37:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:37:24 +0000
Received: by mail-qc0-f175.google.com with SMTP id p6so13579496qcv.6
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 10:36:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=BxSEnSESbp9KTRr4rhXKWk0HuNEE7dPJMUOltgjpJTA=;
        b=VqcReeCVo2GDbLhXhAVnSbtGDd+gp4amLyibRzFgVpMJQZ7IxdZ+aZuPlijGgaWUyi
         MWPzfWEELnMFRpypjYKm7U0+4dvf8+gf5Q3+5xmQX708az80qZrecQ6Zc5O1Ydt4A9qM
         Ac21jWahGp9H2kIX4/S8PpCUtKunnNx/wzvSSLP8qybc3HdYy3t1Q4yjtfCRySGys/KR
         zM+vyRr9bmjRK1VYjD1Nwtfe+u4iz7vk8iEah9YFOIs2d9lhuEL4slsT4LBJkJPD7gX4
         GbyKPXU6822pzippMvccBPx646+6Odbb8jMaiUqUDqI0Y6HqmtZ3Iwy/dJ07wJVuah4x
         gULw==
X-Gm-Message-State: ALoCoQlHpzYN7DWsX7MCgXul22Rrs7u7lXOes3diqPD7Hw/TDnqQggLUTtZb2HhmKGlkgRpOiQKQ
X-Received: by 10.140.22.49 with SMTP id 46mr7264860qgm.29.1421347002882; Thu,
 15 Jan 2015 10:36:42 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 15 Jan 2015 10:36:21 -0800 (PST)
In-Reply-To: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
References: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 15 Jan 2015 10:36:21 -0800
Message-ID: <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
Subject: Re: Join implementation in SparkSQL
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1195635b440050cb52337
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1195635b440050cb52337
Content-Type: text/plain; charset=UTF-8

It's a bunch of strategies defined here:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala

In most common use cases (e.g. inner equi join), filters are pushed below
the join or into the join. Doing a cartesian product followed by a filter
is too expensive.


On Thu, Jan 15, 2015 at 7:39 AM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Hello,
>
> Where can I find docs about how joins are implemented in SparkSQL? In
> particular, I'd like to know whether they are implemented according to
> their relational algebra definition as filters on top of a cartesian
> product.
>
> Thanks,
>
> Alex
>

--001a11c1195635b440050cb52337--

From dev-return-11139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 18:39:21 2015
Return-Path: <dev-return-11139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EB6A172F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 18:39:21 +0000 (UTC)
Received: (qmail 75243 invoked by uid 500); 15 Jan 2015 18:39:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75156 invoked by uid 500); 15 Jan 2015 18:39:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 67268 invoked by uid 99); 15 Jan 2015 17:00:07 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jayhutfles@gmail.com designates 209.85.213.179 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=63SPEOZlBDcnieYi7SQzO22aZ9wTlSg0pgIYv+cIgwE=;
        b=TimsGnJvQNaoV9IKmrQvhZ7W6JGENCFSERq2xSmwewFIcaKzG4aCQzUKm02sk2WNXo
         cotxWBkcFhyvzKuYQzFcZMvp2AaXV31unYtEWAuvmb1K5PhwqpsCEhAHI/Y1KHl6MKvO
         2jjEbRuTNrALhmI3S9Lk7q93KK2dZq3ofUwBFr49QTpNpHJhu1gC6ciikep1X6emTYxH
         i03abgtfYWMBeMzuoRr78/MPB/IKSl+W5BoO1dzQ0ymyM9ceMdK8L+pGtwC6tpN8Uznf
         p23KOoQUpn9SU8kRujy94Q4ggunxiu3eqN6tqzGu+5AJsn+580RQ4aNghqkbStXIlB7i
         l4fw==
X-Received: by 10.50.32.70 with SMTP id g6mr11477892igi.35.1421341136159; Thu,
 15 Jan 2015 08:58:56 -0800 (PST)
MIME-Version: 1.0
From: Jay Hutfles <jayhutfles@gmail.com>
Date: Thu, 15 Jan 2015 16:58:55 +0000
Message-ID: <CALO5299KVehPE9-7aLbc3hjQGQJvX+9zY4=i9k+n_evX2Hw5Pg@mail.gmail.com>
Subject: Graphx TripletFields written in Java?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b10ca55868ddc050cb3c588
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10ca55868ddc050cb3c588
Content-Type: text/plain; charset=UTF-8

Hi all,
  Does anyone know the reasoning behind implementing
org.apache.spark.graphx.TripletFields in Java instead of Scala?  It doesn't
look like there's anything in there that couldn't be done in Scala.
Nothing serious, just curious.  Thanks!
   -Jay

--047d7b10ca55868ddc050cb3c588--

From dev-return-11140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 18:43:39 2015
Return-Path: <dev-return-11140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 911A017315
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 18:43:39 +0000 (UTC)
Received: (qmail 86742 invoked by uid 500); 15 Jan 2015 18:43:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86670 invoked by uid 500); 15 Jan 2015 18:43:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86659 invoked by uid 99); 15 Jan 2015 18:43:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:43:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:43:34 +0000
Received: by mail-qc0-f174.google.com with SMTP id c9so13576574qcz.5
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 10:40:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=V7NwMBuLqgQ5djDZyZoRmM8rnf1cAvJQBEGaDyS/K64=;
        b=Ai9cjImyvIEce7KWGfHNEEQ/9pJHCiUxuA+/Pnr5CxXcACvaF1QU8pQHriIaQkHCxC
         d8P9GMqQ8jdouMT98P+V+vRNYoPk8b0HvcEgi90fGuiaghivJ74g+YNb+CSfVXT6BD6B
         5SXAwtsr9K7Tn+/zoUbj5ptIbR3vImmw05ErGDD7sKPlW1rDckjapa0EPLHtYyJnkemh
         ZCpFXS3PFBFdxM/Oirew3AFO+1mCy+WwpF2Jox0Pdv23/adTrtW47fEOnqdrzNBbpKZ7
         odBr44wQQXFS1ANZS8in4a0YBwheoji0KFeyRDxLSe0unJjdaBiSgWH168+q8GVpTmzg
         F9ow==
X-Gm-Message-State: ALoCoQnSlBhIyq3uvOvqhuz0FEgbHHzDUYA8+Y9b0ccjv0Lcm+LYtPLZ3OCCw4/Oz8N9+MLz+mKi
X-Received: by 10.229.27.73 with SMTP id h9mr18687473qcc.3.1421347237892; Thu,
 15 Jan 2015 10:40:37 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 15 Jan 2015 10:40:17 -0800 (PST)
In-Reply-To: <CALO5299KVehPE9-7aLbc3hjQGQJvX+9zY4=i9k+n_evX2Hw5Pg@mail.gmail.com>
References: <CALO5299KVehPE9-7aLbc3hjQGQJvX+9zY4=i9k+n_evX2Hw5Pg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 15 Jan 2015 10:40:17 -0800
Message-ID: <CAPh_B=bKO_PPN1=fhEDjZv3_q3TFnTYwHq3ixnCLxa6xUuBTQA@mail.gmail.com>
Subject: Re: Graphx TripletFields written in Java?
To: Jay Hutfles <jayhutfles@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133c3fe37a272050cb53183
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133c3fe37a272050cb53183
Content-Type: text/plain; charset=UTF-8

The static fields - Scala can't express JVM static fields unfortunately.
Those will be important once we provide the Java API.



On Thu, Jan 15, 2015 at 8:58 AM, Jay Hutfles <jayhutfles@gmail.com> wrote:

> Hi all,
>   Does anyone know the reasoning behind implementing
> org.apache.spark.graphx.TripletFields in Java instead of Scala?  It doesn't
> look like there's anything in there that couldn't be done in Scala.
> Nothing serious, just curious.  Thanks!
>    -Jay
>

--001a1133c3fe37a272050cb53183--

From dev-return-11141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 18:44:06 2015
Return-Path: <dev-return-11141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1ACA717318
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 18:44:06 +0000 (UTC)
Received: (qmail 88655 invoked by uid 500); 15 Jan 2015 18:44:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88585 invoked by uid 500); 15 Jan 2015 18:44:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88573 invoked by uid 99); 15 Jan 2015 18:44:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:44:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of devl.development@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 18:43:36 +0000
Received: by mail-qc0-f175.google.com with SMTP id p6so13607859qcv.6
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 10:42:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ABM5HryPiQ3zEHXw7QynsT3MuGT0cqEl6LWp66adcGs=;
        b=mlsAGyfheijpTEEa8DkfMZWYACJU9iX1hS03She6zxS56nJlwX4DuM8ywout7bN9Am
         13V66ae5NR3et2su2okcim7xSkAYJRboX2H/9h+4WzBe7F7E3lwTdurVITzs/G58rwfn
         3QCowoUTG/ra++EB9oTGn3wymgqmcqe5KxdghzdDESYo3pq2UICjytSG4vp0W6ls5N7y
         bxkFkx0E0zmAcErP9eEVthqHM5gN253xOHUwvy+Lw9tLqgFL5qxO15klwtzwgW67czAE
         3xzY6ahiExVVZDfB06M1InppVfLzTxC3jwhfoEwwts1DoRBhaiJSiy8B9hsj8Wy9WHar
         TZeQ==
MIME-Version: 1.0
X-Received: by 10.224.34.137 with SMTP id l9mr18742828qad.57.1421347369699;
 Thu, 15 Jan 2015 10:42:49 -0800 (PST)
Received: by 10.140.102.111 with HTTP; Thu, 15 Jan 2015 10:42:49 -0800 (PST)
In-Reply-To: <56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
References: <1421340401649-10127.post@n3.nabble.com>
	<56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
Date: Thu, 15 Jan 2015 18:42:49 +0000
Message-ID: <CAMQ+LQNLYXWL+cYUci7nTEFhGuDqn-vArvPUhjcqdDPAPM=6hg@mail.gmail.com>
Subject: Re: LinearRegressionWithSGD accuracy
From: Devl Devel <devl.development@gmail.com>
To: Robin East <robin.east@xense.co.uk>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2952a12d0a2050cb539d6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2952a12d0a2050cb539d6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks, that helps a bit at least with the NaN but the MSE is still very
high even with that step size and 10k iterations:

training Mean Squared Error =3D 3.3322561285919316E7

Does this method need say 100k iterations?






On Thu, Jan 15, 2015 at 5:42 PM, Robin East <robin.east@xense.co.uk> wrote:

> -dev, +user
>
> You=E2=80=99ll need to set the gradient descent step size to something sm=
all - a
> bit of trial and error shows that 0.00000001 works.
>
> You=E2=80=99ll need to create a LinearRegressionWithSGD instance and set =
the step
> size explicitly:
>
> val lr =3D new LinearRegressionWithSGD()
> lr.optimizer.setStepSize(0.00000001)
> lr.optimizer.setNumIterations(100)
> val model =3D lr.run(parsedData)
>
> On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.com>
> wrote:
>
> From what I gather, you use LinearRegressionWithSGD to predict y or the
> response variable given a feature vector x.
>
> In a simple example I used a perfectly linear dataset such that x=3Dy
> y,x
> 1,1
> 2,2
> ...
>
> 10000,10000
>
> Using the out-of-box example from the website (with and without scaling):
>
> val data =3D sc.textFile(file)
>
>    val parsedData =3D data.map { line =3D>
>      val parts =3D line.split(',')
>     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) //y
> and x
>
>    }
>    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D true)
>      .fit(parsedData.map(x =3D> x.features))
>    val scaledData =3D parsedData
>      .map(x =3D>
>      LabeledPoint(x.label,
>        scaler.transform(Vectors.dense(x.features.toArray))))
>
>    // Building the model
>    val numIterations =3D 100
>    val model =3D LinearRegressionWithSGD.train(parsedData, numIterations)
>
>    // Evaluate model on training examples and compute training error *
> tried using both scaledData and parsedData
>    val valuesAndPreds =3D scaledData.map { point =3D>
>      val prediction =3D model.predict(point.features)
>      (point.label, prediction)
>    }
>    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p), 2)}.m=
ean()
>    println("training Mean Squared Error =3D " + MSE)
>
> Both scaled and unscaled attempts give:
>
> training Mean Squared Error =3D NaN
>
> I've even tried x, y+(sample noise from normal with mean 0 and stddev 1)
> still comes up with the same thing.
>
> Is this not supposed to work for x and y or 2 dimensional plots? Is there
> something I'm missing or wrong in the code above? Or is there a limitatio=
n
> in the method?
>
> Thanks for any advice.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegressio=
nWithSGD-accuracy-tp10127.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>

--001a11c2952a12d0a2050cb539d6--

From dev-return-11142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 20:17:30 2015
Return-Path: <dev-return-11142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5093C1778B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 20:17:30 +0000 (UTC)
Received: (qmail 69035 invoked by uid 500); 15 Jan 2015 20:17:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68970 invoked by uid 500); 15 Jan 2015 20:17:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68956 invoked by uid 99); 15 Jan 2015 20:17:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:17:30 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of cjnolet@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:17:05 +0000
Received: by mail-ie0-f180.google.com with SMTP id rp18so17032970iec.11
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 12:17:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=K6vUus8wxgAxh+qzGmypzWla6ikpL0MmPU9uVgBiszE=;
        b=olOMElRhYHHSINvVQiICfPmerEXKc4lQpPlt82pLNbj2sNrZm+1Jbupmka3j1+QKiZ
         4CRrdbHcWHP3pAD8FpsLzANwWEtpoTVebarDfc3Nbd/JT+1FTdWLzlNlDazBREBcLflc
         7lEBDXvhdoH2CbWE7xEFu2KarDC3UINHN9fuMYfg7dhmd+6foVYTuxh9KVQdV5WK4i/r
         N5F9HbobPFFe9l7wo/naXDlLi7N2s5mOWRyIyJXj9M0NPy/1IEiUfRcKA2zG0CLyYbfF
         eBFNWxYI3anY/ca6XL3DEnYb8N/uKn2ScP9RrSIcxaDPKDWxml98f5kMF+H1Co3UrAnz
         73Ig==
X-Received: by 10.107.12.214 with SMTP id 83mr12029917iom.61.1421353023426;
 Thu, 15 Jan 2015 12:17:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.166.106 with HTTP; Thu, 15 Jan 2015 12:16:43 -0800 (PST)
In-Reply-To: <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
 <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com> <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com>
From: Corey Nolet <cjnolet@gmail.com>
Date: Thu, 15 Jan 2015 15:16:43 -0500
Message-ID: <CAOHP_tHDJJntWMYhV_+7v5MTXDiESrdtHeDbv_sjnMYOJFkb4A@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Reynold Xin <rxin@databricks.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fc0ee100168050cb68a51
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fc0ee100168050cb68a51
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Reynold,

One thing I'd like worked into the public portion of the API is the json
inferencing logic that creates a Set[(String, StructType)] out of
Map[String,Any]. SPARK-5260 addresses this so that I can use Accumulators
to infer my schema instead of forcing a map/reduce phase to occur on an RDD
in order to get the final schema. Do you (or anyone else) see a path
forward in exposing this to users? A utility class perhaps?

On Thu, Jan 15, 2015 at 1:33 PM, Reynold Xin <rxin@databricks.com> wrote:

> Alex,
>
> I didn't communicate properly. By "private", I simply meant the expectati=
on
> that it is not a public API. The plan is to still omit it from the
> scaladoc/javadoc generation, but no language visibility modifier will be
> applied on them.
>
> After 1.3, you will likely no longer need to use things in sql.catalyst
> package directly. Programmatically construct SchemaRDDs is going to be a
> first class public API. Data types have already been moved out of the
> sql.catalyst package and now lives in sql.types. They are becoming stable
> public APIs. When the "data frame" patch is submitted, you will see a
> public expression library also. There will be few reason for end users or
> library developers to hook into things in sql.catalyst. For the bravest a=
nd
> the most advanced, they can still use them, with the expectation that it =
is
> subject to change.
>
>
>
>
>
> On Thu, Jan 15, 2015 at 7:53 AM, Alessandro Baretta <alexbaretta@gmail.co=
m
> >
> wrote:
>
> > Reynold,
> >
> > Thanks for the heads up. In general, I strongly oppose the use of
> > "private" to restrict access to certain parts of the API, the reason
> being
> > that I might find the need to use some of the internals of a library fr=
om
> > my own project. I find that a @DeveloperAPI annotation serves the same
> > purpose as "private" without imposing unnecessary restrictions: it
> > discourages people from using the annotated API and reserves the right
> for
> > the core developers to change it suddenly in backwards incompatible way=
s.
> >
> > In particular, I would like to express the desire that the APIs to
> > programmatically construct SchemaRDDs from an RDD[Row] and a StructType
> > remain public. All the SparkSQL data type objects should be exposed by
> the
> > API, and the jekyll build should not hide the docs as it does now.
> >
> > Thanks.
> >
> > Alex
> >
> > On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> >> Hi Spark devs,
> >>
> >> Given the growing number of developers that are building on Spark SQL,
> we
> >> would like to stabilize the API in 1.3 so users and developers can be
> >> confident to build on it. This also gives us a chance to improve the
> API.
> >>
> >> In particular, we are proposing the following major changes. This shou=
ld
> >> have no impact for most users (i.e. those running SQL through the JDBC
> >> client or SQLContext.sql method).
> >>
> >> 1. Everything in sql.catalyst package is private to the project.
> >>
> >> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
> >> SchemaRDD and logical plans in order to construct test cases. We have
> >> received feedback from a lot of users that the DSL can be incredibly
> >> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it sui=
table for
> >> not
> >> only constructing test cases, but also in everyday data pipelines. The
> new
> >> SchemaRDD API is inspired by the data frame concept in Pandas and R.
> >>
> >> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose
> one
> >> set of APIs that will work for both Java and Scala. The current Java A=
PI
> >> (sql.api.java) does not share any common ancestor with the Scala API.
> This
> >> led to high maintenance burden for us as Spark developers and for
> library
> >> developers. We propose to eliminate the Java specific API, and simply
> work
> >> on the existing Scala API to make it also usable for Java. This will
> make
> >> Java a first class citizen as Scala. This effectively means that all
> >> public
> >> classes should be usable for both Scala and Java, including SQLContext=
,
> >> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
> >>
> >>
> >> Again, this should have no impact on most users since the existing DSL
> is
> >> rarely used by end users. However, library developers might need to
> change
> >> the import statements because we are moving certain classes around. We
> >> will
> >> keep you posted as patches are merged.
> >>
> >
> >
>

--001a113fc0ee100168050cb68a51--

From dev-return-11143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 20:26:00 2015
Return-Path: <dev-return-11143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6EDA6177C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 20:26:00 +0000 (UTC)
Received: (qmail 93322 invoked by uid 500); 15 Jan 2015 20:26:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93247 invoked by uid 500); 15 Jan 2015 20:26:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93236 invoked by uid 99); 15 Jan 2015 20:26:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:26:00 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:25:35 +0000
Received: by mail-oi0-f41.google.com with SMTP id i138so14242739oig.0
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 12:23:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=YWRiXi16BhSOPblIFIiMa7VbfdEupN8ygxMwWagxR3c=;
        b=OazgJSXglogCyJXq2Io2ciKYbdeXw8HqaVL1eKmC4CVoBGsQskphDWyFJqnFItCmm3
         wgGf2uY15oND2Cxn7zhZRGbxFHd53Nhj5Omo4LBBUf1Dk/QK7KffZiEAZLkcX8bODMnB
         +DX5Ubkveam1iTMY3/49YrUfyfnUXW6TsorfURRCcRUAR+EohMH9h+UPL6xheheXmnAS
         n2AapWJd3JnMMA5lAxAIB4WoWsFtIzx2KYoB3iVhydlzPvyqO6+gx9l92LOeAubWPmhq
         KhJdx9/aSOq9Ee7WFeJBZvIjat8Gt6j4Yw69BER6JbDbuKZ7rhitB/UXdBgXTo56L58l
         mDeA==
X-Gm-Message-State: ALoCoQk6KVwMZZYp12X/Z8FYHKnVzaPhfY6eDMgALjNroe9IjwKQzHwGkdPKc66sBG8i71Asbqml
MIME-Version: 1.0
X-Received: by 10.182.71.73 with SMTP id s9mr7223710obu.15.1421353423158; Thu,
 15 Jan 2015 12:23:43 -0800 (PST)
Received: by 10.60.4.133 with HTTP; Thu, 15 Jan 2015 12:23:43 -0800 (PST)
In-Reply-To: <CAMQ+LQNLYXWL+cYUci7nTEFhGuDqn-vArvPUhjcqdDPAPM=6hg@mail.gmail.com>
References: <1421340401649-10127.post@n3.nabble.com>
	<56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
	<CAMQ+LQNLYXWL+cYUci7nTEFhGuDqn-vArvPUhjcqdDPAPM=6hg@mail.gmail.com>
Date: Thu, 15 Jan 2015 12:23:43 -0800
Message-ID: <CAF7ADNq7yhYURfzAPNSZvWZwv89JVcUyGb8POJU8pbd6qw4nVg@mail.gmail.com>
Subject: Re: LinearRegressionWithSGD accuracy
From: Joseph Bradley <joseph@databricks.com>
To: Devl Devel <devl.development@gmail.com>
Cc: Robin East <robin.east@xense.co.uk>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8fb1fa7ee35545050cb6a19a
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb1fa7ee35545050cb6a19a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It looks like you're training on the non-scaled data but testing on the
scaled data.  Have you tried this training & testing on only the scaled
data?

On Thu, Jan 15, 2015 at 10:42 AM, Devl Devel <devl.development@gmail.com>
wrote:

> Thanks, that helps a bit at least with the NaN but the MSE is still very
> high even with that step size and 10k iterations:
>
> training Mean Squared Error =3D 3.3322561285919316E7
>
> Does this method need say 100k iterations?
>
>
>
>
>
>
> On Thu, Jan 15, 2015 at 5:42 PM, Robin East <robin.east@xense.co.uk>
> wrote:
>
> > -dev, +user
> >
> > You=E2=80=99ll need to set the gradient descent step size to something =
small - a
> > bit of trial and error shows that 0.00000001 works.
> >
> > You=E2=80=99ll need to create a LinearRegressionWithSGD instance and se=
t the step
> > size explicitly:
> >
> > val lr =3D new LinearRegressionWithSGD()
> > lr.optimizer.setStepSize(0.00000001)
> > lr.optimizer.setNumIterations(100)
> > val model =3D lr.run(parsedData)
> >
> > On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.com>
> > wrote:
> >
> > From what I gather, you use LinearRegressionWithSGD to predict y or the
> > response variable given a feature vector x.
> >
> > In a simple example I used a perfectly linear dataset such that x=3Dy
> > y,x
> > 1,1
> > 2,2
> > ...
> >
> > 10000,10000
> >
> > Using the out-of-box example from the website (with and without scaling=
):
> >
> > val data =3D sc.textFile(file)
> >
> >    val parsedData =3D data.map { line =3D>
> >      val parts =3D line.split(',')
> >     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) /=
/y
> > and x
> >
> >    }
> >    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D tru=
e)
> >      .fit(parsedData.map(x =3D> x.features))
> >    val scaledData =3D parsedData
> >      .map(x =3D>
> >      LabeledPoint(x.label,
> >        scaler.transform(Vectors.dense(x.features.toArray))))
> >
> >    // Building the model
> >    val numIterations =3D 100
> >    val model =3D LinearRegressionWithSGD.train(parsedData, numIteration=
s)
> >
> >    // Evaluate model on training examples and compute training error *
> > tried using both scaledData and parsedData
> >    val valuesAndPreds =3D scaledData.map { point =3D>
> >      val prediction =3D model.predict(point.features)
> >      (point.label, prediction)
> >    }
> >    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p),
> 2)}.mean()
> >    println("training Mean Squared Error =3D " + MSE)
> >
> > Both scaled and unscaled attempts give:
> >
> > training Mean Squared Error =3D NaN
> >
> > I've even tried x, y+(sample noise from normal with mean 0 and stddev 1=
)
> > still comes up with the same thing.
> >
> > Is this not supposed to work for x and y or 2 dimensional plots? Is the=
re
> > something I'm missing or wrong in the code above? Or is there a
> limitation
> > in the method?
> >
> > Thanks for any advice.
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegressio=
nWithSGD-accuracy-tp10127.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
> >
>

--e89a8fb1fa7ee35545050cb6a19a--

From dev-return-11144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 20:53:18 2015
Return-Path: <dev-return-11144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 267DA17872
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 20:53:18 +0000 (UTC)
Received: (qmail 31317 invoked by uid 500); 15 Jan 2015 20:53:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31242 invoked by uid 500); 15 Jan 2015 20:53:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31225 invoked by uid 99); 15 Jan 2015 20:53:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:53:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 20:52:53 +0000
Received: by mail-ob0-f174.google.com with SMTP id wo20so5739889obc.5
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 12:52:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=nKpr/e6F1ewXholz7cHrBzwm2ZMrPMitg72xkuWLkCg=;
        b=Il0j1PLZq7XXwZHOdi+UosbDoUSNNFBsfzt3+82MC3LEK6A1W48JSVsfAD1Cz11VJt
         2HVyhexWJ7MuNMy+vNiVqmoHo4e/EitM+zz/yQDXPQWw6sgOLMiUFelwZ/FlC2Ja/SLR
         Je7wPLD2OPArYZUYnaisbBuf/aPL/y86fNwUmJCczYeXGUyP06aDd3TJPDD5ZhC7SeGj
         ibzD+Ki0C2x39OtET/jI7nzlTXfbvndZzT/x70Hz9ohcU7ZtCkB0Kmo8HY0M9u63LCFD
         Z0hcO6lxo4KtTLZ+yEOYA3ufLXUyPb7cvG5VtPoZ1J7dm1psXNF/nriIBsXou/j5Xwtl
         e5Vw==
X-Gm-Message-State: ALoCoQkZbigvz/HS1e3xWhwuGco+xa7goV8/rnW+4HBwdGE7rlHRDwKlbfRqsK2I17qDGgFE7e1U
X-Received: by 10.202.4.5 with SMTP id 5mr6901843oie.22.1421355150846;
        Thu, 15 Jan 2015 12:52:30 -0800 (PST)
Received: from mail-ob0-f172.google.com (mail-ob0-f172.google.com. [209.85.214.172])
        by mx.google.com with ESMTPSA id x65sm1167382oix.23.2015.01.15.12.52.29
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 15 Jan 2015 12:52:29 -0800 (PST)
Received: by mail-ob0-f172.google.com with SMTP id va8so15707298obc.3
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 12:52:29 -0800 (PST)
X-Received: by 10.182.181.106 with SMTP id dv10mr7298928obc.9.1421355149074;
 Thu, 15 Jan 2015 12:52:29 -0800 (PST)
MIME-Version: 1.0
Received: by 10.182.182.7 with HTTP; Thu, 15 Jan 2015 12:52:08 -0800 (PST)
In-Reply-To: <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
References: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
 <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 15 Jan 2015 15:52:08 -0500
Message-ID: <CA+-p3AEidaQgANoUGOoM0856swWxPUpDuzutFJsA=efSrg4f5Q@mail.gmail.com>
Subject: Re: Join implementation in SparkSQL
To: Reynold Xin <rxin@databricks.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01294cd2c2a9da050cb7089a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01294cd2c2a9da050cb7089a
Content-Type: text/plain; charset=UTF-8

What Reynold is describing is a performance optimization in implementation,
but the semantics of the join (cartesian product plus relational algebra
filter) should be the same and produce the same results.

On Thu, Jan 15, 2015 at 1:36 PM, Reynold Xin <rxin@databricks.com> wrote:

> It's a bunch of strategies defined here:
>
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
>
> In most common use cases (e.g. inner equi join), filters are pushed below
> the join or into the join. Doing a cartesian product followed by a filter
> is too expensive.
>
>
> On Thu, Jan 15, 2015 at 7:39 AM, Alessandro Baretta <alexbaretta@gmail.com
> >
> wrote:
>
> > Hello,
> >
> > Where can I find docs about how joins are implemented in SparkSQL? In
> > particular, I'd like to know whether they are implemented according to
> > their relational algebra definition as filters on top of a cartesian
> > product.
> >
> > Thanks,
> >
> > Alex
> >
>

--089e01294cd2c2a9da050cb7089a--

From dev-return-11145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 21:28:41 2015
Return-Path: <dev-return-11145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6782117970
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 21:28:41 +0000 (UTC)
Received: (qmail 3642 invoked by uid 500); 15 Jan 2015 21:28:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3568 invoked by uid 500); 15 Jan 2015 21:28:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3543 invoked by uid 99); 15 Jan 2015 21:28:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 21:28:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 21:28:16 +0000
Received: by mail-qa0-f51.google.com with SMTP id f12so12140295qad.10
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 13:25:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=4h9FE621ovAdX+wAeOJ8FtnDD9M7mX9q2KT8uInIdq4=;
        b=ihZmwPmBSnWiaKOf+QzbyMXbDlctG+E2AlrI9mzmCiIWlBgSBn7ltyvbGnc7nTJt1c
         JrxeTSIjEHi9YIUwigQaK6miphCPZTn2yJAWNi4h+tJrmW8C7CMLtCWk6uHHfToDfGph
         +X0JTLyzod53mOZ9fw2b+OE7nW1/pkxV+j6/wiGzxeSNEc4NzW4xY+F9NiS8ZTDMKW9e
         07eHlqBfTzz8rOu5IhS+t7DTbhIE7A/ZXyIqJFP+9emSaDmzkCbv7a8yGoSLZOkAYU4P
         HKeuTbgETvKWaaGbV8jf/1CZnSuAomX1jT6yojRoVbaDsPl9OSmTrR+LleOFTrV7KUXm
         sZSQ==
X-Gm-Message-State: ALoCoQkgt32UPlcxl+8YyocJZ1EIzLZZ7Ks3dOFwVe0YxS52bAolEaxq/tmjQna3jr7fakejIQbp
X-Received: by 10.224.7.69 with SMTP id c5mr19585257qac.71.1421357138382; Thu,
 15 Jan 2015 13:25:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 15 Jan 2015 13:25:18 -0800 (PST)
In-Reply-To: <CAOHP_tHDJJntWMYhV_+7v5MTXDiESrdtHeDbv_sjnMYOJFkb4A@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
 <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com>
 <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com> <CAOHP_tHDJJntWMYhV_+7v5MTXDiESrdtHeDbv_sjnMYOJFkb4A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 15 Jan 2015 13:25:18 -0800
Message-ID: <CAPh_B=b-GM5GCVrWQQ-i-5L-7-aOdQzM30BODkKJ4cm+TJhCtA@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Corey Nolet <cjnolet@gmail.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Yin Huai <yhuai@databricks.com>
Content-Type: multipart/alternative; boundary=001a11c24eda5537b9050cb77fe8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24eda5537b9050cb77fe8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

We can look into some sort of util class in sql.types for general type
inference. In general many methods in JsonRDD might be useful enough to
extract. Those will probably be marked as DeveloperAPI with less stability
guarantees.

On Thu, Jan 15, 2015 at 12:16 PM, Corey Nolet <cjnolet@gmail.com> wrote:

> Reynold,
>
> One thing I'd like worked into the public portion of the API is the json
> inferencing logic that creates a Set[(String, StructType)] out of
> Map[String,Any]. SPARK-5260 addresses this so that I can use Accumulators
> to infer my schema instead of forcing a map/reduce phase to occur on an R=
DD
> in order to get the final schema. Do you (or anyone else) see a path
> forward in exposing this to users? A utility class perhaps?
>
> On Thu, Jan 15, 2015 at 1:33 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Alex,
>>
>> I didn't communicate properly. By "private", I simply meant the
>> expectation
>> that it is not a public API. The plan is to still omit it from the
>> scaladoc/javadoc generation, but no language visibility modifier will be
>> applied on them.
>>
>> After 1.3, you will likely no longer need to use things in sql.catalyst
>> package directly. Programmatically construct SchemaRDDs is going to be a
>> first class public API. Data types have already been moved out of the
>> sql.catalyst package and now lives in sql.types. They are becoming stabl=
e
>> public APIs. When the "data frame" patch is submitted, you will see a
>> public expression library also. There will be few reason for end users o=
r
>> library developers to hook into things in sql.catalyst. For the bravest
>> and
>> the most advanced, they can still use them, with the expectation that it
>> is
>> subject to change.
>>
>>
>>
>>
>>
>> On Thu, Jan 15, 2015 at 7:53 AM, Alessandro Baretta <
>> alexbaretta@gmail.com>
>> wrote:
>>
>> > Reynold,
>> >
>> > Thanks for the heads up. In general, I strongly oppose the use of
>> > "private" to restrict access to certain parts of the API, the reason
>> being
>> > that I might find the need to use some of the internals of a library
>> from
>> > my own project. I find that a @DeveloperAPI annotation serves the same
>> > purpose as "private" without imposing unnecessary restrictions: it
>> > discourages people from using the annotated API and reserves the right
>> for
>> > the core developers to change it suddenly in backwards incompatible
>> ways.
>> >
>> > In particular, I would like to express the desire that the APIs to
>> > programmatically construct SchemaRDDs from an RDD[Row] and a StructTyp=
e
>> > remain public. All the SparkSQL data type objects should be exposed by
>> the
>> > API, and the jekyll build should not hide the docs as it does now.
>> >
>> > Thanks.
>> >
>> > Alex
>> >
>> > On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com>
>> wrote:
>> >
>> >> Hi Spark devs,
>> >>
>> >> Given the growing number of developers that are building on Spark SQL=
,
>> we
>> >> would like to stabilize the API in 1.3 so users and developers can be
>> >> confident to build on it. This also gives us a chance to improve the
>> API.
>> >>
>> >> In particular, we are proposing the following major changes. This
>> should
>> >> have no impact for most users (i.e. those running SQL through the JDB=
C
>> >> client or SQLContext.sql method).
>> >>
>> >> 1. Everything in sql.catalyst package is private to the project.
>> >>
>> >> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL fo=
r
>> >> SchemaRDD and logical plans in order to construct test cases. We have
>> >> received feedback from a lot of users that the DSL can be incredibly
>> >> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it su=
itable for
>> >> not
>> >> only constructing test cases, but also in everyday data pipelines. Th=
e
>> new
>> >> SchemaRDD API is inspired by the data frame concept in Pandas and R.
>> >>
>> >> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expos=
e
>> one
>> >> set of APIs that will work for both Java and Scala. The current Java
>> API
>> >> (sql.api.java) does not share any common ancestor with the Scala API.
>> This
>> >> led to high maintenance burden for us as Spark developers and for
>> library
>> >> developers. We propose to eliminate the Java specific API, and simply
>> work
>> >> on the existing Scala API to make it also usable for Java. This will
>> make
>> >> Java a first class citizen as Scala. This effectively means that all
>> >> public
>> >> classes should be usable for both Scala and Java, including SQLContex=
t,
>> >> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
>> >>
>> >>
>> >> Again, this should have no impact on most users since the existing DS=
L
>> is
>> >> rarely used by end users. However, library developers might need to
>> change
>> >> the import statements because we are moving certain classes around. W=
e
>> >> will
>> >> keep you posted as patches are merged.
>> >>
>> >
>> >
>>
>
>

--001a11c24eda5537b9050cb77fe8--

From dev-return-11146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 15 23:30:34 2015
Return-Path: <dev-return-11146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6D4117D91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 Jan 2015 23:30:34 +0000 (UTC)
Received: (qmail 70729 invoked by uid 500); 15 Jan 2015 23:30:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70645 invoked by uid 500); 15 Jan 2015 23:30:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70597 invoked by uid 99); 15 Jan 2015 23:30:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 23:30:35 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of devl.development@gmail.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 Jan 2015 23:30:31 +0000
Received: by mail-ie0-f172.google.com with SMTP id tr6so17888053ieb.3
        for <dev@spark.apache.org>; Thu, 15 Jan 2015 15:30:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=2TcR/EM0W//mwkXzYSvhdk3uboBZMa/snLiWTcYeHkM=;
        b=lACM7OR6O2cZzXKsgM+SdMNf1wZWAOf7MT17wu26HGyMEyPQsKBbK4gmtlpgUwgqHi
         XCv9hXAD+kzQxuoWvxvZ8KmMs3w1+mvWq0dXG1Fh2gGaN1CZ6JnqTViniAlztLEtu5I5
         y9lPx3R8KzvsLFTKYErs3f0Xpz+1Ssk3IJwYVlUCtM8H5U5P/cCDLIUE1MwlnJnv+Ulb
         6lEWEpngCzYrkkuIF0a9t0c/ajyoUcGenhd7rqeU6161EvUJNq/obTl02zhiu2ZQkUbx
         WL77mGZCTiFcJbbPzRrMT8mDDdomfn3LdtGtnvoWKR7zdAeTtunMtSuRBANTxKIFf2Cj
         pT1g==
MIME-Version: 1.0
X-Received: by 10.107.10.207 with SMTP id 76mr13316395iok.78.1421364610318;
 Thu, 15 Jan 2015 15:30:10 -0800 (PST)
Received: by 10.64.130.98 with HTTP; Thu, 15 Jan 2015 15:30:10 -0800 (PST)
In-Reply-To: <CAF7ADNq7yhYURfzAPNSZvWZwv89JVcUyGb8POJU8pbd6qw4nVg@mail.gmail.com>
References: <1421340401649-10127.post@n3.nabble.com>
	<56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
	<CAMQ+LQNLYXWL+cYUci7nTEFhGuDqn-vArvPUhjcqdDPAPM=6hg@mail.gmail.com>
	<CAF7ADNq7yhYURfzAPNSZvWZwv89JVcUyGb8POJU8pbd6qw4nVg@mail.gmail.com>
Date: Thu, 15 Jan 2015 23:30:10 +0000
Message-ID: <CAMQ+LQMGPD1HJW8BH6UDm+=CiqJXY-4Gcvdp1VM_Xtw25y5vpQ@mail.gmail.com>
Subject: Re: LinearRegressionWithSGD accuracy
From: Devl Devel <devl.development@gmail.com>
To: Joseph Bradley <joseph@databricks.com>
Cc: Robin East <robin.east@xense.co.uk>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f9968b1e9b8050cb93cae
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f9968b1e9b8050cb93cae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It was a bug in the code, however adding the step parameter got the results
to work.  Mean Squared Error =3D 2.610379825794694E-5

I've also opened a jira to put the step parameter in the examples so that
people new to mllib have a way to improve the MSE.

https://issues.apache.org/jira/browse/SPARK-5273

On Thu, Jan 15, 2015 at 8:23 PM, Joseph Bradley <joseph@databricks.com>
wrote:

> It looks like you're training on the non-scaled data but testing on the
> scaled data.  Have you tried this training & testing on only the scaled
> data?
>
> On Thu, Jan 15, 2015 at 10:42 AM, Devl Devel <devl.development@gmail.com>
> wrote:
>
>> Thanks, that helps a bit at least with the NaN but the MSE is still very
>> high even with that step size and 10k iterations:
>>
>> training Mean Squared Error =3D 3.3322561285919316E7
>>
>> Does this method need say 100k iterations?
>>
>>
>>
>>
>>
>>
>> On Thu, Jan 15, 2015 at 5:42 PM, Robin East <robin.east@xense.co.uk>
>> wrote:
>>
>> > -dev, +user
>> >
>> > You=E2=80=99ll need to set the gradient descent step size to something=
 small - a
>> > bit of trial and error shows that 0.00000001 works.
>> >
>> > You=E2=80=99ll need to create a LinearRegressionWithSGD instance and s=
et the
>> step
>> > size explicitly:
>> >
>> > val lr =3D new LinearRegressionWithSGD()
>> > lr.optimizer.setStepSize(0.00000001)
>> > lr.optimizer.setNumIterations(100)
>> > val model =3D lr.run(parsedData)
>> >
>> > On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.com=
>
>> > wrote:
>> >
>> > From what I gather, you use LinearRegressionWithSGD to predict y or th=
e
>> > response variable given a feature vector x.
>> >
>> > In a simple example I used a perfectly linear dataset such that x=3Dy
>> > y,x
>> > 1,1
>> > 2,2
>> > ...
>> >
>> > 10000,10000
>> >
>> > Using the out-of-box example from the website (with and without
>> scaling):
>> >
>> > val data =3D sc.textFile(file)
>> >
>> >    val parsedData =3D data.map { line =3D>
>> >      val parts =3D line.split(',')
>> >     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble))
>> //y
>> > and x
>> >
>> >    }
>> >    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D tr=
ue)
>> >      .fit(parsedData.map(x =3D> x.features))
>> >    val scaledData =3D parsedData
>> >      .map(x =3D>
>> >      LabeledPoint(x.label,
>> >        scaler.transform(Vectors.dense(x.features.toArray))))
>> >
>> >    // Building the model
>> >    val numIterations =3D 100
>> >    val model =3D LinearRegressionWithSGD.train(parsedData, numIteratio=
ns)
>> >
>> >    // Evaluate model on training examples and compute training error *
>> > tried using both scaledData and parsedData
>> >    val valuesAndPreds =3D scaledData.map { point =3D>
>> >      val prediction =3D model.predict(point.features)
>> >      (point.label, prediction)
>> >    }
>> >    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p),
>> 2)}.mean()
>> >    println("training Mean Squared Error =3D " + MSE)
>> >
>> > Both scaled and unscaled attempts give:
>> >
>> > training Mean Squared Error =3D NaN
>> >
>> > I've even tried x, y+(sample noise from normal with mean 0 and stddev =
1)
>> > still comes up with the same thing.
>> >
>> > Is this not supposed to work for x and y or 2 dimensional plots? Is
>> there
>> > something I'm missing or wrong in the code above? Or is there a
>> limitation
>> > in the method?
>> >
>> > Thanks for any advice.
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegressi=
onWithSGD-accuracy-tp10127.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>> >
>>
>
>

--001a113f9968b1e9b8050cb93cae--

From dev-return-11147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 01:25:17 2015
Return-Path: <dev-return-11147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B68BCC185
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 01:25:17 +0000 (UTC)
Received: (qmail 57317 invoked by uid 500); 16 Jan 2015 01:25:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57250 invoked by uid 500); 16 Jan 2015 01:25:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57239 invoked by uid 99); 16 Jan 2015 01:25:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 01:25:17 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hao.cheng@intel.com designates 192.55.52.88 as permitted sender)
Received: from [192.55.52.88] (HELO mga01.intel.com) (192.55.52.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 01:24:53 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga101.fm.intel.com with ESMTP; 15 Jan 2015 17:23:50 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.09,407,1418112000"; 
   d="scan'208";a="662674890"
Received: from pgsmsx104.gar.corp.intel.com ([10.221.44.91])
  by fmsmga002.fm.intel.com with ESMTP; 15 Jan 2015 17:23:49 -0800
Received: from shsmsx101.ccr.corp.intel.com (10.239.4.153) by
 PGSMSX104.gar.corp.intel.com (10.221.44.91) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Fri, 16 Jan 2015 09:22:11 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.238]) by
 SHSMSX101.ccr.corp.intel.com ([169.254.1.64]) with mapi id 14.03.0195.001;
 Fri, 16 Jan 2015 09:22:09 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Andrew Ash <andrew@andrewash.com>, Reynold Xin <rxin@databricks.com>
CC: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Join implementation in SparkSQL
Thread-Topic: Join implementation in SparkSQL
Thread-Index: AQHQMNnpBUWxaaf790qoCXZNT2fzE5zA/ICAgAAl8ACAAMViMA==
Date: Fri, 16 Jan 2015 01:22:09 +0000
Message-ID: <80833ADD533E324CA05C160E41B63661027DBC68@shsmsx102.ccr.corp.intel.com>
References: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
 <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
 <CA+-p3AEidaQgANoUGOoM0856swWxPUpDuzutFJsA=efSrg4f5Q@mail.gmail.com>
In-Reply-To: <CA+-p3AEidaQgANoUGOoM0856swWxPUpDuzutFJsA=efSrg4f5Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Tm90IHNvIHN1cmUgYWJvdXQgeW91ciBxdWVzdGlvbiwgYnV0IHRoZSBTcGFya1N0cmF0ZWdpZXMu
c2NhbGEgYW5kIE9wdGltaXplci5zY2FsYSBpcyBhIGdvb2Qgc3RhcnQgaWYgeW91IHdhbnQgdG8g
Z2V0IGRldGFpbHMgb2YgdGhlIGpvaW4gaW1wbGVtZW50YXRpb24gb3Igb3B0aW1pemF0aW9uLg0K
DQotLS0tLU9yaWdpbmFsIE1lc3NhZ2UtLS0tLQ0KRnJvbTogQW5kcmV3IEFzaCBbbWFpbHRvOmFu
ZHJld0BhbmRyZXdhc2guY29tXSANClNlbnQ6IEZyaWRheSwgSmFudWFyeSAxNiwgMjAxNSA0OjUy
IEFNDQpUbzogUmV5bm9sZCBYaW4NCkNjOiBBbGVzc2FuZHJvIEJhcmV0dGE7IGRldkBzcGFyay5h
cGFjaGUub3JnDQpTdWJqZWN0OiBSZTogSm9pbiBpbXBsZW1lbnRhdGlvbiBpbiBTcGFya1NRTA0K
DQpXaGF0IFJleW5vbGQgaXMgZGVzY3JpYmluZyBpcyBhIHBlcmZvcm1hbmNlIG9wdGltaXphdGlv
biBpbiBpbXBsZW1lbnRhdGlvbiwgYnV0IHRoZSBzZW1hbnRpY3Mgb2YgdGhlIGpvaW4gKGNhcnRl
c2lhbiBwcm9kdWN0IHBsdXMgcmVsYXRpb25hbCBhbGdlYnJhDQpmaWx0ZXIpIHNob3VsZCBiZSB0
aGUgc2FtZSBhbmQgcHJvZHVjZSB0aGUgc2FtZSByZXN1bHRzLg0KDQpPbiBUaHUsIEphbiAxNSwg
MjAxNSBhdCAxOjM2IFBNLCBSZXlub2xkIFhpbiA8cnhpbkBkYXRhYnJpY2tzLmNvbT4gd3JvdGU6
DQoNCj4gSXQncyBhIGJ1bmNoIG9mIHN0cmF0ZWdpZXMgZGVmaW5lZCBoZXJlOg0KPg0KPiBodHRw
czovL2dpdGh1Yi5jb20vYXBhY2hlL3NwYXJrL2Jsb2IvbWFzdGVyL3NxbC9jb3JlL3NyYy9tYWlu
L3NjYWxhL29yDQo+IGcvYXBhY2hlL3NwYXJrL3NxbC9leGVjdXRpb24vU3BhcmtTdHJhdGVnaWVz
LnNjYWxhDQo+DQo+IEluIG1vc3QgY29tbW9uIHVzZSBjYXNlcyAoZS5nLiBpbm5lciBlcXVpIGpv
aW4pLCBmaWx0ZXJzIGFyZSBwdXNoZWQgDQo+IGJlbG93IHRoZSBqb2luIG9yIGludG8gdGhlIGpv
aW4uIERvaW5nIGEgY2FydGVzaWFuIHByb2R1Y3QgZm9sbG93ZWQgYnkgDQo+IGEgZmlsdGVyIGlz
IHRvbyBleHBlbnNpdmUuDQo+DQo+DQo+IE9uIFRodSwgSmFuIDE1LCAyMDE1IGF0IDc6MzkgQU0s
IEFsZXNzYW5kcm8gQmFyZXR0YSANCj4gPGFsZXhiYXJldHRhQGdtYWlsLmNvbQ0KPiA+DQo+IHdy
b3RlOg0KPg0KPiA+IEhlbGxvLA0KPiA+DQo+ID4gV2hlcmUgY2FuIEkgZmluZCBkb2NzIGFib3V0
IGhvdyBqb2lucyBhcmUgaW1wbGVtZW50ZWQgaW4gU3BhcmtTUUw/IA0KPiA+IEluIHBhcnRpY3Vs
YXIsIEknZCBsaWtlIHRvIGtub3cgd2hldGhlciB0aGV5IGFyZSBpbXBsZW1lbnRlZCANCj4gPiBh
Y2NvcmRpbmcgdG8gdGhlaXIgcmVsYXRpb25hbCBhbGdlYnJhIGRlZmluaXRpb24gYXMgZmlsdGVy
cyBvbiB0b3AgDQo+ID4gb2YgYSBjYXJ0ZXNpYW4gcHJvZHVjdC4NCj4gPg0KPiA+IFRoYW5rcywN
Cj4gPg0KPiA+IEFsZXgNCj4gPg0KPg0K
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 13:49:56 2015
Return-Path: <dev-return-11148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 22A36108F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 13:49:56 +0000 (UTC)
Received: (qmail 95383 invoked by uid 500); 16 Jan 2015 13:49:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95308 invoked by uid 500); 16 Jan 2015 13:49:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95297 invoked by uid 99); 16 Jan 2015 13:49:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 13:49:55 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [212.124.192.207] (HELO vicki.2020media.net.uk) (212.124.192.207)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 13:49:50 +0000
Received: from [86.7.249.6] (helo=[192.168.0.25])
	by vicki.2020media.net.uk with esmtpsa (TLSv1:AES256-SHA:256)
	(Exim 4.72)
	(envelope-from <robin.east@xense.co.uk>)
	id 1YC7GE-0001sW-Cq
	for dev@spark.apache.org; Fri, 16 Jan 2015 13:48:26 +0000
From: Robin East <robin.east@xense.co.uk>
Content-Type: multipart/alternative;
	boundary=Apple-Mail-244BE641-E9E2-4041-90DA-0ED02C755CBB
Content-Transfer-Encoding: 7bit
Mime-Version: 1.0 (1.0)
Subject: Fwd: LinearRegressionWithSGD accuracy
Message-Id: <15B3D656-6BF7-4251-B1D6-370AE91E420F@xense.co.uk>
Date: Fri, 16 Jan 2015 13:44:53 +0000
References: <3C517F4E-8013-42E0-A575-FB573816E418@xense.co.uk>
To: dev@spark.apache.org
X-Mailer: iPhone Mail (12B411)
X-2020-Relay: Sent using 2020MEDIA.net.uk relay with auth code: xense
 Send Abuse reports to abuse@2020media.net.uk
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-244BE641-E9E2-4041-90DA-0ED02C755CBB
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable



Sent from my iPhone

Begin forwarded message:

> From: Robin East <robin.east@xense.co.uk>
> Date: 16 January 2015 11:35:23 GMT
> To: Joseph Bradley <joseph@databricks.com>
> Cc: Yana Kadiyska <yana.kadiyska@gmail.com>, Devl Devel <devl.development@=
gmail.com>
> Subject: Re: LinearRegressionWithSGD accuracy
>=20
> Yes with scaled data intercept would be 5000 but the code as it stands is r=
unning a model where intercept will be 0.00. You need to call setIntercept(t=
rue) to include the intercept in the model.
>=20
> Robin
>=20
> Sent from my iPhone
>=20
>> On 16 Jan 2015, at 02:01, Joseph Bradley <joseph@databricks.com> wrote:
>>=20
>> Good point about using the intercept.  When scaling uses the mean (shifti=
ng the feature values), then the "true" model now has an intercept of 5000.5=
, whereas the original data's "true" model has an intercept of 0.  I think t=
hat's the issue.
>>=20
>>> On Thu, Jan 15, 2015 at 5:16 PM, Yana Kadiyska <yana.kadiyska@gmail.com>=
 wrote:
>>> I can actually reproduce his MSE -- with the scaled data only (non-scale=
d works out just fine)
>>>=20
>>> import org.apache.spark.mllib.regression._
>>> import org.apache.spark.mllib.linalg.{Vector, Vectors}
>>>=20
>>> val t=3D(1 to 10000).map(x=3D>(x,x))
>>> val rdd =3D sc.parallelize(t)
>>> val parsedData =3D  rdd.map(q=3D>LabeledPoint(q._1.toDouble,Vectors.dens=
e(q._2.toDouble))
>>>=20
>>> val lr =3D new LinearRegressionWithSGD()
>>> lr.optimizer.setStepSize(0.00000001)
>>> lr.optimizer.setNumIterations(100)
>>>=20
>>> val scaledData =3D parsedData.map(x =3D> LabeledPoint(x.label, scaler.tr=
ansform(Vectors.dense(x.features.toArray))))
>>> val model =3D lr.run(scaledData)
>>>=20
>>> val valuesAndPreds =3D scaledData.map { point =3D>
>>>       val prediction =3D model.predict(point.features)
>>>       (prediction,point.label)
>>>     }
>>> val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p), 2)}.mea=
n()
>>> Last few lines read as:
>>>=20
>>> 15/01/15 16:16:40 INFO GradientDescent: GradientDescent.runMiniBatchSGD f=
inished. Last 10 stochastic losses 3.3338313007386144E7, 3.333831299679853E7=
, 3.333831298621632E7, 3.333831297563938E7, 3.3338312965067785E7, 3.33383129=
54501465E7, 3.333831294394051E7, 3.3338312933384743E7, 3.33383129228344E7, 3=
.3338312912289333E7
>>> 15/01/15 16:16:40 WARN LinearRegressionWithSGD: The input data was not d=
irectly cached, which may hurt performance if its parent RDDs are also uncac=
hed.
>>> model: org.apache.spark.mllib.regression.LinearRegressionModel =3D (weig=
hts=3D[0.003567902277776811], intercept=3D0.0)
>>>=20
>>> So I am a bit puzzled as I was under the impression that a scaled model w=
ould only converge faster. Non-scaled version produced near perfect results a=
t alpha=3D0.00000001,numIterations=3D100
>>>=20
>>> According to R the weights should be a lot higher:
>>> y=3Dseq(1, 10000)
>>> X=3Dscale(a, center =3D TRUE, scale =3D TRUE)
>>> dt=3Ddata.frame(y,X)
>>> names(dt) =3D c("y","x")
>>> model=3D lm(y~x,data=3Ddt)
>>> #intercept:5000.5,2886.896
>>> new <- data.frame(x=3Ddt$x)
>>> preds =3D predict(model,new)
>>> mean( (preds-dt$y)^2 , na.rm =3D TRUE )
>>> Coefficients:
>>> (Intercept)            x =20
>>>        5000.5,  2886.896
>>>=20
>>> I did have success with the following model and scaled features as shown=
 in the original code block:
>>>=20
>>> val lr =3D new LinearRegressionWithSGD().setIntercept(true)
>>> lr.optimizer.setStepSize(0.1)
>>> lr.optimizer.setNumIterations(1000)
>>>=20
>>> scala> model
>>> res12: org.apache.spark.mllib.regression.LinearRegressionModel =3D (weig=
hts=3D[2886.885094323781], intercept=3D5000.48169121784)
>>> MSE: Double =3D 4.472548743491049E-4
>>>=20
>>> Not sure that it's a question for the dev list as much as someone who un=
derstands ML well -- I'd appreciate if you guys have any insight on why the s=
mall alpha/numIters did so poorly on the scaled data (I've removed the dev l=
ist)
>>>=20
>>>=20
>>>=20
>>>=20
>>>> On Thu, Jan 15, 2015 at 3:23 PM, Joseph Bradley <joseph@databricks.com>=
 wrote:
>>>=20
>>>> It looks like you're training on the non-scaled data but testing on the=

>>>> scaled data.  Have you tried this training & testing on only the scaled=

>>>> data?
>>>>=20
>>>> On Thu, Jan 15, 2015 at 10:42 AM, Devl Devel <devl.development@gmail.co=
m>
>>>> wrote:
>>>>=20
>>>> > Thanks, that helps a bit at least with the NaN but the MSE is still v=
ery
>>>> > high even with that step size and 10k iterations:
>>>> >
>>>> > training Mean Squared Error =3D 3.3322561285919316E7
>>>> >
>>>> > Does this method need say 100k iterations?
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > On Thu, Jan 15, 2015 at 5:42 PM, Robin East <robin.east@xense.co.uk>
>>>> > wrote:
>>>> >
>>>> > > -dev, +user
>>>> > >
>>>> > > You=E2=80=99ll need to set the gradient descent step size to someth=
ing small - a
>>>> > > bit of trial and error shows that 0.00000001 works.
>>>> > >
>>>> > > You=E2=80=99ll need to create a LinearRegressionWithSGD instance an=
d set the step
>>>> > > size explicitly:
>>>> > >
>>>> > > val lr =3D new LinearRegressionWithSGD()
>>>> > > lr.optimizer.setStepSize(0.00000001)
>>>> > > lr.optimizer.setNumIterations(100)
>>>> > > val model =3D lr.run(parsedData)
>>>> > >
>>>> > > On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.=
com>
>>>> > > wrote:
>>>> > >
>>>> > > =46rom what I gather, you use LinearRegressionWithSGD to predict y o=
r the
>>>> > > response variable given a feature vector x.
>>>> > >
>>>> > > In a simple example I used a perfectly linear dataset such that x=3D=
y
>>>> > > y,x
>>>> > > 1,1
>>>> > > 2,2
>>>> > > ...
>>>> > >
>>>> > > 10000,10000
>>>> > >
>>>> > > Using the out-of-box example from the website (with and without sca=
ling):
>>>> > >
>>>> > > val data =3D sc.textFile(file)
>>>> > >
>>>> > >    val parsedData =3D data.map { line =3D>
>>>> > >      val parts =3D line.split(',')
>>>> > >     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble=
)) //y
>>>> > > and x
>>>> > >
>>>> > >    }
>>>> > >    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D=
 true)
>>>> > >      .fit(parsedData.map(x =3D> x.features))
>>>> > >    val scaledData =3D parsedData
>>>> > >      .map(x =3D>
>>>> > >      LabeledPoint(x.label,
>>>> > >        scaler.transform(Vectors.dense(x.features.toArray))))
>>>> > >
>>>> > >    // Building the model
>>>> > >    val numIterations =3D 100
>>>> > >    val model =3D LinearRegressionWithSGD.train(parsedData, numItera=
tions)
>>>> > >
>>>> > >    // Evaluate model on training examples and compute training erro=
r *
>>>> > > tried using both scaledData and parsedData
>>>> > >    val valuesAndPreds =3D scaledData.map { point =3D>
>>>> > >      val prediction =3D model.predict(point.features)
>>>> > >      (point.label, prediction)
>>>> > >    }
>>>> > >    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p),=

>>>> > 2)}.mean()
>>>> > >    println("training Mean Squared Error =3D " + MSE)
>>>> > >
>>>> > > Both scaled and unscaled attempts give:
>>>> > >
>>>> > > training Mean Squared Error =3D NaN
>>>> > >
>>>> > > I've even tried x, y+(sample noise from normal with mean 0 and stdd=
ev 1)
>>>> > > still comes up with the same thing.
>>>> > >
>>>> > > Is this not supposed to work for x and y or 2 dimensional plots? Is=
 there
>>>> > > something I'm missing or wrong in the code above? Or is there a
>>>> > limitation
>>>> > > in the method?
>>>> > >
>>>> > > Thanks for any advice.
>>>> > >
>>>> > >
>>>> > >
>>>> > > --
>>>> > > View this message in context:
>>>> > >
>>>> > http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegre=
ssionWithSGD-accuracy-tp10127.html
>>>> > > Sent from the Apache Spark Developers List mailing list archive at
>>>> > > Nabble.com.
>>>> > >
>>>> > > -------------------------------------------------------------------=
--
>>>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> > > For additional commands, e-mail: dev-help@spark.apache.org
>>>> > >
>>>> > >
>>>> > >
>>>> >
>>=20

--Apple-Mail-244BE641-E9E2-4041-90DA-0ED02C755CBB--

From dev-return-11149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 16:18:05 2015
Return-Path: <dev-return-11149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E9D110FCB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 16:18:05 +0000 (UTC)
Received: (qmail 94646 invoked by uid 500); 16 Jan 2015 16:18:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94574 invoked by uid 500); 16 Jan 2015 16:18:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94563 invoked by uid 99); 16 Jan 2015 16:18:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 16:18:05 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of michael.belldavies@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 16:17:39 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 69D4610BF3B4
	for <dev@spark.apache.org>; Fri, 16 Jan 2015 08:17:09 -0800 (PST)
Date: Fri, 16 Jan 2015 09:17:07 -0700 (MST)
From: Mick Davies <michael.belldavies@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421425027439-10141.post@n3.nabble.com>
Subject: Optimize encoding/decoding strings when using Parquet
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, 

It seems that a reasonably large proportion of query time using Spark SQL
seems to be spent decoding Parquet Binary objects to produce Java Strings.
Has anyone considered trying to optimize these conversions as many are
duplicated.

Details are outlined in the conversation in the user mailing list 
http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html
<http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html> 
, I have copied a bit of that discussion here.

It seems that as Spark processes each row from Parquet it makes a call to
convert the Binary representation for each String column into a Java String.
However in many (probably most) circumstances the underlying Binary instance
from Parquet will have come from a Dictionary, for example when column
cardinality is low. Therefore Spark is converting the same byte array to a
copy of the same Java String over and over again. This is bad due to extra
cpu, extra memory used for these strings, and probably results in more
expensive grouping comparisons. 


I tested a simple hack to cache the last Binary->String conversion per
column in ParquetConverter and this led to a 25% performance improvement for
the queries I used. Admittedly this was over a data set with lots or runs of
the same Strings in the queried columns. 

These costs are quite significant for the type of data that I expect will be
stored in Parquet which will often have denormalized tables and probably
lots of fairly low cardinality string columns 

I think a good way to optimize this would be if changes could be made to
Parquet so that  the encoding/decoding of Objects to Binary is handled on
Parquet side of fence. Parquet could deal with Objects (Strings) as the
client understands them and only use encoding/decoding to store/read from
underlying storage medium. Doing this I think Parquet could ensure that the
encoding/decoding of each Object occurs only once. 

Does anyone have an opinion on this, has it been considered already?

Cheers Mick







--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 16:26:55 2015
Return-Path: <dev-return-11150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEC8310046
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 16:26:55 +0000 (UTC)
Received: (qmail 13308 invoked by uid 500); 16 Jan 2015 16:26:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13231 invoked by uid 500); 16 Jan 2015 16:26:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13213 invoked by uid 99); 16 Jan 2015 16:26:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 16:26:56 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [157.193.49.127] (HELO smtp3.ugent.be) (157.193.49.127)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 16:26:30 +0000
Received: from localhost (mcheck2.ugent.be [157.193.49.249])
	by smtp3.ugent.be (Postfix) with ESMTP id 0A8E4C3AD
	for <dev@spark.apache.org>; Fri, 16 Jan 2015 17:26:29 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp3.ugent.be ([IPv6:::ffff:157.193.49.127])
	by localhost (mcheck2.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id mmuZsNTwr0m0 for <dev@spark.apache.org>;
	Fri, 16 Jan 2015 17:26:28 +0100 (CET)
Received: from [157.193.44.243] (gast044c.ugent.be [157.193.44.243])
	(Authenticated sender: ehiggs)
	by smtp3.ugent.be (Postfix) with ESMTPSA id E4FE3C375
	for <dev@spark.apache.org>; Fri, 16 Jan 2015 17:26:27 +0100 (CET)
Message-ID: <54B93BB3.10401@ugent.be>
Date: Fri, 16 Jan 2015 17:26:27 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: RDD order guarantees
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Miltered: at jchkm1 with ID 54B93BB3.00A by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 54B93BB3.00A from gast044c.ugent.be/gast044c.ugent.be/157.193.44.243/[157.193.44.243]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 54B93BB3.00A on smtp3.ugent.be : j-chkmail score : . : R=. U=. O=. B=0.000 -> S=0.000
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,
Quick one: when reading files, are the orders of partitions guaranteed 
to be preserved? I am finding some weird behaviour where I run 
sortByKeys() on an RDD (which has 16 byte keys) and write it to disk. If 
I open a python shell and run the following:

for part in range(29):
     print map(ord, 
open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part), 
'r').read(16))

Then each partition is in order based on the first value of each partition.

I can also call TeraValidate.validate from TeraSort and it is happy with 
the results. It seems to be on loading the file that the reordering 
happens. If this is expected, is there a way to ask Spark nicely to give 
me the RDD in the order it was saved?

This is based on trying to fix my TeraValidate code on this branch:
https://github.com/ehiggs/spark/tree/terasort

Thanks,
Ewan

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 17:58:05 2015
Return-Path: <dev-return-11151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3ACEE104B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 17:58:05 +0000 (UTC)
Received: (qmail 9042 invoked by uid 500); 16 Jan 2015 17:58:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8970 invoked by uid 500); 16 Jan 2015 17:58:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8958 invoked by uid 99); 16 Jan 2015 17:58:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 17:58:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sparkhealthanalytics@gmail.com designates 209.85.217.194 as permitted sender)
Received: from [209.85.217.194] (HELO mail-lb0-f194.google.com) (209.85.217.194)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 17:57:39 +0000
Received: by mail-lb0-f194.google.com with SMTP id u14so2017765lbd.1
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 09:56:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=9rCvup6J7sMmJSMuDP/iLoxLf1I4rh3GJzzm7fTdwuU=;
        b=lW3fkqAOQY0O4sf+ZRCdvRJmI/kvy8YSIwkhzp9E5F8jK0gptkIQNuRdtHJk9IWKoe
         4siUa1Q51WvlrOu6yRqHgC2gx7BKMRKhs1PcdSVDudtERcgU2SWAJ6Jhdnxhv+OzrIc0
         p6YlMxz3HJJbefJ9BYMKHgAfEnqWMJVU9vxZ+LRiRFaEcmHaapW+3oidefhA8mhM6BbP
         7jfOdhhz+hnCOBdnQ+ZazudyTo4yb8WAqRU6EJQ2H5rwumsucEsaKAPxT8YZFYTYO+S4
         HqT6WQqsEGJ63DZMUhDvy9n1Cix0ViNQn3ejAUBxVZ7CX6ky4jPCT5KltQ7b6rPpbpRq
         7ADA==
MIME-Version: 1.0
X-Received: by 10.153.5.1 with SMTP id ci1mr16599332lad.67.1421430968124; Fri,
 16 Jan 2015 09:56:08 -0800 (PST)
Received: by 10.112.5.5 with HTTP; Fri, 16 Jan 2015 09:56:08 -0800 (PST)
Date: Fri, 16 Jan 2015 12:56:08 -0500
Message-ID: <CAERo_Xsdb8-xz2XXzovZEYUnQnJgn7z6VpvxcZgP7U6Y7pBfiQ@mail.gmail.com>
Subject: Setting JVM options to Spark executors in Standalone mode
From: Michel Dufresne <sparkhealthanalytics@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133a74eed8b0d050cc8afcc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a74eed8b0d050cc8afcc
Content-Type: text/plain; charset=UTF-8

Hi All,

I'm trying to set some JVM options to the executor processes in a
standalone cluster. Here's what I have in *spark-env.sh*:

jmx_opt="-Dcom.sun.management.jmxremote"
> jmx_opt="${jmx_opt} -Djava.net.preferIPv4Stack=true"
> jmx_opt="${jmx_opt} -Dcom.sun.management.jmxremote.port=9999"
> jmx_opt="${jmx_opt} -Dcom.sun.management.jmxremote.rmi.port=9998"
> jmx_opt="${jmx_opt} -Dcom.sun.management.jmxremote.ssl=false"
> jmx_opt="${jmx_opt} -Dcom.sun.management.jmxremote.authenticate=false"
> jmx_opt="${jmx_opt} -Djava.rmi.server.hostname=${SPARK_PUBLIC_DNS}"
> export SPARK_WORKER_OPTS="${jmx_opt}"


However the option are showing up on the *daemon* JVM not the *workers*. It
has the same effect as if I was using SPARK_DAEMON_JAVA_OPTS (which should
set it on the daemon process).

Thanks in advance for your help,

Michel

--001a1133a74eed8b0d050cc8afcc--

From dev-return-11152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 18:03:18 2015
Return-Path: <dev-return-11152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AC951050B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 18:03:18 +0000 (UTC)
Received: (qmail 25854 invoked by uid 500); 16 Jan 2015 18:03:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25775 invoked by uid 500); 16 Jan 2015 18:03:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25764 invoked by uid 99); 16 Jan 2015 18:03:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:03:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zzhang@hortonworks.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:02:51 +0000
Received: by mail-pa0-f54.google.com with SMTP id fb1so25627028pad.13
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 10:02:49 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to:content-type:content-transfer-encoding;
        bh=IVSD2nMgRh8zCrcS0S1+UpNex6JTThlF/X9/HwBLwVs=;
        b=iAWmlJJVBl0ulJHhm2m2Pa6hK/FLJA9+Ss+7fkkUtQb4hZF6vfNuSD2FqIdmXEPBf9
         rpc+VNpNLJC4EXGrSOJspY3EqVayoIicYPiRuynOnpwy5ADrDXzFyP8JmGSiPuw1i1Q7
         UbuNVnfBvS5DSUiCOg0PwaSmEKjnSBrZU3LUviMcHdYnil+/bh+fLdYvPHDk9Vc+QRtD
         mktKPitHcWQ+NLAX4bevfYrl2/uZP7mXLxqlg2ytrc373/9jPUK/6JZhGTaFpWXxBbgi
         pkDulPniyvt5VINFhFSgKorvgsXXMAJzNIBNVCyQTx/dPVP3nbsL6tJESLdD9kmKS7Np
         Lqnw==
X-Gm-Message-State: ALoCoQlBypmv9Ac5KnW+yP93z1XGInLnTuL5AZz4DBXSpp1hWQbMZf63fOq1dPF9yRB5w6ezp16qJn1b7MMYOsI7aO37TT+mCNX4RVYVivdsDi7JDwX/k88=
X-Received: by 10.66.97.97 with SMTP id dz1mr24665738pab.90.1421431369677;
        Fri, 16 Jan 2015 10:02:49 -0800 (PST)
Received: from [10.11.3.202] (outbound.hortonworks.com. [192.175.27.2])
        by mx.google.com with ESMTPSA id qh4sm4657716pdb.57.2015.01.16.10.02.48
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 16 Jan 2015 10:02:49 -0800 (PST)
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Setting JVM options to Spark executors in Standalone mode
From: Zhan Zhang <zzhang@hortonworks.com>
In-Reply-To: <CAERo_Xsdb8-xz2XXzovZEYUnQnJgn7z6VpvxcZgP7U6Y7pBfiQ@mail.gmail.com>
Date: Fri, 16 Jan 2015 10:02:47 -0800
Cc: dev@spark.apache.org
Message-Id: <CDBFF4F1-9D83-4913-96E9-F8F0F2D9B81D@hortonworks.com>
References: <CAERo_Xsdb8-xz2XXzovZEYUnQnJgn7z6VpvxcZgP7U6Y7pBfiQ@mail.gmail.com>
To: Michel Dufresne <sparkhealthanalytics@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

You can try to add it in in conf/spark-defaults.conf

 # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=3Dvalue -Dnum=
bers=3D"one two three=94

Thanks.

Zhan Zhang

On Jan 16, 2015, at 9:56 AM, Michel Dufresne <sparkhealthanalytics@gmail.co=
m> wrote:

> Hi All,
>=20
> I'm trying to set some JVM options to the executor processes in a
> standalone cluster. Here's what I have in *spark-env.sh*:
>=20
> jmx_opt=3D"-Dcom.sun.management.jmxremote"
>> jmx_opt=3D"${jmx_opt} -Djava.net.preferIPv4Stack=3Dtrue"
>> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.port=3D9999"
>> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.rmi.port=3D9998"
>> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.ssl=3Dfalse"
>> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.authenticate=3Dfals=
e"
>> jmx_opt=3D"${jmx_opt} -Djava.rmi.server.hostname=3D${SPARK_PUBLIC_DNS}"
>> export SPARK_WORKER_OPTS=3D"${jmx_opt}"
>=20
>=20
> However the option are showing up on the *daemon* JVM not the *workers*. =
It
> has the same effect as if I was using SPARK_DAEMON_JAVA_OPTS (which shoul=
d
> set it on the daemon process).
>=20
> Thanks in advance for your help,
>=20
> Michel


--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 18:08:42 2015
Return-Path: <dev-return-11153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C4DF10561
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 18:08:42 +0000 (UTC)
Received: (qmail 41549 invoked by uid 500); 16 Jan 2015 18:08:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41471 invoked by uid 500); 16 Jan 2015 18:08:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41459 invoked by uid 99); 16 Jan 2015 18:08:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:08:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sparkhealthanalytics@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:08:38 +0000
Received: by mail-lb0-f172.google.com with SMTP id l4so7036143lbv.3
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 10:07:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tOmqQfTqzKNGChup/z0Hy2xDXrBOJWacaxLJC8hAhBw=;
        b=onyRqphOCRqpp5XZ2tC8V5nmScrewSLXmmy5jcABEp3p4NZ4RKaONpIUiWN3gS8na0
         j6UD4nwDBxJL1stXJMGTDbk2D2Tnh6x2HzvOKALlf1XEzN0SwuXS8XpWU+aEykJy6YIs
         sUzcrVr+8Gr57P/76HprSU1Mf6IXAtY3yI6U0dDo4oRPI+fcj0OXVkGNCJECT1xVELw7
         l2gDpf1LevDoDPb3wjpabGU5HRy3W29xO81ttQMfJc+HiUApia2MkIihakeF8jlXfTQG
         pMG3Rfx0tkXxfBK6fHF2KDycSVHXe6XluUAdVNZAM41E7HR4a/9RFvDJHVRgWYgqxvsI
         G3xw==
MIME-Version: 1.0
X-Received: by 10.112.131.1 with SMTP id oi1mr16867034lbb.2.1421431651823;
 Fri, 16 Jan 2015 10:07:31 -0800 (PST)
Received: by 10.112.5.5 with HTTP; Fri, 16 Jan 2015 10:07:31 -0800 (PST)
In-Reply-To: <CDBFF4F1-9D83-4913-96E9-F8F0F2D9B81D@hortonworks.com>
References: <CAERo_Xsdb8-xz2XXzovZEYUnQnJgn7z6VpvxcZgP7U6Y7pBfiQ@mail.gmail.com>
	<CDBFF4F1-9D83-4913-96E9-F8F0F2D9B81D@hortonworks.com>
Date: Fri, 16 Jan 2015 13:07:31 -0500
Message-ID: <CAERo_XvKxa0mhAYJy=DAWftykAdL0odcxXEG79UU_Ju7L64BUw@mail.gmail.com>
Subject: Re: Setting JVM options to Spark executors in Standalone mode
From: Michel Dufresne <sparkhealthanalytics@gmail.com>
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b343106adf612050cc8d8c8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b343106adf612050cc8d8c8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thank for your reply, I've should have mentioned that spark-env.sh is the
only option i found because:

   - I'm passing the public IP address of the slave (which is determined in
   the shell script)
   - I'm creating the SpeakConf/SparkContext from a Play Application
   (therefore I'm not using spark-submit script)

Thanks

On Fri, Jan 16, 2015 at 1:02 PM, Zhan Zhang <zzhang@hortonworks.com> wrote:

> You can try to add it in in conf/spark-defaults.conf
>
>  # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=3Dvalue
> -Dnumbers=3D"one two three=E2=80=9D
>
> Thanks.
>
> Zhan Zhang
>
> On Jan 16, 2015, at 9:56 AM, Michel Dufresne <
> sparkhealthanalytics@gmail.com> wrote:
>
> > Hi All,
> >
> > I'm trying to set some JVM options to the executor processes in a
> > standalone cluster. Here's what I have in *spark-env.sh*:
> >
> > jmx_opt=3D"-Dcom.sun.management.jmxremote"
> >> jmx_opt=3D"${jmx_opt} -Djava.net.preferIPv4Stack=3Dtrue"
> >> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.port=3D9999"
> >> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.rmi.port=3D9998"
> >> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.ssl=3Dfalse"
> >> jmx_opt=3D"${jmx_opt} -Dcom.sun.management.jmxremote.authenticate=3Dfa=
lse"
> >> jmx_opt=3D"${jmx_opt} -Djava.rmi.server.hostname=3D${SPARK_PUBLIC_DNS}=
"
> >> export SPARK_WORKER_OPTS=3D"${jmx_opt}"
> >
> >
> > However the option are showing up on the *daemon* JVM not the *workers*=
.
> It
> > has the same effect as if I was using SPARK_DAEMON_JAVA_OPTS (which
> should
> > set it on the daemon process).
> >
> > Thanks in advance for your help,
> >
> > Michel
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>

--047d7b343106adf612050cc8d8c8--

From dev-return-11154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 18:18:22 2015
Return-Path: <dev-return-11154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8205105BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 18:18:22 +0000 (UTC)
Received: (qmail 68067 invoked by uid 500); 16 Jan 2015 18:18:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67990 invoked by uid 500); 16 Jan 2015 18:18:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67978 invoked by uid 99); 16 Jan 2015 18:18:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:18:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:18:18 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so9083773qgd.13
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 10:15:42 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=qyWUcTQhtCgXPxklfwjPLXxybcgcPhqZ/qurexBRvGM=;
        b=LFHsiQjrD07L2djnA8S6BWvxr8dmtxLFdfBAwnwN8nAbMbhQX0EsFeph5MVeEWr8tU
         owUo+aiYdvhnzfQm2Tu1kIG+W4z3fjnKuHdfX9D86SPoao6zkaCjwjcwNBNXSF+VM+ry
         WFw9674xox4EqZ56zSOY2eLUM40xHDA6EE3KUPFPIqyK2gUA8PBsct5J8UQauofomRID
         ifpBP9BdmTt8OfFTHWav+YKJ/ALjXZzI4Jf9nFR00us57ip/ylnAAyYRLT/pDOVIWTtT
         c1oO3Wr7GK4LfANKfSW/4P6kKRZrk9/6Y5VK/Q4ciEwhZjrbeBi0BdZXknbkTG0OnGzt
         eyzA==
X-Gm-Message-State: ALoCoQkEJfeYKMYMWqVvWfKLisTd5TUE9j3Y1ulkQJtKnnYgWpakXIs2d/CL8XVLf662xIkjl5jK
MIME-Version: 1.0
X-Received: by 10.140.100.234 with SMTP id s97mr17077829qge.96.1421432142407;
 Fri, 16 Jan 2015 10:15:42 -0800 (PST)
Received: by 10.229.155.2 with HTTP; Fri, 16 Jan 2015 10:15:42 -0800 (PST)
In-Reply-To: <CAERo_XvKxa0mhAYJy=DAWftykAdL0odcxXEG79UU_Ju7L64BUw@mail.gmail.com>
References: <CAERo_Xsdb8-xz2XXzovZEYUnQnJgn7z6VpvxcZgP7U6Y7pBfiQ@mail.gmail.com>
	<CDBFF4F1-9D83-4913-96E9-F8F0F2D9B81D@hortonworks.com>
	<CAERo_XvKxa0mhAYJy=DAWftykAdL0odcxXEG79UU_Ju7L64BUw@mail.gmail.com>
Date: Fri, 16 Jan 2015 10:15:42 -0800
Message-ID: <CAAOnQ7vyjZ612u55+LiGPZZbgOcH5H35dAikTMb0SrF0wcvimA@mail.gmail.com>
Subject: Re: Setting JVM options to Spark executors in Standalone mode
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Michel Dufresne <sparkhealthanalytics@gmail.com>
Cc: Zhan Zhang <zzhang@hortonworks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, Jan 16, 2015 at 10:07 AM, Michel Dufresne
<sparkhealthanalytics@gmail.com> wrote:
> Thank for your reply, I've should have mentioned that spark-env.sh is the
> only option i found because:
>
>    - I'm creating the SpeakConf/SparkContext from a Play Application
>    (therefore I'm not using spark-submit script)

Then you can set that configuration Zhan mentions directly in your
SparkConf object.

BTW the env variable for what you want is SPARK_EXECUTOR_OPTS, but the
use of env variables to set app configuration is discouraged.


-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 18:44:08 2015
Return-Path: <dev-return-11155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DEAC71072C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 18:44:07 +0000 (UTC)
Received: (qmail 62972 invoked by uid 500); 16 Jan 2015 18:44:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62897 invoked by uid 500); 16 Jan 2015 18:44:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62882 invoked by uid 99); 16 Jan 2015 18:44:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:44:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:44:04 +0000
Received: by mail-qc0-f180.google.com with SMTP id r5so12201449qcx.11
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 10:41:52 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=56iFjQ7eCMgJZ4Gtu9p34Hm9RAUYr1lMIMRj7y2SDyE=;
        b=ZlsHUvWTKqzxS5wUVwe5/UNGrdBnGUmVM3opVh4+NzejnIXuDGphWAdRPk02SwzI67
         vrUChLtnhZWE5w3/yud03CbVt6ySVPxwj/VMyFsoSJIgb1BySDKFSO2JjFWFn0HCiTqy
         nlKeLDhOYljkyZ8M8WVFLQ+dSAqvNl5wRC8S/KQDv9AiGAJSQJGV6x31PZW0wPoMMLWT
         rpTYpOiWGeqo3YPz4MgzOWqauNoKzMvvglWLqK8xOlm4lKFRoaJuRtsl3tqQdcvVEqO/
         OS2uldw88V+LFLNLe+EtD7UJCYkwZAbyXoCr0hVtjlJk06kojgLzBNEW3W17QzUsqjIm
         ZSGg==
X-Gm-Message-State: ALoCoQlSwJcSUCJf5yeoi6rABQlytaRiVIYp0DdKP7DUHHAe3AgjgSOkiRknNjmYX8UloqkFSlZz
X-Received: by 10.140.38.114 with SMTP id s105mr26162174qgs.106.1421433712220;
 Fri, 16 Jan 2015 10:41:52 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Fri, 16 Jan 2015 10:41:32 -0800 (PST)
In-Reply-To: <54B93BB3.10401@ugent.be>
References: <54B93BB3.10401@ugent.be>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 16 Jan 2015 10:41:32 -0800
Message-ID: <CAPh_B=ZzCLJAus-7Jt2VYMkngZkDyMbsqdKt4DZEHcGZCwxswQ@mail.gmail.com>
Subject: Re: RDD order guarantees
To: Ewan Higgs <ewan.higgs@ugent.be>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1303e7d312c050cc95372
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1303e7d312c050cc95372
Content-Type: text/plain; charset=UTF-8

You are running on a local file system right? HDFS orders the file based on
names, but local file system often don't. I think that's why the difference.

We might be able to do a sort and order the partitions when we create a RDD
to make this universal though.

On Fri, Jan 16, 2015 at 8:26 AM, Ewan Higgs <ewan.higgs@ugent.be> wrote:

> Hi all,
> Quick one: when reading files, are the orders of partitions guaranteed to
> be preserved? I am finding some weird behaviour where I run sortByKeys() on
> an RDD (which has 16 byte keys) and write it to disk. If I open a python
> shell and run the following:
>
> for part in range(29):
>     print map(ord, open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part),
> 'r').read(16))
>
> Then each partition is in order based on the first value of each partition.
>
> I can also call TeraValidate.validate from TeraSort and it is happy with
> the results. It seems to be on loading the file that the reordering
> happens. If this is expected, is there a way to ask Spark nicely to give me
> the RDD in the order it was saved?
>
> This is based on trying to fix my TeraValidate code on this branch:
> https://github.com/ehiggs/spark/tree/terasort
>
> Thanks,
> Ewan
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c1303e7d312c050cc95372--

From dev-return-11156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 18:55:04 2015
Return-Path: <dev-return-11156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A7BD107C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 18:55:04 +0000 (UTC)
Received: (qmail 88814 invoked by uid 500); 16 Jan 2015 18:55:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88742 invoked by uid 500); 16 Jan 2015 18:55:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88731 invoked by uid 99); 16 Jan 2015 18:55:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:55:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 18:54:56 +0000
Received: by mail-la0-f52.google.com with SMTP id hs14so20496238lab.11
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 10:54:15 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Dypt0N7w+wakEt1//BO8GlMpFzUXiR7XzX1jVTqMn2Y=;
        b=KRSQ86jIPn+kW0EVqHSGQpjWslhAWjNefD5icLM1iJC/Q06q4HB1rLUpr/8mlDVmaP
         OHsj9kC7aeglKVb40Z28RLhg4lYyUI/alr+uXuh29zbNK2+HRoo2Gr/8CIOXTMnMBeNs
         E0jlj3+c0kx8P7C8nXStqTayDy3TwTTxDEwi0MNtGI3mN0m8jU4v/DFRBaeiH+F4S/UD
         m3PA8HBamcnMDMGhDOkyD/lONsXni9c2itnYYyNhD+2ToVgMQDO6sHRuh2xe+yeEwE1N
         W8i0BUsUKUrdzALbNUiXSll+BAgH35XXzgLgbP+o8b3JSkPMmJqEXYjERMyuoQBhAP6L
         C9Lw==
X-Gm-Message-State: ALoCoQnIRhgvLiUUnO4rNyr9vMaspuEeQaKY9bmDEsN+ANi4S2cxLt/UU21Oh1d/ljZ6o6ssi/U9
X-Received: by 10.112.198.233 with SMTP id jf9mr11312649lbc.9.1421434454889;
 Fri, 16 Jan 2015 10:54:14 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Fri, 16 Jan 2015 10:53:54 -0800 (PST)
In-Reply-To: <1421425027439-10141.post@n3.nabble.com>
References: <1421425027439-10141.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 16 Jan 2015 10:53:54 -0800
Message-ID: <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
Subject: Re: Optimize encoding/decoding strings when using Parquet
To: Mick Davies <michael.belldavies@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c32cf4c16f11050cc97f60
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c32cf4c16f11050cc97f60
Content-Type: text/plain; charset=UTF-8

+1 to adding such an optimization to parquet.  The bytes are tagged
specially as UTF8 in the parquet schema so it seem like it would be
possible to add this.

On Fri, Jan 16, 2015 at 8:17 AM, Mick Davies <michael.belldavies@gmail.com>
wrote:

> Hi,
>
> It seems that a reasonably large proportion of query time using Spark SQL
> seems to be spent decoding Parquet Binary objects to produce Java Strings.
> Has anyone considered trying to optimize these conversions as many are
> duplicated.
>
> Details are outlined in the conversation in the user mailing list
>
> http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html
> <
> http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html
> >
> , I have copied a bit of that discussion here.
>
> It seems that as Spark processes each row from Parquet it makes a call to
> convert the Binary representation for each String column into a Java
> String.
> However in many (probably most) circumstances the underlying Binary
> instance
> from Parquet will have come from a Dictionary, for example when column
> cardinality is low. Therefore Spark is converting the same byte array to a
> copy of the same Java String over and over again. This is bad due to extra
> cpu, extra memory used for these strings, and probably results in more
> expensive grouping comparisons.
>
>
> I tested a simple hack to cache the last Binary->String conversion per
> column in ParquetConverter and this led to a 25% performance improvement
> for
> the queries I used. Admittedly this was over a data set with lots or runs
> of
> the same Strings in the queried columns.
>
> These costs are quite significant for the type of data that I expect will
> be
> stored in Parquet which will often have denormalized tables and probably
> lots of fairly low cardinality string columns
>
> I think a good way to optimize this would be if changes could be made to
> Parquet so that  the encoding/decoding of Objects to Binary is handled on
> Parquet side of fence. Parquet could deal with Objects (Strings) as the
> client understands them and only use encoding/decoding to store/read from
> underlying storage medium. Doing this I think Parquet could ensure that the
> encoding/decoding of each Object occurs only once.
>
> Does anyone have an opinion on this, has it been considered already?
>
> Cheers Mick
>
>
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c32cf4c16f11050cc97f60--

From dev-return-11157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 19:53:09 2015
Return-Path: <dev-return-11157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4980610A9D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 19:53:09 +0000 (UTC)
Received: (qmail 41945 invoked by uid 500); 16 Jan 2015 19:53:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41872 invoked by uid 500); 16 Jan 2015 19:53:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41856 invoked by uid 99); 16 Jan 2015 19:53:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 19:53:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andrew.musselman@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 19:53:04 +0000
Received: by mail-qg0-f45.google.com with SMTP id z107so17809381qgd.4
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 11:51:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=QGwJKUEKFrDk043gwLnVQL3KCPDJFWhyKLK0D2aaH/A=;
        b=fn8yxxA/jJ1M+k6i/BQS6ZsGw5FuhjJYZtHQ+6JmVqJ/MjJvvMw6fhf5wg/+qOMV8A
         u4L8kvHz38jRYJvaOkM1asgkFE5Wenq++hq+Y6B3ir2W9crzRJ86FdYjZ3cz8Y/2p8U4
         bwsqaY64iX/NhvrrIDfgg0vt/uT9fIeDsYuv0kbfM9XxbeNB+HL0IfJUOh3lpWEFYFSh
         dSqcOYREoqriL9wWPiS0w5HP7jTb5McSPc3qnlCfygZ41kcdC/taxs7b4LpHa87Q90m7
         9/foECp94aNKJBEkyy9KJpADQlF5VNPn+u3zd1w5d6+ORrGiVcZ7q0E5Wl8jll5jmCPw
         tM4w==
MIME-Version: 1.0
X-Received: by 10.140.17.70 with SMTP id 64mr26324363qgc.53.1421437873595;
 Fri, 16 Jan 2015 11:51:13 -0800 (PST)
Received: by 10.140.27.236 with HTTP; Fri, 16 Jan 2015 11:51:13 -0800 (PST)
Date: Fri, 16 Jan 2015 11:51:13 -0800
Message-ID: <CANg8BGCqzd6xCcixqFMmC1Wtcrv7oSJf7kM7LTu2wfkHCSf0Gg@mail.gmail.com>
Subject: Spark
From: Andrew Musselman <andrew.musselman@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c0b2aa86adc4050cca4b95
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c0b2aa86adc4050cca4b95
Content-Type: text/plain; charset=UTF-8



--001a11c0b2aa86adc4050cca4b95--

From dev-return-11158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 20:06:46 2015
Return-Path: <dev-return-11158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 169E410B29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 20:06:46 +0000 (UTC)
Received: (qmail 89527 invoked by uid 500); 16 Jan 2015 20:06:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89464 invoked by uid 500); 16 Jan 2015 20:06:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89452 invoked by uid 99); 16 Jan 2015 20:06:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 20:06:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andrew.musselman@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 20:06:19 +0000
Received: by mail-qc0-f174.google.com with SMTP id c9so18706341qcz.5
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 12:04:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=4kM1CqMkmTs5h8qcRG4QfyTd1osj4H1Ayv5j2VxBWNQ=;
        b=cLZOwhcbllDDFS3ADr/kNAt2b42G0RLH8bH6Y5e61Z2a3pTwmDWx1DMGy2tTOQBOBr
         Sm4YeZ5VYONBvihAm9Sd3LLis67layMUSlhgONpEjAwlrAWLcs/ddTHz0ZtuRqoMBOrY
         2CFBKOn+vxFbPFKSQdBdThpI0D3GK1OemOu1sZTOMOZ9pjktDqvHbOgs/dzFXNzEKolS
         Ks2sbsZhdcQecPdGsrERdCvRD9QB+eXkLG6rIuKT2eo6s02n7Qtnfv2Of8nId4oq56Ef
         RHndVJIYZDJ32bPtA2e6UUGl854ruE9HVFXrPvPiaydrWrGqGQZIHnkEvViKwB9D95u1
         y3FA==
MIME-Version: 1.0
X-Received: by 10.229.130.65 with SMTP id r1mr28400876qcs.16.1421438687605;
 Fri, 16 Jan 2015 12:04:47 -0800 (PST)
Received: by 10.140.27.236 with HTTP; Fri, 16 Jan 2015 12:04:47 -0800 (PST)
Date: Fri, 16 Jan 2015 12:04:47 -0800
Message-ID: <CANg8BGDW4Bx31qHNALZ-_Y4uSxy1RsAJmcD9-bdAxzqgemC2Sg@mail.gmail.com>
Subject: Spectral clustering
From: Andrew Musselman <andrew.musselman@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133d79a0b7a02050cca7c76
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133d79a0b7a02050cca7c76
Content-Type: text/plain; charset=UTF-8

Hi, thinking of picking up this Jira ticket:
https://issues.apache.org/jira/browse/SPARK-4259

Anyone done any work on this to date?  Any thoughts on it before we go too
far in?

Thanks!

Best
Andrew

--001a1133d79a0b7a02050cca7c76--

From dev-return-11159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 21:02:20 2015
Return-Path: <dev-return-11159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D04610D75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 21:02:20 +0000 (UTC)
Received: (qmail 33134 invoked by uid 500); 16 Jan 2015 21:02:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33059 invoked by uid 500); 16 Jan 2015 21:02:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33047 invoked by uid 99); 16 Jan 2015 21:02:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 21:02:20 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kushal.datta@gmail.com designates 209.85.223.182 as permitted sender)
Received: from [209.85.223.182] (HELO mail-ie0-f182.google.com) (209.85.223.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 21:01:55 +0000
Received: by mail-ie0-f182.google.com with SMTP id x19so22794358ier.13
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 13:01:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=W4PV9A0qHJDscEXqUQsxNLEvVaj5oRn2yXj4wmLcFJo=;
        b=x0khfgXT55dw45Zd0dIsNN/MqVj2kvDVQVqNLQYjyrQOPoOwKF9NnZMwtFYo9Z7PiO
         vY0WUN5NUIJtZsFjDZTmb+LUAOR63hrPkmlTDALRtaiK2hgTDIJBth9PyKZEF6jr5ICC
         6CfvIRAFc//CxQXbMUam5yx6zX+Yr9ZX04cHCXt0CrPdM/Oow9JCBGkjlygFU7bFBDR2
         PS59Xl4WScGXKm1SDh87Xpn/aUgBnALxIGJrl6i/1kVf72deOLwkK+iWlqdOgCKl/le4
         SOo5aNtFiMuIRvDVDOet7IGg2NJkeLbajPsKZiX262lb2kXbGsMwYAFDl18T0u+HfUI/
         Hqig==
X-Received: by 10.107.131.133 with SMTP id n5mr18672774ioi.30.1421442113343;
 Fri, 16 Jan 2015 13:01:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.64.199 with HTTP; Fri, 16 Jan 2015 13:01:33 -0800 (PST)
In-Reply-To: <1421339495591-10126.post@n3.nabble.com>
References: <CAKXMip3DT9sRwCu2VkWN5KkfSKu+az+_RtV6m-GVhuK52_55SA@mail.gmail.com>
 <D08139AE.44FB%brennon.york@capitalone.com> <CANjHi9q7rwGUVroDQF2+u=P+2ZBFevq4PjQxpykSvWZwD4wRZg@mail.gmail.com>
 <D08150FA.45D7%brennon.york@capitalone.com> <CAKXMip1PhE37UmezLM0EG+NZo20Pqdicii0AQE7AYJoXt0ceHQ@mail.gmail.com>
 <CAPh_B=YiOJiOJ8uSy4Hn4YKjTPgTkH+QZEW=WMoOihmpgKgh6g@mail.gmail.com>
 <CAKXMip0k5ZGO+QO79G53=b-Epz+-oBFr=BiMDV0hCgaMurWdKA@mail.gmail.com>
 <CANjHi9qNEtk1PGnUceLMo1xupaP8MhtFXVvF6bQVuXOrD5Ab+Q@mail.gmail.com>
 <CAKXMip21Yx3fKioQ+5OpJrqVybuOoopAjocsn2g0JMv=HktFCg@mail.gmail.com>
 <CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com> <1421339495591-10126.post@n3.nabble.com>
From: Kushal Datta <kushal.datta@gmail.com>
Date: Fri, 16 Jan 2015 13:01:33 -0800
Message-ID: <CANjHi9okJXCN7YFQRF7Gox7pWZB2QJBqbcnvDDGHUL4JjGms6w@mail.gmail.com>
Subject: Re: Implementing TinkerPop on top of GraphX
To: David Robinson <drobin1437@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ebbf83c5512050ccb488f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ebbf83c5512050ccb488f
Content-Type: text/plain; charset=UTF-8

Hi David,


Yes, we are still headed in that direction.
Please take a look at the repo I sent earlier.
I think that's a good starting point.

Thanks,
-Kushal.

On Thu, Jan 15, 2015 at 8:31 AM, David Robinson <drobin1437@gmail.com>
wrote:

> I am new to Spark and GraphX, however, I use Tinkerpop backed graphs and
> think the idea of using Tinkerpop as the API for GraphX is a great idea and
> hope you are still headed in that direction.  I noticed that Tinkerpop 3 is
> moving into the Apache family:
> http://wiki.apache.org/incubator/TinkerPopProposal  which might alleviate
> concerns about having an API definition "outside" of Spark.
>
> Thanks,
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Implementing-TinkerPop-on-top-of-GraphX-tp9169p10126.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113ebbf83c5512050ccb488f--

From dev-return-11160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 21:55:23 2015
Return-Path: <dev-return-11160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A71EB10006
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 21:55:23 +0000 (UTC)
Received: (qmail 35149 invoked by uid 500); 16 Jan 2015 21:55:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35073 invoked by uid 500); 16 Jan 2015 21:55:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35046 invoked by uid 99); 16 Jan 2015 21:55:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 21:55:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 21:55:19 +0000
Received: by mail-oi0-f41.google.com with SMTP id i138so19451875oig.0
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 13:54:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=MjpFWCCQVRkou9unD+Mik5LV5hSOUTHWJzH+ZlFgUt8=;
        b=tGvEPmkL8QKSsVUYzD2zLunoQfLQimrrne+9KPq+LkgRMz8yjDBhSodVmKnBCskw2y
         IFcNQWjKemupezJUYRO3Izg2tzgRut7au5wLpx7Xxadxplq3T4DB0gNGQoLl4AXwrXNi
         4q1r/c3Z+pMzhCww/8B3bvdZ9sfm/HpRRTHtPvvi4wXSJzMSGyjqa7ROlCY7lymhP5cu
         mIRgH0ToZ9kjgXfh+K0nB15ESF5h36wlabulpH9L1SbqbYAr6Y4rYNr0h3+hRbreaewk
         xrLF8NpIbY3Dvw7P+TanZ2ErgSzRaa0obHoKi6C8wSHPMJJEW3gly/9I8IeJvBoYnnl4
         AUFQ==
X-Received: by 10.202.216.9 with SMTP id p9mr10369926oig.94.1421445299337;
 Fri, 16 Jan 2015 13:54:59 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Fri, 16 Jan 2015 13:54:39 -0800 (PST)
In-Reply-To: <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
References: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
 <CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 16 Jan 2015 13:54:39 -0800
Message-ID: <CAJc_syKxx5cjJ9E+2G9vTaJ9PnkO5ZB9ht5ujEp0HfzM5yfauw@mail.gmail.com>
Subject: Re: Join implementation in SparkSQL
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d37ea227fc4050ccc0600
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d37ea227fc4050ccc0600
Content-Type: text/plain; charset=UTF-8

Reynold,

The source file you are directing me to is a little too terse for me to
understand what exactly is going on. Let me tell you what I'm trying to do
and what problems I'm encountering, so that you might be able to better
direct me investigation of the SparkSQL codebase.

I am computing the join of three tables, sharing the same primary key,
composed of three fields, and having several other fields. My first attempt
at computing this join was in SQL, with a query much like this slightly
simplified one:

         SELECT
          a.key1 key1, a.key2 key2, a.key3 key3,
          a.data1   adata1,    a.data2    adata2,    ...
          b.data1   bdata1,    b.data2    bdata2,    ...
          c.data1   cdata1,    c.data2    cdata2,    ...
        FROM a, b, c
        WHERE
          a.key1 = b.key1 AND a.key2 = b.key2 AND a.key3 = b.key3
          b.key1 = c.key1 AND b.key2 = c.key2 AND b.key3 = c.key3

This code yielded a SparkSQL job containing 40,000 stages, which failed
after filling up all available disk space on the worker nodes.

I then wrote this join as a plain mapreduce join. The code looks roughly
like this:
val a_ = a.map(row => (key(row), ("a", row))
val b_ = b.map(row => (key(row), ("b", row))
val c_ = c.map(row => (key(row), ("c", row"))
val join = UnionRDD(sc, List(a_, b_, c_)).groupByKey

This implementation yields approximately 1600 stages and completes in a few
minutes on a 256 core cluster. The huge difference in scale of the two jobs
makes me think that SparkSQL is implementing my join as cartesian product.
This is they query plan--I'm not sure I can read it, but it does seem to
imply that the filter conditions are not being pushed far down enough:

 'Project [...]
 'Filter (((((('a.key1 = 'b.key1)) && ('a.key2 = b.key2)) && ...)
  'Join Inner, None
   'Join Inner, None

Is maybe SparkSQL unable to push join conditions down from the WHERE clause
into the join itself?

Alex

On Thu, Jan 15, 2015 at 10:36 AM, Reynold Xin <rxin@databricks.com> wrote:

> It's a bunch of strategies defined here:
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
>
> In most common use cases (e.g. inner equi join), filters are pushed below
> the join or into the join. Doing a cartesian product followed by a filter
> is too expensive.
>
>
> On Thu, Jan 15, 2015 at 7:39 AM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Hello,
>>
>> Where can I find docs about how joins are implemented in SparkSQL? In
>> particular, I'd like to know whether they are implemented according to
>> their relational algebra definition as filters on top of a cartesian
>> product.
>>
>> Thanks,
>>
>> Alex
>>
>
>

--001a113d37ea227fc4050ccc0600--

From dev-return-11161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:12:05 2015
Return-Path: <dev-return-11161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3293100AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:12:05 +0000 (UTC)
Received: (qmail 60539 invoked by uid 500); 16 Jan 2015 22:12:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60466 invoked by uid 500); 16 Jan 2015 22:12:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60451 invoked by uid 99); 16 Jan 2015 22:12:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:12:03 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kellrott@soe.ucsc.edu designates 209.85.213.51 as permitted sender)
Received: from [209.85.213.51] (HELO mail-yh0-f51.google.com) (209.85.213.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:11:38 +0000
Received: by mail-yh0-f51.google.com with SMTP id a41so11307896yho.10
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:11:36 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=D47H9vUqgLxvl0kZfqkw9meF4HDZw5HuUmOqAp+9aZs=;
        b=PsQq9mBwuLiE9hphcGC0umfJGceRRmE3hd9m/hHDQkQMrMf0VTj9iG+dqFG4aLEwP8
         asCGX43EHbyCuuCyvh1A80vsp82XkkRjODxZq4F0AB6CHRjVs/WA3iXI1m0vmYU+DACt
         6AB7NGClvlk653vYBR+p0o1NHnjA19kpPUSfHtsqtjgUGGn/Zbg5Gv0xcdVOwqBXUMZq
         DIZo0630YGZL8zLGWzDw6f2j546hlXVJ+zetJ3Vyy2fwCcXvmlN7/tObR7/nafHdykC8
         sRZm7TN/VSRiTtZvUoUe+KA8nntmSXTPdQ7FeWrwkuOhEJtxfT684W+QCVoNhc73dKJq
         MaQA==
X-Gm-Message-State: ALoCoQkZiPwmqRitB4RyXdHPlfbTZ+XbJQYw2Bm8gGwfVAQIYsmg1exv2xL0y+7HoMh0NJ+PXoOD
MIME-Version: 1.0
X-Received: by 10.170.187.210 with SMTP id d201mr12450533yke.88.1421446296215;
 Fri, 16 Jan 2015 14:11:36 -0800 (PST)
Received: by 10.170.113.150 with HTTP; Fri, 16 Jan 2015 14:11:36 -0800 (PST)
In-Reply-To: <CANjHi9okJXCN7YFQRF7Gox7pWZB2QJBqbcnvDDGHUL4JjGms6w@mail.gmail.com>
References: <CAKXMip3DT9sRwCu2VkWN5KkfSKu+az+_RtV6m-GVhuK52_55SA@mail.gmail.com>
	<D08139AE.44FB%brennon.york@capitalone.com>
	<CANjHi9q7rwGUVroDQF2+u=P+2ZBFevq4PjQxpykSvWZwD4wRZg@mail.gmail.com>
	<D08150FA.45D7%brennon.york@capitalone.com>
	<CAKXMip1PhE37UmezLM0EG+NZo20Pqdicii0AQE7AYJoXt0ceHQ@mail.gmail.com>
	<CAPh_B=YiOJiOJ8uSy4Hn4YKjTPgTkH+QZEW=WMoOihmpgKgh6g@mail.gmail.com>
	<CAKXMip0k5ZGO+QO79G53=b-Epz+-oBFr=BiMDV0hCgaMurWdKA@mail.gmail.com>
	<CANjHi9qNEtk1PGnUceLMo1xupaP8MhtFXVvF6bQVuXOrD5Ab+Q@mail.gmail.com>
	<CAKXMip21Yx3fKioQ+5OpJrqVybuOoopAjocsn2g0JMv=HktFCg@mail.gmail.com>
	<CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com>
	<1421339495591-10126.post@n3.nabble.com>
	<CANjHi9okJXCN7YFQRF7Gox7pWZB2QJBqbcnvDDGHUL4JjGms6w@mail.gmail.com>
Date: Fri, 16 Jan 2015 14:11:36 -0800
Message-ID: <CAKXMip2bAUGrDo7AMJfXWVOFg1Ju-mmO2Kes_3hz6Vgd1bOLCA@mail.gmail.com>
Subject: Re: Implementing TinkerPop on top of GraphX
From: Kyle Ellrott <kellrott@soe.ucsc.edu>
To: Kushal Datta <kushal.datta@gmail.com>
Cc: David Robinson <drobin1437@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113997308db90c050ccc417d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113997308db90c050ccc417d
Content-Type: text/plain; charset=UTF-8

Looking at https://github.com/kdatta/tinkerpop3/compare/graphx-gremlin I
only see a maven build file. Do you have some source code some place else?

I've worked on a spark based implementation (
https://github.com/kellrott/spark-gremlin ), but its not done and I've been
tied up on other projects.
It also look Tinkerpop3 is a bit of a moving target. I had targeted the
work done for gremlin-giraph (
http://www.tinkerpop.com/docs/3.0.0.M5/#giraph-gremlin ) that was part of
the M5 release, as a base model for implementation. But that appears to
have been refactored into gremlin-hadoop (
http://www.tinkerpop.com/docs/3.0.0.M6/#hadoop-gremlin ) in the M6 release.
I need to assess how much this changes the code.

Most of the code that needs to be changes from Giraph to Spark will be
simply replacing classes with spark derived ones. The main place where the
logic will need changed is in the 'GraphComputer' class (
https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/process/computer/giraph/GiraphGraphComputer.java
) which is created by the Graph when the 'compute' method is called (
https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/structure/HadoopGraph.java#L135
).


Kyle



On Fri, Jan 16, 2015 at 1:01 PM, Kushal Datta <kushal.datta@gmail.com>
wrote:

> Hi David,
>
>
> Yes, we are still headed in that direction.
> Please take a look at the repo I sent earlier.
> I think that's a good starting point.
>
> Thanks,
> -Kushal.
>
> On Thu, Jan 15, 2015 at 8:31 AM, David Robinson <drobin1437@gmail.com>
> wrote:
>
> > I am new to Spark and GraphX, however, I use Tinkerpop backed graphs and
> > think the idea of using Tinkerpop as the API for GraphX is a great idea
> and
> > hope you are still headed in that direction.  I noticed that Tinkerpop 3
> is
> > moving into the Apache family:
> > http://wiki.apache.org/incubator/TinkerPopProposal  which might
> alleviate
> > concerns about having an API definition "outside" of Spark.
> >
> > Thanks,
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Implementing-TinkerPop-on-top-of-GraphX-tp9169p10126.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a113997308db90c050ccc417d--

From dev-return-11162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:13:40 2015
Return-Path: <dev-return-11162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56AB2100B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:13:40 +0000 (UTC)
Received: (qmail 68309 invoked by uid 500); 16 Jan 2015 22:13:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68232 invoked by uid 500); 16 Jan 2015 22:13:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68220 invoked by uid 99); 16 Jan 2015 22:13:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:13:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:13:36 +0000
Received: by mail-ob0-f174.google.com with SMTP id wo20so11432790obc.5
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:11:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=lTSH9u9Fa/Y1pQ9OZ1WfMKDi4/1HTkLiU1tgjdQIWEk=;
        b=WjzbWNYr+FV5Cdg5NgOZh1iH6LFMkGKTBV53CJsr+ol+xdNRup7W/f84hD4so0kzoP
         GxtDTRd/MBceBT6V3hrI15z85nWmMM3+63EAsY+4uOYLKPssFga5BSMKLowkDUBldy9U
         fJ2Yx4RgTNrlTF3fNHyLeP3zLsFVH2beTsNX4Vy0VphQXSkLLtAeBF/363BKZm+gXCYq
         rbjYAmOnlq62z4WuCuDdPmFIQeYn9GQdachc0YCxaY8UB342KBrZCrMgLEi8GJPF4prO
         5u0b9+u2krOWvBLXeiyL+aJM53RSgyy7gcXSoXdRrsyYvPO2NVogJg6dh7DxP+IycFM3
         kZWg==
X-Received: by 10.182.28.196 with SMTP id d4mr11151117obh.66.1421446305562;
 Fri, 16 Jan 2015 14:11:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Fri, 16 Jan 2015 14:11:25 -0800 (PST)
In-Reply-To: <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
 <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com> <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 16 Jan 2015 14:11:25 -0800
Message-ID: <CAJc_syLLkoH5k9BEpVw4sRCAKGs1zk4dmREDhuh10MpnaThpcw@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c29d461c4604050ccc4232
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29d461c4604050ccc4232
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Reynold,

Your clarification is much appreciated. One issue though, that I would
strongly encourage you to work on, is to make sure that the Scaladoc CAN be
generated manually if needed (a "Use at your own risk" clause would be
perfectly legitimate here). The reason I say this is that currently even
hacking SparkBuild.scala to include SparkSQL in the unidoc target doesn't
help, as scaladoc itself fails with errors such as these.

[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/cata=
lyst/dsl/package.scala:359:
polymorphic expression cannot be instantiated to expected type;
[error]  found   : [T(in method
apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)=
]
[error]  required:
org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in method
functionToUdfBuilder)]
[error]   implicit def functionToUdfBuilder[T: TypeTag](func: Function22[_,
_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, T]):
ScalaUdfBuilder[T] =3D ScalaUdfBuilder(func)
[error]

                            ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/cata=
lyst/expressions/codegen/CodeGenerator.scala:147:
value q is not a member of StringContext
[error]  Note: implicit class Evaluate2 is not applicable here because it
comes after the application point and it lacks an explicit result type
[error]         q"""
[error]         ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/cata=
lyst/expressions/codegen/CodeGenerator.scala:181:
value q is not a member of StringContext
[error]         q"""
[error]         ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/cata=
lyst/expressions/codegen/CodeGenerator.scala:198:
value q is not a member of StringContext

While I understand you desire to discourage users from relying on the
internal "private" APIs, there is no reason to prevent people from gaining
a better understanding of how things work by allow them--with some
effort--to get to the docs.

Thanks,

Alex

On Thu, Jan 15, 2015 at 10:33 AM, Reynold Xin <rxin@databricks.com> wrote:

> Alex,
>
> I didn't communicate properly. By "private", I simply meant the
> expectation that it is not a public API. The plan is to still omit it fro=
m
> the scaladoc/javadoc generation, but no language visibility modifier will
> be applied on them.
>
> After 1.3, you will likely no longer need to use things in sql.catalyst
> package directly. Programmatically construct SchemaRDDs is going to be a
> first class public API. Data types have already been moved out of the
> sql.catalyst package and now lives in sql.types. They are becoming stable
> public APIs. When the "data frame" patch is submitted, you will see a
> public expression library also. There will be few reason for end users or
> library developers to hook into things in sql.catalyst. For the bravest a=
nd
> the most advanced, they can still use them, with the expectation that it =
is
> subject to change.
>
>
>
>
>
> On Thu, Jan 15, 2015 at 7:53 AM, Alessandro Baretta <alexbaretta@gmail.co=
m
> > wrote:
>
>> Reynold,
>>
>> Thanks for the heads up. In general, I strongly oppose the use of
>> "private" to restrict access to certain parts of the API, the reason bei=
ng
>> that I might find the need to use some of the internals of a library fro=
m
>> my own project. I find that a @DeveloperAPI annotation serves the same
>> purpose as "private" without imposing unnecessary restrictions: it
>> discourages people from using the annotated API and reserves the right f=
or
>> the core developers to change it suddenly in backwards incompatible ways=
.
>>
>> In particular, I would like to express the desire that the APIs to
>> programmatically construct SchemaRDDs from an RDD[Row] and a StructType
>> remain public. All the SparkSQL data type objects should be exposed by t=
he
>> API, and the jekyll build should not hide the docs as it does now.
>>
>> Thanks.
>>
>> Alex
>>
>> On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com> wrote=
:
>>
>>> Hi Spark devs,
>>>
>>> Given the growing number of developers that are building on Spark SQL, =
we
>>> would like to stabilize the API in 1.3 so users and developers can be
>>> confident to build on it. This also gives us a chance to improve the AP=
I.
>>>
>>> In particular, we are proposing the following major changes. This shoul=
d
>>> have no impact for most users (i.e. those running SQL through the JDBC
>>> client or SQLContext.sql method).
>>>
>>> 1. Everything in sql.catalyst package is private to the project.
>>>
>>> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
>>> SchemaRDD and logical plans in order to construct test cases. We have
>>> received feedback from a lot of users that the DSL can be incredibly
>>> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it suit=
able for
>>> not
>>> only constructing test cases, but also in everyday data pipelines. The
>>> new
>>> SchemaRDD API is inspired by the data frame concept in Pandas and R.
>>>
>>> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose
>>> one
>>> set of APIs that will work for both Java and Scala. The current Java AP=
I
>>> (sql.api.java) does not share any common ancestor with the Scala API.
>>> This
>>> led to high maintenance burden for us as Spark developers and for libra=
ry
>>> developers. We propose to eliminate the Java specific API, and simply
>>> work
>>> on the existing Scala API to make it also usable for Java. This will ma=
ke
>>> Java a first class citizen as Scala. This effectively means that all
>>> public
>>> classes should be usable for both Scala and Java, including SQLContext,
>>> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
>>>
>>>
>>> Again, this should have no impact on most users since the existing DSL =
is
>>> rarely used by end users. However, library developers might need to
>>> change
>>> the import statements because we are moving certain classes around. We
>>> will
>>> keep you posted as patches are merged.
>>>
>>
>>
>

--001a11c29d461c4604050ccc4232--

From dev-return-11163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:14:55 2015
Return-Path: <dev-return-11163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B962F100C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:14:55 +0000 (UTC)
Received: (qmail 72787 invoked by uid 500); 16 Jan 2015 22:14:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72719 invoked by uid 500); 16 Jan 2015 22:14:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72707 invoked by uid 99); 16 Jan 2015 22:14:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:14:56 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kushal.datta@gmail.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:14:52 +0000
Received: by mail-ig0-f178.google.com with SMTP id hl2so3836312igb.5
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:13:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=hVmA2VHNGEehwk0wWnPsYXfw7ADDaEbSfpPcz5x7wCs=;
        b=rtUF+UeiTLsi6Ip7OQSZSyvpuhqM4LOfvLmnR9XHqoSf3t0vHMOaT8CEUrBjTubC5x
         o+ApjbV5uOtbxdl6C7aE8Kmn8Lfz67q9oIAWMKvUhI+s/kXRzsSw7P6fepesvz2abT1P
         qQpwKdiALEICZBD15OAGoFA2Ks2aLaOuo2YilXrZ4LUls2ktAPou3FqDmpcPZousPqYv
         n74Iym9FEBSfEZXtfqP09jfHh6P6nU7feV1bYDPjG5F7YxtC91/uOK/CNi/CyT9k+8de
         63MgRPA4H/nea3PtrnYQ4K6W53DnXsvv/dvWkPPft+v3dvrkBfxHON54FsWy6aeT/pQ+
         JwRQ==
X-Received: by 10.50.56.70 with SMTP id y6mr6202620igp.27.1421446427007; Fri,
 16 Jan 2015 14:13:47 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.64.199 with HTTP; Fri, 16 Jan 2015 14:13:26 -0800 (PST)
In-Reply-To: <CAKXMip2bAUGrDo7AMJfXWVOFg1Ju-mmO2Kes_3hz6Vgd1bOLCA@mail.gmail.com>
References: <CAKXMip3DT9sRwCu2VkWN5KkfSKu+az+_RtV6m-GVhuK52_55SA@mail.gmail.com>
 <D08139AE.44FB%brennon.york@capitalone.com> <CANjHi9q7rwGUVroDQF2+u=P+2ZBFevq4PjQxpykSvWZwD4wRZg@mail.gmail.com>
 <D08150FA.45D7%brennon.york@capitalone.com> <CAKXMip1PhE37UmezLM0EG+NZo20Pqdicii0AQE7AYJoXt0ceHQ@mail.gmail.com>
 <CAPh_B=YiOJiOJ8uSy4Hn4YKjTPgTkH+QZEW=WMoOihmpgKgh6g@mail.gmail.com>
 <CAKXMip0k5ZGO+QO79G53=b-Epz+-oBFr=BiMDV0hCgaMurWdKA@mail.gmail.com>
 <CANjHi9qNEtk1PGnUceLMo1xupaP8MhtFXVvF6bQVuXOrD5Ab+Q@mail.gmail.com>
 <CAKXMip21Yx3fKioQ+5OpJrqVybuOoopAjocsn2g0JMv=HktFCg@mail.gmail.com>
 <CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com>
 <1421339495591-10126.post@n3.nabble.com> <CANjHi9okJXCN7YFQRF7Gox7pWZB2QJBqbcnvDDGHUL4JjGms6w@mail.gmail.com>
 <CAKXMip2bAUGrDo7AMJfXWVOFg1Ju-mmO2Kes_3hz6Vgd1bOLCA@mail.gmail.com>
From: Kushal Datta <kushal.datta@gmail.com>
Date: Fri, 16 Jan 2015 14:13:26 -0800
Message-ID: <CANjHi9oe-7EHVRTcFGQ9WYAwb6eqMYXrNwSxi1VR359+=v-AoQ@mail.gmail.com>
Subject: Re: Implementing TinkerPop on top of GraphX
To: Kyle Ellrott <kellrott@soe.ucsc.edu>
Cc: David Robinson <drobin1437@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01537d0a5967d2050ccc49fa
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01537d0a5967d2050ccc49fa
Content-Type: text/plain; charset=UTF-8

The source code is under a new module named 'graphx'. let me double check.

On Fri, Jan 16, 2015 at 2:11 PM, Kyle Ellrott <kellrott@soe.ucsc.edu> wrote:

> Looking at https://github.com/kdatta/tinkerpop3/compare/graphx-gremlin I
> only see a maven build file. Do you have some source code some place else?
>
> I've worked on a spark based implementation (
> https://github.com/kellrott/spark-gremlin ), but its not done and I've
> been tied up on other projects.
> It also look Tinkerpop3 is a bit of a moving target. I had targeted the
> work done for gremlin-giraph (
> http://www.tinkerpop.com/docs/3.0.0.M5/#giraph-gremlin ) that was part of
> the M5 release, as a base model for implementation. But that appears to
> have been refactored into gremlin-hadoop (
> http://www.tinkerpop.com/docs/3.0.0.M6/#hadoop-gremlin ) in the M6
> release. I need to assess how much this changes the code.
>
> Most of the code that needs to be changes from Giraph to Spark will be
> simply replacing classes with spark derived ones. The main place where the
> logic will need changed is in the 'GraphComputer' class (
> https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/process/computer/giraph/GiraphGraphComputer.java
> ) which is created by the Graph when the 'compute' method is called (
> https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/structure/HadoopGraph.java#L135
> ).
>
>
> Kyle
>
>
>
> On Fri, Jan 16, 2015 at 1:01 PM, Kushal Datta <kushal.datta@gmail.com>
> wrote:
>
>> Hi David,
>>
>>
>> Yes, we are still headed in that direction.
>> Please take a look at the repo I sent earlier.
>> I think that's a good starting point.
>>
>> Thanks,
>> -Kushal.
>>
>> On Thu, Jan 15, 2015 at 8:31 AM, David Robinson <drobin1437@gmail.com>
>> wrote:
>>
>> > I am new to Spark and GraphX, however, I use Tinkerpop backed graphs and
>> > think the idea of using Tinkerpop as the API for GraphX is a great idea
>> and
>> > hope you are still headed in that direction.  I noticed that Tinkerpop
>> 3 is
>> > moving into the Apache family:
>> > http://wiki.apache.org/incubator/TinkerPopProposal  which might
>> alleviate
>> > concerns about having an API definition "outside" of Spark.
>> >
>> > Thanks,
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Implementing-TinkerPop-on-top-of-GraphX-tp9169p10126.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>

--089e01537d0a5967d2050ccc49fa--

From dev-return-11164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:34:42 2015
Return-Path: <dev-return-11164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F495101EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:34:42 +0000 (UTC)
Received: (qmail 11909 invoked by uid 500); 16 Jan 2015 22:34:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11839 invoked by uid 500); 16 Jan 2015 22:34:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11828 invoked by uid 99); 16 Jan 2015 22:34:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:34:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:34:38 +0000
Received: by mail-qg0-f46.google.com with SMTP id f51so9719882qge.5
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:33:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=01VGu0lJG3BsBn7VQp6qroBQ3ZoGFRbFFgVO7nT8cIc=;
        b=aScnoB02GA/UDncB5xbz6M/3FuubropKibFf4gDcxfYpfEOTYrJRLi7F7Yp8jBkvHF
         O1qoPgDoCaCBtLrLsn5VAv1COtB8zf1/YqxelMfDcG8kXnY8MYkwnkq/qPLXe26oGxkn
         3O0SOKiBmWoZwjTPrF5dULgFUpgHAFUm/Ymf3FO2kEa2NBredL+c0kwBWnLvV8Ap3kAH
         hNaVuhXjdcIhnm6VQZUKS/J1aIFbOPTFCRgGoGOyOFe/krgtnuL5hYgiqDhF/6TwxeB2
         ncOi7WRIDJ1iqG48B9mmOGvnBMpr0ImP6vScawXJivUdi8bIC5efvOrQzfk4xErXmODT
         igEA==
X-Gm-Message-State: ALoCoQnwJbAhM67ac1EDXRHBp07hkzwH7/sXSGVACE7HqmsWxkaoAHmmNBrS+5mxSUl7EpYEOg8z
X-Received: by 10.224.63.20 with SMTP id z20mr29748374qah.62.1421447637166;
 Fri, 16 Jan 2015 14:33:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Fri, 16 Jan 2015 14:33:36 -0800 (PST)
In-Reply-To: <CAJc_syLLkoH5k9BEpVw4sRCAKGs1zk4dmREDhuh10MpnaThpcw@mail.gmail.com>
References: <CAPh_B=aCdHP2gZ_yNALfxYQj+PbPeaA-UB7MzhqT7yv4bD3OgA@mail.gmail.com>
 <CAJc_syJqN6Qv9P0Ezd71dnvCo+3ndKSRzY4Jhs1EXnnOsLLoqA@mail.gmail.com>
 <CAPh_B=b4JHEWzkcD72CyptrMmDUWwqJLqH3nJ1w_ksGv7JnfyA@mail.gmail.com> <CAJc_syLLkoH5k9BEpVw4sRCAKGs1zk4dmREDhuh10MpnaThpcw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 16 Jan 2015 14:33:36 -0800
Message-ID: <CAPh_B=atXNdMbdOf0fVrb76af-RW9aQUA4cdhcp0iCrj5qqgxQ@mail.gmail.com>
Subject: Re: Spark SQL API changes and stabilization
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf0cc007b4483050ccc918a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0cc007b4483050ccc918a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That's a good idea. We didn't intentionally break the doc generation. The
doc generation for Catalyst is broken because we use Scala macros and we
haven't had time to investigate how to fix it yet.

If you have a minute and want to investigate, I can merge it in as soon as
possible.





On Fri, Jan 16, 2015 at 2:11 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Reynold,
>
> Your clarification is much appreciated. One issue though, that I would
> strongly encourage you to work on, is to make sure that the Scaladoc CAN =
be
> generated manually if needed (a "Use at your own risk" clause would be
> perfectly legitimate here). The reason I say this is that currently even
> hacking SparkBuild.scala to include SparkSQL in the unidoc target doesn't
> help, as scaladoc itself fails with errors such as these.
>
> [error]
> /Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/ca=
talyst/dsl/package.scala:359:
> polymorphic expression cannot be instantiated to expected type;
> [error]  found   : [T(in method
> apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method appl=
y)]
> [error]  required:
> org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in method
> functionToUdfBuilder)]
> [error]   implicit def functionToUdfBuilder[T: TypeTag](func:
> Function22[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _,
> _, T]): ScalaUdfBuilder[T] =3D ScalaUdfBuilder(func)
> [error]
>
>                               ^
> [error]
> /Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/ca=
talyst/expressions/codegen/CodeGenerator.scala:147:
> value q is not a member of StringContext
> [error]  Note: implicit class Evaluate2 is not applicable here because it
> comes after the application point and it lacks an explicit result type
> [error]         q"""
> [error]         ^
> [error]
> /Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/ca=
talyst/expressions/codegen/CodeGenerator.scala:181:
> value q is not a member of StringContext
> [error]         q"""
> [error]         ^
> [error]
> /Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/ca=
talyst/expressions/codegen/CodeGenerator.scala:198:
> value q is not a member of StringContext
>
> While I understand you desire to discourage users from relying on the
> internal "private" APIs, there is no reason to prevent people from gainin=
g
> a better understanding of how things work by allow them--with some
> effort--to get to the docs.
>
> Thanks,
>
> Alex
>
> On Thu, Jan 15, 2015 at 10:33 AM, Reynold Xin <rxin@databricks.com> wrote=
:
>
>> Alex,
>>
>> I didn't communicate properly. By "private", I simply meant the
>> expectation that it is not a public API. The plan is to still omit it fr=
om
>> the scaladoc/javadoc generation, but no language visibility modifier wil=
l
>> be applied on them.
>>
>> After 1.3, you will likely no longer need to use things in sql.catalyst
>> package directly. Programmatically construct SchemaRDDs is going to be a
>> first class public API. Data types have already been moved out of the
>> sql.catalyst package and now lives in sql.types. They are becoming stabl=
e
>> public APIs. When the "data frame" patch is submitted, you will see a
>> public expression library also. There will be few reason for end users o=
r
>> library developers to hook into things in sql.catalyst. For the bravest =
and
>> the most advanced, they can still use them, with the expectation that it=
 is
>> subject to change.
>>
>>
>>
>>
>>
>> On Thu, Jan 15, 2015 at 7:53 AM, Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>>> Reynold,
>>>
>>> Thanks for the heads up. In general, I strongly oppose the use of
>>> "private" to restrict access to certain parts of the API, the reason be=
ing
>>> that I might find the need to use some of the internals of a library fr=
om
>>> my own project. I find that a @DeveloperAPI annotation serves the same
>>> purpose as "private" without imposing unnecessary restrictions: it
>>> discourages people from using the annotated API and reserves the right =
for
>>> the core developers to change it suddenly in backwards incompatible way=
s.
>>>
>>> In particular, I would like to express the desire that the APIs to
>>> programmatically construct SchemaRDDs from an RDD[Row] and a StructType
>>> remain public. All the SparkSQL data type objects should be exposed by =
the
>>> API, and the jekyll build should not hide the docs as it does now.
>>>
>>> Thanks.
>>>
>>> Alex
>>>
>>> On Wed, Jan 14, 2015 at 9:45 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>>
>>>> Hi Spark devs,
>>>>
>>>> Given the growing number of developers that are building on Spark SQL,
>>>> we
>>>> would like to stabilize the API in 1.3 so users and developers can be
>>>> confident to build on it. This also gives us a chance to improve the
>>>> API.
>>>>
>>>> In particular, we are proposing the following major changes. This shou=
ld
>>>> have no impact for most users (i.e. those running SQL through the JDBC
>>>> client or SQLContext.sql method).
>>>>
>>>> 1. Everything in sql.catalyst package is private to the project.
>>>>
>>>> 2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
>>>> SchemaRDD and logical plans in order to construct test cases. We have
>>>> received feedback from a lot of users that the DSL can be incredibly
>>>> powerful. In 1.3, we=E2=80=99d like to refactor the DSL to make it sui=
table for
>>>> not
>>>> only constructing test cases, but also in everyday data pipelines. The
>>>> new
>>>> SchemaRDD API is inspired by the data frame concept in Pandas and R.
>>>>
>>>> 3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose
>>>> one
>>>> set of APIs that will work for both Java and Scala. The current Java A=
PI
>>>> (sql.api.java) does not share any common ancestor with the Scala API.
>>>> This
>>>> led to high maintenance burden for us as Spark developers and for
>>>> library
>>>> developers. We propose to eliminate the Java specific API, and simply
>>>> work
>>>> on the existing Scala API to make it also usable for Java. This will
>>>> make
>>>> Java a first class citizen as Scala. This effectively means that all
>>>> public
>>>> classes should be usable for both Scala and Java, including SQLContext=
,
>>>> HiveContext, SchemaRDD, data types, and the aforementioned DSL.
>>>>
>>>>
>>>> Again, this should have no impact on most users since the existing DSL
>>>> is
>>>> rarely used by end users. However, library developers might need to
>>>> change
>>>> the import statements because we are moving certain classes around. We
>>>> will
>>>> keep you posted as patches are merged.
>>>>
>>>
>>>
>>
>

--047d7bf0cc007b4483050ccc918a--

From dev-return-11165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:49:56 2015
Return-Path: <dev-return-11165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4E2D102A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:49:56 +0000 (UTC)
Received: (qmail 46527 invoked by uid 500); 16 Jan 2015 22:49:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46451 invoked by uid 500); 16 Jan 2015 22:49:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46439 invoked by uid 99); 16 Jan 2015 22:49:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:49:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:49:32 +0000
Received: by mail-la0-f49.google.com with SMTP id hs14so21299658lab.8
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:47:40 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=I1jmvhAO2h+D1p8rdmB4A7yg139dp75tSFjj6IaiBRk=;
        b=Gm4Vy4USdg5EUkLDRHEsvmRVDEhM1cWZ84rzeLAZcfCaWbqcdD+V/dPtCVx55uZlB8
         4NdNeFXAdih87on2FaeosH8zGe9jX3SZHqb7lpWG9z+szdxecPXOmd+9enePuL4/h+9/
         hcTo99E+nOs17zglOH3lFTIjNH96A7mkBRBSTCAodi0cCrB0jz2wdsNoRG98wNFhqDOd
         FQcjA5XCipLpsT/4xX1cSKSrUngJ8l2HTVt/1or8ZUX5exP0Xa8XeaQeoEYDbcN6lZYv
         2+c6k64kx5y+OlI/jKivHJnx69hhWAmJqSeso4+yQDLts9cCksXBczJvgDvWdLBiA5hZ
         nNrQ==
X-Gm-Message-State: ALoCoQkxmcqil3IWbgQjAYPH5CFtH002oYgE4a0fAa8r8mgoLJsf4ebn33XVVahJleCH5jOkjp+m
MIME-Version: 1.0
X-Received: by 10.152.88.4 with SMTP id bc4mr18229605lab.5.1421448460260; Fri,
 16 Jan 2015 14:47:40 -0800 (PST)
Received: by 10.25.37.79 with HTTP; Fri, 16 Jan 2015 14:47:40 -0800 (PST)
In-Reply-To: <CAJc_syKxx5cjJ9E+2G9vTaJ9PnkO5ZB9ht5ujEp0HfzM5yfauw@mail.gmail.com>
References: <CAJc_syJqsnE-mP9j5RyV-=ZOLW9rQMxQEbdAKfnBUdbEhgfo8g@mail.gmail.com>
	<CAPh_B=Z0PdZa7kaxEFTFsX88QuM6H+nFKUDKKYxkm8W+Qkx9sg@mail.gmail.com>
	<CAJc_syKxx5cjJ9E+2G9vTaJ9PnkO5ZB9ht5ujEp0HfzM5yfauw@mail.gmail.com>
Date: Fri, 16 Jan 2015 14:47:40 -0800
Message-ID: <CAHP0waKYEo=BU4fPS0UQWGhK5xG1-YM3zdYN5SXOpLMcpu+d4g@mail.gmail.com>
Subject: Re: Join implementation in SparkSQL
From: Yin Huai <yhuai@databricks.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c365b48a733f050cccc20a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c365b48a733f050cccc20a
Content-Type: text/plain; charset=UTF-8

Hi Alex,

Can you attach the output of sql("explain extended <your
query>").collect.foreach(println)?

Thanks,

Yin

On Fri, Jan 16, 2015 at 1:54 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Reynold,
>
> The source file you are directing me to is a little too terse for me to
> understand what exactly is going on. Let me tell you what I'm trying to do
> and what problems I'm encountering, so that you might be able to better
> direct me investigation of the SparkSQL codebase.
>
> I am computing the join of three tables, sharing the same primary key,
> composed of three fields, and having several other fields. My first attempt
> at computing this join was in SQL, with a query much like this slightly
> simplified one:
>
>          SELECT
>           a.key1 key1, a.key2 key2, a.key3 key3,
>           a.data1   adata1,    a.data2    adata2,    ...
>           b.data1   bdata1,    b.data2    bdata2,    ...
>           c.data1   cdata1,    c.data2    cdata2,    ...
>         FROM a, b, c
>         WHERE
>           a.key1 = b.key1 AND a.key2 = b.key2 AND a.key3 = b.key3
>           b.key1 = c.key1 AND b.key2 = c.key2 AND b.key3 = c.key3
>
> This code yielded a SparkSQL job containing 40,000 stages, which failed
> after filling up all available disk space on the worker nodes.
>
> I then wrote this join as a plain mapreduce join. The code looks roughly
> like this:
> val a_ = a.map(row => (key(row), ("a", row))
> val b_ = b.map(row => (key(row), ("b", row))
> val c_ = c.map(row => (key(row), ("c", row"))
> val join = UnionRDD(sc, List(a_, b_, c_)).groupByKey
>
> This implementation yields approximately 1600 stages and completes in a few
> minutes on a 256 core cluster. The huge difference in scale of the two jobs
> makes me think that SparkSQL is implementing my join as cartesian product.
> This is they query plan--I'm not sure I can read it, but it does seem to
> imply that the filter conditions are not being pushed far down enough:
>
>  'Project [...]
>  'Filter (((((('a.key1 = 'b.key1)) && ('a.key2 = b.key2)) && ...)
>   'Join Inner, None
>    'Join Inner, None
>
> Is maybe SparkSQL unable to push join conditions down from the WHERE clause
> into the join itself?
>
> Alex
>
> On Thu, Jan 15, 2015 at 10:36 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > It's a bunch of strategies defined here:
> >
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
> >
> > In most common use cases (e.g. inner equi join), filters are pushed below
> > the join or into the join. Doing a cartesian product followed by a filter
> > is too expensive.
> >
> >
> > On Thu, Jan 15, 2015 at 7:39 AM, Alessandro Baretta <
> alexbaretta@gmail.com
> > > wrote:
> >
> >> Hello,
> >>
> >> Where can I find docs about how joins are implemented in SparkSQL? In
> >> particular, I'd like to know whether they are implemented according to
> >> their relational algebra definition as filters on top of a cartesian
> >> product.
> >>
> >> Thanks,
> >>
> >> Alex
> >>
> >
> >
>

--001a11c365b48a733f050cccc20a--

From dev-return-11166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 22:58:22 2015
Return-Path: <dev-return-11166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BE50102F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 22:58:22 +0000 (UTC)
Received: (qmail 62152 invoked by uid 500); 16 Jan 2015 22:58:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62077 invoked by uid 500); 16 Jan 2015 22:58:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62065 invoked by uid 99); 16 Jan 2015 22:58:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:58:22 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kushal.datta@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 22:58:18 +0000
Received: by mail-ig0-f179.google.com with SMTP id l13so5752634iga.0
        for <dev@spark.apache.org>; Fri, 16 Jan 2015 14:57:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=FcG9f2H7NpsVZiip1/vuTfza+qSHTmE1La2/i90Ipek=;
        b=nS7WABUzTiaeG9Iz2A6LxMWADTNg9YleETHksrwCZAfzBjAoVlX4bgobl7ek32u4je
         EpJVz/DCH1kiybLkwSn2cFNHxdEK2bspp1Z+LfF3aR89RuhApiICM+tagQkuId59yCsX
         t21IUzDS9l4zZ+qQIcVpcRweVX9RFtzaAaswpoHsh/3ModCktOrJCJjEhJObaJBO+FQR
         5qoA05HHwDLbjrknmA+evEw/GGFFupA/E+nvZQLIFN2knsDRTbb4s2mHxBu57HnCLY5K
         WzeJg0hkU9Fud+XChvSFmidbW90Sx4N0ZYDwOK+rfhKM2VZQIvuh/ERqeKYydVjNtAeQ
         mEnA==
X-Received: by 10.42.86.136 with SMTP id u8mr16854078icl.91.1421449078169;
 Fri, 16 Jan 2015 14:57:58 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.64.199 with HTTP; Fri, 16 Jan 2015 14:57:37 -0800 (PST)
In-Reply-To: <CANjHi9oe-7EHVRTcFGQ9WYAwb6eqMYXrNwSxi1VR359+=v-AoQ@mail.gmail.com>
References: <CAKXMip3DT9sRwCu2VkWN5KkfSKu+az+_RtV6m-GVhuK52_55SA@mail.gmail.com>
 <D08139AE.44FB%brennon.york@capitalone.com> <CANjHi9q7rwGUVroDQF2+u=P+2ZBFevq4PjQxpykSvWZwD4wRZg@mail.gmail.com>
 <D08150FA.45D7%brennon.york@capitalone.com> <CAKXMip1PhE37UmezLM0EG+NZo20Pqdicii0AQE7AYJoXt0ceHQ@mail.gmail.com>
 <CAPh_B=YiOJiOJ8uSy4Hn4YKjTPgTkH+QZEW=WMoOihmpgKgh6g@mail.gmail.com>
 <CAKXMip0k5ZGO+QO79G53=b-Epz+-oBFr=BiMDV0hCgaMurWdKA@mail.gmail.com>
 <CANjHi9qNEtk1PGnUceLMo1xupaP8MhtFXVvF6bQVuXOrD5Ab+Q@mail.gmail.com>
 <CAKXMip21Yx3fKioQ+5OpJrqVybuOoopAjocsn2g0JMv=HktFCg@mail.gmail.com>
 <CANjHi9qward_JFHbT8kJoutQci0uZdwes1RAUstSnJ1RU_GuvA@mail.gmail.com>
 <1421339495591-10126.post@n3.nabble.com> <CANjHi9okJXCN7YFQRF7Gox7pWZB2QJBqbcnvDDGHUL4JjGms6w@mail.gmail.com>
 <CAKXMip2bAUGrDo7AMJfXWVOFg1Ju-mmO2Kes_3hz6Vgd1bOLCA@mail.gmail.com> <CANjHi9oe-7EHVRTcFGQ9WYAwb6eqMYXrNwSxi1VR359+=v-AoQ@mail.gmail.com>
From: Kushal Datta <kushal.datta@gmail.com>
Date: Fri, 16 Jan 2015 14:57:37 -0800
Message-ID: <CANjHi9pZAkfREJ1Uyc76ndzuWm85riS9cVMqjnTckr+MCc_iJg@mail.gmail.com>
Subject: Re: Implementing TinkerPop on top of GraphX
To: Kyle Ellrott <kellrott@soe.ucsc.edu>
Cc: David Robinson <drobin1437@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30223f435ee649050ccce784
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30223f435ee649050ccce784
Content-Type: text/plain; charset=UTF-8

code updated. sorry, wrong branch uploaded before.

On Fri, Jan 16, 2015 at 2:13 PM, Kushal Datta <kushal.datta@gmail.com>
wrote:

> The source code is under a new module named 'graphx'. let me double check.
>
> On Fri, Jan 16, 2015 at 2:11 PM, Kyle Ellrott <kellrott@soe.ucsc.edu>
> wrote:
>
>> Looking at https://github.com/kdatta/tinkerpop3/compare/graphx-gremlin I
>> only see a maven build file. Do you have some source code some place else?
>>
>> I've worked on a spark based implementation (
>> https://github.com/kellrott/spark-gremlin ), but its not done and I've
>> been tied up on other projects.
>> It also look Tinkerpop3 is a bit of a moving target. I had targeted the
>> work done for gremlin-giraph (
>> http://www.tinkerpop.com/docs/3.0.0.M5/#giraph-gremlin ) that was part
>> of the M5 release, as a base model for implementation. But that appears to
>> have been refactored into gremlin-hadoop (
>> http://www.tinkerpop.com/docs/3.0.0.M6/#hadoop-gremlin ) in the M6
>> release. I need to assess how much this changes the code.
>>
>> Most of the code that needs to be changes from Giraph to Spark will be
>> simply replacing classes with spark derived ones. The main place where the
>> logic will need changed is in the 'GraphComputer' class (
>> https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/process/computer/giraph/GiraphGraphComputer.java
>> ) which is created by the Graph when the 'compute' method is called (
>> https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/structure/HadoopGraph.java#L135
>> ).
>>
>>
>> Kyle
>>
>>
>>
>> On Fri, Jan 16, 2015 at 1:01 PM, Kushal Datta <kushal.datta@gmail.com>
>> wrote:
>>
>>> Hi David,
>>>
>>>
>>> Yes, we are still headed in that direction.
>>> Please take a look at the repo I sent earlier.
>>> I think that's a good starting point.
>>>
>>> Thanks,
>>> -Kushal.
>>>
>>> On Thu, Jan 15, 2015 at 8:31 AM, David Robinson <drobin1437@gmail.com>
>>> wrote:
>>>
>>> > I am new to Spark and GraphX, however, I use Tinkerpop backed graphs
>>> and
>>> > think the idea of using Tinkerpop as the API for GraphX is a great
>>> idea and
>>> > hope you are still headed in that direction.  I noticed that Tinkerpop
>>> 3 is
>>> > moving into the Apache family:
>>> > http://wiki.apache.org/incubator/TinkerPopProposal  which might
>>> alleviate
>>> > concerns about having an API definition "outside" of Spark.
>>> >
>>> > Thanks,
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> >
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Implementing-TinkerPop-on-top-of-GraphX-tp9169p10126.html
>>> > Sent from the Apache Spark Developers List mailing list archive at
>>> > Nabble.com.
>>> >
>>> > ---------------------------------------------------------------------
>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>> >
>>> >
>>>
>>
>>
>

--20cf30223f435ee649050ccce784--

From dev-return-11167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 16 23:38:03 2015
Return-Path: <dev-return-11167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D72A310569
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 Jan 2015 23:38:03 +0000 (UTC)
Received: (qmail 63657 invoked by uid 500); 16 Jan 2015 23:38:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63583 invoked by uid 500); 16 Jan 2015 23:38:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63571 invoked by uid 99); 16 Jan 2015 23:38:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 23:38:03 +0000
X-ASF-Spam-Status: No, hits=1.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [157.193.49.126] (HELO smtp2.ugent.be) (157.193.49.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 Jan 2015 23:37:58 +0000
Received: from localhost (mcheck3.ugent.be [157.193.71.89])
	by smtp2.ugent.be (Postfix) with ESMTP id BC40A12C49C;
	Sat, 17 Jan 2015 00:36:56 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp2.ugent.be ([IPv6:::ffff:157.193.49.126])
	by localhost (mcheck3.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id uUSxi8yBay6h; Sat, 17 Jan 2015 00:36:56 +0100 (CET)
Received: from [192.168.0.212] (78-22-121-243.access.telenet.be [78.22.121.243])
	(Authenticated sender: ehiggs)
	by smtp2.ugent.be (Postfix) with ESMTPSA id 3B6BA12C480;
	Sat, 17 Jan 2015 00:36:56 +0100 (CET)
Message-ID: <54B9A097.6020100@ugent.be>
Date: Sat, 17 Jan 2015 00:36:55 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Reynold Xin <rxin@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: RDD order guarantees
References: <54B93BB3.10401@ugent.be> <CAPh_B=ZzCLJAus-7Jt2VYMkngZkDyMbsqdKt4DZEHcGZCwxswQ@mail.gmail.com>
In-Reply-To: <CAPh_B=ZzCLJAus-7Jt2VYMkngZkDyMbsqdKt4DZEHcGZCwxswQ@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------050407020404000603070007"
X-Miltered: at jchkm1 with ID 54B9A097.004 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 54B9A097.004 from 78-22-121-243.access.telenet.be/78-22-121-243.access.telenet.be/78.22.121.243/[192.168.0.212]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 54B9A097.004 on smtp2.ugent.be : j-chkmail score : X : R=. U=. O=## B=0.000 -> S=0.166
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

--------------050407020404000603070007
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Yes, I am running on a local file system.

Is there a bug open for this? Mingyu Kim reported the problem last April:
http://apache-spark-user-list.1001560.n3.nabble.com/Spark-reads-partitions-in-a-wrong-order-td4818.html

-Ewan

On 01/16/2015 07:41 PM, Reynold Xin wrote:
> You are running on a local file system right? HDFS orders the file 
> based on names, but local file system often don't. I think that's why 
> the difference.
>
> We might be able to do a sort and order the partitions when we create 
> a RDD to make this universal though.
>
> On Fri, Jan 16, 2015 at 8:26 AM, Ewan Higgs <ewan.higgs@ugent.be 
> <mailto:ewan.higgs@ugent.be>> wrote:
>
>     Hi all,
>     Quick one: when reading files, are the orders of partitions
>     guaranteed to be preserved? I am finding some weird behaviour
>     where I run sortByKeys() on an RDD (which has 16 byte keys) and
>     write it to disk. If I open a python shell and run the following:
>
>     for part in range(29):
>         print map(ord,
>     open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part),
>     'r').read(16))
>
>     Then each partition is in order based on the first value of each
>     partition.
>
>     I can also call TeraValidate.validate from TeraSort and it is
>     happy with the results. It seems to be on loading the file that
>     the reordering happens. If this is expected, is there a way to ask
>     Spark nicely to give me the RDD in the order it was saved?
>
>     This is based on trying to fix my TeraValidate code on this branch:
>     https://github.com/ehiggs/spark/tree/terasort
>
>     Thanks,
>     Ewan
>
>     ---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>


--------------050407020404000603070007--

From dev-return-11168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 02:56:40 2015
Return-Path: <dev-return-11168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B6A310B65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 02:56:40 +0000 (UTC)
Received: (qmail 77735 invoked by uid 500); 17 Jan 2015 02:56:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77659 invoked by uid 500); 17 Jan 2015 02:56:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77636 invoked by uid 99); 17 Jan 2015 02:56:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 02:56:40 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of angellandros@yahoo.com designates 98.138.91.248 as permitted sender)
Received: from [98.138.91.248] (HELO nm26-vm5.bullet.mail.ne1.yahoo.com) (98.138.91.248)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 02:56:12 +0000
Received: from [98.138.226.177] by nm26.bullet.mail.ne1.yahoo.com with NNFMP; 17 Jan 2015 02:56:10 -0000
Received: from [98.138.88.233] by tm12.bullet.mail.ne1.yahoo.com with NNFMP; 17 Jan 2015 02:56:10 -0000
Received: from [127.0.0.1] by omp1033.mail.ne1.yahoo.com with NNFMP; 17 Jan 2015 02:56:10 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 309249.37945.bm@omp1033.mail.ne1.yahoo.com
X-YMail-OSG: R.hyBJQVM1lLfyy3z1QYy9vFfk8ZfWxSFmA.YsbNhlAJeFQizhjojiAlROkWR09
 X6b.yHWrr20s7kyKzuAQ0aXdGCxFUNdXSn7EM8aAtBD7sUTDOnnXFzeX0uZEvlXuo3uNPGk3fCTk
 O82odVzCjyCNtR.U9DRywji0iI_Zrq3t0V_VOQ3w.tPtDd_gqTjk3JDNH5G6YZZp5MWnZhdZv6m7
 wQbZr8b5sDT3UfDTvCQN0zTp51l36D.tN0WkAoK5gpsoD.3LiQZcC3EN6oPJj1Gp9b8h6IZ75hM3
 rhnNwQzw52EXGzMx6UOBr0.EO3dLJSIoY6EzNBJajXuazmhktzQo8Qx7FEmaFrGERyBVQc.7Epzr
 pFhT9NaHbkh3dZsxNk2KAmhdiW.agdi6MsEsri26W3ybJASvOdOgZteQn3JophnQy1J7bOMfc91M
 iTaCoOeu86hoFziqJlZe9HPPeNyEN4KRtCW4ywuBY6GqSuweE7b1x9PsuzYYoO93Wlz0HmEfC5mK
 x5s9oWGOP_0lGWaaTuzFGXnPJwDyolO6JIilvplQcuzDgVuamcFyzrWzBCWqZd7U_1vNwsCSSUkA-
Received: by 98.138.105.225; Sat, 17 Jan 2015 02:56:09 +0000 
Date: Sat, 17 Jan 2015 02:56:09 +0000 (UTC)
From: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>
Reply-To: =?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1422661772.1484375.1421463369586.JavaMail.yahoo@jws100158.mail.ne1.yahoo.com>
In-Reply-To: <CAJgQjQ_-MuN=oDJHkjsSUsEyeH68aJhwXmzmrad6AFAATuftzQ@mail.gmail.com>
References: <CAJgQjQ_-MuN=oDJHkjsSUsEyeH68aJhwXmzmrad6AFAATuftzQ@mail.gmail.com>
Subject: Re: DBSCAN for MLlib
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_1484374_599834736.1421463369583"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_1484374_599834736.1421463369583
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Please find my answers on JIRA page.
Muhammad-Ali=20

     On Thursday, January 15, 2015 3:25 AM, Xiangrui Meng <mengxr@gmail.com=
> wrote:
  =20

 Please find my comments on the JRIA page. -Xiangrui

On Tue, Jan 13, 2015 at 1:49 PM, Muhammad Ali A'r=C3=A5by
<angellandros@yahoo.com.invalid> wrote:
> I have to say, I have created a Jira task for it:
> [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRA
>
> |=C2=A0 |
> |=C2=A0 |=C2=A0 |=C2=A0 |=C2=A0 |=C2=A0 |
> | [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRAMLlib i=
s all k-means now, and I think we should add some new clustering algorithms=
 to it. First candidate is DBSCAN as I think.=C2=A0 |
> |=C2=A0 |
> | View on issues.apache.org | Preview by Yahoo |
> |=C2=A0 |
> |=C2=A0 |
>
>
>
>=C2=A0 =C2=A0 =C2=A0 On Wednesday, January 14, 2015 1:09 AM, Muhammad Ali =
A'r=C3=A5by <angellandros@yahoo.com> wrote:
>
>
>=C2=A0 Dear all,
> I think MLlib needs more clustering algorithms and DBSCAN is my first can=
didate. I am starting to implement it. Any advice?
> Muhammad-Ali
>
>

   
------=_Part_1484374_599834736.1421463369583--

From dev-return-11169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 08:57:42 2015
Return-Path: <dev-return-11169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D4A017298
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 08:57:42 +0000 (UTC)
Received: (qmail 12532 invoked by uid 500); 17 Jan 2015 08:57:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12468 invoked by uid 500); 17 Jan 2015 08:57:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11366 invoked by uid 99); 17 Jan 2015 08:57:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 08:57:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 08:57:35 +0000
Received: by mail-we0-f169.google.com with SMTP id p10so293654wes.0
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 00:56:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=t2fJxhGXdTZywZAE7xZsbV8qdP3iSuCRyk1LFcSDWiE=;
        b=Jp3E4EsbIqkBELSsvZfX32fpf9qFs1wb2/T/UGoVT84+4QrCi/6g78NQu/7GY8qwRb
         N+RBYFVvS6ZxSqXUdNRUe3rsPplNP0xKijpnMZXupq4cUrZqkozvhUBBi2mQBCJ26hq6
         psbMxHKk30lIqefQFdgAVfIZrqiON/ElWcM20abPq+iduZBm4JOhqkmnC2vb2HxGzMay
         qcnZP3uagZ4GQ3j7ldjVAShtO3c2afLtb1qZa0LosfJHjqPcRr5w4W3TF/llmpZ9PWb2
         EQsS9YIUXQl+IIgjAjuWlE8wLgYM0GOTVmtmtpXHgsniWf4nlDyRMqHCb76Iat/mUeug
         Jwmg==
X-Gm-Message-State: ALoCoQmgYoYqxhV9n1FrFm3AIvwoVMDd9u4JTX6+92m4P4K0fF8S83Wro/FQHxsKpF8gtx65mUmL
MIME-Version: 1.0
X-Received: by 10.194.6.70 with SMTP id y6mr37019688wjy.97.1421485013944; Sat,
 17 Jan 2015 00:56:53 -0800 (PST)
Received: by 10.217.97.4 with HTTP; Sat, 17 Jan 2015 00:56:53 -0800 (PST)
Date: Sat, 17 Jan 2015 14:26:53 +0530
Message-ID: <CAHUQ+_YaOh867Pt3_f+6a9X+OdPq0kgGJR=-NfZ=Egq92nP2Sg@mail.gmail.com>
Subject: Bouncing Mails
From: Akhil Das <akhil@sigmoidanalytics.com>
To: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d98af4f618d050cd54507
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d98af4f618d050cd54507
Content-Type: text/plain; charset=UTF-8

My mails to the mailing list are getting rejected, have opened a Jira
issue, can someone take a look at it?

https://issues.apache.org/jira/browse/INFRA-9032






Thanks
Best Regards

--047d7b5d98af4f618d050cd54507--

From dev-return-11170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 10:29:03 2015
Return-Path: <dev-return-11170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4347173A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 10:29:03 +0000 (UTC)
Received: (qmail 93207 invoked by uid 500); 17 Jan 2015 10:29:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93136 invoked by uid 500); 17 Jan 2015 10:29:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93125 invoked by uid 99); 17 Jan 2015 10:29:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:29:04 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:28:39 +0000
Received: by mail-qc0-f172.google.com with SMTP id i8so11772576qcq.3
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 02:28:18 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=IUEAj/M4XIr9VnKrsAQJ1aG8Pzhi0oLuXr7EMg2vQ2E=;
        b=JnlHQA9ql3UdDXXJvgPkYJZzdlwxPAHSNSklHGiEzGNayDZC3bzrtnqXXg/MSkzktS
         DMBwje67i24JUD3O3KWA0mtvsIfBa7TQSc79pYgN1ZPIfoo8vam8zeCwW9OXoS+eISJX
         fSXN7Mwt3okKbau85W8lzpFmydKufqmXSSvS35wuTGULDc4U4ZJwQk7LkNkDA1DoekBL
         6rPmNwTdJj16msS/d0GVPOolByVFUmVc5ytkMhVGxNJeImCOxdveHYFruQ0vJjzujm2F
         cxG7jvHKW7aSegW7UllplhmnGisc6RxFP5e4bDk5ioy++zG9E3Y9pVVSBVZMeAlKIqvt
         Tb0g==
X-Gm-Message-State: ALoCoQm+Li3or6htubpBdUgP3YLI9HQ1fiNODpLE33D7Rzpopl1wEG62MV4lJjuHa0oWFTQ3O2sg
MIME-Version: 1.0
X-Received: by 10.224.89.65 with SMTP id d1mr29256556qam.4.1421490497244; Sat,
 17 Jan 2015 02:28:17 -0800 (PST)
Received: by 10.229.2.136 with HTTP; Sat, 17 Jan 2015 02:28:17 -0800 (PST)
In-Reply-To: <1421340401649-10127.post@n3.nabble.com>
References: <1421340401649-10127.post@n3.nabble.com>
Date: Sat, 17 Jan 2015 02:28:17 -0800
Message-ID: <CAEYYnxbk8dfLe5nWbzrTeyfQcGGOEQSA9=pbAEO-7u8nZB=GLQ@mail.gmail.com>
Subject: Re: LinearRegressionWithSGD accuracy
From: DB Tsai <dbtsai@dbtsai.com>
To: "devl.development" <devl.development@gmail.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm working on LinearRegressionWithElasticNet using OWLQN now. This
will do the data standardization internally so it's transparent to
users. With OWLQN, you don't have to manually choose stepSize. Will
send out PR soon next week.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



On Thu, Jan 15, 2015 at 8:46 AM, devl.development
<devl.development@gmail.com> wrote:
> From what I gather, you use LinearRegressionWithSGD to predict y or the
> response variable given a feature vector x.
>
> In a simple example I used a perfectly linear dataset such that x=y
> y,x
> 1,1
> 2,2
> ...
>
> 10000,10000
>
> Using the out-of-box example from the website (with and without scaling):
>
>  val data = sc.textFile(file)
>
>     val parsedData = data.map { line =>
>       val parts = line.split(',')
>      LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) //y
> and x
>
>     }
>     val scaler = new StandardScaler(withMean = true, withStd = true)
>       .fit(parsedData.map(x => x.features))
>     val scaledData = parsedData
>       .map(x =>
>       LabeledPoint(x.label,
>         scaler.transform(Vectors.dense(x.features.toArray))))
>
>     // Building the model
>     val numIterations = 100
>     val model = LinearRegressionWithSGD.train(parsedData, numIterations)
>
>     // Evaluate model on training examples and compute training error *
> tried using both scaledData and parsedData
>     val valuesAndPreds = scaledData.map { point =>
>       val prediction = model.predict(point.features)
>       (point.label, prediction)
>     }
>     val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
>     println("training Mean Squared Error = " + MSE)
>
> Both scaled and unscaled attempts give:
>
> training Mean Squared Error = NaN
>
> I've even tried x, y+(sample noise from normal with mean 0 and stddev 1)
> still comes up with the same thing.
>
> Is this not supposed to work for x and y or 2 dimensional plots? Is there
> something I'm missing or wrong in the code above? Or is there a limitation
> in the method?
>
> Thanks for any advice.
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/LinearRegressionWithSGD-accuracy-tp10127.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 10:29:55 2015
Return-Path: <dev-return-11171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CD4E173A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 10:29:55 +0000 (UTC)
Received: (qmail 98596 invoked by uid 500); 17 Jan 2015 10:29:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98524 invoked by uid 500); 17 Jan 2015 10:29:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98513 invoked by uid 99); 17 Jan 2015 10:29:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:29:54 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of yaochunnan@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:29:50 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id EEA5310D12F3
	for <dev@spark.apache.org>; Sat, 17 Jan 2015 02:29:29 -0800 (PST)
Date: Sat, 17 Jan 2015 03:29:29 -0700 (MST)
From: Chunnan Yao <yaochunnan@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421490569611-10163.post@n3.nabble.com>
In-Reply-To: <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

*I followed the procedures instructed by
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ. 
But problems still occurs which has made me a little bit annoyed. 

My environment settings are:JAVA 1.7.0 Scala: 2.10.4 Spark:1.2.0, Intellij
Idea 14.0.2, Ubuntu 14.04 

Firstly I got the scala plugin correctly installed. 

I choosed maven-3, hadoop-2.4, scala-2.10 as my profiles when importing the
project. 

After importing, I first turned on "View-Tool Windows-Maven Projects". I see
the "hbase-hadoop1" is selected, but I had not chosen it in the import
process. So I deselected it to leave the hadoop-2.4, maven-3, scala-2.10 to
be the only three selected items in "Maven Projects-Profiles".

According to the Wiki, the next step should be "Generate Sources and Update
Folders For All Projects". I did so, and waited for some minutes to get the
sub-projects prepared. 

Then I cleared the "Additional compiler options" in the
"File-Settings-Build, Execution, Deployment-Compiler-Scala Compiler". 

Finally I choosed Build-reBuild project. 

However, the compiler failed with "value q is not a member of stringcontext"
errors. *

(partial screen shot)
-------------------------------------------------------
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateProjection.scala
Error:(42, 21) value q is not a member of StringContext
    val lengthDef = q"final val length = $tupleLength"
                    ^
Error:(54, 7) value q is not a member of StringContext
      q"""
      ^
Error:(66, 9) value q is not a member of StringContext
        q"""
        ^
Error:(83, 9) value q is not a member of StringContext
        q"if(isNullAt($iLit)) { null } else { ${newTermName(s"c$i")} }"
        ^
Error:(85, 7) value q is not a member of StringContext
      q"override def iterator = Iterator[Any](..$allColumns)"
      ^
Error:(88, 27) value q is not a member of StringContext
    val accessorFailure = q"""scala.sys.error("Invalid ordinal:" + i)"""
                          ^
Error:(95, 9) value q is not a member of StringContext
        q"if(i == $ordinal) { if(isNullAt($i)) return null else return
$elementName }"
        ^
Error:(97, 7) value q is not a member of StringContext
      q"override def apply(i: Int): Any = { ..$cases; $accessorFailure }"
      ^
Error:(106, 9) value q is not a member of StringContext
        q"""
        ^
Error:(117, 7) value q is not a member of StringContext
      q"override def update(i: Int, value: Any): Unit = { ..$cases;
$accessorFailure }"
      ^
Error:(126, 11) value q is not a member of StringContext
          q"if(i == $i) return $elementName" :: Nil
          ^
Error:(130, 7) value q is not a member of StringContext
      q"""
      ^
Error:(143, 11) value q is not a member of StringContext
          q"if(i == $i) { nullBits($i) = false; $elementName = value; return
}" :: Nil
          ^
Error:(147, 7) value q is not a member of StringContext
      q"""
      ^
Error:(157, 29) value q is not a member of StringContext
        case BooleanType => q"if ($elementName) 0 else 1"
                            ^
Error:(158, 52) value q is not a member of StringContext
        case ByteType | ShortType | IntegerType => q"$elementName.toInt"
                                                   ^
Error:(159, 26) value q is not a member of StringContext
        case LongType => q"($elementName ^ ($elementName >>> 32)).toInt"
                         ^
Error:(160, 27) value q is not a member of StringContext
        case FloatType => q"java.lang.Float.floatToIntBits($elementName)"
                          ^
Error:(162, 11) value q is not a member of StringContext
          q"{ val b = java.lang.Double.doubleToLongBits($elementName); (b ^
(b >>>32)).toInt }"
          ^
Error:(163, 19) value q is not a member of StringContext
        case _ => q"$elementName.hashCode"
                  ^
Error:(165, 7) value q is not a member of StringContext
      q"if (isNullAt($i)) 0 else $nonNull"
      ^
Error:(168, 54) value q is not a member of StringContext
    val hashUpdates: Seq[Tree] = hashValues.map(v => q"""result = 37 *
result + $v""": Tree)
                                                     ^
Error:(171, 7) value q is not a member of StringContext
      q"""
      ^
Error:(181, 7) value q is not a member of StringContext
      q"if (this.$elementName != specificType.$elementName) return false"
      ^
Error:(185, 7) value q is not a member of StringContext
      q"""
      ^
Error:(195, 7) value q is not a member of StringContext
      q"""
      ^
Error:(210, 16) value q is not a member of StringContext
    val code = q"""
               ^
---------------------------------------------------------------------
*
Then I tried
http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea,
which does not clear the "Additional compiler options", but change the -P in
to -Xplugin. 
So now my "Additional Compiler Options" is like this
"-Xplugin:/home/yaochunnan/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar"

Then I reBuild again with the following errors: *
(partial screen shot)
----------------------------------------------------------------------
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
Error:(169, 38) not found: value HiveShim
          Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
                                     ^
Error:(177, 31) not found: value HiveShim
          tableParameters.put(HiveShim.getStatsSetupConstTotalSize,
newTotalSize.toString)
                              ^
Error:(292, 36) not found: value HiveShim
      val proc: CommandProcessor =
HiveShim.getCommandProcessor(Array(tokens(0)), hiveconf)
                                   ^
Error:(304, 25) not found: value HiveShim
          val results = HiveShim.createDriverResultsArray
                        ^
Error:(314, 11) not found: value HiveShim
          HiveShim.processResults(results)
          ^
Error:(418, 7) not found: value HiveShim
      HiveShim.createDecimal(decimal.toBigDecimal.underlying()).toString
      ^
Error:(420, 7) not found: value HiveShim
      HiveShim.createDecimal(decimal.underlying()).toString
      ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala
Error:(97, 7) not found: value HiveShim
      HiveShim.toCatalystDecimal(
      ^
Error:(123, 46) not found: value HiveShim
    case hdoi: HiveDecimalObjectInspector =>
HiveShim.toCatalystDecimal(hdoi, data)
                                             ^
Error:(156, 19) not found: value HiveShim
      (o: Any) =>
HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())
                  ^
Error:(210, 31) not found: value HiveShim
        case b: BigDecimal => HiveShim.createDecimal(b.underlying())
                              ^
Error:(211, 28) not found: value HiveShim
        case d: Decimal =>
HiveShim.createDecimal(d.toBigDecimal.underlying())
                           ^
Error:(283, 7) not found: value HiveShim
      HiveShim.getStringWritableConstantObjectInspector(value)
      ^
Error:(285, 7) not found: value HiveShim
      HiveShim.getIntWritableConstantObjectInspector(value)
      ^
Error:(287, 7) not found: value HiveShim
      HiveShim.getDoubleWritableConstantObjectInspector(value)
      ^
Error:(289, 7) not found: value HiveShim
      HiveShim.getBooleanWritableConstantObjectInspector(value)
      ^
Error:(291, 7) not found: value HiveShim
      HiveShim.getLongWritableConstantObjectInspector(value)
      ^
Error:(293, 7) not found: value HiveShim
      HiveShim.getFloatWritableConstantObjectInspector(value)
      ^
Error:(295, 7) not found: value HiveShim
      HiveShim.getShortWritableConstantObjectInspector(value)
      ^
Error:(297, 7) not found: value HiveShim
      HiveShim.getByteWritableConstantObjectInspector(value)
      ^
Error:(299, 7) not found: value HiveShim
      HiveShim.getBinaryWritableConstantObjectInspector(value)
      ^
Error:(301, 7) not found: value HiveShim
      HiveShim.getDateWritableConstantObjectInspector(value)
      ^
Error:(303, 7) not found: value HiveShim
      HiveShim.getTimestampWritableConstantObjectInspector(value)
      ^
Error:(305, 7) not found: value HiveShim
      HiveShim.getDecimalWritableConstantObjectInspector(value)
      ^
Error:(307, 7) not found: value HiveShim
      HiveShim.getPrimitiveNullWritableConstantObjectInspector
      ^
Error:(363, 51) not found: value HiveShim
    case w: WritableHiveDecimalObjectInspector =>
HiveShim.decimalTypeInfoToCatalyst(w)
                                                  ^
Error:(364, 47) not found: value HiveShim
    case j: JavaHiveDecimalObjectInspector =>
HiveShim.decimalTypeInfoToCatalyst(j)
                                              ^
Error:(393, 30) not found: value HiveShim
      case d: DecimalType => HiveShim.decimalTypeInfo(d)
                             ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
Error:(78, 11) not found: value HiveShim
          HiveShim.getAllPartitionsOf(client, table).toSeq
          ^
Error:(205, 7) not found: value HiveShim
      HiveShim.setLocation(tbl, crtTbl)
      ^
Error:(443, 28) not found: value HiveShim
    case d: DecimalType => HiveShim.decimalMetastoreString(d)
                           ^
Error:(472, 53) not found: value HiveShim
      val totalSize =
hiveQlTable.getParameters.get(HiveShim.getStatsSetupConstTotalSize)
                                                    ^
Error:(490, 19) not found: value HiveShim
  val tableDesc = HiveShim.getTableDesc(
                  ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
Error:(255, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(73, 53) not found: type HiveFunctionWrapper
private[hive] case class HiveSimpleUdf(funcWrapper: HiveFunctionWrapper,
children: Seq[Expression])
                                                    ^
Error:(57, 25) not found: type HiveFunctionWrapper
      HiveSimpleUdf(new HiveFunctionWrapper(functionClassName), children)
                        ^
Error:(134, 54) not found: type HiveFunctionWrapper
private[hive] case class HiveGenericUdf(funcWrapper: HiveFunctionWrapper,
children: Seq[Expression])
                                                     ^
Error:(59, 26) not found: type HiveFunctionWrapper
      HiveGenericUdf(new HiveFunctionWrapper(functionClassName), children)
                         ^
Error:(185, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(62, 27) not found: type HiveFunctionWrapper
      HiveGenericUdaf(new HiveFunctionWrapper(functionClassName), children)
                          ^
Error:(214, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(64, 20) not found: type HiveFunctionWrapper
      HiveUdaf(new HiveFunctionWrapper(functionClassName), children)
                   ^
Error:(66, 27) not found: type HiveFunctionWrapper
      HiveGenericUdtf(new HiveFunctionWrapper(functionClassName), Nil,
children)
                          ^
Error:(322, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
Error:(1132, 15) not found: type HiveFunctionWrapper
          new HiveFunctionWrapper(functionName),
              ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala
Error:(44, 34) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim._
                                 ^
Error:(43, 8) object ShimFileSinkDesc is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.{ ShimFileSinkDesc => FileSinkDesc}
       ^
Error:(76, 21) not found: type FileSinkDesc
      fileSinkConf: FileSinkDesc,
                    ^
Error:(142, 23) not found: value HiveShim
    val tmpLocation = HiveShim.getExternalTmpPath(hiveContext,
tableLocation)
                      ^
Error:(143, 28) not found: type FileSinkDesc
    val fileSinkConf = new FileSinkDesc(tmpLocation.toString, tableDesc,
false)
                           ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala
Error:(141, 22) not found: value HiveShim
      val partPath = HiveShim.getDataLocationPath(partition)
                     ^
Error:(298, 33) not found: value HiveShim
            row.update(ordinal, HiveShim.toCatalystDecimal(oi, value))
                                ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TestHive.scala
Error:(384, 3) not found: value HiveShim
  HiveShim.createDefaultDBIfNeeded(this)
  ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/DescribeHiveTableCommand.scala
Error:(29, 8) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim
       ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScan.scala
Error:(89, 5) not found: value HiveShim
    HiveShim.appendReadColumns(hiveConf, neededColumnIDs,
attributes.map(_.name))
    ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala
Error:(38, 34) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim._
                                 ^
Error:(37, 8) object ShimFileSinkDesc is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.{ShimFileSinkDesc => FileSinkDesc}
       ^
Error:(174, 19) not found: type FileSinkDesc
    fileSinkConf: FileSinkDesc,
                  ^
Error:(46, 19) not found: type FileSinkDesc
    fileSinkConf: FileSinkDesc)
                  ^
Error:(220, 33) not found: type FileSinkDesc
      val newFileSinkDesc = new FileSinkDesc(
                                ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala
Warning:(34, 2) @deprecated now takes two arguments; see the scaladoc.
@deprecated("No code should depend on FakeParquetHiveSerDe as it is only
intended as a " +
 ^
------------------------------------------------------------------------

*I thought it was the problem from Maven Profiles. So I tried reselecting
hbase-hadoop1 or hive or hbase-hadoop2. The error still occurs. Please help
me. This has annoyed me for a whole afternoon!*




-----
Feel the sparking Spark!
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-development-with-IntelliJ-tp10032p10163.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 10:35:28 2015
Return-Path: <dev-return-11172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EEC4173B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 10:35:28 +0000 (UTC)
Received: (qmail 6143 invoked by uid 500); 17 Jan 2015 10:35:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6073 invoked by uid 500); 17 Jan 2015 10:35:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6059 invoked by uid 99); 17 Jan 2015 10:35:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:35:29 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 10:35:03 +0000
Received: by mail-we0-f172.google.com with SMTP id k11so24034926wes.3
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 02:35:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=SHq1DwRnqX8yWWHmJhg5jFCKSOYdhHYXT3gL7EMXi/Y=;
        b=RpmEUzY+WrDmabcDzi4eyrp5s6fAVrXMlmr1BHMTLtGyaMQU3Y5y8vOChCCm7d/H4Y
         afqgbmnzaEUAg/ZUGkDSyDsGdvk5TvMiFWKgfVE7Kfc5r1ZP/kpBgyIFNEYpg5osBFyC
         CZdVMLIol3cxU4tkHISRhDYCGsFb6EPtSSbTsO1ylG2Rgy3GpvuqtNhdAAizY3y6gMQ3
         8PgCn6VZQX6Pq5a8LEMS4wVgU0NSdFcRvsub8UFw9C7m5RXSw4UKzTfKBNI9VoEH/c58
         vuZ/w4aukYr4XLDDgSR5aEk6G6Fc4Pdy+OPjU7e6ScfSRMlgprqG2LHR2SY3oI6cf1Y1
         ApZw==
X-Gm-Message-State: ALoCoQlLoIO+olNriE7gdfcD1gTa6EIX5AJKZoFzuPygA/FovmmLS4bdaAUrIehd1ed6OaGuDzd8
X-Received: by 10.194.92.235 with SMTP id cp11mr37393184wjb.112.1421490902593;
 Sat, 17 Jan 2015 02:35:02 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Sat, 17 Jan 2015 02:34:42 -0800 (PST)
In-Reply-To: <1421490569611-10163.post@n3.nabble.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
 <1421490569611-10163.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 17 Jan 2015 10:34:42 +0000
Message-ID: <CAMAsSd+2Z=DzBQwcKY4TFrh+ubr-xUiHx8Haw+N5BdFkRRtLjQ@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
To: Chunnan Yao <yaochunnan@gmail.com>, Imran Rashid <irashid@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes I've seen that error in the past too, and was just talking to
Imran the other day about it. I thought it only occurred when the hive
module was enabled, which I don't enable.

The problem is that the plugin that causes an error in IntelliJ for
scalac is what parses these values.* I think he got it to work with
this change: http://stackoverflow.com/questions/26788367/quasiquotes-in-intellij-14/26908554#26908554

If that works for you let's put it on the wiki.

* probably an ignorant question but is this feature important enough
to warrant the extra scala compiler plugin? the quasiquotes syntax I
mean.

On Sat, Jan 17, 2015 at 10:29 AM, Chunnan Yao <yaochunnan@gmail.com> wrote:
> *I followed the procedures instructed by
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ.
> But problems still occurs which has made me a little bit annoyed.
>
> My environment settings are:JAVA 1.7.0 Scala: 2.10.4 Spark:1.2.0, Intellij
> Idea 14.0.2, Ubuntu 14.04
>
> Firstly I got the scala plugin correctly installed.
>
> I choosed maven-3, hadoop-2.4, scala-2.10 as my profiles when importing the
> project.
>
> After importing, I first turned on "View-Tool Windows-Maven Projects". I see
> the "hbase-hadoop1" is selected, but I had not chosen it in the import
> process. So I deselected it to leave the hadoop-2.4, maven-3, scala-2.10 to
> be the only three selected items in "Maven Projects-Profiles".
>
> According to the Wiki, the next step should be "Generate Sources and Update
> Folders For All Projects". I did so, and waited for some minutes to get the
> sub-projects prepared.
>
> Then I cleared the "Additional compiler options" in the
> "File-Settings-Build, Execution, Deployment-Compiler-Scala Compiler".
>
> Finally I choosed Build-reBuild project.
>
> However, the compiler failed with "value q is not a member of stringcontext"
> errors. *
>
> (partial screen shot)
> -------------------------------------------------------
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateProjection.scala
> Error:(42, 21) value q is not a member of StringContext
>     val lengthDef = q"final val length = $tupleLength"
>                     ^
> Error:(54, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(66, 9) value q is not a member of StringContext
>         q"""
>         ^
> Error:(83, 9) value q is not a member of StringContext
>         q"if(isNullAt($iLit)) { null } else { ${newTermName(s"c$i")} }"
>         ^
> Error:(85, 7) value q is not a member of StringContext
>       q"override def iterator = Iterator[Any](..$allColumns)"
>       ^
> Error:(88, 27) value q is not a member of StringContext
>     val accessorFailure = q"""scala.sys.error("Invalid ordinal:" + i)"""
>                           ^
> Error:(95, 9) value q is not a member of StringContext
>         q"if(i == $ordinal) { if(isNullAt($i)) return null else return
> $elementName }"
>         ^
> Error:(97, 7) value q is not a member of StringContext
>       q"override def apply(i: Int): Any = { ..$cases; $accessorFailure }"
>       ^
> Error:(106, 9) value q is not a member of StringContext
>         q"""
>         ^
> Error:(117, 7) value q is not a member of StringContext
>       q"override def update(i: Int, value: Any): Unit = { ..$cases;
> $accessorFailure }"
>       ^
> Error:(126, 11) value q is not a member of StringContext
>           q"if(i == $i) return $elementName" :: Nil
>           ^
> Error:(130, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(143, 11) value q is not a member of StringContext
>           q"if(i == $i) { nullBits($i) = false; $elementName = value; return
> }" :: Nil
>           ^
> Error:(147, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(157, 29) value q is not a member of StringContext
>         case BooleanType => q"if ($elementName) 0 else 1"
>                             ^
> Error:(158, 52) value q is not a member of StringContext
>         case ByteType | ShortType | IntegerType => q"$elementName.toInt"
>                                                    ^
> Error:(159, 26) value q is not a member of StringContext
>         case LongType => q"($elementName ^ ($elementName >>> 32)).toInt"
>                          ^
> Error:(160, 27) value q is not a member of StringContext
>         case FloatType => q"java.lang.Float.floatToIntBits($elementName)"
>                           ^
> Error:(162, 11) value q is not a member of StringContext
>           q"{ val b = java.lang.Double.doubleToLongBits($elementName); (b ^
> (b >>>32)).toInt }"
>           ^
> Error:(163, 19) value q is not a member of StringContext
>         case _ => q"$elementName.hashCode"
>                   ^
> Error:(165, 7) value q is not a member of StringContext
>       q"if (isNullAt($i)) 0 else $nonNull"
>       ^
> Error:(168, 54) value q is not a member of StringContext
>     val hashUpdates: Seq[Tree] = hashValues.map(v => q"""result = 37 *
> result + $v""": Tree)
>                                                      ^
> Error:(171, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(181, 7) value q is not a member of StringContext
>       q"if (this.$elementName != specificType.$elementName) return false"
>       ^
> Error:(185, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(195, 7) value q is not a member of StringContext
>       q"""
>       ^
> Error:(210, 16) value q is not a member of StringContext
>     val code = q"""
>                ^
> ---------------------------------------------------------------------
> *
> Then I tried
> http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea,
> which does not clear the "Additional compiler options", but change the -P in
> to -Xplugin.
> So now my "Additional Compiler Options" is like this
> "-Xplugin:/home/yaochunnan/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar"
>
> Then I reBuild again with the following errors: *
> (partial screen shot)
> ----------------------------------------------------------------------
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
> Error:(169, 38) not found: value HiveShim
>           Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
>                                      ^
> Error:(177, 31) not found: value HiveShim
>           tableParameters.put(HiveShim.getStatsSetupConstTotalSize,
> newTotalSize.toString)
>                               ^
> Error:(292, 36) not found: value HiveShim
>       val proc: CommandProcessor =
> HiveShim.getCommandProcessor(Array(tokens(0)), hiveconf)
>                                    ^
> Error:(304, 25) not found: value HiveShim
>           val results = HiveShim.createDriverResultsArray
>                         ^
> Error:(314, 11) not found: value HiveShim
>           HiveShim.processResults(results)
>           ^
> Error:(418, 7) not found: value HiveShim
>       HiveShim.createDecimal(decimal.toBigDecimal.underlying()).toString
>       ^
> Error:(420, 7) not found: value HiveShim
>       HiveShim.createDecimal(decimal.underlying()).toString
>       ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala
> Error:(97, 7) not found: value HiveShim
>       HiveShim.toCatalystDecimal(
>       ^
> Error:(123, 46) not found: value HiveShim
>     case hdoi: HiveDecimalObjectInspector =>
> HiveShim.toCatalystDecimal(hdoi, data)
>                                              ^
> Error:(156, 19) not found: value HiveShim
>       (o: Any) =>
> HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())
>                   ^
> Error:(210, 31) not found: value HiveShim
>         case b: BigDecimal => HiveShim.createDecimal(b.underlying())
>                               ^
> Error:(211, 28) not found: value HiveShim
>         case d: Decimal =>
> HiveShim.createDecimal(d.toBigDecimal.underlying())
>                            ^
> Error:(283, 7) not found: value HiveShim
>       HiveShim.getStringWritableConstantObjectInspector(value)
>       ^
> Error:(285, 7) not found: value HiveShim
>       HiveShim.getIntWritableConstantObjectInspector(value)
>       ^
> Error:(287, 7) not found: value HiveShim
>       HiveShim.getDoubleWritableConstantObjectInspector(value)
>       ^
> Error:(289, 7) not found: value HiveShim
>       HiveShim.getBooleanWritableConstantObjectInspector(value)
>       ^
> Error:(291, 7) not found: value HiveShim
>       HiveShim.getLongWritableConstantObjectInspector(value)
>       ^
> Error:(293, 7) not found: value HiveShim
>       HiveShim.getFloatWritableConstantObjectInspector(value)
>       ^
> Error:(295, 7) not found: value HiveShim
>       HiveShim.getShortWritableConstantObjectInspector(value)
>       ^
> Error:(297, 7) not found: value HiveShim
>       HiveShim.getByteWritableConstantObjectInspector(value)
>       ^
> Error:(299, 7) not found: value HiveShim
>       HiveShim.getBinaryWritableConstantObjectInspector(value)
>       ^
> Error:(301, 7) not found: value HiveShim
>       HiveShim.getDateWritableConstantObjectInspector(value)
>       ^
> Error:(303, 7) not found: value HiveShim
>       HiveShim.getTimestampWritableConstantObjectInspector(value)
>       ^
> Error:(305, 7) not found: value HiveShim
>       HiveShim.getDecimalWritableConstantObjectInspector(value)
>       ^
> Error:(307, 7) not found: value HiveShim
>       HiveShim.getPrimitiveNullWritableConstantObjectInspector
>       ^
> Error:(363, 51) not found: value HiveShim
>     case w: WritableHiveDecimalObjectInspector =>
> HiveShim.decimalTypeInfoToCatalyst(w)
>                                                   ^
> Error:(364, 47) not found: value HiveShim
>     case j: JavaHiveDecimalObjectInspector =>
> HiveShim.decimalTypeInfoToCatalyst(j)
>                                               ^
> Error:(393, 30) not found: value HiveShim
>       case d: DecimalType => HiveShim.decimalTypeInfo(d)
>                              ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
> Error:(78, 11) not found: value HiveShim
>           HiveShim.getAllPartitionsOf(client, table).toSeq
>           ^
> Error:(205, 7) not found: value HiveShim
>       HiveShim.setLocation(tbl, crtTbl)
>       ^
> Error:(443, 28) not found: value HiveShim
>     case d: DecimalType => HiveShim.decimalMetastoreString(d)
>                            ^
> Error:(472, 53) not found: value HiveShim
>       val totalSize =
> hiveQlTable.getParameters.get(HiveShim.getStatsSetupConstTotalSize)
>                                                     ^
> Error:(490, 19) not found: value HiveShim
>   val tableDesc = HiveShim.getTableDesc(
>                   ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
> Error:(255, 18) not found: type HiveFunctionWrapper
>     funcWrapper: HiveFunctionWrapper,
>                  ^
> Error:(73, 53) not found: type HiveFunctionWrapper
> private[hive] case class HiveSimpleUdf(funcWrapper: HiveFunctionWrapper,
> children: Seq[Expression])
>                                                     ^
> Error:(57, 25) not found: type HiveFunctionWrapper
>       HiveSimpleUdf(new HiveFunctionWrapper(functionClassName), children)
>                         ^
> Error:(134, 54) not found: type HiveFunctionWrapper
> private[hive] case class HiveGenericUdf(funcWrapper: HiveFunctionWrapper,
> children: Seq[Expression])
>                                                      ^
> Error:(59, 26) not found: type HiveFunctionWrapper
>       HiveGenericUdf(new HiveFunctionWrapper(functionClassName), children)
>                          ^
> Error:(185, 18) not found: type HiveFunctionWrapper
>     funcWrapper: HiveFunctionWrapper,
>                  ^
> Error:(62, 27) not found: type HiveFunctionWrapper
>       HiveGenericUdaf(new HiveFunctionWrapper(functionClassName), children)
>                           ^
> Error:(214, 18) not found: type HiveFunctionWrapper
>     funcWrapper: HiveFunctionWrapper,
>                  ^
> Error:(64, 20) not found: type HiveFunctionWrapper
>       HiveUdaf(new HiveFunctionWrapper(functionClassName), children)
>                    ^
> Error:(66, 27) not found: type HiveFunctionWrapper
>       HiveGenericUdtf(new HiveFunctionWrapper(functionClassName), Nil,
> children)
>                           ^
> Error:(322, 18) not found: type HiveFunctionWrapper
>     funcWrapper: HiveFunctionWrapper,
>                  ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
> Error:(1132, 15) not found: type HiveFunctionWrapper
>           new HiveFunctionWrapper(functionName),
>               ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala
> Error:(44, 34) object HiveShim is not a member of package
> org.apache.spark.sql.hive
> import org.apache.spark.sql.hive.HiveShim._
>                                  ^
> Error:(43, 8) object ShimFileSinkDesc is not a member of package
> org.apache.spark.sql.hive
> import org.apache.spark.sql.hive.{ ShimFileSinkDesc => FileSinkDesc}
>        ^
> Error:(76, 21) not found: type FileSinkDesc
>       fileSinkConf: FileSinkDesc,
>                     ^
> Error:(142, 23) not found: value HiveShim
>     val tmpLocation = HiveShim.getExternalTmpPath(hiveContext,
> tableLocation)
>                       ^
> Error:(143, 28) not found: type FileSinkDesc
>     val fileSinkConf = new FileSinkDesc(tmpLocation.toString, tableDesc,
> false)
>                            ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala
> Error:(141, 22) not found: value HiveShim
>       val partPath = HiveShim.getDataLocationPath(partition)
>                      ^
> Error:(298, 33) not found: value HiveShim
>             row.update(ordinal, HiveShim.toCatalystDecimal(oi, value))
>                                 ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TestHive.scala
> Error:(384, 3) not found: value HiveShim
>   HiveShim.createDefaultDBIfNeeded(this)
>   ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/DescribeHiveTableCommand.scala
> Error:(29, 8) object HiveShim is not a member of package
> org.apache.spark.sql.hive
> import org.apache.spark.sql.hive.HiveShim
>        ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScan.scala
> Error:(89, 5) not found: value HiveShim
>     HiveShim.appendReadColumns(hiveConf, neededColumnIDs,
> attributes.map(_.name))
>     ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala
> Error:(38, 34) object HiveShim is not a member of package
> org.apache.spark.sql.hive
> import org.apache.spark.sql.hive.HiveShim._
>                                  ^
> Error:(37, 8) object ShimFileSinkDesc is not a member of package
> org.apache.spark.sql.hive
> import org.apache.spark.sql.hive.{ShimFileSinkDesc => FileSinkDesc}
>        ^
> Error:(174, 19) not found: type FileSinkDesc
>     fileSinkConf: FileSinkDesc,
>                   ^
> Error:(46, 19) not found: type FileSinkDesc
>     fileSinkConf: FileSinkDesc)
>                   ^
> Error:(220, 33) not found: type FileSinkDesc
>       val newFileSinkDesc = new FileSinkDesc(
>                                 ^
> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala
> Warning:(34, 2) @deprecated now takes two arguments; see the scaladoc.
> @deprecated("No code should depend on FakeParquetHiveSerDe as it is only
> intended as a " +
>  ^
> ------------------------------------------------------------------------
>
> *I thought it was the problem from Maven Profiles. So I tried reselecting
> hbase-hadoop1 or hive or hbase-hadoop2. The error still occurs. Please help
> me. This has annoyed me for a whole afternoon!*
>
>
>
>
> -----
> Feel the sparking Spark!
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-development-with-IntelliJ-tp10032p10163.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 20:18:35 2015
Return-Path: <dev-return-11173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8EDD17CC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 20:18:35 +0000 (UTC)
Received: (qmail 98250 invoked by uid 500); 17 Jan 2015 20:18:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98179 invoked by uid 500); 17 Jan 2015 20:18:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98163 invoked by uid 99); 17 Jan 2015 20:18:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:18:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yaochunnan@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:18:26 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so12248026qgd.13
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 12:16:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FoVBvEJ/P1P+2saQdxrYnGheXC4DVZ353k1149uAMiM=;
        b=j5Bf3G6hVedT7bLCip6tiLADK+H/wrdVbXihH5fpkkRtWR/CAyGoAzhNqeJheXwVHH
         ih26NDrnTYpQhATFFLdzToeVcxdw6SCe9PKQgsW4jRNwUwkiywO6iC/TEEl4pVkJkjVA
         TVqmYEWAT7TVUtKU/YUgn934JF4rFT0z7UP9lMxOysRU6YtCk5rvZNXRWhezRB4bXAra
         xSLiPvGAFy7mPbrtF+yUQKDrUT1mSoBY7rNiuln9FnymEJn+hJAG6uijhfVsQWMHbYYZ
         iRdOyCWDqogTlBm5nBboX1jytqBx6oFYhMdUk31e56ISkt0fd1sv9sjWJIhKtSUjtJWi
         DD7Q==
MIME-Version: 1.0
X-Received: by 10.140.82.47 with SMTP id g44mr33419097qgd.42.1421525794826;
 Sat, 17 Jan 2015 12:16:34 -0800 (PST)
Received: by 10.229.146.201 with HTTP; Sat, 17 Jan 2015 12:16:34 -0800 (PST)
In-Reply-To: <CAL34EqFvZbdpDXyb+UPtFdt7cSc80EdzmE5m3UGkH7wayqOAsA@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz>
	<CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
	<1421490569611-10163.post@n3.nabble.com>
	<CAMAsSd+2Z=DzBQwcKY4TFrh+ubr-xUiHx8Haw+N5BdFkRRtLjQ@mail.gmail.com>
	<CA+3qhFTmvt1xcf7P-H--Tbw_gGp8hLDzZA_3ZsazsZtJ8W5GGQ@mail.gmail.com>
	<CAL34EqFxBXGCezNEXTYXAqLKnvtVM3Ft-ZhPj1oNmLO1iSjxTA@mail.gmail.com>
	<CA+3qhFSO9RyAUyyBZFaBrC3Rhs6xpjEmRcmq6BbUuVwFBnm46A@mail.gmail.com>
	<CAL34EqHrXEXi+Q+EOoGoW_+=h4q01eTZTdhfjJLXSs-=MAK95Q@mail.gmail.com>
	<CA+3qhFSTpTQKicKJLSThvu=p_ZDfUXJiXUce8HCcHKRRE=8GyA@mail.gmail.com>
	<CAL34EqFvZbdpDXyb+UPtFdt7cSc80EdzmE5m3UGkH7wayqOAsA@mail.gmail.com>
Date: Sun, 18 Jan 2015 04:16:34 +0800
Message-ID: <CAL34EqHOGsvrL9rSUT4FjvoX86gkjS5aWtRSNP6HnMLNc39bZA@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
From: Chunnan Yao <yaochunnan@gmail.com>
To: Imran Rashid <irashid@cloudera.com>, Sean Owen <sowen@cloudera.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c129980a3061050cdec4b8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c129980a3061050cdec4b8
Content-Type: text/plain; charset=UTF-8

Followed is the discussion between Imran and me.

2015-01-18 4:12 GMT+08:00 Chunnan Yao <yaochunnan@gmail.com>:

> Thank you for your patience! Im now not so familiar with the mailing list.
> I just clicked "reply" in Gmail, thinking it would be automatically
> attached to the list. I will later post the missed information to the
> spark-user list :) Your suggestions really helps!
>
> 2015-01-18 4:05 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>
>> ah, that is a very different question.  The point of using Intellij
>> really is not to build the fully packaged binaries -- its really just for
>> the features of the IDE while developing, eg. code navigation, debugger,
>> etc.  Use either sbt or maven for building the packaged binaries.  (There
>> probably is some way to get Intellij to build the packages, by calling
>> maven, but I can't see much advantage in doing that.)
>>
>> The instructions for building are here:
>>
>> https://spark.apache.org/docs/latest/building-spark.html#building-with-sbt
>>
>> eg., for building with maven, you can do:
>>
>> export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
>>
>> mvn -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -DskipTests clean package
>>
>>
>> or for sbt, you can do:
>>
>> sbt/sbt -Pyarn -Phadoop-2.3 assembly
>>
>>
>>
>> btw, these replies are only going to me, not the spark-user list, not
>> sure if that was intentional?
>>
>> hope this helps,
>> Imran
>>
>> On Sat, Jan 17, 2015 at 11:00 AM, Chunnan Yao <yaochunnan@gmail.com>
>> wrote:
>>
>>> Thank you very much for your reply! But how can I generate deployable
>>> spark binary package like those pre-built packages? I am new with Maven.
>>>
>>> 2015-01-18 1:44 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>>>
>>>> The build output location is set by the maven build which is
>>>>
>>>> <sub-project>/target/scala-<version>/[test-]classes/
>>>>
>>>> eg.
>>>>
>>>> core/target/scala-2.10/classes/
>>>>
>>>>
>>>> On Sat, Jan 17, 2015 at 8:51 AM, Chunnan Yao <yaochunnan@gmail.com>
>>>> wrote:
>>>>
>>>>> I don't know if it's a naive question.  Although the fix  (moving the
>>>>> paradise jar from "Additional compiler options" to "compiler
>>>>> plugins") works fine (it has removed all the errors I was faced, but still
>>>>> leaves 83 warnings), but I cannot find my compile results (which should be
>>>>> in the /spark-1.2.0/out dictionary. What's the problem? The compiler has
>>>>> told me the compilation completed successfully.
>>>>>
>>>>> 2015-01-17 23:28 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>>>>>
>>>>>> I experienced these errors in Intellij even without the hive mode
>>>>>> enabled.  I think its also a question of which project you are trying t
>>>>>> compile.  eg. core built fine, but I got these errors when I tried to build
>>>>>> sql.  If the fix works for you (moving the paradise jar from "Additional
>>>>>> compiler options" to "compiler plugins") then we should definitely put it
>>>>>> on the wiki.
>>>>>>
>>>>>> writing macros is really painful with quasiquotes, so it is probably
>>>>>> worth it ...
>>>>>>
>>>>>> On Sat, Jan 17, 2015 at 2:34 AM, Sean Owen <sowen@cloudera.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Yes I've seen that error in the past too, and was just talking to
>>>>>>> Imran the other day about it. I thought it only occurred when the
>>>>>>> hive
>>>>>>> module was enabled, which I don't enable.
>>>>>>>
>>>>>>> The problem is that the plugin that causes an error in IntelliJ for
>>>>>>> scalac is what parses these values.* I think he got it to work with
>>>>>>> this change:
>>>>>>> http://stackoverflow.com/questions/26788367/quasiquotes-in-intellij-14/26908554#26908554
>>>>>>>
>>>>>>> If that works for you let's put it on the wiki.
>>>>>>>
>>>>>>> * probably an ignorant question but is this feature important enough
>>>>>>> to warrant the extra scala compiler plugin? the quasiquotes syntax I
>>>>>>> mean.
>>>>>>>
>>>>>>> On Sat, Jan 17, 2015 at 10:29 AM, Chunnan Yao <yaochunnan@gmail.com>
>>>>>>> wrote:
>>>>>>> > *I followed the procedures instructed by
>>>>>>> >
>>>>>>> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ
>>>>>>> .
>>>>>>> > But problems still occurs which has made me a little bit annoyed.
>>>>>>> >
>>>>>>> > My environment settings are:JAVA 1.7.0 Scala: 2.10.4 Spark:1.2.0,
>>>>>>> Intellij
>>>>>>> > Idea 14.0.2, Ubuntu 14.04
>>>>>>> >
>>>>>>> > Firstly I got the scala plugin correctly installed.
>>>>>>> >
>>>>>>> > I choosed maven-3, hadoop-2.4, scala-2.10 as my profiles when
>>>>>>> importing the
>>>>>>> > project.
>>>>>>> >
>>>>>>> > After importing, I first turned on "View-Tool Windows-Maven
>>>>>>> Projects". I see
>>>>>>> > the "hbase-hadoop1" is selected, but I had not chosen it in the
>>>>>>> import
>>>>>>> > process. So I deselected it to leave the hadoop-2.4, maven-3,
>>>>>>> scala-2.10 to
>>>>>>> > be the only three selected items in "Maven Projects-Profiles".
>>>>>>> >
>>>>>>> > According to the Wiki, the next step should be "Generate Sources
>>>>>>> and Update
>>>>>>> > Folders For All Projects". I did so, and waited for some minutes
>>>>>>> to get the
>>>>>>> > sub-projects prepared.
>>>>>>> >
>>>>>>> > Then I cleared the "Additional compiler options" in the
>>>>>>> > "File-Settings-Build, Execution, Deployment-Compiler-Scala
>>>>>>> Compiler".
>>>>>>> >
>>>>>>> > Finally I choosed Build-reBuild project.
>>>>>>> >
>>>>>>> > However, the compiler failed with "value q is not a member of
>>>>>>> stringcontext"
>>>>>>> > errors. *
>>>>>>> >
>>>>>>> > (partial screen shot)
>>>>>>> > -------------------------------------------------------
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateProjection.scala
>>>>>>> > Error:(42, 21) value q is not a member of StringContext
>>>>>>> >     val lengthDef = q"final val length = $tupleLength"
>>>>>>> >                     ^
>>>>>>> > Error:(54, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(66, 9) value q is not a member of StringContext
>>>>>>> >         q"""
>>>>>>> >         ^
>>>>>>> > Error:(83, 9) value q is not a member of StringContext
>>>>>>> >         q"if(isNullAt($iLit)) { null } else {
>>>>>>> ${newTermName(s"c$i")} }"
>>>>>>> >         ^
>>>>>>> > Error:(85, 7) value q is not a member of StringContext
>>>>>>> >       q"override def iterator = Iterator[Any](..$allColumns)"
>>>>>>> >       ^
>>>>>>> > Error:(88, 27) value q is not a member of StringContext
>>>>>>> >     val accessorFailure = q"""scala.sys.error("Invalid ordinal:" +
>>>>>>> i)"""
>>>>>>> >                           ^
>>>>>>> > Error:(95, 9) value q is not a member of StringContext
>>>>>>> >         q"if(i == $ordinal) { if(isNullAt($i)) return null else
>>>>>>> return
>>>>>>> > $elementName }"
>>>>>>> >         ^
>>>>>>> > Error:(97, 7) value q is not a member of StringContext
>>>>>>> >       q"override def apply(i: Int): Any = { ..$cases;
>>>>>>> $accessorFailure }"
>>>>>>> >       ^
>>>>>>> > Error:(106, 9) value q is not a member of StringContext
>>>>>>> >         q"""
>>>>>>> >         ^
>>>>>>> > Error:(117, 7) value q is not a member of StringContext
>>>>>>> >       q"override def update(i: Int, value: Any): Unit = { ..$cases;
>>>>>>> > $accessorFailure }"
>>>>>>> >       ^
>>>>>>> > Error:(126, 11) value q is not a member of StringContext
>>>>>>> >           q"if(i == $i) return $elementName" :: Nil
>>>>>>> >           ^
>>>>>>> > Error:(130, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(143, 11) value q is not a member of StringContext
>>>>>>> >           q"if(i == $i) { nullBits($i) = false; $elementName =
>>>>>>> value; return
>>>>>>> > }" :: Nil
>>>>>>> >           ^
>>>>>>> > Error:(147, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(157, 29) value q is not a member of StringContext
>>>>>>> >         case BooleanType => q"if ($elementName) 0 else 1"
>>>>>>> >                             ^
>>>>>>> > Error:(158, 52) value q is not a member of StringContext
>>>>>>> >         case ByteType | ShortType | IntegerType =>
>>>>>>> q"$elementName.toInt"
>>>>>>> >                                                    ^
>>>>>>> > Error:(159, 26) value q is not a member of StringContext
>>>>>>> >         case LongType => q"($elementName ^ ($elementName >>>
>>>>>>> 32)).toInt"
>>>>>>> >                          ^
>>>>>>> > Error:(160, 27) value q is not a member of StringContext
>>>>>>> >         case FloatType =>
>>>>>>> q"java.lang.Float.floatToIntBits($elementName)"
>>>>>>> >                           ^
>>>>>>> > Error:(162, 11) value q is not a member of StringContext
>>>>>>> >           q"{ val b =
>>>>>>> java.lang.Double.doubleToLongBits($elementName); (b ^
>>>>>>> > (b >>>32)).toInt }"
>>>>>>> >           ^
>>>>>>> > Error:(163, 19) value q is not a member of StringContext
>>>>>>> >         case _ => q"$elementName.hashCode"
>>>>>>> >                   ^
>>>>>>> > Error:(165, 7) value q is not a member of StringContext
>>>>>>> >       q"if (isNullAt($i)) 0 else $nonNull"
>>>>>>> >       ^
>>>>>>> > Error:(168, 54) value q is not a member of StringContext
>>>>>>> >     val hashUpdates: Seq[Tree] = hashValues.map(v => q"""result =
>>>>>>> 37 *
>>>>>>> > result + $v""": Tree)
>>>>>>> >                                                      ^
>>>>>>> > Error:(171, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(181, 7) value q is not a member of StringContext
>>>>>>> >       q"if (this.$elementName != specificType.$elementName) return
>>>>>>> false"
>>>>>>> >       ^
>>>>>>> > Error:(185, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(195, 7) value q is not a member of StringContext
>>>>>>> >       q"""
>>>>>>> >       ^
>>>>>>> > Error:(210, 16) value q is not a member of StringContext
>>>>>>> >     val code = q"""
>>>>>>> >                ^
>>>>>>> >
>>>>>>> ---------------------------------------------------------------------
>>>>>>> > *
>>>>>>> > Then I tried
>>>>>>> >
>>>>>>> http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea
>>>>>>> ,
>>>>>>> > which does not clear the "Additional compiler options", but change
>>>>>>> the -P in
>>>>>>> > to -Xplugin.
>>>>>>> > So now my "Additional Compiler Options" is like this
>>>>>>> >
>>>>>>> "-Xplugin:/home/yaochunnan/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar"
>>>>>>> >
>>>>>>> > Then I reBuild again with the following errors: *
>>>>>>> > (partial screen shot)
>>>>>>> >
>>>>>>> ----------------------------------------------------------------------
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
>>>>>>> > Error:(169, 38) not found: value HiveShim
>>>>>>> >
>>>>>>>  Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
>>>>>>> >                                      ^
>>>>>>> > Error:(177, 31) not found: value HiveShim
>>>>>>> >           tableParameters.put(HiveShim.getStatsSetupConstTotalSize,
>>>>>>> > newTotalSize.toString)
>>>>>>> >                               ^
>>>>>>> > Error:(292, 36) not found: value HiveShim
>>>>>>> >       val proc: CommandProcessor =
>>>>>>> > HiveShim.getCommandProcessor(Array(tokens(0)), hiveconf)
>>>>>>> >                                    ^
>>>>>>> > Error:(304, 25) not found: value HiveShim
>>>>>>> >           val results = HiveShim.createDriverResultsArray
>>>>>>> >                         ^
>>>>>>> > Error:(314, 11) not found: value HiveShim
>>>>>>> >           HiveShim.processResults(results)
>>>>>>> >           ^
>>>>>>> > Error:(418, 7) not found: value HiveShim
>>>>>>> >
>>>>>>>  HiveShim.createDecimal(decimal.toBigDecimal.underlying()).toString
>>>>>>> >       ^
>>>>>>> > Error:(420, 7) not found: value HiveShim
>>>>>>> >       HiveShim.createDecimal(decimal.underlying()).toString
>>>>>>> >       ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala
>>>>>>> > Error:(97, 7) not found: value HiveShim
>>>>>>> >       HiveShim.toCatalystDecimal(
>>>>>>> >       ^
>>>>>>> > Error:(123, 46) not found: value HiveShim
>>>>>>> >     case hdoi: HiveDecimalObjectInspector =>
>>>>>>> > HiveShim.toCatalystDecimal(hdoi, data)
>>>>>>> >                                              ^
>>>>>>> > Error:(156, 19) not found: value HiveShim
>>>>>>> >       (o: Any) =>
>>>>>>> >
>>>>>>> HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())
>>>>>>> >                   ^
>>>>>>> > Error:(210, 31) not found: value HiveShim
>>>>>>> >         case b: BigDecimal =>
>>>>>>> HiveShim.createDecimal(b.underlying())
>>>>>>> >                               ^
>>>>>>> > Error:(211, 28) not found: value HiveShim
>>>>>>> >         case d: Decimal =>
>>>>>>> > HiveShim.createDecimal(d.toBigDecimal.underlying())
>>>>>>> >                            ^
>>>>>>> > Error:(283, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getStringWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(285, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getIntWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(287, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getDoubleWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(289, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getBooleanWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(291, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getLongWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(293, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getFloatWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(295, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getShortWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(297, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getByteWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(299, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getBinaryWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(301, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getDateWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(303, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getTimestampWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(305, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getDecimalWritableConstantObjectInspector(value)
>>>>>>> >       ^
>>>>>>> > Error:(307, 7) not found: value HiveShim
>>>>>>> >       HiveShim.getPrimitiveNullWritableConstantObjectInspector
>>>>>>> >       ^
>>>>>>> > Error:(363, 51) not found: value HiveShim
>>>>>>> >     case w: WritableHiveDecimalObjectInspector =>
>>>>>>> > HiveShim.decimalTypeInfoToCatalyst(w)
>>>>>>> >                                                   ^
>>>>>>> > Error:(364, 47) not found: value HiveShim
>>>>>>> >     case j: JavaHiveDecimalObjectInspector =>
>>>>>>> > HiveShim.decimalTypeInfoToCatalyst(j)
>>>>>>> >                                               ^
>>>>>>> > Error:(393, 30) not found: value HiveShim
>>>>>>> >       case d: DecimalType => HiveShim.decimalTypeInfo(d)
>>>>>>> >                              ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
>>>>>>> > Error:(78, 11) not found: value HiveShim
>>>>>>> >           HiveShim.getAllPartitionsOf(client, table).toSeq
>>>>>>> >           ^
>>>>>>> > Error:(205, 7) not found: value HiveShim
>>>>>>> >       HiveShim.setLocation(tbl, crtTbl)
>>>>>>> >       ^
>>>>>>> > Error:(443, 28) not found: value HiveShim
>>>>>>> >     case d: DecimalType => HiveShim.decimalMetastoreString(d)
>>>>>>> >                            ^
>>>>>>> > Error:(472, 53) not found: value HiveShim
>>>>>>> >       val totalSize =
>>>>>>> > hiveQlTable.getParameters.get(HiveShim.getStatsSetupConstTotalSize)
>>>>>>> >                                                     ^
>>>>>>> > Error:(490, 19) not found: value HiveShim
>>>>>>> >   val tableDesc = HiveShim.getTableDesc(
>>>>>>> >                   ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
>>>>>>> > Error:(255, 18) not found: type HiveFunctionWrapper
>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>> >                  ^
>>>>>>> > Error:(73, 53) not found: type HiveFunctionWrapper
>>>>>>> > private[hive] case class HiveSimpleUdf(funcWrapper:
>>>>>>> HiveFunctionWrapper,
>>>>>>> > children: Seq[Expression])
>>>>>>> >                                                     ^
>>>>>>> > Error:(57, 25) not found: type HiveFunctionWrapper
>>>>>>> >       HiveSimpleUdf(new HiveFunctionWrapper(functionClassName),
>>>>>>> children)
>>>>>>> >                         ^
>>>>>>> > Error:(134, 54) not found: type HiveFunctionWrapper
>>>>>>> > private[hive] case class HiveGenericUdf(funcWrapper:
>>>>>>> HiveFunctionWrapper,
>>>>>>> > children: Seq[Expression])
>>>>>>> >                                                      ^
>>>>>>> > Error:(59, 26) not found: type HiveFunctionWrapper
>>>>>>> >       HiveGenericUdf(new HiveFunctionWrapper(functionClassName),
>>>>>>> children)
>>>>>>> >                          ^
>>>>>>> > Error:(185, 18) not found: type HiveFunctionWrapper
>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>> >                  ^
>>>>>>> > Error:(62, 27) not found: type HiveFunctionWrapper
>>>>>>> >       HiveGenericUdaf(new HiveFunctionWrapper(functionClassName),
>>>>>>> children)
>>>>>>> >                           ^
>>>>>>> > Error:(214, 18) not found: type HiveFunctionWrapper
>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>> >                  ^
>>>>>>> > Error:(64, 20) not found: type HiveFunctionWrapper
>>>>>>> >       HiveUdaf(new HiveFunctionWrapper(functionClassName),
>>>>>>> children)
>>>>>>> >                    ^
>>>>>>> > Error:(66, 27) not found: type HiveFunctionWrapper
>>>>>>> >       HiveGenericUdtf(new HiveFunctionWrapper(functionClassName),
>>>>>>> Nil,
>>>>>>> > children)
>>>>>>> >                           ^
>>>>>>> > Error:(322, 18) not found: type HiveFunctionWrapper
>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>> >                  ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
>>>>>>> > Error:(1132, 15) not found: type HiveFunctionWrapper
>>>>>>> >           new HiveFunctionWrapper(functionName),
>>>>>>> >               ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala
>>>>>>> > Error:(44, 34) object HiveShim is not a member of package
>>>>>>> > org.apache.spark.sql.hive
>>>>>>> > import org.apache.spark.sql.hive.HiveShim._
>>>>>>> >                                  ^
>>>>>>> > Error:(43, 8) object ShimFileSinkDesc is not a member of package
>>>>>>> > org.apache.spark.sql.hive
>>>>>>> > import org.apache.spark.sql.hive.{ ShimFileSinkDesc =>
>>>>>>> FileSinkDesc}
>>>>>>> >        ^
>>>>>>> > Error:(76, 21) not found: type FileSinkDesc
>>>>>>> >       fileSinkConf: FileSinkDesc,
>>>>>>> >                     ^
>>>>>>> > Error:(142, 23) not found: value HiveShim
>>>>>>> >     val tmpLocation = HiveShim.getExternalTmpPath(hiveContext,
>>>>>>> > tableLocation)
>>>>>>> >                       ^
>>>>>>> > Error:(143, 28) not found: type FileSinkDesc
>>>>>>> >     val fileSinkConf = new FileSinkDesc(tmpLocation.toString,
>>>>>>> tableDesc,
>>>>>>> > false)
>>>>>>> >                            ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala
>>>>>>> > Error:(141, 22) not found: value HiveShim
>>>>>>> >       val partPath = HiveShim.getDataLocationPath(partition)
>>>>>>> >                      ^
>>>>>>> > Error:(298, 33) not found: value HiveShim
>>>>>>> >             row.update(ordinal, HiveShim.toCatalystDecimal(oi,
>>>>>>> value))
>>>>>>> >                                 ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TestHive.scala
>>>>>>> > Error:(384, 3) not found: value HiveShim
>>>>>>> >   HiveShim.createDefaultDBIfNeeded(this)
>>>>>>> >   ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/DescribeHiveTableCommand.scala
>>>>>>> > Error:(29, 8) object HiveShim is not a member of package
>>>>>>> > org.apache.spark.sql.hive
>>>>>>> > import org.apache.spark.sql.hive.HiveShim
>>>>>>> >        ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScan.scala
>>>>>>> > Error:(89, 5) not found: value HiveShim
>>>>>>> >     HiveShim.appendReadColumns(hiveConf, neededColumnIDs,
>>>>>>> > attributes.map(_.name))
>>>>>>> >     ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala
>>>>>>> > Error:(38, 34) object HiveShim is not a member of package
>>>>>>> > org.apache.spark.sql.hive
>>>>>>> > import org.apache.spark.sql.hive.HiveShim._
>>>>>>> >                                  ^
>>>>>>> > Error:(37, 8) object ShimFileSinkDesc is not a member of package
>>>>>>> > org.apache.spark.sql.hive
>>>>>>> > import org.apache.spark.sql.hive.{ShimFileSinkDesc => FileSinkDesc}
>>>>>>> >        ^
>>>>>>> > Error:(174, 19) not found: type FileSinkDesc
>>>>>>> >     fileSinkConf: FileSinkDesc,
>>>>>>> >                   ^
>>>>>>> > Error:(46, 19) not found: type FileSinkDesc
>>>>>>> >     fileSinkConf: FileSinkDesc)
>>>>>>> >                   ^
>>>>>>> > Error:(220, 33) not found: type FileSinkDesc
>>>>>>> >       val newFileSinkDesc = new FileSinkDesc(
>>>>>>> >                                 ^
>>>>>>> >
>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala
>>>>>>> > Warning:(34, 2) @deprecated now takes two arguments; see the
>>>>>>> scaladoc.
>>>>>>> > @deprecated("No code should depend on FakeParquetHiveSerDe as it
>>>>>>> is only
>>>>>>> > intended as a " +
>>>>>>> >  ^
>>>>>>> >
>>>>>>> ------------------------------------------------------------------------
>>>>>>> >
>>>>>>> > *I thought it was the problem from Maven Profiles. So I tried
>>>>>>> reselecting
>>>>>>> > hbase-hadoop1 or hive or hbase-hadoop2. The error still occurs.
>>>>>>> Please help
>>>>>>> > me. This has annoyed me for a whole afternoon!*
>>>>>>> >
>>>>>>> >
>>>>>>> >
>>>>>>> >
>>>>>>> > -----
>>>>>>> > Feel the sparking Spark!
>>>>>>> > --
>>>>>>> > View this message in context:
>>>>>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-development-with-IntelliJ-tp10032p10163.html
>>>>>>> > Sent from the Apache Spark Developers List mailing list archive at
>>>>>>> Nabble.com.
>>>>>>> >
>>>>>>> >
>>>>>>> ---------------------------------------------------------------------
>>>>>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>> >
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c129980a3061050cdec4b8--

From dev-return-11174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 20:36:59 2015
Return-Path: <dev-return-11174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 144F417D00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 20:36:59 +0000 (UTC)
Received: (qmail 20394 invoked by uid 500); 17 Jan 2015 20:36:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20318 invoked by uid 500); 17 Jan 2015 20:36:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20307 invoked by uid 99); 17 Jan 2015 20:36:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:36:59 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of irashid@cloudera.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:36:55 +0000
Received: by mail-wi0-f170.google.com with SMTP id z2so10073154wiv.1
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 12:34:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CSiehhRFQHyPWEDycCrnNIa5tqOAMYXooYGq1QY3qCw=;
        b=A4I57ka7HhZcRK5hUu2Q0GOrpi2FbgU7q75oZoTC2vamY+PxPuZiIoZLTcuU83Xkc3
         PCmtZ079vxhlpNt2AAObligENqxH5r5FVqBXgfMJIGuRriD3ztedq30jyLozxxBwVkoL
         e4qsJmupLl9Dcw/s2zNh8RMOJX32jGoQyvL7mmdMbKxW1fIev8vC+NaqTJwRtDaNvSZ7
         zy89AtDryYT7HCZINgMG7txiMo8gQlfrvLw7Ul+tSARRwILjaLy7DIWIabDapaPZMvnN
         sVCkfWUatdipLthXCad6N4nhSsVgtagoq4kjKXauQsQjyQCpfi4kDdPTeQ0nJZMDnSjw
         sruw==
X-Gm-Message-State: ALoCoQm9+kvZQSTlCV6M0KO9JmlEjnik8OBe4iqfh6FGFjKNvWqBXe7RR1sEFBte8ixQ5B0dHBR8
X-Received: by 10.194.92.235 with SMTP id cp11mr41704327wjb.112.1421526858867;
 Sat, 17 Jan 2015 12:34:18 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.47.132 with HTTP; Sat, 17 Jan 2015 12:33:58 -0800 (PST)
In-Reply-To: <CAL34EqHOGsvrL9rSUT4FjvoX86gkjS5aWtRSNP6HnMLNc39bZA@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com>
 <1421490569611-10163.post@n3.nabble.com> <CAMAsSd+2Z=DzBQwcKY4TFrh+ubr-xUiHx8Haw+N5BdFkRRtLjQ@mail.gmail.com>
 <CA+3qhFTmvt1xcf7P-H--Tbw_gGp8hLDzZA_3ZsazsZtJ8W5GGQ@mail.gmail.com>
 <CAL34EqFxBXGCezNEXTYXAqLKnvtVM3Ft-ZhPj1oNmLO1iSjxTA@mail.gmail.com>
 <CA+3qhFSO9RyAUyyBZFaBrC3Rhs6xpjEmRcmq6BbUuVwFBnm46A@mail.gmail.com>
 <CAL34EqHrXEXi+Q+EOoGoW_+=h4q01eTZTdhfjJLXSs-=MAK95Q@mail.gmail.com>
 <CA+3qhFSTpTQKicKJLSThvu=p_ZDfUXJiXUce8HCcHKRRE=8GyA@mail.gmail.com>
 <CAL34EqFvZbdpDXyb+UPtFdt7cSc80EdzmE5m3UGkH7wayqOAsA@mail.gmail.com> <CAL34EqHOGsvrL9rSUT4FjvoX86gkjS5aWtRSNP6HnMLNc39bZA@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Sat, 17 Jan 2015 12:33:58 -0800
Message-ID: <CA+3qhFQTYtNzN9WzsGgbcmpNYtFShkwBr5qAJz0f_uxNm3kK0g@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
To: Chunnan Yao <yaochunnan@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd91ec276405a050cdf0350
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd91ec276405a050cdf0350
Content-Type: text/plain; charset=UTF-8

btw, I updated the wiki to include instructions on making the
macro-paradise jar a compiler plugin for Intellij:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ

On Sat, Jan 17, 2015 at 12:16 PM, Chunnan Yao <yaochunnan@gmail.com> wrote:

> Followed is the discussion between Imran and me.
>
>
> 2015-01-18 4:12 GMT+08:00 Chunnan Yao <yaochunnan@gmail.com>:
>
>> Thank you for your patience! Im now not so familiar with the mailing
>> list. I just clicked "reply" in Gmail, thinking it would be automatically
>> attached to the list. I will later post the missed information to the
>> spark-user list :) Your suggestions really helps!
>>
>> 2015-01-18 4:05 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>>
>>> ah, that is a very different question.  The point of using Intellij
>>> really is not to build the fully packaged binaries -- its really just for
>>> the features of the IDE while developing, eg. code navigation, debugger,
>>> etc.  Use either sbt or maven for building the packaged binaries.  (There
>>> probably is some way to get Intellij to build the packages, by calling
>>> maven, but I can't see much advantage in doing that.)
>>>
>>> The instructions for building are here:
>>>
>>>
>>> https://spark.apache.org/docs/latest/building-spark.html#building-with-sbt
>>>
>>> eg., for building with maven, you can do:
>>>
>>> export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
>>>
>>> mvn -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -DskipTests clean package
>>>
>>>
>>> or for sbt, you can do:
>>>
>>> sbt/sbt -Pyarn -Phadoop-2.3 assembly
>>>
>>>
>>>
>>> btw, these replies are only going to me, not the spark-user list, not
>>> sure if that was intentional?
>>>
>>> hope this helps,
>>> Imran
>>>
>>> On Sat, Jan 17, 2015 at 11:00 AM, Chunnan Yao <yaochunnan@gmail.com>
>>> wrote:
>>>
>>>> Thank you very much for your reply! But how can I generate deployable
>>>> spark binary package like those pre-built packages? I am new with Maven.
>>>>
>>>> 2015-01-18 1:44 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>>>>
>>>>> The build output location is set by the maven build which is
>>>>>
>>>>> <sub-project>/target/scala-<version>/[test-]classes/
>>>>>
>>>>> eg.
>>>>>
>>>>> core/target/scala-2.10/classes/
>>>>>
>>>>>
>>>>> On Sat, Jan 17, 2015 at 8:51 AM, Chunnan Yao <yaochunnan@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> I don't know if it's a naive question.  Although the fix  (moving
>>>>>> the paradise jar from "Additional compiler options" to "compiler
>>>>>> plugins") works fine (it has removed all the errors I was faced, but still
>>>>>> leaves 83 warnings), but I cannot find my compile results (which should be
>>>>>> in the /spark-1.2.0/out dictionary. What's the problem? The compiler has
>>>>>> told me the compilation completed successfully.
>>>>>>
>>>>>> 2015-01-17 23:28 GMT+08:00 Imran Rashid <irashid@cloudera.com>:
>>>>>>
>>>>>>> I experienced these errors in Intellij even without the hive mode
>>>>>>> enabled.  I think its also a question of which project you are trying t
>>>>>>> compile.  eg. core built fine, but I got these errors when I tried to build
>>>>>>> sql.  If the fix works for you (moving the paradise jar from "Additional
>>>>>>> compiler options" to "compiler plugins") then we should definitely put it
>>>>>>> on the wiki.
>>>>>>>
>>>>>>> writing macros is really painful with quasiquotes, so it is probably
>>>>>>> worth it ...
>>>>>>>
>>>>>>> On Sat, Jan 17, 2015 at 2:34 AM, Sean Owen <sowen@cloudera.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Yes I've seen that error in the past too, and was just talking to
>>>>>>>> Imran the other day about it. I thought it only occurred when the
>>>>>>>> hive
>>>>>>>> module was enabled, which I don't enable.
>>>>>>>>
>>>>>>>> The problem is that the plugin that causes an error in IntelliJ for
>>>>>>>> scalac is what parses these values.* I think he got it to work with
>>>>>>>> this change:
>>>>>>>> http://stackoverflow.com/questions/26788367/quasiquotes-in-intellij-14/26908554#26908554
>>>>>>>>
>>>>>>>> If that works for you let's put it on the wiki.
>>>>>>>>
>>>>>>>> * probably an ignorant question but is this feature important enough
>>>>>>>> to warrant the extra scala compiler plugin? the quasiquotes syntax I
>>>>>>>> mean.
>>>>>>>>
>>>>>>>> On Sat, Jan 17, 2015 at 10:29 AM, Chunnan Yao <yaochunnan@gmail.com>
>>>>>>>> wrote:
>>>>>>>> > *I followed the procedures instructed by
>>>>>>>> >
>>>>>>>> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ
>>>>>>>> .
>>>>>>>> > But problems still occurs which has made me a little bit annoyed.
>>>>>>>> >
>>>>>>>> > My environment settings are:JAVA 1.7.0 Scala: 2.10.4 Spark:1.2.0,
>>>>>>>> Intellij
>>>>>>>> > Idea 14.0.2, Ubuntu 14.04
>>>>>>>> >
>>>>>>>> > Firstly I got the scala plugin correctly installed.
>>>>>>>> >
>>>>>>>> > I choosed maven-3, hadoop-2.4, scala-2.10 as my profiles when
>>>>>>>> importing the
>>>>>>>> > project.
>>>>>>>> >
>>>>>>>> > After importing, I first turned on "View-Tool Windows-Maven
>>>>>>>> Projects". I see
>>>>>>>> > the "hbase-hadoop1" is selected, but I had not chosen it in the
>>>>>>>> import
>>>>>>>> > process. So I deselected it to leave the hadoop-2.4, maven-3,
>>>>>>>> scala-2.10 to
>>>>>>>> > be the only three selected items in "Maven Projects-Profiles".
>>>>>>>> >
>>>>>>>> > According to the Wiki, the next step should be "Generate Sources
>>>>>>>> and Update
>>>>>>>> > Folders For All Projects". I did so, and waited for some minutes
>>>>>>>> to get the
>>>>>>>> > sub-projects prepared.
>>>>>>>> >
>>>>>>>> > Then I cleared the "Additional compiler options" in the
>>>>>>>> > "File-Settings-Build, Execution, Deployment-Compiler-Scala
>>>>>>>> Compiler".
>>>>>>>> >
>>>>>>>> > Finally I choosed Build-reBuild project.
>>>>>>>> >
>>>>>>>> > However, the compiler failed with "value q is not a member of
>>>>>>>> stringcontext"
>>>>>>>> > errors. *
>>>>>>>> >
>>>>>>>> > (partial screen shot)
>>>>>>>> > -------------------------------------------------------
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateProjection.scala
>>>>>>>> > Error:(42, 21) value q is not a member of StringContext
>>>>>>>> >     val lengthDef = q"final val length = $tupleLength"
>>>>>>>> >                     ^
>>>>>>>> > Error:(54, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(66, 9) value q is not a member of StringContext
>>>>>>>> >         q"""
>>>>>>>> >         ^
>>>>>>>> > Error:(83, 9) value q is not a member of StringContext
>>>>>>>> >         q"if(isNullAt($iLit)) { null } else {
>>>>>>>> ${newTermName(s"c$i")} }"
>>>>>>>> >         ^
>>>>>>>> > Error:(85, 7) value q is not a member of StringContext
>>>>>>>> >       q"override def iterator = Iterator[Any](..$allColumns)"
>>>>>>>> >       ^
>>>>>>>> > Error:(88, 27) value q is not a member of StringContext
>>>>>>>> >     val accessorFailure = q"""scala.sys.error("Invalid ordinal:"
>>>>>>>> + i)"""
>>>>>>>> >                           ^
>>>>>>>> > Error:(95, 9) value q is not a member of StringContext
>>>>>>>> >         q"if(i == $ordinal) { if(isNullAt($i)) return null else
>>>>>>>> return
>>>>>>>> > $elementName }"
>>>>>>>> >         ^
>>>>>>>> > Error:(97, 7) value q is not a member of StringContext
>>>>>>>> >       q"override def apply(i: Int): Any = { ..$cases;
>>>>>>>> $accessorFailure }"
>>>>>>>> >       ^
>>>>>>>> > Error:(106, 9) value q is not a member of StringContext
>>>>>>>> >         q"""
>>>>>>>> >         ^
>>>>>>>> > Error:(117, 7) value q is not a member of StringContext
>>>>>>>> >       q"override def update(i: Int, value: Any): Unit = {
>>>>>>>> ..$cases;
>>>>>>>> > $accessorFailure }"
>>>>>>>> >       ^
>>>>>>>> > Error:(126, 11) value q is not a member of StringContext
>>>>>>>> >           q"if(i == $i) return $elementName" :: Nil
>>>>>>>> >           ^
>>>>>>>> > Error:(130, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(143, 11) value q is not a member of StringContext
>>>>>>>> >           q"if(i == $i) { nullBits($i) = false; $elementName =
>>>>>>>> value; return
>>>>>>>> > }" :: Nil
>>>>>>>> >           ^
>>>>>>>> > Error:(147, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(157, 29) value q is not a member of StringContext
>>>>>>>> >         case BooleanType => q"if ($elementName) 0 else 1"
>>>>>>>> >                             ^
>>>>>>>> > Error:(158, 52) value q is not a member of StringContext
>>>>>>>> >         case ByteType | ShortType | IntegerType =>
>>>>>>>> q"$elementName.toInt"
>>>>>>>> >                                                    ^
>>>>>>>> > Error:(159, 26) value q is not a member of StringContext
>>>>>>>> >         case LongType => q"($elementName ^ ($elementName >>>
>>>>>>>> 32)).toInt"
>>>>>>>> >                          ^
>>>>>>>> > Error:(160, 27) value q is not a member of StringContext
>>>>>>>> >         case FloatType =>
>>>>>>>> q"java.lang.Float.floatToIntBits($elementName)"
>>>>>>>> >                           ^
>>>>>>>> > Error:(162, 11) value q is not a member of StringContext
>>>>>>>> >           q"{ val b =
>>>>>>>> java.lang.Double.doubleToLongBits($elementName); (b ^
>>>>>>>> > (b >>>32)).toInt }"
>>>>>>>> >           ^
>>>>>>>> > Error:(163, 19) value q is not a member of StringContext
>>>>>>>> >         case _ => q"$elementName.hashCode"
>>>>>>>> >                   ^
>>>>>>>> > Error:(165, 7) value q is not a member of StringContext
>>>>>>>> >       q"if (isNullAt($i)) 0 else $nonNull"
>>>>>>>> >       ^
>>>>>>>> > Error:(168, 54) value q is not a member of StringContext
>>>>>>>> >     val hashUpdates: Seq[Tree] = hashValues.map(v => q"""result =
>>>>>>>> 37 *
>>>>>>>> > result + $v""": Tree)
>>>>>>>> >                                                      ^
>>>>>>>> > Error:(171, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(181, 7) value q is not a member of StringContext
>>>>>>>> >       q"if (this.$elementName != specificType.$elementName)
>>>>>>>> return false"
>>>>>>>> >       ^
>>>>>>>> > Error:(185, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(195, 7) value q is not a member of StringContext
>>>>>>>> >       q"""
>>>>>>>> >       ^
>>>>>>>> > Error:(210, 16) value q is not a member of StringContext
>>>>>>>> >     val code = q"""
>>>>>>>> >                ^
>>>>>>>> >
>>>>>>>> ---------------------------------------------------------------------
>>>>>>>> > *
>>>>>>>> > Then I tried
>>>>>>>> >
>>>>>>>> http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea
>>>>>>>> ,
>>>>>>>> > which does not clear the "Additional compiler options", but
>>>>>>>> change the -P in
>>>>>>>> > to -Xplugin.
>>>>>>>> > So now my "Additional Compiler Options" is like this
>>>>>>>> >
>>>>>>>> "-Xplugin:/home/yaochunnan/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar"
>>>>>>>> >
>>>>>>>> > Then I reBuild again with the following errors: *
>>>>>>>> > (partial screen shot)
>>>>>>>> >
>>>>>>>> ----------------------------------------------------------------------
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
>>>>>>>> > Error:(169, 38) not found: value HiveShim
>>>>>>>> >
>>>>>>>>  Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
>>>>>>>> >                                      ^
>>>>>>>> > Error:(177, 31) not found: value HiveShim
>>>>>>>> >
>>>>>>>>  tableParameters.put(HiveShim.getStatsSetupConstTotalSize,
>>>>>>>> > newTotalSize.toString)
>>>>>>>> >                               ^
>>>>>>>> > Error:(292, 36) not found: value HiveShim
>>>>>>>> >       val proc: CommandProcessor =
>>>>>>>> > HiveShim.getCommandProcessor(Array(tokens(0)), hiveconf)
>>>>>>>> >                                    ^
>>>>>>>> > Error:(304, 25) not found: value HiveShim
>>>>>>>> >           val results = HiveShim.createDriverResultsArray
>>>>>>>> >                         ^
>>>>>>>> > Error:(314, 11) not found: value HiveShim
>>>>>>>> >           HiveShim.processResults(results)
>>>>>>>> >           ^
>>>>>>>> > Error:(418, 7) not found: value HiveShim
>>>>>>>> >
>>>>>>>>  HiveShim.createDecimal(decimal.toBigDecimal.underlying()).toString
>>>>>>>> >       ^
>>>>>>>> > Error:(420, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.createDecimal(decimal.underlying()).toString
>>>>>>>> >       ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala
>>>>>>>> > Error:(97, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.toCatalystDecimal(
>>>>>>>> >       ^
>>>>>>>> > Error:(123, 46) not found: value HiveShim
>>>>>>>> >     case hdoi: HiveDecimalObjectInspector =>
>>>>>>>> > HiveShim.toCatalystDecimal(hdoi, data)
>>>>>>>> >                                              ^
>>>>>>>> > Error:(156, 19) not found: value HiveShim
>>>>>>>> >       (o: Any) =>
>>>>>>>> >
>>>>>>>> HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())
>>>>>>>> >                   ^
>>>>>>>> > Error:(210, 31) not found: value HiveShim
>>>>>>>> >         case b: BigDecimal =>
>>>>>>>> HiveShim.createDecimal(b.underlying())
>>>>>>>> >                               ^
>>>>>>>> > Error:(211, 28) not found: value HiveShim
>>>>>>>> >         case d: Decimal =>
>>>>>>>> > HiveShim.createDecimal(d.toBigDecimal.underlying())
>>>>>>>> >                            ^
>>>>>>>> > Error:(283, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getStringWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(285, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getIntWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(287, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getDoubleWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(289, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getBooleanWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(291, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getLongWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(293, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getFloatWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(295, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getShortWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(297, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getByteWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(299, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getBinaryWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(301, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getDateWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(303, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getTimestampWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(305, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getDecimalWritableConstantObjectInspector(value)
>>>>>>>> >       ^
>>>>>>>> > Error:(307, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.getPrimitiveNullWritableConstantObjectInspector
>>>>>>>> >       ^
>>>>>>>> > Error:(363, 51) not found: value HiveShim
>>>>>>>> >     case w: WritableHiveDecimalObjectInspector =>
>>>>>>>> > HiveShim.decimalTypeInfoToCatalyst(w)
>>>>>>>> >                                                   ^
>>>>>>>> > Error:(364, 47) not found: value HiveShim
>>>>>>>> >     case j: JavaHiveDecimalObjectInspector =>
>>>>>>>> > HiveShim.decimalTypeInfoToCatalyst(j)
>>>>>>>> >                                               ^
>>>>>>>> > Error:(393, 30) not found: value HiveShim
>>>>>>>> >       case d: DecimalType => HiveShim.decimalTypeInfo(d)
>>>>>>>> >                              ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
>>>>>>>> > Error:(78, 11) not found: value HiveShim
>>>>>>>> >           HiveShim.getAllPartitionsOf(client, table).toSeq
>>>>>>>> >           ^
>>>>>>>> > Error:(205, 7) not found: value HiveShim
>>>>>>>> >       HiveShim.setLocation(tbl, crtTbl)
>>>>>>>> >       ^
>>>>>>>> > Error:(443, 28) not found: value HiveShim
>>>>>>>> >     case d: DecimalType => HiveShim.decimalMetastoreString(d)
>>>>>>>> >                            ^
>>>>>>>> > Error:(472, 53) not found: value HiveShim
>>>>>>>> >       val totalSize =
>>>>>>>> >
>>>>>>>> hiveQlTable.getParameters.get(HiveShim.getStatsSetupConstTotalSize)
>>>>>>>> >                                                     ^
>>>>>>>> > Error:(490, 19) not found: value HiveShim
>>>>>>>> >   val tableDesc = HiveShim.getTableDesc(
>>>>>>>> >                   ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
>>>>>>>> > Error:(255, 18) not found: type HiveFunctionWrapper
>>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>>> >                  ^
>>>>>>>> > Error:(73, 53) not found: type HiveFunctionWrapper
>>>>>>>> > private[hive] case class HiveSimpleUdf(funcWrapper:
>>>>>>>> HiveFunctionWrapper,
>>>>>>>> > children: Seq[Expression])
>>>>>>>> >                                                     ^
>>>>>>>> > Error:(57, 25) not found: type HiveFunctionWrapper
>>>>>>>> >       HiveSimpleUdf(new HiveFunctionWrapper(functionClassName),
>>>>>>>> children)
>>>>>>>> >                         ^
>>>>>>>> > Error:(134, 54) not found: type HiveFunctionWrapper
>>>>>>>> > private[hive] case class HiveGenericUdf(funcWrapper:
>>>>>>>> HiveFunctionWrapper,
>>>>>>>> > children: Seq[Expression])
>>>>>>>> >                                                      ^
>>>>>>>> > Error:(59, 26) not found: type HiveFunctionWrapper
>>>>>>>> >       HiveGenericUdf(new HiveFunctionWrapper(functionClassName),
>>>>>>>> children)
>>>>>>>> >                          ^
>>>>>>>> > Error:(185, 18) not found: type HiveFunctionWrapper
>>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>>> >                  ^
>>>>>>>> > Error:(62, 27) not found: type HiveFunctionWrapper
>>>>>>>> >       HiveGenericUdaf(new HiveFunctionWrapper(functionClassName),
>>>>>>>> children)
>>>>>>>> >                           ^
>>>>>>>> > Error:(214, 18) not found: type HiveFunctionWrapper
>>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>>> >                  ^
>>>>>>>> > Error:(64, 20) not found: type HiveFunctionWrapper
>>>>>>>> >       HiveUdaf(new HiveFunctionWrapper(functionClassName),
>>>>>>>> children)
>>>>>>>> >                    ^
>>>>>>>> > Error:(66, 27) not found: type HiveFunctionWrapper
>>>>>>>> >       HiveGenericUdtf(new HiveFunctionWrapper(functionClassName),
>>>>>>>> Nil,
>>>>>>>> > children)
>>>>>>>> >                           ^
>>>>>>>> > Error:(322, 18) not found: type HiveFunctionWrapper
>>>>>>>> >     funcWrapper: HiveFunctionWrapper,
>>>>>>>> >                  ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
>>>>>>>> > Error:(1132, 15) not found: type HiveFunctionWrapper
>>>>>>>> >           new HiveFunctionWrapper(functionName),
>>>>>>>> >               ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala
>>>>>>>> > Error:(44, 34) object HiveShim is not a member of package
>>>>>>>> > org.apache.spark.sql.hive
>>>>>>>> > import org.apache.spark.sql.hive.HiveShim._
>>>>>>>> >                                  ^
>>>>>>>> > Error:(43, 8) object ShimFileSinkDesc is not a member of package
>>>>>>>> > org.apache.spark.sql.hive
>>>>>>>> > import org.apache.spark.sql.hive.{ ShimFileSinkDesc =>
>>>>>>>> FileSinkDesc}
>>>>>>>> >        ^
>>>>>>>> > Error:(76, 21) not found: type FileSinkDesc
>>>>>>>> >       fileSinkConf: FileSinkDesc,
>>>>>>>> >                     ^
>>>>>>>> > Error:(142, 23) not found: value HiveShim
>>>>>>>> >     val tmpLocation = HiveShim.getExternalTmpPath(hiveContext,
>>>>>>>> > tableLocation)
>>>>>>>> >                       ^
>>>>>>>> > Error:(143, 28) not found: type FileSinkDesc
>>>>>>>> >     val fileSinkConf = new FileSinkDesc(tmpLocation.toString,
>>>>>>>> tableDesc,
>>>>>>>> > false)
>>>>>>>> >                            ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala
>>>>>>>> > Error:(141, 22) not found: value HiveShim
>>>>>>>> >       val partPath = HiveShim.getDataLocationPath(partition)
>>>>>>>> >                      ^
>>>>>>>> > Error:(298, 33) not found: value HiveShim
>>>>>>>> >             row.update(ordinal, HiveShim.toCatalystDecimal(oi,
>>>>>>>> value))
>>>>>>>> >                                 ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TestHive.scala
>>>>>>>> > Error:(384, 3) not found: value HiveShim
>>>>>>>> >   HiveShim.createDefaultDBIfNeeded(this)
>>>>>>>> >   ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/DescribeHiveTableCommand.scala
>>>>>>>> > Error:(29, 8) object HiveShim is not a member of package
>>>>>>>> > org.apache.spark.sql.hive
>>>>>>>> > import org.apache.spark.sql.hive.HiveShim
>>>>>>>> >        ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScan.scala
>>>>>>>> > Error:(89, 5) not found: value HiveShim
>>>>>>>> >     HiveShim.appendReadColumns(hiveConf, neededColumnIDs,
>>>>>>>> > attributes.map(_.name))
>>>>>>>> >     ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala
>>>>>>>> > Error:(38, 34) object HiveShim is not a member of package
>>>>>>>> > org.apache.spark.sql.hive
>>>>>>>> > import org.apache.spark.sql.hive.HiveShim._
>>>>>>>> >                                  ^
>>>>>>>> > Error:(37, 8) object ShimFileSinkDesc is not a member of package
>>>>>>>> > org.apache.spark.sql.hive
>>>>>>>> > import org.apache.spark.sql.hive.{ShimFileSinkDesc =>
>>>>>>>> FileSinkDesc}
>>>>>>>> >        ^
>>>>>>>> > Error:(174, 19) not found: type FileSinkDesc
>>>>>>>> >     fileSinkConf: FileSinkDesc,
>>>>>>>> >                   ^
>>>>>>>> > Error:(46, 19) not found: type FileSinkDesc
>>>>>>>> >     fileSinkConf: FileSinkDesc)
>>>>>>>> >                   ^
>>>>>>>> > Error:(220, 33) not found: type FileSinkDesc
>>>>>>>> >       val newFileSinkDesc = new FileSinkDesc(
>>>>>>>> >                                 ^
>>>>>>>> >
>>>>>>>> /home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala
>>>>>>>> > Warning:(34, 2) @deprecated now takes two arguments; see the
>>>>>>>> scaladoc.
>>>>>>>> > @deprecated("No code should depend on FakeParquetHiveSerDe as it
>>>>>>>> is only
>>>>>>>> > intended as a " +
>>>>>>>> >  ^
>>>>>>>> >
>>>>>>>> ------------------------------------------------------------------------
>>>>>>>> >
>>>>>>>> > *I thought it was the problem from Maven Profiles. So I tried
>>>>>>>> reselecting
>>>>>>>> > hbase-hadoop1 or hive or hbase-hadoop2. The error still occurs.
>>>>>>>> Please help
>>>>>>>> > me. This has annoyed me for a whole afternoon!*
>>>>>>>> >
>>>>>>>> >
>>>>>>>> >
>>>>>>>> >
>>>>>>>> > -----
>>>>>>>> > Feel the sparking Spark!
>>>>>>>> > --
>>>>>>>> > View this message in context:
>>>>>>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-development-with-IntelliJ-tp10032p10163.html
>>>>>>>> > Sent from the Apache Spark Developers List mailing list archive
>>>>>>>> at Nabble.com.
>>>>>>>> >
>>>>>>>> >
>>>>>>>> ---------------------------------------------------------------------
>>>>>>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>> >
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7bd91ec276405a050cdf0350--

From dev-return-11175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 20:45:07 2015
Return-Path: <dev-return-11175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5BF0C17D1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 20:45:07 +0000 (UTC)
Received: (qmail 28830 invoked by uid 500); 17 Jan 2015 20:45:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28755 invoked by uid 500); 17 Jan 2015 20:45:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28737 invoked by uid 99); 17 Jan 2015 20:45:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:45:02 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of yaochunnan@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 20:44:57 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id E951810DA10A
	for <dev@spark.apache.org>; Sat, 17 Jan 2015 12:44:35 -0800 (PST)
Date: Sat, 17 Jan 2015 13:44:35 -0700 (MST)
From: Chunnan Yao <yaochunnan@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421527475282-10167.post@n3.nabble.com>
In-Reply-To: <CA+3qhFQTYtNzN9WzsGgbcmpNYtFShkwBr5qAJz0f_uxNm3kK0g@mail.gmail.com>
References: <3e2b.3c1oD.1eXJJnmoJFQ.1KhbKe@seznam.cz> <CAMAsSdKPW9p9N95BZ4Wrg7F-quAyuL2a62-vckpMi5wzDr=iaw@mail.gmail.com> <1421490569611-10163.post@n3.nabble.com> <CAMAsSd+2Z=DzBQwcKY4TFrh+ubr-xUiHx8Haw+N5BdFkRRtLjQ@mail.gmail.com> <CAL34EqHOGsvrL9rSUT4FjvoX86gkjS5aWtRSNP6HnMLNc39bZA@mail.gmail.com> <CA+3qhFQTYtNzN9WzsGgbcmpNYtFShkwBr5qAJz0f_uxNm3kK0g@mail.gmail.com>
Subject: Re: Spark development with IntelliJ
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Nice! 



-----
Feel the sparking Spark!
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-development-with-IntelliJ-tp10032p10167.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 17 23:58:46 2015
Return-Path: <dev-return-11176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 66D4010061
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 Jan 2015 23:58:46 +0000 (UTC)
Received: (qmail 27532 invoked by uid 500); 17 Jan 2015 23:58:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27472 invoked by uid 500); 17 Jan 2015 23:58:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26279 invoked by uid 99); 17 Jan 2015 23:58:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 23:58:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 Jan 2015 23:58:38 +0000
Received: by mail-oi0-f43.google.com with SMTP id i138so22289023oig.2;
        Sat, 17 Jan 2015 15:58:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=97cMuAgEWwHynBmrxRMzstgHEHoLYJrCRXjdFAwkZBA=;
        b=VS9o49deZtOkPy5+3PxejW4yGRL9t1vdhYmfZNYLAyMMgVRnvk5gWQPa5OjrYYUGv2
         2IoRTp8X5oX5UzlOX30qXKpcQf/7Q9a1OjoOjaVVgkY5xX+LyMF+Sm/z3DnCUZ14qJBc
         2YCRclr4WbGa8TDuXqSqoDKJIAQwyxymrAAoF4tFdNXIYWHtaBBE2l9/q3XToGOfSMmJ
         ehcBP2/SJke+5ntv2C9m2Z7fsOQQIkd6QHsX7t6MKaiPggOGbSv1ZR5Jnm1OCJ2+5CO4
         UX/wmHWXxuFWt9d5iO2V6jQSgu82AaTFrsm3t/hMX93ZL8YkYFNs4EZoeGig2MfzkvDd
         3OZw==
MIME-Version: 1.0
X-Received: by 10.60.47.112 with SMTP id c16mr8103785oen.83.1421539097650;
 Sat, 17 Jan 2015 15:58:17 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Sat, 17 Jan 2015 15:58:17 -0800 (PST)
In-Reply-To: <CAHUQ+_YaOh867Pt3_f+6a9X+OdPq0kgGJR=-NfZ=Egq92nP2Sg@mail.gmail.com>
References: <CAHUQ+_YaOh867Pt3_f+6a9X+OdPq0kgGJR=-NfZ=Egq92nP2Sg@mail.gmail.com>
Date: Sat, 17 Jan 2015 15:58:17 -0800
Message-ID: <CABPQxssfRNmVJZy=6U3_aBYjyMHw-SJfktbOMmRSu3wdjoAYKw@mail.gmail.com>
Subject: Re: Bouncing Mails
From: Patrick Wendell <pwendell@gmail.com>
To: Akhil Das <akhil@sigmoidanalytics.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Akhil,

Those are handled by ASF infrastructure, not anyone in the Spark
project. So this list is not the appropriate place to ask for help.

- Patrick

On Sat, Jan 17, 2015 at 12:56 AM, Akhil Das <akhil@sigmoidanalytics.com> wrote:
> My mails to the mailing list are getting rejected, have opened a Jira issue,
> can someone take a look at it?
>
> https://issues.apache.org/jira/browse/INFRA-9032
>
>
>
>
>
>
> Thanks
> Best Regards

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 01:41:05 2015
Return-Path: <dev-return-11177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3ACA01024C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 01:41:05 +0000 (UTC)
Received: (qmail 7734 invoked by uid 500); 18 Jan 2015 01:41:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7656 invoked by uid 500); 18 Jan 2015 01:41:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7644 invoked by uid 99); 18 Jan 2015 01:41:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 01:41:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 01:41:00 +0000
Received: by mail-ob0-f181.google.com with SMTP id gq1so24399835obb.12
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 17:40:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=5AMMFnHkjX9Es9efhBdbJDQMxlWZFBULUJau66a939I=;
        b=YUaL0wWVZuOsfAplpISO+f+JPcn1qaBR3RPQKRMvX+bz1r6jNuy4g6bRg8jBQC27G/
         z/pXGMaeNJDN9lCzmbCpksz154Mhu3HAhMwKwwRZvMU+LWFFCn39YM49cGa9Pmlh3eK6
         RCLPVo6WUmQyQFIA+I8gEArng2q2LdG9AHNfgMqktVl+ruldm4HUXzYGMg/pKOgEUtVE
         YmxTWWhF/MXw0nsY8jAnfZw0wn4mHHGPSrExzb+kArVhBTxB8SaKYS1oF43xZPziSqm2
         pnlHOc237LhJZhwvR7mc9lOTlTWcZMT4Ra3NVOwtr8NLL4TWnIH4kGy8rw//Q75X4JEw
         aFUg==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr13223223oie.78.1421545239672;
 Sat, 17 Jan 2015 17:40:39 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Sat, 17 Jan 2015 17:40:39 -0800 (PST)
Date: Sat, 17 Jan 2015 17:40:39 -0800
Message-ID: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
Subject: Semantics of LGTM
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Just wanted to ping about a minor issue - but one that ends up having
consequence given Spark's volume of reviews and commits. As much as
possible, I think that we should try and gear towards "Google Style"
LGTM on reviews. What I mean by this is that LGTM has the following
semantics:

"I know this code well, or I've looked at it close enough to feel
confident it should be merged. If there are issues/bugs with this code
later on, I feel confident I can help with them."

Here is an alternative semantic:

"Based on what I know about this part of the code, I don't see any
show-stopper problems with this patch".

The issue with the latter is that it ultimately erodes the
significance of LGTM, since subsequent reviewers need to reason about
what the person meant by saying LGTM. In contrast, having strong
semantics around LGTM can help streamline reviews a lot, especially as
reviewers get more experienced and gain trust from the comittership.

There are several easy ways to give a more limited endorsement of a patch:
- "I'm not familiar with this code, but style, etc look good" (general
endorsement)
- "The build changes in this code LGTM, but I haven't reviewed the
rest" (limited LGTM)

If people are okay with this, I might add a short note on the wiki.
I'm sending this e-mail first, though, to see whether anyone wants to
express agreement or disagreement with this approach.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 02:17:11 2015
Return-Path: <dev-return-11178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43A1E102BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 02:17:11 +0000 (UTC)
Received: (qmail 30957 invoked by uid 500); 18 Jan 2015 02:17:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30889 invoked by uid 500); 18 Jan 2015 02:17:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30877 invoked by uid 99); 18 Jan 2015 02:17:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 02:17:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 02:17:04 +0000
Received: by mail-ig0-f171.google.com with SMTP id h15so817490igd.4
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 18:16:23 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=gPl2Nr+5yvKVNLZVnWhEEaTOMNsO+EVrBuzFaZJTCE0=;
        b=QzZ5vyBgc4Q/gnJw8Rnna3u9GiBpbT9XCqrnRgi4vwLH8r0jatKMYCfCHP4yPAmuJa
         +OVQPry/MrERy+Bw1Rw9FsTgyZTMCj0DyDF0orA+VpwOls3O7+RS4uHFAUqclNaBnT+B
         GpYLK00ZR84Aspw5+fm+QiiTzjWgn3MR6NpYuQWeDDIAXu4rDehid69l1EY3H2eU+g8X
         FzQtC3D/KYLCqmSl3Y/pxqcGJGT6gEp/YHdPZJIV2HH+ATtQpzcSGr6vTjWBU9yPHxnR
         TGi3Z09XjCVdPyLGRDUNisZuUFn+xNe+xOEUwi68DKgPJ+EOhQnYbFMu/s0U3F4TII64
         9jTA==
X-Gm-Message-State: ALoCoQlAx17uLPOxXuZZE817ACJjmvU5QiipxfNXX32IrXtdHcFzdgcfvc6mUxCK41DOhlS3e6Ah
MIME-Version: 1.0
X-Received: by 10.43.6.71 with SMTP id oj7mr3345531icb.87.1421547383323; Sat,
 17 Jan 2015 18:16:23 -0800 (PST)
Received: by 10.107.14.208 with HTTP; Sat, 17 Jan 2015 18:16:23 -0800 (PST)
In-Reply-To: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
Date: Sat, 17 Jan 2015 18:16:23 -0800
Message-ID: <CAHuE29YtHofn==_TczCvu3dopmajr-mjHo8Zkk3QJZ5Y7q3E6g@mail.gmail.com>
Subject: Re: Semantics of LGTM
From: Reza Zadeh <reza@databricks.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec5158d91d09425050ce3cacd
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5158d91d09425050ce3cacd
Content-Type: text/plain; charset=UTF-8

LGTM

On Sat, Jan 17, 2015 at 5:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> Just wanted to ping about a minor issue - but one that ends up having
> consequence given Spark's volume of reviews and commits. As much as
> possible, I think that we should try and gear towards "Google Style"
> LGTM on reviews. What I mean by this is that LGTM has the following
> semantics:
>
> "I know this code well, or I've looked at it close enough to feel
> confident it should be merged. If there are issues/bugs with this code
> later on, I feel confident I can help with them."
>
> Here is an alternative semantic:
>
> "Based on what I know about this part of the code, I don't see any
> show-stopper problems with this patch".
>
> The issue with the latter is that it ultimately erodes the
> significance of LGTM, since subsequent reviewers need to reason about
> what the person meant by saying LGTM. In contrast, having strong
> semantics around LGTM can help streamline reviews a lot, especially as
> reviewers get more experienced and gain trust from the comittership.
>
> There are several easy ways to give a more limited endorsement of a patch:
> - "I'm not familiar with this code, but style, etc look good" (general
> endorsement)
> - "The build changes in this code LGTM, but I haven't reviewed the
> rest" (limited LGTM)
>
> If people are okay with this, I might add a short note on the wiki.
> I'm sending this e-mail first, though, to see whether anyone wants to
> express agreement or disagreement with this approach.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--bcaec5158d91d09425050ce3cacd--

From dev-return-11179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 02:19:58 2015
Return-Path: <dev-return-11179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B2A3102C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 02:19:58 +0000 (UTC)
Received: (qmail 32836 invoked by uid 500); 18 Jan 2015 02:19:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32761 invoked by uid 500); 18 Jan 2015 02:19:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32749 invoked by uid 99); 18 Jan 2015 02:19:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 02:19:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 02:19:32 +0000
Received: by mail-pa0-f51.google.com with SMTP id ey11so31667709pad.10
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 18:19:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=zX6tX7O2Mg2HMxoeNuGRDhowRBx9eyQeZaQyqg+ZlAs=;
        b=VNUfeuB6wbfNzybr8CCKRu35tWQP3RHqsmTnEMKtdsb7zLYu84WQBMTfDgs67n4T7J
         +kT8LibGZGDJnqOY3AHBzFwnaw1mJpQI2+H6bx7RJflUb3bJkNSlfyIGwGeNpaXRsfRR
         kEqqIeFnPzOHJhNjMzie/ipgQAnWhdCWp07VMgcZ+XM7Rp29HNLX4qtiwFdlN/C6XOPB
         fHKMxkj5Bxcpxwxl8f6pxnRvRw4udaXtN12W0WDBeXt2QdWrpQG/jZeQiwz8NWX/bjUv
         +kJyBdo0xAGDysfuZv9D6DkOBW+xchjFX0eqC9NRaKzxdrSc0MxQthzr2EUmoc4Z/t8v
         L8fw==
X-Received: by 10.70.95.232 with SMTP id dn8mr15850267pdb.142.1421547570697;
        Sat, 17 Jan 2015 18:19:30 -0800 (PST)
Received: from ip-172-27-235-159.us-west-2.compute.internal (ec2-54-201-102-27.us-west-2.compute.amazonaws.com. [54.201.102.27])
        by mx.google.com with ESMTPSA id ak5sm7855377pad.44.2015.01.17.18.19.29
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 Jan 2015 18:19:30 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: Semantics of LGTM
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAHuE29YtHofn==_TczCvu3dopmajr-mjHo8Zkk3QJZ5Y7q3E6g@mail.gmail.com>
Date: Sat, 17 Jan 2015 18:19:28 -0800
Cc: Patrick Wendell <pwendell@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <4204D8C6-16B0-4778-A86F-EE61BBC6EDEB@gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com> <CAHuE29YtHofn==_TczCvu3dopmajr-mjHo8Zkk3QJZ5Y7q3E6g@mail.gmail.com>
To: Reza Zadeh <reza@databricks.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

+1 on this.

> On Jan 17, 2015, at 6:16 PM, Reza Zadeh <reza@databricks.com> wrote:
>=20
> LGTM
>=20
> On Sat, Jan 17, 2015 at 5:40 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>=20
>> Hey All,
>>=20
>> Just wanted to ping about a minor issue - but one that ends up having
>> consequence given Spark's volume of reviews and commits. As much as
>> possible, I think that we should try and gear towards "Google Style"
>> LGTM on reviews. What I mean by this is that LGTM has the following
>> semantics:
>>=20
>> "I know this code well, or I've looked at it close enough to feel
>> confident it should be merged. If there are issues/bugs with this =
code
>> later on, I feel confident I can help with them."
>>=20
>> Here is an alternative semantic:
>>=20
>> "Based on what I know about this part of the code, I don't see any
>> show-stopper problems with this patch".
>>=20
>> The issue with the latter is that it ultimately erodes the
>> significance of LGTM, since subsequent reviewers need to reason about
>> what the person meant by saying LGTM. In contrast, having strong
>> semantics around LGTM can help streamline reviews a lot, especially =
as
>> reviewers get more experienced and gain trust from the comittership.
>>=20
>> There are several easy ways to give a more limited endorsement of a =
patch:
>> - "I'm not familiar with this code, but style, etc look good" =
(general
>> endorsement)
>> - "The build changes in this code LGTM, but I haven't reviewed the
>> rest" (limited LGTM)
>>=20
>> If people are okay with this, I might add a short note on the wiki.
>> I'm sending this e-mail first, though, to see whether anyone wants to
>> express agreement or disagreement with this approach.
>>=20
>> - Patrick
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 03:01:40 2015
Return-Path: <dev-return-11180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4513A1031C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 03:01:40 +0000 (UTC)
Received: (qmail 59105 invoked by uid 500); 18 Jan 2015 03:01:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59038 invoked by uid 500); 18 Jan 2015 03:01:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59027 invoked by uid 99); 18 Jan 2015 03:01:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:01:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:01:13 +0000
Received: by mail-pd0-f176.google.com with SMTP id r10so29897831pdi.7
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 18:59:41 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:content-transfer-encoding:message-id:references
         :to;
        bh=4UtAVHpjocph7kKXKC+V0O2xDHDP6UygHigmBskDJwQ=;
        b=JGzmZIiKxc1ya/RPfujbvp3Uy+WhCsILRyb6Lpd6cn11tvslyQrG7GEThbsptYvMGZ
         MHTWSQpfe453sALyHfD28n5839D4ulwTHbjMYJkthH816bPquJa4XanepDHnfnqEViZ9
         kW3jy3ozbRVqE7hFW2QJHXixN11TnZObe0gZW/zTyWi3CoTEoPMZImuBjvKar/IvCORv
         EX9RoUleV6P9BpABRdmvHeXv68RY7z9aFtMx8y3fexe6sJH8CMr6pPjMIAJQKhw+GLjS
         4aqOMR6CpP64np36RvetUuerEaH9PrCyngKUmD+54o8YNp5X6qUo3wgDEsMdyb0Zf7yd
         CMuQ==
X-Gm-Message-State: ALoCoQnSn+aJ0mBlZ15A9eJ+UBUueKNTWrc9bLaun8BJOu2KEAi/8D3VdOMNLP2qQ/2PRApf2zA4
X-Received: by 10.68.219.231 with SMTP id pr7mr23746405pbc.73.1421549981449;
        Sat, 17 Jan 2015 18:59:41 -0800 (PST)
Received: from [100.73.158.133] (11.sub-70-197-14.myvzw.com. [70.197.14.11])
        by mx.google.com with ESMTPSA id l4sm7854471pdj.47.2015.01.17.18.59.40
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 Jan 2015 18:59:40 -0800 (PST)
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Semantics of LGTM
From: sandy.ryza@cloudera.com
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
Date: Sat, 17 Jan 2015 18:59:39 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

I think clarifying these semantics is definitely worthwhile. Maybe this comp=
licates the process with additional terminology, but the way I've used these=
 has been:

+1 - I think this is safe to merge and, barring objections from others, woul=
d merge it immediately.

LGTM - I have no concerns about this patch, but I don't necessarily feel qua=
lified to make a final call about it.  The TM part acknowledges the judgment=
 as a little more subjective.

I think having some concise way to express both of these is useful.

-Sandy

> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>=20
> Hey All,
>=20
> Just wanted to ping about a minor issue - but one that ends up having
> consequence given Spark's volume of reviews and commits. As much as
> possible, I think that we should try and gear towards "Google Style"
> LGTM on reviews. What I mean by this is that LGTM has the following
> semantics:
>=20
> "I know this code well, or I've looked at it close enough to feel
> confident it should be merged. If there are issues/bugs with this code
> later on, I feel confident I can help with them."
>=20
> Here is an alternative semantic:
>=20
> "Based on what I know about this part of the code, I don't see any
> show-stopper problems with this patch".
>=20
> The issue with the latter is that it ultimately erodes the
> significance of LGTM, since subsequent reviewers need to reason about
> what the person meant by saying LGTM. In contrast, having strong
> semantics around LGTM can help streamline reviews a lot, especially as
> reviewers get more experienced and gain trust from the comittership.
>=20
> There are several easy ways to give a more limited endorsement of a patch:=

> - "I'm not familiar with this code, but style, etc look good" (general
> endorsement)
> - "The build changes in this code LGTM, but I haven't reviewed the
> rest" (limited LGTM)
>=20
> If people are okay with this, I might add a short note on the wiki.
> I'm sending this e-mail first, though, to see whether anyone wants to
> express agreement or disagreement with this approach.
>=20
> - Patrick
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 03:02:35 2015
Return-Path: <dev-return-11181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 134FF10328
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 03:02:35 +0000 (UTC)
Received: (qmail 61110 invoked by uid 500); 18 Jan 2015 03:02:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61035 invoked by uid 500); 18 Jan 2015 03:02:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61023 invoked by uid 99); 18 Jan 2015 03:02:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:02:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:02:10 +0000
Received: by mail-la0-f47.google.com with SMTP id hz20so24133640lab.6
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 18:59:34 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=YRtWEgEq1jM1E6OzYj8G3IKdNNJRl5P9eCx7BmwvMls=;
        b=RJDP1iacLfnPaZ/KC/CJVR2ez4BiyKf+0LZXPCcT+UPZwdPDgqQbq9zvI8hT4luz7T
         yWJXAuorwqYU5l0ZMBHrYwdX0zWJx6b4EqpiikzlF+dMVSvMqHdvy2Eq5/dYgEzvDnWy
         wC8WFfTXppwXna86bPSpPvPzrL7wvJhYViHpKz7UjQ9UnNs4iF7JYQ+opLBX+V/8KK3D
         pbr4m8IYS7HgH4jKM7jPUcyT14Pv5uRM9Lzijiy5LPoN4eFJO1/J7bqnCf83V7HdV30C
         14+NpUhu57c1wNOklogRvpykWYqp2cJYZ2/R0u7pyrfY6B9uOpAbzjArDiieqO1QSLBb
         +1BA==
X-Gm-Message-State: ALoCoQmDVGdnAgmg0z6vp/VhswNLZw4TopiEyEHZWED0VMmmNQ02TFtFB36bxFvpdzenc07zV7me
MIME-Version: 1.0
X-Received: by 10.152.234.140 with SMTP id ue12mr16996636lac.78.1421549974057;
 Sat, 17 Jan 2015 18:59:34 -0800 (PST)
Received: by 10.152.114.10 with HTTP; Sat, 17 Jan 2015 18:59:33 -0800 (PST)
Received: by 10.152.114.10 with HTTP; Sat, 17 Jan 2015 18:59:33 -0800 (PST)
In-Reply-To: <CABPQxssfRNmVJZy=6U3_aBYjyMHw-SJfktbOMmRSu3wdjoAYKw@mail.gmail.com>
References: <CAHUQ+_YaOh867Pt3_f+6a9X+OdPq0kgGJR=-NfZ=Egq92nP2Sg@mail.gmail.com>
	<CABPQxssfRNmVJZy=6U3_aBYjyMHw-SJfktbOMmRSu3wdjoAYKw@mail.gmail.com>
Date: Sun, 18 Jan 2015 08:29:34 +0530
Message-ID: <CAHUQ+_YJkkUJ7WN8xBb_2ZNN-MDZZF0uiY+UgxqKOCzB-yX3Fg@mail.gmail.com>
Subject: Re: Bouncing Mails
From: Akhil Das <akhil@sigmoidanalytics.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: user@spark.apache.org, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a55e3c0bdc050ce46555
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a55e3c0bdc050ce46555
Content-Type: text/plain; charset=UTF-8

Yep. They have sorted it out it seems.
On 18 Jan 2015 03:58, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Akhil,
>
> Those are handled by ASF infrastructure, not anyone in the Spark
> project. So this list is not the appropriate place to ask for help.
>
> - Patrick
>
> On Sat, Jan 17, 2015 at 12:56 AM, Akhil Das <akhil@sigmoidanalytics.com>
> wrote:
> > My mails to the mailing list are getting rejected, have opened a Jira
> issue,
> > can someone take a look at it?
> >
> > https://issues.apache.org/jira/browse/INFRA-9032
> >
> >
> >
> >
> >
> >
> > Thanks
> > Best Regards
>

--001a1133a55e3c0bdc050ce46555--

From dev-return-11182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 03:11:05 2015
Return-Path: <dev-return-11182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31ABA10353
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 03:11:05 +0000 (UTC)
Received: (qmail 65492 invoked by uid 500); 18 Jan 2015 03:11:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65420 invoked by uid 500); 18 Jan 2015 03:11:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65408 invoked by uid 99); 18 Jan 2015 03:11:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:11:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:10:40 +0000
Received: by mail-qg0-f48.google.com with SMTP id j5so21246077qga.7
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 19:09:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=l0pDPyyeu2HMaaMSWWStnvN0j1sCSgJt/ja7AeZyPv8=;
        b=kmeLosaXC+mFWyr6HRvC+irKUTen3mqAVnt6EZ3YH6VBMrSlkblXOwLBvt7dxLolPK
         Y/E+pJ4Hqev71KoTjcRoGuqzoinr64EKy4Nsx7hucdCknJLvpMZf7lg2/znOCxIZsqw5
         qEgscozGBzMpPa+uNl0YsfPKlDfZGMLheYcwtFgfaCAe3m/41iHvES0zJS5ZziRECyy4
         Htm2ftJ9aA3PuUZ2LU7wIs3iD2eqqvIsOm4bRCQB7L8xQVlF5IfyDk8NdNE3wi973qZ0
         Ov0hlJ8evPi5xAM9Ko2qekP3ZvHHJl8a77O/jb8/gT/v5iBzk9yCTd8iReBAR/DB6uKI
         7xwQ==
X-Received: by 10.140.101.135 with SMTP id u7mr35511880qge.2.1421550593399;
 Sat, 17 Jan 2015 19:09:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.33.247 with HTTP; Sat, 17 Jan 2015 19:09:33 -0800 (PST)
In-Reply-To: <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
 <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Sat, 17 Jan 2015 19:09:33 -0800
Message-ID: <CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
Subject: Re: Semantics of LGTM
To: sandy.ryza@cloudera.com
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1670c2663d3050ce48a2a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1670c2663d3050ce48a2a
Content-Type: text/plain; charset=UTF-8

I think I've seen something like +2 = "strong LGTM" and +1 = "weak LGTM;
someone else should review" before. It's nice to have a shortcut which
isn't a sentence when talking about weaker forms of LGTM.

On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:

> I think clarifying these semantics is definitely worthwhile. Maybe this
> complicates the process with additional terminology, but the way I've used
> these has been:
>
> +1 - I think this is safe to merge and, barring objections from others,
> would merge it immediately.
>
> LGTM - I have no concerns about this patch, but I don't necessarily feel
> qualified to make a final call about it.  The TM part acknowledges the
> judgment as a little more subjective.
>
> I think having some concise way to express both of these is useful.
>
> -Sandy
>
> > On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> >
> > Hey All,
> >
> > Just wanted to ping about a minor issue - but one that ends up having
> > consequence given Spark's volume of reviews and commits. As much as
> > possible, I think that we should try and gear towards "Google Style"
> > LGTM on reviews. What I mean by this is that LGTM has the following
> > semantics:
> >
> > "I know this code well, or I've looked at it close enough to feel
> > confident it should be merged. If there are issues/bugs with this code
> > later on, I feel confident I can help with them."
> >
> > Here is an alternative semantic:
> >
> > "Based on what I know about this part of the code, I don't see any
> > show-stopper problems with this patch".
> >
> > The issue with the latter is that it ultimately erodes the
> > significance of LGTM, since subsequent reviewers need to reason about
> > what the person meant by saying LGTM. In contrast, having strong
> > semantics around LGTM can help streamline reviews a lot, especially as
> > reviewers get more experienced and gain trust from the comittership.
> >
> > There are several easy ways to give a more limited endorsement of a
> patch:
> > - "I'm not familiar with this code, but style, etc look good" (general
> > endorsement)
> > - "The build changes in this code LGTM, but I haven't reviewed the
> > rest" (limited LGTM)
> >
> > If people are okay with this, I might add a short note on the wiki.
> > I'm sending this e-mail first, though, to see whether anyone wants to
> > express agreement or disagreement with this approach.
> >
> > - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c1670c2663d3050ce48a2a--

From dev-return-11183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 03:27:34 2015
Return-Path: <dev-return-11183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55663103E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 03:27:34 +0000 (UTC)
Received: (qmail 78178 invoked by uid 500); 18 Jan 2015 03:27:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78106 invoked by uid 500); 18 Jan 2015 03:27:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78087 invoked by uid 99); 18 Jan 2015 03:27:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:27:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 03:27:08 +0000
Received: by mail-ob0-f172.google.com with SMTP id wp18so3420342obc.3
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 19:25:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lKoUKo8Zxjmt0zJ1zlXfSgKzLS/FXsd8ndYhVHhE1gA=;
        b=GN+faW5nW0wWnMwrqvEsNxf/TCswipm5ARfzNnARUJIGgD7faSlnJVU1YeA0wcLRb8
         B8dw1BiosTZjXAQdcUd2A0LWnoVe4VeSTFKjJ9TEEsMp92GwT4P1+ow0x/boAO44I/XD
         o1ZbiALIlR6WejIZpHd4SIvLcM2iLFgI8QiFGAPFQTUrnr687Se+wBJFqZhEKuB+pNaO
         KSYfzvU3wuJsuRmkqamUsacXty0LGdlIOk/TXMhjJrUHVz3hndIu4YxO9uDALlI7+s8A
         k6TX6E4zI9JbJkIPVUZeEl4RyXmht86+cRNys+RcSDq2yDbuVNRAiDALqkU6xzrwgt+b
         msAQ==
MIME-Version: 1.0
X-Received: by 10.60.47.112 with SMTP id c16mr8419760oen.83.1421551537134;
 Sat, 17 Jan 2015 19:25:37 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Sat, 17 Jan 2015 19:25:37 -0800 (PST)
In-Reply-To: <CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
	<1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
	<CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
Date: Sat, 17 Jan 2015 19:25:37 -0800
Message-ID: <CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
Subject: Re: Semantics of LGTM
From: Patrick Wendell <pwendell@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: Sandy Ryza <sandy.ryza@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I think the ASF +1 is *slightly* different than Google's LGTM, because
it might convey wanting the patch/feature to be merged but not
necessarily saying you did a thorough review and stand behind it's
technical contents. For instance, I've seen people pile on +1's to try
and indicate support for a feature or patch in some projects, even
though they didn't do a thorough technical review. This +1 is
definitely a useful mechanism.

There is definitely much overlap though in the meaning, though, and
it's largely because Spark had it's own culture around reviews before
it was donated to the ASF, so there is a mix of two styles.

Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
proposed originally (unlike the one Sandy proposed, e.g.). This is
what I've seen every project using the LGTM convention do (Google, and
some open source projects such as Impala) to indicate technical
sign-off.

- Patrick

On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> I think I've seen something like +2 = "strong LGTM" and +1 = "weak LGTM;
> someone else should review" before. It's nice to have a shortcut which isn't
> a sentence when talking about weaker forms of LGTM.
>
> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
>>
>> I think clarifying these semantics is definitely worthwhile. Maybe this
>> complicates the process with additional terminology, but the way I've used
>> these has been:
>>
>> +1 - I think this is safe to merge and, barring objections from others,
>> would merge it immediately.
>>
>> LGTM - I have no concerns about this patch, but I don't necessarily feel
>> qualified to make a final call about it.  The TM part acknowledges the
>> judgment as a little more subjective.
>>
>> I think having some concise way to express both of these is useful.
>>
>> -Sandy
>>
>> > On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> >
>> > Hey All,
>> >
>> > Just wanted to ping about a minor issue - but one that ends up having
>> > consequence given Spark's volume of reviews and commits. As much as
>> > possible, I think that we should try and gear towards "Google Style"
>> > LGTM on reviews. What I mean by this is that LGTM has the following
>> > semantics:
>> >
>> > "I know this code well, or I've looked at it close enough to feel
>> > confident it should be merged. If there are issues/bugs with this code
>> > later on, I feel confident I can help with them."
>> >
>> > Here is an alternative semantic:
>> >
>> > "Based on what I know about this part of the code, I don't see any
>> > show-stopper problems with this patch".
>> >
>> > The issue with the latter is that it ultimately erodes the
>> > significance of LGTM, since subsequent reviewers need to reason about
>> > what the person meant by saying LGTM. In contrast, having strong
>> > semantics around LGTM can help streamline reviews a lot, especially as
>> > reviewers get more experienced and gain trust from the comittership.
>> >
>> > There are several easy ways to give a more limited endorsement of a
>> > patch:
>> > - "I'm not familiar with this code, but style, etc look good" (general
>> > endorsement)
>> > - "The build changes in this code LGTM, but I haven't reviewed the
>> > rest" (limited LGTM)
>> >
>> > If people are okay with this, I might add a short note on the wiki.
>> > I'm sending this e-mail first, though, to see whether anyone wants to
>> > express agreement or disagreement with this approach.
>> >
>> > - Patrick
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 04:05:55 2015
Return-Path: <dev-return-11184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4CC31049A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 04:05:55 +0000 (UTC)
Received: (qmail 6734 invoked by uid 500); 18 Jan 2015 04:05:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6654 invoked by uid 500); 18 Jan 2015 04:05:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6643 invoked by uid 99); 18 Jan 2015 04:05:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 04:05:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 04:05:30 +0000
Received: by mail-pa0-f52.google.com with SMTP id kx10so1300757pab.11
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 20:04:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:content-transfer-encoding:message-id:references
         :to;
        bh=PZdtSe8ojbU9jFptj6cirzfoXYnieBc+JRPtHU5kDUI=;
        b=RI6FZc1w4vvx/rtk/8eSQ/nrzDW4OtvvyQBtU2WRw2cP798EOtBUVidk8CA4KZSdWs
         kAnMfp9LvAyQkLPQoifMuKEKuj3C0GDmVisEefjUF0wrDex7gi/XuDiAUhtKM0ACmfbc
         4yA3ArStpM3+Zl11fi3pp0k44tNIuXpk1XjVwaoYiL91eGaMpTG5YP5TeCwPSWg/aO43
         UiPeaLNT/dv0bi7LqjeqjsYmZgwyO+ICiPFhMWHNAkuZzePGIi8ewuqKc3gkMQvHAS3J
         Od6i3k3riXK8FzjIKnmJU6MQTFEPUFtHV8AG27fO9Exo57KTbu7ENq/C+6VMz5PJF9KD
         kLGQ==
X-Gm-Message-State: ALoCoQmJ+CzJRZI9OtrVHt8a/U9GxExeCiIX3bJMZXsiTUcRouCqiWZHDcl/sj5gtBPQhz2WmRWF
X-Received: by 10.68.241.102 with SMTP id wh6mr34211856pbc.18.1421553883416;
        Sat, 17 Jan 2015 20:04:43 -0800 (PST)
Received: from [100.106.16.74] (11.sub-70-197-14.myvzw.com. [70.197.14.11])
        by mx.google.com with ESMTPSA id b3sm2911408pbu.44.2015.01.17.20.04.39
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 Jan 2015 20:04:42 -0800 (PST)
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Semantics of LGTM
From: sandy.ryza@cloudera.com
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
Date: Sat, 17 Jan 2015 20:04:39 -0800
Cc: Aaron Davidson <ilikerps@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com> <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com> <CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com> <CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, the ASF +1 has become partly overloaded to mean both "I would like to s=
ee this feature" and "this patch should be committed", although, at least in=
 Hadoop, using +1 on JIRA (as opposed to, say, in a release vote) should una=
mbiguously mean the latter unless qualified in some other way.

I don't have any opinion on the specific characters, but I agree with Aaron t=
hat it would be nice to have some sort of abbreviation for both the strong a=
nd weak forms of approval.

-Sandy

> On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>=20
> I think the ASF +1 is *slightly* different than Google's LGTM, because
> it might convey wanting the patch/feature to be merged but not
> necessarily saying you did a thorough review and stand behind it's
> technical contents. For instance, I've seen people pile on +1's to try
> and indicate support for a feature or patch in some projects, even
> though they didn't do a thorough technical review. This +1 is
> definitely a useful mechanism.
>=20
> There is definitely much overlap though in the meaning, though, and
> it's largely because Spark had it's own culture around reviews before
> it was donated to the ASF, so there is a mix of two styles.
>=20
> Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
> proposed originally (unlike the one Sandy proposed, e.g.). This is
> what I've seen every project using the LGTM convention do (Google, and
> some open source projects such as Impala) to indicate technical
> sign-off.
>=20
> - Patrick
>=20
>> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson <ilikerps@gmail.com> wrot=
e:
>> I think I've seen something like +2 =3D "strong LGTM" and +1 =3D "weak LG=
TM;
>> someone else should review" before. It's nice to have a shortcut which is=
n't
>> a sentence when talking about weaker forms of LGTM.
>>=20
>> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
>>>=20
>>> I think clarifying these semantics is definitely worthwhile. Maybe this
>>> complicates the process with additional terminology, but the way I've us=
ed
>>> these has been:
>>>=20
>>> +1 - I think this is safe to merge and, barring objections from others,
>>> would merge it immediately.
>>>=20
>>> LGTM - I have no concerns about this patch, but I don't necessarily feel=

>>> qualified to make a final call about it.  The TM part acknowledges the
>>> judgment as a little more subjective.
>>>=20
>>> I think having some concise way to express both of these is useful.
>>>=20
>>> -Sandy
>>>=20
>>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:
>>>>=20
>>>> Hey All,
>>>>=20
>>>> Just wanted to ping about a minor issue - but one that ends up having
>>>> consequence given Spark's volume of reviews and commits. As much as
>>>> possible, I think that we should try and gear towards "Google Style"
>>>> LGTM on reviews. What I mean by this is that LGTM has the following
>>>> semantics:
>>>>=20
>>>> "I know this code well, or I've looked at it close enough to feel
>>>> confident it should be merged. If there are issues/bugs with this code
>>>> later on, I feel confident I can help with them."
>>>>=20
>>>> Here is an alternative semantic:
>>>>=20
>>>> "Based on what I know about this part of the code, I don't see any
>>>> show-stopper problems with this patch".
>>>>=20
>>>> The issue with the latter is that it ultimately erodes the
>>>> significance of LGTM, since subsequent reviewers need to reason about
>>>> what the person meant by saying LGTM. In contrast, having strong
>>>> semantics around LGTM can help streamline reviews a lot, especially as
>>>> reviewers get more experienced and gain trust from the comittership.
>>>>=20
>>>> There are several easy ways to give a more limited endorsement of a
>>>> patch:
>>>> - "I'm not familiar with this code, but style, etc look good" (general
>>>> endorsement)
>>>> - "The build changes in this code LGTM, but I haven't reviewed the
>>>> rest" (limited LGTM)
>>>>=20
>>>> If people are okay with this, I might add a short note on the wiki.
>>>> I'm sending this e-mail first, though, to see whether anyone wants to
>>>> express agreement or disagreement with this approach.
>>>>=20
>>>> - Patrick
>>>>=20
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 04:56:20 2015
Return-Path: <dev-return-11185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED51C1051A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 04:56:20 +0000 (UTC)
Received: (qmail 33997 invoked by uid 500); 18 Jan 2015 04:56:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33915 invoked by uid 500); 18 Jan 2015 04:56:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33904 invoked by uid 99); 18 Jan 2015 04:56:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 04:56:21 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.145 as permitted sender)
Received: from [169.229.218.145] (HELO cm04fe.IST.Berkeley.EDU) (169.229.218.145)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 04:56:16 +0000
Received: from mail-yk0-f175.google.com ([209.85.160.175])
	by cm04fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1YChns-0002nj-Eg
	for dev@spark.apache.org; Sat, 17 Jan 2015 20:49:37 -0800
Received: by mail-yk0-f175.google.com with SMTP id q9so1216657ykb.6
        for <dev@spark.apache.org>; Sat, 17 Jan 2015 20:49:35 -0800 (PST)
MIME-Version: 1.0
X-Received: by 10.236.206.13 with SMTP id k13mr14462803yho.51.1421556575773;
 Sat, 17 Jan 2015 20:49:35 -0800 (PST)
Received: by 10.170.114.6 with HTTP; Sat, 17 Jan 2015 20:49:35 -0800 (PST)
In-Reply-To: <FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
	<1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
	<CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
	<CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
	<FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com>
Date: Sat, 17 Jan 2015 20:49:35 -0800
Message-ID: <CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
Subject: Re: Semantics of LGTM
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Aaron Davidson <ilikerps@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b66fdb5ba2f98050ce5ee98
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b66fdb5ba2f98050ce5ee98
Content-Type: text/plain; charset=UTF-8

+1 to Patrick's proposal of strong LGTM semantics.  On past projects, I've
heard the semantics of "LGTM" expressed as "I've looked at this thoroughly
and take as much ownership as if I wrote the patch myself".  My
understanding is that this is the level of review we expect for all patches
that ultimately go into Spark, so it's important to have a way to concisely
describe when this has been done.

Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
cases I've seen, if someone else says "I looked at this very quickly and
didn't see any glaring problems", it doesn't add any value for subsequent
reviewers (someone still needs to take a thorough look).

-Kay

On Sat, Jan 17, 2015 at 8:04 PM, <sandy.ryza@cloudera.com> wrote:

> Yeah, the ASF +1 has become partly overloaded to mean both "I would like
> to see this feature" and "this patch should be committed", although, at
> least in Hadoop, using +1 on JIRA (as opposed to, say, in a release vote)
> should unambiguously mean the latter unless qualified in some other way.
>
> I don't have any opinion on the specific characters, but I agree with
> Aaron that it would be nice to have some sort of abbreviation for both the
> strong and weak forms of approval.
>
> -Sandy
>
> > On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> >
> > I think the ASF +1 is *slightly* different than Google's LGTM, because
> > it might convey wanting the patch/feature to be merged but not
> > necessarily saying you did a thorough review and stand behind it's
> > technical contents. For instance, I've seen people pile on +1's to try
> > and indicate support for a feature or patch in some projects, even
> > though they didn't do a thorough technical review. This +1 is
> > definitely a useful mechanism.
> >
> > There is definitely much overlap though in the meaning, though, and
> > it's largely because Spark had it's own culture around reviews before
> > it was donated to the ASF, so there is a mix of two styles.
> >
> > Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
> > proposed originally (unlike the one Sandy proposed, e.g.). This is
> > what I've seen every project using the LGTM convention do (Google, and
> > some open source projects such as Impala) to indicate technical
> > sign-off.
> >
> > - Patrick
> >
> >> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >> I think I've seen something like +2 = "strong LGTM" and +1 = "weak LGTM;
> >> someone else should review" before. It's nice to have a shortcut which
> isn't
> >> a sentence when talking about weaker forms of LGTM.
> >>
> >> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
> >>>
> >>> I think clarifying these semantics is definitely worthwhile. Maybe this
> >>> complicates the process with additional terminology, but the way I've
> used
> >>> these has been:
> >>>
> >>> +1 - I think this is safe to merge and, barring objections from others,
> >>> would merge it immediately.
> >>>
> >>> LGTM - I have no concerns about this patch, but I don't necessarily
> feel
> >>> qualified to make a final call about it.  The TM part acknowledges the
> >>> judgment as a little more subjective.
> >>>
> >>> I think having some concise way to express both of these is useful.
> >>>
> >>> -Sandy
> >>>
> >>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>>>
> >>>> Hey All,
> >>>>
> >>>> Just wanted to ping about a minor issue - but one that ends up having
> >>>> consequence given Spark's volume of reviews and commits. As much as
> >>>> possible, I think that we should try and gear towards "Google Style"
> >>>> LGTM on reviews. What I mean by this is that LGTM has the following
> >>>> semantics:
> >>>>
> >>>> "I know this code well, or I've looked at it close enough to feel
> >>>> confident it should be merged. If there are issues/bugs with this code
> >>>> later on, I feel confident I can help with them."
> >>>>
> >>>> Here is an alternative semantic:
> >>>>
> >>>> "Based on what I know about this part of the code, I don't see any
> >>>> show-stopper problems with this patch".
> >>>>
> >>>> The issue with the latter is that it ultimately erodes the
> >>>> significance of LGTM, since subsequent reviewers need to reason about
> >>>> what the person meant by saying LGTM. In contrast, having strong
> >>>> semantics around LGTM can help streamline reviews a lot, especially as
> >>>> reviewers get more experienced and gain trust from the comittership.
> >>>>
> >>>> There are several easy ways to give a more limited endorsement of a
> >>>> patch:
> >>>> - "I'm not familiar with this code, but style, etc look good" (general
> >>>> endorsement)
> >>>> - "The build changes in this code LGTM, but I haven't reviewed the
> >>>> rest" (limited LGTM)
> >>>>
> >>>> If people are okay with this, I might add a short note on the wiki.
> >>>> I'm sending this e-mail first, though, to see whether anyone wants to
> >>>> express agreement or disagreement with this approach.
> >>>>
> >>>> - Patrick
> >>>>
> >>>> ---------------------------------------------------------------------
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b66fdb5ba2f98050ce5ee98--

From dev-return-11186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 06:27:43 2015
Return-Path: <dev-return-11186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01867105FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 06:27:43 +0000 (UTC)
Received: (qmail 73905 invoked by uid 500); 18 Jan 2015 06:27:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73834 invoked by uid 500); 18 Jan 2015 06:27:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73820 invoked by uid 99); 18 Jan 2015 06:27:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 06:27:42 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gilv@il.ibm.com designates 195.75.94.109 as permitted sender)
Received: from [195.75.94.109] (HELO e06smtp13.uk.ibm.com) (195.75.94.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 06:27:13 +0000
Received: from /spool/local
	by e06smtp13.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Sun, 18 Jan 2015 06:26:42 -0000
Received: from d06dlp03.portsmouth.uk.ibm.com (9.149.20.15)
	by e06smtp13.uk.ibm.com (192.168.101.143) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Sun, 18 Jan 2015 06:26:40 -0000
Received: from b06cxnps3074.portsmouth.uk.ibm.com (d06relay09.portsmouth.uk.ibm.com [9.149.109.194])
	by d06dlp03.portsmouth.uk.ibm.com (Postfix) with ESMTP id 8A5D81B0804B
	for <dev@spark.apache.org>; Sun, 18 Jan 2015 06:26:39 +0000 (GMT)
Received: from d06av08.portsmouth.uk.ibm.com (d06av08.portsmouth.uk.ibm.com [9.149.37.249])
	by b06cxnps3074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id t0I6QdM960686514
	for <dev@spark.apache.org>; Sun, 18 Jan 2015 06:26:39 GMT
Received: from d06av08.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id t0I6QdGO022258
	for <dev@spark.apache.org>; Sat, 17 Jan 2015 23:26:39 -0700
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id t0I6QdjW022255
	for <dev@spark.apache.org>; Sat, 17 Jan 2015 23:26:39 -0700
To: dev <dev@spark.apache.org>
MIME-Version: 1.0
Subject: run time exceptions in Spark 1.2.0 manual build together with OpenStack
 hadoop driver
X-KeepSent: 67E8C1B6:947D6EC2-C2257DD1:00215569;
 type=4; name=$KeepSent
X-Mailer: IBM Notes Release 9.0.1SHF211 December 19, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF67E8C1B6.947D6EC2-ONC2257DD1.00215569-C2257DD1.00236627@il.ibm.com>
Date: Sun, 18 Jan 2015 08:26:38 +0200
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 9.0.1FP2|August  03, 2014) at
 18/01/2015 08:26:38,
	Serialize complete at 18/01/2015 08:26:38
Content-Type: multipart/alternative; boundary="=_alternative 00236547C2257DD1_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15011806-0013-0000-0000-000002A4B431
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 00236547C2257DD1_=
Content-Type: text/plain; charset="US-ASCII"

Hi,

I took a source code of Spark 1.2.0 and tried to build it together with 
hadoop-openstack.jar ( To allow Spark an access to OpenStack Swift )
I used Hadoop 2.6.0.

The build was fine without problems, however in run time, while trying to 
access "swift://" name space i got an exception:
java.lang.NoClassDefFoundError: org/codehaus/jackson/annotate/JsonClass
                 at 
org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector.findDeserializationType(JacksonAnnotationIntrospector.java:524)
                 at 
org.codehaus.jackson.map.deser.BasicDeserializerFactory.modifyTypeByAnnotation(BasicDeserializerFactory.java:732)
                ...and the long stack trace goes here

Digging into the problem i saw the following:
Jackson versions 1.9.X are not backward compatible, in particular they 
removed JsonClass annotation.
Hadoop 2.6.0 uses jackson-asl version 1.9.13, while Spark has reference to 
older version of jackson.

This is the main  pom.xml of Spark 1.2.0 :

      <dependency>
        <!-- Matches the version of jackson-core-asl pulled in by avro -->
        <groupId>org.codehaus.jackson</groupId>
        <artifactId>jackson-mapper-asl</artifactId>
        <version>1.8.8</version>
      </dependency>

Referencing 1.8.8 version, which is not compatible with Hadoop 2.6.0 .
If we change version to 1.9.13, than all will work fine and there will be 
no run time exceptions while accessing Swift. The following change will 
solve the problem:

      <dependency>
        <!-- Matches the version of jackson-core-asl pulled in by avro -->
        <groupId>org.codehaus.jackson</groupId>
        <artifactId>jackson-mapper-asl</artifactId>
        <version>1.9.13</version>
      </dependency>

I am trying to resolve this somehow so people will not get into this 
issue.
Is there any particular need in Spark for jackson 1.8.8 and not 1.9.13?
Can we remove 1.8.8 and put 1.9.13 for Avro? 
It looks to me that all works fine when Spark build with jackson 1.9.13, 
but i am not an expert and not sure what should be tested.

Thanks,
Gil Vernik.

--=_alternative 00236547C2257DD1_=--


From dev-return-11187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 09:05:09 2015
Return-Path: <dev-return-11187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 343DC10827
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 09:05:09 +0000 (UTC)
Received: (qmail 43537 invoked by uid 500); 18 Jan 2015 09:05:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43465 invoked by uid 500); 18 Jan 2015 09:05:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43453 invoked by uid 99); 18 Jan 2015 09:05:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 09:05:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 09:04:35 +0000
Received: by mail-qc0-f179.google.com with SMTP id w7so2192591qcr.10
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 01:04:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=bi9ztWoHCIv5ccJkP1syBf0Ttss4VE7VU1+HCCNcOSg=;
        b=gkN0xQksBqbBGhTqmPsTyTst186wB27Nu6+B7EdnoOEsSnwu+BSfNJOkUwJfMO7feL
         hXzCa1JPLpqPwXTv1OADmW00X1GISEAOgqCC+ZD7Q2z2JWqTmRCcN8ne6LlnhCLGLSy1
         5i5E7dGnjcoXBh35x/m2K9HGzW40MWwCG3fTJA1Ac97k9VerMBZhOX+TXmXDkJp2K9cz
         jNBwgq+BOwEKXPZSEdK5OjH+xMm8WKl8abYyrYTmZx9RbyYz256c7XIvJtQlTs1wKnau
         VWZMyICbH1uVMQGmkvvoJEaNf3+KmphPAxV1eAhRNE8DDIRugb4xFRRx3SkHAqWSFKwM
         a4qg==
X-Gm-Message-State: ALoCoQk+Ih0VhGdCwpXbdwA1BQVdHlM1beXwvdkamNMqZcz2ULy9CJBK/pDEKYgDQROxwWoUtumU
X-Received: by 10.224.99.3 with SMTP id s3mr27326010qan.79.1421571853030; Sun,
 18 Jan 2015 01:04:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Sun, 18 Jan 2015 01:03:51 -0800 (PST)
In-Reply-To: <CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
 <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com> <CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
 <CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
 <FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com> <CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 18 Jan 2015 01:03:51 -0800
Message-ID: <CAPh_B=bWNhPy6B8Ay=bA24jftuShxLhrz2FZ9b-sXjVK6p-ZTQ@mail.gmail.com>
Subject: Re: Semantics of LGTM
To: Kay Ousterhout <keo@eecs.berkeley.edu>
Cc: Sandy Ryza <sandy.ryza@cloudera.com>, Patrick Wendell <pwendell@gmail.com>, 
	Aaron Davidson <ilikerps@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b67738452add7050ce97de6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b67738452add7050ce97de6
Content-Type: text/plain; charset=UTF-8

Maybe just to avoid LGTM as a single token when it is not actually
according to Patrick's definition, but anybody can still leave comments
like:

"The direction of the PR looks good to me." or "+1 on the direction"

"The build part looks good to me"

...


On Sat, Jan 17, 2015 at 8:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
wrote:

> +1 to Patrick's proposal of strong LGTM semantics.  On past projects, I've
> heard the semantics of "LGTM" expressed as "I've looked at this thoroughly
> and take as much ownership as if I wrote the patch myself".  My
> understanding is that this is the level of review we expect for all patches
> that ultimately go into Spark, so it's important to have a way to concisely
> describe when this has been done.
>
> Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
> cases I've seen, if someone else says "I looked at this very quickly and
> didn't see any glaring problems", it doesn't add any value for subsequent
> reviewers (someone still needs to take a thorough look).
>
> -Kay
>
> On Sat, Jan 17, 2015 at 8:04 PM, <sandy.ryza@cloudera.com> wrote:
>
> > Yeah, the ASF +1 has become partly overloaded to mean both "I would like
> > to see this feature" and "this patch should be committed", although, at
> > least in Hadoop, using +1 on JIRA (as opposed to, say, in a release vote)
> > should unambiguously mean the latter unless qualified in some other way.
> >
> > I don't have any opinion on the specific characters, but I agree with
> > Aaron that it would be nice to have some sort of abbreviation for both
> the
> > strong and weak forms of approval.
> >
> > -Sandy
> >
> > > On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > >
> > > I think the ASF +1 is *slightly* different than Google's LGTM, because
> > > it might convey wanting the patch/feature to be merged but not
> > > necessarily saying you did a thorough review and stand behind it's
> > > technical contents. For instance, I've seen people pile on +1's to try
> > > and indicate support for a feature or patch in some projects, even
> > > though they didn't do a thorough technical review. This +1 is
> > > definitely a useful mechanism.
> > >
> > > There is definitely much overlap though in the meaning, though, and
> > > it's largely because Spark had it's own culture around reviews before
> > > it was donated to the ASF, so there is a mix of two styles.
> > >
> > > Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
> > > proposed originally (unlike the one Sandy proposed, e.g.). This is
> > > what I've seen every project using the LGTM convention do (Google, and
> > > some open source projects such as Impala) to indicate technical
> > > sign-off.
> > >
> > > - Patrick
> > >
> > >> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson <ilikerps@gmail.com>
> > wrote:
> > >> I think I've seen something like +2 = "strong LGTM" and +1 = "weak
> LGTM;
> > >> someone else should review" before. It's nice to have a shortcut which
> > isn't
> > >> a sentence when talking about weaker forms of LGTM.
> > >>
> > >> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
> > >>>
> > >>> I think clarifying these semantics is definitely worthwhile. Maybe
> this
> > >>> complicates the process with additional terminology, but the way I've
> > used
> > >>> these has been:
> > >>>
> > >>> +1 - I think this is safe to merge and, barring objections from
> others,
> > >>> would merge it immediately.
> > >>>
> > >>> LGTM - I have no concerns about this patch, but I don't necessarily
> > feel
> > >>> qualified to make a final call about it.  The TM part acknowledges
> the
> > >>> judgment as a little more subjective.
> > >>>
> > >>> I think having some concise way to express both of these is useful.
> > >>>
> > >>> -Sandy
> > >>>
> > >>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > >>>>
> > >>>> Hey All,
> > >>>>
> > >>>> Just wanted to ping about a minor issue - but one that ends up
> having
> > >>>> consequence given Spark's volume of reviews and commits. As much as
> > >>>> possible, I think that we should try and gear towards "Google Style"
> > >>>> LGTM on reviews. What I mean by this is that LGTM has the following
> > >>>> semantics:
> > >>>>
> > >>>> "I know this code well, or I've looked at it close enough to feel
> > >>>> confident it should be merged. If there are issues/bugs with this
> code
> > >>>> later on, I feel confident I can help with them."
> > >>>>
> > >>>> Here is an alternative semantic:
> > >>>>
> > >>>> "Based on what I know about this part of the code, I don't see any
> > >>>> show-stopper problems with this patch".
> > >>>>
> > >>>> The issue with the latter is that it ultimately erodes the
> > >>>> significance of LGTM, since subsequent reviewers need to reason
> about
> > >>>> what the person meant by saying LGTM. In contrast, having strong
> > >>>> semantics around LGTM can help streamline reviews a lot, especially
> as
> > >>>> reviewers get more experienced and gain trust from the comittership.
> > >>>>
> > >>>> There are several easy ways to give a more limited endorsement of a
> > >>>> patch:
> > >>>> - "I'm not familiar with this code, but style, etc look good"
> (general
> > >>>> endorsement)
> > >>>> - "The build changes in this code LGTM, but I haven't reviewed the
> > >>>> rest" (limited LGTM)
> > >>>>
> > >>>> If people are okay with this, I might add a short note on the wiki.
> > >>>> I'm sending this e-mail first, though, to see whether anyone wants
> to
> > >>>> express agreement or disagreement with this approach.
> > >>>>
> > >>>> - Patrick
> > >>>>
> > >>>>
> ---------------------------------------------------------------------
> > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> > >>>
> > >>> ---------------------------------------------------------------------
> > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > >>
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7b67738452add7050ce97de6--

From dev-return-11188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 15:42:23 2015
Return-Path: <dev-return-11188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 825D3C035
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 15:42:23 +0000 (UTC)
Received: (qmail 9344 invoked by uid 500); 18 Jan 2015 15:42:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9282 invoked by uid 500); 18 Jan 2015 15:42:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9098 invoked by uid 99); 18 Jan 2015 15:42:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 15:42:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.44 as permitted sender)
Received: from [209.85.213.44] (HELO mail-yh0-f44.google.com) (209.85.213.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 15:42:17 +0000
Received: by mail-yh0-f44.google.com with SMTP id c41so13627251yho.3
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 07:41:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ltmfcxVW/sLJRA+fOsE27hiLz5xBnSIjEE1s0zBsLbQ=;
        b=WpO9D/gMAqPBe0n85nI6xfsmvZBJ4EoN7Q6HRifIWX4C7bulhbTDApvnTK6dKpfVca
         vmNukEUfPO1d85HJ66g944C4zu47RxS0S8PktNDhRp9TZz+2Yg8dFDQ9JhOpw7BYMdhx
         vbvrjruo7Wtqp3ixogx9J/iW1EGtRQgqMyMtGIlc7EkkpBlbrAF+Ewips0jBj930+V8N
         A5ZGl1kUjNCQJqoql9mEiDFjXvdJLqB0kn085s5T8F/YxyNVgOVYoRajub8lxKzrlTwR
         Odty3/Qw2ggYCT0iSjYpeQWYgNyq0trE73lkUekgkU/RXnG+B9BWJP8H19wlbP2PLuio
         F7KQ==
MIME-Version: 1.0
X-Received: by 10.236.47.131 with SMTP id t3mr2998766yhb.97.1421595716575;
 Sun, 18 Jan 2015 07:41:56 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Sun, 18 Jan 2015 07:41:56 -0800 (PST)
In-Reply-To: <OF67E8C1B6.947D6EC2-ONC2257DD1.00215569-C2257DD1.00236627@il.ibm.com>
References: <OF67E8C1B6.947D6EC2-ONC2257DD1.00215569-C2257DD1.00236627@il.ibm.com>
Date: Sun, 18 Jan 2015 07:41:56 -0800
Message-ID: <CALte62y9m9zNmnmWqXXXVQ-Dd7OixoB4BRD8sJX_BgSXP-6-HA@mail.gmail.com>
Subject: Re: run time exceptions in Spark 1.2.0 manual build together with
 OpenStack hadoop driver
From: Ted Yu <yuzhihong@gmail.com>
To: Gil Vernik <GILV@il.ibm.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c21840b36698050cef0b47
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c21840b36698050cef0b47
Content-Type: text/plain; charset=UTF-8

Please tale a look at SPARK-4048 and SPARK-5108

Cheers

On Sat, Jan 17, 2015 at 10:26 PM, Gil Vernik <GILV@il.ibm.com> wrote:

> Hi,
>
> I took a source code of Spark 1.2.0 and tried to build it together with
> hadoop-openstack.jar ( To allow Spark an access to OpenStack Swift )
> I used Hadoop 2.6.0.
>
> The build was fine without problems, however in run time, while trying to
> access "swift://" name space i got an exception:
> java.lang.NoClassDefFoundError: org/codehaus/jackson/annotate/JsonClass
>                  at
>
> org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector.findDeserializationType(JacksonAnnotationIntrospector.java:524)
>                  at
>
> org.codehaus.jackson.map.deser.BasicDeserializerFactory.modifyTypeByAnnotation(BasicDeserializerFactory.java:732)
>                 ...and the long stack trace goes here
>
> Digging into the problem i saw the following:
> Jackson versions 1.9.X are not backward compatible, in particular they
> removed JsonClass annotation.
> Hadoop 2.6.0 uses jackson-asl version 1.9.13, while Spark has reference to
> older version of jackson.
>
> This is the main  pom.xml of Spark 1.2.0 :
>
>       <dependency>
>         <!-- Matches the version of jackson-core-asl pulled in by avro -->
>         <groupId>org.codehaus.jackson</groupId>
>         <artifactId>jackson-mapper-asl</artifactId>
>         <version>1.8.8</version>
>       </dependency>
>
> Referencing 1.8.8 version, which is not compatible with Hadoop 2.6.0 .
> If we change version to 1.9.13, than all will work fine and there will be
> no run time exceptions while accessing Swift. The following change will
> solve the problem:
>
>       <dependency>
>         <!-- Matches the version of jackson-core-asl pulled in by avro -->
>         <groupId>org.codehaus.jackson</groupId>
>         <artifactId>jackson-mapper-asl</artifactId>
>         <version>1.9.13</version>
>       </dependency>
>
> I am trying to resolve this somehow so people will not get into this
> issue.
> Is there any particular need in Spark for jackson 1.8.8 and not 1.9.13?
> Can we remove 1.8.8 and put 1.9.13 for Avro?
> It looks to me that all works fine when Spark build with jackson 1.9.13,
> but i am not an expert and not sure what should be tested.
>
> Thanks,
> Gil Vernik.
>

--001a11c21840b36698050cef0b47--

From dev-return-11189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jan 18 18:23:16 2015
Return-Path: <dev-return-11189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF7AEC3FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 Jan 2015 18:23:16 +0000 (UTC)
Received: (qmail 86194 invoked by uid 500); 18 Jan 2015 18:23:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86109 invoked by uid 500); 18 Jan 2015 18:23:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85625 invoked by uid 99); 18 Jan 2015 18:23:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 18:23:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 Jan 2015 18:23:10 +0000
Received: by mail-wg0-f43.google.com with SMTP id k14so28237140wgh.2
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 10:21:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=aLxDKpQVCARzubzS4vu/qGjDPSTdEu9SCFTK847Dw4U=;
        b=A9T3RsVhBWIEJwBs4i01ceF7v5TbpkxZ/8RgyA+NUXb/dP8QIDsfUIgl2eRmGeALaG
         EsQypnsngl3oetuC3dhCloI4cxX/12/XbMLyDyWvB+3UzO170WjrDk/1OlCob09/cvza
         SjBjatz/34+Y2qDcLrbN03RPFyMktHQ9VLKOeUFnoXOkcLAXdWZg6ovsvbzmNP8qV0fb
         Chdc/XySl8JWc9QYMwvUKaZ7rHyZpXFUbPMHmgizV4sGg16sbvYXK6VHd17NuBXRYt2s
         O966Hf7OuNELsDK72+w7HYrhIc3A5bMZVMZVX4pam5XAvN/zDHGmbpwJIzd3qineBwAw
         1kAA==
X-Gm-Message-State: ALoCoQnixgkRj2rUGRldL9I8KHYLnLQJXw8jAhsJQPrnphOU4Yzwzqh2HpdBcPwIhdn/gn4MGdoF
X-Received: by 10.194.24.195 with SMTP id w3mr49260553wjf.135.1421605279974;
 Sun, 18 Jan 2015 10:21:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Sun, 18 Jan 2015 10:20:59 -0800 (PST)
In-Reply-To: <CALte62y9m9zNmnmWqXXXVQ-Dd7OixoB4BRD8sJX_BgSXP-6-HA@mail.gmail.com>
References: <OF67E8C1B6.947D6EC2-ONC2257DD1.00215569-C2257DD1.00236627@il.ibm.com>
 <CALte62y9m9zNmnmWqXXXVQ-Dd7OixoB4BRD8sJX_BgSXP-6-HA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 18 Jan 2015 18:20:59 +0000
Message-ID: <CAMAsSdJ7q_wVaD8cOg35OtcUW6WShGMo6YxfJ=saxBSYFWEneQ@mail.gmail.com>
Subject: Re: run time exceptions in Spark 1.2.0 manual build together with
 OpenStack hadoop driver
To: Ted Yu <yuzhihong@gmail.com>
Cc: Gil Vernik <GILV@il.ibm.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Agree, I think this can / should be fixed with a slightly more
conservative version of https://github.com/apache/spark/pull/3938
related to SPARK-5108.

On Sun, Jan 18, 2015 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> Please tale a look at SPARK-4048 and SPARK-5108
>
> Cheers
>
> On Sat, Jan 17, 2015 at 10:26 PM, Gil Vernik <GILV@il.ibm.com> wrote:
>
>> Hi,
>>
>> I took a source code of Spark 1.2.0 and tried to build it together with
>> hadoop-openstack.jar ( To allow Spark an access to OpenStack Swift )
>> I used Hadoop 2.6.0.
>>
>> The build was fine without problems, however in run time, while trying to
>> access "swift://" name space i got an exception:
>> java.lang.NoClassDefFoundError: org/codehaus/jackson/annotate/JsonClass
>>                  at
>>
>> org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector.findDeserializationType(JacksonAnnotationIntrospector.java:524)
>>                  at
>>
>> org.codehaus.jackson.map.deser.BasicDeserializerFactory.modifyTypeByAnnotation(BasicDeserializerFactory.java:732)
>>                 ...and the long stack trace goes here
>>
>> Digging into the problem i saw the following:
>> Jackson versions 1.9.X are not backward compatible, in particular they
>> removed JsonClass annotation.
>> Hadoop 2.6.0 uses jackson-asl version 1.9.13, while Spark has reference to
>> older version of jackson.
>>
>> This is the main  pom.xml of Spark 1.2.0 :
>>
>>       <dependency>
>>         <!-- Matches the version of jackson-core-asl pulled in by avro -->
>>         <groupId>org.codehaus.jackson</groupId>
>>         <artifactId>jackson-mapper-asl</artifactId>
>>         <version>1.8.8</version>
>>       </dependency>
>>
>> Referencing 1.8.8 version, which is not compatible with Hadoop 2.6.0 .
>> If we change version to 1.9.13, than all will work fine and there will be
>> no run time exceptions while accessing Swift. The following change will
>> solve the problem:
>>
>>       <dependency>
>>         <!-- Matches the version of jackson-core-asl pulled in by avro -->
>>         <groupId>org.codehaus.jackson</groupId>
>>         <artifactId>jackson-mapper-asl</artifactId>
>>         <version>1.9.13</version>
>>       </dependency>
>>
>> I am trying to resolve this somehow so people will not get into this
>> issue.
>> Is there any particular need in Spark for jackson 1.8.8 and not 1.9.13?
>> Can we remove 1.8.8 and put 1.9.13 for Avro?
>> It looks to me that all works fine when Spark build with jackson 1.9.13,
>> but i am not an expert and not sure what should be tested.
>>
>> Thanks,
>> Gil Vernik.
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 04:36:11 2015
Return-Path: <dev-return-11190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CCB581009D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 04:36:11 +0000 (UTC)
Received: (qmail 19772 invoked by uid 500); 19 Jan 2015 04:36:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19683 invoked by uid 500); 19 Jan 2015 04:36:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19670 invoked by uid 99); 19 Jan 2015 04:36:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 04:36:12 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.95 as permitted sender)
Received: from [98.139.213.95] (HELO nm8-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.95)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 04:35:44 +0000
Received: from [98.139.212.151] by nm8.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 04:35:42 -0000
Received: from [98.139.215.251] by tm8.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 04:35:42 -0000
Received: from [127.0.0.1] by omp1064.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 04:35:42 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 459630.77920.bm@omp1064.mail.bf1.yahoo.com
X-YMail-OSG: WIqAlg4VM1nz4KMXm8zDdz8a3UB0T1MXEafLaD9FkZVzoJv9KVMqUF6X95VuLMt
 t48yY23iBepX4Q_siMGwZCmyfY4ACy.s6VN0TVAarNO.OUPkcoGLH8szC.5rO0AALH54kX0.5qWp
 _9tHgYdROUi9WQWqIE1TCAlKlgNrtAbVOPnlbO9kXBmsPXiWtv_Ly3ewkjaMI9VgAHe8GtxzaAaC
 DizIS_inceNflBG.UY8p96gID50iQS4TK_3MLj3otXP2EQfNTB.HjSIxdr4RRQjqhjtulAMesmJA
 22gDYP3KPp6R_0j9YXdL1fM9Iiz45hVYsZiDyEOtekk8u59tyAxJJxtDxuFPAku6K5aMZ7ZbVsXt
 9DAlwXc0XTpQm.y5aBeqMTB0.EzukurvabZ4AahxRxBhKtJxQq7KuVXGV_AxmcMejOcYhpFy03Xf
 IwvMk5wxv8Y8OQrtNLJqS39OPgV281AN9nqbumBuPRuH_zAdTwAECBT5Gt3fCyjBCcWq9DON6AKC
 0Cv_.tqVRInxvChXhlUcGEn90HjEG6ykFG6_cztRG2tVvfNeYhBHMqVPciU22yQyJOvpQFeZxj0v
 LgzmcfTxG0503.oKlRRExCzpYm7k-
Received: by 66.196.80.121; Mon, 19 Jan 2015 04:34:02 +0000 
Date: Mon, 19 Jan 2015 04:34:02 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1022675498.3105424.1421642042334.JavaMail.yahoo@jws106121.mail.bf1.yahoo.com>
Subject: GraphX doc: triangleCount() requirement overstatement?
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

According to:
https://spark.apache.org/docs/1.2.0/graphx-programming-guide.html#triangle-counting 

"Note that TriangleCount requires the edges to be in canonical orientation (srcId < dstId)"

But isn't this overstating the requirement? Isn't the requirement really that IF there are duplicate edges between two vertices, THEN those edges must all be in the same direction (in order for the groupEdges() at the beginning of triangleCount() to produce the intermediate results that triangleCount() expects)?

If so, should I enter a JIRA ticket to clarify the documentation?

Or is it the case that https://issues.apache.org/jira/browse/SPARK-3650 will make it into Spark 1.3 anyway?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 06:07:08 2015
Return-Path: <dev-return-11191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1BC531021D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 06:07:08 +0000 (UTC)
Received: (qmail 6522 invoked by uid 500); 19 Jan 2015 06:07:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6469 invoked by uid 500); 19 Jan 2015 06:07:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6457 invoked by uid 99); 19 Jan 2015 06:07:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:07:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:06:41 +0000
Received: by mail-oi0-f48.google.com with SMTP id u20so25287705oif.7
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 22:06:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=Js85nxNU2rcStueOznmVD0GD5nUGepjFwxEPQ4xisVQ=;
        b=m0kDeC4sXrMOJr14qep7xJ1EV6Qd6No+BTn03xDSxXveHKf0edqz8EXZaP8xT9ri5W
         Kud8s9PtxbBuRFGuMwrN9rk78yQr/YajtVWgJr0267tVrab7m2MiuzCuVJ8Avgnj+amx
         xeA6xqjrN3na7fKAKMoGQr0Ih17HYnvWYYorm+bS51BUyZD4ICZ1iqGsY+ct+PfBe+iE
         t3Bl/YaPYrJyDqbzC8HJPBBRhpmbuotl9CEY8UUJ6U5WQtghXjyHwJdQoGI+mSu2Mr1B
         MGxMxWasODKzm5bYqtV54GnE7S3xNGe8ONKZVZUwhDAbeCXW5TH1xl/7K5j2riBoCH+w
         9NLA==
X-Received: by 10.202.44.210 with SMTP id s201mr5407633ois.106.1421647600277;
 Sun, 18 Jan 2015 22:06:40 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Sun, 18 Jan 2015 22:06:20 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Sun, 18 Jan 2015 22:06:20 -0800
Message-ID: <CAJc_sy+z0XAr4dMczj8WeowM8=oOZDsxV5Xb+J+eJWVtWP+zAw@mail.gmail.com>
Subject: Memory config issues
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1137c64235e1c1050cfb20b2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137c64235e1c1050cfb20b2
Content-Type: text/plain; charset=UTF-8

 All,

I'm getting out of memory exceptions in SparkSQL GROUP BY queries. I have
plenty of RAM, so I should be able to brute-force my way through, but I
can't quite figure out what memory option affects what process.

My current memory configuration is the following:
export SPARK_WORKER_MEMORY=83971m
export SPARK_DAEMON_MEMORY=15744m

What does each of these config options do exactly?

Also, how come the executors page of the web UI shows no memory usage:

0.0 B / 42.4 GB

And where does 42.4 GB come from?

Alex

--001a1137c64235e1c1050cfb20b2--

From dev-return-11192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 06:12:03 2015
Return-Path: <dev-return-11192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE13610230
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 06:12:03 +0000 (UTC)
Received: (qmail 14644 invoked by uid 500); 19 Jan 2015 06:12:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14574 invoked by uid 500); 19 Jan 2015 06:12:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13822 invoked by uid 99); 19 Jan 2015 06:12:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:12:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:11:36 +0000
Received: by mail-qc0-f173.google.com with SMTP id m20so13705581qcx.4
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 22:11:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=w4yRgh6DhcZfHlYYT9+gb7WOVPnA0zQSG1pCtlt5ci8=;
        b=HLSi6BJWDM7xnW1PYwUFRl/fLrlw4aUXGojx4yRrryKRPqhamttdtjEyj4vWWua/2m
         7DDv11O7sUoX8vUiCR8LSVCckDiCN5DMFJl6iDSqQAmTitvB0hzmfFULpjTWH07yqG0G
         wrPjwhFIjUMngIgUQ6Ceah5egbIuwhxunjmG1CE9A7X/O57PXN/Tj8cXZ89eiuR7dZ+I
         SwqUr5jrvmr4oMkqViIu8P2vtWBa/tiSVTTUyxUwr2JJfL24tekiXJSYYJddFF5psroe
         ube09bFS1SIPr2QmTGrBjiM8FLnnmHYfJavmVaYmRrTTurRnoWIdlF07A3RweZTpbAxN
         hkYg==
X-Gm-Message-State: ALoCoQlOSy2gv7pudGKyIiJ9Z4DrEgcH3Whu1HxXhF5XlHhcP63xBIJHOHrZhIvcf0ZiYmdHh4W5
X-Received: by 10.224.127.193 with SMTP id h1mr32262175qas.98.1421647873053;
 Sun, 18 Jan 2015 22:11:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Sun, 18 Jan 2015 22:10:52 -0800 (PST)
In-Reply-To: <1022675498.3105424.1421642042334.JavaMail.yahoo@jws106121.mail.bf1.yahoo.com>
References: <1022675498.3105424.1421642042334.JavaMail.yahoo@jws106121.mail.bf1.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 18 Jan 2015 22:10:52 -0800
Message-ID: <CAPh_B=aiXpoeRAnHs1K6vZRiEqg_fmSMhr5rudqvvF6hzVRraw@mail.gmail.com>
Subject: Re: GraphX doc: triangleCount() requirement overstatement?
To: Michael Malak <michaelmalak@yahoo.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ebe2782c8b050cfb30d2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ebe2782c8b050cfb30d2
Content-Type: text/plain; charset=UTF-8

We will merge https://issues.apache.org/jira/browse/SPARK-3650  for 1.3.
Thanks for reminding!


On Sun, Jan 18, 2015 at 8:34 PM, Michael Malak <
michaelmalak@yahoo.com.invalid> wrote:

> According to:
>
> https://spark.apache.org/docs/1.2.0/graphx-programming-guide.html#triangle-counting
>
> "Note that TriangleCount requires the edges to be in canonical orientation
> (srcId < dstId)"
>
> But isn't this overstating the requirement? Isn't the requirement really
> that IF there are duplicate edges between two vertices, THEN those edges
> must all be in the same direction (in order for the groupEdges() at the
> beginning of triangleCount() to produce the intermediate results that
> triangleCount() expects)?
>
> If so, should I enter a JIRA ticket to clarify the documentation?
>
> Or is it the case that https://issues.apache.org/jira/browse/SPARK-3650
> will make it into Spark 1.3 anyway?
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1132ebe2782c8b050cfb30d2--

From dev-return-11193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 06:30:20 2015
Return-Path: <dev-return-11193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F063E1027E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 06:30:19 +0000 (UTC)
Received: (qmail 36613 invoked by uid 500); 19 Jan 2015 06:30:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36507 invoked by uid 500); 19 Jan 2015 06:30:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36414 invoked by uid 99); 19 Jan 2015 06:30:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:30:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:30:12 +0000
Received: by mail-la0-f43.google.com with SMTP id q1so11546099lam.2
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 22:29:31 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=yjBvFMZb5tPM8x63G5hMRwzqaC0GTGmVeJI5+du5gug=;
        b=ahfezhtGuYu9usOrExHTf6svTwVJk3AnOjb/hFZ8k+h/UnsLCm1op9EOSWzPOccmI7
         PPlC/dHD+5a6FI4D6i2HszQ2CpqWW/S1mMhsPK9mmWwJ5AAvh5IH4B1TH1Qpua7i05sD
         yd/Qb2/P7ODH50/Nau5jY983TqV+cnChEMKLmbyNU/HGF9LuK2SX+zr+aqlWqhrw6L8+
         yiufj/kh/cko3faXVid+7buCHlYpo380Cjtrw48Z4KxcAFmvBFDbYsOLKZ50fXzzn/DJ
         8LdknKoJ4FV4XN+BTnBRpyoMOTVuSTb+EnfmTkLtn6kWdyE6qH2g842tAvVGmviwpFS9
         IHxw==
X-Gm-Message-State: ALoCoQnx0qXt1fO+AaOj9x1Zbrorprrpc0u1Vlz9bqjYLQbs1S0ddEwjSp50DpUqqLM8QA6v5KGH
MIME-Version: 1.0
X-Received: by 10.152.234.140 with SMTP id ue12mr21891197lac.78.1421648971143;
 Sun, 18 Jan 2015 22:29:31 -0800 (PST)
Received: by 10.152.114.10 with HTTP; Sun, 18 Jan 2015 22:29:31 -0800 (PST)
In-Reply-To: <CAJc_sy+z0XAr4dMczj8WeowM8=oOZDsxV5Xb+J+eJWVtWP+zAw@mail.gmail.com>
References: <CAJc_sy+z0XAr4dMczj8WeowM8=oOZDsxV5Xb+J+eJWVtWP+zAw@mail.gmail.com>
Date: Mon, 19 Jan 2015 11:59:31 +0530
Message-ID: <CAHUQ+_ZDY0M=0e4DxHVtefD0AGVAB7tETzmRo6yp5ev_7P_mYw@mail.gmail.com>
Subject: Re: Memory config issues
From: Akhil Das <akhil@sigmoidanalytics.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a55eebae90050cfb716e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a55eebae90050cfb716e
Content-Type: text/plain; charset=UTF-8

Its the executor memory (spark.executor.memory) which you can set while
creating the spark context. By default it uses 0.6% of the executor memory
for Storage. Now, to show some memory usage, you need to cache (persist)
the RDD. Regarding the OOM Exception, you can increase the level of
parallelism (also you can increase the number of partitions depending on
your data size) and it should be fine.

Thanks
Best Regards

On Mon, Jan 19, 2015 at 11:36 AM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

>  All,
>
> I'm getting out of memory exceptions in SparkSQL GROUP BY queries. I have
> plenty of RAM, so I should be able to brute-force my way through, but I
> can't quite figure out what memory option affects what process.
>
> My current memory configuration is the following:
> export SPARK_WORKER_MEMORY=83971m
> export SPARK_DAEMON_MEMORY=15744m
>
> What does each of these config options do exactly?
>
> Also, how come the executors page of the web UI shows no memory usage:
>
> 0.0 B / 42.4 GB
>
> And where does 42.4 GB come from?
>
> Alex
>

--001a1133a55eebae90050cfb716e--

From dev-return-11194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 06:55:10 2015
Return-Path: <dev-return-11194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1480210308
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 06:55:10 +0000 (UTC)
Received: (qmail 78217 invoked by uid 500); 19 Jan 2015 06:55:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78140 invoked by uid 500); 19 Jan 2015 06:55:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78121 invoked by uid 99); 19 Jan 2015 06:55:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:55:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.44 as permitted sender)
Received: from [209.85.218.44] (HELO mail-oi0-f44.google.com) (209.85.218.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 06:54:42 +0000
Received: by mail-oi0-f44.google.com with SMTP id a3so1691331oib.3
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 22:54:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=D8Da/SO2l1eNJFGdViw762ZOmVd4y4xJfMRMGakhYKY=;
        b=qATTfu3dpAf1gIpbdVgbR5TfPLKmFguexWwNJd8N27GaSlrrpvf0tghA4jtyBAzINC
         mSI/uc4/XPF4v1XXD4BG+2q3tjfctfXVHLlJP6f3yWi6BSddq2c8P+x/LWV0r3gfAdmp
         8UkXUFQgrf7MiogKI3A1HMunj0P+HoA4wvylOD66tzaH+8EmK9mzruKEMcmvHyH4/QHw
         nYlb2gB1CiwSAH+HazturAkVQsMnNaJVGC0Yi+sZy4PoJnAFmCOeig+/wFPbB5kL9KEZ
         v+3uMfe0O9myzk3SshSzlY1wZUNzGkFVuUPpwdBJTTScC9xSrzlZEI/F+IjeL+7FY7/w
         OjzA==
X-Received: by 10.202.44.210 with SMTP id s201mr5499560ois.106.1421650481098;
 Sun, 18 Jan 2015 22:54:41 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Sun, 18 Jan 2015 22:54:20 -0800 (PST)
In-Reply-To: <CAHUQ+_ZDY0M=0e4DxHVtefD0AGVAB7tETzmRo6yp5ev_7P_mYw@mail.gmail.com>
References: <CAJc_sy+z0XAr4dMczj8WeowM8=oOZDsxV5Xb+J+eJWVtWP+zAw@mail.gmail.com>
 <CAHUQ+_ZDY0M=0e4DxHVtefD0AGVAB7tETzmRo6yp5ev_7P_mYw@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Sun, 18 Jan 2015 22:54:20 -0800
Message-ID: <CAJc_syL7LqGZ9wJ3J2JJZ-mAWtQaQJOj1EHFJpD4Lwxkbrk5Lw@mail.gmail.com>
Subject: Re: Memory config issues
To: Akhil Das <akhil@sigmoidanalytics.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1137c642ebb59e050cfbcb33
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137c642ebb59e050cfbcb33
Content-Type: text/plain; charset=UTF-8

Akhil,

Ah, very good point. I guess "SET spark.sql.shuffle.partitions=1024" should
do it.

Alex

On Sun, Jan 18, 2015 at 10:29 PM, Akhil Das <akhil@sigmoidanalytics.com>
wrote:

> Its the executor memory (spark.executor.memory) which you can set while
> creating the spark context. By default it uses 0.6% of the executor memory
> for Storage. Now, to show some memory usage, you need to cache (persist)
> the RDD. Regarding the OOM Exception, you can increase the level of
> parallelism (also you can increase the number of partitions depending on
> your data size) and it should be fine.
>
> Thanks
> Best Regards
>
> On Mon, Jan 19, 2015 at 11:36 AM, Alessandro Baretta <
> alexbaretta@gmail.com> wrote:
>
>>  All,
>>
>> I'm getting out of memory exceptions in SparkSQL GROUP BY queries. I have
>> plenty of RAM, so I should be able to brute-force my way through, but I
>> can't quite figure out what memory option affects what process.
>>
>> My current memory configuration is the following:
>> export SPARK_WORKER_MEMORY=83971m
>> export SPARK_DAEMON_MEMORY=15744m
>>
>> What does each of these config options do exactly?
>>
>> Also, how come the executors page of the web UI shows no memory usage:
>>
>> 0.0 B / 42.4 GB
>>
>> And where does 42.4 GB come from?
>>
>> Alex
>>
>
>

--001a1137c642ebb59e050cfbcb33--

From dev-return-11195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 07:41:04 2015
Return-Path: <dev-return-11195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98EEC1046A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 07:41:04 +0000 (UTC)
Received: (qmail 22596 invoked by uid 500); 19 Jan 2015 07:41:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22514 invoked by uid 500); 19 Jan 2015 07:41:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22395 invoked by uid 99); 19 Jan 2015 07:41:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 07:41:03 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 07:41:00 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so24847346qcv.1
        for <dev@spark.apache.org>; Sun, 18 Jan 2015 23:39:33 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=wfd61pfnz4LQFpUcwuLbzM+NiRugWP1ItBtinIoQBYI=;
        b=f3+X1qjjUL1U9jBiYVkUbpoD+xdngC0uZ5lSLJIjOf05+UlqnVII4+5acbTwFaVXIB
         3sCGUDnSML6lW0F5+aS0jjkNClxeNAE3Bcqnlw3v83nthImb6PfcVvo915TY49LEqakj
         K2RKgtdaTgJQ9xcX19xmDNhZupPt6d7Nvv8lK5m6gcj8rdT3s3YiHjxU/Zyp4ew2BdKu
         MXjctlZtJekq0WbLm/pj564eTi2lPM+tdmW74dvqKXmz0cX4ui9wwwVit/BsMxShGkwm
         KY1nTCaTa3uutskLUYW8drbsF2XaQAPII39Gfjruedd3a1dyDqGiiKltlcA4L0UNKceQ
         6Q2A==
X-Gm-Message-State: ALoCoQknJbnNLXfThw3MQmkGB4/1Iuq3EZPrQw4MwUz4ysrYS362ZboDScTL/Oj1DVOMeK80UBoe
X-Received: by 10.224.127.193 with SMTP id h1mr32650484qas.98.1421653173495;
 Sun, 18 Jan 2015 23:39:33 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Sun, 18 Jan 2015 23:39:13 -0800 (PST)
In-Reply-To: <54B9A097.6020100@ugent.be>
References: <54B93BB3.10401@ugent.be> <CAPh_B=ZzCLJAus-7Jt2VYMkngZkDyMbsqdKt4DZEHcGZCwxswQ@mail.gmail.com>
 <54B9A097.6020100@ugent.be>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 18 Jan 2015 23:39:13 -0800
Message-ID: <CAPh_B=YygEtKXmrPD9AnNhDM3Epy6Sje1M3Rsb4ha=KNW5WLQQ@mail.gmail.com>
Subject: Re: RDD order guarantees
To: Ewan Higgs <ewan.higgs@ugent.be>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Aaron Davidson <aaron@databricks.com>
Content-Type: multipart/alternative; boundary=001a1132ebe2667ce4050cfc6caa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ebe2667ce4050cfc6caa
Content-Type: text/plain; charset=UTF-8

Hi Ewan,

Not sure if there is a JIRA ticket (there are too many that I lose track).

I chatted briefly with Aaron on this. The way we can solve it is to create
a new FileSystem implementation that overrides the listStatus method, and
then in Hadoop Conf set the fs.file.impl to that.

Shouldn't be too hard. Would you be interested in working on it?




On Fri, Jan 16, 2015 at 3:36 PM, Ewan Higgs <ewan.higgs@ugent.be> wrote:

>  Yes, I am running on a local file system.
>
> Is there a bug open for this? Mingyu Kim reported the problem last April:
>
> http://apache-spark-user-list.1001560.n3.nabble.com/Spark-reads-partitions-in-a-wrong-order-td4818.html
>
> -Ewan
>
>
> On 01/16/2015 07:41 PM, Reynold Xin wrote:
>
> You are running on a local file system right? HDFS orders the file based
> on names, but local file system often don't. I think that's why the
> difference.
>
>  We might be able to do a sort and order the partitions when we create a
> RDD to make this universal though.
>
> On Fri, Jan 16, 2015 at 8:26 AM, Ewan Higgs <ewan.higgs@ugent.be> wrote:
>
>> Hi all,
>> Quick one: when reading files, are the orders of partitions guaranteed to
>> be preserved? I am finding some weird behaviour where I run sortByKeys() on
>> an RDD (which has 16 byte keys) and write it to disk. If I open a python
>> shell and run the following:
>>
>> for part in range(29):
>>     print map(ord,
>> open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part),
>> 'r').read(16))
>>
>> Then each partition is in order based on the first value of each
>> partition.
>>
>> I can also call TeraValidate.validate from TeraSort and it is happy with
>> the results. It seems to be on loading the file that the reordering
>> happens. If this is expected, is there a way to ask Spark nicely to give me
>> the RDD in the order it was saved?
>>
>> This is based on trying to fix my TeraValidate code on this branch:
>> https://github.com/ehiggs/spark/tree/terasort
>>
>> Thanks,
>> Ewan
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>
>

--001a1132ebe2667ce4050cfc6caa--

From dev-return-11196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 09:03:47 2015
Return-Path: <dev-return-11196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 461A0106F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 09:03:47 +0000 (UTC)
Received: (qmail 31718 invoked by uid 500); 19 Jan 2015 09:03:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31616 invoked by uid 500); 19 Jan 2015 09:03:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30409 invoked by uid 99); 19 Jan 2015 09:03:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:03:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:03:17 +0000
Received: by mail-pd0-f179.google.com with SMTP id v10so17228757pde.10;
        Mon, 19 Jan 2015 01:01:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:organization:user-agent:mime-version:to
         :subject:content-type;
        bh=9BwCDhzCYIb3nOb6IkuNYc8jVkxABaNAiZVNs5CVfz8=;
        b=TOIyyxkY4JrT83mjBIBn0UxSqm9V3CXLU69I+zHs7UU9UKKHWITPtocG9PTQNJzZ0X
         b4lwsTgKlBYZjV1LhDEMOt5SgCIy5QzczyUeewZu3i5E9TcZQkR+yI9kh8dNoIB5ciy4
         6uQeeAF41USJ8HtQFiFonQe54EhN9ldT6y1YyhSdu0l8isUCFs6J392RNYzJmy7jk/l3
         3nN+vPO7y9IgDUqqGi6yMUaYQKA/RCqYDHiB3OWax10c6zp2qual8HFjcyaqUBcQ++Id
         gtC4WVZqbQexg6+p6O13J7f/AJVIZURUC+qKSsjb95lDzeqIjoTHCTIr4rK56sulCaE8
         GXIg==
X-Received: by 10.70.56.34 with SMTP id x2mr25495822pdp.127.1421658060102;
        Mon, 19 Jan 2015 01:01:00 -0800 (PST)
Received: from [192.168.99.2] ([199.101.117.133])
        by mx.google.com with ESMTPSA id bx13sm11058900pdb.19.2015.01.19.01.00.58
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 19 Jan 2015 01:00:59 -0800 (PST)
Message-ID: <54BCC7C8.8060403@gmail.com>
Date: Mon, 19 Jan 2015 17:00:56 +0800
From: Yi Tian <tianyi.asiainfo@gmail.com>
Organization: Asiainfo.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: user@spark.apache.org, dev@spark.apache.org
Subject: Is there any way to support multiple users executing SQL on thrift
 server?
Content-Type: multipart/alternative;
 boundary="------------010906020405090301020101"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------010906020405090301020101
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Is there any way to support multiple users executing SQL on one thrift 
server?

I think there are some problems for spark 1.2.0, for example:

 1. Start thrift server with user A
 2. Connect to thrift server via beeline with user B
 3. Execute “insert into table dest select … from table src”

then we found these items on hdfs:

|drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000
drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary
drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0
drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/_temporary
drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000
-rw-r--r--   3 A supergroup       2671 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000/part-00000
|

You can see all the temporary path created on driver side (thrift server 
side) is owned by user B (which is what we expected).

But all the output data created on executor side is owned by user A, 
(which is NOT what we expected).
error owner of the output data cause 
|org.apache.hadoop.security.AccessControlException| while the driver 
side moving output data into |dest| table.

Is anyone know how to resolve this problem?

​

--------------010906020405090301020101--

From dev-return-11197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 09:05:16 2015
Return-Path: <dev-return-11197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 79F4B106FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 09:05:16 +0000 (UTC)
Received: (qmail 35156 invoked by uid 500); 19 Jan 2015 09:05:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35083 invoked by uid 500); 19 Jan 2015 09:05:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35066 invoked by uid 99); 19 Jan 2015 09:05:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:05:10 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of michael.belldavies@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:05:05 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 4C95410F98F7
	for <dev@spark.apache.org>; Mon, 19 Jan 2015 01:04:16 -0800 (PST)
Date: Mon, 19 Jan 2015 02:04:14 -0700 (MST)
From: Mick Davies <michael.belldavies@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421658254509-10189.post@n3.nabble.com>
In-Reply-To: <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
References: <1421425027439-10141.post@n3.nabble.com> <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
Subject: Re: Optimize encoding/decoding strings when using Parquet
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Added a JIRA to track
https://issues.apache.org/jira/browse/SPARK-5309



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10189.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 09:22:13 2015
Return-Path: <dev-return-11198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 061BE10778
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 09:22:13 +0000 (UTC)
Received: (qmail 65731 invoked by uid 500); 19 Jan 2015 09:22:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65653 invoked by uid 500); 19 Jan 2015 09:22:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65641 invoked by uid 99); 19 Jan 2015 09:22:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:22:13 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 09:21:48 +0000
Received: by mail-we0-f170.google.com with SMTP id x3so3804548wes.1
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 01:21:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=a3zuPnX7QUxIbBQo5HJ9+CjMFllDu91jCjMtd5lI4SU=;
        b=IDtaLW0jC74DPfRILyH5X4XlPwaQQLvaQZ4nzJJ8+62TVIN/47PcXT3IcU3l5uv/Nq
         2atJ1cWb3Jsx5QVEV9Jl6YnX7nIsx+Ee40gWy4TMN7zortqVvd8KBSh1S27cT6Z0Ne20
         ho4id+wjr6sLvSrkhJv3PrAvWianDNUA5LLzOoeDtBsnNZzWa/gWnfgHIq9zs6xH2myE
         tRuemrnV4iXWt+bHhTYMpJ6J4U54y/U9eHP9E3wiheRmHCI31Csmpd/dRiFd1d7L5t3l
         s4J1iGd9ecEc+grCbhQDGhwr2sbCbaqahjZeXXHkMBr6yo+eIMnlmpzYAgRjVTHscwtJ
         H44A==
X-Gm-Message-State: ALoCoQn2rUY4sRA6QJACNxuLNc/0MlpK9x+G9CP5Y/xNgw0m4EeK4WMp1DDbcMnZIWpwskbbWsKZ
X-Received: by 10.194.10.68 with SMTP id g4mr42151340wjb.5.1421659262373; Mon,
 19 Jan 2015 01:21:02 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Mon, 19 Jan 2015 01:20:42 -0800 (PST)
In-Reply-To: <CAHUQ+_ZDY0M=0e4DxHVtefD0AGVAB7tETzmRo6yp5ev_7P_mYw@mail.gmail.com>
References: <CAJc_sy+z0XAr4dMczj8WeowM8=oOZDsxV5Xb+J+eJWVtWP+zAw@mail.gmail.com>
 <CAHUQ+_ZDY0M=0e4DxHVtefD0AGVAB7tETzmRo6yp5ev_7P_mYw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 19 Jan 2015 09:20:42 +0000
Message-ID: <CAMAsSdK5452rpzGNEWowr2UOXqqvgOZD=4LkLwtsEFGuG5zpEw@mail.gmail.com>
Subject: Re: Memory config issues
To: Akhil Das <akhil@sigmoidanalytics.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Jan 19, 2015 at 6:29 AM, Akhil Das <akhil@sigmoidanalytics.com> wrote:
> Its the executor memory (spark.executor.memory) which you can set while
> creating the spark context. By default it uses 0.6% of the executor memory

(Uses 0.6 or 60%)

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 10:41:07 2015
Return-Path: <dev-return-11199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 769F210A33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 10:41:07 +0000 (UTC)
Received: (qmail 45212 invoked by uid 500); 19 Jan 2015 10:41:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45134 invoked by uid 500); 19 Jan 2015 10:41:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45122 invoked by uid 99); 19 Jan 2015 10:41:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 10:41:06 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of scrapcodes@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 10:41:02 +0000
Received: by mail-we0-f182.google.com with SMTP id l61so5366279wev.13
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 02:40:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=XU/LI8uEJBnxzW6OFNc4OVM3AZN8wVEo8a7lHMEGq5s=;
        b=vYACq6AuDfDZdNfNpcjdHzR9dLVMqJfXIwveprfWC/zvuUVm/vTJSifVztmOx4vuC6
         YxkA+FlQW9ojKX2aivu0sQOiDXANgP9Mam2IZI94o7E6cDlglL7C6FUnusipAjGzRUCy
         asBfbi6II9Vbetj2Y5hNVvcHgWy0q4t6zbCc5FmzL3lSAgOS2O5ET5TIC/gztppiB4ZM
         k60eon/z0Rz1+KWWp/yub/4ElBsbdTUEtU5R/ufr7Bgma46EDT44kVPl1n8Ri5XtEjxK
         xm/qYfxYaK4pSWpnVoWRF8CgHmSO9C2R4p+lnhc9/2zySHYF3kdlLlbgTWKHV+f2+e0K
         tCzw==
X-Received: by 10.180.88.33 with SMTP id bd1mr33681028wib.10.1421664041625;
 Mon, 19 Jan 2015 02:40:41 -0800 (PST)
MIME-Version: 1.0
Received: by 10.216.66.137 with HTTP; Mon, 19 Jan 2015 02:40:21 -0800 (PST)
In-Reply-To: <CAPh_B=bWNhPy6B8Ay=bA24jftuShxLhrz2FZ9b-sXjVK6p-ZTQ@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
 <1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com> <CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
 <CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
 <FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com> <CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
 <CAPh_B=bWNhPy6B8Ay=bA24jftuShxLhrz2FZ9b-sXjVK6p-ZTQ@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Mon, 19 Jan 2015 16:10:21 +0530
Message-ID: <CAOYDGoBixc8O2qQv5sYbx58_oJiCgmB2r01N0P8Z=iT9GrbiXg@mail.gmail.com>
Subject: Re: Semantics of LGTM
To: Reynold Xin <rxin@databricks.com>
Cc: Kay Ousterhout <keo@eecs.berkeley.edu>, Sandy Ryza <sandy.ryza@cloudera.com>, 
	Patrick Wendell <pwendell@gmail.com>, Aaron Davidson <ilikerps@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0444e8fd30f049050cfef48b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444e8fd30f049050cfef48b
Content-Type: text/plain; charset=UTF-8

Patrick's original proposal LGTM :).  However until now, I have been in the
impression of LGTM with special emphasis on TM part. That said, I will be
okay/happy(or Responsible ) for the patch, if it goes in.

Prashant Sharma



On Sun, Jan 18, 2015 at 2:33 PM, Reynold Xin <rxin@databricks.com> wrote:

> Maybe just to avoid LGTM as a single token when it is not actually
> according to Patrick's definition, but anybody can still leave comments
> like:
>
> "The direction of the PR looks good to me." or "+1 on the direction"
>
> "The build part looks good to me"
>
> ...
>
>
> On Sat, Jan 17, 2015 at 8:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
> wrote:
>
> > +1 to Patrick's proposal of strong LGTM semantics.  On past projects,
> I've
> > heard the semantics of "LGTM" expressed as "I've looked at this
> thoroughly
> > and take as much ownership as if I wrote the patch myself".  My
> > understanding is that this is the level of review we expect for all
> patches
> > that ultimately go into Spark, so it's important to have a way to
> concisely
> > describe when this has been done.
> >
> > Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
> > cases I've seen, if someone else says "I looked at this very quickly and
> > didn't see any glaring problems", it doesn't add any value for subsequent
> > reviewers (someone still needs to take a thorough look).
> >
> > -Kay
> >
> > On Sat, Jan 17, 2015 at 8:04 PM, <sandy.ryza@cloudera.com> wrote:
> >
> > > Yeah, the ASF +1 has become partly overloaded to mean both "I would
> like
> > > to see this feature" and "this patch should be committed", although, at
> > > least in Hadoop, using +1 on JIRA (as opposed to, say, in a release
> vote)
> > > should unambiguously mean the latter unless qualified in some other
> way.
> > >
> > > I don't have any opinion on the specific characters, but I agree with
> > > Aaron that it would be nice to have some sort of abbreviation for both
> > the
> > > strong and weak forms of approval.
> > >
> > > -Sandy
> > >
> > > > On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > >
> > > > I think the ASF +1 is *slightly* different than Google's LGTM,
> because
> > > > it might convey wanting the patch/feature to be merged but not
> > > > necessarily saying you did a thorough review and stand behind it's
> > > > technical contents. For instance, I've seen people pile on +1's to
> try
> > > > and indicate support for a feature or patch in some projects, even
> > > > though they didn't do a thorough technical review. This +1 is
> > > > definitely a useful mechanism.
> > > >
> > > > There is definitely much overlap though in the meaning, though, and
> > > > it's largely because Spark had it's own culture around reviews before
> > > > it was donated to the ASF, so there is a mix of two styles.
> > > >
> > > > Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
> > > > proposed originally (unlike the one Sandy proposed, e.g.). This is
> > > > what I've seen every project using the LGTM convention do (Google,
> and
> > > > some open source projects such as Impala) to indicate technical
> > > > sign-off.
> > > >
> > > > - Patrick
> > > >
> > > >> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson <ilikerps@gmail.com
> >
> > > wrote:
> > > >> I think I've seen something like +2 = "strong LGTM" and +1 = "weak
> > LGTM;
> > > >> someone else should review" before. It's nice to have a shortcut
> which
> > > isn't
> > > >> a sentence when talking about weaker forms of LGTM.
> > > >>
> > > >> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
> > > >>>
> > > >>> I think clarifying these semantics is definitely worthwhile. Maybe
> > this
> > > >>> complicates the process with additional terminology, but the way
> I've
> > > used
> > > >>> these has been:
> > > >>>
> > > >>> +1 - I think this is safe to merge and, barring objections from
> > others,
> > > >>> would merge it immediately.
> > > >>>
> > > >>> LGTM - I have no concerns about this patch, but I don't necessarily
> > > feel
> > > >>> qualified to make a final call about it.  The TM part acknowledges
> > the
> > > >>> judgment as a little more subjective.
> > > >>>
> > > >>> I think having some concise way to express both of these is useful.
> > > >>>
> > > >>> -Sandy
> > > >>>
> > > >>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > > >>>>
> > > >>>> Hey All,
> > > >>>>
> > > >>>> Just wanted to ping about a minor issue - but one that ends up
> > having
> > > >>>> consequence given Spark's volume of reviews and commits. As much
> as
> > > >>>> possible, I think that we should try and gear towards "Google
> Style"
> > > >>>> LGTM on reviews. What I mean by this is that LGTM has the
> following
> > > >>>> semantics:
> > > >>>>
> > > >>>> "I know this code well, or I've looked at it close enough to feel
> > > >>>> confident it should be merged. If there are issues/bugs with this
> > code
> > > >>>> later on, I feel confident I can help with them."
> > > >>>>
> > > >>>> Here is an alternative semantic:
> > > >>>>
> > > >>>> "Based on what I know about this part of the code, I don't see any
> > > >>>> show-stopper problems with this patch".
> > > >>>>
> > > >>>> The issue with the latter is that it ultimately erodes the
> > > >>>> significance of LGTM, since subsequent reviewers need to reason
> > about
> > > >>>> what the person meant by saying LGTM. In contrast, having strong
> > > >>>> semantics around LGTM can help streamline reviews a lot,
> especially
> > as
> > > >>>> reviewers get more experienced and gain trust from the
> comittership.
> > > >>>>
> > > >>>> There are several easy ways to give a more limited endorsement of
> a
> > > >>>> patch:
> > > >>>> - "I'm not familiar with this code, but style, etc look good"
> > (general
> > > >>>> endorsement)
> > > >>>> - "The build changes in this code LGTM, but I haven't reviewed the
> > > >>>> rest" (limited LGTM)
> > > >>>>
> > > >>>> If people are okay with this, I might add a short note on the
> wiki.
> > > >>>> I'm sending this e-mail first, though, to see whether anyone wants
> > to
> > > >>>> express agreement or disagreement with this approach.
> > > >>>>
> > > >>>> - Patrick
> > > >>>>
> > > >>>>
> > ---------------------------------------------------------------------
> > > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > >>>> For additional commands, e-mail: dev-help@spark.apache.org
> > > >>>
> > > >>>
> ---------------------------------------------------------------------
> > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > > >>
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > >
> >
>

--f46d0444e8fd30f049050cfef48b--

From dev-return-11200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 14:01:28 2015
Return-Path: <dev-return-11200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D463217303
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 14:01:28 +0000 (UTC)
Received: (qmail 9820 invoked by uid 500); 19 Jan 2015 14:01:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9743 invoked by uid 500); 19 Jan 2015 14:01:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9729 invoked by uid 99); 19 Jan 2015 14:01:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 14:01:29 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of romi@totango.com does not designate 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 14:01:04 +0000
Received: by mail-qa0-f50.google.com with SMTP id k15so24003644qaq.9
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 06:00:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=7uVZ6oH1Fe6B03ix06RU0rLuT3CV+R0VOSobZtPM+vo=;
        b=fXIBMGMxihaf4xFBNvuG7y+xYqkh/ZStY4tnteVqRJ6/MTQBlfZAK3JNSaRKTwtPb+
         BmsZArOdnsLwVeytmvvnMpN/XH/mF+gkjn0Yl0/W091yjx817aLMZxSMwSnqpf7pnszJ
         Y65fZKmKcwFG10II6z8RZr3+KSMsThv4ayFEkQ9X1m8hCD+dvRxB2kl/PpFSj1K/POPl
         KDb4/xcOaVb5I7zspRo9eRCcS/r5jyllHhuh1PxiCMe9zSYgR61+axpLWVeDbA/ICF8B
         juloJ/kySbNDAmtL2gsmrTnfxjzjW28Mg2GN0SRGmoVInShBx4Xx1azxserj3iqf4WMo
         Zh/A==
X-Gm-Message-State: ALoCoQkLDhKYKd4kOMWdbiwcsnXwHCLM2xb/zxRxq3siRVAgil8PCkW6fjAPCWqUJe21zy7VhCtV
X-Received: by 10.229.248.69 with SMTP id mf5mr47834384qcb.29.1421676017055;
 Mon, 19 Jan 2015 06:00:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.101.180 with HTTP; Mon, 19 Jan 2015 05:59:36 -0800 (PST)
In-Reply-To: <1421322724150-10122.post@n3.nabble.com>
References: <1421322724150-10122.post@n3.nabble.com>
From: Romi Kuntsman <romi@totango.com>
Date: Mon, 19 Jan 2015 15:59:36 +0200
Message-ID: <CACNcKkEQLUq90_JzvwDZQcZGqagjWKgZOaTRscApL10RGX+_Aw@mail.gmail.com>
Subject: Re: Spark client reconnect to driver in yarn-cluster deployment mode
To: preeze <etander@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113340dafb920b050d01bd1a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113340dafb920b050d01bd1a
Content-Type: text/plain; charset=UTF-8

"in yarn-client mode it only controls the environment of the executor
launcher"

So you either use yarn-client mode, and then your app keeps running and
controlling the process
Or you use yarn-cluster mode, and then you send a jar to YARN, and that jar
should have code to report the result back to you

*Romi Kuntsman*, *Big Data Engineer*
 http://www.totango.com

On Thu, Jan 15, 2015 at 1:52 PM, preeze <etander@gmail.com> wrote:

> From the official spark documentation
> (http://spark.apache.org/docs/1.2.0/running-on-yarn.html):
>
> "In yarn-cluster mode, the Spark driver runs inside an application master
> process which is managed by YARN on the cluster, and the client can go away
> after initiating the application."
>
> Is there any designed way that the client connects back to the driver
> (still
> running in YARN) for collecting results at a later stage?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-client-reconnect-to-driver-in-yarn-cluster-deployment-mode-tp10122.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113340dafb920b050d01bd1a--

From dev-return-11201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 14:39:12 2015
Return-Path: <dev-return-11201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4486117447
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 14:39:12 +0000 (UTC)
Received: (qmail 14359 invoked by uid 500); 19 Jan 2015 14:39:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14283 invoked by uid 500); 19 Jan 2015 14:39:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14264 invoked by uid 99); 19 Jan 2015 14:39:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 14:39:12 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of michael.belldavies@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 14:38:46 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 9D19D1100860
	for <dev@spark.apache.org>; Mon, 19 Jan 2015 06:38:46 -0800 (PST)
Date: Mon, 19 Jan 2015 07:38:44 -0700 (MST)
From: Mick Davies <michael.belldavies@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421678324708-10193.post@n3.nabble.com>
In-Reply-To: <1421658254509-10189.post@n3.nabble.com>
References: <1421425027439-10141.post@n3.nabble.com> <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com> <1421658254509-10189.post@n3.nabble.com>
Subject: Re: Optimize encoding/decoding strings when using Parquet
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Here are some timings showing effect of caching last Binary->String
conversion. Query times are reduced significantly and variation in timings
due to reduction in garbage is very significant.

Set of sample queries selecting various columns, applying some filtering and
then aggregating

Spark 1.2.0
Query 1 mean time 8353.3 millis, std deviation 480.91511147441025 millis
Query 2 mean time 8677.6 millis, std deviation 3193.345518417949 millis
Query 3 mean time 11302.5 millis, std deviation 2989.9406998950476 millis
Query 4 mean time 10537.0 millis, std deviation 5166.024024549462 millis
Query 5 mean time 9559.9 millis, std deviation 4141.487667493409 millis
Query 6 mean time 12638.1 millis, std deviation 3639.4505522430477 millis


Spark 1.2.0 - cache last Binary->String conversion
Query 1 mean time 5118.9 millis, std deviation 549.6670608448152 millis
Query 2 mean time 3761.3 millis, std deviation 202.57785883183013 millis
Query 3 mean time 7358.8 millis, std deviation 242.58918176850162 millis
Query 4 mean time 4173.5 millis, std deviation 179.802515122688 millis
Query 5 mean time 3857.0 millis, std deviation 140.71957930579526 millis
Query 6 mean time 7512.0 millis, std deviation 198.32633040858022 millis




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10193.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 15:05:36 2015
Return-Path: <dev-return-11202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F1351754E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 15:05:36 +0000 (UTC)
Received: (qmail 83096 invoked by uid 500); 19 Jan 2015 15:05:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83028 invoked by uid 500); 19 Jan 2015 15:05:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83014 invoked by uid 99); 19 Jan 2015 15:05:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 15:05:36 +0000
X-ASF-Spam-Status: No, hits=1.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [157.193.49.126] (HELO smtp2.ugent.be) (157.193.49.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 15:05:32 +0000
Received: from localhost (mcheck2.ugent.be [157.193.49.249])
	by smtp2.ugent.be (Postfix) with ESMTP id 68A1312C43F;
	Mon, 19 Jan 2015 16:04:29 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp2.ugent.be ([IPv6:::ffff:157.193.49.126])
	by localhost (mcheck2.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id eWPnrsUyxl6f; Mon, 19 Jan 2015 16:04:28 +0100 (CET)
Received: from [157.193.44.243] (gast044c.ugent.be [157.193.44.243])
	(Authenticated sender: ehiggs)
	by smtp2.ugent.be (Postfix) with ESMTPSA id ADC0C12C3A0;
	Mon, 19 Jan 2015 16:04:28 +0100 (CET)
Message-ID: <54BD1CFC.1000106@ugent.be>
Date: Mon, 19 Jan 2015 16:04:28 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Reynold Xin <rxin@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, 
 Aaron Davidson <aaron@databricks.com>
Subject: Re: RDD order guarantees
References: <54B93BB3.10401@ugent.be> <CAPh_B=ZzCLJAus-7Jt2VYMkngZkDyMbsqdKt4DZEHcGZCwxswQ@mail.gmail.com> <54B9A097.6020100@ugent.be> <CAPh_B=YygEtKXmrPD9AnNhDM3Epy6Sje1M3Rsb4ha=KNW5WLQQ@mail.gmail.com>
In-Reply-To: <CAPh_B=YygEtKXmrPD9AnNhDM3Epy6Sje1M3Rsb4ha=KNW5WLQQ@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------090000060803010604060608"
X-Miltered: at jchkm1 with ID 54BD1CFC.004 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 54BD1CFC.004 from gast044c.ugent.be/gast044c.ugent.be/157.193.44.243/[157.193.44.243]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 54BD1CFC.004 on smtp2.ugent.be : j-chkmail score : . : R=. U=. O=. B=0.000 -> S=0.000
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

--------------090000060803010604060608
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Hi Reynold.
I'll take a look.

SPARK-5300 is open for this issue.
-Ewan

On 19/01/15 08:39, Reynold Xin wrote:
> Hi Ewan,
>
> Not sure if there is a JIRA ticket (there are too many that I lose track).
>
> I chatted briefly with Aaron on this. The way we can solve it is to 
> create a new FileSystem implementation that overrides the listStatus 
> method, and then in Hadoop Conf set the fs.file.impl to that.
>
> Shouldn't be too hard. Would you be interested in working on it?
>
>
>
>
> On Fri, Jan 16, 2015 at 3:36 PM, Ewan Higgs <ewan.higgs@ugent.be 
> <mailto:ewan.higgs@ugent.be>> wrote:
>
>     Yes, I am running on a local file system.
>
>     Is there a bug open for this? Mingyu Kim reported the problem last
>     April:
>     http://apache-spark-user-list.1001560.n3.nabble.com/Spark-reads-partitions-in-a-wrong-order-td4818.html
>
>     -Ewan
>
>
>     On 01/16/2015 07:41 PM, Reynold Xin wrote:
>>     You are running on a local file system right? HDFS orders the
>>     file based on names, but local file system often don't. I think
>>     that's why the difference.
>>
>>     We might be able to do a sort and order the partitions when we
>>     create a RDD to make this universal though.
>>
>>     On Fri, Jan 16, 2015 at 8:26 AM, Ewan Higgs <ewan.higgs@ugent.be
>>     <mailto:ewan.higgs@ugent.be>> wrote:
>>
>>         Hi all,
>>         Quick one: when reading files, are the orders of partitions
>>         guaranteed to be preserved? I am finding some weird behaviour
>>         where I run sortByKeys() on an RDD (which has 16 byte keys)
>>         and write it to disk. If I open a python shell and run the
>>         following:
>>
>>         for part in range(29):
>>             print map(ord,
>>         open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part),
>>         'r').read(16))
>>
>>         Then each partition is in order based on the first value of
>>         each partition.
>>
>>         I can also call TeraValidate.validate from TeraSort and it is
>>         happy with the results. It seems to be on loading the file
>>         that the reordering happens. If this is expected, is there a
>>         way to ask Spark nicely to give me the RDD in the order it
>>         was saved?
>>
>>         This is based on trying to fix my TeraValidate code on this
>>         branch:
>>         https://github.com/ehiggs/spark/tree/terasort
>>
>>         Thanks,
>>         Ewan
>>
>>         ---------------------------------------------------------------------
>>         To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>         <mailto:dev-unsubscribe@spark.apache.org>
>>         For additional commands, e-mail: dev-help@spark.apache.org
>>         <mailto:dev-help@spark.apache.org>
>>
>>
>
>


--------------090000060803010604060608--

From dev-return-11203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 18:10:28 2015
Return-Path: <dev-return-11203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB06D17E15
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 18:10:28 +0000 (UTC)
Received: (qmail 5299 invoked by uid 500); 19 Jan 2015 18:10:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5229 invoked by uid 500); 19 Jan 2015 18:10:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5218 invoked by uid 99); 19 Jan 2015 18:10:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 18:10:29 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of michael.belldavies@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 18:10:03 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5DE3A1105C7D
	for <dev@spark.apache.org>; Mon, 19 Jan 2015 10:10:03 -0800 (PST)
Date: Mon, 19 Jan 2015 11:10:01 -0700 (MST)
From: Mick Davies <michael.belldavies@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421691001338-10195.post@n3.nabble.com>
In-Reply-To: <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
References: <1421425027439-10141.post@n3.nabble.com> <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
Subject: Re: Optimize encoding/decoding strings when using Parquet
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


Looking at Parquet code - it looks like hooks are already in place to
support this.

In particular PrimitiveConverter has methods hasDictionarySupport and
addValueFromDictionary for this purpose. These are not used by
CatalystPrimitiveConverter.

I think that it would be pretty straightforward to add this. Has anyone
considered this? Shall I get a pull request  together for it.

Mick



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10195.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 18:38:20 2015
Return-Path: <dev-return-11204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 900D117FA7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 18:38:20 +0000 (UTC)
Received: (qmail 82975 invoked by uid 500); 19 Jan 2015 18:38:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82904 invoked by uid 500); 19 Jan 2015 18:38:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82887 invoked by uid 99); 19 Jan 2015 18:38:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 18:38:16 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 18:38:12 +0000
Received: by mail-qc0-f179.google.com with SMTP id w7so6986517qcr.10
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 10:37:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=p6tyrymED3RI1s24q/zax8Y1WxCaG2IBAxHANQkcGPk=;
        b=OGvjqyP079YptQQAny8bDhhUkVMR1NgcpvqkP5N299lJQTEtT1lReDNULIhau/oR82
         +tP+p4Rb3Az6cY38pjdLmleNszElmkEUcFkKBvQ7DP3sfF/QVlTCBPSuxf32Lvy5WWJS
         ZNmckrKSfG4AD+NiiG5dJGdCvoMGJTQ3ykf+izfcPKDtHpnC1syt4L9pUWLeQYaNokCz
         PhYVc0KrgWRyGH7Z4ILpsQ0TUwW2w8aTc/gmqwV98V4Jx0GolyP4wuT5Oom46gwf/U6X
         xokpTb2ey6aIRPY8QgPqU49eHAx5RyvnmrzY87cLHUuwz17MEuhRW5r7n2Ny5ISTJNW2
         LiWQ==
X-Gm-Message-State: ALoCoQm7j3AFtslsouMEFXk7V5lwG7hMUsuF0KWCkDGhcqO+SvfBbXZ6DxAtcA3hhlnJ7pMGvudX
X-Received: by 10.140.16.50 with SMTP id 47mr48372811qga.105.1421692280716;
 Mon, 19 Jan 2015 10:31:20 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Mon, 19 Jan 2015 10:31:00 -0800 (PST)
In-Reply-To: <1421691001338-10195.post@n3.nabble.com>
References: <1421425027439-10141.post@n3.nabble.com> <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com>
 <1421691001338-10195.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 Jan 2015 10:31:00 -0800
Message-ID: <CAPh_B=YUwK0iDbmKRPdqhWdOm-8QZ4_P5FFwCoe+XCs0RPdo1g@mail.gmail.com>
Subject: Re: Optimize encoding/decoding strings when using Parquet
To: Mick Davies <michael.belldavies@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1005a5f594e050d058789
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1005a5f594e050d058789
Content-Type: text/plain; charset=UTF-8

Definitely go for a pull request!


On Mon, Jan 19, 2015 at 10:10 AM, Mick Davies <michael.belldavies@gmail.com>
wrote:

>
> Looking at Parquet code - it looks like hooks are already in place to
> support this.
>
> In particular PrimitiveConverter has methods hasDictionarySupport and
> addValueFromDictionary for this purpose. These are not used by
> CatalystPrimitiveConverter.
>
> I think that it would be pretty straightforward to add this. Has anyone
> considered this? Shall I get a pull request  together for it.
>
> Mick
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10195.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c1005a5f594e050d058789--

From dev-return-11205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 20:01:06 2015
Return-Path: <dev-return-11205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E8B810512
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 20:01:06 +0000 (UTC)
Received: (qmail 16821 invoked by uid 500); 19 Jan 2015 20:01:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16745 invoked by uid 500); 19 Jan 2015 20:01:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16733 invoked by uid 99); 19 Jan 2015 20:01:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:01:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:01:03 +0000
Received: by mail-oi0-f41.google.com with SMTP id z81so544047oif.0
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 12:00:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Rz6AdJiUV3cEwdXN2PaBMU5Ki977TQAHdy2AynkL0SQ=;
        b=Fxgi9v+bhZkK/k3V1awogDxUxQ+hF863M0vX2DxgrJ7cBH70x96ISa9+h2Bz9Vt4et
         Q3At/3gX6Rp6Sfqey99LluhO/6QZBW6vcrhUBAd6ungkmNqbE5qx+0T2ldAkkMbqVVDZ
         y0tNIOCpXk9O/kyhUxFgDqcjbV0lZt4RKrb4sgfu8aNKFHAygGLhxb/cPWgn4Bru6BoZ
         fkXQwxicmqa4kHdw/CBZYWuHBv4Q1RXa0+IBUcRhfvet+jCgRk9c1w/mKL0HkoX8f9gm
         JebNC6MR5y1ue2D0mD07h4sBPcgKOyWzivYm74TldrYA/qrJ90IMTZ/w7GwbahQyvFtn
         +aIg==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr18334540oit.100.1421697642506;
 Mon, 19 Jan 2015 12:00:42 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 19 Jan 2015 12:00:42 -0800 (PST)
In-Reply-To: <CAOYDGoBixc8O2qQv5sYbx58_oJiCgmB2r01N0P8Z=iT9GrbiXg@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
	<1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
	<CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
	<CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
	<FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com>
	<CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
	<CAPh_B=bWNhPy6B8Ay=bA24jftuShxLhrz2FZ9b-sXjVK6p-ZTQ@mail.gmail.com>
	<CAOYDGoBixc8O2qQv5sYbx58_oJiCgmB2r01N0P8Z=iT9GrbiXg@mail.gmail.com>
Date: Mon, 19 Jan 2015 12:00:42 -0800
Message-ID: <CABPQxsugNmW4+KBr6px0hL9Brw1SzeDNTehkF2o8e5J7Gp51=A@mail.gmail.com>
Subject: Re: Semantics of LGTM
From: Patrick Wendell <pwendell@gmail.com>
To: Prashant Sharma <scrapcodes@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Kay Ousterhout <keo@eecs.berkeley.edu>, 
	Sandy Ryza <sandy.ryza@cloudera.com>, Aaron Davidson <ilikerps@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Okay - so given all this I was going to put the following on the wiki
tentatively:

## Reviewing Code
Community code review is Spark's fundamental quality assurance
process. When reviewing a patch, your goal should be to help
streamline the committing process by giving committers confidence this
patch has been verified by an additional party. It's encouraged to
(politely) submit technical feedback to the author to identify areas
for improvement or potential bugs.

If you feel a patch is ready for inclusion in Spark, indicate this to
committers with a comment: "I think this patch looks good". Spark uses
the LGTM convention for indicating the highest level of technical
sign-off on a patch: simply comment with the word "LGTM". An LGTM is a
strong statement, it should be interpreted as the following: "I've
looked at this thoroughly and take as much ownership as if I wrote the
patch myself". If you comment LGTM you will be expected to help with
bugs or follow-up issues on the patch. Judicious use of LGTM's is a
great way to gain credibility as a reviewer with the broader
community.

It's also welcome for reviewers to argue against the inclusion of a
feature or patch. Simply indicate this in the comments.

- Patrick

On Mon, Jan 19, 2015 at 2:40 AM, Prashant Sharma <scrapcodes@gmail.com> wrote:
> Patrick's original proposal LGTM :).  However until now, I have been in the
> impression of LGTM with special emphasis on TM part. That said, I will be
> okay/happy(or Responsible ) for the patch, if it goes in.
>
> Prashant Sharma
>
>
>
> On Sun, Jan 18, 2015 at 2:33 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> Maybe just to avoid LGTM as a single token when it is not actually
>> according to Patrick's definition, but anybody can still leave comments
>> like:
>>
>> "The direction of the PR looks good to me." or "+1 on the direction"
>>
>> "The build part looks good to me"
>>
>> ...
>>
>>
>> On Sat, Jan 17, 2015 at 8:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
>> wrote:
>>
>> > +1 to Patrick's proposal of strong LGTM semantics.  On past projects,
>> > I've
>> > heard the semantics of "LGTM" expressed as "I've looked at this
>> > thoroughly
>> > and take as much ownership as if I wrote the patch myself".  My
>> > understanding is that this is the level of review we expect for all
>> > patches
>> > that ultimately go into Spark, so it's important to have a way to
>> > concisely
>> > describe when this has been done.
>> >
>> > Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
>> > cases I've seen, if someone else says "I looked at this very quickly and
>> > didn't see any glaring problems", it doesn't add any value for
>> > subsequent
>> > reviewers (someone still needs to take a thorough look).
>> >
>> > -Kay
>> >
>> > On Sat, Jan 17, 2015 at 8:04 PM, <sandy.ryza@cloudera.com> wrote:
>> >
>> > > Yeah, the ASF +1 has become partly overloaded to mean both "I would
>> > > like
>> > > to see this feature" and "this patch should be committed", although,
>> > > at
>> > > least in Hadoop, using +1 on JIRA (as opposed to, say, in a release
>> > > vote)
>> > > should unambiguously mean the latter unless qualified in some other
>> > > way.
>> > >
>> > > I don't have any opinion on the specific characters, but I agree with
>> > > Aaron that it would be nice to have some sort of abbreviation for both
>> > the
>> > > strong and weak forms of approval.
>> > >
>> > > -Sandy
>> > >
>> > > > On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> > > >
>> > > > I think the ASF +1 is *slightly* different than Google's LGTM,
>> > > > because
>> > > > it might convey wanting the patch/feature to be merged but not
>> > > > necessarily saying you did a thorough review and stand behind it's
>> > > > technical contents. For instance, I've seen people pile on +1's to
>> > > > try
>> > > > and indicate support for a feature or patch in some projects, even
>> > > > though they didn't do a thorough technical review. This +1 is
>> > > > definitely a useful mechanism.
>> > > >
>> > > > There is definitely much overlap though in the meaning, though, and
>> > > > it's largely because Spark had it's own culture around reviews
>> > > > before
>> > > > it was donated to the ASF, so there is a mix of two styles.
>> > > >
>> > > > Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
>> > > > proposed originally (unlike the one Sandy proposed, e.g.). This is
>> > > > what I've seen every project using the LGTM convention do (Google,
>> > > > and
>> > > > some open source projects such as Impala) to indicate technical
>> > > > sign-off.
>> > > >
>> > > > - Patrick
>> > > >
>> > > >> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson
>> > > >> <ilikerps@gmail.com>
>> > > wrote:
>> > > >> I think I've seen something like +2 = "strong LGTM" and +1 = "weak
>> > LGTM;
>> > > >> someone else should review" before. It's nice to have a shortcut
>> > > >> which
>> > > isn't
>> > > >> a sentence when talking about weaker forms of LGTM.
>> > > >>
>> > > >> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
>> > > >>>
>> > > >>> I think clarifying these semantics is definitely worthwhile. Maybe
>> > this
>> > > >>> complicates the process with additional terminology, but the way
>> > > >>> I've
>> > > used
>> > > >>> these has been:
>> > > >>>
>> > > >>> +1 - I think this is safe to merge and, barring objections from
>> > others,
>> > > >>> would merge it immediately.
>> > > >>>
>> > > >>> LGTM - I have no concerns about this patch, but I don't
>> > > >>> necessarily
>> > > feel
>> > > >>> qualified to make a final call about it.  The TM part acknowledges
>> > the
>> > > >>> judgment as a little more subjective.
>> > > >>>
>> > > >>> I think having some concise way to express both of these is
>> > > >>> useful.
>> > > >>>
>> > > >>> -Sandy
>> > > >>>
>> > > >>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com>
>> > > wrote:
>> > > >>>>
>> > > >>>> Hey All,
>> > > >>>>
>> > > >>>> Just wanted to ping about a minor issue - but one that ends up
>> > having
>> > > >>>> consequence given Spark's volume of reviews and commits. As much
>> > > >>>> as
>> > > >>>> possible, I think that we should try and gear towards "Google
>> > > >>>> Style"
>> > > >>>> LGTM on reviews. What I mean by this is that LGTM has the
>> > > >>>> following
>> > > >>>> semantics:
>> > > >>>>
>> > > >>>> "I know this code well, or I've looked at it close enough to feel
>> > > >>>> confident it should be merged. If there are issues/bugs with this
>> > code
>> > > >>>> later on, I feel confident I can help with them."
>> > > >>>>
>> > > >>>> Here is an alternative semantic:
>> > > >>>>
>> > > >>>> "Based on what I know about this part of the code, I don't see
>> > > >>>> any
>> > > >>>> show-stopper problems with this patch".
>> > > >>>>
>> > > >>>> The issue with the latter is that it ultimately erodes the
>> > > >>>> significance of LGTM, since subsequent reviewers need to reason
>> > about
>> > > >>>> what the person meant by saying LGTM. In contrast, having strong
>> > > >>>> semantics around LGTM can help streamline reviews a lot,
>> > > >>>> especially
>> > as
>> > > >>>> reviewers get more experienced and gain trust from the
>> > > >>>> comittership.
>> > > >>>>
>> > > >>>> There are several easy ways to give a more limited endorsement of
>> > > >>>> a
>> > > >>>> patch:
>> > > >>>> - "I'm not familiar with this code, but style, etc look good"
>> > (general
>> > > >>>> endorsement)
>> > > >>>> - "The build changes in this code LGTM, but I haven't reviewed
>> > > >>>> the
>> > > >>>> rest" (limited LGTM)
>> > > >>>>
>> > > >>>> If people are okay with this, I might add a short note on the
>> > > >>>> wiki.
>> > > >>>> I'm sending this e-mail first, though, to see whether anyone
>> > > >>>> wants
>> > to
>> > > >>>> express agreement or disagreement with this approach.
>> > > >>>>
>> > > >>>> - Patrick
>> > > >>>>
>> > > >>>>
>> > ---------------------------------------------------------------------
>> > > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > >>>> For additional commands, e-mail: dev-help@spark.apache.org
>> > > >>>
>> > > >>>
>> > > >>> ---------------------------------------------------------------------
>> > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > >>> For additional commands, e-mail: dev-help@spark.apache.org
>> > > >>
>> > >
>> > > ---------------------------------------------------------------------
>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > For additional commands, e-mail: dev-help@spark.apache.org
>> > >
>> > >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 20:04:08 2015
Return-Path: <dev-return-11206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EDB81054A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 20:04:08 +0000 (UTC)
Received: (qmail 26996 invoked by uid 500); 19 Jan 2015 20:04:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26925 invoked by uid 500); 19 Jan 2015 20:04:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26908 invoked by uid 99); 19 Jan 2015 20:04:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:04:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:04:05 +0000
Received: by mail-ob0-f172.google.com with SMTP id wp18so9517567obc.3
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 12:03:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=YUGw7GWpB708UK4XVpv6DhyLkjdwiKrTkczT0+Uq0ew=;
        b=oIZarI45JpZwIE+Q2+xzjbd4iT+EDRBKxMz36t6qKEmCMRNfpzf7z5/vjS/gKQ3/5N
         n7uH1UFvfs2now7Vma4zzur21yf0ENWyu2q3I+vagUeZKGTobvxRjLam33l6IuW1xCKN
         PHwb7a1CMV79cic8njPTrBV35Z+7dZS+it6kYNr2ezjHLQ62vxOmNHK4vz4RORiSMlk3
         M5M+wGI/137yHPZXdyTsw5VCJ7JVKlWDXeGRT29DP33sqGvLqe3VNb7thVFHX8HIpnVG
         I4tJ0SIad+szpAIrvKnpzDfWwow94jzep/0PhNUJ6VWi3iNLaQL2ciktdyT9z6DmlMAD
         JUeA==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr18342507oit.100.1421697824861;
 Mon, 19 Jan 2015 12:03:44 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 19 Jan 2015 12:03:44 -0800 (PST)
In-Reply-To: <CABPQxsugNmW4+KBr6px0hL9Brw1SzeDNTehkF2o8e5J7Gp51=A@mail.gmail.com>
References: <CABPQxsvk1CiGjnX7=Chy2-ehQg2Q0ERCGFW6a7voXnZf+5Kh5w@mail.gmail.com>
	<1917C56E-ADFD-4797-99B7-5A90C3125AAC@cloudera.com>
	<CANGvG8oMv_GFY9kY_Rb3Bqs6N-YQ-jg+Rs35=B0LZ+cEmQCKXw@mail.gmail.com>
	<CABPQxsvu6DcoUa_eoq7WqBkv_WSm=3Bp=3N5TUcRvtD9h_Z9tA@mail.gmail.com>
	<FC298D45-3F6E-4E5F-99C6-DEFDCFC09482@cloudera.com>
	<CAKJXNjGdLf03+16gK_GCSwCF3BLuCvckDAVC+O9KsxMCRu2VbQ@mail.gmail.com>
	<CAPh_B=bWNhPy6B8Ay=bA24jftuShxLhrz2FZ9b-sXjVK6p-ZTQ@mail.gmail.com>
	<CAOYDGoBixc8O2qQv5sYbx58_oJiCgmB2r01N0P8Z=iT9GrbiXg@mail.gmail.com>
	<CABPQxsugNmW4+KBr6px0hL9Brw1SzeDNTehkF2o8e5J7Gp51=A@mail.gmail.com>
Date: Mon, 19 Jan 2015 12:03:44 -0800
Message-ID: <CABPQxsuJ1D_iowC_D+YNKek6mWw5VYwW4v5NYR64t0mK9Te4Vg@mail.gmail.com>
Subject: Re: Semantics of LGTM
From: Patrick Wendell <pwendell@gmail.com>
To: Prashant Sharma <scrapcodes@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Kay Ousterhout <keo@eecs.berkeley.edu>, 
	Sandy Ryza <sandy.ryza@cloudera.com>, Aaron Davidson <ilikerps@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

The wiki does not seem to be operational ATM, but I will do this when
it is back up.

On Mon, Jan 19, 2015 at 12:00 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Okay - so given all this I was going to put the following on the wiki
> tentatively:
>
> ## Reviewing Code
> Community code review is Spark's fundamental quality assurance
> process. When reviewing a patch, your goal should be to help
> streamline the committing process by giving committers confidence this
> patch has been verified by an additional party. It's encouraged to
> (politely) submit technical feedback to the author to identify areas
> for improvement or potential bugs.
>
> If you feel a patch is ready for inclusion in Spark, indicate this to
> committers with a comment: "I think this patch looks good". Spark uses
> the LGTM convention for indicating the highest level of technical
> sign-off on a patch: simply comment with the word "LGTM". An LGTM is a
> strong statement, it should be interpreted as the following: "I've
> looked at this thoroughly and take as much ownership as if I wrote the
> patch myself". If you comment LGTM you will be expected to help with
> bugs or follow-up issues on the patch. Judicious use of LGTM's is a
> great way to gain credibility as a reviewer with the broader
> community.
>
> It's also welcome for reviewers to argue against the inclusion of a
> feature or patch. Simply indicate this in the comments.
>
> - Patrick
>
> On Mon, Jan 19, 2015 at 2:40 AM, Prashant Sharma <scrapcodes@gmail.com> wrote:
>> Patrick's original proposal LGTM :).  However until now, I have been in the
>> impression of LGTM with special emphasis on TM part. That said, I will be
>> okay/happy(or Responsible ) for the patch, if it goes in.
>>
>> Prashant Sharma
>>
>>
>>
>> On Sun, Jan 18, 2015 at 2:33 PM, Reynold Xin <rxin@databricks.com> wrote:
>>>
>>> Maybe just to avoid LGTM as a single token when it is not actually
>>> according to Patrick's definition, but anybody can still leave comments
>>> like:
>>>
>>> "The direction of the PR looks good to me." or "+1 on the direction"
>>>
>>> "The build part looks good to me"
>>>
>>> ...
>>>
>>>
>>> On Sat, Jan 17, 2015 at 8:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
>>> wrote:
>>>
>>> > +1 to Patrick's proposal of strong LGTM semantics.  On past projects,
>>> > I've
>>> > heard the semantics of "LGTM" expressed as "I've looked at this
>>> > thoroughly
>>> > and take as much ownership as if I wrote the patch myself".  My
>>> > understanding is that this is the level of review we expect for all
>>> > patches
>>> > that ultimately go into Spark, so it's important to have a way to
>>> > concisely
>>> > describe when this has been done.
>>> >
>>> > Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
>>> > cases I've seen, if someone else says "I looked at this very quickly and
>>> > didn't see any glaring problems", it doesn't add any value for
>>> > subsequent
>>> > reviewers (someone still needs to take a thorough look).
>>> >
>>> > -Kay
>>> >
>>> > On Sat, Jan 17, 2015 at 8:04 PM, <sandy.ryza@cloudera.com> wrote:
>>> >
>>> > > Yeah, the ASF +1 has become partly overloaded to mean both "I would
>>> > > like
>>> > > to see this feature" and "this patch should be committed", although,
>>> > > at
>>> > > least in Hadoop, using +1 on JIRA (as opposed to, say, in a release
>>> > > vote)
>>> > > should unambiguously mean the latter unless qualified in some other
>>> > > way.
>>> > >
>>> > > I don't have any opinion on the specific characters, but I agree with
>>> > > Aaron that it would be nice to have some sort of abbreviation for both
>>> > the
>>> > > strong and weak forms of approval.
>>> > >
>>> > > -Sandy
>>> > >
>>> > > > On Jan 17, 2015, at 7:25 PM, Patrick Wendell <pwendell@gmail.com>
>>> > wrote:
>>> > > >
>>> > > > I think the ASF +1 is *slightly* different than Google's LGTM,
>>> > > > because
>>> > > > it might convey wanting the patch/feature to be merged but not
>>> > > > necessarily saying you did a thorough review and stand behind it's
>>> > > > technical contents. For instance, I've seen people pile on +1's to
>>> > > > try
>>> > > > and indicate support for a feature or patch in some projects, even
>>> > > > though they didn't do a thorough technical review. This +1 is
>>> > > > definitely a useful mechanism.
>>> > > >
>>> > > > There is definitely much overlap though in the meaning, though, and
>>> > > > it's largely because Spark had it's own culture around reviews
>>> > > > before
>>> > > > it was donated to the ASF, so there is a mix of two styles.
>>> > > >
>>> > > > Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
>>> > > > proposed originally (unlike the one Sandy proposed, e.g.). This is
>>> > > > what I've seen every project using the LGTM convention do (Google,
>>> > > > and
>>> > > > some open source projects such as Impala) to indicate technical
>>> > > > sign-off.
>>> > > >
>>> > > > - Patrick
>>> > > >
>>> > > >> On Sat, Jan 17, 2015 at 7:09 PM, Aaron Davidson
>>> > > >> <ilikerps@gmail.com>
>>> > > wrote:
>>> > > >> I think I've seen something like +2 = "strong LGTM" and +1 = "weak
>>> > LGTM;
>>> > > >> someone else should review" before. It's nice to have a shortcut
>>> > > >> which
>>> > > isn't
>>> > > >> a sentence when talking about weaker forms of LGTM.
>>> > > >>
>>> > > >> On Sat, Jan 17, 2015 at 6:59 PM, <sandy.ryza@cloudera.com> wrote:
>>> > > >>>
>>> > > >>> I think clarifying these semantics is definitely worthwhile. Maybe
>>> > this
>>> > > >>> complicates the process with additional terminology, but the way
>>> > > >>> I've
>>> > > used
>>> > > >>> these has been:
>>> > > >>>
>>> > > >>> +1 - I think this is safe to merge and, barring objections from
>>> > others,
>>> > > >>> would merge it immediately.
>>> > > >>>
>>> > > >>> LGTM - I have no concerns about this patch, but I don't
>>> > > >>> necessarily
>>> > > feel
>>> > > >>> qualified to make a final call about it.  The TM part acknowledges
>>> > the
>>> > > >>> judgment as a little more subjective.
>>> > > >>>
>>> > > >>> I think having some concise way to express both of these is
>>> > > >>> useful.
>>> > > >>>
>>> > > >>> -Sandy
>>> > > >>>
>>> > > >>>> On Jan 17, 2015, at 5:40 PM, Patrick Wendell <pwendell@gmail.com>
>>> > > wrote:
>>> > > >>>>
>>> > > >>>> Hey All,
>>> > > >>>>
>>> > > >>>> Just wanted to ping about a minor issue - but one that ends up
>>> > having
>>> > > >>>> consequence given Spark's volume of reviews and commits. As much
>>> > > >>>> as
>>> > > >>>> possible, I think that we should try and gear towards "Google
>>> > > >>>> Style"
>>> > > >>>> LGTM on reviews. What I mean by this is that LGTM has the
>>> > > >>>> following
>>> > > >>>> semantics:
>>> > > >>>>
>>> > > >>>> "I know this code well, or I've looked at it close enough to feel
>>> > > >>>> confident it should be merged. If there are issues/bugs with this
>>> > code
>>> > > >>>> later on, I feel confident I can help with them."
>>> > > >>>>
>>> > > >>>> Here is an alternative semantic:
>>> > > >>>>
>>> > > >>>> "Based on what I know about this part of the code, I don't see
>>> > > >>>> any
>>> > > >>>> show-stopper problems with this patch".
>>> > > >>>>
>>> > > >>>> The issue with the latter is that it ultimately erodes the
>>> > > >>>> significance of LGTM, since subsequent reviewers need to reason
>>> > about
>>> > > >>>> what the person meant by saying LGTM. In contrast, having strong
>>> > > >>>> semantics around LGTM can help streamline reviews a lot,
>>> > > >>>> especially
>>> > as
>>> > > >>>> reviewers get more experienced and gain trust from the
>>> > > >>>> comittership.
>>> > > >>>>
>>> > > >>>> There are several easy ways to give a more limited endorsement of
>>> > > >>>> a
>>> > > >>>> patch:
>>> > > >>>> - "I'm not familiar with this code, but style, etc look good"
>>> > (general
>>> > > >>>> endorsement)
>>> > > >>>> - "The build changes in this code LGTM, but I haven't reviewed
>>> > > >>>> the
>>> > > >>>> rest" (limited LGTM)
>>> > > >>>>
>>> > > >>>> If people are okay with this, I might add a short note on the
>>> > > >>>> wiki.
>>> > > >>>> I'm sending this e-mail first, though, to see whether anyone
>>> > > >>>> wants
>>> > to
>>> > > >>>> express agreement or disagreement with this approach.
>>> > > >>>>
>>> > > >>>> - Patrick
>>> > > >>>>
>>> > > >>>>
>>> > ---------------------------------------------------------------------
>>> > > >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > > >>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> > > >>>
>>> > > >>>
>>> > > >>> ---------------------------------------------------------------------
>>> > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > > >>> For additional commands, e-mail: dev-help@spark.apache.org
>>> > > >>
>>> > >
>>> > > ---------------------------------------------------------------------
>>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > > For additional commands, e-mail: dev-help@spark.apache.org
>>> > >
>>> > >
>>> >
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 20:24:58 2015
Return-Path: <dev-return-11207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A32941067C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 20:24:58 +0000 (UTC)
Received: (qmail 8483 invoked by uid 500); 19 Jan 2015 20:24:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8408 invoked by uid 500); 19 Jan 2015 20:24:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8396 invoked by uid 99); 19 Jan 2015 20:24:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:24:59 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 72.30.239.201 as permitted sender)
Received: from [72.30.239.201] (HELO nm33-vm1.bullet.mail.bf1.yahoo.com) (72.30.239.201)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 20:24:31 +0000
Received: from [98.139.215.141] by nm33.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 20:21:20 -0000
Received: from [98.139.212.198] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 20:21:20 -0000
Received: from [127.0.0.1] by omp1007.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 20:21:20 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 573249.76834.bm@omp1007.mail.bf1.yahoo.com
X-YMail-OSG: ZfFX4EAVM1nPRcpfZIEbVqehlatiFfzve9gyfvzOsdMAcn34Y6NtaM4GpDaicd8
 S6PiL3nUGpy6Fv6DeJBxbJg7_tvhlTYL3PqRQldw_TpTyxSXTVNvQOFb2tcWOrj3jci0nvlaAWHL
 Hoscb8rm89NebXheX1tEWuQvLQbb2jdBIKgXMIabIjV1O4CG.ypned38qlzUIY56.aI8BpxEFiJm
 CgXDQACtLsd539CqAFFCXg.pl_t76VYM_.sQGxx0hmlTz6piJHOU.sopo.ABdR.dwrVa0jNswDcQ
 Ah0IVcmwk17iB4vkTU_pPeH1zhcde9jcyTHdTvav4G5B4XoBgj7duhHzJs06XxJSiURJM80FnPQS
 Z1_GqpcSeLglQb8bN0Acj0YzJNdQ.QEKbhjqvQni5._MVgg3NWqyiwWQOSjC_PiIj3qNM.ovxKbQ
 33xu0qqw4rOal8GtkvL_WsOg8nRqmFMXx7.tV87n.G4N89W7GkCN0QCIoFaxqZTIJ0erGvAY1_.O
 WtVEcSH2V.2xYOw--
Received: by 76.13.26.159; Mon, 19 Jan 2015 20:21:19 +0000 
Date: Mon, 19 Jan 2015 20:20:14 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1182142291.3472691.1421698814066.JavaMail.yahoo@jws10604.mail.bf1.yahoo.com>
Subject: GraphX vertex partition/location strategy
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Does GraphX make an effort to co-locate vertices onto the same workers as the majority (or even some) of its edges?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 21:09:26 2015
Return-Path: <dev-return-11208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 856E91088E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 21:09:26 +0000 (UTC)
Received: (qmail 81373 invoked by uid 500); 19 Jan 2015 21:09:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81308 invoked by uid 500); 19 Jan 2015 21:09:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81296 invoked by uid 99); 19 Jan 2015 21:09:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 21:09:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ankurdave@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 21:09:00 +0000
Received: by mail-qg0-f44.google.com with SMTP id l89so18010226qgf.3
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 13:08:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=EDbYUjCvK1ARJQAGHV5TPre35iUeJWi0n3NpNdltzm0=;
        b=wytHZa1CGFDOV1NfuGC+oTCmYSXwBYENno9NRvLBbWRjsmjxml3ePondn1UyWm5YHE
         DBCXISSX5lZFI/qPlrvAp5l6+rnttcB50h9SvwvJlxWTOsStjH+nqKQSvG4hyxf3eD1r
         fqHrqdUABM9EzRcz4xKyETCJddxuGQx+RdAaUqmJRBFjpyuIhiMIlZuPzK3INb9S1EIF
         4BhEhN8LR4jqRS0AKWjvjjdpDPyXxUqnrlHCu6D6uZpXWWPZM/zyoh5UIFcEC6MGrhcE
         VhfDki/3413BUb1V84e0AlsqGZHKkI8vGd9PNxlkJY/EAoFRI8YrkSSTwUH50sV4x8vy
         CGlg==
X-Received: by 10.224.23.6 with SMTP id p6mr6347092qab.55.1421701737946; Mon,
 19 Jan 2015 13:08:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.101.51 with HTTP; Mon, 19 Jan 2015 13:08:37 -0800 (PST)
In-Reply-To: <1182142291.3472691.1421698814066.JavaMail.yahoo@jws10604.mail.bf1.yahoo.com>
References: <1182142291.3472691.1421698814066.JavaMail.yahoo@jws10604.mail.bf1.yahoo.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Mon, 19 Jan 2015 13:08:37 -0800
Message-ID: <CAK1A71wrfXFYd1dSvZTJzTFstL7GruNVa4tSW5k=cKwJ7LmZ=Q@mail.gmail.com>
Subject: Re: GraphX vertex partition/location strategy
To: Michael Malak <michaelmalak@yahoo.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2b31c11274b050d07bb07
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b31c11274b050d07bb07
Content-Type: text/plain; charset=UTF-8

No - the vertices are hash-partitioned onto workers independently of the
edges. It would be nice for each vertex to be on the worker with the most
adjacent edges, but we haven't done this yet since it would add a lot of
complexity to avoid load imbalance while reducing the overall communication
by a small factor.

We refer to the number of partitions containing adjacent edges for a
particular vertex as the vertex's replication factor. I think the typical
replication factor for power-law graphs with 100-200 partitions is 10-15,
and placing the vertex at the ideal location would only reduce the
replication factor by 1.

Ankur <http://www.ankurdave.com/>

On Mon, Jan 19, 2015 at 12:20 PM, Michael Malak <
michaelmalak@yahoo.com.invalid> wrote:

> Does GraphX make an effort to co-locate vertices onto the same workers as
> the majority (or even some) of its edges?
>

--001a11c2b31c11274b050d07bb07--

From dev-return-11209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 19 21:33:51 2015
Return-Path: <dev-return-11209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FEB1109FF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 Jan 2015 21:33:51 +0000 (UTC)
Received: (qmail 40915 invoked by uid 500); 19 Jan 2015 21:33:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40843 invoked by uid 500); 19 Jan 2015 21:33:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40831 invoked by uid 99); 19 Jan 2015 21:33:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 21:33:52 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 72.30.238.139 as permitted sender)
Received: from [72.30.238.139] (HELO nm36-vm3.bullet.mail.bf1.yahoo.com) (72.30.238.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 Jan 2015 21:33:25 +0000
Received: from [98.139.215.140] by nm36.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 21:31:17 -0000
Received: from [98.139.212.235] by tm11.bullet.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 21:31:17 -0000
Received: from [127.0.0.1] by omp1044.mail.bf1.yahoo.com with NNFMP; 19 Jan 2015 21:31:17 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 459895.41600.bm@omp1044.mail.bf1.yahoo.com
X-YMail-OSG: 7TcELP4VM1nGJdilS9ov5FnjBNvnZX8bgZWkY0NEhXXACIdEJNUV0CNV83IdxpI
 gP40b546qCb3UXRZ2Mkk41Ez.KmqyugqNW5.X_YrhHdU3GWMcl1P01csy.9megrf39dFP3SO1BXs
 h5kQ5FPbjMySB4gWf2XSyRJctswDtOnBYfWJe_d2X_Z72f0d5QxPfWT6YzPb.Mn0SOEnxWZN479V
 N_0ajjhs00cH12OShjEleV9ZIkq7Dr6gzIrqfRxEIpgkxDNL0_c85WOKdC6A_n_XRo_xR3Wk4vsd
 jQ7IyJJ9YMu5Gom0e1ffNL89Di9KxZToncKMlij7r_kyasNiZwMwJb_m808mnuotdsFN6ZtvaQ7u
 hx6rY02SdwUwmJrXjHBzUTx6LNsD9eS2plai_512TIBvVi_UJatsxscYOqZYMgpTNenhVrpzzwHq
 Txbp81obr7wJjRolp0TSZYbBOLxAYJsVwb22BuxnMOzZbjY3FLLeN0gHMll8SVaitzA8hq7efrVG
 kaRuqhpKzU1Y1BhF_Yaq4AFyNpFauL_p4T.btghkJbe.0fzi8kSMzZZ.fn1DhDJ11gOZuvCYiOWp
 UtFZYI2wIfg--
Received: by 76.13.26.110; Mon, 19 Jan 2015 21:31:17 +0000 
Date: Mon, 19 Jan 2015 21:31:16 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: Ankur Dave <ankurdave@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <2083736914.3504894.1421703076676.JavaMail.yahoo@jws10646.mail.bf1.yahoo.com>
In-Reply-To: <CAK1A71wrfXFYd1dSvZTJzTFstL7GruNVa4tSW5k=cKwJ7LmZ=Q@mail.gmail.com>
References: <CAK1A71wrfXFYd1dSvZTJzTFstL7GruNVa4tSW5k=cKwJ7LmZ=Q@mail.gmail.com>
Subject: Re: GraphX vertex partition/location strategy
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_3504893_2065268764.1421703076669"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_3504893_2065268764.1421703076669
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

But wouldn't the gain be greater under something similar to EdgePartition1D (but perhaps better load-balanced based on number of edges for each vertex) and an algorithm that primarily follows edges in the forward direction?
      From: Ankur Dave <ankurdave@gmail.com>
 To: Michael Malak <michaelmalak@yahoo.com> 
Cc: "dev@spark.apache.org" <dev@spark.apache.org> 
 Sent: Monday, January 19, 2015 2:08 PM
 Subject: Re: GraphX vertex partition/location strategy
   
No - the vertices are hash-partitioned onto workers independently of the edges. It would be nice for each vertex to be on the worker with the most adjacent edges, but we haven't done this yet since it would add a lot of complexity to avoid load imbalance while reducing the overall communication by a small factor.
We refer to the number of partitions containing adjacent edges for a particular vertex as the vertex's replication factor. I think the typical replication factor for power-law graphs with 100-200 partitions is 10-15, and placing the vertex at the ideal location would only reduce the replication factor by 1.

Ankur


On Mon, Jan 19, 2015 at 12:20 PM, Michael Malak <michaelmalak@yahoo.com.invalid> wrote:

Does GraphX make an effort to co-locate vertices onto the same workers as the majority (or even some) of its edges?



   
------=_Part_3504893_2065268764.1421703076669--

From dev-return-11210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 00:12:41 2015
Return-Path: <dev-return-11210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A809171ED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 00:12:41 +0000 (UTC)
Received: (qmail 50497 invoked by uid 500); 20 Jan 2015 00:12:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50425 invoked by uid 500); 20 Jan 2015 00:12:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50408 invoked by uid 99); 20 Jan 2015 00:12:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 00:12:41 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.162 as permitted sender)
Received: from [98.139.213.162] (HELO nm19-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.162)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 00:12:15 +0000
Received: from [98.139.215.140] by nm19.bullet.mail.bf1.yahoo.com with NNFMP; 20 Jan 2015 00:09:04 -0000
Received: from [98.139.212.223] by tm11.bullet.mail.bf1.yahoo.com with NNFMP; 20 Jan 2015 00:09:04 -0000
Received: from [127.0.0.1] by omp1032.mail.bf1.yahoo.com with NNFMP; 20 Jan 2015 00:09:04 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 67157.23863.bm@omp1032.mail.bf1.yahoo.com
X-YMail-OSG: iEPHlskVM1lwbAhH1S9bR..Qg59060llDkDX2SpUPHRBYSzJTlFHQ3lU._ig5H2
 h9nNthBv8.eVu8h0r4gR0vKOwN6P9.9moQh272Wdd0M7ZC0UrIyoRXtW0OnTVbrXmQn_OPm0n00X
 G_C54jTO1BG6tD2hMeHNgGXqIoYLIpbAws_zdbtCn7bbdT9tmJe24oPGN.OD3Fczte897zFoLk2L
 B7GFxdOlnPxbAw1dw0P1dON53WA7x2L1XGm53z9XHOItgyQp9P1tDi2HcSU57LG4otx7_Ri.Wr22
 k099OKZ.LSAxE5.DwACf9cC4SBKZTD99botoM13YrGlKH1xSPaAkUQ8oqWl9XUreu3Qb.6Nid1YE
 JWqRtFcfNOKi07ChYnXSo2OnLI3zIn.m9UMiUZlIyFKdBzhqRnH0LEgWrkI6JTav5jO52hFKti9v
 yTLRgSNNm7wgEIFp8tilauyZGqpHLU1UaD1fZlOlrkHy9c5s0RcPhGkeIqBFUPKXySC93z5zZjDL
 UpK4Yqhpr_R7hvMz.4tnm9hmazVn6smViaV3Q6GMdPM3ZNUJyoNPzRpK7Q23M5gOFwkzS6gJxY.a
 USiYKUDF8ZNS1Db6Yb6usLj7_T7cAYRiiQhIE7eqyHfoxlF_CTdXvw04EncCYDwv24knnHlzJHxY
 Q2woS6PJfxRjg6bTQ418TNLO7YMzYJ8z1JQEsSxbozr_rbFlT
Received: by 76.13.26.110; Tue, 20 Jan 2015 00:09:03 +0000 
Date: Tue, 20 Jan 2015 00:09:01 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <730673985.3579069.1421712541433.JavaMail.yahoo@jws106145.mail.bf1.yahoo.com>
Subject: GraphX ShortestPaths backwards?
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""), (2L,""), (3L,""))), sc.makeRDD(Array(Edge(1L,2L,""), Edge(2L,3L,""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

If I am not mistaken about my assessment, then I believe the following changes will make it run "forward":

Change one occurrence of "src" to "dst" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of "dst" to "src" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 02:27:34 2015
Return-Path: <dev-return-11211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7FBAA17552
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 02:27:34 +0000 (UTC)
Received: (qmail 75237 invoked by uid 500); 20 Jan 2015 02:27:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75078 invoked by uid 500); 20 Jan 2015 02:27:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75019 invoked by uid 99); 20 Jan 2015 02:27:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 02:27:34 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xuelincao2014@gmail.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 02:27:29 +0000
Received: by mail-qc0-f180.google.com with SMTP id r5so22772023qcx.11
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 18:27:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=NDsDSbpiDbYAexo1/0tK3qfOrRqc3/69st/IO63L4gE=;
        b=NJMmmOZUHPajZYVLC7Hj7/cVNCfJwrFsYBGZgyNJZj/f35t91wcZK5nuz9gBSHTXRQ
         fkR200JlDDEAXqVNZzIkaIaRGPVCiBkwMHhCAQ9+qQAHZNcAkDEjAgw8AjIP4TjGaCLP
         P5FHahh2x1fCa7XeTfStc8RBUrdWTRMuxSGp261blCZwIUDXCAW4idTUZgJsCVXRSK9o
         TJr7uHnRnPUGFAOgdqT3kVdgeYogt7qZ+ICpQeaXRI69cUfW1Rs8iHWDKHEdbIvJXiSp
         lphl4szhT2IeQZmzov7srcBFzeTNgQsPfTXJpzoXm32y0WbLkft/ZeolkboIHZvFOqOh
         p54A==
MIME-Version: 1.0
X-Received: by 10.140.92.138 with SMTP id b10mr50826589qge.59.1421720828096;
 Mon, 19 Jan 2015 18:27:08 -0800 (PST)
Received: by 10.140.81.42 with HTTP; Mon, 19 Jan 2015 18:27:08 -0800 (PST)
Date: Tue, 20 Jan 2015 10:27:08 +0800
Message-ID: <CABjPPTQFrR0GTMHutn0D50_5YCeC-8Gig4D3i=+AenUY3=QVug@mail.gmail.com>
Subject: Will Spark-SQL support vectorized query engine someday?
From: Xuelin Cao <xuelincao2014@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a5d5eedd545050d0c2c1c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a5d5eedd545050d0c2c1c
Content-Type: text/plain; charset=UTF-8

Hi,

     Correct me if I were wrong. It looks like, the current version of
Spark-SQL is *tuple-at-a-time* module. Basically, each time the physical
operator produces a tuple by recursively call child->execute .

     There are papers that illustrate the benefits of vectorized query
engine. And Hive-Stinger also embrace this style.

     So, the question is, will Spark-SQL give a support to vectorized query
execution someday?

     Thanks

--001a113a5d5eedd545050d0c2c1c--

From dev-return-11212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 02:51:39 2015
Return-Path: <dev-return-11212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10557175A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 02:51:39 +0000 (UTC)
Received: (qmail 95968 invoked by uid 500); 20 Jan 2015 02:51:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95903 invoked by uid 500); 20 Jan 2015 02:51:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 90253 invoked by uid 99); 20 Jan 2015 02:46:05 -0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jingjingwang929@gmail.com designates 209.85.212.178 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=NCUWDPLuEJiQVQmFvrZlzncE18oXUOexzJEWRXei/EY=;
        b=O1vDicixx+hZ3D1Zg/oa99fS5SGiXMnCWPSGjlXoz30PR0Zzp8mNLHC8gpt9f+pa/T
         0Hv6KTOA3FDFfT7oF2fkdpqVisSzglTeZJd40QlVd82TNUhli3EBKjV2YniK+BNw8j7y
         qtNE9h+lmxQIy1HeSI1DqI93Hs1rhfYsv4Xl4ewGm81wN5bVUDAGTtgukgaT5lzeAh8t
         GCyNHo82OJm5RE6rQD8TqeU4AqN/Y46W/QtyCHxRhjU6pkVaFOtwgcYK1eNEeEQd+VUh
         oIL7Ibxh2jUdJLmJBZ3zVd3JZxvcShB0zpa+LiNQTl3Gw89GY0HVVEVCB83vL2faXcvJ
         t59Q==
MIME-Version: 1.0
X-Received: by 10.194.79.226 with SMTP id m2mr65425988wjx.60.1421721848258;
 Mon, 19 Jan 2015 18:44:08 -0800 (PST)
Date: Mon, 19 Jan 2015 21:44:08 -0500
Message-ID: <CA+YMWQd9t1YPyemXNfdykDTJ3qO+7sbo7qF+OUijGGcii8V=kg@mail.gmail.com>
Subject: Join the developer community of spark
From: Jeff Wang <jingjingwang929@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bf0c538bc24c5050d0c693d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0c538bc24c5050d0c693d
Content-Type: text/plain; charset=UTF-8

Hi:

I would like to contribute to the code of spark. Can I join the community?

Thanks,

Jeff

--047d7bf0c538bc24c5050d0c693d--

From dev-return-11213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 02:56:03 2015
Return-Path: <dev-return-11213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CE0F175B5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 02:56:03 +0000 (UTC)
Received: (qmail 3028 invoked by uid 500); 20 Jan 2015 02:56:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2946 invoked by uid 500); 20 Jan 2015 02:56:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2934 invoked by uid 99); 20 Jan 2015 02:56:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 02:56:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 02:55:58 +0000
Received: by mail-oi0-f50.google.com with SMTP id v63so11113355oia.9
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 18:53:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=1b9O73bKoc9CFEA/Zm1Zl1zgaSjPo4bLRCsA19OxIIw=;
        b=w5iSsPILorrPQeWX2EqmiU1zoMVlsaFziyG/MbokTot6EHaVQS5kjFOXaXEJzL7tgu
         obcRQHoUvI2LVrpLTcbQh8zMQskSR0lvuXrW3lb+5vNNErhSldL9qA4opsYngHVDfXit
         C4KeiqAnt9QGjlQ3QbCf4QzwNJGeAzZdE3BGX27zG1wmNMQqg+5QohEE0eAsxl0dloG9
         oQ45sX5bLE7gJQO4b4nSj17dDL5T/LZpSXvVjYcVX33rdgHc+TTnt4vwb4ovPm+NP4FE
         1g7tDc9C+3GBK5wn3us/6J66Eg3NTDaTfjpA4bQFuoUZZFGDLYtq6HWQ0jyMRGGym/oL
         WW8Q==
X-Received: by 10.182.19.167 with SMTP id g7mr20165854obe.75.1421722403077;
 Mon, 19 Jan 2015 18:53:23 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.116.232 with HTTP; Mon, 19 Jan 2015 18:53:02 -0800 (PST)
In-Reply-To: <CA+YMWQd9t1YPyemXNfdykDTJ3qO+7sbo7qF+OUijGGcii8V=kg@mail.gmail.com>
References: <CA+YMWQd9t1YPyemXNfdykDTJ3qO+7sbo7qF+OUijGGcii8V=kg@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Mon, 19 Jan 2015 18:53:02 -0800
Message-ID: <CAJc_syKFx+2Z7Y5U=i+qHqvxJnka=jMJBeci-FHX73uQRmKMFg@mail.gmail.com>
Subject: Re: Join the developer community of spark
To: Jeff Wang <jingjingwang929@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ea20ce07a2050d0c8aab
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ea20ce07a2050d0c8aab
Content-Type: text/plain; charset=UTF-8

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Enjoy!

Alex

On Mon, Jan 19, 2015 at 6:44 PM, Jeff Wang <jingjingwang929@gmail.com>
wrote:

> Hi:
>
> I would like to contribute to the code of spark. Can I join the community?
>
> Thanks,
>
> Jeff
>

--001a11c2ea20ce07a2050d0c8aab--

From dev-return-11214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 07:56:45 2015
Return-Path: <dev-return-11214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EADE617C75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 07:56:44 +0000 (UTC)
Received: (qmail 91176 invoked by uid 500); 20 Jan 2015 07:56:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91078 invoked by uid 500); 20 Jan 2015 07:56:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90409 invoked by uid 99); 20 Jan 2015 07:56:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 07:56:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 07:56:34 +0000
Received: by mail-qg0-f53.google.com with SMTP id a108so20082174qge.12
        for <dev@spark.apache.org>; Mon, 19 Jan 2015 23:55:52 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=WfN4RLwrojCqkVikfehEAZXxqIFlur0XrMkQlCLFUpM=;
        b=kpY6d0yshFdQxq5FCwT4oZflc0AehjLRY3Zde5QN+tpUhnVBSAaD2VeBefaB1ziDAD
         LAz53xIQGoYT4tsByX0KBkH4CH6EuTnvS2W1OjRmuPNtpoUrI1rPKVhHy/5Tsy9kXcSl
         +DFGAsWrexlwhUxM+3WMOU9s105Ie9zx0eQp0V2ZjFajG3/IxPp4Vu1oayseM0UcG8Ij
         yFexngLlT3cjgIdJQbo3vfR9cXJbLlaz8MDyVjpgCfBhVToNtE1hAPcM+OBwe0J6BOZ9
         aDGJoUG/IgK2XnRAWcvOM56VucAef/Y8kPesImCJtX8hC+rfmJe8xj7xKlNfrc96Evvf
         cMPw==
X-Gm-Message-State: ALoCoQl33Rm165BzPcJy8LeNMNxCw4hBF8IJVg7ruVPgXEkKP/dKc2KQEuKT/R9rDH4A07Wbwvca
X-Received: by 10.140.16.50 with SMTP id 47mr53554535qga.105.1421740552355;
 Mon, 19 Jan 2015 23:55:52 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Mon, 19 Jan 2015 23:55:32 -0800 (PST)
In-Reply-To: <CABjPPTQFrR0GTMHutn0D50_5YCeC-8Gig4D3i=+AenUY3=QVug@mail.gmail.com>
References: <CABjPPTQFrR0GTMHutn0D50_5YCeC-8Gig4D3i=+AenUY3=QVug@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 Jan 2015 23:55:32 -0800
Message-ID: <CAPh_B=YNe5_Zxq+n_8cB461oDbtqUKu4Dxv5DPeN=B+_szUHsA@mail.gmail.com>
Subject: Re: Will Spark-SQL support vectorized query engine someday?
To: Xuelin Cao <xuelincao2014@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1005a9614e9050d10c492
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1005a9614e9050d10c492
Content-Type: text/plain; charset=UTF-8

It will probably eventually make its way into part of the query engine, one
way or another. Note that there are in general a lot of other lower hanging
fruits before you have to do vectorization.

As far as I know, Hive doesn't really have vectorization because the
vectorization in Hive is simply writing everything in small batches, in
order to avoid the virtual function call overhead, and hoping the JVM can
unroll some of the loops. There is no SIMD involved.

Something that is pretty useful, which isn't exactly from vectorization but
comes from similar lines of research, is being able to push predicates down
into the columnar compression encoding. For example, one can turn string
comparisons into integer comparisons. These will probably give much larger
performance improvements in common queries.


On Mon, Jan 19, 2015 at 6:27 PM, Xuelin Cao <xuelincao2014@gmail.com> wrote:

> Hi,
>
>      Correct me if I were wrong. It looks like, the current version of
> Spark-SQL is *tuple-at-a-time* module. Basically, each time the physical
> operator produces a tuple by recursively call child->execute .
>
>      There are papers that illustrate the benefits of vectorized query
> engine. And Hive-Stinger also embrace this style.
>
>      So, the question is, will Spark-SQL give a support to vectorized query
> execution someday?
>
>      Thanks
>

--001a11c1005a9614e9050d10c492--

From dev-return-11215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 08:31:31 2015
Return-Path: <dev-return-11215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 236B517D47
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 08:31:31 +0000 (UTC)
Received: (qmail 62529 invoked by uid 500); 20 Jan 2015 08:31:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62467 invoked by uid 500); 20 Jan 2015 08:31:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62453 invoked by uid 99); 20 Jan 2015 08:31:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 08:31:31 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xuelincao2014@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 08:31:27 +0000
Received: by mail-qg0-f44.google.com with SMTP id l89so19665627qgf.3
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 00:30:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7Quao/UAD+HJcMkck9u5KlcsNVKe655YdLsZs8GkjJg=;
        b=V0hniLDYnzkCrFxEgCl/9TF1UAeiHZVAbCXv+bAz8nb+fHLPHnGn+SIxWHj2sRoGZs
         d1ktjfNA6UbkGcqQOfG+j13aslhu6bhFHpOdWpe/6/XargGGuvIadiYImis/YtBzuZu1
         n/bRSWuUv094Y27JHsUMiH7FSCgzTx+E0/9dkexvPbgvjvHT13aGFPA1xV401I4kEEw/
         yHDUknvPGZqHLLb3rLaqv/kiI2jrx5m1Yw/nYfS4AolrctOuxlgynNh0QFO4nJBnaoPy
         hGaFgMBC9W0hPavIWbyW7/UPPFZvRfftR6QrLOD/+FJg4//kIQCx2wunKqgRMSO/mPMD
         BITg==
MIME-Version: 1.0
X-Received: by 10.224.16.131 with SMTP id o3mr56325026qaa.31.1421742621682;
 Tue, 20 Jan 2015 00:30:21 -0800 (PST)
Received: by 10.140.81.42 with HTTP; Tue, 20 Jan 2015 00:30:21 -0800 (PST)
In-Reply-To: <CAPh_B=YNe5_Zxq+n_8cB461oDbtqUKu4Dxv5DPeN=B+_szUHsA@mail.gmail.com>
References: <CABjPPTQFrR0GTMHutn0D50_5YCeC-8Gig4D3i=+AenUY3=QVug@mail.gmail.com>
	<CAPh_B=YNe5_Zxq+n_8cB461oDbtqUKu4Dxv5DPeN=B+_szUHsA@mail.gmail.com>
Date: Tue, 20 Jan 2015 16:30:21 +0800
Message-ID: <CABjPPTTNDY2eeCsz09n74G8ZbBzx5-oOkGt1cBSzxof6TK-0+g@mail.gmail.com>
Subject: Re: Will Spark-SQL support vectorized query engine someday?
From: Xuelin Cao <xuelincao2014@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc9e8eed71a1050d113fe0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc9e8eed71a1050d113fe0
Content-Type: text/plain; charset=UTF-8

Thanks, Reynold

      Regarding the "lower hanging fruits", can you give me some example?
Where can I find them in JIRA?


On Tue, Jan 20, 2015 at 3:55 PM, Reynold Xin <rxin@databricks.com> wrote:

> It will probably eventually make its way into part of the query engine,
> one way or another. Note that there are in general a lot of other lower
> hanging fruits before you have to do vectorization.
>
> As far as I know, Hive doesn't really have vectorization because the
> vectorization in Hive is simply writing everything in small batches, in
> order to avoid the virtual function call overhead, and hoping the JVM can
> unroll some of the loops. There is no SIMD involved.
>
> Something that is pretty useful, which isn't exactly from vectorization
> but comes from similar lines of research, is being able to push predicates
> down into the columnar compression encoding. For example, one can turn
> string comparisons into integer comparisons. These will probably give much
> larger performance improvements in common queries.
>
>
> On Mon, Jan 19, 2015 at 6:27 PM, Xuelin Cao <xuelincao2014@gmail.com>
> wrote:
>
>> Hi,
>>
>>      Correct me if I were wrong. It looks like, the current version of
>> Spark-SQL is *tuple-at-a-time* module. Basically, each time the physical
>> operator produces a tuple by recursively call child->execute .
>>
>>      There are papers that illustrate the benefits of vectorized query
>> engine. And Hive-Stinger also embrace this style.
>>
>>      So, the question is, will Spark-SQL give a support to vectorized
>> query
>> execution someday?
>>
>>      Thanks
>>
>
>

--047d7bdc9e8eed71a1050d113fe0--

From dev-return-11216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 08:37:29 2015
Return-Path: <dev-return-11216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0019917D6B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 08:37:28 +0000 (UTC)
Received: (qmail 74143 invoked by uid 500); 20 Jan 2015 08:37:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74069 invoked by uid 500); 20 Jan 2015 08:37:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74058 invoked by uid 99); 20 Jan 2015 08:37:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 08:37:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 08:37:04 +0000
Received: by mail-qa0-f52.google.com with SMTP id x12so27084246qac.11
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 00:34:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=fh60Sv8a3XglpYiW5gEdR0CaSdKwN3V7LeQbTyQAlLU=;
        b=SF+sbA91ZsOJGhvoKu4fFV1LgU8eVE8SswYkJxL695nvvxOCqBxD24t1pZqkvRsi5l
         Iqq1OVh0otkGPceUM+OlVJNkWY7QzopdmHP1bf3nzTyFvcUqZc3Pr3zkMz/jpIj8O4mh
         q3Tz0BaatcxZxIrLRK4gctrwMIKC3QWDIxwmYRKOXMTF4oEwiBkFMbRbVImajOSZ0Y1H
         QJNqahHdYpCJ9KuXqSQQYp60gqHuBS+4swGtkhRqU+xqvuHKfcWZ2yEMj5swqxg171uj
         bl57FXSLSuiZ1w4glq0DJKefBfrNN7a/iDAmkfJJaS0HpB/4qhl2j+f4SsVDNEH0UKBo
         S47w==
X-Gm-Message-State: ALoCoQln8UyOGKDGMCX0VcHDkbAaOP4LMgHw+4nR0LnQcRqY7Wj0agOOEjrgquIN9jy+5MMuaGQu
X-Received: by 10.224.63.20 with SMTP id z20mr56313280qah.62.1421742867003;
 Tue, 20 Jan 2015 00:34:27 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 20 Jan 2015 00:34:06 -0800 (PST)
In-Reply-To: <CABjPPTTNDY2eeCsz09n74G8ZbBzx5-oOkGt1cBSzxof6TK-0+g@mail.gmail.com>
References: <CABjPPTQFrR0GTMHutn0D50_5YCeC-8Gig4D3i=+AenUY3=QVug@mail.gmail.com>
 <CAPh_B=YNe5_Zxq+n_8cB461oDbtqUKu4Dxv5DPeN=B+_szUHsA@mail.gmail.com> <CABjPPTTNDY2eeCsz09n74G8ZbBzx5-oOkGt1cBSzxof6TK-0+g@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 20 Jan 2015 00:34:06 -0800
Message-ID: <CAPh_B=Yt8W7edD99xxrhU1yLTtQE7VrEfzu7QHe41W4q8gcBcw@mail.gmail.com>
Subject: Re: Will Spark-SQL support vectorized query engine someday?
To: Xuelin Cao <xuelincao2014@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf0cc008cd15d050d114e6a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0cc008cd15d050d114e6a
Content-Type: text/plain; charset=UTF-8

I don't know if there is a list, but in general running performance
profiler can identify a lot of things...

On Tue, Jan 20, 2015 at 12:30 AM, Xuelin Cao <xuelincao2014@gmail.com>
wrote:

>
> Thanks, Reynold
>
>       Regarding the "lower hanging fruits", can you give me some example?
> Where can I find them in JIRA?
>
>
> On Tue, Jan 20, 2015 at 3:55 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> It will probably eventually make its way into part of the query engine,
>> one way or another. Note that there are in general a lot of other lower
>> hanging fruits before you have to do vectorization.
>>
>> As far as I know, Hive doesn't really have vectorization because the
>> vectorization in Hive is simply writing everything in small batches, in
>> order to avoid the virtual function call overhead, and hoping the JVM can
>> unroll some of the loops. There is no SIMD involved.
>>
>> Something that is pretty useful, which isn't exactly from vectorization
>> but comes from similar lines of research, is being able to push predicates
>> down into the columnar compression encoding. For example, one can turn
>> string comparisons into integer comparisons. These will probably give much
>> larger performance improvements in common queries.
>>
>>
>> On Mon, Jan 19, 2015 at 6:27 PM, Xuelin Cao <xuelincao2014@gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>>      Correct me if I were wrong. It looks like, the current version of
>>> Spark-SQL is *tuple-at-a-time* module. Basically, each time the physical
>>> operator produces a tuple by recursively call child->execute .
>>>
>>>      There are papers that illustrate the benefits of vectorized query
>>> engine. And Hive-Stinger also embrace this style.
>>>
>>>      So, the question is, will Spark-SQL give a support to vectorized
>>> query
>>> execution someday?
>>>
>>>      Thanks
>>>
>>
>>
>

--047d7bf0cc008cd15d050d114e6a--

From dev-return-11217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 12:36:19 2015
Return-Path: <dev-return-11217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C65AC1046E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 12:36:19 +0000 (UTC)
Received: (qmail 22793 invoked by uid 500); 20 Jan 2015 12:36:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22720 invoked by uid 500); 20 Jan 2015 12:36:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22708 invoked by uid 99); 20 Jan 2015 12:36:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 12:36:18 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alcaid1801@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 12:36:13 +0000
Received: by mail-wi0-f173.google.com with SMTP id r20so22862984wiv.0
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 04:35:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=an6JH6bOtA0UZOfjFKtBUQdMiVeWSbHuPihCu2u5Y60=;
        b=OfrSqU3YQCWQg/hSfz6L7jr1ofn70IjySsmx+PZ6l3NIhREtQFmal/gpaK6G0SuSSw
         yKuJW3YvTSh7hoZSYSNYxmPz7Jt2ZFdxS+aaJYADQ6vaff3nYQTVivqz0OWk+XzldkR6
         /uGp6vXMxVA7cuaQym79GZMiJujYyBZHGbF+XSwF4OmlW0fPPw1k9x2npT/MVHKKqWyR
         3rB8mCRjtQPkExmz2GUkq5aBVn/PyT2Fm/x211E2uraWGTDktADqXayJMqrs4XqUHA2r
         jkl4a6MEDAYdJq2P+Q9amkQIN/MILpYu+3nlDFx7pCqy0ChH1QSzz41VEfthRH1Oz6o9
         1t1Q==
MIME-Version: 1.0
X-Received: by 10.180.109.79 with SMTP id hq15mr44595990wib.47.1421757307859;
 Tue, 20 Jan 2015 04:35:07 -0800 (PST)
Received: by 10.216.159.68 with HTTP; Tue, 20 Jan 2015 04:35:07 -0800 (PST)
Date: Tue, 20 Jan 2015 20:35:07 +0800
Message-ID: <CACdk1M6=9-UsTKVAJqVK5kneQa1VH+w8ueFWFnvwy7f798Mw1w@mail.gmail.com>
Subject: not found: type LocalSparkContext
From: James <alcaid1801@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f3ba3494aba16050d14ab61
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3ba3494aba16050d14ab61
Content-Type: text/plain; charset=UTF-8

Hi all,

When I was trying to write a test on my spark application I met

```
Error:(14, 43) not found: type LocalSparkContext
class HyperANFSuite extends FunSuite with LocalSparkContext {
```

At the source code of spark-core I could not found "LocalSparkContext",
thus I wonder how to write a test like [this] (
https://github.com/apache/spark/blob/master/graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala
)

Alcaid

--e89a8f3ba3494aba16050d14ab61--

From dev-return-11218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 18:05:35 2015
Return-Path: <dev-return-11218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6976B17520
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 18:05:35 +0000 (UTC)
Received: (qmail 5792 invoked by uid 500); 20 Jan 2015 18:05:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5718 invoked by uid 500); 20 Jan 2015 18:05:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5706 invoked by uid 99); 20 Jan 2015 18:05:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:05:33 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.39 as permitted sender)
Received: from [209.132.183.39] (HELO mx6-phx2.redhat.com) (209.132.183.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:05:09 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx6-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id t0KI53ir032335;
	Tue, 20 Jan 2015 13:05:03 -0500
Date: Tue, 20 Jan 2015 13:05:01 -0500 (EST)
From: Will Benton <willb@redhat.com>
To: James <alcaid1801@gmail.com>
Cc: dev@spark.apache.org
Message-ID: <1798639825.13120811.1421777101798.JavaMail.zimbra@redhat.com>
In-Reply-To: <CACdk1M6=9-UsTKVAJqVK5kneQa1VH+w8ueFWFnvwy7f798Mw1w@mail.gmail.com>
References: <CACdk1M6=9-UsTKVAJqVK5kneQa1VH+w8ueFWFnvwy7f798Mw1w@mail.gmail.com>
Subject: Re: not found: type LocalSparkContext
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF34 (Mac)/8.0.6_GA_5922)
Thread-Topic: not found: type LocalSparkContext
Thread-Index: eEduGCxJn0l3eBsmfwLWoedt8EFIlw==
X-Virus-Checked: Checked by ClamAV on apache.org

It's declared here:

  https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/LocalSparkContext.scala

I assume you're already importing LocalSparkContext, but since the test classes aren't included in Spark packages, you'll also need to package them up in order to use them in your application (viz., outside of Spark).



best,
wb

----- Original Message -----
> From: "James" <alcaid1801@gmail.com>
> To: dev@spark.apache.org
> Sent: Tuesday, January 20, 2015 6:35:07 AM
> Subject: not found: type LocalSparkContext
> 
> Hi all,
> 
> When I was trying to write a test on my spark application I met
> 
> ```
> Error:(14, 43) not found: type LocalSparkContext
> class HyperANFSuite extends FunSuite with LocalSparkContext {
> ```
> 
> At the source code of spark-core I could not found "LocalSparkContext",
> thus I wonder how to write a test like [this] (
> https://github.com/apache/spark/blob/master/graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala
> )
> 
> Alcaid
> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 18:50:07 2015
Return-Path: <dev-return-11219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB69817743
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 18:50:07 +0000 (UTC)
Received: (qmail 64400 invoked by uid 500); 20 Jan 2015 18:50:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64324 invoked by uid 500); 20 Jan 2015 18:50:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64313 invoked by uid 99); 20 Jan 2015 18:50:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:50:06 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:49:41 +0000
Received: by mail-la0-f45.google.com with SMTP id gd6so12365995lab.4
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 10:47:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=RyIgvEPxdA9Mp6F2oBolhipwgstCS+dWJPm2P5I6upg=;
        b=eHtJ4Bke11aP7SG4u2O8uV9/iz5RrCM38hXLgqvIUfdx9M9+mfESM45w6FR/a8wt0r
         5s4CdHpPD0VtGcskkcpzHJCWVDeO2hdlc7Q6RO7b6USF5Pef0MV9EMQnPJC31Q7oltgl
         j+B9arQZUWOfrYtqw44Aof/GIffbEO3DMuIYEpHEDdFVIckx7O4tPHJDE14Dki6Y5RsF
         ow1DmTDwB9BDD2uh5N+Zw/6xpehN+fo+b9lzcEgjEYtJ3mk/mh8bSynxfaMv1OE14neQ
         L4FAbzP4oc8MRrgTjK6JUbqOsqg2sw7dPyqkryTUApn7QWSYPWS4UB1QXVP/TOXSNk1W
         0+Pg==
X-Gm-Message-State: ALoCoQnE7OsS0lx7p1T36R7HhbLVWu2mbwRvPZ+DNoOCfNvV3QAX4E8U5ATFixyPvlUBDPGMBldP
MIME-Version: 1.0
X-Received: by 10.152.224.161 with SMTP id rd1mr12103219lac.86.1421779624379;
 Tue, 20 Jan 2015 10:47:04 -0800 (PST)
Received: by 10.113.4.10 with HTTP; Tue, 20 Jan 2015 10:47:04 -0800 (PST)
In-Reply-To: <CACNcKkEQLUq90_JzvwDZQcZGqagjWKgZOaTRscApL10RGX+_Aw@mail.gmail.com>
References: <1421322724150-10122.post@n3.nabble.com>
	<CACNcKkEQLUq90_JzvwDZQcZGqagjWKgZOaTRscApL10RGX+_Aw@mail.gmail.com>
Date: Tue, 20 Jan 2015 10:47:04 -0800
Message-ID: <CAMJOb8=1j7oM=dPYPsk6ba9TajWSch0jf-guHYiHx0sCQP5KmA@mail.gmail.com>
Subject: Re: Spark client reconnect to driver in yarn-cluster deployment mode
From: Andrew Or <andrew@databricks.com>
To: Romi Kuntsman <romi@totango.com>
Cc: preeze <etander@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113364f675e07a050d19ddff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113364f675e07a050d19ddff
Content-Type: text/plain; charset=UTF-8

Hi Preeze,

> Is there any designed way that the client connects back to the driver
(still running in YARN) for collecting results at a later stage?

No, there is not support built into Spark for this. For this to happen
seamlessly the driver will have to start a server (pull model) or send the
results to some other server once the jobs complete (push model), both of
which add complexity to the driver. Alternatively, you can just poll on the
output files that your application produces; e.g. you can have your driver
write the results of a count to a file and poll on that file. Something
like that.

-Andrew

2015-01-19 5:59 GMT-08:00 Romi Kuntsman <romi@totango.com>:

> "in yarn-client mode it only controls the environment of the executor
> launcher"
>
> So you either use yarn-client mode, and then your app keeps running and
> controlling the process
> Or you use yarn-cluster mode, and then you send a jar to YARN, and that jar
> should have code to report the result back to you
>
> *Romi Kuntsman*, *Big Data Engineer*
>  http://www.totango.com
>
> On Thu, Jan 15, 2015 at 1:52 PM, preeze <etander@gmail.com> wrote:
>
> > From the official spark documentation
> > (http://spark.apache.org/docs/1.2.0/running-on-yarn.html):
> >
> > "In yarn-cluster mode, the Spark driver runs inside an application master
> > process which is managed by YARN on the cluster, and the client can go
> away
> > after initiating the application."
> >
> > Is there any designed way that the client connects back to the driver
> > (still
> > running in YARN) for collecting results at a later stage?
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-client-reconnect-to-driver-in-yarn-cluster-deployment-mode-tp10122.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a113364f675e07a050d19ddff--

From dev-return-11220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 19:32:05 2015
Return-Path: <dev-return-11220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C90BF1795A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 19:32:05 +0000 (UTC)
Received: (qmail 29947 invoked by uid 500); 20 Jan 2015 19:32:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29863 invoked by uid 500); 20 Jan 2015 19:32:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28753 invoked by uid 99); 20 Jan 2015 19:32:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 19:32:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 19:31:56 +0000
Received: by mail-pd0-f176.google.com with SMTP id y10so5248639pdj.7;
        Tue, 20 Jan 2015 11:30:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type;
        bh=BvS2WwqnUQb026rgBuDl7Pdbt4tpf2PXVov+IAzwQp0=;
        b=YU7rxwQmY7V5MDzQeW1qaEECYmXC99DuPg4sIRQPn0ooHBC4MInCeHOAr5y9d/0cpX
         5gtiFEKEAKEIxS14qnWW04Xbtv2IDbLdxlYJZ/J6ZuPucw4T8A0izTuuiCeA/WFFUray
         J520YY1bqEfvxWV/6Ci6ayUldrKXz74iIgGiwmnyTvEkoult62TzGzNOI4eVBdAyTLwc
         PeLjEK2PQluhMcQWU9Pp9JHGlmSuFJcvG9nIbikOQpMmo3tQ+CkpU9i3V7ndklJ7jTF7
         fX97F5wCfAhlVJWqxBsjWaCal1uhpNyildwh6fN06pFCZhmYLZPJOEnJwhA/Mc8Ak+b0
         I5ww==
X-Received: by 10.66.63.68 with SMTP id e4mr30339598pas.69.1421782206560;
        Tue, 20 Jan 2015 11:30:06 -0800 (PST)
Received: from [192.168.1.168] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id vz1sm3676656pbc.55.2015.01.20.11.30.05
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 20 Jan 2015 11:30:05 -0800 (PST)
Message-ID: <54BEACBC.2000109@gmail.com>
Date: Tue, 20 Jan 2015 11:30:04 -0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: Yi Tian <tianyi.asiainfo@gmail.com>, user@spark.apache.org, 
 dev@spark.apache.org
Subject: Re: Is there any way to support multiple users executing SQL on thrift
 server?
References: <54BCC7C8.8060403@gmail.com>
In-Reply-To: <54BCC7C8.8060403@gmail.com>
Content-Type: multipart/alternative;
 boundary="------------050801010809010602070707"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------050801010809010602070707
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hey Yi,

I'm quite unfamiliar with Hadoop/HDFS auth mechanisms for now, but would 
like to investigate this issue later. Would you please open an JIRA for 
it? Thanks!

Cheng

On 1/19/15 1:00 AM, Yi Tian wrote:
>
> Is there any way to support multiple users executing SQL on one thrift 
> server?
>
> I think there are some problems for spark 1.2.0, for example:
>
>  1. Start thrift server with user A
>  2. Connect to thrift server via beeline with user B
>  3. Execute “insert into table dest select … from table src”
>
> then we found these items on hdfs:
>
> |drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000
> drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary
> drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0
> drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/_temporary
> drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000
> -rw-r--r--   3 A supergroup       2671 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000/part-00000
> |
>
> You can see all the temporary path created on driver side (thrift 
> server side) is owned by user B (which is what we expected).
>
> But all the output data created on executor side is owned by user A, 
> (which is NOT what we expected).
> error owner of the output data cause 
> |org.apache.hadoop.security.AccessControlException| while the driver 
> side moving output data into |dest| table.
>
> Is anyone know how to resolve this problem?
>
> ​


--------------050801010809010602070707--

From dev-return-11221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 20:59:16 2015
Return-Path: <dev-return-11221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B454B17CBA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 20:59:16 +0000 (UTC)
Received: (qmail 72278 invoked by uid 500); 20 Jan 2015 20:59:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72199 invoked by uid 500); 20 Jan 2015 20:59:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72187 invoked by uid 99); 20 Jan 2015 20:59:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 20:59:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 20:58:48 +0000
Received: by mail-wg0-f47.google.com with SMTP id n12so7007525wgh.6
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 12:56:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/88uGqcACBy0KrLb4VcLVmIcQnaj92/0wOJxlwcLT+c=;
        b=S30Jl2oq46W57T+0/yw7S0HUHfDIE21rwfiDU3AGFOYfno9Placq3iSo0dzxoqIvpp
         ldNMZ0E+y1Nd/SQzDYKRubSCdoH1RXhMUYTH6+lTxj3WXewFh6WWvENofHB+Jjf4xWcJ
         RxD7yBBTR1C+uvefM3/QJ8dsFVZ56sVMlJgIfy8MTYNC+43CCT4z3SFOcx6gQ+uWHBc3
         cVxIS9coOaxia8E/KzdjuSwlAWwOrj/mpg7jIw/uW1je+XVMyFNXvvzh7vW9BgLqDbSO
         S1wZ524LXxInyYlad0Y/kS1glMYEKyplc+ZiPMFJ2UP1nNUJ4AFeFok+523gYDDE+eNE
         jdjg==
MIME-Version: 1.0
X-Received: by 10.180.221.72 with SMTP id qc8mr50710474wic.19.1421787392511;
 Tue, 20 Jan 2015 12:56:32 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Tue, 20 Jan 2015 12:56:32 -0800 (PST)
In-Reply-To: <CANg8BGDW4Bx31qHNALZ-_Y4uSxy1RsAJmcD9-bdAxzqgemC2Sg@mail.gmail.com>
References: <CANg8BGDW4Bx31qHNALZ-_Y4uSxy1RsAJmcD9-bdAxzqgemC2Sg@mail.gmail.com>
Date: Tue, 20 Jan 2015 12:56:32 -0800
Message-ID: <CAJgQjQ_Zdpj5m5A=OO2=MnoeTEhkHnni-R4hy_Umbkxd-3_WkQ@mail.gmail.com>
Subject: Re: Spectral clustering
From: Xiangrui Meng <mengxr@gmail.com>
To: Andrew Musselman <andrew.musselman@gmail.com>
Cc: dev <dev@spark.apache.org>, Fan Jiang <fjiang6@gmail.com>, 
	Stephen Boesch <javadba@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Fan and Stephen (cc'ed) are working on this feature. They will update
the JIRA page and report progress soon. -Xiangrui

On Fri, Jan 16, 2015 at 12:04 PM, Andrew Musselman
<andrew.musselman@gmail.com> wrote:
> Hi, thinking of picking up this Jira ticket:
> https://issues.apache.org/jira/browse/SPARK-4259
>
> Anyone done any work on this to date?  Any thoughts on it before we go too
> far in?
>
> Thanks!
>
> Best
> Andrew

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 20 21:05:16 2015
Return-Path: <dev-return-11222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4D07D17D0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 21:05:16 +0000 (UTC)
Received: (qmail 92029 invoked by uid 500); 20 Jan 2015 21:05:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91946 invoked by uid 500); 20 Jan 2015 21:05:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90117 invoked by uid 99); 20 Jan 2015 21:05:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 21:05:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andrew.musselman@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 21:04:56 +0000
Received: by mail-qg0-f53.google.com with SMTP id a108so23347538qge.12
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 13:03:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=LmeiWJ6d4RmavTka6fY3fTlC87xRmb7lfVblFzCujDQ=;
        b=lzHsH/1cUezTeXqN4F3NNy8DhGsOBfd0j5obpmC5Wlcl34k20wSbPklS7amwQ7K59t
         IJqykIR2zR0a4ttwtuG9FWyuenawz0bCWl4xp+xJQNYEoP2rfsIrZj1ArGS7F7taEe7S
         rW/qnqVzqWXlAqYnY8qcig3NMJiI5+eZDqgE/9ybhU6b9K9HrgMbkzJ6rEgv1CXdpLla
         mTikfB3vl+z0spa45wiSlA4yA3dG1AOaG+I+D3npdZgaASeb9RueL2vocpBahMpG2OKU
         AO7ornGs/kTKXNyzZ6wexXXGkSzA4sTeisymEUpfNHTN9C1R1P3tixspEf8u/t8pOow4
         ci6w==
MIME-Version: 1.0
X-Received: by 10.140.94.195 with SMTP id g61mr41327559qge.80.1421787785193;
 Tue, 20 Jan 2015 13:03:05 -0800 (PST)
Received: by 10.140.27.236 with HTTP; Tue, 20 Jan 2015 13:03:05 -0800 (PST)
In-Reply-To: <CAJgQjQ_Zdpj5m5A=OO2=MnoeTEhkHnni-R4hy_Umbkxd-3_WkQ@mail.gmail.com>
References: <CANg8BGDW4Bx31qHNALZ-_Y4uSxy1RsAJmcD9-bdAxzqgemC2Sg@mail.gmail.com>
	<CAJgQjQ_Zdpj5m5A=OO2=MnoeTEhkHnni-R4hy_Umbkxd-3_WkQ@mail.gmail.com>
Date: Tue, 20 Jan 2015 13:03:05 -0800
Message-ID: <CANg8BGC_Bk0+kwu5xdA7BCzxRqtwXEG6LQ-+OhFJ+DP=Qr75qw@mail.gmail.com>
Subject: Re: Spectral clustering
From: Andrew Musselman <andrew.musselman@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>, Fan Jiang <fjiang6@gmail.com>, 
	Stephen Boesch <javadba@gmail.com>
Content-Type: multipart/alternative; boundary=001a113a59eee1f001050d1bc36f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a59eee1f001050d1bc36f
Content-Type: text/plain; charset=UTF-8

Awesome, thanks

On Tue, Jan 20, 2015 at 12:56 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Fan and Stephen (cc'ed) are working on this feature. They will update
> the JIRA page and report progress soon. -Xiangrui
>
> On Fri, Jan 16, 2015 at 12:04 PM, Andrew Musselman
> <andrew.musselman@gmail.com> wrote:
> > Hi, thinking of picking up this Jira ticket:
> > https://issues.apache.org/jira/browse/SPARK-4259
> >
> > Anyone done any work on this to date?  Any thoughts on it before we go
> too
> > far in?
> >
> > Thanks!
> >
> > Best
> > Andrew
>

--001a113a59eee1f001050d1bc36f--

From dev-return-11223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 00:14:46 2015
Return-Path: <dev-return-11223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B236B17666
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 00:14:46 +0000 (UTC)
Received: (qmail 58978 invoked by uid 500); 21 Jan 2015 00:14:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58905 invoked by uid 500); 21 Jan 2015 00:14:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58893 invoked by uid 99); 21 Jan 2015 00:14:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:14:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:14:19 +0000
Received: by mail-ig0-f175.google.com with SMTP id hn18so6241649igb.2
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 16:13:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=+YTndSYBF2o8eACu5r3u+Jg1sGeSkVMzoOYurYOC4YQ=;
        b=cqyGvl/jMFUm5fWV27vO4CKYv7arySzHe2iNAP2nxsSfDW3rats0HMk1V6z8ok6C5s
         s9iHM+y6DvFUQMF4uSJa+mgeix3xNFb87kNOaRxWYKDdeo4ZkG0b+e2xqXfGzcJDbS8k
         WG2DdQLdqHBT5temvVE07RUpG4JLYX1/YRkWsGvl/qNzdKQ4TBnDKLJ+bE4Ldfsctpi8
         c75aOqfW6xUfqdeI1GtSIKWmFkWFqVEXMovIw/SjKJMASf4S/Q2tz32cMw1iRIMU/aFW
         DSUvpqqQE3CNl9l0e9CZVpKlWsKKABq3kxt64fiyoJbp7mJG2gKUo/ekj2tY3Wsqiah5
         uK/g==
X-Received: by 10.50.112.98 with SMTP id ip2mr13547251igb.15.1421799212669;
 Tue, 20 Jan 2015 16:13:32 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 21 Jan 2015 00:13:31 +0000
Message-ID: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
Subject: Standardized Spark dev environment
To: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4141620374c8050d1e6d91
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4141620374c8050d1e6d91
Content-Type: text/plain; charset=UTF-8

What do y'all think of creating a standardized Spark development
environment, perhaps encoded as a Vagrantfile, and publishing it under
`dev/`?

The goal would be to make it easier for new developers to get started with
all the right configs and tools pre-installed.

If we use something like Vagrant, we may even be able to make it so that a
single Vagrantfile creates equivalent development environments across OS X,
Linux, and Windows, without having to do much (or any) OS-specific work.

I imagine for committers and regular contributors, this exercise may seem
pointless, since y'all are probably already very comfortable with your
workflow.

I wonder, though, if any of you think this would be worthwhile as a
improvement to the "new Spark developer" experience.

Nick

--047d7b4141620374c8050d1e6d91--

From dev-return-11224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 00:31:05 2015
Return-Path: <dev-return-11224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 701D2176F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 00:31:05 +0000 (UTC)
Received: (qmail 90032 invoked by uid 500); 21 Jan 2015 00:31:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89960 invoked by uid 500); 21 Jan 2015 00:31:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89946 invoked by uid 99); 21 Jan 2015 00:31:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:31:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shenyanls@gmail.com designates 209.85.215.49 as permitted sender)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:30:37 +0000
Received: by mail-la0-f49.google.com with SMTP id hs14so37524294lab.8
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 16:29:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/fMod//CYJzQ4YkM0hH5URClw5LfBt6qzPFsCmdvR7g=;
        b=IScNfs/5dF8OwhMB9bZ1njBzrOPHPkynADuh6OmorgO/UvFpD4Q++M2t6z+5yYRA9D
         0MlxN3CrYwZvu/7t6lnWKYG804tCvdTeXIZFdGvFCxLjmf/yZgTmIaYOE/TMYCK0GPMP
         APoHUNMCU+TeBUa8MUAp1dCqn+2HKM6ArXri5n6qVW4BQA1S1hdUiW03AN+2DY8dKRSz
         gYTX6HjkaSO84Pa96yyTNgbgHDix/MdmPmwbyIrIpRBLhIobZIdEWoZJDsKWO2tCGzFO
         c8GHvmZixVV7Lpyv2n9NUbGaBvRqx+ZsqZMho2QxiV1l/4rqX6kPPuV2Ql6xL0pj3uYk
         kDfg==
MIME-Version: 1.0
X-Received: by 10.152.88.44 with SMTP id bd12mr34662745lab.88.1421800146465;
 Tue, 20 Jan 2015 16:29:06 -0800 (PST)
Received: by 10.25.84.9 with HTTP; Tue, 20 Jan 2015 16:29:06 -0800 (PST)
Received: by 10.25.84.9 with HTTP; Tue, 20 Jan 2015 16:29:06 -0800 (PST)
In-Reply-To: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
Date: Tue, 20 Jan 2015 19:29:06 -0500
Message-ID: <CACAosbGRX2wUEJk_+F1E08LvqUfH_raff8ZZrNnRYW02C+vGBg@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: shenyan zhen <shenyanls@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33a78abfd6f050d1ea47d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33a78abfd6f050d1ea47d
Content-Type: text/plain; charset=UTF-8

Great suggestion.
On Jan 20, 2015 7:14 PM, "Nicholas Chammas" <nicholas.chammas@gmail.com>
wrote:

> What do y'all think of creating a standardized Spark development
> environment, perhaps encoded as a Vagrantfile, and publishing it under
> `dev/`?
>
> The goal would be to make it easier for new developers to get started with
> all the right configs and tools pre-installed.
>
> If we use something like Vagrant, we may even be able to make it so that a
> single Vagrantfile creates equivalent development environments across OS X,
> Linux, and Windows, without having to do much (or any) OS-specific work.
>
> I imagine for committers and regular contributors, this exercise may seem
> pointless, since y'all are probably already very comfortable with your
> workflow.
>
> I wonder, though, if any of you think this would be worthwhile as a
> improvement to the "new Spark developer" experience.
>
> Nick
>

--001a11c33a78abfd6f050d1ea47d--

From dev-return-11225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 00:35:46 2015
Return-Path: <dev-return-11225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 35DCC17713
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 00:35:46 +0000 (UTC)
Received: (qmail 98143 invoked by uid 500); 21 Jan 2015 00:35:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98068 invoked by uid 500); 21 Jan 2015 00:35:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98056 invoked by uid 99); 21 Jan 2015 00:35:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:35:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.45 as permitted sender)
Received: from [209.85.213.45] (HELO mail-yh0-f45.google.com) (209.85.213.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 00:35:18 +0000
Received: by mail-yh0-f45.google.com with SMTP id f73so177035yha.4
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 16:35:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BJhKaekj9uu+KWryCbdNzzACoCe0L33ufAz9e4juFQE=;
        b=0OFAXXi2a2ygU89bP/ixs1msXxCQQ8IaWwVHVx2on5u92wKBwcmiTHUfW+epr9uXIa
         1WiLCcF3zMPHnbgGcEf/qJzsNidcApyFwb4yb3NtnVu5QtmJJEIQj8gXB0lCD4fJMpna
         uUyrWruYouNiq1vvoi2zWJn95opMw57Deq28Ml9F/fbSddK+fokEEmDEK6QuJVvRX4ac
         kBh4O5hNk5iTrHNcU9WzzMctZ2pE0UrsEqNid0qB13NAP8DHjCEbnLPl9tOhaShyRB58
         5atRsYXTGbouMbnQveZ04zFaTZOU46CywJqRX/VoZHk0brVkBS+rYQ76YHv1jTLXbGdQ
         7bxw==
MIME-Version: 1.0
X-Received: by 10.170.141.6 with SMTP id i6mr13554028ykc.122.1421800516752;
 Tue, 20 Jan 2015 16:35:16 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Tue, 20 Jan 2015 16:35:16 -0800 (PST)
In-Reply-To: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
Date: Tue, 20 Jan 2015 16:35:16 -0800
Message-ID: <CALte62yqtW8RxmGEV_VSk+BySAWzyQeFeXsL7vm4Cm-RQ_XFuQ@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Ted Yu <yuzhihong@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139f3cabe1afd050d1ebaa0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139f3cabe1afd050d1ebaa0
Content-Type: text/plain; charset=UTF-8

How many profiles (hadoop / hive /scala) would this development environment
support ?

Cheers

On Tue, Jan 20, 2015 at 4:13 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> What do y'all think of creating a standardized Spark development
> environment, perhaps encoded as a Vagrantfile, and publishing it under
> `dev/`?
>
> The goal would be to make it easier for new developers to get started with
> all the right configs and tools pre-installed.
>
> If we use something like Vagrant, we may even be able to make it so that a
> single Vagrantfile creates equivalent development environments across OS X,
> Linux, and Windows, without having to do much (or any) OS-specific work.
>
> I imagine for committers and regular contributors, this exercise may seem
> pointless, since y'all are probably already very comfortable with your
> workflow.
>
> I wonder, though, if any of you think this would be worthwhile as a
> improvement to the "new Spark developer" experience.
>
> Nick
>

--001a1139f3cabe1afd050d1ebaa0--

From dev-return-11226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 01:09:06 2015
Return-Path: <dev-return-11226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4351D1782C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 01:09:06 +0000 (UTC)
Received: (qmail 57180 invoked by uid 500); 21 Jan 2015 01:09:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57109 invoked by uid 500); 21 Jan 2015 01:09:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57098 invoked by uid 99); 21 Jan 2015 01:09:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:09:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:08:59 +0000
Received: by mail-wi0-f173.google.com with SMTP id r20so29011545wiv.0
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 17:08:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=TFXGRWeJymFkhEzmhqke2VCtyzbz8/htMHqVfY+UBm0=;
        b=WZPCHj3lYqoivibKOi07iX9tAgICSpb3aI8o9Xpp5jzCw8R/Uqpv+KzfsdlxqZ0Ld0
         rFnD8OtV9ZDvlAxKEETDYvT8+caRPYy4Ilqao/uxKJ+OehuqWWWds8s3HTS5Jhk3myou
         oj6Rhm2tzlswXp/nh0PsxubQqW4T1vrYNY13KS2H6BdOSy0cg9Q6Id50Zw5VWYLUlyB0
         dzNXDtQdEzjOAKzctx0X0DQPiSgWOes3dIyVtY7wNh9LbsCPj19i2PkjfCy3sLlnjxUk
         RsQJiRKaQBRlQNd48OQIfm56luM3G+cYu5h+6jEccu3XkvXnItFJdYqf/HQ5ApJO54s+
         GcNQ==
X-Gm-Message-State: ALoCoQnBlbXZrrZbh/QRhkkcFIFDcF791NlhriSCr3vo5/HuRpefY+zH1AV6guUchwr75msM0t7S
MIME-Version: 1.0
X-Received: by 10.194.77.97 with SMTP id r1mr20430998wjw.51.1421802518616;
 Tue, 20 Jan 2015 17:08:38 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 20 Jan 2015 17:08:38 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 20 Jan 2015 17:08:38 -0800 (PST)
In-Reply-To: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
Date: Wed, 21 Jan 2015 01:08:38 +0000
Message-ID: <CAMAsSdKhK5bi8b-6+0gV09FJh2H_=uv-e1FnPvYXTGPA5Ug1mQ@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Sean Owen <sowen@cloudera.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf2f610314a050d1f32d7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf2f610314a050d1f32d7
Content-Type: text/plain; charset=UTF-8

My concern would mostly be maintenance. It adds to an already very complex
build. It only assists developers who are a small audience. What does this
provide, concretely?
On Jan 21, 2015 12:14 AM, "Nicholas Chammas" <nicholas.chammas@gmail.com>
wrote:

> What do y'all think of creating a standardized Spark development
> environment, perhaps encoded as a Vagrantfile, and publishing it under
> `dev/`?
>
> The goal would be to make it easier for new developers to get started with
> all the right configs and tools pre-installed.
>
> If we use something like Vagrant, we may even be able to make it so that a
> single Vagrantfile creates equivalent development environments across OS X,
> Linux, and Windows, without having to do much (or any) OS-specific work.
>
> I imagine for committers and regular contributors, this exercise may seem
> pointless, since y'all are probably already very comfortable with your
> workflow.
>
> I wonder, though, if any of you think this would be worthwhile as a
> improvement to the "new Spark developer" experience.
>
> Nick
>

--047d7bfcf2f610314a050d1f32d7--

From dev-return-11227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 01:14:27 2015
Return-Path: <dev-return-11227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56D4317847
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 01:14:27 +0000 (UTC)
Received: (qmail 73452 invoked by uid 500); 21 Jan 2015 01:14:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73380 invoked by uid 500); 21 Jan 2015 01:14:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73217 invoked by uid 99); 21 Jan 2015 01:14:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:14:25 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [216.70.64.187] (HELO n60.mail01.mtsvc.net) (216.70.64.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:13:59 +0000
Received: from [24.5.171.198] (port=49868 helo=bebopredux)
	by n60.mail01.mtsvc.net with esmtpsa (UNKNOWN:AES256-GCM-SHA384:256)
	(Exim 4.72)
	(envelope-from <nate@reactor8.com>)
	id 1YDjqN-0003Wc-9A
	for dev@spark.apache.org; Tue, 20 Jan 2015 20:12:27 -0500
From: <nate@reactor8.com>
To: "'dev'" <dev@spark.apache.org>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com> <CAMAsSdKhK5bi8b-6+0gV09FJh2H_=uv-e1FnPvYXTGPA5Ug1mQ@mail.gmail.com>
In-Reply-To: <CAMAsSdKhK5bi8b-6+0gV09FJh2H_=uv-e1FnPvYXTGPA5Ug1mQ@mail.gmail.com>
Subject: RE: Standardized Spark dev environment
Date: Tue, 20 Jan 2015 17:12:24 -0800
Message-ID: <d38d01d03517$517ad9f0$f4708dd0$@reactor8.com>
MIME-Version: 1.0
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: quoted-printable
X-Mailer: Microsoft Outlook 15.0
Thread-Index: AQKunV5ax2jD/e2qMcCkz+nLD92J6QFw9tgPmwFrV9A=
Content-Language: en-us
X-Authenticated-User: 917868 nate@reactor8.com
X-MT-ID: 3CB1E6102B94980D10544FA93FD5F01BC02E0A63
X-Virus-Checked: Checked by ClamAV on apache.org

If there is some interest in more standardization and setup of dev/test =
environments spark community might be interested in starting to =
participate in apache bigtop effort:

http://bigtop.apache.org/

While the project had its start and initial focus on packaging, testing, =
deploying Hadoop/hdfs related stack its looking like we will be =
targeting "data engineers" going forward, thus spark is looking to =
become bigger central piece to bigtop effort as the project moves =
towards a "v1" release.

We will be doing a bigtop/bigdata workshop late Feb at the SocalLinux =
Conference:

http://www.socallinuxexpo.org/scale/13x

Right now scoping some content that will be getting started spark =
related for the event, targeted intro of bigtop/spark puppet powered =
deployment components going into the event as well.

Also the group will be holding a meetup at Amazon's Palo Alto office on =
Jan 27th if any folks are interested.

Nate

-----Original Message-----
From: Sean Owen [mailto:sowen@cloudera.com]=20
Sent: Tuesday, January 20, 2015 5:09 PM
To: Nicholas Chammas
Cc: dev
Subject: Re: Standardized Spark dev environment

My concern would mostly be maintenance. It adds to an already very =
complex build. It only assists developers who are a small audience. What =
does this provide, concretely?
On Jan 21, 2015 12:14 AM, "Nicholas Chammas" =
<nicholas.chammas@gmail.com>
wrote:

> What do y'all think of creating a standardized Spark development=20
> environment, perhaps encoded as a Vagrantfile, and publishing it under =

> `dev/`?
>
> The goal would be to make it easier for new developers to get started=20
> with all the right configs and tools pre-installed.
>
> If we use something like Vagrant, we may even be able to make it so=20
> that a single Vagrantfile creates equivalent development environments=20
> across OS X, Linux, and Windows, without having to do much (or any) =
OS-specific work.
>
> I imagine for committers and regular contributors, this exercise may=20
> seem pointless, since y'all are probably already very comfortable with =

> your workflow.
>
> I wonder, though, if any of you think this would be worthwhile as a=20
> improvement to the "new Spark developer" experience.
>
> Nick
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 01:26:14 2015
Return-Path: <dev-return-11228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EEED17886
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 01:26:14 +0000 (UTC)
Received: (qmail 95189 invoked by uid 500); 21 Jan 2015 01:26:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95114 invoked by uid 500); 21 Jan 2015 01:26:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94837 invoked by uid 99); 21 Jan 2015 01:26:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:26:09 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 01:25:44 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id t0L1Me1X029354;
	Tue, 20 Jan 2015 20:22:40 -0500
Date: Tue, 20 Jan 2015 20:22:38 -0500 (EST)
From: Will Benton <willb@redhat.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Message-ID: <243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF34 (Mac)/8.0.6_GA_5922)
Thread-Topic: Standardized Spark dev environment
Thread-Index: 1KEZSQWKnAR0nmbput8ZhZ3Ar2uQbg==
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nick,

I did something similar with a Docker image last summer; I haven't updated the images to cache the dependencies for the current Spark master, but it would be trivial to do so:

http://chapeau.freevariable.com/2014/08/jvm-test-docker.html


best,
wb


----- Original Message -----
> From: "Nicholas Chammas" <nicholas.chammas@gmail.com>
> To: "Spark dev list" <dev@spark.apache.org>
> Sent: Tuesday, January 20, 2015 6:13:31 PM
> Subject: Standardized Spark dev environment
> 
> What do y'all think of creating a standardized Spark development
> environment, perhaps encoded as a Vagrantfile, and publishing it under
> `dev/`?
> 
> The goal would be to make it easier for new developers to get started with
> all the right configs and tools pre-installed.
> 
> If we use something like Vagrant, we may even be able to make it so that a
> single Vagrantfile creates equivalent development environments across OS X,
> Linux, and Windows, without having to do much (or any) OS-specific work.
> 
> I imagine for committers and regular contributors, this exercise may seem
> pointless, since y'all are probably already very comfortable with your
> workflow.
> 
> I wonder, though, if any of you think this would be worthwhile as a
> improvement to the "new Spark developer" experience.
> 
> Nick
> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 03:21:52 2015
Return-Path: <dev-return-11229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA51417B3A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 03:21:52 +0000 (UTC)
Received: (qmail 23907 invoked by uid 500); 21 Jan 2015 03:21:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23832 invoked by uid 500); 21 Jan 2015 03:21:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23820 invoked by uid 99); 21 Jan 2015 03:21:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:21:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:21:46 +0000
Received: by mail-ig0-f182.google.com with SMTP id z20so10778908igj.3
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 19:21:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=rpt+wLvdpYkhaQqUjGCF0cidr3jY9fkM4LX3mVvo4Dg=;
        b=YF0gO/0z4neYU8hOYiKxVScukvi9RhQZ1nUBL2ZCb+/E5LedvtbAeE8mBb3ee80RYO
         wKWEFX03Mv8SL381RmJwDqDe+j/iLkEJ1jR4KY2nEcs9otgrGg4iYJbY8RteZ0dpigb6
         eYkj2xqAHkY7WNqeiXkpTU84Dq8EoudWJF0VKoZzcJS+GrCyq9aeonJlLFXIT8YiQh6z
         xGKPPa+Gx6uWVWVt2iphiYdCNhkhbR4cGYAIfIJo6Xzn1nX035p2lfOkJ3FaHNqOmb28
         jnhxldwmykzkcJR+KplUaaTS2DMtdzzC3GCGyaFP4ec7MxYvozqoy00opzNrAaL/EIhv
         Fd9w==
X-Received: by 10.42.169.197 with SMTP id c5mr37475982icz.72.1421810485475;
 Tue, 20 Jan 2015 19:21:25 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
 <243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 21 Jan 2015 03:21:24 +0000
Message-ID: <CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
To: Will Benton <willb@redhat.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba6e8a5cecc0b3050d210c27
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e8a5cecc0b3050d210c27
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

How many profiles (hadoop / hive /scala) would this development environment
support ?

As many as we want. We probably want to cover a good chunk of the build
matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
officially supports.

What does this provide, concretely?

It provides a reliable way to create a =E2=80=9Cgood=E2=80=9D Spark develop=
ment
environment. Roughly speaking, this probably should mean an environment
that matches Jenkins, since that=E2=80=99s where we run =E2=80=9Cofficial=
=E2=80=9D testing and
builds.

For example, Spark has to run on Java 6 and Python 2.6. When devs build and
run Spark locally, we can make sure they=E2=80=99re doing it on these versi=
ons of
the languages with a simple vagrant up.

Nate, could you comment on how something like this would relate to the
Bigtop effort?

http://chapeau.freevariable.com/2014/08/jvm-test-docker.html

Will, that=E2=80=99s pretty sweet. I tried something similar a few months a=
go as an
experiment to try building/testing Spark within a container. Here=E2=80=99s=
 the
shell script I used <https://gist.github.com/nchammas/60b04141f3b9f053faaa>
against the base CentOS Docker image to setup an environment ready to build
and test Spark.

We want to run Spark unit tests within containers on Jenkins, so it might
make sense to develop a single Docker image that can be used as both a =E2=
=80=9Cdev
environment=E2=80=9D as well as execution container on Jenkins.

Perhaps that=E2=80=99s the approach to take instead of looking into Vagrant=
.

Nick

On Tue Jan 20 2015 at 8:22:41 PM Will Benton <willb@redhat.com> wrote:

Hey Nick,
>
> I did something similar with a Docker image last summer; I haven't update=
d
> the images to cache the dependencies for the current Spark master, but it
> would be trivial to do so:
>
> http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
>
>
> best,
> wb
>
>
> ----- Original Message -----
> > From: "Nicholas Chammas" <nicholas.chammas@gmail.com>
> > To: "Spark dev list" <dev@spark.apache.org>
> > Sent: Tuesday, January 20, 2015 6:13:31 PM
> > Subject: Standardized Spark dev environment
> >
> > What do y'all think of creating a standardized Spark development
> > environment, perhaps encoded as a Vagrantfile, and publishing it under
> > `dev/`?
> >
> > The goal would be to make it easier for new developers to get started
> with
> > all the right configs and tools pre-installed.
> >
> > If we use something like Vagrant, we may even be able to make it so tha=
t
> a
> > single Vagrantfile creates equivalent development environments across O=
S
> X,
> > Linux, and Windows, without having to do much (or any) OS-specific work=
.
> >
> > I imagine for committers and regular contributors, this exercise may se=
em
> > pointless, since y'all are probably already very comfortable with your
> > workflow.
> >
> > I wonder, though, if any of you think this would be worthwhile as a
> > improvement to the "new Spark developer" experience.
> >
> > Nick
> >
>
=E2=80=8B

--90e6ba6e8a5cecc0b3050d210c27--

From dev-return-11230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 03:30:21 2015
Return-Path: <dev-return-11230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 67C4E17B7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 03:30:21 +0000 (UTC)
Received: (qmail 39144 invoked by uid 500); 21 Jan 2015 03:30:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39074 invoked by uid 500); 21 Jan 2015 03:30:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39060 invoked by uid 99); 21 Jan 2015 03:30:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:30:19 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alcaid1801@gmail.com designates 74.125.82.54 as permitted sender)
Received: from [74.125.82.54] (HELO mail-wg0-f54.google.com) (74.125.82.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:29:55 +0000
Received: by mail-wg0-f54.google.com with SMTP id b13so8217474wgh.13
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 19:27:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=x9k2aRGrLtlrXtZlr6D3FjP9MYLiHtitX80YC5Lce0g=;
        b=mbXpJR1p6NOMtO9rZ0CCCufNN6MY+OLnGTZFmBaVY9uuaHkUSH00+1JeULvn09BQ1q
         Eoici+vA23DjU8wHk2emm8szDMFLFH04sXLo0ffHZDmowgzJEPG5fRxXLO084N+fGrcA
         VBH0jcKB1kxDVQWSakN7d7z7EOISwu82g2Dkw+tjnBFtyHLQbcFDBGwqaZdrs82vwoNt
         qUNDmlLCewiKFjNGr+89ybw05Jy2jrOhLXpNce+JgKEithFa8W4TTzn2SOaFQhWSoZSR
         zXwlyJdLG0r+CZK1xFBm0ABnTvdxFlvMrMG4ZRvszF6QFHLmIvBKFeOymWR2chNDxsEg
         KLEw==
MIME-Version: 1.0
X-Received: by 10.194.175.69 with SMTP id by5mr19139776wjc.32.1421810858868;
 Tue, 20 Jan 2015 19:27:38 -0800 (PST)
Received: by 10.216.159.68 with HTTP; Tue, 20 Jan 2015 19:27:38 -0800 (PST)
In-Reply-To: <1798639825.13120811.1421777101798.JavaMail.zimbra@redhat.com>
References: <CACdk1M6=9-UsTKVAJqVK5kneQa1VH+w8ueFWFnvwy7f798Mw1w@mail.gmail.com>
	<1798639825.13120811.1421777101798.JavaMail.zimbra@redhat.com>
Date: Wed, 21 Jan 2015 11:27:38 +0800
Message-ID: <CACdk1M56v-_OfGcXbpT8+AHgG7RexQ1mS4XJRJ8xwFTJ-Rgenw@mail.gmail.com>
Subject: Re: not found: type LocalSparkContext
From: James <alcaid1801@gmail.com>
To: Will Benton <willb@redhat.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013c60b82e4766050d212309
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c60b82e4766050d212309
Content-Type: text/plain; charset=UTF-8

I could not correctly import org.apache.spark.LocalSparkContext,

I use sbt on Intellij for developing,here is my build sbt.

```
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.2.0"

libraryDependencies += "org.apache.spark" %% "spark-graphx" % "1.2.0"

libraryDependencies += "com.clearspring.analytics" % "stream" % "2.7.0"

libraryDependencies += "org.scalatest" % "scalatest_2.10" % "2.0"

resolvers += "Akka Repository" at "http://repo.akka.io/releases/"
```

I think maybe I have make some mistakes on the library setting, as a new
developer of spark application, I wonder what is the standard procedure of
developing a spark application.

Any reply is appreciated.


Alcaid


2015-01-21 2:05 GMT+08:00 Will Benton <willb@redhat.com>:

> It's declared here:
>
>
> https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/LocalSparkContext.scala
>
> I assume you're already importing LocalSparkContext, but since the test
> classes aren't included in Spark packages, you'll also need to package them
> up in order to use them in your application (viz., outside of Spark).
>
>
>
> best,
> wb
>
> ----- Original Message -----
> > From: "James" <alcaid1801@gmail.com>
> > To: dev@spark.apache.org
> > Sent: Tuesday, January 20, 2015 6:35:07 AM
> > Subject: not found: type LocalSparkContext
> >
> > Hi all,
> >
> > When I was trying to write a test on my spark application I met
> >
> > ```
> > Error:(14, 43) not found: type LocalSparkContext
> > class HyperANFSuite extends FunSuite with LocalSparkContext {
> > ```
> >
> > At the source code of spark-core I could not found "LocalSparkContext",
> > thus I wonder how to write a test like [this] (
> >
> https://github.com/apache/spark/blob/master/graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala
> > )
> >
> > Alcaid
> >
>

--089e013c60b82e4766050d212309--

From dev-return-11231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 03:45:01 2015
Return-Path: <dev-return-11231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C39717BD9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 03:45:01 +0000 (UTC)
Received: (qmail 62554 invoked by uid 500); 21 Jan 2015 03:45:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62474 invoked by uid 500); 21 Jan 2015 03:45:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62438 invoked by uid 99); 21 Jan 2015 03:44:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:44:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jayunit100.apache@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:44:55 +0000
Received: by mail-wi0-f176.google.com with SMTP id em10so11708253wid.3
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 19:44:35 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Aet+fFyHj0rhQiUg4RYs2YhybYUlwG3dcBxAjEOL1so=;
        b=w57LWAj18QjV+cy1N9Xi32hUW19E3QJbcRY+WF6Lm7cKXbIkG7FW9qA5MGV+VdELLC
         MU8c2a8GOVU+lG9oRXZDLjBvqdW2NI9nd0YPlByPfaT8oIy/1mJcobwvEpRY0wXRBiuG
         yLfh6O9ocpCDKN7Uh+msLAUeTVhUAWfumUS996E0SjMQe1mBB8S60ASzpLoO3kFjksDu
         k8z2l9vCVLFpb+X+gmlS6pNJvtlGzBwQn4UIzOgzaPPyGbwPcVQiRBGhohvi0IQUtQIq
         5/WXabypguCEaej/3rRgrZFJNto9YFsU+HIYCiw9u/6StYnGEW8zgtflHooLofc4w90a
         Z+Kw==
MIME-Version: 1.0
X-Received: by 10.194.88.131 with SMTP id bg3mr75050308wjb.99.1421811874980;
 Tue, 20 Jan 2015 19:44:34 -0800 (PST)
Received: by 10.27.144.214 with HTTP; Tue, 20 Jan 2015 19:44:34 -0800 (PST)
In-Reply-To: <CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
Date: Tue, 20 Jan 2015 22:44:34 -0500
Message-ID: <CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: jay vyas <jayunit100.apache@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Will Benton <willb@redhat.com>, Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e010d849ebeeb9c050d215fb9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e010d849ebeeb9c050d215fb9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I can comment on both...  hi will and nate :)

1) Will's Dockerfile solution is  the most  simple direct solution to the
dev environment question : its a  efficient way to build and develop spark
environments for dev/test..  It would be cool to put that Dockerfile
(and/or maybe a shell script which uses it) in the top level of spark as
the build entry point.  For total platform portability, u could wrap in a
vagrantfile to launch a lightweight vm, so that windows worked equally
well.

2) However, since nate mentioned  vagrant and bigtop, i have to chime in :)
the vagrant recipes in bigtop are a nice reference deployment of how to
deploy spark in a heterogenous hadoop style environment, and tighter
integration testing w/ bigtop for spark releases would be lovely !  The
vagrant stuff use puppet to deploy an n node VM or docker based cluster, in
which users can easily select components (including
spark,yarn,hbase,hadoop,etc...) by simnply editing a YAML file :
https://github.com/apache/bigtop/blob/master/bigtop-deploy/vm/vagrant-puppe=
t/vagrantconfig.yaml....
As nate said, it would be alot of fun to get more cross collaboration
between the spark and bigtop communities.   Input on how we can better
integrate spark (wether its spork, hbase integration, smoke tests aroudn
the mllib stuff, or whatever, is always welcome )






On Tue, Jan 20, 2015 at 10:21 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> How many profiles (hadoop / hive /scala) would this development environme=
nt
> support ?
>
> As many as we want. We probably want to cover a good chunk of the build
> matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
> officially supports.
>
> What does this provide, concretely?
>
> It provides a reliable way to create a =E2=80=9Cgood=E2=80=9D Spark devel=
opment
> environment. Roughly speaking, this probably should mean an environment
> that matches Jenkins, since that=E2=80=99s where we run =E2=80=9Cofficial=
=E2=80=9D testing and
> builds.
>
> For example, Spark has to run on Java 6 and Python 2.6. When devs build a=
nd
> run Spark locally, we can make sure they=E2=80=99re doing it on these ver=
sions of
> the languages with a simple vagrant up.
>
> Nate, could you comment on how something like this would relate to the
> Bigtop effort?
>
> http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
>
> Will, that=E2=80=99s pretty sweet. I tried something similar a few months=
 ago as an
> experiment to try building/testing Spark within a container. Here=E2=80=
=99s the
> shell script I used <https://gist.github.com/nchammas/60b04141f3b9f053faa=
a
> >
> against the base CentOS Docker image to setup an environment ready to bui=
ld
> and test Spark.
>
> We want to run Spark unit tests within containers on Jenkins, so it might
> make sense to develop a single Docker image that can be used as both a =
=E2=80=9Cdev
> environment=E2=80=9D as well as execution container on Jenkins.
>
> Perhaps that=E2=80=99s the approach to take instead of looking into Vagra=
nt.
>
> Nick
>
> On Tue Jan 20 2015 at 8:22:41 PM Will Benton <willb@redhat.com> wrote:
>
> Hey Nick,
> >
> > I did something similar with a Docker image last summer; I haven't
> updated
> > the images to cache the dependencies for the current Spark master, but =
it
> > would be trivial to do so:
> >
> > http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
> >
> >
> > best,
> > wb
> >
> >
> > ----- Original Message -----
> > > From: "Nicholas Chammas" <nicholas.chammas@gmail.com>
> > > To: "Spark dev list" <dev@spark.apache.org>
> > > Sent: Tuesday, January 20, 2015 6:13:31 PM
> > > Subject: Standardized Spark dev environment
> > >
> > > What do y'all think of creating a standardized Spark development
> > > environment, perhaps encoded as a Vagrantfile, and publishing it unde=
r
> > > `dev/`?
> > >
> > > The goal would be to make it easier for new developers to get started
> > with
> > > all the right configs and tools pre-installed.
> > >
> > > If we use something like Vagrant, we may even be able to make it so
> that
> > a
> > > single Vagrantfile creates equivalent development environments across
> OS
> > X,
> > > Linux, and Windows, without having to do much (or any) OS-specific
> work.
> > >
> > > I imagine for committers and regular contributors, this exercise may
> seem
> > > pointless, since y'all are probably already very comfortable with you=
r
> > > workflow.
> > >
> > > I wonder, though, if any of you think this would be worthwhile as a
> > > improvement to the "new Spark developer" experience.
> > >
> > > Nick
> > >
> >
> =E2=80=8B
>



--=20
jay vyas

--089e010d849ebeeb9c050d215fb9--

From dev-return-11232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 04:22:48 2015
Return-Path: <dev-return-11232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8031017CC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 04:22:48 +0000 (UTC)
Received: (qmail 21710 invoked by uid 500); 21 Jan 2015 04:22:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21639 invoked by uid 500); 21 Jan 2015 04:22:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21620 invoked by uid 99); 21 Jan 2015 04:22:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 04:22:46 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 98.139.212.166 as permitted sender)
Received: from [98.139.212.166] (HELO nm7.bullet.mail.bf1.yahoo.com) (98.139.212.166)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 04:22:20 +0000
Received: from [98.139.215.142] by nm7.bullet.mail.bf1.yahoo.com with NNFMP; 21 Jan 2015 04:21:14 -0000
Received: from [98.139.212.206] by tm13.bullet.mail.bf1.yahoo.com with NNFMP; 21 Jan 2015 04:21:14 -0000
Received: from [127.0.0.1] by omp1015.mail.bf1.yahoo.com with NNFMP; 21 Jan 2015 04:21:14 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 926406.2357.bm@omp1015.mail.bf1.yahoo.com
X-YMail-OSG: wdZUK.MVM1mzE2mux73gxZXHSnSoa5X4m89JOj6pfQyyLHGLCKTNWd4nQvY9LtK
 fsj28lcQrfXN26CGPOiT_Big.C8lpqa7rTJnDkQjlPBQg4xxJly7qOeHpjRGKHsif12.JauH.zec
 5fjM6E.d6eop2n8SR1gdBtHXgUTpRozyVmAmmD4pDxgER_TFrsUHyMjhgQg5VlnNh41RWwYq6CjS
 UzsQswYUaGZudlX4A1KKmbPAu_ulpSaTGrUCHYYNycPkthHNiAxrI5rMdRmfVR69F2Uu39xadAr0
 SdI..MEZ3RcguLcLHAQhs78zAJt6CQx.7qPdY7V5bp5XsZoI3f66dI3rhLeJvI.4IK5v9l0iWIlV
 9oujm0XtnRKBcbn26OlYh.gtTJUSXeGfoVU2UwYpMgQgwvoI9v7SzIKrHlbfeDLL7kp3yP9BkeYN
 36tBezXR4h.Q_b9zEK0RAwrOCpg9qdWDzEWwd2QXpV1mkQ7iRT3i4snNS3pKmjtjWjvBxZTWHdMG
 nZptUhH3FZyEs9pv3fML8Zi0N9ks9GZr9BbNS9KnjEcHvw3UFd3_.HZg8N.Bf229RmWd.42Eg5Lx
 7E.U1CIt.9rZrbCKiJetN.ETeC3TnOBe4Rw38_VkZU0pTSMDTjkNNvHc2ZpOzQku2bkjNlAsQ8j0
 H1DD61SUD0kb6XCMlFPea_3NYnX9hVhgsVfIUCYsryX.Md_XV
Received: by 76.13.26.66; Wed, 21 Jan 2015 04:21:14 +0000 
Date: Wed, 21 Jan 2015 04:21:13 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1938135724.4296779.1421814073879.JavaMail.yahoo@jws10632.mail.bf1.yahoo.com>
In-Reply-To: <730673985.3579069.1421712541433.JavaMail.yahoo@jws106145.mail.bf1.yahoo.com>
References: <730673985.3579069.1421712541433.JavaMail.yahoo@jws106145.mail.bf1.yahoo.com>
Subject: Re: GraphX ShortestPaths backwards?
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I created https://issues.apache.org/jira/browse/SPARK-5343 for this.


----- Original Message -----
From: Michael Malak <michaelmalak@yahoo.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: 
Sent: Monday, January 19, 2015 5:09 PM
Subject: GraphX ShortestPaths backwards?

GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""), (2L,""), (3L,""))), sc.makeRDD(Array(Edge(1L,2L,""), Edge(2L,3L,""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

If I am not mistaken about my assessment, then I believe the following changes will make it run "forward":

Change one occurrence of "src" to "dst" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of "dst" to "src" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 05:44:37 2015
Return-Path: <dev-return-11233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC62717E50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 05:44:37 +0000 (UTC)
Received: (qmail 53277 invoked by uid 500); 21 Jan 2015 05:44:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53205 invoked by uid 500); 21 Jan 2015 05:44:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53194 invoked by uid 99); 21 Jan 2015 05:44:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 05:44:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 05:44:31 +0000
Received: by mail-qc0-f176.google.com with SMTP id c9so13115867qcz.7
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 21:43:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=9brgFl1C44jLOTNiKnRwb63tz9LcSiePxrthCPe5CGE=;
        b=FbM65WqUv6FDnECDbUm8SKiTHFsEBQ3YrPthsT9abjEYOFgQ8AX6Hgq7Fuez4GwRe9
         WAjruYrICWw0J2P+RMr9QqQ8zqrge+e/onh8dSpRrDhAhpMJLM5dtsKHXhyN8Pg/Kt1D
         nh8fa+3tUoFbRiKWzSRwFAcurpYpEn1ebzO66c/CsEeVl9nFMKfcryOyEQpnqCEe6vyY
         rHHGWoKbB8/TfKWl4JBlvUQcWyiVS0HhTXByb6Rq+kOZhPNkOnhtB7UlY6X4+V9DdJBK
         PpqbH1DAlhswEsaZTgwnRYZNGJlFmEIAS1vcBZiUrgCHF4qByIgiguLu+xYzSMkIwE8y
         6Awg==
X-Gm-Message-State: ALoCoQkJfWChNCOKfNxJoJEClWOIbku1Q3Dh6bi0Rr92+ge0o67K7neKzOCP+fHGTXaiT2M69wlH
X-Received: by 10.224.11.5 with SMTP id r5mr66284807qar.10.1421818984530; Tue,
 20 Jan 2015 21:43:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 20 Jan 2015 21:42:43 -0800 (PST)
In-Reply-To: <CACdk1M56v-_OfGcXbpT8+AHgG7RexQ1mS4XJRJ8xwFTJ-Rgenw@mail.gmail.com>
References: <CACdk1M6=9-UsTKVAJqVK5kneQa1VH+w8ueFWFnvwy7f798Mw1w@mail.gmail.com>
 <1798639825.13120811.1421777101798.JavaMail.zimbra@redhat.com> <CACdk1M56v-_OfGcXbpT8+AHgG7RexQ1mS4XJRJ8xwFTJ-Rgenw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 20 Jan 2015 21:42:43 -0800
Message-ID: <CAPh_B=ZO-DqK+TCrqHR0185sy4Z3xH6idEEsh=HT9WXZNSDWqQ@mail.gmail.com>
Subject: Re: not found: type LocalSparkContext
To: James <alcaid1801@gmail.com>
Cc: Will Benton <willb@redhat.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c01e821ff2050d2307a7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c01e821ff2050d2307a7
Content-Type: text/plain; charset=UTF-8

You don't need the LocalSparkContext. It is only for Spark's own unit test.

You can just create a SparkContext and use it in your unit tests, e.g.

val sc = new SparkContext("local", "my test app", new SparkConf)

On Tue, Jan 20, 2015 at 7:27 PM, James <alcaid1801@gmail.com> wrote:

> I could not correctly import org.apache.spark.LocalSparkContext,
>
> I use sbt on Intellij for developing,here is my build sbt.
>
> ```
> libraryDependencies += "org.apache.spark" %% "spark-core" % "1.2.0"
>
> libraryDependencies += "org.apache.spark" %% "spark-graphx" % "1.2.0"
>
> libraryDependencies += "com.clearspring.analytics" % "stream" % "2.7.0"
>
> libraryDependencies += "org.scalatest" % "scalatest_2.10" % "2.0"
>
> resolvers += "Akka Repository" at "http://repo.akka.io/releases/"
> ```
>
> I think maybe I have make some mistakes on the library setting, as a new
> developer of spark application, I wonder what is the standard procedure of
> developing a spark application.
>
> Any reply is appreciated.
>
>
> Alcaid
>
>
> 2015-01-21 2:05 GMT+08:00 Will Benton <willb@redhat.com>:
>
> > It's declared here:
> >
> >
> >
> https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/LocalSparkContext.scala
> >
> > I assume you're already importing LocalSparkContext, but since the test
> > classes aren't included in Spark packages, you'll also need to package
> them
> > up in order to use them in your application (viz., outside of Spark).
> >
> >
> >
> > best,
> > wb
> >
> > ----- Original Message -----
> > > From: "James" <alcaid1801@gmail.com>
> > > To: dev@spark.apache.org
> > > Sent: Tuesday, January 20, 2015 6:35:07 AM
> > > Subject: not found: type LocalSparkContext
> > >
> > > Hi all,
> > >
> > > When I was trying to write a test on my spark application I met
> > >
> > > ```
> > > Error:(14, 43) not found: type LocalSparkContext
> > > class HyperANFSuite extends FunSuite with LocalSparkContext {
> > > ```
> > >
> > > At the source code of spark-core I could not found "LocalSparkContext",
> > > thus I wonder how to write a test like [this] (
> > >
> >
> https://github.com/apache/spark/blob/master/graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala
> > > )
> > >
> > > Alcaid
> > >
> >
>

--001a11c2c01e821ff2050d2307a7--

From dev-return-11234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 05:55:55 2015
Return-Path: <dev-return-11234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6FCA617E97
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 05:55:55 +0000 (UTC)
Received: (qmail 78424 invoked by uid 500); 21 Jan 2015 05:55:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78324 invoked by uid 500); 21 Jan 2015 05:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77197 invoked by uid 99); 21 Jan 2015 05:55:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 05:55:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of msdevanms@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 05:55:26 +0000
Received: by mail-qc0-f174.google.com with SMTP id s11so14166981qcv.5;
        Tue, 20 Jan 2015 21:55:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Ng4IjrZMKV3GaEWJvlyfwIKnRlzB0+yoUk5WrK0pJq0=;
        b=fYAFgYAMTTiBSJxQoD8Py+5U3VbP0fDV2DcP2nD/HW/6tF+TtYlnHo3jv29OSn7WI0
         MSxhzPr5wOIUMcX7wEX9H177SatTozGGm4FCiLzxcHqX6Nvljuari2VLS88TC71lnXRp
         ePsYmGAYsWmsUg8UTvjWwCoxgOVVUmNN1Q+PvIY/jlLmgDp5N9icWTv4Y+TLw5kTE/dr
         97KIFfASeR9jeNj3v7EZO6+mS6EYQUNckJ9SYmu/wXBZ/XdYdbgc5TW+8kdvV7S9hs35
         7CZ0Qz0JNmu6sdMe8VZ3PPEPwIUt2yInhtLfzT0iBojargcySfRopPrG6RtVloB4TQeR
         2+8A==
MIME-Version: 1.0
X-Received: by 10.140.100.234 with SMTP id s97mr301253qge.96.1421819724338;
 Tue, 20 Jan 2015 21:55:24 -0800 (PST)
Received: by 10.229.172.4 with HTTP; Tue, 20 Jan 2015 21:55:24 -0800 (PST)
Date: Wed, 21 Jan 2015 11:25:24 +0530
Message-ID: <CADkoF-rJ6BVmHg0aqm0qcejrNQsJggyh=jVcA2iFDfkO+Bt5-Q@mail.gmail.com>
Subject: KNN for large data set
From: "DEVAN M.S." <msdevanms@gmail.com>
To: User <user@spark.apache.org>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134f40e9a9f65050d233328
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134f40e9a9f65050d233328
Content-Type: text/plain; charset=UTF-8

Hi all,

Please help me to find out best way for K-nearest neighbor using spark for
large data sets.

--001a1134f40e9a9f65050d233328--

From dev-return-11235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 06:31:27 2015
Return-Path: <dev-return-11235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1749817F56
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 06:31:27 +0000 (UTC)
Received: (qmail 45634 invoked by uid 500); 21 Jan 2015 06:31:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45558 invoked by uid 500); 21 Jan 2015 06:31:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45547 invoked by uid 99); 21 Jan 2015 06:31:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 06:31:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of paolo.platter@agilelab.it designates 157.56.112.57 as permitted sender)
Received: from [157.56.112.57] (HELO emea01-am1-obe.outbound.protection.outlook.com) (157.56.112.57)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 06:31:20 +0000
Received: from AM3PR05MB0822.eurprd05.prod.outlook.com (25.161.33.13) by
 AM3PR05MB0823.eurprd05.prod.outlook.com (25.161.33.139) with Microsoft SMTP
 Server (TLS) id 15.1.59.20; Wed, 21 Jan 2015 06:30:36 +0000
Received: from AM3PR05MB0822.eurprd05.prod.outlook.com ([25.161.33.13]) by
 AM3PR05MB0822.eurprd05.prod.outlook.com ([25.161.33.13]) with mapi id
 15.01.0059.007; Wed, 21 Jan 2015 06:30:36 +0000
From: Paolo Platter <paolo.platter@agilelab.it>
To: jay vyas <jayunit100.apache@gmail.com>, Nicholas Chammas
	<nicholas.chammas@gmail.com>
CC: Will Benton <willb@redhat.com>, Spark dev list <dev@spark.apache.org>
Subject: R: Standardized Spark dev environment
Thread-Topic: Standardized Spark dev environment
Thread-Index: AQHQNQ9EUyUEkMn/50C6mv0nXjVhZpzJx10AgAAhVCiAAAZUAIAALmQv
Date: Wed, 21 Jan 2015 06:30:36 +0000
Message-ID: <AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>,<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
In-Reply-To: <CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
Accept-Language: it-IT, en-US
Content-Language: it-IT
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [91.253.139.248]
authentication-results: spf=none (sender IP is )
 smtp.mailfrom=paolo.platter@agilelab.it; 
x-dmarcaction-test: None
x-microsoft-antispam: BCL:0;PCL:0;RULEID:(3005004);SRVR:AM3PR05MB0823;
x-exchange-antispam-report-test: UriScan:;
x-exchange-antispam-report-cfa-test: BCL:0;PCL:0;RULEID:;SRVR:AM3PR05MB0823;
x-forefront-prvs: 04631F8F77
x-forefront-antispam-report: SFV:NSPM;SFS:(10009020)(189002)(13464003)(199003)(53754006)(51704005)(377454003)(24454002)(54206007)(54606007)(19580395003)(19580405001)(54356999)(76176999)(50986999)(93886004)(86362001)(16236675004)(64706001)(66066001)(76576001)(62966003)(77156002)(122556002)(92566002)(19617315012)(74316001)(229853001)(40100003)(97736003)(102836002)(68736005)(105586002)(15975445007)(2900100001)(2950100001)(74482002)(46102003)(106356001)(87936001)(106116001)(2656002)(33656002)(101416001)(19625215002)(18265965002);DIR:OUT;SFP:1101;SCL:1;SRVR:AM3PR05MB0823;H:AM3PR05MB0822.eurprd05.prod.outlook.com;FPR:;SPF:None;MLV:sfv;PTR:InfoNoRecords;MX:1;A:1;LANG:en;
received-spf: None (protection.outlook.com: agilelab.it does not designate
 permitted sender hosts)
Content-Type: multipart/alternative;
	boundary="_000_AM3PR05MB08227F4BA4361B02175F0976F9480AM3PR05MB0822eurp_"
MIME-Version: 1.0
X-OriginatorOrg: agilelab.it
X-MS-Exchange-CrossTenant-originalarrivaltime: 21 Jan 2015 06:30:36.2376
 (UTC)
X-MS-Exchange-CrossTenant-fromentityheader: Hosted
X-MS-Exchange-CrossTenant-id: eee7e750-299f-468f-a6c3-9f28923f6133
X-MS-Exchange-Transport-CrossTenantHeadersStamped: AM3PR05MB0823
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_AM3PR05MB08227F4BA4361B02175F0976F9480AM3PR05MB0822eurp_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_AM3PR05MB08227F4BA4361B02175F0976F9480AM3PR05MB0822eurp_--

From dev-return-11236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 07:30:06 2015
Return-Path: <dev-return-11236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A701910151
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 07:30:06 +0000 (UTC)
Received: (qmail 68312 invoked by uid 500); 21 Jan 2015 07:30:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68234 invoked by uid 500); 21 Jan 2015 07:30:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68100 invoked by uid 99); 21 Jan 2015 07:30:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 07:30:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 07:29:36 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so17458798obc.6
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 23:28:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DHfeOm/VDmS73dT59VxiIy7PyV9fTAtoOG0YmuYBQH0=;
        b=Pi3vb01TfPJHY4EM8tVCMsCNmyVubbtr1bQLWtOEpTthTE07lztPXL+O4tvA595bz+
         bsqTGhvw6ZccbKKLYS9baEfhjHFvZqrjDCkR26A8h/NduHbrMBYyVljpo6D3/lrwEnGL
         vHjU82A3HXEOHotqjAoKFRwtTHqdBYroAs3dFY/gK6v3+yu81kVJi4zjKGceaZUngY+i
         GSFU2p31muVRgnua2dlq72AelomW+p5LRlp6LGThfNcXYHTSvKaJJV2vybfFPplPvD7X
         Mq/KAcbU1hp/VDU/ARYsbfrHu+uJxwnn7EVVNzkC9ko5ADDWATW7ewrlxXLV9LtbIX6v
         85mw==
MIME-Version: 1.0
X-Received: by 10.202.223.3 with SMTP id w3mr22387129oig.82.1421825330019;
 Tue, 20 Jan 2015 23:28:50 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 20 Jan 2015 23:28:49 -0800 (PST)
In-Reply-To: <AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
	<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
	<AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
Date: Tue, 20 Jan 2015 23:28:49 -0800
Message-ID: <CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Patrick Wendell <pwendell@gmail.com>
To: Paolo Platter <paolo.platter@agilelab.it>
Cc: jay vyas <jayunit100.apache@gmail.com>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, Will Benton <willb@redhat.com>, 
	Spark dev list <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

To respond to the original suggestion by Nick. I always thought it
would be useful to have a Docker image on which we run the tests and
build releases, so that we could have a consistent environment that
other packagers or people trying to exhaustively run Spark tests could
replicate (or at least look at) to understand exactly how we recommend
building Spark. Sean - do you think that is too high of overhead?

In terms of providing images that we encourage as standard deployment
images of Spark and want to make portable across environments, that's
a much larger project and one with higher associated maintenance
overhead. So I'd be interested in seeing that evolve as its own
project (spark-deploy) or something associated with bigtop, etc.

- Patrick

On Tue, Jan 20, 2015 at 10:30 PM, Paolo Platter
<paolo.platter@agilelab.it> wrote:
> Hi all,
> I also tried the docker way and it works well.
> I suggest to look at sequenceiq/spark dockers, they are very active on that field.
>
> Paolo
>
> Inviata dal mio Windows Phone
> ________________________________
> Da: jay vyas<mailto:jayunit100.apache@gmail.com>
> Inviato: 21/01/2015 04:45
> A: Nicholas Chammas<mailto:nicholas.chammas@gmail.com>
> Cc: Will Benton<mailto:willb@redhat.com>; Spark dev list<mailto:dev@spark.apache.org>
> Oggetto: Re: Standardized Spark dev environment
>
> I can comment on both...  hi will and nate :)
>
> 1) Will's Dockerfile solution is  the most  simple direct solution to the
> dev environment question : its a  efficient way to build and develop spark
> environments for dev/test..  It would be cool to put that Dockerfile
> (and/or maybe a shell script which uses it) in the top level of spark as
> the build entry point.  For total platform portability, u could wrap in a
> vagrantfile to launch a lightweight vm, so that windows worked equally
> well.
>
> 2) However, since nate mentioned  vagrant and bigtop, i have to chime in :)
> the vagrant recipes in bigtop are a nice reference deployment of how to
> deploy spark in a heterogenous hadoop style environment, and tighter
> integration testing w/ bigtop for spark releases would be lovely !  The
> vagrant stuff use puppet to deploy an n node VM or docker based cluster, in
> which users can easily select components (including
> spark,yarn,hbase,hadoop,etc...) by simnply editing a YAML file :
> https://github.com/apache/bigtop/blob/master/bigtop-deploy/vm/vagrant-puppet/vagrantconfig.yaml....
> As nate said, it would be alot of fun to get more cross collaboration
> between the spark and bigtop communities.   Input on how we can better
> integrate spark (wether its spork, hbase integration, smoke tests aroudn
> the mllib stuff, or whatever, is always welcome )
>
>
>
>
>
>
> On Tue, Jan 20, 2015 at 10:21 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> How many profiles (hadoop / hive /scala) would this development environment
>> support ?
>>
>> As many as we want. We probably want to cover a good chunk of the build
>> matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
>> officially supports.
>>
>> What does this provide, concretely?
>>
>> It provides a reliable way to create a "good" Spark development
>> environment. Roughly speaking, this probably should mean an environment
>> that matches Jenkins, since that's where we run "official" testing and
>> builds.
>>
>> For example, Spark has to run on Java 6 and Python 2.6. When devs build and
>> run Spark locally, we can make sure they're doing it on these versions of
>> the languages with a simple vagrant up.
>>
>> Nate, could you comment on how something like this would relate to the
>> Bigtop effort?
>>
>> http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
>>
>> Will, that's pretty sweet. I tried something similar a few months ago as an
>> experiment to try building/testing Spark within a container. Here's the
>> shell script I used <https://gist.github.com/nchammas/60b04141f3b9f053faaa
>> >
>> against the base CentOS Docker image to setup an environment ready to build
>> and test Spark.
>>
>> We want to run Spark unit tests within containers on Jenkins, so it might
>> make sense to develop a single Docker image that can be used as both a "dev
>> environment" as well as execution container on Jenkins.
>>
>> Perhaps that's the approach to take instead of looking into Vagrant.
>>
>> Nick
>>
>> On Tue Jan 20 2015 at 8:22:41 PM Will Benton <willb@redhat.com> wrote:
>>
>> Hey Nick,
>> >
>> > I did something similar with a Docker image last summer; I haven't
>> updated
>> > the images to cache the dependencies for the current Spark master, but it
>> > would be trivial to do so:
>> >
>> > http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
>> >
>> >
>> > best,
>> > wb
>> >
>> >
>> > ----- Original Message -----
>> > > From: "Nicholas Chammas" <nicholas.chammas@gmail.com>
>> > > To: "Spark dev list" <dev@spark.apache.org>
>> > > Sent: Tuesday, January 20, 2015 6:13:31 PM
>> > > Subject: Standardized Spark dev environment
>> > >
>> > > What do y'all think of creating a standardized Spark development
>> > > environment, perhaps encoded as a Vagrantfile, and publishing it under
>> > > `dev/`?
>> > >
>> > > The goal would be to make it easier for new developers to get started
>> > with
>> > > all the right configs and tools pre-installed.
>> > >
>> > > If we use something like Vagrant, we may even be able to make it so
>> that
>> > a
>> > > single Vagrantfile creates equivalent development environments across
>> OS
>> > X,
>> > > Linux, and Windows, without having to do much (or any) OS-specific
>> work.
>> > >
>> > > I imagine for committers and regular contributors, this exercise may
>> seem
>> > > pointless, since y'all are probably already very comfortable with your
>> > > workflow.
>> > >
>> > > I wonder, though, if any of you think this would be worthwhile as a
>> > > improvement to the "new Spark developer" experience.
>> > >
>> > > Nick
>> > >
>> >
>>
>>
>
>
>
> --
> jay vyas

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 07:52:14 2015
Return-Path: <dev-return-11237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D43DB10256
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 07:52:14 +0000 (UTC)
Received: (qmail 17546 invoked by uid 500); 21 Jan 2015 07:52:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17477 invoked by uid 500); 21 Jan 2015 07:52:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17464 invoked by uid 99); 21 Jan 2015 07:52:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 07:52:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 07:52:06 +0000
Received: by mail-wi0-f175.google.com with SMTP id fb4so24756185wid.2
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 23:51:45 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=EbS4PyUd/Z+I4l1j2kBC58G997FF5Deer6pItghRmJs=;
        b=N+QFbX09EsXqFbTITyx2JcHC5xHl//6w0zfRdvw/+AAUHtE/avdOGp/VG7qhk3G6he
         g8Q/xe2Owsbwa6QPnvA1I7S2DJmiKH59jVl/Le319R0mHHOgxWipIPu/3s8tE/hiN/eu
         dv0ivX7iUhxVrULfkO8JKXU80fPRiIc6ggPWh52OkVIGstQ2bMSv5eq04rNDX5FO/K5V
         fpT0EySNzJQV7XEFek6QOeKefAcunTVb4Kxr0CwmzboG+cRkTyrrfRPv1M6MMNgGbamG
         s+YXFRAhmpolYoEKKuPu1bPDYUmEtON+1V/ayVMpL/GcrcpSpEZmpr0FOkQueIbOCpmy
         Szkw==
X-Gm-Message-State: ALoCoQnB3gxn/K7EDY4Y6aGrYIQHZqV9oB0zKDQ2Y/Kzv8GeTCx8X7JbNt1Z9H2S3cwYCWk+kgFa
MIME-Version: 1.0
X-Received: by 10.194.77.97 with SMTP id r1mr22925437wjw.51.1421826705597;
 Tue, 20 Jan 2015 23:51:45 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 20 Jan 2015 23:51:45 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 20 Jan 2015 23:51:45 -0800 (PST)
In-Reply-To: <CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
	<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
	<AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
	<CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
Date: Wed, 21 Jan 2015 07:51:45 +0000
Message-ID: <CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Sean Owen <sowen@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: dev <dev@spark.apache.org>, jay vyas <jayunit100.apache@gmail.com>, 
	Paolo Platter <paolo.platter@agilelab.it>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Will Benton <willb@redhat.com>
Content-Type: multipart/alternative; boundary=047d7bfcf2f6b84164050d24d3a3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf2f6b84164050d24d3a3
Content-Type: text/plain; charset=UTF-8

If the goal is a reproducible test environment then I think that is what
Jenkins is. Granted you can only ask it for a test. But presumably you get
the same result if you start from the same VM image as Jenkins and run the
same steps.

I bet it is not hard to set up and maintain. I bet it is easier than a VM.
But unless Jenkins is using it aren't we just making another different
standard build env in an effort to standardize? If it is not the same then
it loses value as being exactly the same as the reference build env. Has a
problem come up that this solves?

If the goal is just easing developer set up then what does a Docker image
do - what does it set up for me? I don't know of stuff I need set up on OS
X for me beyond the IDE.
On Jan 21, 2015 7:30 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> To respond to the original suggestion by Nick. I always thought it
> would be useful to have a Docker image on which we run the tests and
> build releases, so that we could have a consistent environment that
> other packagers or people trying to exhaustively run Spark tests could
> replicate (or at least look at) to understand exactly how we recommend
> building Spark. Sean - do you think that is too high of overhead?
>
> In terms of providing images that we encourage as standard deployment
> images of Spark and want to make portable across environments, that's
> a much larger project and one with higher associated maintenance
> overhead. So I'd be interested in seeing that evolve as its own
> project (spark-deploy) or something associated with bigtop, etc.
>
> - Patrick
>
> On Tue, Jan 20, 2015 at 10:30 PM, Paolo Platter
> <paolo.platter@agilelab.it> wrote:
> > Hi all,
> > I also tried the docker way and it works well.
> > I suggest to look at sequenceiq/spark dockers, they are very active on
> that field.
> >
> > Paolo
> >
> > Inviata dal mio Windows Phone
> > ________________________________
> > Da: jay vyas<mailto:jayunit100.apache@gmail.com>
> > Inviato: 21/01/2015 04:45
> > A: Nicholas Chammas<mailto:nicholas.chammas@gmail.com>
> > Cc: Will Benton<mailto:willb@redhat.com>; Spark dev list<mailto:
> dev@spark.apache.org>
> > Oggetto: Re: Standardized Spark dev environment
> >
> > I can comment on both...  hi will and nate :)
> >
> > 1) Will's Dockerfile solution is  the most  simple direct solution to the
> > dev environment question : its a  efficient way to build and develop
> spark
> > environments for dev/test..  It would be cool to put that Dockerfile
> > (and/or maybe a shell script which uses it) in the top level of spark as
> > the build entry point.  For total platform portability, u could wrap in a
> > vagrantfile to launch a lightweight vm, so that windows worked equally
> > well.
> >
> > 2) However, since nate mentioned  vagrant and bigtop, i have to chime in
> :)
> > the vagrant recipes in bigtop are a nice reference deployment of how to
> > deploy spark in a heterogenous hadoop style environment, and tighter
> > integration testing w/ bigtop for spark releases would be lovely !  The
> > vagrant stuff use puppet to deploy an n node VM or docker based cluster,
> in
> > which users can easily select components (including
> > spark,yarn,hbase,hadoop,etc...) by simnply editing a YAML file :
> >
> https://github.com/apache/bigtop/blob/master/bigtop-deploy/vm/vagrant-puppet/vagrantconfig.yaml..
> ..
> > As nate said, it would be alot of fun to get more cross collaboration
> > between the spark and bigtop communities.   Input on how we can better
> > integrate spark (wether its spork, hbase integration, smoke tests aroudn
> > the mllib stuff, or whatever, is always welcome )
> >
> >
> >
> >
> >
> >
> > On Tue, Jan 20, 2015 at 10:21 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >
> >> How many profiles (hadoop / hive /scala) would this development
> environment
> >> support ?
> >>
> >> As many as we want. We probably want to cover a good chunk of the build
> >> matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
> >> officially supports.
> >>
> >> What does this provide, concretely?
> >>
> >> It provides a reliable way to create a "good" Spark development
> >> environment. Roughly speaking, this probably should mean an environment
> >> that matches Jenkins, since that's where we run "official" testing and
> >> builds.
> >>
> >> For example, Spark has to run on Java 6 and Python 2.6. When devs build
> and
> >> run Spark locally, we can make sure they're doing it on these versions
> of
> >> the languages with a simple vagrant up.
> >>
> >> Nate, could you comment on how something like this would relate to the
> >> Bigtop effort?
> >>
> >> http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
> >>
> >> Will, that's pretty sweet. I tried something similar a few months ago
> as an
> >> experiment to try building/testing Spark within a container. Here's the
> >> shell script I used <
> https://gist.github.com/nchammas/60b04141f3b9f053faaa
> >> >
> >> against the base CentOS Docker image to setup an environment ready to
> build
> >> and test Spark.
> >>
> >> We want to run Spark unit tests within containers on Jenkins, so it
> might
> >> make sense to develop a single Docker image that can be used as both a
> "dev
> >> environment" as well as execution container on Jenkins.
> >>
> >> Perhaps that's the approach to take instead of looking into Vagrant.
> >>
> >> Nick
> >>
> >> On Tue Jan 20 2015 at 8:22:41 PM Will Benton <willb@redhat.com> wrote:
> >>
> >> Hey Nick,
> >> >
> >> > I did something similar with a Docker image last summer; I haven't
> >> updated
> >> > the images to cache the dependencies for the current Spark master,
> but it
> >> > would be trivial to do so:
> >> >
> >> > http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
> >> >
> >> >
> >> > best,
> >> > wb
> >> >
> >> >
> >> > ----- Original Message -----
> >> > > From: "Nicholas Chammas" <nicholas.chammas@gmail.com>
> >> > > To: "Spark dev list" <dev@spark.apache.org>
> >> > > Sent: Tuesday, January 20, 2015 6:13:31 PM
> >> > > Subject: Standardized Spark dev environment
> >> > >
> >> > > What do y'all think of creating a standardized Spark development
> >> > > environment, perhaps encoded as a Vagrantfile, and publishing it
> under
> >> > > `dev/`?
> >> > >
> >> > > The goal would be to make it easier for new developers to get
> started
> >> > with
> >> > > all the right configs and tools pre-installed.
> >> > >
> >> > > If we use something like Vagrant, we may even be able to make it so
> >> that
> >> > a
> >> > > single Vagrantfile creates equivalent development environments
> across
> >> OS
> >> > X,
> >> > > Linux, and Windows, without having to do much (or any) OS-specific
> >> work.
> >> > >
> >> > > I imagine for committers and regular contributors, this exercise may
> >> seem
> >> > > pointless, since y'all are probably already very comfortable with
> your
> >> > > workflow.
> >> > >
> >> > > I wonder, though, if any of you think this would be worthwhile as a
> >> > > improvement to the "new Spark developer" experience.
> >> > >
> >> > > Nick
> >> > >
> >> >
> >>
> >>
> >
> >
> >
> > --
> > jay vyas
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bfcf2f6b84164050d24d3a3--

From dev-return-11238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 08:10:06 2015
Return-Path: <dev-return-11238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41995102E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 08:10:06 +0000 (UTC)
Received: (qmail 60029 invoked by uid 500); 21 Jan 2015 08:10:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59956 invoked by uid 500); 21 Jan 2015 08:10:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59926 invoked by uid 99); 21 Jan 2015 08:10:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 08:10:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 08:09:56 +0000
Received: by mail-ob0-f173.google.com with SMTP id vb8so12423015obc.4
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 00:09:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qpFxPZIthFYWt3CxTrPKw8FbRI32iuaPgJYaA2rCx6U=;
        b=urmOLHjZMCdwaYBdhKo4Z0I9M+n4SslAZd8t6+beb287OAy3T6IAtcvr1nvwvGfrwC
         4bjxzPDbhs18HOtWAtKYpXllGBI8XI1E3s24twQWlGikUtTF3dThFg5FwCow01DIibpR
         4P9HiZI/rQMctVDCIeRm63lSHRa4/eNKnzvDpShwNfgSvmZ29mrXGgwd3gT3osTlVykh
         sOqGM2cHwrY/p4PGVDtDOeUzOmZLsT1vrt0LVrQIzuijMZUrQ4HdPzscgBzRx4YN29Wh
         AidDru6peZX2Y9J2Xf9w62LKLGdYlFdnS4DYQfPiT4mlbx+2DH+CBAOvzVR1OAbVemjQ
         i5dA==
MIME-Version: 1.0
X-Received: by 10.60.47.112 with SMTP id c16mr18312594oen.83.1421827775958;
 Wed, 21 Jan 2015 00:09:35 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 21 Jan 2015 00:09:35 -0800 (PST)
In-Reply-To: <CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
	<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
	<AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
	<CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
	<CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com>
Date: Wed, 21 Jan 2015 00:09:35 -0800
Message-ID: <CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>, jay vyas <jayunit100.apache@gmail.com>, 
	Paolo Platter <paolo.platter@agilelab.it>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Will Benton <willb@redhat.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

> If the goal is a reproducible test environment then I think that is what
> Jenkins is. Granted you can only ask it for a test. But presumably you get
> the same result if you start from the same VM image as Jenkins and run the
> same steps.

But the issue is when users can't reproduce Jenkins failures. We don't
publish anywhere what the exact set of packages and versions is that
is installed on Jenkins. And it can change since it's a shared
infrastructure with other projects. So why not publish this manifest
as a docker file and then have it run on jenkins using that image? My
point is that this "VM image + steps" is not public anywhere.

> I bet it is not hard to set up and maintain. I bet it is easier than a VM.
> But unless Jenkins is using it aren't we just making another different
> standard build env in an effort to standardize? If it is not the same then
> it loses value as being exactly the same as the reference build env. Has a
> problem come up that this solves?

Right now the reference build env is an AMI I created and keep adding
stuff to when Spark gets new dependencies (e.g. the version of ruby we
need to create the docs, new python stats libraries, etc). So if we
had a docker image, then I would use that for making the RC's as well
and it could serve as a definitive reference for people who want to
understand exactly what set of things they need to build Spark.

>
> If the goal is just easing developer set up then what does a Docker image do
> - what does it set up for me? I don't know of stuff I need set up on OS X
> for me beyond the IDE.

There are actually a good number of packages you need to do a full
build of Spark including a compliant python version, Java version,
certain python packages, ruby and jekyll stuff for the docs, etc
(mentioned a bit earlier).

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 08:51:26 2015
Return-Path: <dev-return-11239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50A5E1048C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 08:51:26 +0000 (UTC)
Received: (qmail 67401 invoked by uid 500); 21 Jan 2015 08:51:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67332 invoked by uid 500); 21 Jan 2015 08:51:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66921 invoked by uid 99); 21 Jan 2015 08:51:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 08:51:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 08:50:57 +0000
Received: by mail-wi0-f182.google.com with SMTP id n3so29624075wiv.3
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 00:50:56 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=yqK0sfPauNXEIa5TvDtXekeHSHeMOfPLmhiPNPNzDb4=;
        b=I/g3zHIQ8QJ07uts7lSCh3UN1Y4T1hvn4gKrPwBI4HrL1pVZkdit39qaLlP1u0QEIs
         qt9UZJ1pimwo8Cd4VqAKxSSOafhfBAhgMgz23XFAMmNhCTweK5mZx/l5wugLf6Gbnmio
         F0f8fkbFIxFmP3NONvYVcRUODNpRm1F5EBGrY/ehWmg7v+CRtydpBTbDLhPky4GZjywI
         N5ITG8qCR3A1LGGb1u3m9RnYnCcjm75riAQ0AkO+ZyjdFjOrWqzOCp0UyFgwyvOI4WMm
         xx3yeql8KRAjQDoAWtkYYTjvbJtOyeA6N/Z4rmvpFv6BRLV10z6/dHV3iaJxrOfIv9cZ
         30OA==
X-Gm-Message-State: ALoCoQmAdSQnJWpgs6bvkZsRlI3UMwV52lOaWCjo9/KjXz9bYLbANrybjU7j6Z8u/E+ndQ0B0kTe
MIME-Version: 1.0
X-Received: by 10.195.12.35 with SMTP id en3mr25287930wjd.129.1421830256571;
 Wed, 21 Jan 2015 00:50:56 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Wed, 21 Jan 2015 00:50:55 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Wed, 21 Jan 2015 00:50:55 -0800 (PST)
In-Reply-To: <CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
	<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
	<AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
	<CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
	<CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com>
	<CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
Date: Wed, 21 Jan 2015 08:50:55 +0000
Message-ID: <CAMAsSdK0b5apq2mirFMa4cgF6m8VUNN7C1RPDgO-xkbscPxUxg@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Sean Owen <sowen@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: jay vyas <jayunit100.apache@gmail.com>, Paolo Platter <paolo.platter@agilelab.it>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, Will Benton <willb@redhat.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf7885fec8e050d25a78b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf7885fec8e050d25a78b
Content-Type: text/plain; charset=UTF-8

Sure, can Jenkins use this new image too? If not then it doesn't help with
reproducing a Jenkins failure, most of which even Jenkins can't reproduce.
But if it does and it can be used for builds then that does seem like it is
reducing rather than increasing environment configurations which is good.

That's different from developer setup. Surely that is a large number of
permutations to maintain? Win, Linux, OS X at least. Whereas I have not
needed nor probably would want a whole second tool chain on my machine for
Spark... for me it doesn't solve a problem. So just wondering how many
people this will help as devs versus some apparent big maintenance
overhead.

Although if this could replace the scripts that try to fetch sbt and mvn et
al that alone could save enough complexity to make it worthwhile. Would it
do that?
On Jan 21, 2015 9:09 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> > If the goal is a reproducible test environment then I think that is what
> > Jenkins is. Granted you can only ask it for a test. But presumably you
> get
> > the same result if you start from the same VM image as Jenkins and run
> the
> > same steps.
>
> But the issue is when users can't reproduce Jenkins failures. We don't
> publish anywhere what the exact set of packages and versions is that
> is installed on Jenkins. And it can change since it's a shared
> infrastructure with other projects. So why not publish this manifest
> as a docker file and then have it run on jenkins using that image? My
> point is that this "VM image + steps" is not public anywhere.
>
> > I bet it is not hard to set up and maintain. I bet it is easier than a
> VM.
> > But unless Jenkins is using it aren't we just making another different
> > standard build env in an effort to standardize? If it is not the same
> then
> > it loses value as being exactly the same as the reference build env. Has
> a
> > problem come up that this solves?
>
> Right now the reference build env is an AMI I created and keep adding
> stuff to when Spark gets new dependencies (e.g. the version of ruby we
> need to create the docs, new python stats libraries, etc). So if we
> had a docker image, then I would use that for making the RC's as well
> and it could serve as a definitive reference for people who want to
> understand exactly what set of things they need to build Spark.
>
> >
> > If the goal is just easing developer set up then what does a Docker
> image do
> > - what does it set up for me? I don't know of stuff I need set up on OS X
> > for me beyond the IDE.
>
> There are actually a good number of packages you need to do a full
> build of Spark including a compliant python version, Java version,
> certain python packages, ruby and jekyll stuff for the docs, etc
> (mentioned a bit earlier).
>
> - Patrick
>

--047d7bfcf7885fec8e050d25a78b--

From dev-return-11240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 13:46:17 2015
Return-Path: <dev-return-11240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABF7E10FBB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 13:46:17 +0000 (UTC)
Received: (qmail 12844 invoked by uid 500); 21 Jan 2015 13:46:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12772 invoked by uid 500); 21 Jan 2015 13:46:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12760 invoked by uid 99); 21 Jan 2015 13:46:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 13:46:15 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 13:45:51 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id t0LDggNl004429;
	Wed, 21 Jan 2015 08:42:42 -0500
Date: Wed, 21 Jan 2015 08:42:40 -0500 (EST)
From: Will Benton <willb@redhat.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>,
        jay vyas <jayunit100.apache@gmail.com>,
        Paolo Platter <paolo.platter@agilelab.it>,
        Nicholas Chammas <nicholas.chammas@gmail.com>
Message-ID: <812445935.13688800.1421847760911.JavaMail.zimbra@redhat.com>
In-Reply-To: <CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com> <243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com> <CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com> <CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com> <AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com> <CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com> <CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com> <CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF34 (Mac)/8.0.6_GA_5922)
Thread-Topic: Standardized Spark dev environment
Thread-Index: g1XsaSyAj1yoE47iTwOcVB3Tqe7fMA==
X-Virus-Checked: Checked by ClamAV on apache.org

----- Original Message -----
> From: "Patrick Wendell" <pwendell@gmail.com>
> To: "Sean Owen" <sowen@cloudera.com>
> Cc: "dev" <dev@spark.apache.org>, "jay vyas" <jayunit100.apache@gmail.com>, "Paolo Platter"
> <paolo.platter@agilelab.it>, "Nicholas Chammas" <nicholas.chammas@gmail.com>, "Will Benton" <willb@redhat.com>
> Sent: Wednesday, January 21, 2015 2:09:35 AM
> Subject: Re: Standardized Spark dev environment

> But the issue is when users can't reproduce Jenkins failures.

Yeah, to answer Sean's question, this was part of the problem I was trying to solve.  The other part was teasing out differences between the Fedora Java environment and a more conventional Java environment.  I agree with Sean (and I think this is your suggestion as well, Patrick) that making the environment Jenkins runs a standard image that is available for public consumption would be useful in general.



best,
wb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 16:39:37 2015
Return-Path: <dev-return-11241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A522C80C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 16:39:37 +0000 (UTC)
Received: (qmail 15566 invoked by uid 500); 21 Jan 2015 16:39:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15493 invoked by uid 500); 21 Jan 2015 16:39:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 64981 invoked by uid 99); 21 Jan 2015 14:51:38 -0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of 0000014b0cf9c936-16dd24d4-0c08-4688-938a-267bda00619d-000000@amazonses.com designates 54.240.8.81 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/simple;
	s=224i4yxa5dv7c2xz3womw6peuasteono; d=amazonses.com; t=1421851871;
	h=MIME-Version:Content-Type:Content-Transfer-Encoding:Message-ID:Date:From:To:Subject:Feedback-ID;
	bh=/nVq9B9h4YEyE/0wh4zWa2ZEpqQDmBy71vhpY0xIxjA=;
	b=tRqMsarEHeT0d0Ivj/6wxmEfqh2Q1U2c+GsvC2gi2ZGxKoaSfvVj+y+UUli2aJ7P
	RINhwpzcJMYeJKSERJIq+3EqJ7qsss7xrkS9lSq/UAgzXnUcoCvBtQMf1Oaz/7OCpVi
	9hZxS9mgw/+R5ayKtpVr9ojll2/KDeZq19Q0x7NU=
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/simple;
	s=kzvfyjuxgwm4sxhrcthxz7y5qcjy24us; d=synsys.com; t=1421851871;
	h=MIME-Version:Content-Type:Content-Transfer-Encoding:Message-ID:Date:From:To:Subject;
	bh=/nVq9B9h4YEyE/0wh4zWa2ZEpqQDmBy71vhpY0xIxjA=;
	b=D+Dw7nHfRdHX6Th8zo5Dh3E+CWlv+xCv8mO5LkW8VsyOTc8g9skWTIiVIJ5hIdXP
	G2JqT/CWVBqBrV30u6zSITg9KP1dzvQ/H5jEuJKOnTWUV7Bhao9WI3V+DZO+MkEIgEh
	YX6wCtJ5z/Xr2aFQUOsWVRwNcVUfcKgjcAsmC6so=
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-UserIsAuth: true
Message-ID: <0000014b0cf9c936-16dd24d4-0c08-4688-938a-267bda00619d-000000@email.amazonses.com>
Date: Wed, 21 Jan 2015 14:51:11 +0000
From: Stephen Brewin <sbrewin@synsys.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
To: dev@spark.apache.org
Subject: SPARK-5267 : Add a streaming module to ingest Apache Camel Messages
 from a configured endpoints
X-SES-Outgoing: 2015.01.21-54.240.8.81
Feedback-ID: us-east-1.c2JyZXdpbkBzeW5zeXMuY29t:AmazonSES
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All

Any thoughts, comments or questions regarding the proposal outlined at 
https://issues.apache.org/jira/browse/SPARK-5267?

Cheers
Steve

- - - - - - - - - - - - - - - - - -

This private and confidential e-mail has been sent to you by Synergy Systems Limited. It may not represent the views of Synergy Systems Limited.

If you are not the intended recipient of this e-mail and have received it in error, please notify the sender by replying with "received in error" as the subject and then delete it from your mailbox.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 16:45:27 2015
Return-Path: <dev-return-11242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB0B2C84D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 16:45:26 +0000 (UTC)
Received: (qmail 37314 invoked by uid 500); 21 Jan 2015 16:45:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37240 invoked by uid 500); 21 Jan 2015 16:45:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37227 invoked by uid 99); 21 Jan 2015 16:45:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 16:45:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 16:45:20 +0000
Received: by mail-ob0-f182.google.com with SMTP id gq1so3701813obb.13
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 08:44:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=gqVOr9DKl5BGnEI09Abs4DulxEzYOYT2e1EtF1Enpmw=;
        b=esjUGCou6YUDuE27SFOa3NDXiVLhEoGJGlJF2zG9QOanzNFoGfCWkYLrvOJenRCYDp
         wUAV2+BweMe82L6K252xKDHoz7ZSZZCH9KudkO7m8C+oZBwOoeuUdL02ZB8WhE2IRbdM
         7/IsS8HV0nLxbyRKG6vEFozrz2KpSq47oShzoro/M3S/PbWS7vI0UsIKei7YkPz3dUfU
         5sjVghffqpKxnuwk2rYwPliIbkD1D8T+DGwmQ0mwcXp15xdH+RoPIXhOVCN/B+sGXwLl
         wN1rfjDRBuEpj3DNsYYqGseViZXcJ8kYvYw7yUo1VgPjF+KFzA+PVogPZJo7G80mDLYP
         Ndog==
X-Received: by 10.202.228.9 with SMTP id b9mr15920083oih.40.1421858654718;
 Wed, 21 Jan 2015 08:44:14 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.18 with HTTP; Wed, 21 Jan 2015 08:43:34 -0800 (PST)
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Wed, 21 Jan 2015 14:43:34 -0200
Message-ID: <CAO4-Pq_eYH+Eo8vc1_yDxt3aUCTcfow5OBVhEDyEWB592oKiDQ@mail.gmail.com>
Subject: Issue with repartition and cache
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a114138600910e6050d2c4414
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114138600910e6050d2c4414
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi guys, have anyone find something like this?
I have a training set, and when I repartition it, if I call cache it throw
a classcastexception when I try to execute anything that access it

val rep120 =3D train.repartition(120)
val cached120 =3D rep120.cache
cached120.map(f =3D> f(1).asInstanceOf[Int]).sum

Cell Toolbar:
   In [1]:

ClusterSettings.executorMemory=3DSome("28g")

ClusterSettings.maxResultSize =3D "20g"

ClusterSettings.resume=3Dtrue

ClusterSettings.coreInstanceType=3D"r3.xlarge"

ClusterSettings.coreInstanceCount =3D 30

ClusterSettings.clusterName=3D"UberdataContextCluster-Dirceu"

uc.applyDateFormat("YYMMddHH")

Searching for existing cluster UberdataContextCluster-Dirceu ...
Spark standalone cluster started at
http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:8080
Found 1 master(s), 30 slaves
Ganglia started at
http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:5080/ganglia

In [37]:

import org.apache.spark.sql.catalyst.types._

import eleflow.uberdata.util.IntStringImplicitTypeConverter._

import eleflow.uberdata.enums.SupportedAlgorithm._

import eleflow.uberdata.data._

import org.apache.spark.mllib.tree.DecisionTree

import eleflow.uberdata.enums.DateSplitType._

import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.linalg.Vectors

import org.apache.spark.mllib.classification._

import eleflow.uberdata.model._

import eleflow.uberdata.data.stat.Statistics

import eleflow.uberdata.enums.ValidationMethod._

import org.apache.spark.rdd.RDD

In [5]:

val train =3D uc.load(uc.toHDFSURI("/tmp/data/input/train_rev4.csv")).apply=
ColumnTypes(Seq(DecimalType(),
LongType,TimestampType, StringType,


             StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


             StringType, StringType, StringType, StringType,


              LongType, LongType,StringType, StringType,StringType,


              StringType,StringType))

.formatDateValues(2,DayOfAWeek | Period).slice(excludes =3D Seq(12,13))

Out[5]:
idclickhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domai=
n
app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21
100000941815109427302.03.0100501fbe01fef384576728905ebdecad23867801e8d9
07d7df2244956a241215706320501722035-1791000016934911786371502.03.010050
1fbe01fef384576728905ebdecad23867801e8d907d7df22711ee1201015704320501722035
100084791000037190421511948602.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df228a4875bd1015704320501722035100084791000064072448083837602.0
3.0100501fbe01fef384576728905ebdecad23867801e8d907d7df226332421a10157063205=
0
1722035100084791000067905641704209602.03.010051fe8cc4489166c1610569f928
ecad23867801e8d907d7df22779d90c21018993320502161035-11571000072075780110386=
9
02.03.010050d6137915bb1ef334f028772becad23867801e8d907d7df228a4875bd1016920
32050189904311000771171000072472998854491102.03.0100508fda644b25d4cfcd
f028772becad23867801e8d907d7df22be6db1d71020362320502333039-1157
In [7]:

val test =3D uc.load(uc.toHDFSURI("/tmp/data/input/test_rev4.csv")).applyCo=
lumnTypes(Seq(DecimalType(),
TimestampType, StringType,


             StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


             StringType, StringType, StringType, StringType,


              LongType, LongType,StringType, StringType,StringType,


              StringType,StringType)).

formatDateValues(1,DayOfAWeek | Period).slice(excludes =3DSeq(11,12))

Out[7]:
idhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domain
app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21
100001740588092635695.03.010050235ba823f6ebf28ef028772becad23867801e8d9
07d7df220eb711ec10833032050761317510007523100001825269208554285.03.010050
1fbe01fef384576728905ebdecad23867801e8d907d7df22ecb851b21022676320502616035
10008351100005541398292139845.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df221f0bc64f102267632050261603510008351100010946378097988455.0
3.01005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8542422a710186483205=
0
1092380910015661100013770415586707455.03.01005085f751fdc4e18dd650e219e0
9c13b4192347f47af95efa071f0bc64f1023160320502667047-12211000152120415335372=
4
5.03.01005157fe1b205b626596f028772becad23867801e8d907d7df2268b6db2c10656332=
0
50572239-132100019110567070233785.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df22d4897fef102281332050264723910014823
In [ ]:

val (validationPrediction2, logRegModel2, testDataSet2,
validationDataSet2, trainDataSet2, testPrediction2) =3D

        eleflow.uberdata.data.Predictor.predict(train,test,excludes=3D
Seq(6,7,9,10,12,13), iterations =3D 100, algorithm =3D
BinaryLogisticRegressionBFGS)

spent time 1943

Out[5]:

MappedRDD[165] at map at Predictor.scala:265

In [ ]:

val corr2 =3D eleflow.uberdata.data.stat.Statistics.targetCorrelation(valid=
ationDataSet2)

In [ ]:

val correlated =3D corr2.filter(_._1>0.01)

In [ ]:

val correlated2 =3D correlated.map(_._2)

Out[8]:

Array(11768, 11285, 11278, 11289, 12051, 11279, 42, 11805, 11767, 46,
22, 12063, 20, 8388, 11438, 11783, 8981, 11408, 8386, 11360, 11377,
12059, 11418, 12044, 11771, 11359, 11839, 9118, 9116, 8666, 11986,
8665, 8888, 8887, 18, 12058, 11925, 11468, 11336, 11769, 9254, 9029,
11404, 9028, 71, 11982, 11798, 63, 7401, 8673, 12040, 8664, 4986, 452,
11949, 12050, 76, 11800, 8975, 11189, 11743, 11956, 11801, 12026,
8976, 11784, 2418, 11808, 12054, 11904, 1819, 7, 1840, 11429, 11608,
11983, 11387, 9403, 11495, 11985, 8658, 1020, 11626, 8384, 41, 8387,
11778, 4390, 7067, 11489, 11542, 3, 8381, 9154, 11766, 11479, 9077,
10782, 11680, 11830, 12043, 8926, 8982, 11409, 11391, 11364, 8656,
1274, 5523, 9, 12025, 8279, 1528, 10, 11490, 12046, 6771, 3937, 11450,
11811, 8632, 38, 8898, 11382, 12028, 12053, 4563, 5040, 11330, 1983,
11799, 11327, 11672, 8628, 11342, 11813, 6450, 11825, 8941, 10407,
11806, 11643, 8940, 9405, 11757, 9075, 12056, 11522, 11688, 10406,
11322, 9076, 29, 12064, 8637, 11347, 10831, 11406, 11773, 40, 10560,
11645, 9404, 11789, 11651, 9743, 11835, 11843, 9382, 11971, 11646,
12065, 11984, 8681, 10563, 12039, 9383, 8680, 8391, 3260, 5453, 10120,
8602, 11649, 9385, 4320, 9384, 11210, 11750, 11319, 11787, 11506,
11628, 11415, 11777, 10576, 240, 12017, 0, 10121, 11644, 8929, 11392,
12024, 5602, 9280, 11473, 884, 11812, 10741, 11780, 11503, 8672,
11357, 11966, 12055, 11539, 8644, 11350, 11836, 9058, 11271, 11764,
5094, 7881, 11504, 11698, 11424, 11831, 11587, 11426, 2577, 11610,
8948, 11987, 10744, 9290, 11477, 11497, 11367, 8622, 11969, 12030,
8062, 11664, 11704, 10949, 11508, 10530, 10225, 7655, 4274, 10534,
11394, 8934, 15, 11671, 11845, 12069, 6767, 3713, 8979, 11310, 10670,
8978, 11498, 11281, 11291, 11549, 11840, 10119, 10419, 897, 5875,
11482, 10617, 9331, 10618, 11662, 12060, 11496, 10654, 9742, 11422,
12027, 11545, 6612, 9757, 11881, 19, 11321, 11402, 11256, 8389, 9379,
9741, 11705, 5188, 2780, 8593, 11325, 9452, 11255, 9304, 11990, 8393,
11853, 11619, 9312, 9061, 11425, 8385, 11642, 12023, 9303, 8885,
11375, 6807, 8576, 11528, 11485, 11786, 8518, 11834, 12066, 2257,
11345, 11333, 11903, 9918, 11992, 11257, 11488, 11637, 7215, 10556,
11744, 12018, 12031, 1990, 542, 6099, 9005, 11900, 9739, 11566, 11481,
11314, 12052, 11307, 1828, 12072, 5, 10020, 11413, 10138, 11295, 8959,
8025)

In [ ]:

val trained =3D trainDataSet2.map{f =3D>

                                val array =3D f._2.features.toArray

                                new LabeledPoint(f._2.label,Vectors.dense(

correlated2.map(i =3D> array(i))))}.cache

Out[9]:

MappedRDD[175] at map at <console>:52

In [ ]:

val m =3D Predictor.binaryLogisticRegressionModelSGD(trained,100)

In [23]:

val validated =3D validationDataSet2.map{f =3D>

                                val array =3D f._2.features.toArray

                                (f._1,new LabeledPoint(f._2.label,Vectors.d=
ense(

correlated2.map(i =3D> array(i)))))}.cache

Out[23]:

MappedRDD[682] at map at <console>:71

In [24]:

val prediction =3D validated.map {

      case (ids, point) =3D>

        (ids, m.model.asInstanceOf[LogisticRegressionModel].predict(point.f=
eatures))

    }

Out[24]:

MappedRDD[683] at map at <console>:79

In [20]:

validated.take(2)

Out[20]:

Array((0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0]),
(0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0]))

In [26]:

val logloss =3D eleflow.uberdata.data.stat.Statistics.logLoss(prediction)

Out[26]:

5.861273254972684

In [17]:

validationDataSet2.take(3)

Out[17]:

Array((0,(0.0,(12073,[0,1,4,9,18,42,4563,8382,8386,8575,11279,11289,11322,1=
1766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.=
0,1.0,1.0,1.0,1.0]))),
(0,(0.0,(12073,[0,1,4,9,18,42,3260,8382,8386,8577,11279,11289,11322,11766,1=
1803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,=
1.0,1.0,1.0]))),
(0,(0.0,(12073,[0,1,4,10,40,42,4729,8382,8386,8672,11279,11289,11357,11768,=
11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0=
,1.0,1.0,1.0]))))

In [ ]:

trained.take(4)

In [7]:



import org.apache.spark.mllib.classification._

In [8]:

val steps =3D Seq(Step(10,2),new Step(7,3), new Step(6,4))

Out[8]:
0Step(10,2)1Step(7,3)2Step(6,4)
In [ ]:

val predictor  =3D
eleflow.uberdata.data.Predictor.evolutivePredict(train.repartition(240),
test, algorithm =3D BinaryLogisticRegressionBFGS,

                             validationMethod =3D LogarithmicLoss, steps
=3D steps, iterations =3D 30)

In [ ]:

uc.terminate

In [11]:

train.partitions.size

Out[11]:

94

In [20]:

val rep60 =3D train.repartition(120)

Out[20]:
idclickhourC1banner_possite_idsite_domainsite_categoryapp_idapp_domain
app_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14C15
C16C17C18C19C20C211769841751868484765301410221210051e8f79e60c4342784f028772=
b
ecad23867801e8d907d7df22a99f214ab526ff2ce9b8d8d71020634320502374339-123
17703074559452740131141022121005085f751fdc4e18dd650e219e0399477562347f47a
cef3e64932d58615ab5a307674de3ee61221768320502506035-11571770805478454288971=
1
0141022121005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8a99f214a
d30ecac3542422a7102161132050248032971001116117713001998424865357114102212
1005085f751fdc4e18dd650e219e0bc44c87d7801e8d90f2161f8ad97ca8caa305f51
43836a961020633320502374339-123177175933008005586270141022121005085f751fd
c4e18dd650e219e0f888bf4c5b9c592b0f2161f89a5442e768bc961a1f0bc64f10211533205=
0
2420235-169177224932175731189110141022121005085f751fdc4e18dd650e219e0
e96773f02347f47a0f2161f8a99f214abf741817ef726eae1021767320502506035-1157
17727816327614515164014102212100505bcf81a29d54950bf028772becad23867801e8d9
07d7df22a99f214a5e4ee78bbe87996b1221770320502507035100176157
In [ ]:

val cach60 =3D rep60.cache

In [28]:

cach60.map(f =3D> f(1).asInstanceOf[Int]).sum

org.apache.spark.SparkException: Job aborted due to stage failure:
Task 53 in stage 53.0 failed 4 times, most recent failure: Lost task
53.3 in stage 53.0 (TID 1322,
ip-172-31-0-62.us-west-2.compute.internal):
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$=
iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonf=
un$1.apply(<console>:59)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$=
iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonf=
un$1.apply(<console>:59)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala=
:172)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:853)
	at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:851)
	at org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350=
)
	at org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350=
)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.ja=
va:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.j=
ava:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGS=
cheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAG=
Scheduler.scala:1203)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAG=
Scheduler.scala:1202)
    scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc=
ala:59)
    scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1=
202)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.=
apply(DAGScheduler.scala:696)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.=
apply(DAGScheduler.scala:696)
    scala.Option.foreach(Option.scala:236)
    org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGSchedule=
r.scala:696)
    org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$recei=
ve$2.applyOrElse(DAGScheduler.scala:1420)
    akka.actor.Actor$class.aroundReceive(Actor.scala:465)
    org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(=
DAGScheduler.scala:1375)
    akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
    akka.actor.ActorCell.invoke(ActorCell.scala:487)
    akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
    akka.dispatch.Mailbox.run(Mailbox.scala:220)
    akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abstra=
ctDispatcher.scala:393)
    scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.j=
ava:1339)
    scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979=
)
    scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread=
.java:107)



R

--001a114138600910e6050d2c4414--

From dev-return-11243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 19:10:53 2015
Return-Path: <dev-return-11243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C06BCD0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 19:10:53 +0000 (UTC)
Received: (qmail 48396 invoked by uid 500); 21 Jan 2015 18:58:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30246 invoked by uid 500); 21 Jan 2015 18:58:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10636 invoked by uid 99); 21 Jan 2015 18:42:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 18:42:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 18:42:21 +0000
Received: by mail-ob0-f181.google.com with SMTP id nt9so10291586obb.12
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 10:41:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=iulreG/qXXQfaE+9qa/v+txbYKBc/FvhqrTj/vzDuJo=;
        b=rgwO7J9yyab0Fp0ak2ys3cysc2mZGKqY8a2A9sCnfliR3APH5s7TgkwzKd2uB6ljZY
         CdRzm59QdPgBzv0TelR7atvjceQhf7wHswnyygTYbpF7j1qrqZ77aR72A7VkIUZiiypt
         ICU3GEiAo52pDW9vUZ9Z4fWnJfP154rLtDMlg8zCnh0YMGlBqvJv4NsoFTPXTSvYpHv+
         6EObMjbqN6Pwg2A86CRQeTwLwmr5uNarweWcY8JcNEwrl5cJaGIBJHz8i1aT7wAn2Jpq
         wa8OXOmcI/MGE/XkMgDOznUtmUmFZKVEpqDjzbD5MweSLcYxPTyfESZpY3r7L2l2W4JD
         MbVQ==
X-Received: by 10.60.230.6 with SMTP id su6mr23121894oec.44.1421865694100;
 Wed, 21 Jan 2015 10:41:34 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.18 with HTTP; Wed, 21 Jan 2015 10:40:53 -0800 (PST)
In-Reply-To: <CACBYxKLcqfa=bO62ScaUd1nebpRQc_4th+-5_zKr9GuDu2gnSQ@mail.gmail.com>
References: <CAO4-Pq_eYH+Eo8vc1_yDxt3aUCTcfow5OBVhEDyEWB592oKiDQ@mail.gmail.com>
 <CACBYxKLcqfa=bO62ScaUd1nebpRQc_4th+-5_zKr9GuDu2gnSQ@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Wed, 21 Jan 2015 16:40:53 -0200
Message-ID: <CAO4-Pq9pPgRbezeLUpsdd6csXBwYGysmOc_aCOYJrRWsP7AmSw@mail.gmail.com>
Subject: Re: Issue with repartition and cache
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136400a9d8442050d2de7fa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136400a9d8442050d2de7fa
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Sandy, thanks for the reply.

I tried to run this code without the cache and it worked.
Also if I cache before repartition, it also works, the problem seems to be
something related with repartition and caching.
My train is a SchemaRDD, and if I make all my columns as StringType, the
error doesn't happen, but if I have anything else, this exception is thrown=
.



2015-01-21 16:37 GMT-02:00 Sandy Ryza <sandy.ryza@cloudera.com>:

> Hi Dirceu,
>
> Does the issue not show up if you run "map(f =3D>
> f(1).asInstanceOf[Int]).sum" on the "train" RDD?  It appears that f(1) is
> an String, not an Int.  If you're looking to parse and convert it, "toInt=
"
> should be used instead of "asInstanceOf".
>
> -Sandy
>
> On Wed, Jan 21, 2015 at 8:43 AM, Dirceu Semighini Filho <
> dirceu.semighini@gmail.com> wrote:
>
>> Hi guys, have anyone find something like this?
>> I have a training set, and when I repartition it, if I call cache it thr=
ow
>> a classcastexception when I try to execute anything that access it
>>
>> val rep120 =3D train.repartition(120)
>> val cached120 =3D rep120.cache
>> cached120.map(f =3D> f(1).asInstanceOf[Int]).sum
>>
>> Cell Toolbar:
>>    In [1]:
>>
>> ClusterSettings.executorMemory=3DSome("28g")
>>
>> ClusterSettings.maxResultSize =3D "20g"
>>
>> ClusterSettings.resume=3Dtrue
>>
>> ClusterSettings.coreInstanceType=3D"r3.xlarge"
>>
>> ClusterSettings.coreInstanceCount =3D 30
>>
>> ClusterSettings.clusterName=3D"UberdataContextCluster-Dirceu"
>>
>> uc.applyDateFormat("YYMMddHH")
>>
>> Searching for existing cluster UberdataContextCluster-Dirceu ...
>> Spark standalone cluster started at
>> http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:8080
>> Found 1 master(s), 30 slaves
>> Ganglia started at
>> http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:5080/ganglia
>>
>> In [37]:
>>
>> import org.apache.spark.sql.catalyst.types._
>>
>> import eleflow.uberdata.util.IntStringImplicitTypeConverter._
>>
>> import eleflow.uberdata.enums.SupportedAlgorithm._
>>
>> import eleflow.uberdata.data._
>>
>> import org.apache.spark.mllib.tree.DecisionTree
>>
>> import eleflow.uberdata.enums.DateSplitType._
>>
>> import org.apache.spark.mllib.regression.LabeledPoint
>>
>> import org.apache.spark.mllib.linalg.Vectors
>>
>> import org.apache.spark.mllib.classification._
>>
>> import eleflow.uberdata.model._
>>
>> import eleflow.uberdata.data.stat.Statistics
>>
>> import eleflow.uberdata.enums.ValidationMethod._
>>
>> import org.apache.spark.rdd.RDD
>>
>> In [5]:
>>
>> val train =3D
>> uc.load(uc.toHDFSURI("/tmp/data/input/train_rev4.csv")).applyColumnTypes=
(Seq(DecimalType(),
>> LongType,TimestampType, StringType,
>>
>>
>>              StringType, StringType, StringType, StringType,
>>
>>
>>               StringType, StringType, StringType, StringType,
>>
>>
>>               StringType, StringType, StringType, StringType,
>>
>>
>>              StringType, StringType, StringType, StringType,
>>
>>
>>               LongType, LongType,StringType, StringType,StringType,
>>
>>
>>               StringType,StringType))
>>
>> .formatDateValues(2,DayOfAWeek | Period).slice(excludes =3D Seq(12,13))
>>
>> Out[5]:
>>
>> idclickhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_do=
main
>>
>> app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20=
C21
>> 100000941815109427302.03.0100501fbe01fef384576728905ebdecad23867801e8d9
>> 07d7df2244956a241215706320501722035-1791000016934911786371502.03.010050
>>
>> 1fbe01fef384576728905ebdecad23867801e8d907d7df22711ee1201015704320501722=
035
>> 100084791000037190421511948602.03.0100501fbe01fef384576728905ebdecad2386
>>
>> 7801e8d907d7df228a4875bd101570432050172203510008479100006407244808383760=
2.0
>>
>> 3.0100501fbe01fef384576728905ebdecad23867801e8d907d7df226332421a10157063=
2050
>> 1722035100084791000067905641704209602.03.010051fe8cc4489166c1610569f928
>>
>> ecad23867801e8d907d7df22779d90c21018993320502161035-11571000072075780110=
3869
>>
>> 02.03.010050d6137915bb1ef334f028772becad23867801e8d907d7df228a4875bd1016=
920
>> 32050189904311000771171000072472998854491102.03.0100508fda644b25d4cfcd
>> f028772becad23867801e8d907d7df22be6db1d71020362320502333039-1157
>> In [7]:
>>
>> val test =3D
>> uc.load(uc.toHDFSURI("/tmp/data/input/test_rev4.csv")).applyColumnTypes(=
Seq(DecimalType(),
>> TimestampType, StringType,
>>
>>
>>              StringType, StringType, StringType, StringType,
>>
>>
>>               StringType, StringType, StringType, StringType,
>>
>>
>>               StringType, StringType, StringType, StringType,
>>
>>
>>              StringType, StringType, StringType, StringType,
>>
>>
>>               LongType, LongType,StringType, StringType,StringType,
>>
>>
>>               StringType,StringType)).
>>
>> formatDateValues(1,DayOfAWeek | Period).slice(excludes =3DSeq(11,12))
>>
>> Out[7]:
>> idhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domain
>>
>> app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20=
C21
>> 100001740588092635695.03.010050235ba823f6ebf28ef028772becad23867801e8d9
>> 07d7df220eb711ec10833032050761317510007523100001825269208554285.03.01005=
0
>>
>> 1fbe01fef384576728905ebdecad23867801e8d907d7df22ecb851b21022676320502616=
035
>> 10008351100005541398292139845.03.0100501fbe01fef384576728905ebdecad2386
>> 7801e8d907d7df221f0bc64f102267632050261603510008351100010946378097988455=
.0
>>
>> 3.01005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8542422a710186483=
2050
>> 1092380910015661100013770415586707455.03.01005085f751fdc4e18dd650e219e0
>>
>> 9c13b4192347f47af95efa071f0bc64f1023160320502667047-12211000152120415335=
3724
>>
>> 5.03.01005157fe1b205b626596f028772becad23867801e8d907d7df2268b6db2c10656=
3320
>>
>> 50572239-132100019110567070233785.03.0100501fbe01fef384576728905ebdecad2=
386
>> 7801e8d907d7df22d4897fef102281332050264723910014823
>> In [ ]:
>>
>> val (validationPrediction2, logRegModel2, testDataSet2,
>> validationDataSet2, trainDataSet2, testPrediction2) =3D
>>
>>         eleflow.uberdata.data.Predictor.predict(train,test,excludes=3D
>> Seq(6,7,9,10,12,13), iterations =3D 100, algorithm =3D
>> BinaryLogisticRegressionBFGS)
>>
>> spent time 1943
>>
>> Out[5]:
>>
>> MappedRDD[165] at map at Predictor.scala:265
>>
>> In [ ]:
>>
>> val corr2 =3D
>> eleflow.uberdata.data.stat.Statistics.targetCorrelation(validationDataSe=
t2)
>>
>> In [ ]:
>>
>> val correlated =3D corr2.filter(_._1>0.01)
>>
>> In [ ]:
>>
>> val correlated2 =3D correlated.map(_._2)
>>
>> Out[8]:
>>
>> Array(11768, 11285, 11278, 11289, 12051, 11279, 42, 11805, 11767, 46,
>> 22, 12063, 20, 8388, 11438, 11783, 8981, 11408, 8386, 11360, 11377,
>> 12059, 11418, 12044, 11771, 11359, 11839, 9118, 9116, 8666, 11986,
>> 8665, 8888, 8887, 18, 12058, 11925, 11468, 11336, 11769, 9254, 9029,
>> 11404, 9028, 71, 11982, 11798, 63, 7401, 8673, 12040, 8664, 4986, 452,
>> 11949, 12050, 76, 11800, 8975, 11189, 11743, 11956, 11801, 12026,
>> 8976, 11784, 2418, 11808, 12054, 11904, 1819, 7, 1840, 11429, 11608,
>> 11983, 11387, 9403, 11495, 11985, 8658, 1020, 11626, 8384, 41, 8387,
>> 11778, 4390, 7067, 11489, 11542, 3, 8381, 9154, 11766, 11479, 9077,
>> 10782, 11680, 11830, 12043, 8926, 8982, 11409, 11391, 11364, 8656,
>> 1274, 5523, 9, 12025, 8279, 1528, 10, 11490, 12046, 6771, 3937, 11450,
>> 11811, 8632, 38, 8898, 11382, 12028, 12053, 4563, 5040, 11330, 1983,
>> 11799, 11327, 11672, 8628, 11342, 11813, 6450, 11825, 8941, 10407,
>> 11806, 11643, 8940, 9405, 11757, 9075, 12056, 11522, 11688, 10406,
>> 11322, 9076, 29, 12064, 8637, 11347, 10831, 11406, 11773, 40, 10560,
>> 11645, 9404, 11789, 11651, 9743, 11835, 11843, 9382, 11971, 11646,
>> 12065, 11984, 8681, 10563, 12039, 9383, 8680, 8391, 3260, 5453, 10120,
>> 8602, 11649, 9385, 4320, 9384, 11210, 11750, 11319, 11787, 11506,
>> 11628, 11415, 11777, 10576, 240, 12017, 0, 10121, 11644, 8929, 11392,
>> 12024, 5602, 9280, 11473, 884, 11812, 10741, 11780, 11503, 8672,
>> 11357, 11966, 12055, 11539, 8644, 11350, 11836, 9058, 11271, 11764,
>> 5094, 7881, 11504, 11698, 11424, 11831, 11587, 11426, 2577, 11610,
>> 8948, 11987, 10744, 9290, 11477, 11497, 11367, 8622, 11969, 12030,
>> 8062, 11664, 11704, 10949, 11508, 10530, 10225, 7655, 4274, 10534,
>> 11394, 8934, 15, 11671, 11845, 12069, 6767, 3713, 8979, 11310, 10670,
>> 8978, 11498, 11281, 11291, 11549, 11840, 10119, 10419, 897, 5875,
>> 11482, 10617, 9331, 10618, 11662, 12060, 11496, 10654, 9742, 11422,
>> 12027, 11545, 6612, 9757, 11881, 19, 11321, 11402, 11256, 8389, 9379,
>> 9741, 11705, 5188, 2780, 8593, 11325, 9452, 11255, 9304, 11990, 8393,
>> 11853, 11619, 9312, 9061, 11425, 8385, 11642, 12023, 9303, 8885,
>> 11375, 6807, 8576, 11528, 11485, 11786, 8518, 11834, 12066, 2257,
>> 11345, 11333, 11903, 9918, 11992, 11257, 11488, 11637, 7215, 10556,
>> 11744, 12018, 12031, 1990, 542, 6099, 9005, 11900, 9739, 11566, 11481,
>> 11314, 12052, 11307, 1828, 12072, 5, 10020, 11413, 10138, 11295, 8959,
>> 8025)
>>
>> In [ ]:
>>
>> val trained =3D trainDataSet2.map{f =3D>
>>
>>                                 val array =3D f._2.features.toArray
>>
>>                                 new LabeledPoint(f._2.label,Vectors.dens=
e(
>>
>> correlated2.map(i =3D> array(i))))}.cache
>>
>> Out[9]:
>>
>> MappedRDD[175] at map at <console>:52
>>
>> In [ ]:
>>
>> val m =3D Predictor.binaryLogisticRegressionModelSGD(trained,100)
>>
>> In [23]:
>>
>> val validated =3D validationDataSet2.map{f =3D>
>>
>>                                 val array =3D f._2.features.toArray
>>
>>                                 (f._1,new
>> LabeledPoint(f._2.label,Vectors.dense(
>>
>> correlated2.map(i =3D> array(i)))))}.cache
>>
>> Out[23]:
>>
>> MappedRDD[682] at map at <console>:71
>>
>> In [24]:
>>
>> val prediction =3D validated.map {
>>
>>       case (ids, point) =3D>
>>
>>         (ids,
>> m.model.asInstanceOf[LogisticRegressionModel].predict(point.features))
>>
>>     }
>>
>> Out[24]:
>>
>> MappedRDD[683] at map at <console>:79
>>
>> In [20]:
>>
>> validated.take(2)
>>
>> Out[20]:
>>
>>
>> Array((0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0]),
>>
>> (0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0]))
>>
>> In [26]:
>>
>> val logloss =3D eleflow.uberdata.data.stat.Statistics.logLoss(prediction=
)
>>
>> Out[26]:
>>
>> 5.861273254972684
>>
>> In [17]:
>>
>> validationDataSet2.take(3)
>>
>> Out[17]:
>>
>>
>> Array((0,(0.0,(12073,[0,1,4,9,18,42,4563,8382,8386,8575,11279,11289,1132=
2,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0=
,1.0,1.0,1.0,1.0,1.0]))),
>>
>> (0,(0.0,(12073,[0,1,4,9,18,42,3260,8382,8386,8577,11279,11289,11322,1176=
6,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1=
.0,1.0,1.0,1.0]))),
>>
>> (0,(0.0,(12073,[0,1,4,10,40,42,4729,8382,8386,8672,11279,11289,11357,117=
68,11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,=
1.0,1.0,1.0,1.0]))))
>>
>> In [ ]:
>>
>> trained.take(4)
>>
>> In [7]:
>>
>>
>>
>> import org.apache.spark.mllib.classification._
>>
>> In [8]:
>>
>> val steps =3D Seq(Step(10,2),new Step(7,3), new Step(6,4))
>>
>> Out[8]:
>> 0Step(10,2)1Step(7,3)2Step(6,4)
>> In [ ]:
>>
>> val predictor  =3D
>> eleflow.uberdata.data.Predictor.evolutivePredict(train.repartition(240),
>> test, algorithm =3D BinaryLogisticRegressionBFGS,
>>
>>                              validationMethod =3D LogarithmicLoss, steps
>> =3D steps, iterations =3D 30)
>>
>> In [ ]:
>>
>> uc.terminate
>>
>> In [11]:
>>
>> train.partitions.size
>>
>> Out[11]:
>>
>> 94
>>
>> In [20]:
>>
>> val rep60 =3D train.repartition(120)
>>
>> Out[20]:
>> idclickhourC1banner_possite_idsite_domainsite_categoryapp_idapp_domain
>>
>> app_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14=
C15
>>
>> C16C17C18C19C20C211769841751868484765301410221210051e8f79e60c4342784f028=
772b
>> ecad23867801e8d907d7df22a99f214ab526ff2ce9b8d8d71020634320502374339-123
>> 17703074559452740131141022121005085f751fdc4e18dd650e219e0399477562347f47=
a
>>
>> cef3e64932d58615ab5a307674de3ee61221768320502506035-11571770805478454288=
9711
>> 0141022121005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8a99f214a
>> d30ecac3542422a710216113205024803297100111611771300199842486535711410221=
2
>> 1005085f751fdc4e18dd650e219e0bc44c87d7801e8d90f2161f8ad97ca8caa305f51
>> 43836a961020633320502374339-123177175933008005586270141022121005085f751f=
d
>>
>> c4e18dd650e219e0f888bf4c5b9c592b0f2161f89a5442e768bc961a1f0bc64f10211533=
2050
>> 2420235-169177224932175731189110141022121005085f751fdc4e18dd650e219e0
>> e96773f02347f47a0f2161f8a99f214abf741817ef726eae1021767320502506035-1157
>> 17727816327614515164014102212100505bcf81a29d54950bf028772becad23867801e8=
d9
>> 07d7df22a99f214a5e4ee78bbe87996b1221770320502507035100176157
>> In [ ]:
>>
>> val cach60 =3D rep60.cache
>>
>> In [28]:
>>
>> cach60.map(f =3D> f(1).asInstanceOf[Int]).sum
>>
>> org.apache.spark.SparkException: Job aborted due to stage failure:
>> Task 53 in stage 53.0 failed 4 times, most recent failure: Lost task
>> 53.3 in stage 53.0 (TID 1322,
>> ip-172-31-0-62.us-west-2.compute.internal):
>> java.lang.ClassCastException: java.lang.String cannot be cast to
>> java.lang.Integer
>>         at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
>>         at
>> $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$i=
wC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfu=
n$1.apply(<console>:59)
>>         at
>> $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$i=
wC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfu=
n$1.apply(<console>:59)
>>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>         at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>         at scala.collection.AbstractIterator.foreach(Iterator.scala:1157=
)
>>         at
>> scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:=
172)
>>         at
>> scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1157)
>>         at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:853)
>>         at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:851)
>>         at
>> org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
>>         at
>> org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
>>         at
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>>         at org.apache.spark.scheduler.Task.run(Task.scala:56)
>>         at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
>>         at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jav=
a:1145)
>>         at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.ja=
va:615)
>>         at java.lang.Thread.run(Thread.java:745)
>>
>> Driver stacktrace:
>>     org.apache.spark.scheduler.DAGScheduler.org
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSch=
eduler.scala:1214)
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGS=
cheduler.scala:1203)
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGS=
cheduler.scala:1202)
>>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sca=
la:59)
>>     scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:12=
02)
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.a=
pply(DAGScheduler.scala:696)
>>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.a=
pply(DAGScheduler.scala:696)
>>     scala.Option.foreach(Option.scala:236)
>>
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler=
.scala:696)
>>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receiv=
e$2.applyOrElse(DAGScheduler.scala:1420)
>>     akka.actor.Actor$class.aroundReceive(Actor.scala:465)
>>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(D=
AGScheduler.scala:1375)
>>     akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
>>     akka.actor.ActorCell.invoke(ActorCell.scala:487)
>>     akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
>>     akka.dispatch.Mailbox.run(Mailbox.scala:220)
>>
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abstrac=
tDispatcher.scala:393)
>>     scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.ja=
va:1339)
>>
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.=
java:107)
>>
>>
>>
>> R
>>
>
>

--001a1136400a9d8442050d2de7fa--

From dev-return-11244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 19:11:02 2015
Return-Path: <dev-return-11244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 84FD3CD5E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 19:11:02 +0000 (UTC)
Received: (qmail 50656 invoked by uid 500); 21 Jan 2015 18:58:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33145 invoked by uid 500); 21 Jan 2015 18:58:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10392 invoked by uid 99); 21 Jan 2015 18:38:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 18:38:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 18:38:15 +0000
Received: by mail-ie0-f180.google.com with SMTP id rl12so9626778iec.11
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 10:37:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=c0rKZ3OANZhziXnjSS1oK4e6Xhlb+BFhNkj5lsNdgAM=;
        b=F8EjqyvcdWECFmY6+M3tUNxXUMB+DAu7Tj+SLAw4sB4GSVv+WHFTAGfPBpnWffn+j7
         m7nalgL7t375xFNXC05rR7MZwDHGs7a6mVfvgMrh5JyjDDcCoTg+PmAZUnkdc/PwmmFf
         B8fc+5tC6v+JrmVfou18fdoRz4QFIbf2H0XFeIKfQCqr9QGCfDWFDxG6hBjGieJEk7f4
         kVywTzk7blcPA3z8/kLpzcJ+K/gpTbdtSlAAS22hs/i4AIKnR3luzcP/L/Co/TLCHmDC
         nrxQFf5Oc3Qb6/v6PR6kHBXQk3UkH6SEgQGJ2U0vpiU78ebPR0mM9ceIO567yWz3kdHQ
         2hUQ==
X-Gm-Message-State: ALoCoQnaeuxcZe1kWJ3vatNyb/+IWla2mLxGbEo94eH+tsk/Z/U6nREVAirYapiVbytiowATL26n
MIME-Version: 1.0
X-Received: by 10.50.13.97 with SMTP id g1mr6367964igc.42.1421865474625; Wed,
 21 Jan 2015 10:37:54 -0800 (PST)
Received: by 10.36.28.13 with HTTP; Wed, 21 Jan 2015 10:37:54 -0800 (PST)
In-Reply-To: <CAO4-Pq_eYH+Eo8vc1_yDxt3aUCTcfow5OBVhEDyEWB592oKiDQ@mail.gmail.com>
References: <CAO4-Pq_eYH+Eo8vc1_yDxt3aUCTcfow5OBVhEDyEWB592oKiDQ@mail.gmail.com>
Date: Wed, 21 Jan 2015 10:37:54 -0800
Message-ID: <CACBYxKLcqfa=bO62ScaUd1nebpRQc_4th+-5_zKr9GuDu2gnSQ@mail.gmail.com>
Subject: Re: Issue with repartition and cache
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013d06e8889dfb050d2ddae3
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d06e8889dfb050d2ddae3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Dirceu,

Does the issue not show up if you run "map(f =3D>
f(1).asInstanceOf[Int]).sum" on the "train" RDD?  It appears that f(1) is
an String, not an Int.  If you're looking to parse and convert it, "toInt"
should be used instead of "asInstanceOf".

-Sandy

On Wed, Jan 21, 2015 at 8:43 AM, Dirceu Semighini Filho <
dirceu.semighini@gmail.com> wrote:

> Hi guys, have anyone find something like this?
> I have a training set, and when I repartition it, if I call cache it thro=
w
> a classcastexception when I try to execute anything that access it
>
> val rep120 =3D train.repartition(120)
> val cached120 =3D rep120.cache
> cached120.map(f =3D> f(1).asInstanceOf[Int]).sum
>
> Cell Toolbar:
>    In [1]:
>
> ClusterSettings.executorMemory=3DSome("28g")
>
> ClusterSettings.maxResultSize =3D "20g"
>
> ClusterSettings.resume=3Dtrue
>
> ClusterSettings.coreInstanceType=3D"r3.xlarge"
>
> ClusterSettings.coreInstanceCount =3D 30
>
> ClusterSettings.clusterName=3D"UberdataContextCluster-Dirceu"
>
> uc.applyDateFormat("YYMMddHH")
>
> Searching for existing cluster UberdataContextCluster-Dirceu ...
> Spark standalone cluster started at
> http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:8080
> Found 1 master(s), 30 slaves
> Ganglia started at
> http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:5080/ganglia
>
> In [37]:
>
> import org.apache.spark.sql.catalyst.types._
>
> import eleflow.uberdata.util.IntStringImplicitTypeConverter._
>
> import eleflow.uberdata.enums.SupportedAlgorithm._
>
> import eleflow.uberdata.data._
>
> import org.apache.spark.mllib.tree.DecisionTree
>
> import eleflow.uberdata.enums.DateSplitType._
>
> import org.apache.spark.mllib.regression.LabeledPoint
>
> import org.apache.spark.mllib.linalg.Vectors
>
> import org.apache.spark.mllib.classification._
>
> import eleflow.uberdata.model._
>
> import eleflow.uberdata.data.stat.Statistics
>
> import eleflow.uberdata.enums.ValidationMethod._
>
> import org.apache.spark.rdd.RDD
>
> In [5]:
>
> val train =3D
> uc.load(uc.toHDFSURI("/tmp/data/input/train_rev4.csv")).applyColumnTypes(=
Seq(DecimalType(),
> LongType,TimestampType, StringType,
>
>
>              StringType, StringType, StringType, StringType,
>
>
>               StringType, StringType, StringType, StringType,
>
>
>               StringType, StringType, StringType, StringType,
>
>
>              StringType, StringType, StringType, StringType,
>
>
>               LongType, LongType,StringType, StringType,StringType,
>
>
>               StringType,StringType))
>
> .formatDateValues(2,DayOfAWeek | Period).slice(excludes =3D Seq(12,13))
>
> Out[5]:
>
> idclickhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_dom=
ain
> app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C=
21
> 100000941815109427302.03.0100501fbe01fef384576728905ebdecad23867801e8d9
> 07d7df2244956a241215706320501722035-1791000016934911786371502.03.010050
> 1fbe01fef384576728905ebdecad23867801e8d907d7df22711ee12010157043205017220=
35
> 100084791000037190421511948602.03.0100501fbe01fef384576728905ebdecad2386
> 7801e8d907d7df228a4875bd1015704320501722035100084791000064072448083837602=
.0
>
> 3.0100501fbe01fef384576728905ebdecad23867801e8d907d7df226332421a101570632=
050
> 1722035100084791000067905641704209602.03.010051fe8cc4489166c1610569f928
>
> ecad23867801e8d907d7df22779d90c21018993320502161035-115710000720757801103=
869
> 02.03.010050d6137915bb1ef334f028772becad23867801e8d907d7df228a4875bd10169=
20
> 32050189904311000771171000072472998854491102.03.0100508fda644b25d4cfcd
> f028772becad23867801e8d907d7df22be6db1d71020362320502333039-1157
> In [7]:
>
> val test =3D
> uc.load(uc.toHDFSURI("/tmp/data/input/test_rev4.csv")).applyColumnTypes(S=
eq(DecimalType(),
> TimestampType, StringType,
>
>
>              StringType, StringType, StringType, StringType,
>
>
>               StringType, StringType, StringType, StringType,
>
>
>               StringType, StringType, StringType, StringType,
>
>
>              StringType, StringType, StringType, StringType,
>
>
>               LongType, LongType,StringType, StringType,StringType,
>
>
>               StringType,StringType)).
>
> formatDateValues(1,DayOfAWeek | Period).slice(excludes =3DSeq(11,12))
>
> Out[7]:
> idhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domain
> app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C=
21
> 100001740588092635695.03.010050235ba823f6ebf28ef028772becad23867801e8d9
> 07d7df220eb711ec10833032050761317510007523100001825269208554285.03.010050
> 1fbe01fef384576728905ebdecad23867801e8d907d7df22ecb851b210226763205026160=
35
> 10008351100005541398292139845.03.0100501fbe01fef384576728905ebdecad2386
> 7801e8d907d7df221f0bc64f102267632050261603510008351100010946378097988455.=
0
>
> 3.01005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8542422a7101864832=
050
> 1092380910015661100013770415586707455.03.01005085f751fdc4e18dd650e219e0
>
> 9c13b4192347f47af95efa071f0bc64f1023160320502667047-122110001521204153353=
724
>
> 5.03.01005157fe1b205b626596f028772becad23867801e8d907d7df2268b6db2c106563=
320
> 50572239-132100019110567070233785.03.0100501fbe01fef384576728905ebdecad23=
86
> 7801e8d907d7df22d4897fef102281332050264723910014823
> In [ ]:
>
> val (validationPrediction2, logRegModel2, testDataSet2,
> validationDataSet2, trainDataSet2, testPrediction2) =3D
>
>         eleflow.uberdata.data.Predictor.predict(train,test,excludes=3D
> Seq(6,7,9,10,12,13), iterations =3D 100, algorithm =3D
> BinaryLogisticRegressionBFGS)
>
> spent time 1943
>
> Out[5]:
>
> MappedRDD[165] at map at Predictor.scala:265
>
> In [ ]:
>
> val corr2 =3D
> eleflow.uberdata.data.stat.Statistics.targetCorrelation(validationDataSet=
2)
>
> In [ ]:
>
> val correlated =3D corr2.filter(_._1>0.01)
>
> In [ ]:
>
> val correlated2 =3D correlated.map(_._2)
>
> Out[8]:
>
> Array(11768, 11285, 11278, 11289, 12051, 11279, 42, 11805, 11767, 46,
> 22, 12063, 20, 8388, 11438, 11783, 8981, 11408, 8386, 11360, 11377,
> 12059, 11418, 12044, 11771, 11359, 11839, 9118, 9116, 8666, 11986,
> 8665, 8888, 8887, 18, 12058, 11925, 11468, 11336, 11769, 9254, 9029,
> 11404, 9028, 71, 11982, 11798, 63, 7401, 8673, 12040, 8664, 4986, 452,
> 11949, 12050, 76, 11800, 8975, 11189, 11743, 11956, 11801, 12026,
> 8976, 11784, 2418, 11808, 12054, 11904, 1819, 7, 1840, 11429, 11608,
> 11983, 11387, 9403, 11495, 11985, 8658, 1020, 11626, 8384, 41, 8387,
> 11778, 4390, 7067, 11489, 11542, 3, 8381, 9154, 11766, 11479, 9077,
> 10782, 11680, 11830, 12043, 8926, 8982, 11409, 11391, 11364, 8656,
> 1274, 5523, 9, 12025, 8279, 1528, 10, 11490, 12046, 6771, 3937, 11450,
> 11811, 8632, 38, 8898, 11382, 12028, 12053, 4563, 5040, 11330, 1983,
> 11799, 11327, 11672, 8628, 11342, 11813, 6450, 11825, 8941, 10407,
> 11806, 11643, 8940, 9405, 11757, 9075, 12056, 11522, 11688, 10406,
> 11322, 9076, 29, 12064, 8637, 11347, 10831, 11406, 11773, 40, 10560,
> 11645, 9404, 11789, 11651, 9743, 11835, 11843, 9382, 11971, 11646,
> 12065, 11984, 8681, 10563, 12039, 9383, 8680, 8391, 3260, 5453, 10120,
> 8602, 11649, 9385, 4320, 9384, 11210, 11750, 11319, 11787, 11506,
> 11628, 11415, 11777, 10576, 240, 12017, 0, 10121, 11644, 8929, 11392,
> 12024, 5602, 9280, 11473, 884, 11812, 10741, 11780, 11503, 8672,
> 11357, 11966, 12055, 11539, 8644, 11350, 11836, 9058, 11271, 11764,
> 5094, 7881, 11504, 11698, 11424, 11831, 11587, 11426, 2577, 11610,
> 8948, 11987, 10744, 9290, 11477, 11497, 11367, 8622, 11969, 12030,
> 8062, 11664, 11704, 10949, 11508, 10530, 10225, 7655, 4274, 10534,
> 11394, 8934, 15, 11671, 11845, 12069, 6767, 3713, 8979, 11310, 10670,
> 8978, 11498, 11281, 11291, 11549, 11840, 10119, 10419, 897, 5875,
> 11482, 10617, 9331, 10618, 11662, 12060, 11496, 10654, 9742, 11422,
> 12027, 11545, 6612, 9757, 11881, 19, 11321, 11402, 11256, 8389, 9379,
> 9741, 11705, 5188, 2780, 8593, 11325, 9452, 11255, 9304, 11990, 8393,
> 11853, 11619, 9312, 9061, 11425, 8385, 11642, 12023, 9303, 8885,
> 11375, 6807, 8576, 11528, 11485, 11786, 8518, 11834, 12066, 2257,
> 11345, 11333, 11903, 9918, 11992, 11257, 11488, 11637, 7215, 10556,
> 11744, 12018, 12031, 1990, 542, 6099, 9005, 11900, 9739, 11566, 11481,
> 11314, 12052, 11307, 1828, 12072, 5, 10020, 11413, 10138, 11295, 8959,
> 8025)
>
> In [ ]:
>
> val trained =3D trainDataSet2.map{f =3D>
>
>                                 val array =3D f._2.features.toArray
>
>                                 new LabeledPoint(f._2.label,Vectors.dense=
(
>
> correlated2.map(i =3D> array(i))))}.cache
>
> Out[9]:
>
> MappedRDD[175] at map at <console>:52
>
> In [ ]:
>
> val m =3D Predictor.binaryLogisticRegressionModelSGD(trained,100)
>
> In [23]:
>
> val validated =3D validationDataSet2.map{f =3D>
>
>                                 val array =3D f._2.features.toArray
>
>                                 (f._1,new
> LabeledPoint(f._2.label,Vectors.dense(
>
> correlated2.map(i =3D> array(i)))))}.cache
>
> Out[23]:
>
> MappedRDD[682] at map at <console>:71
>
> In [24]:
>
> val prediction =3D validated.map {
>
>       case (ids, point) =3D>
>
>         (ids,
> m.model.asInstanceOf[LogisticRegressionModel].predict(point.features))
>
>     }
>
> Out[24]:
>
> MappedRDD[683] at map at <console>:79
>
> In [20]:
>
> validated.take(2)
>
> Out[20]:
>
>
> Array((0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0]),
>
> (0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,=
0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0=
,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.=
0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0=
.0,0.0,0.0,0.0,0.0]))
>
> In [26]:
>
> val logloss =3D eleflow.uberdata.data.stat.Statistics.logLoss(prediction)
>
> Out[26]:
>
> 5.861273254972684
>
> In [17]:
>
> validationDataSet2.take(3)
>
> Out[17]:
>
>
> Array((0,(0.0,(12073,[0,1,4,9,18,42,4563,8382,8386,8575,11279,11289,11322=
,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,=
1.0,1.0,1.0,1.0,1.0]))),
>
> (0,(0.0,(12073,[0,1,4,9,18,42,3260,8382,8386,8577,11279,11289,11322,11766=
,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.=
0,1.0,1.0,1.0]))),
>
> (0,(0.0,(12073,[0,1,4,10,40,42,4729,8382,8386,8672,11279,11289,11357,1176=
8,11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1=
.0,1.0,1.0,1.0]))))
>
> In [ ]:
>
> trained.take(4)
>
> In [7]:
>
>
>
> import org.apache.spark.mllib.classification._
>
> In [8]:
>
> val steps =3D Seq(Step(10,2),new Step(7,3), new Step(6,4))
>
> Out[8]:
> 0Step(10,2)1Step(7,3)2Step(6,4)
> In [ ]:
>
> val predictor  =3D
> eleflow.uberdata.data.Predictor.evolutivePredict(train.repartition(240),
> test, algorithm =3D BinaryLogisticRegressionBFGS,
>
>                              validationMethod =3D LogarithmicLoss, steps
> =3D steps, iterations =3D 30)
>
> In [ ]:
>
> uc.terminate
>
> In [11]:
>
> train.partitions.size
>
> Out[11]:
>
> 94
>
> In [20]:
>
> val rep60 =3D train.repartition(120)
>
> Out[20]:
> idclickhourC1banner_possite_idsite_domainsite_categoryapp_idapp_domain
> app_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14C=
15
>
> C16C17C18C19C20C211769841751868484765301410221210051e8f79e60c4342784f0287=
72b
> ecad23867801e8d907d7df22a99f214ab526ff2ce9b8d8d71020634320502374339-123
> 17703074559452740131141022121005085f751fdc4e18dd650e219e0399477562347f47a
>
> cef3e64932d58615ab5a307674de3ee61221768320502506035-115717708054784542889=
711
> 0141022121005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8a99f214a
> d30ecac3542422a7102161132050248032971001116117713001998424865357114102212
> 1005085f751fdc4e18dd650e219e0bc44c87d7801e8d90f2161f8ad97ca8caa305f51
> 43836a961020633320502374339-123177175933008005586270141022121005085f751fd
>
> c4e18dd650e219e0f888bf4c5b9c592b0f2161f89a5442e768bc961a1f0bc64f102115332=
050
> 2420235-169177224932175731189110141022121005085f751fdc4e18dd650e219e0
> e96773f02347f47a0f2161f8a99f214abf741817ef726eae1021767320502506035-1157
> 17727816327614515164014102212100505bcf81a29d54950bf028772becad23867801e8d=
9
> 07d7df22a99f214a5e4ee78bbe87996b1221770320502507035100176157
> In [ ]:
>
> val cach60 =3D rep60.cache
>
> In [28]:
>
> cach60.map(f =3D> f(1).asInstanceOf[Int]).sum
>
> org.apache.spark.SparkException: Job aborted due to stage failure:
> Task 53 in stage 53.0 failed 4 times, most recent failure: Lost task
> 53.3 in stage 53.0 (TID 1322,
> ip-172-31-0-62.us-west-2.compute.internal):
> java.lang.ClassCastException: java.lang.String cannot be cast to
> java.lang.Integer
>         at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
>         at
> $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iw=
C$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun=
$1.apply(<console>:59)
>         at
> $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iw=
C$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun=
$1.apply(<console>:59)
>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>         at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>         at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>         at
> scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:1=
72)
>         at
> scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1157)
>         at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:853)
>         at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:851)
>         at
> org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
>         at
> org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>         at org.apache.spark.scheduler.Task.run(Task.scala:56)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java=
:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav=
a:615)
>         at java.lang.Thread.run(Thread.java:745)
>
> Driver stacktrace:
>     org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSche=
duler.scala:1214)
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGSc=
heduler.scala:1203)
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGSc=
heduler.scala:1202)
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scal=
a:59)
>     scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:120=
2)
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.ap=
ply(DAGScheduler.scala:696)
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.ap=
ply(DAGScheduler.scala:696)
>     scala.Option.foreach(Option.scala:236)
>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.=
scala:696)
>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive=
$2.applyOrElse(DAGScheduler.scala:1420)
>     akka.actor.Actor$class.aroundReceive(Actor.scala:465)
>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DA=
GScheduler.scala:1375)
>     akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
>     akka.actor.ActorCell.invoke(ActorCell.scala:487)
>     akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
>     akka.dispatch.Mailbox.run(Mailbox.scala:220)
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abstract=
Dispatcher.scala:393)
>     scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.jav=
a:1339)
>
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.j=
ava:107)
>
>
>
> R
>

--089e013d06e8889dfb050d2ddae3--

From dev-return-11245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 19:13:31 2015
Return-Path: <dev-return-11245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59874CFE7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 19:13:31 +0000 (UTC)
Received: (qmail 27340 invoked by uid 500); 21 Jan 2015 19:07:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27271 invoked by uid 500); 21 Jan 2015 19:07:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26951 invoked by uid 99); 21 Jan 2015 19:07:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 19:07:24 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.53 as permitted sender)
Received: from [74.125.82.53] (HELO mail-wg0-f53.google.com) (74.125.82.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 18:57:30 +0000
Received: by mail-wg0-f53.google.com with SMTP id a1so4294446wgh.12;
        Wed, 21 Jan 2015 10:55:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BeLxu8IfowyDmrjeoOepAKp/C5Y7ZnUOEVE3/L46HQg=;
        b=F9Hklfq81FId/Kix/uNPaGCOQ45/Xfgwq4uoz/8LLsdH4jntuXSD/VEnT41+SezWrg
         GOMdKaA5kjkDQC1e45CS2Erz/+G2CuXfFKpnK2HTDN1ioseb1p0HVsqfeah7vCUbo0AL
         bV7QRaijKJIRnunfTuq6XRnOjdMcHZdwWnWkmql+8TVymdUtfQtuhyR2/HK0kNUHoLIV
         kiMfeTsEc5sK/3yTDhF6kbnyMtrF8HamZ1B3oy+JYJZojxRpjrerm/wSRvHjjR+hwInY
         tBUnbFgY6ededSPCorDQlJoZ/uW1huKkGOzfDCJy6quwZZ6OOu5S+jd8OA5kld24cfv+
         aRPA==
MIME-Version: 1.0
X-Received: by 10.195.13.77 with SMTP id ew13mr83056489wjd.106.1421866539261;
 Wed, 21 Jan 2015 10:55:39 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Wed, 21 Jan 2015 10:55:39 -0800 (PST)
In-Reply-To: <CADkoF-rJ6BVmHg0aqm0qcejrNQsJggyh=jVcA2iFDfkO+Bt5-Q@mail.gmail.com>
References: <CADkoF-rJ6BVmHg0aqm0qcejrNQsJggyh=jVcA2iFDfkO+Bt5-Q@mail.gmail.com>
Date: Wed, 21 Jan 2015 10:55:39 -0800
Message-ID: <CAJgQjQ8mj8S9WRk1JA9QzO=rh74NFO-Fw=LEh7DteSi6-4EtXA@mail.gmail.com>
Subject: Re: KNN for large data set
From: Xiangrui Meng <mengxr@gmail.com>
To: "DEVAN M.S." <msdevanms@gmail.com>
Cc: User <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

For large datasets, you need hashing in order to compute k-nearest
neighbors locally. You can start with LSH + k-nearest in Google
scholar: http://scholar.google.com/scholar?q=lsh+k+nearest -Xiangrui

On Tue, Jan 20, 2015 at 9:55 PM, DEVAN M.S. <msdevanms@gmail.com> wrote:
> Hi all,
>
> Please help me to find out best way for K-nearest neighbor using spark for
> large data sets.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 21 20:01:17 2015
Return-Path: <dev-return-11246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0EBD0103F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 20:01:17 +0000 (UTC)
Received: (qmail 28622 invoked by uid 500); 21 Jan 2015 20:01:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28547 invoked by uid 500); 21 Jan 2015 20:01:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28523 invoked by uid 99); 21 Jan 2015 20:01:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 20:01:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 20:00:50 +0000
Received: by mail-oi0-f46.google.com with SMTP id a141so13617667oig.5
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 12:00:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=b39GqGz4GIfHxD4qSO+E/lk6rkUDE5vgvMEvXhWqZ0I=;
        b=SJW4+gvT8cqfsSg7E3+kHuCyDppsE8C+jpM5cKt9qZMqXboCVFuh24bwabg9J9cpLw
         zxtbn6bNqle4GHkZJVa/ozyJ5/F6vNTVsZwFkVnEguycPgogzL5zPUzgTuo025leUgwM
         C0jm47aWCODa5Q7orOmNwdbdqk95PXck9BmEqn+LB0exm2eNweXsvAtIPGWXSR5DUHb7
         AK8eiqLuIiU5HX3Y7s+Z+RVB9m8b7iQTrIVyGmt7ySyT9yAD9rrGdOyOSEmgF53ZJp0a
         x1I8Cj7fdJAXVr7KmrhlJdtoz+jIeABrzY9zgCAuzIONJJp9FAoiPJgepuqpd4fB+hSj
         gIAQ==
MIME-Version: 1.0
X-Received: by 10.60.176.34 with SMTP id cf2mr1087641oec.52.1421870449185;
 Wed, 21 Jan 2015 12:00:49 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 21 Jan 2015 12:00:49 -0800 (PST)
In-Reply-To: <812445935.13688800.1421847760911.JavaMail.zimbra@redhat.com>
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
	<243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
	<CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
	<CACVCA=eMmAhkGq02OKx9sD6FAMXO3i9Ko2z7xiKmfsW4Xq2PUg@mail.gmail.com>
	<AM3PR05MB08227F4BA4361B02175F0976F9480@AM3PR05MB0822.eurprd05.prod.outlook.com>
	<CABPQxssFPRSEHZxXGh+HcjL09JmdYECBV4qUsXsK_oj-x0+a6g@mail.gmail.com>
	<CAMAsSdKWN4c6PdTuChZ1nsTn5TkC2euJ9s4Uu3=ZvBkComJq-Q@mail.gmail.com>
	<CABPQxsvmWKF8jzbpUrFiXRxRV6DQeVOnS+7Wes8+iDfVG5uezA@mail.gmail.com>
	<812445935.13688800.1421847760911.JavaMail.zimbra@redhat.com>
Date: Wed, 21 Jan 2015 12:00:49 -0800
Message-ID: <CABPQxstgKJ0wDA-oYp=ykKKvLNbt=D4tQS+GBhQkHc+92Daz5g@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
From: Patrick Wendell <pwendell@gmail.com>
To: Will Benton <willb@redhat.com>
Cc: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>, 
	jay vyas <jayunit100.apache@gmail.com>, Paolo Platter <paolo.platter@agilelab.it>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yep,

I think it's only useful (and likely to be maintained) if we actually
use this on Jenkins. So that was my proposal. Basically give people a
docker file so they can understand exactly what versions of everything
we use for our reference build. And if they don't want to use docker
directly, this will at least serve as an up-to-date list of
packages/versions they should try to install locally in whatever
environment they have.

- Patrick

On Wed, Jan 21, 2015 at 5:42 AM, Will Benton <willb@redhat.com> wrote:
> ----- Original Message -----
>> From: "Patrick Wendell" <pwendell@gmail.com>
>> To: "Sean Owen" <sowen@cloudera.com>
>> Cc: "dev" <dev@spark.apache.org>, "jay vyas" <jayunit100.apache@gmail.co=
m>, "Paolo Platter"
>> <paolo.platter@agilelab.it>, "Nicholas Chammas" <nicholas.chammas@gmail.=
com>, "Will Benton" <willb@redhat.com>
>> Sent: Wednesday, January 21, 2015 2:09:35 AM
>> Subject: Re: Standardized Spark dev environment
>
>> But the issue is when users can't reproduce Jenkins failures.
>
> Yeah, to answer Sean's question, this was part of the problem I was tryin=
g to solve.  The other part was teasing out differences between the Fedora =
Java environment and a more conventional Java environment.  I agree with Se=
an (and I think this is your suggestion as well, Patrick) that making the e=
nvironment Jenkins runs a standard image that is available for public consu=
mption would be useful in general.
>
>
>
> best,
> wb

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 00:03:08 2015
Return-Path: <dev-return-11247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DDF917542
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 00:03:08 +0000 (UTC)
Received: (qmail 58701 invoked by uid 500); 22 Jan 2015 00:03:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58609 invoked by uid 500); 22 Jan 2015 00:03:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56507 invoked by uid 99); 22 Jan 2015 00:03:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 00:03:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 00:02:59 +0000
Received: by mail-oi0-f50.google.com with SMTP id h136so6660494oig.9
        for <dev@spark.apache.org>; Wed, 21 Jan 2015 16:02:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MF9S71xgJNJWMkwJbkkI3tDaMKHhKLWVGVeEI8+G6V8=;
        b=ZlEYQcL2blgOGnmfAL/CkfUV2S96GaNKFjsCY/NM6yL+fV2YByHWuO12nXG2d2BDOR
         djeEeLhSdCATNDohsjDsWn7+FHDZnez2HwowWjDQuWhKCLJ60BGBghq4y7DABjtq69HL
         O+AIvwe0uRMtosU57ZaAqa2s95fiuKtW6Wc32E13bTrfkWgnhE9A5oJBRFXvyOkqKMy8
         c9UejM7s0boZnjiHXtlTDHul0QI04K2KfViEFpKPLpyBU7s2i7Jl2Ijsu0USAzfvYEcn
         j9Dq0fllvageiHV6OCxNYllkc0cZlpulwsYYEolisb8eP4x4DrpBNbfZmtzxvoFvFyXP
         jLkA==
MIME-Version: 1.0
X-Received: by 10.182.165.9 with SMTP id yu9mr26241425obb.34.1421884959372;
 Wed, 21 Jan 2015 16:02:39 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 21 Jan 2015 16:02:39 -0800 (PST)
Date: Wed, 21 Jan 2015 16:02:39 -0800
Message-ID: <CABPQxss=-UvrEZ7mj31V=exy_aTiEjBgt2s81Evj8=ELY09RNQ@mail.gmail.com>
Subject: Upcoming Spark 1.2.1 RC
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

I am planning to cut a 1.2.1 RC soon and wanted to notify people.

There are a handful of important fixes in the 1.2.1 branch
(http://s.apache.org/Mpn) particularly for Spark SQL. There was also
an issue publishing some of our artifacts with 1.2.0 and this release
would fix it for downstream projects.

You can track outstanding 1.2.1 blocker issues here at
http://s.apache.org/2v2 - I'm guessing all remaining blocker issues
will be fixed today.

I think we have a good handle on the remaining outstanding fixes, but
please let me know if you think there are severe outstanding fixes
that need to be backported into this branch or are not tracked above.

Thanks!
- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 05:10:15 2015
Return-Path: <dev-return-11248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DAEA817E8F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 05:10:15 +0000 (UTC)
Received: (qmail 22267 invoked by uid 500); 22 Jan 2015 05:10:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22185 invoked by uid 500); 22 Jan 2015 05:10:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22173 invoked by uid 99); 22 Jan 2015 05:10:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 05:10:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 05:10:08 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Thu, 22 Jan
 2015 00:09:47 -0500
Message-ID: <54C08618.7090103@flytxt.com>
Date: Thu, 22 Jan 2015 10:39:44 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Test suites in  the python wrapper of kmeans failing
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

The test suites in the Kmeans class in clustering.py is not updated to 
take the seed value and hence it is failing.
Shall I make the changes and submit it along with my PR( Python API for 
Gaussian Mixture Model) or create a JIRA ?

Regards,
Meethu

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 06:51:54 2015
Return-Path: <dev-return-11249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD17110046
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 06:51:54 +0000 (UTC)
Received: (qmail 52767 invoked by uid 500); 22 Jan 2015 06:51:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52697 invoked by uid 500); 22 Jan 2015 06:51:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52685 invoked by uid 99); 22 Jan 2015 06:51:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 06:51:53 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_MESSAGE,HTTP_ESCAPED_HOST,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [66.46.182.52] (HELO relay.ihostexchange.net) (66.46.182.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 06:51:27 +0000
Received: from [192.168.125.249] (125.17.228.30) by smtp.ihostexchange.net
 (66.46.182.50) with Microsoft SMTP Server (TLS) id 8.3.377.0; Thu, 22 Jan
 2015 01:51:24 -0500
Message-ID: <54C09DE9.1030502@flytxt.com>
Date: Thu, 22 Jan 2015 12:21:21 +0530
From: Meethu Mathew <meethu.mathew@flytxt.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Test suites in  the python wrapper of kmeans failing
References: <54C08618.7090103@flytxt.com>
In-Reply-To: <54C08618.7090103@flytxt.com>
Content-Type: multipart/alternative;
	boundary="------------060703070709070102010803"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060703070709070102010803
Content-Type: text/plain; charset="utf-8"; format=flowed
Content-Transfer-Encoding: 7bit

Hi,

Sorry it was my mistake. My code was not properly built.

Regards,
Meethu


_<http://www.linkedin.com/home?trk=hb_tab_home_top>_

On Thursday 22 January 2015 10:39 AM, Meethu Mathew wrote:
> Hi,
>
> The test suites in the Kmeans class in clustering.py is not updated to
> take the seed value and hence it is failing.
> Shall I make the changes and submit it along with my PR( Python API for
> Gaussian Mixture Model) or create a JIRA ?
>
> Regards,
> Meethu
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>


--------------060703070709070102010803--

From dev-return-11250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 09:00:05 2015
Return-Path: <dev-return-11250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 632B3104E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 09:00:05 +0000 (UTC)
Received: (qmail 96044 invoked by uid 500); 22 Jan 2015 09:00:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95970 invoked by uid 500); 22 Jan 2015 09:00:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95838 invoked by uid 99); 22 Jan 2015 09:00:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 09:00:00 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of michael.belldavies@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 08:59:34 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 801F6115717B
	for <dev@spark.apache.org>; Thu, 22 Jan 2015 00:59:04 -0800 (PST)
Date: Thu, 22 Jan 2015 01:59:02 -0700 (MST)
From: Mick Davies <michael.belldavies@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421917142854-10243.post@n3.nabble.com>
Subject: Are there any plans to run Spark on top of Succinct
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


http://succinct.cs.berkeley.edu/wp/wordpress/

Looks like a really interesting piece of work that could dovetail well with
Spark.

I have been trying recently to optimize some queries I have running on Spark
on top of Parquet but the support from Parquet for predicate push down
especially for dictionary based columns is a bit limiting. I am not sure,
but from a cursory view it looks like this format may help in this area.

Mick




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Are-there-any-plans-to-run-Spark-on-top-of-Succinct-tp10243.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 13:31:16 2015
Return-Path: <dev-return-11251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4820510F77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 13:31:16 +0000 (UTC)
Received: (qmail 23485 invoked by uid 500); 22 Jan 2015 13:31:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23373 invoked by uid 500); 22 Jan 2015 13:31:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22280 invoked by uid 99); 22 Jan 2015 13:31:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 13:31:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of msdevanms@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 13:31:07 +0000
Received: by mail-qc0-f169.google.com with SMTP id b13so1144611qcw.0;
        Thu, 22 Jan 2015 05:29:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=jD0d1/g3llGrOWvAU3z9OUVB0UzLpCkyculF4YJsKuQ=;
        b=safDxvrxQoSymujKSHHURiAYnPFoVv4UExDkOtW2UO4qG2Qo+0Iq4qTkWTw4s2ZtKz
         4C04FykOzVylHpzep6Hwskc/5KWbvOwKOVF3mMqEVvBhg1Dyu+lFx9PxTzSoQnUy/F8i
         p58/TUhbtY4Z2BVlX0E2aQI06ZAOt+nmqpuGD3zLvwUa6hro1Oni1qX5+RXYqxIwhye/
         I7/wlbANrBPgUrOygEkSrFgfOhTMbiwcZJ82FBNFwhVI/5QvGc1UJwqNe27XZYa5f9Pi
         GsT8LBza6pWsvUhpgGXvipFSk/JP7+8Ozz/zs8e5k3e2Q3L2/zi+M3B6g+PjgqeqndCF
         5Ttg==
MIME-Version: 1.0
X-Received: by 10.224.60.66 with SMTP id o2mr2427181qah.25.1421933356915; Thu,
 22 Jan 2015 05:29:16 -0800 (PST)
Received: by 10.229.172.4 with HTTP; Thu, 22 Jan 2015 05:29:16 -0800 (PST)
In-Reply-To: <CAJgQjQ8mj8S9WRk1JA9QzO=rh74NFO-Fw=LEh7DteSi6-4EtXA@mail.gmail.com>
References: <CADkoF-rJ6BVmHg0aqm0qcejrNQsJggyh=jVcA2iFDfkO+Bt5-Q@mail.gmail.com>
	<CAJgQjQ8mj8S9WRk1JA9QzO=rh74NFO-Fw=LEh7DteSi6-4EtXA@mail.gmail.com>
Date: Thu, 22 Jan 2015 18:59:16 +0530
Message-ID: <CADkoF-qY3ByaEgFU43_mhE9mpSn7YD0tE8UfCjN3qtiVERYuxg@mail.gmail.com>
Subject: Re: KNN for large data set
From: "DEVAN M.S." <msdevanms@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: User <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3ea84a23ea1050d3da893
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3ea84a23ea1050d3da893
Content-Type: text/plain; charset=UTF-8

Thanks Xiangrui Meng will try this.

And, found this https://github.com/kaushikranjan/knnJoin also.
Will this work with double data ? Can we find out z value of
*Vector(10.3,4.5,3,5)* ?






On Thu, Jan 22, 2015 at 12:25 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> For large datasets, you need hashing in order to compute k-nearest
> neighbors locally. You can start with LSH + k-nearest in Google
> scholar: http://scholar.google.com/scholar?q=lsh+k+nearest -Xiangrui
>
> On Tue, Jan 20, 2015 at 9:55 PM, DEVAN M.S. <msdevanms@gmail.com> wrote:
> > Hi all,
> >
> > Please help me to find out best way for K-nearest neighbor using spark
> for
> > large data sets.
> >
>

--001a11c3ea84a23ea1050d3da893--

From dev-return-11252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 15:06:18 2015
Return-Path: <dev-return-11252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3EFC1753D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 15:06:18 +0000 (UTC)
Received: (qmail 88646 invoked by uid 500); 22 Jan 2015 15:06:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88574 invoked by uid 500); 22 Jan 2015 15:06:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88560 invoked by uid 99); 22 Jan 2015 15:06:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 15:06:17 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of deanwampler@gmail.com designates 209.85.160.170 as permitted sender)
Received: from [209.85.160.170] (HELO mail-yk0-f170.google.com) (209.85.160.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 15:05:52 +0000
Received: by mail-yk0-f170.google.com with SMTP id q9so876625ykb.1
        for <dev@spark.apache.org>; Thu, 22 Jan 2015 07:05:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=T6LDYEBf4v7Izt1jxJnbL5qIiILlrpY5GqC2fg3cVgU=;
        b=lg7WF80QnY4f3n7UBcBxxYAM0z9/HSrutTdmAn/5AJ/oOS8iKCYce1rY+KDXmNiiue
         fF0nGzTyv9yPaLGq/FAHtET6e1LLs6XHzmWBH/HivFbPON5BwlpPLIkXmGLGaLc2UB8V
         UbPHU9CeTUmJ6824B5i6xR7treeUMkIzQS1jWl+AcHeyzFZzzLstC+LbrCdCk0wQk6Q8
         UIvgv/uxHWbNtAQDTHb0sA/1pOrM4g2jCB9zjY49gDaIy3A0An2Gl7mEga14KU0wnOUS
         CidSoEOoh3z72tCdaD92YbWDUTwXaHv2B0gWTQ9aD+EJwkuOWUERwnJVhOhrhrzWM3FN
         pRTA==
X-Received: by 10.170.173.12 with SMTP id p12mr1223124ykd.41.1421939105589;
 Thu, 22 Jan 2015 07:05:05 -0800 (PST)
MIME-Version: 1.0
Received: by 10.170.45.68 with HTTP; Thu, 22 Jan 2015 07:04:44 -0800 (PST)
In-Reply-To: <1421917142854-10243.post@n3.nabble.com>
References: <1421917142854-10243.post@n3.nabble.com>
From: Dean Wampler <deanwampler@gmail.com>
Date: Thu, 22 Jan 2015 09:04:44 -0600
Message-ID: <CAKW0i0zMGemVSX++PboC9Msr=mAfbS9SCr8z+f_JVq+vg6xh=Q@mail.gmail.com>
Subject: Re: Are there any plans to run Spark on top of Succinct
To: Mick Davies <michael.belldavies@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a6f4047f1d7050d3eff6b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a6f4047f1d7050d3eff6b
Content-Type: text/plain; charset=UTF-8

Interesting. I was wondering recently if anyone has explored working with
compressed data directly.

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com

On Thu, Jan 22, 2015 at 2:59 AM, Mick Davies <michael.belldavies@gmail.com>
wrote:

>
> http://succinct.cs.berkeley.edu/wp/wordpress/
>
> Looks like a really interesting piece of work that could dovetail well with
> Spark.
>
> I have been trying recently to optimize some queries I have running on
> Spark
> on top of Parquet but the support from Parquet for predicate push down
> especially for dictionary based columns is a bit limiting. I am not sure,
> but from a cursory view it looks like this format may help in this area.
>
> Mick
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Are-there-any-plans-to-run-Spark-on-top-of-Succinct-tp10243.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113a6f4047f1d7050d3eff6b--

From dev-return-11253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 22 23:20:18 2015
Return-Path: <dev-return-11253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E2A8174FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 Jan 2015 23:20:18 +0000 (UTC)
Received: (qmail 13952 invoked by uid 500); 22 Jan 2015 23:20:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13873 invoked by uid 500); 22 Jan 2015 23:20:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13860 invoked by uid 99); 22 Jan 2015 23:20:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 23:20:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of halcyonic@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 Jan 2015 23:20:10 +0000
Received: by mail-pa0-f43.google.com with SMTP id eu11so4820918pac.2
        for <dev@spark.apache.org>; Thu, 22 Jan 2015 15:19:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:content-transfer-encoding:subject:message-id:date
         :to:mime-version;
        bh=6uNiKV3GJV+eVFkzfzpTTXA0Nz+S9erTayHIddf+e3w=;
        b=uThrFil5kuvfd89zea0119Q/tbw80Z4jeT7H2BgAsnyJdHhxUmi5InUwrNG/2WSBzl
         v4sbl/t5i/iXA+X3NWP1n1YK2SOUxw5qKnPJyvgCBIcUobyHJ0a1hGMiLTEp96X20+bb
         ouTUKoviEeKYJ6bKWJkTucYXMLf1NJMKi+yufHJstlunBl8ABr+2OK6ma1sVfhLkJfPF
         YSPuReIE6KausLdjfMXRdAtCBgfc/0en3Rz3CEXy/k+0F09MwrL3fYM8Fx1GMupkrQhv
         swH+N8IBGxTbIFDnt6r5mfInJFvNxdFge5/0VMSzmJjtpRCIpTcdYeXFuxuyM3emV+a7
         E7nA==
X-Received: by 10.70.95.166 with SMTP id dl6mr6267461pdb.140.1421968789913;
        Thu, 22 Jan 2015 15:19:49 -0800 (PST)
Received: from [10.151.96.86] ([148.87.13.10])
        by mx.google.com with ESMTPSA id yq5sm10361920pac.15.2015.01.22.15.19.47
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 22 Jan 2015 15:19:48 -0800 (PST)
From: Nicholas Murphy <halcyonic@gmail.com>
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable
Subject: query planner design doc?
Message-Id: <C90DE9FD-325F-483B-B596-4923E1B7C210@gmail.com>
Date: Thu, 22 Jan 2015 15:19:43 -0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi-

Quick question: is there a design doc (or something more than =E2=80=9Cloo=
k at the code=E2=80=9D) for the query planner for Spark SQL (i.e., the =
component that takes=E2=80=A6Catalyst?=E2=80=A6operator trees and =
translates them into SPARK operations)?

Thanks,
Nick=

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 03:47:19 2015
Return-Path: <dev-return-11254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D18617C22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 03:47:19 +0000 (UTC)
Received: (qmail 85660 invoked by uid 500); 23 Jan 2015 03:47:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85584 invoked by uid 500); 23 Jan 2015 03:47:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85570 invoked by uid 99); 23 Jan 2015 03:47:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 03:47:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 03:47:12 +0000
Received: by mail-la0-f49.google.com with SMTP id gf13so1037507lab.8
        for <dev@spark.apache.org>; Thu, 22 Jan 2015 19:45:45 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=eaUDXpIke1bl1WefJu3/2dWDX+a9ettghhNcG4q7FbQ=;
        b=Qy2tpwKpS5WVKzl8p9y4OT2OwZ0MYF5UFEpkNpl4k8g+alSOXGImpSi5hAbSVl59TT
         Z0VfZV8tE6gCYcxxKvvpjdAhaUzR1NsLQkSPPMK3OxzzidYj2j+PgUVujh3+9qxfL65S
         oYV58p9C3YJO28E0G+hHsL2QuvDLmvj2mKSfISJL3yf96xAkFtni8aoKNf4+V59MMvGA
         +JUvSfwTdMLY+Oo9ks929vG6t14NDlokwEBOT8hQ37YV1OtR57maX9bRep30vZCYGWeI
         JhHLEsQYbluT0DdUSXzwe8uJC9s1g0Li1G7L3WwE5mVoJCI4vmPcMKCsdyQJu8ed6Kf/
         jqhg==
X-Gm-Message-State: ALoCoQnc7w0wQQIFx7rIqAh9Wd0X28MGQ/ARAubqBUgIaOlnMDdBfPQ4T8VjCrlFU/YuLg9vrZQw
X-Received: by 10.112.44.230 with SMTP id h6mr5039992lbm.98.1421984745039;
 Thu, 22 Jan 2015 19:45:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Thu, 22 Jan 2015 19:45:24 -0800 (PST)
In-Reply-To: <C90DE9FD-325F-483B-B596-4923E1B7C210@gmail.com>
References: <C90DE9FD-325F-483B-B596-4923E1B7C210@gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 22 Jan 2015 19:45:24 -0800
Message-ID: <CAAswR-6ULDMysX9uCVpnbuGNWdejLzpWjsfPcRCW3E9QNXJ0VQ@mail.gmail.com>
Subject: Re: query planner design doc?
To: Nicholas Murphy <halcyonic@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113477c09ad20c050d499f82
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113477c09ad20c050d499f82
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here is the initial design document for catalyst :
https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU=
5M1Y/edit

Strategies (many of which are in SparkStragegies.scala) are the part that
creates the physical operators from a catalyst logical plan.  These
operators have execute() methods that actually call RDD operations.

On Thu, Jan 22, 2015 at 3:19 PM, Nicholas Murphy <halcyonic@gmail.com>
wrote:

> Hi-
>
> Quick question: is there a design doc (or something more than =E2=80=9Clo=
ok at the
> code=E2=80=9D) for the query planner for Spark SQL (i.e., the component t=
hat
> takes=E2=80=A6Catalyst?=E2=80=A6operator trees and translates them into S=
PARK operations)?
>
> Thanks,
> Nick
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113477c09ad20c050d499f82--

From dev-return-11255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 05:25:49 2015
Return-Path: <dev-return-11255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0899917F77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 05:25:49 +0000 (UTC)
Received: (qmail 25877 invoked by uid 500); 23 Jan 2015 05:25:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25795 invoked by uid 500); 23 Jan 2015 05:25:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 48375 invoked by uid 99); 23 Jan 2015 04:43:47 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of williamsmith.mail@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Thu, 22 Jan 2015 21:42:48 -0700 (MST)
From: William-Smith <williamsmith.mail@gmail.com>
To: dev@spark.apache.org
Message-ID: <1421988168826-10250.post@n3.nabble.com>
In-Reply-To: <1411191524.83515.YahooMailNeo@web125504.mail.ne1.yahoo.com>
References: <1411191524.83515.YahooMailNeo@web125504.mail.ne1.yahoo.com>
Subject: Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have had the same issue while using HttpClient from AWS EMR Spark Streaming
to post to a nodejs server.

I have found ... using
Classloder.getResource('org/apache/http/client/HttpClient") .... that the
class 
Is being loaded front the spark-assembly-1.1.0-hadoop2.4.0.jar.

That in itself is not the issue because the version is 4.2.5 .... the same
version I am using on my local machine with success .... using Hadoop cdh 5.



The issue is that HttpClient relies on Httpcore .... and there is an old
commons-httpcore-1.3.jar as well as httpcore-4.5.2 in the spark-assembly
jar.

It looks like the old one is getting loaded first.

So the fix might be to build the Spark jar myself without the httpcore-1.3
and replace it on bootstrap.
I will keep you posted on the outcome.





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-1-0-w-hadoop-2-4-vs-aws-java-sdk-1-7-2-tp8481p10250.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 06:23:40 2015
Return-Path: <dev-return-11256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E632D10180
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 06:23:39 +0000 (UTC)
Received: (qmail 5768 invoked by uid 500); 23 Jan 2015 06:23:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5696 invoked by uid 500); 23 Jan 2015 06:23:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5682 invoked by uid 99); 23 Jan 2015 06:23:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 06:23:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 06:23:12 +0000
Received: by mail-wi0-f177.google.com with SMTP id r20so545867wiv.4
        for <dev@spark.apache.org>; Thu, 22 Jan 2015 22:22:51 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=j4nui9NWpM4gOvW8wVf2oCF54yviYtzAAixxVpHaWTU=;
        b=UaYw02Uj33nfLbROV4+GB/XjX6GtDhApNns7MrO8cRHb5aQHi0wGq+uq5BgP4F6WkT
         ZxplEOcFs5w8o/P3U2pQ89DQNKID4pJLPe+iQLVvUQxjiPYpoh9SLA48jLHWCQ8PHUxM
         w64ex6zlGNVrKw2qqh7s8EXYTiCUsmJo9veQOdB9K1wxKifictIRFdh+verSn33eLWPN
         Jr/PJzpBU0Ozk/akDFkdq6a5z2uqk+lDoTCt8AJMpjiohvD2hORR3+b8LfsnUZc3lJPC
         C0fmNCXdMiI6I4/i8SJX2Ofz9gv52gl1vCIg4r4LCkUtLIv6OrP/hdmNVG+fyLSo08wB
         YCNQ==
X-Gm-Message-State: ALoCoQmcxlvFt1dun8DMjWN/YiewUdMGcXvby3lQIShgvdZtuZHoYpsNSzNODEFiuuflmqi/D69K
X-Received: by 10.194.62.19 with SMTP id u19mr10958768wjr.0.1421994171177;
 Thu, 22 Jan 2015 22:22:51 -0800 (PST)
MIME-Version: 1.0
Received: by 10.216.191.68 with HTTP; Thu, 22 Jan 2015 22:22:21 -0800 (PST)
From: "Saumitra Shahapure (Vizury)" <saumitra.shahapure@vizury.com>
Date: Fri, 23 Jan 2015 11:52:21 +0530
Message-ID: <CAGP031t81jyRkp4Q3Av1t1-OLNihJmk-Z+TZxuq1eUzpR=PbUA@mail.gmail.com>
Subject: Spark performance gains for small queries
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7ba97cd472513f050d4bd104
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba97cd472513f050d4bd104
Content-Type: text/plain; charset=UTF-8

Hello,

We were comparing performance of some of our production hive queries
between Hive and Spark. We compared Hive(0.13)+hadoop (1.2.1) against both
Spark 0.9 and 1.1. We could see that the performance gains have been good
in Spark.

We tried a very simple query,
select count(*) from T where col3=123
in both sparkSQL and Hive (with hive.map.aggr=true) and found that Spark
performance had been 2x better than Hive (120sec vs 60sec). Table T is
stored in S3 and contains 600MB single GZIP file.

My question is, why Spark is faster than Hive here? In both of the cases,
the file will be downloaded, uncompressed and lines will be counted by a
single process. For Hive case, reducer will be identity function
since hive.map.aggr is true.

Note that disk spills and network I/O are very less for Hive's case as well,
--
Regards,
Saumitra Shahapure

--047d7ba97cd472513f050d4bd104--

From dev-return-11257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 08:40:24 2015
Return-Path: <dev-return-11257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D2EE107C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 08:40:24 +0000 (UTC)
Received: (qmail 83560 invoked by uid 500); 23 Jan 2015 08:40:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83490 invoked by uid 500); 23 Jan 2015 08:40:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83476 invoked by uid 99); 23 Jan 2015 08:40:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 08:40:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 08:40:17 +0000
Received: by mail-pd0-f176.google.com with SMTP id y10so7279137pdj.7
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 00:38:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=RdCVbjD9LTb7wBszL7AfujRKjH4ECtcEIdEENWgcnaY=;
        b=WFieF08+KBmwmml6zcrbWF0JVQNXVneCC7d99Lmcifx3AdLKw6jcrPU4uQxJnqt1HV
         34221xeJb/8U1SMEGPpEYmH0EZsjUy6lwNF6jylYvgXW3I4p8aga6U/oC6JvtlNtYIxs
         xnqr3ViVC2rnha4iXFZ8JKX9UncIlq16VyI95sFsz8mx+vvUB3Nb533bjGdtYkSwqTiK
         X3lFu2tR5abEq95DshAiV3cRz2s2rpaLWBNhdHrF8Zh/ME/RWdg+cspyNN2AVEM2XFqA
         QQDYpt/h2rcdq/LePjpOSFwLBMEm66iEBxr8IvYTIUf/iax/rZW+Kjz9Ai3hQ3aRmf3S
         M5/g==
X-Received: by 10.66.184.206 with SMTP id ew14mr9185434pac.3.1422002306532;
        Fri, 23 Jan 2015 00:38:26 -0800 (PST)
Received: from [10.71.1.184] (75-144-26-73-sfba-ca.hfc.comcastbusiness.net. [75.144.26.73])
        by mx.google.com with ESMTPSA id zr4sm1143044pbb.18.2015.01.23.00.38.24
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 23 Jan 2015 00:38:25 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: Spark performance gains for small queries
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAGP031t81jyRkp4Q3Av1t1-OLNihJmk-Z+TZxuq1eUzpR=PbUA@mail.gmail.com>
Date: Fri, 23 Jan 2015 00:38:22 -0800
Cc: dev@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <562D1E3E-2A92-4CCB-98EE-841818EF2B3F@gmail.com>
References: <CAGP031t81jyRkp4Q3Av1t1-OLNihJmk-Z+TZxuq1eUzpR=PbUA@mail.gmail.com>
To: "Saumitra Shahapure (Vizury)" <saumitra.shahapure@vizury.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

It's hard to tell without more details, but the start-up latency in Hive =
can sometimes be high, especially if you are running Hive on MapReduce. =
MR just takes 20-30 seconds per job to spin up even if the job is doing =
nothing.

For real use of Spark SQL for short queries by the way, I'd recommend =
using the JDBC server so that you can have a long-running Spark process. =
It gets quite a bit faster after the first few queries.

Matei

> On Jan 22, 2015, at 10:22 PM, Saumitra Shahapure (Vizury) =
<saumitra.shahapure@vizury.com> wrote:
>=20
> Hello,
>=20
> We were comparing performance of some of our production hive queries
> between Hive and Spark. We compared Hive(0.13)+hadoop (1.2.1) against =
both
> Spark 0.9 and 1.1. We could see that the performance gains have been =
good
> in Spark.
>=20
> We tried a very simple query,
> select count(*) from T where col3=3D123
> in both sparkSQL and Hive (with hive.map.aggr=3Dtrue) and found that =
Spark
> performance had been 2x better than Hive (120sec vs 60sec). Table T is
> stored in S3 and contains 600MB single GZIP file.
>=20
> My question is, why Spark is faster than Hive here? In both of the =
cases,
> the file will be downloaded, uncompressed and lines will be counted by =
a
> single process. For Hive case, reducer will be identity function
> since hive.map.aggr is true.
>=20
> Note that disk spills and network I/O are very less for Hive's case as =
well,
> --
> Regards,
> Saumitra Shahapure


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 10:15:31 2015
Return-Path: <dev-return-11258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5535210C48
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 10:15:31 +0000 (UTC)
Received: (qmail 22596 invoked by uid 500); 23 Jan 2015 10:15:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22521 invoked by uid 500); 23 Jan 2015 10:15:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22510 invoked by uid 99); 23 Jan 2015 10:15:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 10:15:28 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 10:15:04 +0000
Received: by mail-wg0-f42.google.com with SMTP id x13so6607438wgg.1
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 02:14:18 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=IEMKqgFQ64sKF3sSNumzoyYEodnbZ30qRT8NA4Pep34=;
        b=BimzD3vDYxxHK9W91Dp5z2MTi9f3v0V+JbaCBzbodpxkQKNH1zV+aYnNkIuZWtdVhY
         1CGvEmrQMYuDqZrsb/VOZX9lXH0btSGq9bcUWh0v+8JL0Kv2RARXQGSSP8sXar9Dgbnk
         RhO+H/EKIREBrebvlJ78y1i+zwwsDY/5h2OXOuZH6Jy1nUue56R0cepb6DSMNUQmEoGs
         acHwTBUXqX6NLoZDX1rTzd/eRCGUJDx2BtIx81CSjoLqy9ntEJa7EFQUlWDt2Y+JtUwW
         BIIv/YW5vAZSgLnQrE7J8rdCdyP0jbz0Bz1bjMU1/rz0Zh9DrQwV7FDU7kcGDolSKV6k
         bEGQ==
X-Gm-Message-State: ALoCoQmg0LgW0hcO0aKNgIBhFP4hxfXEZCJeCub6ILGv4RHzuiFESUAd3w/NVdxTRUbfwl9CFp2U
X-Received: by 10.180.20.177 with SMTP id o17mr2200921wie.64.1422008057947;
 Fri, 23 Jan 2015 02:14:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Fri, 23 Jan 2015 02:13:57 -0800 (PST)
In-Reply-To: <1421988168826-10250.post@n3.nabble.com>
References: <1411191524.83515.YahooMailNeo@web125504.mail.ne1.yahoo.com> <1421988168826-10250.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 23 Jan 2015 10:13:57 +0000
Message-ID: <CAMAsSdLmp7jxXgLoDq15_BB7A3rs_Xj90DHh4r9Xvh-XcZBnbA@mail.gmail.com>
Subject: Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2
To: William-Smith <williamsmith.mail@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Did you use spark.files.userClassPathFirst = true? it's exactly for
this kind of problem.

On Fri, Jan 23, 2015 at 4:42 AM, William-Smith
<williamsmith.mail@gmail.com> wrote:
> I have had the same issue while using HttpClient from AWS EMR Spark Streaming
> to post to a nodejs server.
>
> I have found ... using
> Classloder.getResource('org/apache/http/client/HttpClient") .... that the
> class
> Is being loaded front the spark-assembly-1.1.0-hadoop2.4.0.jar.
>
> That in itself is not the issue because the version is 4.2.5 .... the same
> version I am using on my local machine with success .... using Hadoop cdh 5.
>
>
>
> The issue is that HttpClient relies on Httpcore .... and there is an old
> commons-httpcore-1.3.jar as well as httpcore-4.5.2 in the spark-assembly
> jar.
>
> It looks like the old one is getting loaded first.
>
> So the fix might be to build the Spark jar myself without the httpcore-1.3
> and replace it on bootstrap.
> I will keep you posted on the outcome.
>
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-1-0-w-hadoop-2-4-vs-aws-java-sdk-1-7-2-tp8481p10250.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 11:51:59 2015
Return-Path: <dev-return-11259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 15CAA10F12
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 11:51:59 +0000 (UTC)
Received: (qmail 88918 invoked by uid 500); 23 Jan 2015 11:51:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88842 invoked by uid 500); 23 Jan 2015 11:51:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88831 invoked by uid 99); 23 Jan 2015 11:51:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 11:51:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 11:51:32 +0000
Received: by mail-we0-f178.google.com with SMTP id k48so7106052wev.9
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 03:50:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CzR6TItcqG7LIDRpduqWTH97nPFCa19socJUw13elDg=;
        b=MtJ7AuK0jMFlqcdrd0O/wALT3JFsBNRa9UqOvx7YAcp3tn3p/NTwjkG5J0inpyGx1W
         4XZ+gJRy016i/e+4IOwNILu/uoYmV4TVoSZTGs6HQLccdgbewQSoXHHc8yweQaOtdbn1
         1/wiFei5ArF52xoWovnmu80ghOivgooDIuSClMea6zKWOLidb99yUmRdxfD5iKG4qVN5
         96JD0xpHPcxcNfZ89sq97zBf7NgV4d82l6ufbxnJHQdWP0FyVL5NFC0nxDjy0BTTiBWQ
         N7aaowHqsMQ6mclIcB2BsdllsO+23r6wdFoACdd/yOBOlCPI31867ZKkeKXyiU2IZQXy
         gxAg==
X-Gm-Message-State: ALoCoQkmJ50nGDvIi8C37rPDhZE1r9bVFTm002mhgJuR+OB4bNSQk3iwNcFV7id7pWpYccUzTbqj
X-Received: by 10.180.19.7 with SMTP id a7mr2939686wie.62.1422013826016; Fri,
 23 Jan 2015 03:50:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.216.191.68 with HTTP; Fri, 23 Jan 2015 03:49:55 -0800 (PST)
In-Reply-To: <562D1E3E-2A92-4CCB-98EE-841818EF2B3F@gmail.com>
References: <CAGP031t81jyRkp4Q3Av1t1-OLNihJmk-Z+TZxuq1eUzpR=PbUA@mail.gmail.com>
 <562D1E3E-2A92-4CCB-98EE-841818EF2B3F@gmail.com>
From: "Saumitra Shahapure (Vizury)" <saumitra.shahapure@vizury.com>
Date: Fri, 23 Jan 2015 17:19:55 +0530
Message-ID: <CAGP031uAYzLQtA4C35QSpadQQPxrgZzAoYHNQsdHVCQYGqGgPQ@mail.gmail.com>
Subject: Re: Spark performance gains for small queries
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec53d5ec7f73bdf050d5064f4
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d5ec7f73bdf050d5064f4
Content-Type: text/plain; charset=UTF-8

Hey Matei,

Thanks for your reply. We would keep in mind to use JDBC server for smaller
queries.

For the mapreduce job start-up, are you pointing towards JVM initialization
latencies in MR? Other than JVM initialization, does Spark do any
optimization (that is not done by mapreduce) to speed up the startup?

--
Regards,
Saumitra Shahapure

On Fri, Jan 23, 2015 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> It's hard to tell without more details, but the start-up latency in Hive
> can sometimes be high, especially if you are running Hive on MapReduce. MR
> just takes 20-30 seconds per job to spin up even if the job is doing
> nothing.
>
> For real use of Spark SQL for short queries by the way, I'd recommend
> using the JDBC server so that you can have a long-running Spark process. It
> gets quite a bit faster after the first few queries.
>
> Matei
>
> > On Jan 22, 2015, at 10:22 PM, Saumitra Shahapure (Vizury) <
> saumitra.shahapure@vizury.com> wrote:
> >
> > Hello,
> >
> > We were comparing performance of some of our production hive queries
> > between Hive and Spark. We compared Hive(0.13)+hadoop (1.2.1) against
> both
> > Spark 0.9 and 1.1. We could see that the performance gains have been good
> > in Spark.
> >
> > We tried a very simple query,
> > select count(*) from T where col3=123
> > in both sparkSQL and Hive (with hive.map.aggr=true) and found that Spark
> > performance had been 2x better than Hive (120sec vs 60sec). Table T is
> > stored in S3 and contains 600MB single GZIP file.
> >
> > My question is, why Spark is faster than Hive here? In both of the cases,
> > the file will be downloaded, uncompressed and lines will be counted by a
> > single process. For Hive case, reducer will be identity function
> > since hive.map.aggr is true.
> >
> > Note that disk spills and network I/O are very less for Hive's case as
> well,
> > --
> > Regards,
> > Saumitra Shahapure
>
>

--bcaec53d5ec7f73bdf050d5064f4--

From dev-return-11260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 17:45:07 2015
Return-Path: <dev-return-11260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B6CC100CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 17:45:07 +0000 (UTC)
Received: (qmail 80995 invoked by uid 500); 23 Jan 2015 17:45:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80914 invoked by uid 500); 23 Jan 2015 17:45:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80891 invoked by uid 99); 23 Jan 2015 17:45:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 17:45:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of halcyonic@gmail.com designates 209.85.220.46 as permitted sender)
Received: from [209.85.220.46] (HELO mail-pa0-f46.google.com) (209.85.220.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 17:44:38 +0000
Received: by mail-pa0-f46.google.com with SMTP id lj1so5341753pab.5
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 09:44:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=PKA7+sO3eJkmaGjKwFhycn6iHEwaUb+bErLolAmgZCU=;
        b=XcvkXz51Mey7SIxeL2/NUwPC4bOG5xsSDSCrzWjHAClxAFqCYiJLWRhC4Dm4cr+51W
         N41T5vnFuAKbFZu/KjQ5cULxyu5vXL9JgslKmrOz7SBXZJxFhVKkE5mrVLHA56xALAJj
         gdMl8Y1dU5zKrp+nZr7xnn+JXAsVeoK+Be130TyyjLKC/xDsoky5Tsutv4aDFZNinuoo
         oJ3n2Bm5PtR37y1/LZW3D4bZQMJUpNtt7RneEXPBatCKvwC6hXAfMk2DUAfRvHlwbFMF
         12/8TBCL78DCAHa4RQzhB1SOt5hDLe5nJ/fahyMrk/dSH8oCN7Cjaq6mTgMr1q3xzr9c
         p7UA==
X-Received: by 10.68.211.193 with SMTP id ne1mr13136725pbc.49.1422035076494;
        Fri, 23 Jan 2015 09:44:36 -0800 (PST)
Received: from [10.151.96.86] ([148.87.13.10])
        by mx.google.com with ESMTPSA id f3sm2524534pdb.41.2015.01.23.09.44.32
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 23 Jan 2015 09:44:35 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_B5DB62EC-97D9-4618-BCFA-4F98658F7A1C"
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: query planner design doc?
From: Nicholas Murphy <halcyonic@gmail.com>
In-Reply-To: <CAAswR-6ULDMysX9uCVpnbuGNWdejLzpWjsfPcRCW3E9QNXJ0VQ@mail.gmail.com>
Date: Fri, 23 Jan 2015 09:44:28 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <A53E8930-6EF9-4495-A738-8D893C7D7461@gmail.com>
References: <C90DE9FD-325F-483B-B596-4923E1B7C210@gmail.com> <CAAswR-6ULDMysX9uCVpnbuGNWdejLzpWjsfPcRCW3E9QNXJ0VQ@mail.gmail.com>
To: Michael Armbrust <michael@databricks.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_B5DB62EC-97D9-4618-BCFA-4F98658F7A1C
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Okay, thanks.  The design document mostly details the infrastructure for =
optimization strategies but doesn=E2=80=99t detail the strategies =
themselves.  I take it the set of strategies are basically embodied in =
SparkStrategies.scala...is there a design doc/roadmap/JIRA issue =
detailing what strategies exist and which are planned?

Thanks,
Nick

> On Jan 22, 2015, at 7:45 PM, Michael Armbrust <michael@databricks.com> =
wrote:
>=20
> Here is the initial design document for catalyst :
> =
https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPG=
U5M1Y/edit =
<https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GP=
GU5M1Y/edit>
>=20
> Strategies (many of which are in SparkStragegies.scala) are the part =
that creates the physical operators from a catalyst logical plan.  These =
operators have execute() methods that actually call RDD operations.
>=20
> On Thu, Jan 22, 2015 at 3:19 PM, Nicholas Murphy <halcyonic@gmail.com =
<mailto:halcyonic@gmail.com>> wrote:
> Hi-
>=20
> Quick question: is there a design doc (or something more than =E2=80=9Cl=
ook at the code=E2=80=9D) for the query planner for Spark SQL (i.e., the =
component that takes=E2=80=A6Catalyst?=E2=80=A6operator trees and =
translates them into SPARK operations)?
>=20
> Thanks,
> Nick
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org =
<mailto:dev-unsubscribe@spark.apache.org>
> For additional commands, e-mail: dev-help@spark.apache.org =
<mailto:dev-help@spark.apache.org>
>=20
>=20


--Apple-Mail=_B5DB62EC-97D9-4618-BCFA-4F98658F7A1C--

From dev-return-11261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 18:02:13 2015
Return-Path: <dev-return-11261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AE242101F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 18:02:13 +0000 (UTC)
Received: (qmail 38773 invoked by uid 500); 23 Jan 2015 18:02:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38701 invoked by uid 500); 23 Jan 2015 18:02:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38690 invoked by uid 99); 23 Jan 2015 18:02:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 18:02:12 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 18:01:44 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id A6614346
	for <dev@spark.apache.org>; Fri, 23 Jan 2015 18:00:41 +0000 (UTC)
Received: from G4W6300.americas.hpqcorp.net (16.210.26.225) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 23 Jan 2015 18:00:06 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.11]) by
 G4W6300.americas.hpqcorp.net ([16.210.26.225]) with mapi id 14.03.0169.001;
 Fri, 23 Jan 2015 18:00:05 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Maximum size of vector that reduce can handle
Thread-Topic: Maximum size of vector that reduce can handle
Thread-Index: AdA3NZh8mfF6QkyPTCa5+/XneVsVew==
Date: Fri, 23 Jan 2015 18:00:05 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Dear Spark developers,

I am trying to measure the Spark reduce performance for big vectors. My mot=
ivation is related to machine learning gradient. Gradient is a vector that =
is computed on each worker and then all results need to be summed up and br=
oadcasted back to workers. For example, present machine learning applicatio=
ns involve very long parameter vectors, for deep neural networks it can be =
up to 2Billions. So, I want to measure the time that is needed for this ope=
ration depending on the size of vector and number of workers. I wrote few l=
ines of code that assume that Spark will distribute partitions among all av=
ailable workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM), =
each runs 2 Workers.

import org.apache.spark.mllib.rdd.RDDFunctions._
import breeze.linalg._
import org.apache.log4j._
Logger.getRootLogger.setLevel(Level.OFF)
val n =3D 60000000
val p =3D 12
val vv =3D sc.parallelize(0 until p, p).map(i =3D> DenseVector.rand[Double]=
( n ))
vv.reduce(_ + _)

When executing in shell with 60M vector it crashes after some period of tim=
e. One of the node contains the following in stdout:
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x000000=
0755500000, 2863661056, 0) failed; error=3D'Cannot allocate memory' (errno=
=3D12)
#
# There is insufficient memory for the Java Runtime Environment to continue=
.
# Native memory allocation (malloc) failed to allocate 2863661056 bytes for=
 committing reserved memory.

I run shell with --executor-memory 8G --driver-memory 8G, so handling 60M v=
ector of Double should not be a problem. Are there any big overheads for th=
is? What is the maximum size of vector that reduce can handle?=20

Best regards, Alexander

P.S.=20

"spark.driver.maxResultSize 0" needs to set in order to run this code. I al=
so needed to change "java.io.tmpdir" and "spark.local.dir" folders because =
my /tmp folder which is default, was too small and Spark swaps heavily into=
 this folder. Without these settings I get either "no space left on device"=
 or "out of memory" exceptions.

I also submitted a bug https://issues.apache.org/jira/browse/SPARK-5386

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 19:36:21 2015
Return-Path: <dev-return-11262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B019310787
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 19:36:21 +0000 (UTC)
Received: (qmail 18417 invoked by uid 500); 23 Jan 2015 19:36:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18345 invoked by uid 500); 23 Jan 2015 19:36:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18331 invoked by uid 99); 23 Jan 2015 19:36:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 19:36:20 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michael.belldavies@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 19:36:16 +0000
Received: by mail-wi0-f181.google.com with SMTP id fb4so5058462wid.2
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 11:35:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=oLCEagA8xRC++PI1Hni46y0udPDtSbrLXmpwEopPaMY=;
        b=hlBL2H79fVPZOJuCFFZ0Z2T/nSUuV5cEzqBNxPGvLa7SntsctSTSFzUjT1/e9RyccJ
         tETPVCDZr2iHJVGzqcV7aaZhVQ8IyNmxzcwWOb91PRa3ViOxiGWe6z6RiaD5Ty3/jbwr
         lAhSAngzMxfWf8IWs+dA9K4R5cnrZn7WbrKdglaR7tbfD93hPj8eZ4BMASw1G+iKiOB0
         Hz6eS9pNjKEIiZOT2WX0VXKhZcy1K+U4pd/By9nLizg90FWILN92n0HZq9hbtPxlzwmf
         rUi2f82rVabfMRlJhkWItln76FR8lVFRUy0ymaOzoGN0OUU2NHI+LoLCSPCkXWB4lcdt
         ye7w==
X-Received: by 10.195.17.225 with SMTP id gh1mr10657728wjd.37.1422041710105;
        Fri, 23 Jan 2015 11:35:10 -0800 (PST)
Received: from michaels-mbp.lan ([87.112.166.55])
        by mx.google.com with ESMTPSA id d7sm3427597wjs.2.2015.01.23.11.35.08
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 23 Jan 2015 11:35:09 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_AA5D37F2-6664-42E7-A55E-D5FC44E90396"
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: Optimize encoding/decoding strings when using Parquet
From: Michael Davies <michael.belldavies@gmail.com>
In-Reply-To: <CAPh_B=YUwK0iDbmKRPdqhWdOm-8QZ4_P5FFwCoe+XCs0RPdo1g@mail.gmail.com>
Date: Fri, 23 Jan 2015 19:35:07 +0000
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <925E91AE-B898-4909-996E-C5061D717A75@gmail.com>
References: <1421425027439-10141.post@n3.nabble.com> <CAAswR-5XQuww+AGXxY9+RK6O3YEQwG4ifRM64xnR4cY4RkceNw@mail.gmail.com> <1421691001338-10195.post@n3.nabble.com> <CAPh_B=YUwK0iDbmKRPdqhWdOm-8QZ4_P5FFwCoe+XCs0RPdo1g@mail.gmail.com>
To: Reynold Xin <rxin@databricks.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_AA5D37F2-6664-42E7-A55E-D5FC44E90396
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Added PR https://github.com/apache/spark/pull/4139 =
<https://github.com/apache/spark/pull/4139> - I think tests have been =
re-arranged so merge necessary

Mick


> On 19 Jan 2015, at 18:31, Reynold Xin <rxin@databricks.com> wrote:
>=20
> Definitely go for a pull request!
>=20
>=20
> On Mon, Jan 19, 2015 at 10:10 AM, Mick Davies =
<michael.belldavies@gmail.com <mailto:michael.belldavies@gmail.com>> =
wrote:
>=20
> Looking at Parquet code - it looks like hooks are already in place to
> support this.
>=20
> In particular PrimitiveConverter has methods hasDictionarySupport and
> addValueFromDictionary for this purpose. These are not used by
> CatalystPrimitiveConverter.
>=20
> I think that it would be pretty straightforward to add this. Has =
anyone
> considered this? Shall I get a pull request  together for it.
>=20
> Mick
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encodin=
g-decoding-strings-when-using-Parquet-tp10141p10195.html =
<http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encodi=
ng-decoding-strings-when-using-Parquet-tp10141p10195.html>
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org =
<mailto:dev-unsubscribe@spark.apache.org>
> For additional commands, e-mail: dev-help@spark.apache.org =
<mailto:dev-help@spark.apache.org>
>=20
>=20


--Apple-Mail=_AA5D37F2-6664-42E7-A55E-D5FC44E90396--

From dev-return-11263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 19:54:06 2015
Return-Path: <dev-return-11263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 739B11092E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 19:54:06 +0000 (UTC)
Received: (qmail 91999 invoked by uid 500); 23 Jan 2015 19:54:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91919 invoked by uid 500); 23 Jan 2015 19:54:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91908 invoked by uid 99); 23 Jan 2015 19:54:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 19:54:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 19:54:01 +0000
Received: by mail-qg0-f51.google.com with SMTP id z107so7694099qgd.10
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 11:53:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type
         :content-transfer-encoding;
        bh=2DxNVLpk8HJDApqUjMkPC12dnOmn5rxdEyV2rPIVqlY=;
        b=DdwkIGys2R+qmXH0Lyj1SsWhzgcgRwKCz+EYq/v+ksYQTh7G4NZBmRCoTMp8xb4tFM
         QVoFswJUlQhWsJjYdxqeR+5tjPwtHVl5uaCBYEj0vlAum7oye1vNntNUtni41MkVptfL
         Ca1Rpx4waDtkQHmIQv8u0oRLUFKsVusHnegERddcdoAO9DpIgSv5PZFcD4wtiV7QUfKG
         7cfmA5JGUUcEILF6fg4n1ETol+bv5s40ZxPZi7BvVQGJ7AJmyBciCmNEnESxIBVfZL/+
         rdGKqrZeO0KYvFNFNSZDTs4Lr8uKoIMW2WC0P7j2p67SqsawLZVQ0My7pK+wQVbZ132g
         0FeQ==
X-Gm-Message-State: ALoCoQka6Kh13qPPTWTb9BLRkybdLMXf3UbWAFROGMzeZ2a9AggFBsQyMjwZlFX1lDMIgpmlX2vk
MIME-Version: 1.0
X-Received: by 10.224.95.71 with SMTP id c7mr17813268qan.70.1422042800528;
 Fri, 23 Jan 2015 11:53:20 -0800 (PST)
Received: by 10.229.2.136 with HTTP; Fri, 23 Jan 2015 11:53:20 -0800 (PST)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
Date: Fri, 23 Jan 2015 11:53:20 -0800
Message-ID: <CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
Subject: Re: Maximum size of vector that reduce can handle
From: DB Tsai <dbtsai@dbtsai.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Alexander,

When you use `reduce` to aggregate the vectors, those will actually be
pulled into driver, and merged over there. Obviously, it's not
scaleable given you are doing deep neural networks which have so many
coefficients.

Please try treeReduce instead which is what we do in linear regression
and logistic regression.

See https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/or=
g/apache/spark/mllib/optimization/LBFGS.scala
for example.

val (gradientSum, lossSum) =3D data.treeAggregate((Vectors.zeros(n), 0.0))(
seqOp =3D (c, v) =3D> (c, v) match { case ((grad, loss), (label, features))=
 =3D>
val l =3D localGradient.compute(
features, label, bcW.value, grad)
(grad, loss + l)
},
combOp =3D (c1, c2) =3D> (c1, c2) match { case ((grad1, loss1), (grad2, los=
s2)) =3D>
axpy(1.0, grad2, grad1)
(grad1, loss1 + loss2)
})

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



On Fri, Jan 23, 2015 at 10:00 AM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Dear Spark developers,
>
> I am trying to measure the Spark reduce performance for big vectors. My m=
otivation is related to machine learning gradient. Gradient is a vector tha=
t is computed on each worker and then all results need to be summed up and =
broadcasted back to workers. For example, present machine learning applicat=
ions involve very long parameter vectors, for deep neural networks it can b=
e up to 2Billions. So, I want to measure the time that is needed for this o=
peration depending on the size of vector and number of workers. I wrote few=
 lines of code that assume that Spark will distribute partitions among all =
available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM)=
, each runs 2 Workers.
>
> import org.apache.spark.mllib.rdd.RDDFunctions._
> import breeze.linalg._
> import org.apache.log4j._
> Logger.getRootLogger.setLevel(Level.OFF)
> val n =3D 60000000
> val p =3D 12
> val vv =3D sc.parallelize(0 until p, p).map(i =3D> DenseVector.rand[Doubl=
e]( n ))
> vv.reduce(_ + _)
>
> When executing in shell with 60M vector it crashes after some period of t=
ime. One of the node contains the following in stdout:
> Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000=
000755500000, 2863661056, 0) failed; error=3D'Cannot allocate memory' (errn=
o=3D12)
> #
> # There is insufficient memory for the Java Runtime Environment to contin=
ue.
> # Native memory allocation (malloc) failed to allocate 2863661056 bytes f=
or committing reserved memory.
>
> I run shell with --executor-memory 8G --driver-memory 8G, so handling 60M=
 vector of Double should not be a problem. Are there any big overheads for =
this? What is the maximum size of vector that reduce can handle?
>
> Best regards, Alexander
>
> P.S.
>
> "spark.driver.maxResultSize 0" needs to set in order to run this code. I =
also needed to change "java.io.tmpdir" and "spark.local.dir" folders becaus=
e my /tmp folder which is default, was too small and Spark swaps heavily in=
to this folder. Without these settings I get either "no space left on devic=
e" or "out of memory" exceptions.
>
> I also submitted a bug https://issues.apache.org/jira/browse/SPARK-5386
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 20:10:37 2015
Return-Path: <dev-return-11264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7D4710A91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 20:10:37 +0000 (UTC)
Received: (qmail 56639 invoked by uid 500); 23 Jan 2015 20:10:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56563 invoked by uid 500); 23 Jan 2015 20:10:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56523 invoked by uid 99); 23 Jan 2015 20:10:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 20:10:36 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 20:10:09 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id 8CBF3168;
	Fri, 23 Jan 2015 20:08:36 +0000 (UTC)
Received: from G9W3614.americas.hpqcorp.net (16.216.186.49) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 23 Jan 2015 20:07:30 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.11]) by
 G9W3614.americas.hpqcorp.net ([16.216.186.49]) with mapi id 14.03.0169.001;
 Fri, 23 Jan 2015 20:07:29 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: DB Tsai <dbtsai@dbtsai.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Maximum size of vector that reduce can handle
Thread-Topic: Maximum size of vector that reduce can handle
Thread-Index: AdA3NZh8mfF6QkyPTCa5+/XneVsVewAEKRAAAAAKPGA=
Date: Fri, 23 Jan 2015 20:07:28 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE5C94@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
 <CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
In-Reply-To: <CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 20:27:29 2015
Return-Path: <dev-return-11265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5451510B82
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 20:27:29 +0000 (UTC)
Received: (qmail 21448 invoked by uid 500); 23 Jan 2015 20:27:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21378 invoked by uid 500); 23 Jan 2015 20:27:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21363 invoked by uid 99); 23 Jan 2015 20:27:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 20:27:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 20:27:23 +0000
Received: by mail-lb0-f172.google.com with SMTP id l4so8998537lbv.3
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 12:25:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=v8kTPFgJn9uhp+WBu5s1eRYXUtz4Hc8dlzt92yalKNQ=;
        b=hOfKYlBDsulJzAXrx1rmI+EGZz1WLVSj49S+MImF6Ql7ocFgvp2MZ71nqxhfvChNTY
         fN2iL+hL7AMVIV72clIU1RDQsgAJtHnSnXgJxIv7/sLH2sZyBg0Ho/8pvXmWhz5mJnZG
         i1zA1Cy3P+4s60nPYxKT2s4FmN2GVInjCD3HRSKe/dqkt128peOEow3yqUhPqobX86F2
         ft0yzssMcYv0bXEe0mQapYvO1m/yhDhjY6cBkwhsrhldDaqFBbvsFgkB+sGsHtb25Ov6
         VrFelgEqfgeFaLubPBLpne3bjPdE+lDMpa/u4hzbjbqQwAVD02muIr8S+JnPQvBZG0qc
         o/fw==
X-Gm-Message-State: ALoCoQmSfxt6nNmKi04P/Fu8gGThUSRllR2nLklOxmk79WRfpT/jgNzlivAdVeTtXpFpvViZqaT7
X-Received: by 10.112.204.233 with SMTP id lb9mr9376556lbc.43.1422044757175;
 Fri, 23 Jan 2015 12:25:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Fri, 23 Jan 2015 12:25:36 -0800 (PST)
In-Reply-To: <A53E8930-6EF9-4495-A738-8D893C7D7461@gmail.com>
References: <C90DE9FD-325F-483B-B596-4923E1B7C210@gmail.com>
 <CAAswR-6ULDMysX9uCVpnbuGNWdejLzpWjsfPcRCW3E9QNXJ0VQ@mail.gmail.com> <A53E8930-6EF9-4495-A738-8D893C7D7461@gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 23 Jan 2015 12:25:36 -0800
Message-ID: <CAAswR-6BMfTWFpi_q94F3kAQgjy9mFTgTN6o5e7+mLn6X91cVw@mail.gmail.com>
Subject: Re: query planner design doc?
To: Nicholas Murphy <halcyonic@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3cb049b61a3050d5798a3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3cb049b61a3050d5798a3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

No, are you looking for something in particular?

On Fri, Jan 23, 2015 at 9:44 AM, Nicholas Murphy <halcyonic@gmail.com>
wrote:

> Okay, thanks.  The design document mostly details the infrastructure for
> optimization strategies but doesn=E2=80=99t detail the strategies themsel=
ves.  I
> take it the set of strategies are basically embodied in
> SparkStrategies.scala...is there a design doc/roadmap/JIRA issue detailin=
g
> what strategies exist and which are planned?
>
> Thanks,
> Nick
>
> On Jan 22, 2015, at 7:45 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
> Here is the initial design document for catalyst :
>
> https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GP=
GU5M1Y/edit
>
> Strategies (many of which are in SparkStragegies.scala) are the part that
> creates the physical operators from a catalyst logical plan.  These
> operators have execute() methods that actually call RDD operations.
>
> On Thu, Jan 22, 2015 at 3:19 PM, Nicholas Murphy <halcyonic@gmail.com>
> wrote:
>
>> Hi-
>>
>> Quick question: is there a design doc (or something more than =E2=80=9Cl=
ook at
>> the code=E2=80=9D) for the query planner for Spark SQL (i.e., the compon=
ent that
>> takes=E2=80=A6Catalyst?=E2=80=A6operator trees and translates them into =
SPARK operations)?
>>
>> Thanks,
>> Nick
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>
>

--001a11c3cb049b61a3050d5798a3--

From dev-return-11266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 21:53:32 2015
Return-Path: <dev-return-11266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B73710F09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 21:53:32 +0000 (UTC)
Received: (qmail 61165 invoked by uid 500); 23 Jan 2015 21:53:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61087 invoked by uid 500); 23 Jan 2015 21:53:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61075 invoked by uid 99); 23 Jan 2015 21:53:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 21:53:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of peter.prettenhofer@gmail.com designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 21:53:05 +0000
Received: by mail-la0-f48.google.com with SMTP id pv20so9599634lab.7
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 13:53:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=JjDxPitaqgKmTFnf8NxKIk8y5vBMAkb7E649COWf3DM=;
        b=nV47qFtx5FxVRmv2LtZxYRvqlc5myWCUOWwUI0FlHkpbK+nEa5++05c6IKmUgPdzvM
         bRC1uXQ04OFF4Kyk4ZC1zHmYUcb/iK9HillwpzAzxwpLQZbssAZKibM5lPH/yVDLZ+lk
         bVaA6+WtIgMS2oq4YG6fi1AmDOCZiuIAFlTGF7ubJJTrEgXLjgGIASedDBMRndfNeN/s
         mHOPskFjxl8QmV1RilmceFNSuI2zJfOOMika0jCTalPiJFh2ux8jxWIBZR+/dAEqKGxp
         Ydx7/zPoROVr21nybIuQ9MV+sJXH6hCalqc5kG/Qr+3haOXG4bsXL58fRSOWn4Al4q4w
         AFEw==
MIME-Version: 1.0
X-Received: by 10.112.188.201 with SMTP id gc9mr9825803lbc.6.1422049984463;
 Fri, 23 Jan 2015 13:53:04 -0800 (PST)
Received: by 10.114.61.47 with HTTP; Fri, 23 Jan 2015 13:53:04 -0800 (PST)
In-Reply-To: <1421327092260-10123.post@n3.nabble.com>
References: <1421327092260-10123.post@n3.nabble.com>
Date: Fri, 23 Jan 2015 22:53:04 +0100
Message-ID: <CAD-7WjqpJbQ9+sh6zn5RHTaqMAc900zqY3zgheOU6cUT8yprSQ@mail.gmail.com>
Subject: Re: Spark 1.2.0: MissingRequirementError
From: Peter Prettenhofer <peter.prettenhofer@gmail.com>
To: PierreB <pierre.borckmans@realimpactanalytics.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c373662d496a050d58d0ab
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c373662d496a050d58d0ab
Content-Type: text/plain; charset=UTF-8

much appreciated if somebody could help fixing this issue -- or at least
give me some hints what might be wrong

thanks,
 Peter

2015-01-15 14:04 GMT+01:00 PierreB <pierre.borckmans@realimpactanalytics.com
>:

> Hi guys,
>
> A few people seem to have the same problem with Spark 1.2.0 so I figured I
> would push it here.
>
> see:
>
> http://apache-spark-user-list.1001560.n3.nabble.com/MissingRequirementError-with-spark-td21149.html
>
> In a nutshell, for sbt test to work, we now need to fork a JVM and also
> give
> more memory to be able to run tests.
>
> See
> also:
> https://github.com/deanwampler/spark-workshop/blob/master/project/Build.scala
>
> This all used to work fine until 1.2.0.
>
> Could u have a look please?
> Thanks
>
> P.
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-2-0-MissingRequirementError-tp10123.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


-- 
Peter Prettenhofer

--001a11c373662d496a050d58d0ab--

From dev-return-11267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 23 22:19:33 2015
Return-Path: <dev-return-11267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F8681726B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 Jan 2015 22:19:33 +0000 (UTC)
Received: (qmail 36593 invoked by uid 500); 23 Jan 2015 22:19:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36513 invoked by uid 500); 23 Jan 2015 22:19:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36502 invoked by uid 99); 23 Jan 2015 22:19:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 22:19:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 Jan 2015 22:19:27 +0000
Received: by mail-qc0-f178.google.com with SMTP id b13so8618647qcw.9
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 14:18:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type:content-transfer-encoding;
        bh=StuH7KeuB/puQhLGQqk/z+zbjU7UC9nZBd264wlBHPM=;
        b=bYVdK7W6JYPIEdLr1xnS6+GJsgIonqnCurErWGhA7lHxrO+G1LMIeuWcvWpYVV1LqR
         O8M4noxPQGYJHDeCX4o4PgxGHuSkPUytBQwruovaXYjp+0CXJzOyMaPE9z+m9/iySLb/
         G+Asn948bz3Yoj7Vb6iZfli5lbjFmZ0038DQa+IxjdtpcZQmDD6TK4em6rg5bNJ5qNON
         Q8shpr0wRi4cumlarb05ERXzvDeCwrEeiWChxvYxpzok7qfsmYKhNNbRjVSEKAm9I9DY
         IeONG71pnDuilExq7A1HU9WdIuQwxHdvak1Wlknba72BJg+9fJff3of65VXwTEDFXliQ
         I/fA==
X-Gm-Message-State: ALoCoQlKhxtc6tPRAypqljM7iEx/Zx5K1Rt0AGKL/qDq1oaMokz3xPpYZnS1gK5QeqeSW55a8P3y
MIME-Version: 1.0
X-Received: by 10.229.69.131 with SMTP id z3mr9127284qci.14.1422051480160;
 Fri, 23 Jan 2015 14:18:00 -0800 (PST)
Received: by 10.229.2.136 with HTTP; Fri, 23 Jan 2015 14:18:00 -0800 (PST)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE5C94@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
	<CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDE5C94@G4W3292.americas.hpqcorp.net>
Date: Fri, 23 Jan 2015 14:18:00 -0800
Message-ID: <CAEYYnxaAmqme9L7ngap=uP7uzKSsAC-azF-aHYEZDw7kb8y6Ug@mail.gmail.com>
Subject: Re: Maximum size of vector that reduce can handle
From: DB Tsai <dbtsai@dbtsai.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Alexander,

For `reduce`, it's an action that will collect all the data from
mapper to driver, and perform the aggregation in driver. As a result,
if the output from the mapper is very large, and the numbers of
partitions in mapper are large, it might cause a problem.

For `treeReduce`, as the name indicates, the way it works is in the
first layer, it aggregates the output of the mappers two by two
resulting half of the numbers of output. And then, we continuously do
the aggregation layer by layer. The final aggregation will be done in
driver but in this time, the numbers of data are small.

By default, depth 2 is used, so if you have so many partitions of
large vector, this may still cause issue. You can increase the depth
into higher numbers such that in the final reduce in driver, the
number of partitions are very small.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



On Fri, Jan 23, 2015 at 12:07 PM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Hi DB Tsai,
>
> Thank you for your suggestion. Actually, I've started my experiments with=
 "treeReduce". Originally, I had "vv.treeReduce(_ + _, 2)" in my script exa=
ctly because MLlib optimizers are using it, as you pointed out with LBFGS. =
However, it leads to the same problems as "reduce", but presumably not so d=
irectly. As far as I understand, treeReduce limits the number of communicat=
ions between workers and master forcing workers to partially compute the re=
duce operation.
>
> Are you sure that driver will first collect all results (or all partial r=
esults in treeReduce) and ONLY then perform aggregation? If that is the pro=
blem, then how to force it to do aggregation after receiving each portion o=
f data from Workers?
>
> Best regards, Alexander
>
> -----Original Message-----
> From: DB Tsai [mailto:dbtsai@dbtsai.com]
> Sent: Friday, January 23, 2015 11:53 AM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org
> Subject: Re: Maximum size of vector that reduce can handle
>
> Hi Alexander,
>
> When you use `reduce` to aggregate the vectors, those will actually be pu=
lled into driver, and merged over there. Obviously, it's not scaleable give=
n you are doing deep neural networks which have so many coefficients.
>
> Please try treeReduce instead which is what we do in linear regression an=
d logistic regression.
>
> See https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/=
org/apache/spark/mllib/optimization/LBFGS.scala
> for example.
>
> val (gradientSum, lossSum) =3D data.treeAggregate((Vectors.zeros(n), 0.0)=
)( seqOp =3D (c, v) =3D> (c, v) match { case ((grad, loss), (label, feature=
s)) =3D> val l =3D localGradient.compute( features, label, bcW.value, grad)=
 (grad, loss + l) }, combOp =3D (c1, c2) =3D> (c1, c2) match { case ((grad1=
, loss1), (grad2, loss2)) =3D> axpy(1.0, grad2, grad1) (grad1, loss1 + loss=
2)
> })
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
>
> On Fri, Jan 23, 2015 at 10:00 AM, Ulanov, Alexander <alexander.ulanov@hp.=
com> wrote:
>> Dear Spark developers,
>>
>> I am trying to measure the Spark reduce performance for big vectors. My =
motivation is related to machine learning gradient. Gradient is a vector th=
at is computed on each worker and then all results need to be summed up and=
 broadcasted back to workers. For example, present machine learning applica=
tions involve very long parameter vectors, for deep neural networks it can =
be up to 2Billions. So, I want to measure the time that is needed for this =
operation depending on the size of vector and number of workers. I wrote fe=
w lines of code that assume that Spark will distribute partitions among all=
 available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM=
), each runs 2 Workers.
>>
>> import org.apache.spark.mllib.rdd.RDDFunctions._
>> import breeze.linalg._
>> import org.apache.log4j._
>> Logger.getRootLogger.setLevel(Level.OFF)
>> val n =3D 60000000
>> val p =3D 12
>> val vv =3D sc.parallelize(0 until p, p).map(i =3D>
>> DenseVector.rand[Double]( n )) vv.reduce(_ + _)
>>
>> When executing in shell with 60M vector it crashes after some period of =
time. One of the node contains the following in stdout:
>> Java HotSpot(TM) 64-Bit Server VM warning: INFO:
>> os::commit_memory(0x0000000755500000, 2863661056, 0) failed;
>> error=3D'Cannot allocate memory' (errno=3D12) # # There is insufficient =
memory for the Java Runtime Environment to continue.
>> # Native memory allocation (malloc) failed to allocate 2863661056 bytes =
for committing reserved memory.
>>
>> I run shell with --executor-memory 8G --driver-memory 8G, so handling 60=
M vector of Double should not be a problem. Are there any big overheads for=
 this? What is the maximum size of vector that reduce can handle?
>>
>> Best regards, Alexander
>>
>> P.S.
>>
>> "spark.driver.maxResultSize 0" needs to set in order to run this code. I=
 also needed to change "java.io.tmpdir" and "spark.local.dir" folders becau=
se my /tmp folder which is default, was too small and Spark swaps heavily i=
nto this folder. Without these settings I get either "no space left on devi=
ce" or "out of memory" exceptions.
>>
>> I also submitted a bug
>> https://issues.apache.org/jira/browse/SPARK-5386
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
>> additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 04:57:34 2015
Return-Path: <dev-return-11268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50A8817B37
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 04:57:34 +0000 (UTC)
Received: (qmail 56922 invoked by uid 500); 24 Jan 2015 04:57:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56848 invoked by uid 500); 24 Jan 2015 04:57:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56836 invoked by uid 99); 24 Jan 2015 04:57:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 04:57:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kartheek.mbms@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 04:57:27 +0000
Received: by mail-we0-f178.google.com with SMTP id k48so844041wev.9
        for <dev@spark.apache.org>; Fri, 23 Jan 2015 20:55:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=C05IyYLJhFduIVrNZSV4D7TUiA283rk3Z9ItieTpBHg=;
        b=AZo0r3hLw6eOI95I6SiwOa4RZcqV8p1tOvu4Gy0OrbO0sHmGp/YsAk2eE1zSd4BUS0
         p5kAZoF8ZYhi45GwrawZQ47Br8Ft0ydbj+mmxnF9aTHcGCgs8gH5m27LV2K0w3E2P+sP
         gQEB0WqiLM6mYS1GIJf+eUD0M0K7N746jOi6AP03m9WX/f32tPlIn4GpHy8KlKJTPWwa
         tNiyza505gFxSDWsx6yXRZgeRTDVNS7hpqK/bia8Mdk8P8LMVmOcCNF8o4IX7lQfPzLD
         FFd6QbAteXpuRFP0yh0kCllKizTsvBd7rn5BVazMZcaPsc4TT7M/GSnD+Tmah/qsz4rX
         d7+w==
MIME-Version: 1.0
X-Received: by 10.180.211.169 with SMTP id nd9mr10296086wic.4.1422075336402;
 Fri, 23 Jan 2015 20:55:36 -0800 (PST)
Received: by 10.194.175.72 with HTTP; Fri, 23 Jan 2015 20:55:36 -0800 (PST)
Date: Sat, 24 Jan 2015 10:25:36 +0530
Message-ID: <CAAbaoBCYtLvQ2Pci90NsVBa5rUP6t_dZUCLDshhQQ73zt3n3-A@mail.gmail.com>
Subject: Find the two storage Locations of each partition of a replicated rdd.
From: Rapelly Kartheek <kartheek.mbms@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c389044531ff050d5eb7ef
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c389044531ff050d5eb7ef
Content-Type: text/plain; charset=UTF-8

hi,

I wanna find the storage locations( BlockManagerIds) of each partition when
the rdd is replicated twice. I mean, If a twice replicated rdd has got 5
partitions, I would like to know the first and second storage locations of
each partition. Basically, I am trying to modify the list of nodes selected
for replicating an rdd.

I just want to checkout where exactly does the first and second copies of
each partition gets stored. I tried upon the rdd storage details in the
webUI, but couldn't gain much.

Any help please!!

Thank you
Karthik

--001a11c389044531ff050d5eb7ef--

From dev-return-11269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 15:53:27 2015
Return-Path: <dev-return-11269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A361106C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 15:53:27 +0000 (UTC)
Received: (qmail 36996 invoked by uid 500); 24 Jan 2015 15:53:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36932 invoked by uid 500); 24 Jan 2015 15:53:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 32507 invoked by uid 99); 24 Jan 2015 15:46:09 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of williamsmith.mail@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Sat, 24 Jan 2015 08:45:11 -0700 (MST)
From: William-Smith <williamsmith.mail@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422114311868-10264.post@n3.nabble.com>
In-Reply-To: <CAMAsSdLmp7jxXgLoDq15_BB7A3rs_Xj90DHh4r9Xvh-XcZBnbA@mail.gmail.com>
References: <1411191524.83515.YahooMailNeo@web125504.mail.ne1.yahoo.com> <1421988168826-10250.post@n3.nabble.com> <CAMAsSdLmp7jxXgLoDq15_BB7A3rs_Xj90DHh4r9Xvh-XcZBnbA@mail.gmail.com>
Subject: Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

I did find that after my post and tried it
....spark.files.userClassPathFirst = true
But it is deemed in the docs as instrumental .... and did not work.
Monitored that additional config via the logs and it did not complain ... no
change though.


So...
Due to my timeline for a Demo....

Used HttpUrl instead for my POST and abandoned using Apache HTTP for now.


My app works but I am sure that this will raise it's head again with other
transitive dependencies.
Surely will repost any findings after Demo and beyond on this subject .. to
benefit the group.

Thx.
Will






--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-1-0-w-hadoop-2-4-vs-aws-java-sdk-1-7-2-tp8481p10264.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 16:27:21 2015
Return-Path: <dev-return-11270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA1B5107AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 16:27:21 +0000 (UTC)
Received: (qmail 87725 invoked by uid 500); 24 Jan 2015 16:27:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87647 invoked by uid 500); 24 Jan 2015 16:27:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87636 invoked by uid 99); 24 Jan 2015 16:27:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 16:27:20 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ogeagla@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 16:27:16 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 96B3511985EF
	for <dev@spark.apache.org>; Sat, 24 Jan 2015 08:26:26 -0800 (PST)
Date: Sat, 24 Jan 2015 09:26:25 -0700 (MST)
From: Octavian Geagla <ogeagla@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422116785722-10265.post@n3.nabble.com>
Subject: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello,

I found it useful to implement the  Hadamard Product
<https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29http://>   as
a VectorTransformer.  It can be applied to scale (by a constant) a certain
dimension (column) of the data set.  

Since I've already implemented it and am using it, I thought I'd see if
there's interest in this feature going in as Experimental.  I'm not sold on
the name 'Weighter', either.

Here's the current branch with the work (docs, impl, tests).
<https://github.com/ogeagla/spark/compare/spark-mllib-weighting>  

The implementation was heavily inspired by those of StandardScalerModel and
Normalizer.

Thanks
Octavian



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Any-interest-in-weighting-VectorTransformer-which-does-component-wise-scaling-tp10265.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 20:43:31 2015
Return-Path: <dev-return-11271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C82D110BFF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 20:43:31 +0000 (UTC)
Received: (qmail 66156 invoked by uid 500); 24 Jan 2015 20:43:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66091 invoked by uid 500); 24 Jan 2015 20:43:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 58072 invoked by uid 99); 24 Jan 2015 20:37:37 -0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of galashrenik55@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Sat, 24 Jan 2015 13:36:09 -0700 (MST)
From: shrenik <galashrenik55@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422131769445-10266.post@n3.nabble.com>
Subject: Project ideas for spark
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello guys, 
I'm a graduate student and have to make a semester long project based on
Data Intensive Computing. I'm interested in using Spark in my project so I
would really appreciate if any body could suggest any project ideas related
to using spark. Thank you in advance.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Project-ideas-for-spark-tp10266.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 22:27:50 2015
Return-Path: <dev-return-11272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F246410E13
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 22:27:49 +0000 (UTC)
Received: (qmail 359 invoked by uid 500); 24 Jan 2015 22:27:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 290 invoked by uid 500); 24 Jan 2015 22:27:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 276 invoked by uid 99); 24 Jan 2015 22:27:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 22:27:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 22:27:44 +0000
Received: by mail-ig0-f171.google.com with SMTP id r10so3009619igi.4
        for <dev@spark.apache.org>; Sat, 24 Jan 2015 14:27:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=6r3mOMg6xHeuRcQLEaJd+mQig8BYIqrKXBAB3MVxmEQ=;
        b=Kw9IL8DDipkN0tkFLeZkSO2LOdZbOnM6PcyzP8i0alj7FXsFLSDnNN+5KVRlqTvbEc
         LtLSFVJU314cRnopaRIAijqepHgdSTwaHmrEJr85wYXXYTfgeWk0++5BI076saBAs6nk
         ONN/ia1jKoTe3TDC1I6uegPu+u2+aJrewcFMRo1BMuN4ZTLG1Ojip8xmzc50x2+A8V26
         4Wf6FyMNyGK5Ppt/YzQSHnhXRsc/gRVYQ0Z17V/VOf0yEjlnZdOEXt167ufFaqLtIC2t
         9G2NFzbBXlK6I4wRmrZZwAZMgzz81wTNvlhEayq4x8a2qzbmhpraSgh24rY8efsSJtcu
         7H8A==
X-Received: by 10.107.11.215 with SMTP id 84mr10893154iol.46.1422138443979;
 Sat, 24 Jan 2015 14:27:23 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 24 Jan 2015 22:27:23 +0000
Message-ID: <CAOhmDzeTj85Pm+CovZBJg-V5sk5pxg47LgP-=OWvUpScmHVFXw@mail.gmail.com>
Subject: Does spark-ec2 support Windows?
To: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f8a74c6646e050d6d6846
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f8a74c6646e050d6d6846
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Is spark-ec2 supposed to run normally from Windows (e.g. to launch a
cluster)?

I ask because I don=E2=80=99t see mention of Windows anywhere in relation t=
o
spark-ec2, and there is an open PR
<https://github.com/apache/spark/pull/4162/files#diff-ada66bbeb2f1327b50823=
2ef6c3805a5R358>
that checks file permissions, and I=E2=80=99m not sure if we need to care a=
bout how
permission masks show up on Windows vs. Unix systems.

Nick
=E2=80=8B

--001a113f8a74c6646e050d6d6846--

From dev-return-11273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 24 22:34:46 2015
Return-Path: <dev-return-11273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71ED910E2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 Jan 2015 22:34:46 +0000 (UTC)
Received: (qmail 7036 invoked by uid 500); 24 Jan 2015 22:34:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6959 invoked by uid 500); 24 Jan 2015 22:34:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6948 invoked by uid 99); 24 Jan 2015 22:34:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 22:34:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 Jan 2015 22:34:40 +0000
Received: by mail-wg0-f46.google.com with SMTP id l2so3295807wgh.5
        for <dev@spark.apache.org>; Sat, 24 Jan 2015 14:34:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=rIZ9tdBE0Vt6IP0gKUJMpDQxswdzmxZ5gIT4Nwbz6xo=;
        b=Ml8yPnt1YRsxaAnkJVifXceSgFahGO53cQFd6Uufa/+X9tGwuWsEOjd2xKElSCMCsP
         2EBnUSGCZIQQVpE5IDsZONIddd8X9YtwaHoRpssiezS1OYQUKGNwbrq97VwqhkefHPWx
         Q0k4xhPKBAGN68jRP6CCMRFCCw1VTzxvtDsVf4kaSMKafKhsVfORXN3lQ9yRJOI5U49y
         vdMaJakHMQiBLL/yIwHAlqigljwPENKbUdL3Xyi60V9frUUlbo5UN5O3/amKTZeG4hGj
         is7AjOKbpQ0z8kATIsOePGTJ4SLEKu3ZMSBNJ/Oc35UCx3SiKQW4svWQ7a+PBEKSpN8Z
         pdzg==
X-Gm-Message-State: ALoCoQkxYPKLTWQ26/Kb2v72TqHCAgsm/V4p5KlRnPN0dRK0CLHMDF6eZ2Xq93k3iJoYlbfFUHq3
MIME-Version: 1.0
X-Received: by 10.195.12.35 with SMTP id en3mr28852039wjd.129.1422138859272;
 Sat, 24 Jan 2015 14:34:19 -0800 (PST)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.163.6 with HTTP; Sat, 24 Jan 2015 14:34:19 -0800 (PST)
In-Reply-To: <CAOhmDzeTj85Pm+CovZBJg-V5sk5pxg47LgP-=OWvUpScmHVFXw@mail.gmail.com>
References: <CAOhmDzeTj85Pm+CovZBJg-V5sk5pxg47LgP-=OWvUpScmHVFXw@mail.gmail.com>
Date: Sat, 24 Jan 2015 14:34:19 -0800
Message-ID: <CAKx7Bf-RBz6Ygn=VqAaJ-6EVoojjLAOwphGh7fJMMpVa77VkAg@mail.gmail.com>
Subject: Re: Does spark-ec2 support Windows?
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf788874b3d050d6d81c5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf788874b3d050d6d81c5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

AFAIK we've never claimed that spark-ec2 is supported on Windows -- for
example in the code, we assume the existence of `ssh` as a binary on PATH
etc. That said I've never tried to run it from Windows, so I am not sure if
it can somehow be used.

Thanks
Shivaram

On Sat, Jan 24, 2015 at 2:27 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Is spark-ec2 supposed to run normally from Windows (e.g. to launch a
> cluster)?
>
> I ask because I don=E2=80=99t see mention of Windows anywhere in relation=
 to
> spark-ec2, and there is an open PR
> <
> https://github.com/apache/spark/pull/4162/files#diff-ada66bbeb2f1327b5082=
32ef6c3805a5R358
> >
> that checks file permissions, and I=E2=80=99m not sure if we need to care=
 about how
> permission masks show up on Windows vs. Unix systems.
>
> Nick
> =E2=80=8B
>

--047d7bfcf788874b3d050d6d81c5--

From dev-return-11274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 26 11:16:31 2015
Return-Path: <dev-return-11274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5472BCA93
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 Jan 2015 11:16:31 +0000 (UTC)
Received: (qmail 84145 invoked by uid 500); 26 Jan 2015 11:16:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84070 invoked by uid 500); 26 Jan 2015 11:16:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84059 invoked by uid 99); 26 Jan 2015 11:16:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 11:16:28 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 11:16:24 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 1448F11BB4BD
	for <dev@spark.apache.org>; Mon, 26 Jan 2015 03:15:43 -0800 (PST)
Date: Mon, 26 Jan 2015 04:15:42 -0700 (MST)
From: PierreB <pierre.borckmans@realimpactanalytics.com>
To: dev@spark.apache.org
Message-ID: <1422270942943-10269.post@n3.nabble.com>
Subject: [SQL] Self join with ArrayType columns problems
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Using Spark 1.2.0, we are facing some weird behaviour when performing self
join on a table with some ArrayType field.
(potential bug ?)

I have set up a minimal non working example here:
https://gist.github.com/pierre-borckmans/4853cd6d0b2f2388bf4f
<https://gist.github.com/pierre-borckmans/4853cd6d0b2f2388bf4f>  

In a nutshell, if the ArrayType column used for the pivot is created
manually in the StructType definition, everything works as expected.
However, if the ArrayType pivot column is obtained by a sql query (be it by
using a "array" wrapper, or using a collect_list operator for instance),
then results are completely off.

Could anyone have a look as this really is a blocking issue.

Thanks!

Cheers

P.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/SQL-Self-join-with-ArrayType-columns-problems-tp10269.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 26 20:38:26 2015
Return-Path: <dev-return-11275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3087C178CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 Jan 2015 20:38:26 +0000 (UTC)
Received: (qmail 5498 invoked by uid 500); 26 Jan 2015 20:38:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5428 invoked by uid 500); 26 Jan 2015 20:38:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5416 invoked by uid 99); 26 Jan 2015 20:38:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 20:38:24 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 20:38:00 +0000
Received: by mail-la0-f53.google.com with SMTP id gq15so9803405lab.12
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 12:37:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=pHv62a4fja99as3WCIkuCkQsim1ffbsAyE+eY5Z5vew=;
        b=X0jafS73dDT2c734zFiZe3/BklHhr1tEVJ1K6SIUiDHo9O+36iyruPAo5h1DfnnbU7
         3wFvt2pHnnTwx4Ix2bLBTCXiL+Qs5E2eav0Tv3jcz/jH4RqJZ6vtcvKWIXPd3m7Fnhr6
         bRHiV0U23WVXu7YrJQck2PkMNi+ZxcMbCFG2ARjGeVNQqEVtPqjr+/vIjezy6qfVyTvc
         AqgoPI9lMZmsR6J+y+uAAwnzgHSuRgWZSz6C9YQn+KKWDylGdRPuUxAxxqeWLWQkc/6z
         fJkowMNlm9m+dKyCXFretV9zUbyE0XmciixZC/DaDnmFzESFWbut5IXSB81d0h/9wmii
         a+oQ==
X-Gm-Message-State: ALoCoQkquFCnXPRleGT1u/g9bX2KvHQ00nQF2xGhHmP8akobJ93Hl8LwKRVa1SuYOMD5Ok53gqRf
X-Received: by 10.112.129.195 with SMTP id ny3mr294642lbb.10.1422304658765;
 Mon, 26 Jan 2015 12:37:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 26 Jan 2015 12:37:18 -0800 (PST)
In-Reply-To: <CAKW0i0zMGemVSX++PboC9Msr=mAfbS9SCr8z+f_JVq+vg6xh=Q@mail.gmail.com>
References: <1421917142854-10243.post@n3.nabble.com> <CAKW0i0zMGemVSX++PboC9Msr=mAfbS9SCr8z+f_JVq+vg6xh=Q@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 26 Jan 2015 12:37:18 -0800
Message-ID: <CAAswR-5vVPTOb+4=KranVi=DOAL-0pdLW89y4rYazvShnfz7Nw@mail.gmail.com>
Subject: Re: Are there any plans to run Spark on top of Succinct
To: Rachit Agarwal <ragarwal@berkeley.edu>
Cc: Mick Davies <michael.belldavies@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Dean Wampler <deanwampler@gmail.com>
Content-Type: multipart/alternative; boundary=047d7b3441c6f2cc4f050d941bb2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3441c6f2cc4f050d941bb2
Content-Type: text/plain; charset=UTF-8

There was work being done at Berkeley on prototyping support for Succinct
in Spark SQL.  Rachit might have more information.

On Thu, Jan 22, 2015 at 7:04 AM, Dean Wampler <deanwampler@gmail.com> wrote:

> Interesting. I was wondering recently if anyone has explored working with
> compressed data directly.
>
> Dean Wampler, Ph.D.
> Author: Programming Scala, 2nd Edition
> <http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
> Typesafe <http://typesafe.com>
> @deanwampler <http://twitter.com/deanwampler>
> http://polyglotprogramming.com
>
> On Thu, Jan 22, 2015 at 2:59 AM, Mick Davies <michael.belldavies@gmail.com
> >
> wrote:
>
> >
> > http://succinct.cs.berkeley.edu/wp/wordpress/
> >
> > Looks like a really interesting piece of work that could dovetail well
> with
> > Spark.
> >
> > I have been trying recently to optimize some queries I have running on
> > Spark
> > on top of Parquet but the support from Parquet for predicate push down
> > especially for dictionary based columns is a bit limiting. I am not sure,
> > but from a cursory view it looks like this format may help in this area.
> >
> > Mick
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Are-there-any-plans-to-run-Spark-on-top-of-Succinct-tp10243.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7b3441c6f2cc4f050d941bb2--

From dev-return-11276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 26 22:20:28 2015
Return-Path: <dev-return-11276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E6C017EC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 Jan 2015 22:20:28 +0000 (UTC)
Received: (qmail 33222 invoked by uid 500); 26 Jan 2015 22:20:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33157 invoked by uid 500); 26 Jan 2015 22:20:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33132 invoked by uid 99); 26 Jan 2015 22:20:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 22:20:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 22:20:21 +0000
Received: by mail-qc0-f177.google.com with SMTP id p6so9383065qcv.8
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 14:18:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=rzS9QVoOfjNAAhjFSG4GyJrV/rEB6YowaE/nRMbn9x0=;
        b=CdeEBy627JL7DLfLsNWHC+jz4HXPGfDTdktUSOnAWcwNZ5s9KWdqBlFdadktkO1mL5
         7Mj9RjSBgbL+SVKE8l6B9dqTwRYhEsYnPM/nbw4ytUPnXi6cAzynmnFuS3EY/357gCJy
         ANKD+AE9eTdI4Gu2cu6RaM9V/owpjXnRjFAxTHrFUi8D+RZ3woH3iIVxqxIupJzjUKhi
         Z+Ij6O86Fe/iVVdGj8noE3bD/ZMbj8F8DohKoYJF2nxfjhJLTsaIFtbfwKrSuqNNbuvq
         U3ydpHbVqPA7HYKzAhuQEQ6OCCmkPGrtJ5PIRuUh6mvoz1EZRkUfpKiwjp/8/vgn7Fta
         I7mA==
X-Gm-Message-State: ALoCoQmtJfPwUP3ZsjYmCYnth8QtiynNN69JR3Q8e9gqD5hkGrfSul/lBuAX/4lt2lc4M2IA18eC
X-Received: by 10.140.86.233 with SMTP id p96mr1421094qgd.49.1422310734671;
 Mon, 26 Jan 2015 14:18:54 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Mon, 26 Jan 2015 14:18:34 -0800 (PST)
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 26 Jan 2015 14:18:34 -0800
Message-ID: <CAPh_B=ZxU-fM4eSh_o-Z1puJbkhyMyf0BUEU8xfvgfooo_CL=g@mail.gmail.com>
Subject: renaming SchemaRDD -> DataFrame
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c120c219e398050d9586c7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c120c219e398050d9586c7
Content-Type: text/plain; charset=UTF-8

Hi,

We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
get the community's opinion.

The context is that SchemaRDD is becoming a common data format used for
bringing data into Spark from external systems, and used for various
components of Spark, e.g. MLlib's new pipeline API. We also expect more and
more users to be programming directly against SchemaRDD API rather than the
core RDD API. SchemaRDD, through its less commonly used DSL originally
designed for writing test cases, always has the data-frame like API. In
1.3, we are redesigning the API to make the API usable for end users.


There are two motivations for the renaming:

1. DataFrame seems to be a more self-evident name than SchemaRDD.

2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
though it would contain some RDD functions like map, flatMap, etc), and
calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
DataFrame.rdd will return the underlying RDD for all RDD methods.


My understanding is that very few users program directly against the
SchemaRDD API at the moment, because they are not well documented. However,
oo maintain backward compatibility, we can create a type alias DataFrame
that is still named SchemaRDD. This will maintain source compatibility for
Scala. That said, we will have to update all existing materials to use
DataFrame rather than SchemaRDD.

--001a11c120c219e398050d9586c7--

From dev-return-11277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 26 23:02:23 2015
Return-Path: <dev-return-11277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 034FF172EA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 Jan 2015 23:02:23 +0000 (UTC)
Received: (qmail 73710 invoked by uid 500); 26 Jan 2015 23:02:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73627 invoked by uid 500); 26 Jan 2015 23:02:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73615 invoked by uid 99); 26 Jan 2015 23:02:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 23:02:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 23:01:57 +0000
Received: by mail-oi0-f49.google.com with SMTP id a3so9697822oib.8
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 15:01:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=aMDIQATIHWaYgP5k7M6656kYxqhJK9Ng7f69IuudIO0=;
        b=FEvVbRjYajs1ziHZ/69eefXMFiswHOsYirWMwJXnE023kodzj3BNjlZRg4EnSpXKaT
         aZy4ALxSKwnWOYQW2qms+324iqBWQjD7U0ou4x6pZcCeJJND116MEwtaVHoe5Yu3WDBr
         navnyiYLL3LSSccWPrBypHQhRrXr7swkGuhgwc5ayYAHLKGU0KiNeIOECQ7QzqzVrBNH
         Dq8SymhVr9kivTi7chLfsJi8lMNmvrCJxWnIh1gIb+iM86n44SXFjGVMa2WhuBzT39Ki
         5nTuQROZ6DZ136SXQ0HIQC4f7IPm3wtIoyrtIXu+J+Qg0WrQIPu7kweLCfHjsdLWP7zc
         2YXw==
MIME-Version: 1.0
X-Received: by 10.60.47.112 with SMTP id c16mr14250440oen.83.1422313315634;
 Mon, 26 Jan 2015 15:01:55 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Mon, 26 Jan 2015 15:01:55 -0800 (PST)
In-Reply-To: <CAPh_B=ZxU-fM4eSh_o-Z1puJbkhyMyf0BUEU8xfvgfooo_CL=g@mail.gmail.com>
References: <CAPh_B=ZxU-fM4eSh_o-Z1puJbkhyMyf0BUEU8xfvgfooo_CL=g@mail.gmail.com>
Date: Mon, 26 Jan 2015 15:01:55 -0800
Message-ID: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

One thing potentially not clear from this e-mail, there will be a 1:1
correspondence where you can get an RDD to/from a DataFrame.

On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> wrote:
> Hi,
>
> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
> get the community's opinion.
>
> The context is that SchemaRDD is becoming a common data format used for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API. We also expect more and
> more users to be programming directly against SchemaRDD API rather than the
> core RDD API. SchemaRDD, through its less commonly used DSL originally
> designed for writing test cases, always has the data-frame like API. In
> 1.3, we are redesigning the API to make the API usable for end users.
>
>
> There are two motivations for the renaming:
>
> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>
> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
> though it would contain some RDD functions like map, flatMap, etc), and
> calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
> DataFrame.rdd will return the underlying RDD for all RDD methods.
>
>
> My understanding is that very few users program directly against the
> SchemaRDD API at the moment, because they are not well documented. However,
> oo maintain backward compatibility, we can create a type alias DataFrame
> that is still named SchemaRDD. This will maintain source compatibility for
> Scala. That said, we will have to update all existing materials to use
> DataFrame rather than SchemaRDD.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jan 26 23:13:59 2015
Return-Path: <dev-return-11278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EFB917354
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 Jan 2015 23:13:59 +0000 (UTC)
Received: (qmail 2151 invoked by uid 500); 26 Jan 2015 23:13:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2069 invoked by uid 500); 26 Jan 2015 23:13:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2057 invoked by uid 99); 26 Jan 2015 23:13:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 23:13:56 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 72.30.238.142 as permitted sender)
Received: from [72.30.238.142] (HELO nm36-vm6.bullet.mail.bf1.yahoo.com) (72.30.238.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 Jan 2015 23:13:51 +0000
Received: from [66.196.81.173] by nm36.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2015 23:11:24 -0000
Received: from [98.139.212.193] by tm19.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2015 23:11:24 -0000
Received: from [127.0.0.1] by omp1002.mail.bf1.yahoo.com with NNFMP; 26 Jan 2015 23:11:24 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 29826.32020.bm@omp1002.mail.bf1.yahoo.com
X-YMail-OSG: KVWsTWIVM1lYtbVOM0Nd6XiGQLN.YAiO7sM167ZNn7Sgo5BG.XZcz7vOdEp5foi
 JIbA4Hdwah7dDvfS3Vb.BMm3pkU_7wYxuIwet2BFg.e8xbF0KUNiFm4vf8de71hCiJ74ol4KkDmL
 88QJYaWQbbXESNCxEvktZK4KRgvUiooEjiYASkfsCYVYMs5W7T9PemZLHTnqEc7pSFlsjNS_DKiv
 MHpv0cHTEY_JTn5qDHb90WlwlkrKuAl48wVDVEE8os4HIbsVSyPvw85D3.W0_8aZSjCne2NQdkND
 OGPb8PafPPXANGntNJGamuIahbAeoX0u6yvunFwJONB.V1Xrjgr9k57F20B2E_cH4vbUscnEiVIB
 yBMagizOaKJrFLJaiLjynDhkFHxz2Z76mYVrBoxGfIj7QQ.PvX2JbKH57R7c.CIzJViJgseW8xA5
 jzugRjGoqqp7Eon7oudiOCHDD1IcBv7P_3ujOXzO9tDkW1XtUnslKu6RhXt7xiZqC8sb1_q7MPk6
 HdanZ7ST4.Oyka8tjnMVyaTZuXa7uKZJvIW156zvkHmXuP7MkZFT3v7qVai875M2spwXyRGyyz35
 _ufenjWb12aOmdxeczLDAaA--
Received: by 66.196.80.123; Mon, 26 Jan 2015 23:11:23 +0000 
Date: Mon, 26 Jan 2015 23:11:22 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
In-Reply-To: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay Area Spark Meetup YouTube contained a wealth of background information on this idea (mostly from Patrick and Reynold :-).

https://www.youtube.com/watch?v=YWppYPWznSQ

________________________________
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com> 
Cc: "dev@spark.apache.org" <dev@spark.apache.org> 
Sent: Monday, January 26, 2015 4:01 PM
Subject: Re: renaming SchemaRDD -> DataFrame


One thing potentially not clear from this e-mail, there will be a 1:1
correspondence where you can get an RDD to/from a DataFrame.


On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> wrote:
> Hi,
>
> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
> get the community's opinion.
>
> The context is that SchemaRDD is becoming a common data format used for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API. We also expect more and
> more users to be programming directly against SchemaRDD API rather than the
> core RDD API. SchemaRDD, through its less commonly used DSL originally
> designed for writing test cases, always has the data-frame like API. In
> 1.3, we are redesigning the API to make the API usable for end users.
>
>
> There are two motivations for the renaming:
>
> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>
> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
> though it would contain some RDD functions like map, flatMap, etc), and
> calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
> DataFrame.rdd will return the underlying RDD for all RDD methods.
>
>
> My understanding is that very few users program directly against the
> SchemaRDD API at the moment, because they are not well documented. However,
> oo maintain backward compatibility, we can create a type alias DataFrame
> that is still named SchemaRDD. This will maintain source compatibility for
> Scala. That said, we will have to update all existing materials to use
> DataFrame rather than SchemaRDD.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 00:28:11 2015
Return-Path: <dev-return-11279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B7FD1766D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 00:28:11 +0000 (UTC)
Received: (qmail 79107 invoked by uid 500); 27 Jan 2015 00:28:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79033 invoked by uid 500); 27 Jan 2015 00:28:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79021 invoked by uid 99); 27 Jan 2015 00:28:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 00:28:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 00:27:44 +0000
Received: by mail-wi0-f178.google.com with SMTP id em10so1712610wid.5
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 16:27:22 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=Y1xgFsJzrP+sTF3IJkp338MUe5Gj7rzeNFwRWBO0KIc=;
        b=Xhq1KNCfon7dFaEE0aIBvqdPYs7b+el+F0iGBlhyU+52HRSzVck38r7L/h+tXtk1O0
         CCNUDeUrjDD2GNdaPhtH6B8CV21NjupY0Nx1BXQTDJgx6PhNiO1nFmMSuIPJrEUJoZ4l
         Au2eZOUbl3hL9xjLdI61QYMc//qY16Y2pZR/ZIq6mlv++zi/QsyZlwgJygeO08az4SuK
         Lj9gkCfy7tkbPnaJdawS9JtOPEhWsSyE4XpcVOVKT9p5nzfPZ/vRob+Xl+8cdDlOn1kX
         attoun3gzKr1PpT84TqxmE1jYRQQzDd7ekndd27kVXX8+uK5pZCpagfvtNDSe3pwLVYX
         uPjw==
X-Gm-Message-State: ALoCoQkzsL7tJ715UKu3xXJPE1BAv8oFsVwb5eyHvAPRY0mNRAiKgiZPCswxXjdK7M1G3WmDR6By
MIME-Version: 1.0
X-Received: by 10.194.201.137 with SMTP id ka9mr50852205wjc.66.1422318442634;
 Mon, 26 Jan 2015 16:27:22 -0800 (PST)
Received: by 10.217.122.200 with HTTP; Mon, 26 Jan 2015 16:27:22 -0800 (PST)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
Date: Mon, 26 Jan 2015 19:27:22 -0500
Message-ID: <CANx3uAgnHWj7FF2fS2Y7QLyRxtYo1rEE42ajUPa-rSU-2sh7WA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Koert Kuipers <koert@tresata.com>
To: Michael Malak <michaelmalak@yahoo.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b873d5e87f6e9050d975185
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b873d5e87f6e9050d975185
Content-Type: text/plain; charset=UTF-8

what i am trying to say is: structured data != sql

On Mon, Jan 26, 2015 at 7:26 PM, Koert Kuipers <koert@tresata.com> wrote:

> "The context is that SchemaRDD is becoming a common data format used for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API."
>
> i agree. this to me also implies it belongs in spark core, not sql
>
> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> michaelmalak@yahoo.com.invalid> wrote:
>
>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
>> Area Spark Meetup YouTube contained a wealth of background information on
>> this idea (mostly from Patrick and Reynold :-).
>>
>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>
>> ________________________________
>> From: Patrick Wendell <pwendell@gmail.com>
>> To: Reynold Xin <rxin@databricks.com>
>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> Sent: Monday, January 26, 2015 4:01 PM
>> Subject: Re: renaming SchemaRDD -> DataFrame
>>
>>
>> One thing potentially not clear from this e-mail, there will be a 1:1
>> correspondence where you can get an RDD to/from a DataFrame.
>>
>>
>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> wrote:
>> > Hi,
>> >
>> > We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
>> > get the community's opinion.
>> >
>> > The context is that SchemaRDD is becoming a common data format used for
>> > bringing data into Spark from external systems, and used for various
>> > components of Spark, e.g. MLlib's new pipeline API. We also expect more
>> and
>> > more users to be programming directly against SchemaRDD API rather than
>> the
>> > core RDD API. SchemaRDD, through its less commonly used DSL originally
>> > designed for writing test cases, always has the data-frame like API. In
>> > 1.3, we are redesigning the API to make the API usable for end users.
>> >
>> >
>> > There are two motivations for the renaming:
>> >
>> > 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>> >
>> > 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
>> > though it would contain some RDD functions like map, flatMap, etc), and
>> > calling it Schema*RDD* while it is not an RDD is highly confusing.
>> Instead.
>> > DataFrame.rdd will return the underlying RDD for all RDD methods.
>> >
>> >
>> > My understanding is that very few users program directly against the
>> > SchemaRDD API at the moment, because they are not well documented.
>> However,
>> > oo maintain backward compatibility, we can create a type alias DataFrame
>> > that is still named SchemaRDD. This will maintain source compatibility
>> for
>> > Scala. That said, we will have to update all existing materials to use
>> > DataFrame rather than SchemaRDD.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--047d7b873d5e87f6e9050d975185--

From dev-return-11280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 00:29:12 2015
Return-Path: <dev-return-11280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 68BCA17671
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 00:29:12 +0000 (UTC)
Received: (qmail 82673 invoked by uid 500); 27 Jan 2015 00:29:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82597 invoked by uid 500); 27 Jan 2015 00:29:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82586 invoked by uid 99); 27 Jan 2015 00:29:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 00:29:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 00:29:06 +0000
Received: by mail-wi0-f169.google.com with SMTP id h11so7665053wiw.0
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 16:26:09 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=LxXSfVe0mwDjcKm/9rfSWTlLOyGT3idzEU/HCRwQ6Uc=;
        b=MPHNSAkBCeVxJW0IvkKzyUIK3Ff89NL2wlUVtwHgX1Siphj66JoTYKgnjg6i4LOX7p
         WZhsucJzYiwCe9hFrzTTUDubJJrJihJ0hfovbrSjTVvgxawV5OtrRgPonh6cHQE/DgA6
         UrnRuDIc/rKijyBYz/96AznymFBwd97gdSLFOeSx5y25EDma9W7GWvZ5BClcJoqQnZNX
         HF3zMLuBdjbryXnYZWJS5DEsCt38VOUExJCci4iZIgFC416O98dFWalniBJmKdwv+xSd
         RvpSfRJfQUbmM/Rr+ykJwMxkwdcHtGwflzLpWk0A3HJPV7F2ETeYl5skFot/uhKek14A
         erXw==
X-Gm-Message-State: ALoCoQkqoNC4i1TVvrQHj+8z0xZQtdWkg7CXZFY1nnKbNOF3cWwm/XX0MXKivnBuk8cKPd7xrIQo
MIME-Version: 1.0
X-Received: by 10.180.82.137 with SMTP id i9mr500281wiy.38.1422318369817; Mon,
 26 Jan 2015 16:26:09 -0800 (PST)
Received: by 10.217.122.200 with HTTP; Mon, 26 Jan 2015 16:26:09 -0800 (PST)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
Date: Mon, 26 Jan 2015 19:26:09 -0500
Message-ID: <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Koert Kuipers <koert@tresata.com>
To: Michael Malak <michaelmalak@yahoo.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0418274230e136050d974d2e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0418274230e136050d974d2e
Content-Type: text/plain; charset=UTF-8

"The context is that SchemaRDD is becoming a common data format used for
bringing data into Spark from external systems, and used for various
components of Spark, e.g. MLlib's new pipeline API."

i agree. this to me also implies it belongs in spark core, not sql

On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
michaelmalak@yahoo.com.invalid> wrote:

> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay Area
> Spark Meetup YouTube contained a wealth of background information on this
> idea (mostly from Patrick and Reynold :-).
>
> https://www.youtube.com/watch?v=YWppYPWznSQ
>
> ________________________________
> From: Patrick Wendell <pwendell@gmail.com>
> To: Reynold Xin <rxin@databricks.com>
> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> Sent: Monday, January 26, 2015 4:01 PM
> Subject: Re: renaming SchemaRDD -> DataFrame
>
>
> One thing potentially not clear from this e-mail, there will be a 1:1
> correspondence where you can get an RDD to/from a DataFrame.
>
>
> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> wrote:
> > Hi,
> >
> > We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
> > get the community's opinion.
> >
> > The context is that SchemaRDD is becoming a common data format used for
> > bringing data into Spark from external systems, and used for various
> > components of Spark, e.g. MLlib's new pipeline API. We also expect more
> and
> > more users to be programming directly against SchemaRDD API rather than
> the
> > core RDD API. SchemaRDD, through its less commonly used DSL originally
> > designed for writing test cases, always has the data-frame like API. In
> > 1.3, we are redesigning the API to make the API usable for end users.
> >
> >
> > There are two motivations for the renaming:
> >
> > 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> >
> > 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
> > though it would contain some RDD functions like map, flatMap, etc), and
> > calling it Schema*RDD* while it is not an RDD is highly confusing.
> Instead.
> > DataFrame.rdd will return the underlying RDD for all RDD methods.
> >
> >
> > My understanding is that very few users program directly against the
> > SchemaRDD API at the moment, because they are not well documented.
> However,
> > oo maintain backward compatibility, we can create a type alias DataFrame
> > that is still named SchemaRDD. This will maintain source compatibility
> for
> > Scala. That said, we will have to update all existing materials to use
> > DataFrame rather than SchemaRDD.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--f46d0418274230e136050d974d2e--

From dev-return-11281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 01:33:27 2015
Return-Path: <dev-return-11281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 624E117886
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 01:33:27 +0000 (UTC)
Received: (qmail 9648 invoked by uid 500); 27 Jan 2015 01:33:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9571 invoked by uid 500); 27 Jan 2015 01:33:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9553 invoked by uid 99); 27 Jan 2015 01:33:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:33:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:33:21 +0000
Received: by mail-pa0-f47.google.com with SMTP id lj1so15041393pab.6
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 17:32:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=YmwpyM5F9xR7oXJB4wZsHDovLzfMMMjq/KunGQ8KLnU=;
        b=Ud/7qUpuh4WI9ANUMopTCWLIIlufITD7CM5aQKgSJgPNKLMvmdx488rZe/BIMjmATC
         7UOyRqb35AGxtq0zh/dEiDJwrnzuDSXzNEKb0naHJ2k5QIhV9HYByaomk6PXAwpto7x/
         l/2ZGEMfle/Gq2jP/EYZAQhXsUx7IMS5L6ToTfXg0I5ftFT8fWvJi38pIO4M5CLCgO3j
         4K/0UK4C5xvZyV0D6KiySZqns82uMhuDaDWB8/iEkpheMZ7+Mnplf0D5hOEHGwDpZpiA
         QgP0sNJvGGKkSySpffc/+akFZb25/UaSAHdc0L65dZGW09cronrWM4Sh9I3lJ41YQRt5
         /6LA==
X-Received: by 10.70.133.70 with SMTP id pa6mr22431789pdb.24.1422322336027;
        Mon, 26 Jan 2015 17:32:16 -0800 (PST)
Received: from [192.168.1.100] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id k3sm10896241pdj.2.2015.01.26.17.32.14
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 26 Jan 2015 17:32:15 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: renaming SchemaRDD -> DataFrame
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
Date: Mon, 26 Jan 2015 17:32:13 -0800
Cc: Michael Malak <michaelmalak@yahoo.com>,
 Patrick Wendell <pwendell@gmail.com>,
 Reynold Xin <rxin@databricks.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com> <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com> <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com> <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
To: Koert Kuipers <koert@tresata.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

(Actually when we designed Spark SQL we thought of giving it another =
name, like Spark Schema, but we decided to stick with SQL since that was =
the most obvious use case to many users.)

Matei

> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:
>=20
> While it might be possible to move this concept to Spark Core =
long-term, supporting structured data efficiently does require quite a =
bit of the infrastructure in Spark SQL, such as query planning and =
columnar storage. The intent of Spark SQL though is to be more than a =
SQL server -- it's meant to be a library for manipulating structured =
data. Since this is possible to build over the core API, it's pretty =
natural to organize it that way, same as Spark Streaming is a library.
>=20
> Matei
>=20
>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
>>=20
>> "The context is that SchemaRDD is becoming a common data format used =
for
>> bringing data into Spark from external systems, and used for various
>> components of Spark, e.g. MLlib's new pipeline API."
>>=20
>> i agree. this to me also implies it belongs in spark core, not sql
>>=20
>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> michaelmalak@yahoo.com.invalid> wrote:
>>=20
>>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 =
Bay Area
>>> Spark Meetup YouTube contained a wealth of background information on =
this
>>> idea (mostly from Patrick and Reynold :-).
>>>=20
>>> https://www.youtube.com/watch?v=3DYWppYPWznSQ
>>>=20
>>> ________________________________
>>> From: Patrick Wendell <pwendell@gmail.com>
>>> To: Reynold Xin <rxin@databricks.com>
>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>>> Sent: Monday, January 26, 2015 4:01 PM
>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>=20
>>>=20
>>> One thing potentially not clear from this e-mail, there will be a =
1:1
>>> correspondence where you can get an RDD to/from a DataFrame.
>>>=20
>>>=20
>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> =
wrote:
>>>> Hi,
>>>>=20
>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and =
wanted to
>>>> get the community's opinion.
>>>>=20
>>>> The context is that SchemaRDD is becoming a common data format used =
for
>>>> bringing data into Spark from external systems, and used for =
various
>>>> components of Spark, e.g. MLlib's new pipeline API. We also expect =
more
>>> and
>>>> more users to be programming directly against SchemaRDD API rather =
than
>>> the
>>>> core RDD API. SchemaRDD, through its less commonly used DSL =
originally
>>>> designed for writing test cases, always has the data-frame like =
API. In
>>>> 1.3, we are redesigning the API to make the API usable for end =
users.
>>>>=20
>>>>=20
>>>> There are two motivations for the renaming:
>>>>=20
>>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>>>>=20
>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore =
(even
>>>> though it would contain some RDD functions like map, flatMap, etc), =
and
>>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>>> Instead.
>>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>>>>=20
>>>>=20
>>>> My understanding is that very few users program directly against =
the
>>>> SchemaRDD API at the moment, because they are not well documented.
>>> However,
>>>> oo maintain backward compatibility, we can create a type alias =
DataFrame
>>>> that is still named SchemaRDD. This will maintain source =
compatibility
>>> for
>>>> Scala. That said, we will have to update all existing materials to =
use
>>>> DataFrame rather than SchemaRDD.
>>>=20
>>> =
---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>> =
---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>>=20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 01:33:46 2015
Return-Path: <dev-return-11282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A6F931788A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 01:33:46 +0000 (UTC)
Received: (qmail 14032 invoked by uid 500); 27 Jan 2015 01:33:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13962 invoked by uid 500); 27 Jan 2015 01:33:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13948 invoked by uid 99); 27 Jan 2015 01:33:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:33:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:33:20 +0000
Received: by mail-pa0-f44.google.com with SMTP id rd3so15052173pab.3
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 17:31:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=6+zCi29uJRSKjGqaT3B6ePBCk6fNSo+GNi0MIzO1BM4=;
        b=mtGfIkYcZ8PjnVMbxlH9cK2bOKOC7wi9ikgGlFFw5QIbnSbphSK159AWmBVHPmcNiC
         IJp4+zfcIN3UwQ4N2mBs9YlHYKnmYgyzIsBa9+B8mSDMcTjRzIR5xy0BvVqZUIlJWyGp
         ny7vC6SgKk9NkfkIE4Cm23sECvut8yHDTGZKua8ZgFVSIYWg9cjDrsGWrFhmxiArQIxi
         V5BdEijYLQEdWnl7IqiQurBNu/vcoDim3jNEvAljkUcgl9ACBcbe12KtUgkdgonzdWG+
         zK7DCRcqd7ILmHFXhK0tLZSTzjr/zRWiUoUH52cvMFELO/XaqeL50JHmyuFCsDPlMDNt
         ZXbA==
X-Received: by 10.66.234.2 with SMTP id ua2mr39927142pac.4.1422322263219;
        Mon, 26 Jan 2015 17:31:03 -0800 (PST)
Received: from [192.168.1.100] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id rn15sm11052649pab.10.2015.01.26.17.31.01
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 26 Jan 2015 17:31:02 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: renaming SchemaRDD -> DataFrame
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
Date: Mon, 26 Jan 2015 17:31:00 -0800
Cc: Michael Malak <michaelmalak@yahoo.com>,
 Patrick Wendell <pwendell@gmail.com>,
 Reynold Xin <rxin@databricks.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com> <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com> <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
To: Koert Kuipers <koert@tresata.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

While it might be possible to move this concept to Spark Core long-term, =
supporting structured data efficiently does require quite a bit of the =
infrastructure in Spark SQL, such as query planning and columnar =
storage. The intent of Spark SQL though is to be more than a SQL server =
-- it's meant to be a library for manipulating structured data. Since =
this is possible to build over the core API, it's pretty natural to =
organize it that way, same as Spark Streaming is a library.

Matei

> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
>=20
> "The context is that SchemaRDD is becoming a common data format used =
for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API."
>=20
> i agree. this to me also implies it belongs in spark core, not sql
>=20
> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> michaelmalak@yahoo.com.invalid> wrote:
>=20
>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay =
Area
>> Spark Meetup YouTube contained a wealth of background information on =
this
>> idea (mostly from Patrick and Reynold :-).
>>=20
>> https://www.youtube.com/watch?v=3DYWppYPWznSQ
>>=20
>> ________________________________
>> From: Patrick Wendell <pwendell@gmail.com>
>> To: Reynold Xin <rxin@databricks.com>
>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> Sent: Monday, January 26, 2015 4:01 PM
>> Subject: Re: renaming SchemaRDD -> DataFrame
>>=20
>>=20
>> One thing potentially not clear from this e-mail, there will be a 1:1
>> correspondence where you can get an RDD to/from a DataFrame.
>>=20
>>=20
>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> =
wrote:
>>> Hi,
>>>=20
>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and =
wanted to
>>> get the community's opinion.
>>>=20
>>> The context is that SchemaRDD is becoming a common data format used =
for
>>> bringing data into Spark from external systems, and used for various
>>> components of Spark, e.g. MLlib's new pipeline API. We also expect =
more
>> and
>>> more users to be programming directly against SchemaRDD API rather =
than
>> the
>>> core RDD API. SchemaRDD, through its less commonly used DSL =
originally
>>> designed for writing test cases, always has the data-frame like API. =
In
>>> 1.3, we are redesigning the API to make the API usable for end =
users.
>>>=20
>>>=20
>>> There are two motivations for the renaming:
>>>=20
>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>>>=20
>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore =
(even
>>> though it would contain some RDD functions like map, flatMap, etc), =
and
>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>> Instead.
>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>>>=20
>>>=20
>>> My understanding is that very few users program directly against the
>>> SchemaRDD API at the moment, because they are not well documented.
>> However,
>>> oo maintain backward compatibility, we can create a type alias =
DataFrame
>>> that is still named SchemaRDD. This will maintain source =
compatibility
>> for
>>> Scala. That said, we will have to update all existing materials to =
use
>>> DataFrame rather than SchemaRDD.
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 01:47:21 2015
Return-Path: <dev-return-11283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BC5C178E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 01:47:21 +0000 (UTC)
Received: (qmail 38803 invoked by uid 500); 27 Jan 2015 01:47:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38742 invoked by uid 500); 27 Jan 2015 01:47:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38731 invoked by uid 99); 27 Jan 2015 01:47:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:47:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 01:46:54 +0000
Received: by mail-ie0-f172.google.com with SMTP id rd18so12439687iec.3
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 17:46:52 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=9laLI48PubSBMTFNdcdQkAM/yB0cAnPj26A7+LjU7MY=;
        b=LcGliZ3/tfgl0KhBCB8crJ5G54kgIMtkYFI5/jc32OTPqgp5yzBvz2UToB36DBQjlS
         N+5rs06dRXtutfF8bZ30TGrlc4yVFAj0NSzu40T4nYqyDsUMwvuOLO+D2s+urba5wLI+
         d1EJPPCK4V0C7jadgl3z6YJiu+SHbaXyoM2Hb5Y3dSMsltPzQfJg492kJLXKjq9rAln2
         3o3HjtusfFx3n5GzfeWdhUsAln3PhT9aNdtXLnpn3+NlN5S408kPnCY4i03ZK7R7R4ec
         pwczRWRPTk5wJe46Km0BjWVVasWY5iN3RVKm7lZrK0K6iLW0LbowtdxzIYEiROBuBgQW
         hD2A==
X-Gm-Message-State: ALoCoQlGiblHpBE+gi3CegMLMFgRF6aCZEsVfavaQWzRFomwQ5H5Avm/iWW2FCoRKj0cju2zD3AW
MIME-Version: 1.0
X-Received: by 10.42.25.144 with SMTP id a16mr471540icc.66.1422323212349; Mon,
 26 Jan 2015 17:46:52 -0800 (PST)
Received: by 10.36.28.13 with HTTP; Mon, 26 Jan 2015 17:46:52 -0800 (PST)
In-Reply-To: <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
Date: Mon, 26 Jan 2015 17:46:52 -0800
Message-ID: <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Koert Kuipers <koert@tresata.com>, Michael Malak <michaelmalak@yahoo.com>, 
	Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303ea582d40fea050d986da7
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303ea582d40fea050d986da7
Content-Type: text/plain; charset=UTF-8

Both SchemaRDD and DataFrame sound fine to me, though I like the former
slightly better because it's more descriptive.

Even if SchemaRDD's needs to rely on Spark SQL under the covers, it would
be more clear from a user-facing perspective to at least choose a package
name for it that omits "sql".

I would also be in favor of adding a separate Spark Schema module for Spark
SQL to rely on, but I imagine that might be too large a change at this
point?

-Sandy

On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> (Actually when we designed Spark SQL we thought of giving it another name,
> like Spark Schema, but we decided to stick with SQL since that was the most
> obvious use case to many users.)
>
> Matei
>
> > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >
> > While it might be possible to move this concept to Spark Core long-term,
> supporting structured data efficiently does require quite a bit of the
> infrastructure in Spark SQL, such as query planning and columnar storage.
> The intent of Spark SQL though is to be more than a SQL server -- it's
> meant to be a library for manipulating structured data. Since this is
> possible to build over the core API, it's pretty natural to organize it
> that way, same as Spark Streaming is a library.
> >
> > Matei
> >
> >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> >>
> >> "The context is that SchemaRDD is becoming a common data format used for
> >> bringing data into Spark from external systems, and used for various
> >> components of Spark, e.g. MLlib's new pipeline API."
> >>
> >> i agree. this to me also implies it belongs in spark core, not sql
> >>
> >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> michaelmalak@yahoo.com.invalid> wrote:
> >>
> >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> Area
> >>> Spark Meetup YouTube contained a wealth of background information on
> this
> >>> idea (mostly from Patrick and Reynold :-).
> >>>
> >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >>>
> >>> ________________________________
> >>> From: Patrick Wendell <pwendell@gmail.com>
> >>> To: Reynold Xin <rxin@databricks.com>
> >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >>> Sent: Monday, January 26, 2015 4:01 PM
> >>> Subject: Re: renaming SchemaRDD -> DataFrame
> >>>
> >>>
> >>> One thing potentially not clear from this e-mail, there will be a 1:1
> >>> correspondence where you can get an RDD to/from a DataFrame.
> >>>
> >>>
> >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>>> Hi,
> >>>>
> >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
> to
> >>>> get the community's opinion.
> >>>>
> >>>> The context is that SchemaRDD is becoming a common data format used
> for
> >>>> bringing data into Spark from external systems, and used for various
> >>>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> more
> >>> and
> >>>> more users to be programming directly against SchemaRDD API rather
> than
> >>> the
> >>>> core RDD API. SchemaRDD, through its less commonly used DSL originally
> >>>> designed for writing test cases, always has the data-frame like API.
> In
> >>>> 1.3, we are redesigning the API to make the API usable for end users.
> >>>>
> >>>>
> >>>> There are two motivations for the renaming:
> >>>>
> >>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> >>>>
> >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> (even
> >>>> though it would contain some RDD functions like map, flatMap, etc),
> and
> >>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> >>> Instead.
> >>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> >>>>
> >>>>
> >>>> My understanding is that very few users program directly against the
> >>>> SchemaRDD API at the moment, because they are not well documented.
> >>> However,
> >>>> oo maintain backward compatibility, we can create a type alias
> DataFrame
> >>>> that is still named SchemaRDD. This will maintain source compatibility
> >>> for
> >>>> Scala. That said, we will have to update all existing materials to use
> >>>> DataFrame rather than SchemaRDD.
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>>
> >
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--20cf303ea582d40fea050d986da7--

From dev-return-11284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 02:03:20 2015
Return-Path: <dev-return-11284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D98817960
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 02:03:20 +0000 (UTC)
Received: (qmail 72097 invoked by uid 500); 27 Jan 2015 02:03:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72020 invoked by uid 500); 27 Jan 2015 02:03:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72008 invoked by uid 99); 27 Jan 2015 02:03:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 02:03:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kushal.datta@gmail.com designates 209.85.223.179 as permitted sender)
Received: from [209.85.223.179] (HELO mail-ie0-f179.google.com) (209.85.223.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 02:03:14 +0000
Received: by mail-ie0-f179.google.com with SMTP id x19so12540331ier.10
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 18:02:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=bxjnqYZI0xHvyOOvByP9WTxT9vrx5MJX7ywPma0Fnyo=;
        b=uv0zsu5RjoEKcDUl1fQv78y2tSnMsaw2yktOgWwc3ZUtk1djRejvdAsk5+dFWOpuIp
         /npRh/BtOiHy5KFSKGCjjjT6rmmcAySsUNjoy8qo1YISQi0C6yN1Y07QrelFBbJmDl5K
         8abY2E5tZWuS2IYbMVg02XlkEPAjaXQXCDw2MqusuZ09HHwP9SqLUeHE5GceW3rVJ3Nu
         0HxiotBBfgdBvtHiWft96Je6zdOV/UbA89YT8p0290gdxANQZk+KlnUgIuI998NvkKJW
         u10K+w5mq3+DhbPGCl5XNvz72VuPdQDQgFTPnpPc4kn3luLTUN68+nzhCBMVcr8qtIAP
         2P0g==
MIME-Version: 1.0
X-Received: by 10.43.144.9 with SMTP id jo9mr480339icc.74.1422324173979; Mon,
 26 Jan 2015 18:02:53 -0800 (PST)
Received: by 10.64.64.199 with HTTP; Mon, 26 Jan 2015 18:02:53 -0800 (PST)
Received: by 10.64.64.199 with HTTP; Mon, 26 Jan 2015 18:02:53 -0800 (PST)
In-Reply-To: <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
Date: Mon, 26 Jan 2015 18:02:53 -0800
Message-ID: <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Kushal Datta <kushal.datta@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Patrick Wendell <pwendell@gmail.com>, 
	Michael Malak <michaelmalak@yahoo.com>, Koert Kuipers <koert@tresata.com>
Content-Type: multipart/alternative; boundary=001a11c2fa0e254519050d98a713
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2fa0e254519050d98a713
Content-Type: text/plain; charset=UTF-8

I want to address the issue that Matei raised about the heavy lifting
required for a full SQL support. It is amazing that even after 30 years of
research there is not a single good open source columnar database like
Vertica. There is a column store option in MySQL, but it is not nearly as
sophisticated as Vertica or MonetDB. But there's a true need for such a
system. I wonder why so and it's high time to change that.
On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com> wrote:

> Both SchemaRDD and DataFrame sound fine to me, though I like the former
> slightly better because it's more descriptive.
>
> Even if SchemaRDD's needs to rely on Spark SQL under the covers, it would
> be more clear from a user-facing perspective to at least choose a package
> name for it that omits "sql".
>
> I would also be in favor of adding a separate Spark Schema module for Spark
> SQL to rely on, but I imagine that might be too large a change at this
> point?
>
> -Sandy
>
> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
> > (Actually when we designed Spark SQL we thought of giving it another
> name,
> > like Spark Schema, but we decided to stick with SQL since that was the
> most
> > obvious use case to many users.)
> >
> > Matei
> >
> > > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> > >
> > > While it might be possible to move this concept to Spark Core
> long-term,
> > supporting structured data efficiently does require quite a bit of the
> > infrastructure in Spark SQL, such as query planning and columnar storage.
> > The intent of Spark SQL though is to be more than a SQL server -- it's
> > meant to be a library for manipulating structured data. Since this is
> > possible to build over the core API, it's pretty natural to organize it
> > that way, same as Spark Streaming is a library.
> > >
> > > Matei
> > >
> > >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> > >>
> > >> "The context is that SchemaRDD is becoming a common data format used
> for
> > >> bringing data into Spark from external systems, and used for various
> > >> components of Spark, e.g. MLlib's new pipeline API."
> > >>
> > >> i agree. this to me also implies it belongs in spark core, not sql
> > >>
> > >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > >> michaelmalak@yahoo.com.invalid> wrote:
> > >>
> > >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> > Area
> > >>> Spark Meetup YouTube contained a wealth of background information on
> > this
> > >>> idea (mostly from Patrick and Reynold :-).
> > >>>
> > >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> > >>>
> > >>> ________________________________
> > >>> From: Patrick Wendell <pwendell@gmail.com>
> > >>> To: Reynold Xin <rxin@databricks.com>
> > >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> > >>> Sent: Monday, January 26, 2015 4:01 PM
> > >>> Subject: Re: renaming SchemaRDD -> DataFrame
> > >>>
> > >>>
> > >>> One thing potentially not clear from this e-mail, there will be a 1:1
> > >>> correspondence where you can get an RDD to/from a DataFrame.
> > >>>
> > >>>
> > >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >>>> Hi,
> > >>>>
> > >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
> wanted
> > to
> > >>>> get the community's opinion.
> > >>>>
> > >>>> The context is that SchemaRDD is becoming a common data format used
> > for
> > >>>> bringing data into Spark from external systems, and used for various
> > >>>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> > more
> > >>> and
> > >>>> more users to be programming directly against SchemaRDD API rather
> > than
> > >>> the
> > >>>> core RDD API. SchemaRDD, through its less commonly used DSL
> originally
> > >>>> designed for writing test cases, always has the data-frame like API.
> > In
> > >>>> 1.3, we are redesigning the API to make the API usable for end
> users.
> > >>>>
> > >>>>
> > >>>> There are two motivations for the renaming:
> > >>>>
> > >>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> > >>>>
> > >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> > (even
> > >>>> though it would contain some RDD functions like map, flatMap, etc),
> > and
> > >>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> > >>> Instead.
> > >>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> > >>>>
> > >>>>
> > >>>> My understanding is that very few users program directly against the
> > >>>> SchemaRDD API at the moment, because they are not well documented.
> > >>> However,
> > >>>> oo maintain backward compatibility, we can create a type alias
> > DataFrame
> > >>>> that is still named SchemaRDD. This will maintain source
> compatibility
> > >>> for
> > >>>> Scala. That said, we will have to update all existing materials to
> use
> > >>>> DataFrame rather than SchemaRDD.
> > >>>
> > >>> ---------------------------------------------------------------------
> > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > >>>
> > >>> ---------------------------------------------------------------------
> > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > >>>
> > >>>
> > >
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a11c2fa0e254519050d98a713--

From dev-return-11285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 06:08:29 2015
Return-Path: <dev-return-11285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC55D17E92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 06:08:28 +0000 (UTC)
Received: (qmail 97063 invoked by uid 500); 27 Jan 2015 06:08:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96992 invoked by uid 500); 27 Jan 2015 06:08:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96981 invoked by uid 99); 27 Jan 2015 06:08:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 06:08:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 06:08:22 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so10429254qgd.1
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 22:05:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=lw+HNy9TVh2xLqA4sCTPHqddJg15c82PQnnVceQT/Eg=;
        b=YZ0fGLmg48wlVEBRd+TbgFWjI97UleZNIqfsV45boN+lHc7RYsmoZ7P9lhrmNPtOsW
         XpMx/WMrnmeQ6/p2cHOMTTKWgqXRsKdV30jtKNIEp9yKnBxtINyiPni4p6Cfex5+xWUo
         UNL8rMbtYYlTmXyGKDWlPorH0x5JkSfFCyDRs9oOzSzpcDZTKiHNfHQEFuqP+GfWrhAz
         vShYofE5sQie1jCM4IymHJkffJBDCGJlC2NlH5SNMeTMxDi+znr9Akc23e+LXadaC3TY
         1YVBRgeHSE1it+h1y/wC9iXHnVEbTPnK35KJFb2CYUrfk+QLud/aV2uxEsJvAVTmXXjw
         TPrg==
X-Gm-Message-State: ALoCoQnP7eZ9mpegjJYivyWePTPzZ6X0I2qp8Kwia387n2BI+culLHw0ZpSW4M1n7eGGrv+wFAM2
X-Received: by 10.224.29.209 with SMTP id r17mr4392697qac.104.1422338726117;
 Mon, 26 Jan 2015 22:05:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Mon, 26 Jan 2015 22:05:05 -0800 (PST)
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 26 Jan 2015 22:05:05 -0800
Message-ID: <CAPh_B=YuGbG-juxL4vdXiX05Y6+G0HQRnfVHFi2AS17rz_bYmQ@mail.gmail.com>
Subject: talk on interface design
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1792855ab3050d9c0a82
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1792855ab3050d9c0a82
Content-Type: text/plain; charset=UTF-8

Hi all,

In Spark, we have done reasonable well historically in interface and API
design, especially compared with some other Big Data systems. However, we
have also made mistakes along the way. I want to share a talk I gave about
interface design at Databricks' internal retreat.

https://speakerdeck.com/rxin/interface-design-for-spark-community

Interface design is a vital part of Spark becoming a long-term sustainable,
thriving framework. Good interfaces can be the project's biggest asset,
while bad interfaces can be the worst technical debt. As the project scales
bigger and bigger, the community is expanding and we are getting a wider
range of contributors that have not thought about this as their everyday
development experience outside Spark.

It is part-art part-science and in some sense acquired taste. However, I
think there are common issues that can be spotted easily, and common
principles that can address a lot of the low hanging fruits. Through this
presentation, I hope to bring to everybody's attention the issue of
interface design and encourage everybody to think hard about interface
design in their contributions.

--047d7bdc1792855ab3050d9c0a82--

From dev-return-11286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 06:24:42 2015
Return-Path: <dev-return-11286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5619217EFF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 06:24:42 +0000 (UTC)
Received: (qmail 20220 invoked by uid 500); 27 Jan 2015 06:24:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20142 invoked by uid 500); 27 Jan 2015 06:24:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20131 invoked by uid 99); 27 Jan 2015 06:24:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 06:24:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 06:24:36 +0000
Received: by mail-oi0-f47.google.com with SMTP id a141so10978210oig.6
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 22:23:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=11vGphMkQnFXFypFoTieY0w6373p9bAGKKFzE71/fEo=;
        b=RB0eshg7W4Z940zQOSPcWH8eAYoZ1k8hNdWTxR8oUzgEzgP7CrPzAkJxI1jTJCWDIC
         xqUBuxMx8OLrDzBPx0k8qdLerY3bibXQk5/hocRpOfjWgjNZ1a36Peokl/RbMgOfNqHq
         hRuaxQbtVaSWUVdTILW2f0i4TVFs+FEjEq8rW498dO1ZjnMQa5OMUIVNzZwD9ffPNTvb
         fbEILuf4rKm5f9Wccrt86lvDVpVV6of9OauG5ib1mm7v+rgMQJj1vm75zub5VZuY7/F7
         xL8XjDK3ERYSvnWfxWtBRxS8bphDdFWqjuMRk6ouzTsrU1UH14fQ5I/vH8pWm9eOPtrC
         34ug==
X-Gm-Message-State: ALoCoQmCVj8TJUo39xUbUspyeKVqda2UJcDrVHjF0BgUN5PjGBcD8ELzm6wC+DN0DbNa0fGT1q93
X-Received: by 10.202.56.133 with SMTP id f127mr781613oia.101.1422339835693;
        Mon, 26 Jan 2015 22:23:55 -0800 (PST)
Received: from mail-ob0-f177.google.com (mail-ob0-f177.google.com. [209.85.214.177])
        by mx.google.com with ESMTPSA id h9sm220397obw.1.2015.01.26.22.23.54
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 26 Jan 2015 22:23:54 -0800 (PST)
Received: by mail-ob0-f177.google.com with SMTP id wo20so1147500obc.8
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 22:23:53 -0800 (PST)
X-Received: by 10.202.189.133 with SMTP id n127mr14388995oif.23.1422339833744;
 Mon, 26 Jan 2015 22:23:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.182.182.7 with HTTP; Mon, 26 Jan 2015 22:23:33 -0800 (PST)
In-Reply-To: <CAPh_B=YuGbG-juxL4vdXiX05Y6+G0HQRnfVHFi2AS17rz_bYmQ@mail.gmail.com>
References: <CAPh_B=YuGbG-juxL4vdXiX05Y6+G0HQRnfVHFi2AS17rz_bYmQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 27 Jan 2015 01:23:33 -0500
Message-ID: <CA+-p3AFRMmo0SbS4qgfbRzUi=ph6U9qxakSLpq3Ksm+Gxy=D0g@mail.gmail.com>
Subject: Re: talk on interface design
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113dd4648a55c1050d9c4cb2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113dd4648a55c1050d9c4cb2
Content-Type: text/plain; charset=UTF-8

In addition to the references you have at the end of the presentation,
there's a great set of practical examples based on the learnings from Qt
posted here: http://www21.in.tum.de/~blanchet/api-design.pdf

Chapter 4's way of showing a principle and then an example from Qt is
particularly instructional.

On Tue, Jan 27, 2015 at 1:05 AM, Reynold Xin <rxin@databricks.com> wrote:

> Hi all,
>
> In Spark, we have done reasonable well historically in interface and API
> design, especially compared with some other Big Data systems. However, we
> have also made mistakes along the way. I want to share a talk I gave about
> interface design at Databricks' internal retreat.
>
> https://speakerdeck.com/rxin/interface-design-for-spark-community
>
> Interface design is a vital part of Spark becoming a long-term sustainable,
> thriving framework. Good interfaces can be the project's biggest asset,
> while bad interfaces can be the worst technical debt. As the project scales
> bigger and bigger, the community is expanding and we are getting a wider
> range of contributors that have not thought about this as their everyday
> development experience outside Spark.
>
> It is part-art part-science and in some sense acquired taste. However, I
> think there are common issues that can be spotted easily, and common
> principles that can address a lot of the low hanging fruits. Through this
> presentation, I hope to bring to everybody's attention the issue of
> interface design and encourage everybody to think hard about interface
> design in their contributions.
>

--001a113dd4648a55c1050d9c4cb2--

From dev-return-11287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 07:04:05 2015
Return-Path: <dev-return-11287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1392710034
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 07:04:05 +0000 (UTC)
Received: (qmail 88629 invoked by uid 500); 27 Jan 2015 07:04:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88555 invoked by uid 500); 27 Jan 2015 07:04:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88532 invoked by uid 99); 27 Jan 2015 07:04:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 07:04:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 07:03:37 +0000
Received: by mail-ob0-f169.google.com with SMTP id va8so12033923obc.0
        for <dev@spark.apache.org>; Mon, 26 Jan 2015 23:02:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=lGN23zKi9IgDcpMW9XSQpZ/6JC9r0PGONX0mpm3kOjE=;
        b=LrdC9k9qu9+zINzvphr6q5s1EtUajNTTFsvfXSRTjxtkaV2Adiip+FQTyEVH67hAPW
         Z8LKhxV7f9qpOnRUcuxlzprU86MHMmRekjhOWr0d4m8ILagFFPt24XKb1fAp6p1cM4it
         GasvFWFpqAWIWYfntXimiGRm7e9g++0IrMjVcjuu0YV63Gi0LwKZJUy9Dsj/IPjRprRK
         63JTMGaNwTn5xjufFqKg01KvEfPaoL6xcn3MdhHdvpdXBOzOCtrZ4dyaBBP85O1Mp3hB
         UpnBHheFDJJdTjuKFWY0H/H3114otHJZWpNyhyZqMTygOxoWueSTbQQmlcNB2MQpH4UR
         2a6g==
MIME-Version: 1.0
X-Received: by 10.60.176.34 with SMTP id cf2mr15012118oec.52.1422342170870;
 Mon, 26 Jan 2015 23:02:50 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Mon, 26 Jan 2015 23:02:50 -0800 (PST)
Date: Mon, 26 Jan 2015 23:02:50 -0800
Message-ID: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.2.1!

The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1061/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.2.1!

The vote is open until Friday, January 30, at 07:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.1
[ ] -1 Do not release this package because ...

For a list of fixes in this release, see http://s.apache.org/Mpn.

To learn more about Apache Spark, please see
http://spark.apache.org/

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 08:05:27 2015
Return-Path: <dev-return-11288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D479D10292
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 08:05:27 +0000 (UTC)
Received: (qmail 80761 invoked by uid 500); 27 Jan 2015 08:05:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80690 invoked by uid 500); 27 Jan 2015 08:05:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80679 invoked by uid 99); 27 Jan 2015 08:05:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 08:05:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 08:05:02 +0000
Received: by mail-qg0-f50.google.com with SMTP id f51so10626179qge.9
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 00:04:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ghZTZzh/GegyiWaKyZZ84etVTMvJZ+lVOsQZgtruO9c=;
        b=GdclcsvS7ImInpUIaZMqClx0/+ZHKoE49O0zGZSduN+5gqed2Nkia/IOjiKxofnbGw
         jUhjNlWCK0NzvvqBkBS+iAD3BYxnKt5IHP0R2JjPXASXEcAk7SL5ZsTaCnDwBV0bj3HI
         HFf9NKEnopi05SSlJkznnlosjdvMPmTktdxrrmDQABLnMLyzT6tEfCtAatfZqtQlpOWF
         k57ArQKz+PNcjyWGe5a9e7mRL/GQh9IPylXJT4QzVfEfQh3BBZHNdgLBkOIzuvfqhR/w
         c/m02Jkd6IyeR66Qf/MCkFsForYPXT/MmZbiWFGyFbRmsyJEeegk66X+Aq8Ta0bp+qdG
         CL6Q==
X-Gm-Message-State: ALoCoQl74m2+4XwCjSxo2Sa92A3NzYDqovJhBrMG0drTmiBERp50XpHccZz3aJyV3gJZemULmSOt
X-Received: by 10.224.129.202 with SMTP id p10mr39984qas.54.1422345879843;
 Tue, 27 Jan 2015 00:04:39 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 27 Jan 2015 00:04:19 -0800 (PST)
In-Reply-To: <CA+-p3AFRMmo0SbS4qgfbRzUi=ph6U9qxakSLpq3Ksm+Gxy=D0g@mail.gmail.com>
References: <CAPh_B=YuGbG-juxL4vdXiX05Y6+G0HQRnfVHFi2AS17rz_bYmQ@mail.gmail.com>
 <CA+-p3AFRMmo0SbS4qgfbRzUi=ph6U9qxakSLpq3Ksm+Gxy=D0g@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 Jan 2015 00:04:19 -0800
Message-ID: <CAPh_B=aNrq7tka0svy3kv4iU9pOmk+bhyoLZqWpGCx84bVkbEQ@mail.gmail.com>
Subject: Re: talk on interface design
To: Andrew Ash <andrew@andrewash.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c124ea8be0050d9db44a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c124ea8be0050d9db44a
Content-Type: text/plain; charset=UTF-8

Thanks, Andrew. That's great material.


On Mon, Jan 26, 2015 at 10:23 PM, Andrew Ash <andrew@andrewash.com> wrote:

> In addition to the references you have at the end of the presentation,
> there's a great set of practical examples based on the learnings from Qt
> posted here: http://www21.in.tum.de/~blanchet/api-design.pdf
>
> Chapter 4's way of showing a principle and then an example from Qt is
> particularly instructional.
>
> On Tue, Jan 27, 2015 at 1:05 AM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Hi all,
>>
>> In Spark, we have done reasonable well historically in interface and API
>> design, especially compared with some other Big Data systems. However, we
>> have also made mistakes along the way. I want to share a talk I gave about
>> interface design at Databricks' internal retreat.
>>
>> https://speakerdeck.com/rxin/interface-design-for-spark-community
>>
>> Interface design is a vital part of Spark becoming a long-term
>> sustainable,
>> thriving framework. Good interfaces can be the project's biggest asset,
>> while bad interfaces can be the worst technical debt. As the project
>> scales
>> bigger and bigger, the community is expanding and we are getting a wider
>> range of contributors that have not thought about this as their everyday
>> development experience outside Spark.
>>
>> It is part-art part-science and in some sense acquired taste. However, I
>> think there are common issues that can be spotted easily, and common
>> principles that can address a lot of the low hanging fruits. Through this
>> presentation, I hope to bring to everybody's attention the issue of
>> interface design and encourage everybody to think hard about interface
>> design in their contributions.
>>
>
>

--001a11c2c124ea8be0050d9db44a--

From dev-return-11289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 11:18:36 2015
Return-Path: <dev-return-11289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7C4610CEA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 11:18:36 +0000 (UTC)
Received: (qmail 21477 invoked by uid 500); 27 Jan 2015 11:18:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21401 invoked by uid 500); 27 Jan 2015 11:18:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21157 invoked by uid 99); 27 Jan 2015 11:18:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 11:18:33 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of angel.alvarez.pascua@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 11:18:08 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 0038F11D83DD
	for <dev@spark.apache.org>; Tue, 27 Jan 2015 03:18:06 -0800 (PST)
Date: Tue, 27 Jan 2015 04:18:06 -0700 (MST)
From: angel__ <angel.alvarez.pascua@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422357486167-10285.post@n3.nabble.com>
In-Reply-To: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
Subject: Re: Use mvn to build Spark 1.2.0  failed
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I had that problem when I tried to build Spark 1.2. I don't exactly know what
is causing it, but I guess it might have something to do with user
permissions.

I could finally fix this by building Spark as "root" user (now I'm dealing
with another problem, but ...that's another story...)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Use-mvn-to-build-Spark-1-2-0-failed-tp9876p10285.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 11:25:41 2015
Return-Path: <dev-return-11290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A985B10D1C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 11:25:41 +0000 (UTC)
Received: (qmail 31010 invoked by uid 500); 27 Jan 2015 11:25:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30933 invoked by uid 500); 27 Jan 2015 11:25:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30922 invoked by uid 99); 27 Jan 2015 11:25:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 11:25:40 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 11:25:15 +0000
Received: by mail-wg0-f42.google.com with SMTP id x13so14221163wgg.1
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 03:23:44 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=h2v2CHVfzF3QO4gc73Rcx2P4TuzZ9oiH3/d1cDRuy8I=;
        b=cysauN2duYqfLTLe5z6VHN3qCb04ri0iLBhFivaZbzQFilDYxL7A6NpTzYPvur/XpR
         HRkcwSofwtx+fNutCk5c++G6MFUBavu06Q3nMOL5QF8HKtpcr7BokG/pINRzUgWroeIj
         M/PF43KfJhQsd92ubajZgtixQ0cITmspC99Z8RhOjwIB8wHpqYLHm9pv73WyTerCCR1i
         2RrktNy1bKJBnXFKdev6GF0asXcixXEtSxAzZv3Qw7vXMpm+8lrpwJX0cJSedk3OCpmy
         xqVKBZnJJz8yy46t9gAXIiQeeLEFdvjgT8ono6d5UBnvwg2m8aYnsUpfCOWj7+QaViDD
         W91w==
X-Gm-Message-State: ALoCoQn3EANvfa17uI6PLFpaGSKLju4I+DDUFcCvr9wgsymXmTw/PjKDya9AOeAfCOoEhPA8tlr1
MIME-Version: 1.0
X-Received: by 10.194.191.227 with SMTP id hb3mr1521745wjc.79.1422357824185;
 Tue, 27 Jan 2015 03:23:44 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 27 Jan 2015 03:23:43 -0800 (PST)
Received: by 10.27.83.76 with HTTP; Tue, 27 Jan 2015 03:23:43 -0800 (PST)
In-Reply-To: <1422357486167-10285.post@n3.nabble.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
	<1422357486167-10285.post@n3.nabble.com>
Date: Tue, 27 Jan 2015 11:23:43 +0000
Message-ID: <CAMAsSd+zO+_QyvwRU8pfeFKBmDcutDTK23qeWXKVPXUyZa79dA@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
From: Sean Owen <sowen@cloudera.com>
To: angel__ <angel.alvarez.pascua@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba97f2adac086050da07c58
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba97f2adac086050da07c58
Content-Type: text/plain; charset=UTF-8

You certainly do not need yo build Spark as root. It might clumsily
overcome a permissions problem in your local env but probably causes other
problems.
On Jan 27, 2015 11:18 AM, "angel__" <angel.alvarez.pascua@gmail.com> wrote:

> I had that problem when I tried to build Spark 1.2. I don't exactly know
> what
> is causing it, but I guess it might have something to do with user
> permissions.
>
> I could finally fix this by building Spark as "root" user (now I'm dealing
> with another problem, but ...that's another story...)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Use-mvn-to-build-Spark-1-2-0-failed-tp9876p10285.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7ba97f2adac086050da07c58--

From dev-return-11291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 13:02:08 2015
Return-Path: <dev-return-11291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 79AF51733B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 13:02:08 +0000 (UTC)
Received: (qmail 21968 invoked by uid 500); 27 Jan 2015 13:02:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21899 invoked by uid 500); 27 Jan 2015 13:02:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21887 invoked by uid 99); 27 Jan 2015 13:02:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 13:02:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 13:02:03 +0000
Received: by mail-wg0-f51.google.com with SMTP id k14so14583898wgh.10
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 05:00:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=qzczKh/T8VtvA6lAHvtx4rFcoTeUZ0FcBqSORgmvd2M=;
        b=VULxssQki7k51PwAUWNF7cGbPqVflYVtHgYZ06fQGacssfsBod2qs5FSU1RqZy3G7l
         BShf2aXVs0yfNCkOs6DuwNfTp4oREilzHWbft/ZURZpidQrmsblbyTfECVfsTN9il4Xl
         rBUYB2QSMHLyjpDZsfshHE+wWXv4Anizk75VfYz9gdaWsRKBjLwTjhAB2u44xlGnp5gH
         anyHNv3nbgVRn2JGMLiTKCczY7eHTKqF+R32j8Jw9WXmc/P43DQr0o8R5OFX5JYL92JV
         k5sUhTDU8PgZHKBB/Mior/7ydf2mSOxAM24g13bKu6G7aao7mC9UqmE735sWm/fR14w0
         6YAA==
X-Gm-Message-State: ALoCoQljXWMXjkJp39z+atRg1YmRIztuxGxvk1gh12P17wRJU6/e9uk0cRFI2eRShF362eN8orq8
X-Received: by 10.180.20.177 with SMTP id o17mr5809738wie.64.1422363657051;
 Tue, 27 Jan 2015 05:00:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Tue, 27 Jan 2015 05:00:36 -0800 (PST)
In-Reply-To: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 27 Jan 2015 13:00:36 +0000
Message-ID: <CAMAsSdLc+jJmT8LMid06dSY3AuSqWMp7UiTqDSQ1smeovpVd6Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I think there are several signing / hash issues that should be fixed
before this release.

Hashes:

http://issues.apache.org/jira/browse/SPARK-5308
https://github.com/apache/spark/pull/4161

The hashes here are correct, but have two issues:

As noted in the JIRA, the format of the hash file is "nonstandard" --
at least, doesn't match what Maven outputs, and apparently which tools
like Leiningen expect, which is just the hash with no file name or
spaces. There are two ways to fix that: different command-line tools
(see PR), or, just ask Maven to generate these hashes (a different,
easy PR).

However, is the script I modified above used to generate these hashes?
It's generating SHA1 sums, but the output in this release candidate
has (correct) SHA512 sums.

This may be more than a nuisance, since last time for some reason
Maven Central did not register the project hashes.

http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.2.0%7Cjar
does not show them but they exist:
http://www.us.apache.org/dist/spark/spark-1.2.0/

It may add up to a problem worth rooting out before this release.


Signing:

As noted in https://issues.apache.org/jira/browse/SPARK-5299 there are
two signing keys in
https://people.apache.org/keys/committer/pwendell.asc (9E4FE3AF,
00799F7E) but only one is in http://www.apache.org/dist/spark/KEYS

However, these artifacts seem to be signed by FC8ED089 which isn't in either.

Details details, but I'd say non-binding -1 at the moment.


On Tue, Jan 27, 2015 at 7:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1061/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until Friday, January 30, at 07:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 14:30:09 2015
Return-Path: <dev-return-11292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 573E5177DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 14:30:09 +0000 (UTC)
Received: (qmail 57966 invoked by uid 500); 27 Jan 2015 14:30:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57886 invoked by uid 500); 27 Jan 2015 14:30:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57872 invoked by uid 99); 27 Jan 2015 14:30:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 14:30:04 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 14:30:00 +0000
Received: by mail-ob0-f178.google.com with SMTP id nt9so13631460obb.9
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 06:29:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:cc
         :content-type;
        bh=Zvs5UOh+thV+Hi4j64UykrO/fLwyj2BrTqMG3NoEDx8=;
        b=pQRlib+7zj9mIyHUcv09PechxNtKumz5O8M7WiWgGr5j57kqtYLUtrF96s7mm31ekN
         VduizvUnUneVYu7G4qmksKHMn8QGEX+KZJP6IeaQSMrTqvS6w8NkMbdhJSTT2ze35Nor
         rdHrK6H8mbxBR2MPy3dfqzdwttCGeXDkZh7b7pqZ/a8gRamHycSPDLil8MBC4UH3itMq
         HvKr2HMBazO31FCl+5nWLCaWmSaa2s0OSEAhtDV7vDulFvFtps3wBMqx2HRSm/D8FhjV
         UCrj6sf6z+AnKxhM4qqh+gQUatRrwHyS8fZMLgghK5gwOiGxXsPwmVlS7LCKIC0v2cwN
         QN1A==
X-Received: by 10.182.73.131 with SMTP id l3mr942734obv.45.1422368980100; Tue,
 27 Jan 2015 06:29:40 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.131 with HTTP; Tue, 27 Jan 2015 06:28:59 -0800 (PST)
In-Reply-To: <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com> <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Tue, 27 Jan 2015 12:28:59 -0200
Message-ID: <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b7c4cc6d50050da315b1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b7c4cc6d50050da315b1
Content-Type: text/plain; charset=UTF-8

Can't the SchemaRDD remain the same, but deprecated, and be removed in the
release 1.5(+/- 1)  for example, and the new code been added to DataFrame?
With this, we don't impact in existing code for the next few releases.



2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:

> I want to address the issue that Matei raised about the heavy lifting
> required for a full SQL support. It is amazing that even after 30 years of
> research there is not a single good open source columnar database like
> Vertica. There is a column store option in MySQL, but it is not nearly as
> sophisticated as Vertica or MonetDB. But there's a true need for such a
> system. I wonder why so and it's high time to change that.
> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com> wrote:
>
> > Both SchemaRDD and DataFrame sound fine to me, though I like the former
> > slightly better because it's more descriptive.
> >
> > Even if SchemaRDD's needs to rely on Spark SQL under the covers, it would
> > be more clear from a user-facing perspective to at least choose a package
> > name for it that omits "sql".
> >
> > I would also be in favor of adding a separate Spark Schema module for
> Spark
> > SQL to rely on, but I imagine that might be too large a change at this
> > point?
> >
> > -Sandy
> >
> > On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> >
> > > (Actually when we designed Spark SQL we thought of giving it another
> > name,
> > > like Spark Schema, but we decided to stick with SQL since that was the
> > most
> > > obvious use case to many users.)
> > >
> > > Matei
> > >
> > > > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > > wrote:
> > > >
> > > > While it might be possible to move this concept to Spark Core
> > long-term,
> > > supporting structured data efficiently does require quite a bit of the
> > > infrastructure in Spark SQL, such as query planning and columnar
> storage.
> > > The intent of Spark SQL though is to be more than a SQL server -- it's
> > > meant to be a library for manipulating structured data. Since this is
> > > possible to build over the core API, it's pretty natural to organize it
> > > that way, same as Spark Streaming is a library.
> > > >
> > > > Matei
> > > >
> > > >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> > > >>
> > > >> "The context is that SchemaRDD is becoming a common data format used
> > for
> > > >> bringing data into Spark from external systems, and used for various
> > > >> components of Spark, e.g. MLlib's new pipeline API."
> > > >>
> > > >> i agree. this to me also implies it belongs in spark core, not sql
> > > >>
> > > >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > > >> michaelmalak@yahoo.com.invalid> wrote:
> > > >>
> > > >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13
> Bay
> > > Area
> > > >>> Spark Meetup YouTube contained a wealth of background information
> on
> > > this
> > > >>> idea (mostly from Patrick and Reynold :-).
> > > >>>
> > > >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> > > >>>
> > > >>> ________________________________
> > > >>> From: Patrick Wendell <pwendell@gmail.com>
> > > >>> To: Reynold Xin <rxin@databricks.com>
> > > >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> > > >>> Sent: Monday, January 26, 2015 4:01 PM
> > > >>> Subject: Re: renaming SchemaRDD -> DataFrame
> > > >>>
> > > >>>
> > > >>> One thing potentially not clear from this e-mail, there will be a
> 1:1
> > > >>> correspondence where you can get an RDD to/from a DataFrame.
> > > >>>
> > > >>>
> > > >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> > > wrote:
> > > >>>> Hi,
> > > >>>>
> > > >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
> > wanted
> > > to
> > > >>>> get the community's opinion.
> > > >>>>
> > > >>>> The context is that SchemaRDD is becoming a common data format
> used
> > > for
> > > >>>> bringing data into Spark from external systems, and used for
> various
> > > >>>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> > > more
> > > >>> and
> > > >>>> more users to be programming directly against SchemaRDD API rather
> > > than
> > > >>> the
> > > >>>> core RDD API. SchemaRDD, through its less commonly used DSL
> > originally
> > > >>>> designed for writing test cases, always has the data-frame like
> API.
> > > In
> > > >>>> 1.3, we are redesigning the API to make the API usable for end
> > users.
> > > >>>>
> > > >>>>
> > > >>>> There are two motivations for the renaming:
> > > >>>>
> > > >>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> > > >>>>
> > > >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> > > (even
> > > >>>> though it would contain some RDD functions like map, flatMap,
> etc),
> > > and
> > > >>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> > > >>> Instead.
> > > >>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> > > >>>>
> > > >>>>
> > > >>>> My understanding is that very few users program directly against
> the
> > > >>>> SchemaRDD API at the moment, because they are not well documented.
> > > >>> However,
> > > >>>> oo maintain backward compatibility, we can create a type alias
> > > DataFrame
> > > >>>> that is still named SchemaRDD. This will maintain source
> > compatibility
> > > >>> for
> > > >>>> Scala. That said, we will have to update all existing materials to
> > use
> > > >>>> DataFrame rather than SchemaRDD.
> > > >>>
> > > >>>
> ---------------------------------------------------------------------
> > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > > >>>
> > > >>>
> ---------------------------------------------------------------------
> > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > > >>>
> > > >>>
> > > >
> > >
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > >
> >
>

--089e0160b7c4cc6d50050da315b1--

From dev-return-11293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 15:57:34 2015
Return-Path: <dev-return-11293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C830217B66
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 15:57:34 +0000 (UTC)
Received: (qmail 95974 invoked by uid 500); 27 Jan 2015 15:57:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95898 invoked by uid 500); 27 Jan 2015 15:57:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95817 invoked by uid 99); 27 Jan 2015 15:57:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 15:57:33 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 15:57:08 +0000
Received: by mail-wi0-f179.google.com with SMTP id l15so5967335wiw.0
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 07:54:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=I3W3KCbLuBN2YGWc9d/PzoukEwBc/qAQieSaxbGrn50=;
        b=Nq15iLdsn7ISVpJQkrizJ4w90WKWg37+aBLYrnr923G1NaR99VStaqKlDSgJid1clp
         0XUA1ROn/yL/rDUgqnmRvg4AKARJjaLvGP/TjwOaL7lvT3auPGuf6aYktlitWmAhWxkb
         6/ezshnEViZod7OL/ggqlYp9ZdCHJZa0qAwyoWTNENogas2v/somSu9Bbp+hOuEVr6AG
         tkuTIP71Z9OsSN9bWT7Ts8bR1Dez/MXoTXjKw6T2MVFwacmSDZ8RNSdNnk4ymSq7/mNU
         lOmtH7MIaCNLnX4p9ZMH4wLVwRwX7LLOrI+3VTE24xSqE1OnrU0aJTZcS9I9Z4QbfOT2
         4kTA==
MIME-Version: 1.0
X-Received: by 10.194.108.9 with SMTP id hg9mr1960003wjb.68.1422374090599;
 Tue, 27 Jan 2015 07:54:50 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Tue, 27 Jan 2015 07:54:50 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Tue, 27 Jan 2015 07:54:50 -0800 (PST)
In-Reply-To: <1422116785722-10265.post@n3.nabble.com>
References: <1422116785722-10265.post@n3.nabble.com>
Date: Tue, 27 Jan 2015 07:54:50 -0800
Message-ID: <CAJgQjQ_zr+xqKCmM0kySqaRnGU18DhUvz-Raqc99919-7Mqn5A@mail.gmail.com>
Subject: Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?
From: Xiangrui Meng <mengxr@gmail.com>
To: ogeagla <ogeagla@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf10b1c687554050da44618
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf10b1c687554050da44618
Content-Type: text/plain; charset=UTF-8

I would call it Scaler. You might want to add it to the spark.ml pipieline
api. Please check the spark.ml.HashingTF implementation. Note that this
should handle sparse vectors efficiently.

Hadamard and FFTs are quite useful. If you are intetested, make sure that
we call an FFT libary that is license-compatible with Apache.

-Xiangrui
On Jan 24, 2015 8:27 AM, "Octavian Geagla" <ogeagla@gmail.com> wrote:

> Hello,
>
> I found it useful to implement the  Hadamard Product
> <https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29http://>
>  as
> a VectorTransformer.  It can be applied to scale (by a constant) a certain
> dimension (column) of the data set.
>
> Since I've already implemented it and am using it, I thought I'd see if
> there's interest in this feature going in as Experimental.  I'm not sold on
> the name 'Weighter', either.
>
> Here's the current branch with the work (docs, impl, tests).
> <https://github.com/ogeagla/spark/compare/spark-mllib-weighting>
>
> The implementation was heavily inspired by those of StandardScalerModel and
> Normalizer.
>
> Thanks
> Octavian
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Any-interest-in-weighting-VectorTransformer-which-does-component-wise-scaling-tp10265.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bf10b1c687554050da44618--

From dev-return-11294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 16:04:52 2015
Return-Path: <dev-return-11294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8EEE17BEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 16:04:52 +0000 (UTC)
Received: (qmail 24808 invoked by uid 500); 27 Jan 2015 16:04:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24734 invoked by uid 500); 27 Jan 2015 16:04:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24721 invoked by uid 99); 27 Jan 2015 16:04:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:04:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:04:26 +0000
Received: by mail-wg0-f46.google.com with SMTP id l2so15603039wgh.5
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 08:03:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=s1+BxgNnBTIk4UApuDPTVAOvEO3YcxXzkFL3PFLEPnM=;
        b=QXwyGj8KXTWaf03C1iC5DS9qpuVjBtLpuM6JTOPhzGnQSH7olOXfTp4n7N7gwX94CE
         uqNWL2BFAn2rYE7yezqWNBvzIn5nDyhVzFc3o4foDIH+n/XJXjheRo9sU/ttGqmLaYQo
         Jt2iC7Ay773AVRY2Xz1sWLo4jTswKte2LgBm9I2zI82NFQFVHKE3Yv5LFRagBlUAbkeK
         nbwmplrdgz3vlbKX+DGw+QfCrIwWFQ7VHy9uS9S3cKXfhJXctjfM1z45XOAdHj7KFJGA
         DwnYuD3CDjFLMJiBnGeH7QSXABYPGRO9MKrn/H7VevFxeJsw9EX3jN/vlGaaA5tFzYX7
         kQ1Q==
MIME-Version: 1.0
X-Received: by 10.180.103.33 with SMTP id ft1mr7388685wib.19.1422374620408;
 Tue, 27 Jan 2015 08:03:40 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Tue, 27 Jan 2015 08:03:40 -0800 (PST)
Received: by 10.194.16.2 with HTTP; Tue, 27 Jan 2015 08:03:40 -0800 (PST)
In-Reply-To: <CAEYYnxaAmqme9L7ngap=uP7uzKSsAC-azF-aHYEZDw7kb8y6Ug@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
	<CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDE5C94@G4W3292.americas.hpqcorp.net>
	<CAEYYnxaAmqme9L7ngap=uP7uzKSsAC-azF-aHYEZDw7kb8y6Ug@mail.gmail.com>
Date: Tue, 27 Jan 2015 08:03:40 -0800
Message-ID: <CAJgQjQ9xJdrqOyP1SNcxk9ehDV+8LDEn7CwqyWyqiVgZnxWFAw@mail.gmail.com>
Subject: Re: Maximum size of vector that reduce can handle
From: Xiangrui Meng <mengxr@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Alexander Ulanov <alexander.ulanov@hp.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0442715cfcb5db050da465e6
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442715cfcb5db050da465e6
Content-Type: text/plain; charset=UTF-8

60m-vector costs 480MB memory. You have 12 of them to be reduced to the
driver. So you need ~6GB memory not counting the temp vectors generated
from '_+_'. You need to increase driver memory to make it work. That being
said, ~10^7 hits the limit for the current impl of glm. -Xiangrui
On Jan 23, 2015 2:19 PM, "DB Tsai" <dbtsai@dbtsai.com> wrote:

> Hi Alexander,
>
> For `reduce`, it's an action that will collect all the data from
> mapper to driver, and perform the aggregation in driver. As a result,
> if the output from the mapper is very large, and the numbers of
> partitions in mapper are large, it might cause a problem.
>
> For `treeReduce`, as the name indicates, the way it works is in the
> first layer, it aggregates the output of the mappers two by two
> resulting half of the numbers of output. And then, we continuously do
> the aggregation layer by layer. The final aggregation will be done in
> driver but in this time, the numbers of data are small.
>
> By default, depth 2 is used, so if you have so many partitions of
> large vector, this may still cause issue. You can increase the depth
> into higher numbers such that in the final reduce in driver, the
> number of partitions are very small.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
>
> On Fri, Jan 23, 2015 at 12:07 PM, Ulanov, Alexander
> <alexander.ulanov@hp.com> wrote:
> > Hi DB Tsai,
> >
> > Thank you for your suggestion. Actually, I've started my experiments
> with "treeReduce". Originally, I had "vv.treeReduce(_ + _, 2)" in my script
> exactly because MLlib optimizers are using it, as you pointed out with
> LBFGS. However, it leads to the same problems as "reduce", but presumably
> not so directly. As far as I understand, treeReduce limits the number of
> communications between workers and master forcing workers to partially
> compute the reduce operation.
> >
> > Are you sure that driver will first collect all results (or all partial
> results in treeReduce) and ONLY then perform aggregation? If that is the
> problem, then how to force it to do aggregation after receiving each
> portion of data from Workers?
> >
> > Best regards, Alexander
> >
> > -----Original Message-----
> > From: DB Tsai [mailto:dbtsai@dbtsai.com]
> > Sent: Friday, January 23, 2015 11:53 AM
> > To: Ulanov, Alexander
> > Cc: dev@spark.apache.org
> > Subject: Re: Maximum size of vector that reduce can handle
> >
> > Hi Alexander,
> >
> > When you use `reduce` to aggregate the vectors, those will actually be
> pulled into driver, and merged over there. Obviously, it's not scaleable
> given you are doing deep neural networks which have so many coefficients.
> >
> > Please try treeReduce instead which is what we do in linear regression
> and logistic regression.
> >
> > See
> https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/optimization/LBFGS.scala
> > for example.
> >
> > val (gradientSum, lossSum) = data.treeAggregate((Vectors.zeros(n),
> 0.0))( seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
> features)) => val l = localGradient.compute( features, label, bcW.value,
> grad) (grad, loss + l) }, combOp = (c1, c2) => (c1, c2) match { case
> ((grad1, loss1), (grad2, loss2)) => axpy(1.0, grad2, grad1) (grad1, loss1 +
> loss2)
> > })
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> >
> > On Fri, Jan 23, 2015 at 10:00 AM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
> >> Dear Spark developers,
> >>
> >> I am trying to measure the Spark reduce performance for big vectors. My
> motivation is related to machine learning gradient. Gradient is a vector
> that is computed on each worker and then all results need to be summed up
> and broadcasted back to workers. For example, present machine learning
> applications involve very long parameter vectors, for deep neural networks
> it can be up to 2Billions. So, I want to measure the time that is needed
> for this operation depending on the size of vector and number of workers. I
> wrote few lines of code that assume that Spark will distribute partitions
> among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores,
> 16GB RAM), each runs 2 Workers.
> >>
> >> import org.apache.spark.mllib.rdd.RDDFunctions._
> >> import breeze.linalg._
> >> import org.apache.log4j._
> >> Logger.getRootLogger.setLevel(Level.OFF)
> >> val n = 60000000
> >> val p = 12
> >> val vv = sc.parallelize(0 until p, p).map(i =>
> >> DenseVector.rand[Double]( n )) vv.reduce(_ + _)
> >>
> >> When executing in shell with 60M vector it crashes after some period of
> time. One of the node contains the following in stdout:
> >> Java HotSpot(TM) 64-Bit Server VM warning: INFO:
> >> os::commit_memory(0x0000000755500000, 2863661056, 0) failed;
> >> error='Cannot allocate memory' (errno=12) # # There is insufficient
> memory for the Java Runtime Environment to continue.
> >> # Native memory allocation (malloc) failed to allocate 2863661056 bytes
> for committing reserved memory.
> >>
> >> I run shell with --executor-memory 8G --driver-memory 8G, so handling
> 60M vector of Double should not be a problem. Are there any big overheads
> for this? What is the maximum size of vector that reduce can handle?
> >>
> >> Best regards, Alexander
> >>
> >> P.S.
> >>
> >> "spark.driver.maxResultSize 0" needs to set in order to run this code.
> I also needed to change "java.io.tmpdir" and "spark.local.dir" folders
> because my /tmp folder which is default, was too small and Spark swaps
> heavily into this folder. Without these settings I get either "no space
> left on device" or "out of memory" exceptions.
> >>
> >> I also submitted a bug
> >> https://issues.apache.org/jira/browse/SPARK-5386
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
> >> additional commands, e-mail: dev-help@spark.apache.org
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--f46d0442715cfcb5db050da465e6--

From dev-return-11295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 16:52:43 2015
Return-Path: <dev-return-11295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A633017E9D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 16:52:43 +0000 (UTC)
Received: (qmail 89929 invoked by uid 500); 27 Jan 2015 16:52:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89855 invoked by uid 500); 27 Jan 2015 16:52:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89843 invoked by uid 99); 27 Jan 2015 16:52:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:52:42 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:52:17 +0000
Received: by mail-vc0-f180.google.com with SMTP id hy10so5062818vcb.11
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 08:51:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=RNLgeVUk2UDuk2x2VU77Nb0gqm95Dh1wisZ1UIszL90=;
        b=dWRUJstG911TGyUi9R2jmhpaJFxxmH4FeQBQDTk3fV+Z+sr9VRM9CzjW2lY83Yrw6J
         UZSyqbSpv2PehAHImeQYuTTE0/f+rBIExPCwbTfrtrYSwwNJ2CHgrYWm93C98+eduf6s
         XuBRXQOUeUi9AwITH4k9jXDotTU909aAEZCuhfGxjbvf7yMft96offJl/YnbPABgIcoL
         16E4/PhgN4vEkbLVXRdaJt48ay2Ez15X+ojHH7KsyIXSkapX9qERmmiUFU4tq86AeYi0
         8S2Q8iT44pzI+V/UYvb3uRG+6HpFyS1WCJ7eIcY9vD+AfOosRjyulMHkLvXIl8lf4x1x
         bwkw==
X-Received: by 10.220.91.7 with SMTP id k7mr1024544vcm.79.1422377490757; Tue,
 27 Jan 2015 08:51:30 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Tue, 27 Jan 2015 08:51:10 -0800 (PST)
In-Reply-To: <CAJgQjQ_zr+xqKCmM0kySqaRnGU18DhUvz-Raqc99919-7Mqn5A@mail.gmail.com>
References: <1422116785722-10265.post@n3.nabble.com> <CAJgQjQ_zr+xqKCmM0kySqaRnGU18DhUvz-Raqc99919-7Mqn5A@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Tue, 27 Jan 2015 08:51:10 -0800
Message-ID: <CABjXkq6GOQKq9e4-XELKM0VZbJEAfrFFqqFSZUevyaHGvfz17A@mail.gmail.com>
Subject: Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?
To: Xiangrui Meng <mengxr@gmail.com>
Cc: ogeagla <ogeagla@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b34402612bea8050da511b2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b34402612bea8050da511b2
Content-Type: text/plain; charset=UTF-8

Hmm... Scaler and Scalar are very close together both in terms of
pronunciation and spelling - and I wouldn't want to create confusion
between the two. Further - this operation (elementwise multiplication by a
static vector) is general enough that maybe it should have a more general
name?

On Tue, Jan 27, 2015 at 7:54 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I would call it Scaler. You might want to add it to the spark.ml pipieline
> api. Please check the spark.ml.HashingTF implementation. Note that this
> should handle sparse vectors efficiently.
>
> Hadamard and FFTs are quite useful. If you are intetested, make sure that
> we call an FFT libary that is license-compatible with Apache.
>
> -Xiangrui
> On Jan 24, 2015 8:27 AM, "Octavian Geagla" <ogeagla@gmail.com> wrote:
>
> > Hello,
> >
> > I found it useful to implement the  Hadamard Product
> > <https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29http://>
> >  as
> > a VectorTransformer.  It can be applied to scale (by a constant) a
> certain
> > dimension (column) of the data set.
> >
> > Since I've already implemented it and am using it, I thought I'd see if
> > there's interest in this feature going in as Experimental.  I'm not sold
> on
> > the name 'Weighter', either.
> >
> > Here's the current branch with the work (docs, impl, tests).
> > <https://github.com/ogeagla/spark/compare/spark-mllib-weighting>
> >
> > The implementation was heavily inspired by those of StandardScalerModel
> and
> > Normalizer.
> >
> > Thanks
> > Octavian
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Any-interest-in-weighting-VectorTransformer-which-does-component-wise-scaling-tp10265.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7b34402612bea8050da511b2--

From dev-return-11296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 16:57:22 2015
Return-Path: <dev-return-11296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E86917EC9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 16:57:22 +0000 (UTC)
Received: (qmail 2205 invoked by uid 500); 27 Jan 2015 16:57:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2132 invoked by uid 500); 27 Jan 2015 16:57:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2120 invoked by uid 99); 27 Jan 2015 16:57:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:57:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 16:56:56 +0000
Received: by mail-vc0-f181.google.com with SMTP id id10so5045480vcb.12
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 08:55:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=xzt5aVylMjIuSjvxcbJcgNy0Ixkn2EA58mAzWaXnYec=;
        b=OpV7+BKAq8JRVYNdmLfmXu2oQ8XCkocS3r/QpjqQw4jDlo66ab0Grf1VtAui76vRqY
         BEn3xQoqdi0p3fbZANccoAOrS2qs+fKtmRYxBCUvfos9dtlbppWTmmqLdA5cZu8AdCAL
         bAk5y5tflyKy2Bb87avh0crJoAJzLXaFii9gH2nYi4ek1VYgCsURngPt+VHLlL08PW1r
         H2TN9IFXjyn8tyv+fNTJLQ/QZ63Vn1ABc6qQvaCfq+XdcINsA2QlL/lsjUrfS6cIEZw4
         N1+Kp8c2oFyBayauWTQmcp0UMEqBMfYU7ggx02643DXACTh3dYXr/2N+8SYAICRlAJ5w
         0HgQ==
X-Received: by 10.52.3.74 with SMTP id a10mr868907vda.95.1422377724288; Tue,
 27 Jan 2015 08:55:24 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Tue, 27 Jan 2015 08:55:04 -0800 (PST)
In-Reply-To: <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Tue, 27 Jan 2015 08:55:04 -0800
Message-ID: <CABjXkq6tYkc8DcN=HGsMepPvXLKk2ycgJpJ+EabJG=iWo8vG0A@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Koert Kuipers <koert@tresata.com>, Michael Malak <michaelmalak@yahoo.com>, 
	Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf3033477ffe241a050da51e7c
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3033477ffe241a050da51e7c
Content-Type: text/plain; charset=UTF-8

I'm +1 on this, although a little worried about unknowingly introducing
SparkSQL dependencies every time someone wants to use this. It would be
great if the interface can be abstract and the implementation (in this
case, SparkSQL backend) could be swapped out.

One alternative suggestion on the name - why not call it DataTable?
DataFrame seems like a name carried over from pandas (and by extension, R),
and it's never been obvious to me what a "Frame" is.



On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> (Actually when we designed Spark SQL we thought of giving it another name,
> like Spark Schema, but we decided to stick with SQL since that was the most
> obvious use case to many users.)
>
> Matei
>
> > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >
> > While it might be possible to move this concept to Spark Core long-term,
> supporting structured data efficiently does require quite a bit of the
> infrastructure in Spark SQL, such as query planning and columnar storage.
> The intent of Spark SQL though is to be more than a SQL server -- it's
> meant to be a library for manipulating structured data. Since this is
> possible to build over the core API, it's pretty natural to organize it
> that way, same as Spark Streaming is a library.
> >
> > Matei
> >
> >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> >>
> >> "The context is that SchemaRDD is becoming a common data format used for
> >> bringing data into Spark from external systems, and used for various
> >> components of Spark, e.g. MLlib's new pipeline API."
> >>
> >> i agree. this to me also implies it belongs in spark core, not sql
> >>
> >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> michaelmalak@yahoo.com.invalid> wrote:
> >>
> >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> Area
> >>> Spark Meetup YouTube contained a wealth of background information on
> this
> >>> idea (mostly from Patrick and Reynold :-).
> >>>
> >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >>>
> >>> ________________________________
> >>> From: Patrick Wendell <pwendell@gmail.com>
> >>> To: Reynold Xin <rxin@databricks.com>
> >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >>> Sent: Monday, January 26, 2015 4:01 PM
> >>> Subject: Re: renaming SchemaRDD -> DataFrame
> >>>
> >>>
> >>> One thing potentially not clear from this e-mail, there will be a 1:1
> >>> correspondence where you can get an RDD to/from a DataFrame.
> >>>
> >>>
> >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>>> Hi,
> >>>>
> >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
> to
> >>>> get the community's opinion.
> >>>>
> >>>> The context is that SchemaRDD is becoming a common data format used
> for
> >>>> bringing data into Spark from external systems, and used for various
> >>>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> more
> >>> and
> >>>> more users to be programming directly against SchemaRDD API rather
> than
> >>> the
> >>>> core RDD API. SchemaRDD, through its less commonly used DSL originally
> >>>> designed for writing test cases, always has the data-frame like API.
> In
> >>>> 1.3, we are redesigning the API to make the API usable for end users.
> >>>>
> >>>>
> >>>> There are two motivations for the renaming:
> >>>>
> >>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> >>>>
> >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> (even
> >>>> though it would contain some RDD functions like map, flatMap, etc),
> and
> >>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> >>> Instead.
> >>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> >>>>
> >>>>
> >>>> My understanding is that very few users program directly against the
> >>>> SchemaRDD API at the moment, because they are not well documented.
> >>> However,
> >>>> oo maintain backward compatibility, we can create a type alias
> DataFrame
> >>>> that is still named SchemaRDD. This will maintain source compatibility
> >>> for
> >>>> Scala. That said, we will have to update all existing materials to use
> >>>> DataFrame rather than SchemaRDD.
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>>
> >
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--20cf3033477ffe241a050da51e7c--

From dev-return-11297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 17:09:41 2015
Return-Path: <dev-return-11297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A17A17F46
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 17:09:41 +0000 (UTC)
Received: (qmail 29214 invoked by uid 500); 27 Jan 2015 17:09:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29139 invoked by uid 500); 27 Jan 2015 17:09:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29127 invoked by uid 99); 27 Jan 2015 17:09:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:09:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:09:35 +0000
Received: by mail-wg0-f52.google.com with SMTP id y19so15921974wgg.11
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 09:08:09 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=NxPtd6pZJKu302pMdYAwdfObRUJ/x0hy62k+snshZko=;
        b=aSJkLYThDw4HPLR98izYN6Y7xBS7wFXiBHtcUO2s2opAKoLIMjhSPvhkvYOJJojiKI
         pGaxY2WSd3XiJOtPRu8Ubc5ZowYq2CBliSiGLuJRB55fvwbWaO7K9gDUqr7qhEDTTksP
         M5w0R4FWOQp8fPqXtUPeysDSNBe9Cc6TE1m8WZyXuW6ZUxWcIF/9I0KV2rDNE3A8MVlO
         b4YM9oNRZwjPOVjyS2Kl86hv1cdGYrPxdDiMIMgUhu25tP4ONqJGbPKdZj4JZbAhrK98
         /KkKrKyKsDx5iqTFHYgE9pPZo6mhW91xJiSKkp1sheUgBSooZTh0KI7HVaPAkzlntiIl
         1zKA==
X-Gm-Message-State: ALoCoQm2koq3McQdILLMmw+9JMyFNG9+W6fwlrvSmIqpQCv8bNhXlcgUSzG3TjXrc79wQXq1wm3w
MIME-Version: 1.0
X-Received: by 10.194.201.137 with SMTP id ka9mr5017786wjc.66.1422378488863;
 Tue, 27 Jan 2015 09:08:08 -0800 (PST)
Received: by 10.217.122.200 with HTTP; Tue, 27 Jan 2015 09:08:08 -0800 (PST)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
Date: Tue, 27 Jan 2015 12:08:08 -0500
Message-ID: <CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Koert Kuipers <koert@tresata.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Michael Malak <michaelmalak@yahoo.com>, Patrick Wendell <pwendell@gmail.com>, 
	Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b873d5e90b37e050da54c1e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b873d5e90b37e050da54c1e
Content-Type: text/plain; charset=UTF-8

hey matei,
i think that stuff such as SchemaRDD, columar storage and perhaps also
query planning can be re-used by many systems that do analysis on
structured data. i can imagine panda-like systems, but also datalog or
scalding-like (which we use at tresata and i might rebase on SchemaRDD at
some point). SchemaRDD should become the interface for all these. and
columnar storage abstractions should be re-used between all these.

currently the sql tie in is way beyond just the (perhaps unfortunate)
naming convention. for example a core part of the SchemaRD abstraction is
Row, which is org.apache.spark.sql.catalyst.expressions.Row, forcing anyone
that want to build on top of SchemaRDD to dig into catalyst, a SQL Parser
(if i understand it correctly, i have not used catalyst, but it looks
neat). i should not need to include a SQL parser just to use structured
data in say a panda-like framework.

best, koert


On Mon, Jan 26, 2015 at 8:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> While it might be possible to move this concept to Spark Core long-term,
> supporting structured data efficiently does require quite a bit of the
> infrastructure in Spark SQL, such as query planning and columnar storage.
> The intent of Spark SQL though is to be more than a SQL server -- it's
> meant to be a library for manipulating structured data. Since this is
> possible to build over the core API, it's pretty natural to organize it
> that way, same as Spark Streaming is a library.
>
> Matei
>
> > On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> >
> > "The context is that SchemaRDD is becoming a common data format used for
> > bringing data into Spark from external systems, and used for various
> > components of Spark, e.g. MLlib's new pipeline API."
> >
> > i agree. this to me also implies it belongs in spark core, not sql
> >
> > On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > michaelmalak@yahoo.com.invalid> wrote:
> >
> >> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> Area
> >> Spark Meetup YouTube contained a wealth of background information on
> this
> >> idea (mostly from Patrick and Reynold :-).
> >>
> >> https://www.youtube.com/watch?v=YWppYPWznSQ
> >>
> >> ________________________________
> >> From: Patrick Wendell <pwendell@gmail.com>
> >> To: Reynold Xin <rxin@databricks.com>
> >> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >> Sent: Monday, January 26, 2015 4:01 PM
> >> Subject: Re: renaming SchemaRDD -> DataFrame
> >>
> >>
> >> One thing potentially not clear from this e-mail, there will be a 1:1
> >> correspondence where you can get an RDD to/from a DataFrame.
> >>
> >>
> >> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>> Hi,
> >>>
> >>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
> to
> >>> get the community's opinion.
> >>>
> >>> The context is that SchemaRDD is becoming a common data format used for
> >>> bringing data into Spark from external systems, and used for various
> >>> components of Spark, e.g. MLlib's new pipeline API. We also expect more
> >> and
> >>> more users to be programming directly against SchemaRDD API rather than
> >> the
> >>> core RDD API. SchemaRDD, through its less commonly used DSL originally
> >>> designed for writing test cases, always has the data-frame like API. In
> >>> 1.3, we are redesigning the API to make the API usable for end users.
> >>>
> >>>
> >>> There are two motivations for the renaming:
> >>>
> >>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> >>>
> >>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
> >>> though it would contain some RDD functions like map, flatMap, etc), and
> >>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> >> Instead.
> >>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> >>>
> >>>
> >>> My understanding is that very few users program directly against the
> >>> SchemaRDD API at the moment, because they are not well documented.
> >> However,
> >>> oo maintain backward compatibility, we can create a type alias
> DataFrame
> >>> that is still named SchemaRDD. This will maintain source compatibility
> >> for
> >>> Scala. That said, we will have to update all existing materials to use
> >>> DataFrame rather than SchemaRDD.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>
>

--047d7b873d5e90b37e050da54c1e--

From dev-return-11298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 17:10:27 2015
Return-Path: <dev-return-11298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 99D7F17F4E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 17:10:27 +0000 (UTC)
Received: (qmail 31537 invoked by uid 500); 27 Jan 2015 17:10:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31462 invoked by uid 500); 27 Jan 2015 17:10:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31450 invoked by uid 99); 27 Jan 2015 17:10:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:10:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of iulian.dragos@typesafe.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:10:00 +0000
Received: by mail-ob0-f182.google.com with SMTP id gq1so14492358obb.13
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 09:09:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=typesafe.com; s=google;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=yap0ZiUwZIaGybkVwYuYGmPWDAhnMw8E84qRud8Z3ac=;
        b=RQfM9B+BTU2pMWqR/YShkG/LW12a/jE3tmEMnWaWktcZctwS8lyjk3QukCh+Eszpbi
         /2a/WZWDiB4qMKEWWdieXrj8ufBNapHEUe8+Q+jTTg7lkw9VB1Rk6E+H2SwyFIrJzBHa
         gqRQesHnBQxoOauwJSH85RExSn4DK5s0qNYeA=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=yap0ZiUwZIaGybkVwYuYGmPWDAhnMw8E84qRud8Z3ac=;
        b=Tkk6e4j8iaROrjk0R6xjOw6jRpThB+OEgfbTF4S7OcYXTsNL4/ZTWHWthq2Gg89dr9
         8gAcuKi3BfFHJ86FRV5H41ESQicMo0MeVmtg9/75TkZg3CX4qo3OmG0+pqz5CO1EEmDN
         SkphTzyoITsgIVP/JVfKW/L3MbekdriOA7RIWGjalEbAN/qZbm8W1qZgQUlnH4YM1jT9
         WAmoLxX44FOUSHuLfOWjY7E7P4QQde3/nzIhqHxI3axY4AZt3ngtigRHhPCX2i7zOQ2k
         F3a8jHAt7kWIJkiDBIkfeVSD4GpNvPXNigE9sxJz3s+0qPoP4/kdGGiG3BGMl77pn+NQ
         OzQQ==
X-Gm-Message-State: ALoCoQkn0mPHUnbZGwm/gtrEjAP4OC8kCfCTeo9n0vIYuTj1v3bG0LI9n3ojhv4MNm0h7h3awxbR
X-Received: by 10.182.56.70 with SMTP id y6mr1413173obp.55.1422378553165; Tue,
 27 Jan 2015 09:09:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.203.67 with HTTP; Tue, 27 Jan 2015 09:08:53 -0800 (PST)
From: =?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>
Date: Tue, 27 Jan 2015 18:08:53 +0100
Message-ID: <CAD2BF0Kunp=FCz51RFDT1Z2RcSt0pQdzmhxYfW+B37yFnEm+zg@mail.gmail.com>
Subject: UnknownHostException while running YarnTestSuite
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8fb1ec5e65f01e050da55034
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb1ec5e65f01e050da55034
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I=E2=80=99m trying to run the Spark test suite on an EC2 instance, but I ca=
n=E2=80=99t get
Yarn tests to pass. The hostname I get on that machine is not resolvable,
but adding a line in /etc/hosts makes the other tests pass, except for Yarn
tests.

Any help is greatly appreciated!

thanks,
iulian

ubuntu@ip-172-30-0-248:~/spark$ cat /etc/hosts
127.0.0.1 localhost
172.30.0.248 ip-172-30-0-248

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts

and the exception:

[info] - run Spark in yarn-client mode *** FAILED *** (2 seconds, 249
milliseconds)
[info]   java.net.UnknownHostException: Invalid host name: local host
is: (unknown); destination host is: "ip-172-30-0-248":57041;
java.net.UnknownHostException; For more details see:
http://wiki.apache.org/hadoop/UnknownHost
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeCon=
structorAccessorImpl.java:57)
[info]   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Deleg=
atingConstructorAccessorImpl.java:45)
[info]   at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
[info]   at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:78=
3)
[info]   at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:742)
[info]   at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
[info]   at org.apache.hadoop.ipc.Client.getConnection(Client.java:1448)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1377)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1359)
[info]   at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(Protobuf=
RpcEngine.java:206)
[info]   at com.sun.proxy.$Proxy69.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProt=
ocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.ja=
va:152)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccesso=
rImpl.java:57)
[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho=
dAccessorImpl.java:43)
[info]   at java.lang.reflect.Method.invoke(Method.java:606)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(=
RetryInvocationHandler.java:186)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryI=
nvocationHandler.java:102)
[info]   at com.sun.proxy.$Proxy70.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnCl=
usterMetrics(YarnClientImpl.java:294)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$=
1.apply(Client.scala:91)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$=
1.apply(Client.scala:91)
[info]   at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
[info]   at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
[info]   at org.apache.spark.deploy.yarn.Client.submitApplication(Client.sc=
ala:90)
[info]   at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.s=
tart(YarnClientSchedulerBackend.scala:57)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedule=
rImpl.scala:141)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:343)
[info]   at org.apache.spark.deploy.yarn.YarnClusterDriver$.main(YarnCluste=
rSuite.scala:175)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply$=
mcV$sp(YarnClusterSuite.scala:118)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(=
YarnClusterSuite.scala:116)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(=
YarnClusterSuite.scala:116)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transfo=
rmer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166=
)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLi=
ke.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLik=
e.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLik=
e.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175=
)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLi=
ke.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLi=
ke.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(E=
ngine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(E=
ngine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsIn=
Branch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:20=
8)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(Fu=
nSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.sc=
ala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.sc=
ala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.org$scalatest$Bef=
oreAndAfterAll$$super$run(YarnClusterSuite.scala:36)
[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAf=
terAll.scala:257)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.sca=
la:256)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.run(YarnClusterSu=
ite.scala:36)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$ru=
nSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.s=
cala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExe=
cutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolEx=
ecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: java.net.UnknownHostException:
[info]   at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
[info]   at org.apache.hadoop.ipc.Client.getConnection(Client.java:1448)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1377)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1359)
[info]   at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(Protobuf=
RpcEngine.java:206)
[info]   at com.sun.proxy.$Proxy69.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProt=
ocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.ja=
va:152)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccesso=
rImpl.java:57)
[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho=
dAccessorImpl.java:43)
[info]   at java.lang.reflect.Method.invoke(Method.java:606)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(=
RetryInvocationHandler.java:186)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryI=
nvocationHandler.java:102)
[info]   at com.sun.proxy.$Proxy70.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnCl=
usterMetrics(YarnClientImpl.java:294)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$=
1.apply(Client.scala:91)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$=
1.apply(Client.scala:91)
[info]   at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
[info]   at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
[info]   at org.apache.spark.deploy.yarn.Client.submitApplication(Client.sc=
ala:90)
[info]   at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.s=
tart(YarnClientSchedulerBackend.scala:57)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedule=
rImpl.scala:141)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:343)
[info]   at org.apache.spark.deploy.yarn.YarnClusterDriver$.main(YarnCluste=
rSuite.scala:175)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply$=
mcV$sp(YarnClusterSuite.scala:118)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(=
YarnClusterSuite.scala:116)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(=
YarnClusterSuite.scala:116)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transfo=
rmer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166=
)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)

=E2=80=8B
--=20

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com

--e89a8fb1ec5e65f01e050da55034--

From dev-return-11299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 17:19:25 2015
Return-Path: <dev-return-11299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 65D9F17FA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 17:19:25 +0000 (UTC)
Received: (qmail 59410 invoked by uid 500); 27 Jan 2015 17:19:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59336 invoked by uid 500); 27 Jan 2015 17:19:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59325 invoked by uid 99); 27 Jan 2015 17:19:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:19:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:18:59 +0000
Received: by mail-wg0-f42.google.com with SMTP id x13so16064171wgg.1
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 09:18:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=3Wt9aFk6CYoXd6D0GU9Wxxy6lfFjtnppM+wAQYexWfk=;
        b=lwG6b6TzVRVQbbrjJX436zmIQKLIM2korXlSjonNHjXnkGmVhaNZcCo2ALUKGj7yHE
         I0SwWPMymK067lCSGmdGZbwBnUNltNj6RB0MsDvypNPKV7Vgf8M5FsYYGaXsQB/YvkCS
         dhj16ZBaNvdPh6/6RfzxzOqho1ZZCPpHhBG/9Q+XB7sv4fnjoO5OuwnEmOnjjk5r7lkO
         L7LDXZ9unwkk08RgtWUuz1xVUc683lHzn6hV0u4WV/cac1vd7AVVNgjzgvrE9Sk6Vjsc
         Dvs0ivFGSLwF0LiLbUSwfedub6Cp/RMkLTpyk2iYfdvfNOFOYyzRYhyVrTXOh/hvIKCc
         IprA==
X-Gm-Message-State: ALoCoQmOHsW4Kfat+aPCha+ZbhvDMw6F3LckH8jh951iHBNh2FsH5G2V5Vh8nWG6HRIubz1hSEhh
MIME-Version: 1.0
X-Received: by 10.180.81.169 with SMTP id b9mr8001118wiy.41.1422379137222;
 Tue, 27 Jan 2015 09:18:57 -0800 (PST)
Received: by 10.216.114.148 with HTTP; Tue, 27 Jan 2015 09:18:57 -0800 (PST)
In-Reply-To: <CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
Date: Tue, 27 Jan 2015 09:18:57 -0800
Message-ID: <CAAsvFPk_M1j-6KgbBcPkRG5fAcEXCJH+OOnxFPvc67jrEMt3_Q@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Mark Hamstra <mark@clearstorydata.com>
To: Koert Kuipers <koert@tresata.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Michael Malak <michaelmalak@yahoo.com>, 
	Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d044401ec35de5d050da57382
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044401ec35de5d050da57382
Content-Type: text/plain; charset=UTF-8

In master, Reynold has already taken care of moving Row
into org.apache.spark.sql; so, even though the implementation of Row (and
GenericRow et al.) is in Catalyst (which is more optimizer than parser),
that needn't be of concern to users of the API in its most recent state.

On Tue, Jan 27, 2015 at 9:08 AM, Koert Kuipers <koert@tresata.com> wrote:

> hey matei,
> i think that stuff such as SchemaRDD, columar storage and perhaps also
> query planning can be re-used by many systems that do analysis on
> structured data. i can imagine panda-like systems, but also datalog or
> scalding-like (which we use at tresata and i might rebase on SchemaRDD at
> some point). SchemaRDD should become the interface for all these. and
> columnar storage abstractions should be re-used between all these.
>
> currently the sql tie in is way beyond just the (perhaps unfortunate)
> naming convention. for example a core part of the SchemaRD abstraction is
> Row, which is org.apache.spark.sql.catalyst.expressions.Row, forcing anyone
> that want to build on top of SchemaRDD to dig into catalyst, a SQL Parser
> (if i understand it correctly, i have not used catalyst, but it looks
> neat). i should not need to include a SQL parser just to use structured
> data in say a panda-like framework.
>
> best, koert
>
>
> On Mon, Jan 26, 2015 at 8:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
> > While it might be possible to move this concept to Spark Core long-term,
> > supporting structured data efficiently does require quite a bit of the
> > infrastructure in Spark SQL, such as query planning and columnar storage.
> > The intent of Spark SQL though is to be more than a SQL server -- it's
> > meant to be a library for manipulating structured data. Since this is
> > possible to build over the core API, it's pretty natural to organize it
> > that way, same as Spark Streaming is a library.
> >
> > Matei
> >
> > > On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> > >
> > > "The context is that SchemaRDD is becoming a common data format used
> for
> > > bringing data into Spark from external systems, and used for various
> > > components of Spark, e.g. MLlib's new pipeline API."
> > >
> > > i agree. this to me also implies it belongs in spark core, not sql
> > >
> > > On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > > michaelmalak@yahoo.com.invalid> wrote:
> > >
> > >> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> > Area
> > >> Spark Meetup YouTube contained a wealth of background information on
> > this
> > >> idea (mostly from Patrick and Reynold :-).
> > >>
> > >> https://www.youtube.com/watch?v=YWppYPWznSQ
> > >>
> > >> ________________________________
> > >> From: Patrick Wendell <pwendell@gmail.com>
> > >> To: Reynold Xin <rxin@databricks.com>
> > >> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> > >> Sent: Monday, January 26, 2015 4:01 PM
> > >> Subject: Re: renaming SchemaRDD -> DataFrame
> > >>
> > >>
> > >> One thing potentially not clear from this e-mail, there will be a 1:1
> > >> correspondence where you can get an RDD to/from a DataFrame.
> > >>
> > >>
> > >> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >>> Hi,
> > >>>
> > >>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
> > to
> > >>> get the community's opinion.
> > >>>
> > >>> The context is that SchemaRDD is becoming a common data format used
> for
> > >>> bringing data into Spark from external systems, and used for various
> > >>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> more
> > >> and
> > >>> more users to be programming directly against SchemaRDD API rather
> than
> > >> the
> > >>> core RDD API. SchemaRDD, through its less commonly used DSL
> originally
> > >>> designed for writing test cases, always has the data-frame like API.
> In
> > >>> 1.3, we are redesigning the API to make the API usable for end users.
> > >>>
> > >>>
> > >>> There are two motivations for the renaming:
> > >>>
> > >>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> > >>>
> > >>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> (even
> > >>> though it would contain some RDD functions like map, flatMap, etc),
> and
> > >>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> > >> Instead.
> > >>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> > >>>
> > >>>
> > >>> My understanding is that very few users program directly against the
> > >>> SchemaRDD API at the moment, because they are not well documented.
> > >> However,
> > >>> oo maintain backward compatibility, we can create a type alias
> > DataFrame
> > >>> that is still named SchemaRDD. This will maintain source
> compatibility
> > >> for
> > >>> Scala. That said, we will have to update all existing materials to
> use
> > >>> DataFrame rather than SchemaRDD.
> > >>
> > >> ---------------------------------------------------------------------
> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> For additional commands, e-mail: dev-help@spark.apache.org
> > >>
> > >> ---------------------------------------------------------------------
> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> For additional commands, e-mail: dev-help@spark.apache.org
> > >>
> > >>
> >
> >
>

--f46d044401ec35de5d050da57382--

From dev-return-11300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 17:38:16 2015
Return-Path: <dev-return-11300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5BA3100E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 17:38:16 +0000 (UTC)
Received: (qmail 18612 invoked by uid 500); 27 Jan 2015 17:38:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18537 invoked by uid 500); 27 Jan 2015 17:38:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18523 invoked by uid 99); 27 Jan 2015 17:38:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:38:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vcsubsvc@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:38:10 +0000
Received: by mail-ie0-f181.google.com with SMTP id rp18so16556780iec.12
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 09:37:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/1TN8edKGqCFxFZs1xOEfMzXUhH2XPXN0v3AcSisS54=;
        b=AavsIYVCBKHLNuvDil17pLb+Mc41NRwZluD9qp/TMD4fbx5wUM8djj5ymL0TrLanjh
         9xbApmQMd6GQQRUVgQjWXyU9cILfmo5RSdjTRsZrEVsg7DVCfceOW9NiWK79y3hmYTw+
         FYd5+8yp2oFdImcM3B04RR8IQuNZKEsXeZhKxFOgU0nUxwaOdo13CGDVZl2M2HciSApD
         FqxSvjGWtfYsTcIBCCx7jJnkAe63chhIjncR5v8vi663F/QSCHVRKUC+qi6sSzwdZsXB
         FncbiL8f6j4cm9nsDzfAINT93F9fY9ArIRZ/0CtTdy2DfzCzK1RaXr7//wSOUXv8jTiL
         bozQ==
MIME-Version: 1.0
X-Received: by 10.50.55.98 with SMTP id r2mr4342825igp.6.1422380270523; Tue,
 27 Jan 2015 09:37:50 -0800 (PST)
Received: by 10.42.62.210 with HTTP; Tue, 27 Jan 2015 09:37:50 -0800 (PST)
In-Reply-To: <CAJgQjQ9xJdrqOyP1SNcxk9ehDV+8LDEn7CwqyWyqiVgZnxWFAw@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDE3BA0@G4W3292.americas.hpqcorp.net>
	<CAEYYnxboxTsbJ-Fy+k80HZANvKeuwwamZFJyfKwgfOAxqDJe3A@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDE5C94@G4W3292.americas.hpqcorp.net>
	<CAEYYnxaAmqme9L7ngap=uP7uzKSsAC-azF-aHYEZDw7kb8y6Ug@mail.gmail.com>
	<CAJgQjQ9xJdrqOyP1SNcxk9ehDV+8LDEn7CwqyWyqiVgZnxWFAw@mail.gmail.com>
Date: Tue, 27 Jan 2015 12:37:50 -0500
Message-ID: <CAA9nqM8FDNKpJeByGAv1V5Sqz_1biPhTt98X7WW0VuXznYE+YA@mail.gmail.com>
Subject: Re: Maximum size of vector that reduce can handle
From: Boromir Widas <vcsubsvc@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, Alexander Ulanov <alexander.ulanov@hp.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f234afdc29c34050da5b669
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f234afdc29c34050da5b669
Content-Type: text/plain; charset=UTF-8

I am running into this issue as well, when storing large Arrays as the
value in a kv pair
and then doing a reducebykey.
Can one of the experts please comment if it would make sense to add an
operation to
add values in place like accumulators do - this would essentially merge the
vectors for
a given key in place, avoiding multiple allocations of temp array/vectors.
This should
be faster for datasets with frequently repeated keys.

On Tue, Jan 27, 2015 at 11:03 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> 60m-vector costs 480MB memory. You have 12 of them to be reduced to the
> driver. So you need ~6GB memory not counting the temp vectors generated
> from '_+_'. You need to increase driver memory to make it work. That being
> said, ~10^7 hits the limit for the current impl of glm. -Xiangrui
> On Jan 23, 2015 2:19 PM, "DB Tsai" <dbtsai@dbtsai.com> wrote:
>
> > Hi Alexander,
> >
> > For `reduce`, it's an action that will collect all the data from
> > mapper to driver, and perform the aggregation in driver. As a result,
> > if the output from the mapper is very large, and the numbers of
> > partitions in mapper are large, it might cause a problem.
> >
> > For `treeReduce`, as the name indicates, the way it works is in the
> > first layer, it aggregates the output of the mappers two by two
> > resulting half of the numbers of output. And then, we continuously do
> > the aggregation layer by layer. The final aggregation will be done in
> > driver but in this time, the numbers of data are small.
> >
> > By default, depth 2 is used, so if you have so many partitions of
> > large vector, this may still cause issue. You can increase the depth
> > into higher numbers such that in the final reduce in driver, the
> > number of partitions are very small.
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> >
> > On Fri, Jan 23, 2015 at 12:07 PM, Ulanov, Alexander
> > <alexander.ulanov@hp.com> wrote:
> > > Hi DB Tsai,
> > >
> > > Thank you for your suggestion. Actually, I've started my experiments
> > with "treeReduce". Originally, I had "vv.treeReduce(_ + _, 2)" in my
> script
> > exactly because MLlib optimizers are using it, as you pointed out with
> > LBFGS. However, it leads to the same problems as "reduce", but presumably
> > not so directly. As far as I understand, treeReduce limits the number of
> > communications between workers and master forcing workers to partially
> > compute the reduce operation.
> > >
> > > Are you sure that driver will first collect all results (or all partial
> > results in treeReduce) and ONLY then perform aggregation? If that is the
> > problem, then how to force it to do aggregation after receiving each
> > portion of data from Workers?
> > >
> > > Best regards, Alexander
> > >
> > > -----Original Message-----
> > > From: DB Tsai [mailto:dbtsai@dbtsai.com]
> > > Sent: Friday, January 23, 2015 11:53 AM
> > > To: Ulanov, Alexander
> > > Cc: dev@spark.apache.org
> > > Subject: Re: Maximum size of vector that reduce can handle
> > >
> > > Hi Alexander,
> > >
> > > When you use `reduce` to aggregate the vectors, those will actually be
> > pulled into driver, and merged over there. Obviously, it's not scaleable
> > given you are doing deep neural networks which have so many coefficients.
> > >
> > > Please try treeReduce instead which is what we do in linear regression
> > and logistic regression.
> > >
> > > See
> >
> https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/optimization/LBFGS.scala
> > > for example.
> > >
> > > val (gradientSum, lossSum) = data.treeAggregate((Vectors.zeros(n),
> > 0.0))( seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
> > features)) => val l = localGradient.compute( features, label, bcW.value,
> > grad) (grad, loss + l) }, combOp = (c1, c2) => (c1, c2) match { case
> > ((grad1, loss1), (grad2, loss2)) => axpy(1.0, grad2, grad1) (grad1,
> loss1 +
> > loss2)
> > > })
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > >
> > > On Fri, Jan 23, 2015 at 10:00 AM, Ulanov, Alexander <
> > alexander.ulanov@hp.com> wrote:
> > >> Dear Spark developers,
> > >>
> > >> I am trying to measure the Spark reduce performance for big vectors.
> My
> > motivation is related to machine learning gradient. Gradient is a vector
> > that is computed on each worker and then all results need to be summed up
> > and broadcasted back to workers. For example, present machine learning
> > applications involve very long parameter vectors, for deep neural
> networks
> > it can be up to 2Billions. So, I want to measure the time that is needed
> > for this operation depending on the size of vector and number of
> workers. I
> > wrote few lines of code that assume that Spark will distribute partitions
> > among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4
> cores,
> > 16GB RAM), each runs 2 Workers.
> > >>
> > >> import org.apache.spark.mllib.rdd.RDDFunctions._
> > >> import breeze.linalg._
> > >> import org.apache.log4j._
> > >> Logger.getRootLogger.setLevel(Level.OFF)
> > >> val n = 60000000
> > >> val p = 12
> > >> val vv = sc.parallelize(0 until p, p).map(i =>
> > >> DenseVector.rand[Double]( n )) vv.reduce(_ + _)
> > >>
> > >> When executing in shell with 60M vector it crashes after some period
> of
> > time. One of the node contains the following in stdout:
> > >> Java HotSpot(TM) 64-Bit Server VM warning: INFO:
> > >> os::commit_memory(0x0000000755500000, 2863661056, 0) failed;
> > >> error='Cannot allocate memory' (errno=12) # # There is insufficient
> > memory for the Java Runtime Environment to continue.
> > >> # Native memory allocation (malloc) failed to allocate 2863661056
> bytes
> > for committing reserved memory.
> > >>
> > >> I run shell with --executor-memory 8G --driver-memory 8G, so handling
> > 60M vector of Double should not be a problem. Are there any big overheads
> > for this? What is the maximum size of vector that reduce can handle?
> > >>
> > >> Best regards, Alexander
> > >>
> > >> P.S.
> > >>
> > >> "spark.driver.maxResultSize 0" needs to set in order to run this code.
> > I also needed to change "java.io.tmpdir" and "spark.local.dir" folders
> > because my /tmp folder which is default, was too small and Spark swaps
> > heavily into this folder. Without these settings I get either "no space
> > left on device" or "out of memory" exceptions.
> > >>
> > >> I also submitted a bug
> > >> https://issues.apache.org/jira/browse/SPARK-5386
> > >>
> > >> ---------------------------------------------------------------------
> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
> > >> additional commands, e-mail: dev-help@spark.apache.org
> > >>
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--e89a8f234afdc29c34050da5b669--

From dev-return-11301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 17:56:13 2015
Return-Path: <dev-return-11301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00B8F101DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 17:56:13 +0000 (UTC)
Received: (qmail 67971 invoked by uid 500); 27 Jan 2015 17:56:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67908 invoked by uid 500); 27 Jan 2015 17:56:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67894 invoked by uid 99); 27 Jan 2015 17:56:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:56:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 17:56:07 +0000
Received: by mail-oi0-f45.google.com with SMTP id g201so13570385oib.4
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 09:55:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=93akd6KuDj0L2ICHI+qDBOipLC0F0108DfeilE0vN7I=;
        b=pA+YAADpO5ltj6xRT2v5G8CJIwtU5Mq0/CNhkpQg2pci+b6rD62ZJ6CJ9uCyy8P3l3
         7eWxbttlt4a7P5p0pKEeeiUBuAgrcmIL4mVRf0i8na17hZebQ9P0SQ40GcVlbg1o59ks
         WolX3wOvh+UiLL8CG989dGIFr6WHSmHmYxuZCJkRWt1P0X1V0d9BINfsaFHBMSdl+mmA
         dFpePdia4LNoO5vqNHd5KuqBO9isYkOWq81N0Cl2T6f+RB52oKjh+D4a76ysZHmj4TZ4
         TiNhHhszLKEb74FI/Hbuo0lus5CrWkuUGBm21s/mKwNjgWxBGBfRESLmJn/lYpncvUUa
         w8Gg==
MIME-Version: 1.0
X-Received: by 10.182.71.73 with SMTP id s9mr1589919obu.15.1422381347420; Tue,
 27 Jan 2015 09:55:47 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Tue, 27 Jan 2015 09:55:47 -0800 (PST)
In-Reply-To: <CAMAsSdLc+jJmT8LMid06dSY3AuSqWMp7UiTqDSQ1smeovpVd6Q@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
	<CAMAsSdLc+jJmT8LMid06dSY3AuSqWMp7UiTqDSQ1smeovpVd6Q@mail.gmail.com>
Date: Tue, 27 Jan 2015 09:55:47 -0800
Message-ID: <CABPQxsu8rUMjp7jEJda2sqRRTaHpVANuN+7Th3TC+V9q2qkWqw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

The release script generates hashes in two places (take a look a bit
further down in the script), one for the published artifacts and the
other for the binaries. In the case of the binaries we use SHA512
because, AFAIK, the ASF does not require you to use SHA1 and SHA512 is
better. In the case of the published Maven artifacts we use SHA1
because my understanding is this is what Maven requires. However, it
does appear that the format is now one that maven cannot parse.

Anyways, it seems fine to just change the format of the hash per your PR.

- Patrick

On Tue, Jan 27, 2015 at 5:00 AM, Sean Owen <sowen@cloudera.com> wrote:
> I think there are several signing / hash issues that should be fixed
> before this release.
>
> Hashes:
>
> http://issues.apache.org/jira/browse/SPARK-5308
> https://github.com/apache/spark/pull/4161
>
> The hashes here are correct, but have two issues:
>
> As noted in the JIRA, the format of the hash file is "nonstandard" --
> at least, doesn't match what Maven outputs, and apparently which tools
> like Leiningen expect, which is just the hash with no file name or
> spaces. There are two ways to fix that: different command-line tools
> (see PR), or, just ask Maven to generate these hashes (a different,
> easy PR).
>
> However, is the script I modified above used to generate these hashes?
> It's generating SHA1 sums, but the output in this release candidate
> has (correct) SHA512 sums.
>
> This may be more than a nuisance, since last time for some reason
> Maven Central did not register the project hashes.
>
> http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.2.0%7Cjar
> does not show them but they exist:
> http://www.us.apache.org/dist/spark/spark-1.2.0/
>
> It may add up to a problem worth rooting out before this release.
>
>
> Signing:
>
> As noted in https://issues.apache.org/jira/browse/SPARK-5299 there are
> two signing keys in
> https://people.apache.org/keys/committer/pwendell.asc (9E4FE3AF,
> 00799F7E) but only one is in http://www.apache.org/dist/spark/KEYS
>
> However, these artifacts seem to be signed by FC8ED089 which isn't in either.
>
> Details details, but I'd say non-binding -1 at the moment.
>
>
> On Tue, Jan 27, 2015 at 7:02 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>>
>> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1061/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.2.1!
>>
>> The vote is open until Friday, January 30, at 07:00 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.2.1
>> [ ] -1 Do not release this package because ...
>>
>> For a list of fixes in this release, see http://s.apache.org/Mpn.
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:01:29 2015
Return-Path: <dev-return-11302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F06810211
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:01:29 +0000 (UTC)
Received: (qmail 88491 invoked by uid 500); 27 Jan 2015 18:01:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88417 invoked by uid 500); 27 Jan 2015 18:01:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88406 invoked by uid 99); 27 Jan 2015 18:01:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:01:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:01:21 +0000
Received: by mail-wi0-f171.google.com with SMTP id l15so6625337wiw.4
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 10:01:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZBk9Rw1F+4FPtLug7pBvwjABl7aeAilZsKygT+/ukMs=;
        b=m1EdQKVCCgrSPHCtWVwops//7cLMIUnX8qDIPQhnyGcGHqQTJlFn7QU8wUkYSE9SO4
         /FzBQChT1c3zAgwuGcZ7ayA1bAsqBF8pT/SEmAAraUvkL6zfsehB0mlluSSlAPtE/t8d
         +Ap/TMqT41NKL/F5uTGomvZPq1Wl4xrrWOV/JMJgbtACCRrCAW9rbz4sboGNsjYlMzfw
         Lo9T/IjpSw1tIC4wMd5MM9Zu9DI5v0S0CfDSVirjK3PnuIk1AhliKq9seuctHa2uBdu7
         NIEtmRGG+OYw9ehY8FpOMSbe2kcanVcilKszsTN7CWb3/IPC3CBUPsIzmvSzshAYeQih
         3ubQ==
X-Gm-Message-State: ALoCoQmeYEWYpIHMLfX6fTdDNAK5nzy5xpR9j5Vw58FYf4DWOAMQruPyL20NnT/XX8LTvZiMjt3/
X-Received: by 10.194.206.97 with SMTP id ln1mr266014wjc.112.1422381660263;
 Tue, 27 Jan 2015 10:01:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Tue, 27 Jan 2015 10:00:40 -0800 (PST)
In-Reply-To: <CABPQxsu8rUMjp7jEJda2sqRRTaHpVANuN+7Th3TC+V9q2qkWqw@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
 <CAMAsSdLc+jJmT8LMid06dSY3AuSqWMp7UiTqDSQ1smeovpVd6Q@mail.gmail.com> <CABPQxsu8rUMjp7jEJda2sqRRTaHpVANuN+7Th3TC+V9q2qkWqw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 27 Jan 2015 18:00:40 +0000
Message-ID: <CAMAsSdLqT0T4oajzVTtR4HFPhZ=R4DBJRkNyNpmR8irWp=uGPA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Got it. Ignore the SHA512 issue since these aren't somehow expected by
a policy or Maven to be in a certain format. Just wondered if the
difference was intended.

The Maven way of generated the SHA1 hashes is to set this on the
install plugin, AFAIK, although I'm not sure if the intent was to hash
files that Maven didn't create:

<configuration>
    <createChecksum>true</createChecksum>
</configuration>

As for the key issue, I think it's just a matter of uploading the new
key in both places.

We should all of course test the release anyway.

On Tue, Jan 27, 2015 at 5:55 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Sean,
>
> The release script generates hashes in two places (take a look a bit
> further down in the script), one for the published artifacts and the
> other for the binaries. In the case of the binaries we use SHA512
> because, AFAIK, the ASF does not require you to use SHA1 and SHA512 is
> better. In the case of the published Maven artifacts we use SHA1
> because my understanding is this is what Maven requires. However, it
> does appear that the format is now one that maven cannot parse.
>
> Anyways, it seems fine to just change the format of the hash per your PR.
>
> - Patrick
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:01:31 2015
Return-Path: <dev-return-11303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED8EF10212
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:01:31 +0000 (UTC)
Received: (qmail 90098 invoked by uid 500); 27 Jan 2015 18:01:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90026 invoked by uid 500); 27 Jan 2015 18:01:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90010 invoked by uid 99); 27 Jan 2015 18:01:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:01:30 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 72.30.239.79 as permitted sender)
Received: from [72.30.239.79] (HELO nm34-vm7.bullet.mail.bf1.yahoo.com) (72.30.239.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:01:25 +0000
Received: from [66.196.81.173] by nm34.bullet.mail.bf1.yahoo.com with NNFMP; 27 Jan 2015 18:00:01 -0000
Received: from [98.139.212.211] by tm19.bullet.mail.bf1.yahoo.com with NNFMP; 27 Jan 2015 18:00:01 -0000
Received: from [127.0.0.1] by omp1020.mail.bf1.yahoo.com with NNFMP; 27 Jan 2015 18:00:01 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 438904.47945.bm@omp1020.mail.bf1.yahoo.com
X-YMail-OSG: 2vyRyLgVM1nD3.g1tZciTaMD690z0S1YZg1LAV2MxNJ_xMK1OCqqjYEX7F6_yU9
 KbX3JD51ZbQbhJ2lJQYL1wpfYEs41x399.3Ip6sfuqVQWPzBY9TqNCfX3bUqB5_FPTc_hEnUp9KC
 6sKa8BAUaYST6.XVFztFqwgNCVMYlGjCXMwQ7xOysWiu4BAnXK_qU_EugDkyD6Yhs1V1DF7hXLaW
 sDFGZz_dmZ5KAlIXnuZh5VmPXYOXRFMCLaRxr0rCA_RYEW70ZkEXmV3ddVP_GPBe1psIMXKH7zYP
 f7sGBUDah0KBM9Iidym3D6rllsAPhWHTkzYrQqVj62SWH45MOofULS6x8x3vVTNJ4WZ2dcXfEGF2
 B_hJOot91FL7MU.iDNnjJCx1LYJi2xafozR9xWM71_TkAJNTrc0ebQWn4jkJQ6KS7SxCOp._M04Q
 uohXxP5deiv_STSZFReyla341HlaGXpMgkI1MkTmj8ooHjIlBl_yzdc1WphTuRVW_0j4pZRKvO_I
 84XA9JjTaAxCcM82JD0oxIR3VjPxIsAoJU1EHteZLFKqV7lhfsfPub4VClgsD9bzHEvZFQ2PXjPb
 KOSI1L6kdqkUI_RFwKDoEyegAsP0lG_eKLoicBKjMyiguvxW.MCdyMmLQn74FsvpDPvzyt6IMvfF
 aDQK21KMUCSzPVOsTGgPN1FF7
Received: by 76.13.26.143; Tue, 27 Jan 2015 18:00:00 +0000 
Date: Tue, 27 Jan 2015 17:59:47 +0000 (UTC)
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>
Cc: Koert Kuipers <koert@tresata.com>, Patrick Wendell <pwendell@gmail.com>, 
	Reynold Xin <rxin@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <506680481.1330933.1422381587859.JavaMail.yahoo@mail.yahoo.com>
In-Reply-To: <CABjXkq6tYkc8DcN=HGsMepPvXLKk2ycgJpJ+EabJG=iWo8vG0A@mail.gmail.com>
References: <CABjXkq6tYkc8DcN=HGsMepPvXLKk2ycgJpJ+EabJG=iWo8vG0A@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I personally have no preference DataFrame vs. DataTable, but only wish to lay out the history and etymology simply because I'm into that sort of thing.

"Frame" comes from Marvin Minsky's 1970's AI construct: "slots" and the data that go in them. The S programming language (precursor to R) adopted this terminology in 1991. R of course became popular with the rise of Data Science around 2012.
http://www.google.com/trends/explore#q=%22data%20science%22%2C%20%22r%20programming%22&cmpt=q&tz=

"DataFrame" would carry the implication that it comes along with its own metadata, whereas "DataTable" might carry the implication that metadata is stored in a central metadata repository.

"DataFrame" is thus technically more correct for SchemaRDD, but is a less familiar (and thus less accessible) term for those not immersed in data science or AI and thus may have narrower appeal.


----- Original Message -----
From: Evan R. Sparks <evan.sparks@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Koert Kuipers <koert@tresata.com>; Michael Malak <michaelmalak@yahoo.com>; Patrick Wendell <pwendell@gmail.com>; Reynold Xin <rxin@databricks.com>; "dev@spark.apache.org" <dev@spark.apache.org>
Sent: Tuesday, January 27, 2015 9:55 AM
Subject: Re: renaming SchemaRDD -> DataFrame

I'm +1 on this, although a little worried about unknowingly introducing
SparkSQL dependencies every time someone wants to use this. It would be
great if the interface can be abstract and the implementation (in this
case, SparkSQL backend) could be swapped out.

One alternative suggestion on the name - why not call it DataTable?
DataFrame seems like a name carried over from pandas (and by extension, R),
and it's never been obvious to me what a "Frame" is.



On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> (Actually when we designed Spark SQL we thought of giving it another name,
> like Spark Schema, but we decided to stick with SQL since that was the most
> obvious use case to many users.)
>
> Matei
>
> > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >
> > While it might be possible to move this concept to Spark Core long-term,
> supporting structured data efficiently does require quite a bit of the
> infrastructure in Spark SQL, such as query planning and columnar storage.
> The intent of Spark SQL though is to be more than a SQL server -- it's
> meant to be a library for manipulating structured data. Since this is
> possible to build over the core API, it's pretty natural to organize it
> that way, same as Spark Streaming is a library.
> >
> > Matei
> >
> >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
> >>
> >> "The context is that SchemaRDD is becoming a common data format used for
> >> bringing data into Spark from external systems, and used for various
> >> components of Spark, e.g. MLlib's new pipeline API."
> >>
> >> i agree. this to me also implies it belongs in spark core, not sql
> >>
> >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> michaelmalak@yahoo.com.invalid> wrote:
> >>
> >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
> Area
> >>> Spark Meetup YouTube contained a wealth of background information on
> this
> >>> idea (mostly from Patrick and Reynold :-).
> >>>
> >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >>>
> >>> ________________________________
> >>> From: Patrick Wendell <pwendell@gmail.com>
> >>> To: Reynold Xin <rxin@databricks.com>
> >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >>> Sent: Monday, January 26, 2015 4:01 PM
> >>> Subject: Re: renaming SchemaRDD -> DataFrame
> >>>
> >>>
> >>> One thing potentially not clear from this e-mail, there will be a 1:1
> >>> correspondence where you can get an RDD to/from a DataFrame.
> >>>
> >>>
> >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>>> Hi,
> >>>>
> >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
> to
> >>>> get the community's opinion.
> >>>>
> >>>> The context is that SchemaRDD is becoming a common data format used
> for
> >>>> bringing data into Spark from external systems, and used for various
> >>>> components of Spark, e.g. MLlib's new pipeline API. We also expect
> more
> >>> and
> >>>> more users to be programming directly against SchemaRDD API rather
> than
> >>> the
> >>>> core RDD API. SchemaRDD, through its less commonly used DSL originally
> >>>> designed for writing test cases, always has the data-frame like API.
> In
> >>>> 1.3, we are redesigning the API to make the API usable for end users.
> >>>>
> >>>>
> >>>> There are two motivations for the renaming:
> >>>>
> >>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
> >>>>
> >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
> (even
> >>>> though it would contain some RDD functions like map, flatMap, etc),
> and
> >>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
> >>> Instead.
> >>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
> >>>>
> >>>>
> >>>> My understanding is that very few users program directly against the
> >>>> SchemaRDD API at the moment, because they are not well documented.
> >>> However,
> >>>> oo maintain backward compatibility, we can create a type alias
> DataFrame
> >>>> that is still named SchemaRDD. This will maintain source compatibility
> >>> for
> >>>> Scala. That said, we will have to update all existing materials to use
> >>>> DataFrame rather than SchemaRDD.
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org

> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>>
> >
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:10:58 2015
Return-Path: <dev-return-11304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2FE7C10270
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:10:58 +0000 (UTC)
Received: (qmail 20470 invoked by uid 500); 27 Jan 2015 18:10:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20397 invoked by uid 500); 27 Jan 2015 18:10:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20384 invoked by uid 99); 27 Jan 2015 18:10:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:10:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:10:27 +0000
Received: by mail-oi0-f43.google.com with SMTP id z81so13659243oif.2
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 10:10:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=feniUl5/txDREBdbUw3MYPWKkdvFiC4lDRUsozA404Q=;
        b=o4j3qtb+rLWIMhYzA/xAuMDP/nuwGPcSwXm3KAvasN7lOZuTgd0b/jFuEEFO1uVSbe
         xoluwVOHcIhw/wM97fsDPtOGtCdiUNKVOKctw3RpQOQRw7x7nSZhzU/3jVLgpT0F2lhS
         a5AabGyWBJjS8gh7pNgy1kZVWSo/2DHDGSalzlbwnEoUftXp6Ym39Jhf0SZ+fwclxdeE
         y6iYR5UfngczcY53QEdwjUiOVveRQqG0+37QRPYjWwElLxcYKyqqGURQ0IgHrkVktjLl
         buLOjAuvTo3nYqtqRXF2Dz0gpezSNGBlFRfAntVVoHH2wRw1Xr2PlOpwJyXt2uxuNPRm
         BPrA==
MIME-Version: 1.0
X-Received: by 10.202.185.198 with SMTP id j189mr1547035oif.72.1422382225407;
 Tue, 27 Jan 2015 10:10:25 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Tue, 27 Jan 2015 10:10:25 -0800 (PST)
In-Reply-To: <CAMAsSdLqT0T4oajzVTtR4HFPhZ=R4DBJRkNyNpmR8irWp=uGPA@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
	<CAMAsSdLc+jJmT8LMid06dSY3AuSqWMp7UiTqDSQ1smeovpVd6Q@mail.gmail.com>
	<CABPQxsu8rUMjp7jEJda2sqRRTaHpVANuN+7Th3TC+V9q2qkWqw@mail.gmail.com>
	<CAMAsSdLqT0T4oajzVTtR4HFPhZ=R4DBJRkNyNpmR8irWp=uGPA@mail.gmail.com>
Date: Tue, 27 Jan 2015 10:10:25 -0800
Message-ID: <CABPQxstE90Oo+TY_9bpnATGeA+h=yeY3c8fUWbRUGNB4fBfm_Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yes - the key issue is just due to me creating new keys this time
around. Anyways let's take another stab at this. In the mean time,
please don't hesitate to test the release itself.

- Patrick

On Tue, Jan 27, 2015 at 10:00 AM, Sean Owen <sowen@cloudera.com> wrote:
> Got it. Ignore the SHA512 issue since these aren't somehow expected by
> a policy or Maven to be in a certain format. Just wondered if the
> difference was intended.
>
> The Maven way of generated the SHA1 hashes is to set this on the
> install plugin, AFAIK, although I'm not sure if the intent was to hash
> files that Maven didn't create:
>
> <configuration>
>     <createChecksum>true</createChecksum>
> </configuration>
>
> As for the key issue, I think it's just a matter of uploading the new
> key in both places.
>
> We should all of course test the release anyway.
>
> On Tue, Jan 27, 2015 at 5:55 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Hey Sean,
>>
>> The release script generates hashes in two places (take a look a bit
>> further down in the script), one for the published artifacts and the
>> other for the binaries. In the case of the binaries we use SHA512
>> because, AFAIK, the ASF does not require you to use SHA1 and SHA512 is
>> better. In the case of the published Maven artifacts we use SHA1
>> because my understanding is this is what Maven requires. However, it
>> does appear that the format is now one that maven cannot parse.
>>
>> Anyways, it seems fine to just change the format of the hash per your PR.
>>
>> - Patrick
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:33:19 2015
Return-Path: <dev-return-11305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A140E10361
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:33:19 +0000 (UTC)
Received: (qmail 77711 invoked by uid 500); 27 Jan 2015 18:33:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77634 invoked by uid 500); 27 Jan 2015 18:33:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77623 invoked by uid 99); 27 Jan 2015 18:33:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:33:17 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.22 as permitted sender)
Received: from [216.64.169.22] (HELO pdxmta01.webtrends.com) (216.64.169.22)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:32:51 +0000
Received: from pdxex1.webtrends.corp (Not Verified[10.61.2.220]) by pdxmta01.webtrends.com with MailMarshal (v7,2,3,6978) (using TLS: SSLv23)
	id <B54c7d99d0000>; Tue, 27 Jan 2015 18:31:57 +0000
Received: from PDXEX2.WebTrends.corp ([172.27.3.221]) by pdxex1.webtrends.corp
 ([172.27.5.220]) with mapi id 14.03.0224.002; Tue, 27 Jan 2015 18:31:45 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
Thread-Topic: [VOTE] Release Apache Spark 1.2.1 (RC1)
Thread-Index: AQHQOf9z2dNycqF5wE2MnM7j4sZ/x5zUSwEA
Date: Tue, 27 Jan 2015 18:31:44 +0000
Message-ID: <DFDE5570-8E7B-49CE-8E8B-6C95D20360AE@webtrends.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
In-Reply-To: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="Windows-1252"
Content-ID: <1DC94A9D960CCC45B1DAA73C149CA91B@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

We=92re using spark on scala 2.11 /w hadoop2.4.  Would it be practical / ma=
ke sense to build a bin version of spark against scala 2.11 for versions ot=
her than just hadoop1 at this time?

Cheers,

Sean


> On Jan 27, 2015, at 12:04 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>=20
> Please vote on releasing the following candidate as Apache Spark version =
1.2.1!
>=20
> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D3e2d=
7d310b76c293b9ac787f204e6880f508f6ec
>=20
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>=20
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1061/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.2.1!
>=20
> The vote is open until Friday, January 30, at 07:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>=20
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> - Patrick
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:35:37 2015
Return-Path: <dev-return-11306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8BE8103AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:35:37 +0000 (UTC)
Received: (qmail 92396 invoked by uid 500); 27 Jan 2015 18:35:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92329 invoked by uid 500); 27 Jan 2015 18:35:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92315 invoked by uid 99); 27 Jan 2015 18:35:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:35:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:35:12 +0000
Received: by mail-oi0-f48.google.com with SMTP id v63so13774305oia.7
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 10:35:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=yikCcpCQxmXd0N84m8pQeFJn1nqLXrryDGZHjl1/yYw=;
        b=F6H99fCu7CheZ9MjqrLDpXvWytoM08svgSqg5R5WPznBX/xC3AXGqb7AMHVws7p5Wu
         vA8LD69Ty5rkdlXzhZfxpGQHmOYw4kQiyC0zKncrN6+q5Bcp9aF6WrWQ3GYGSrybG6YL
         r5tD6RvUeAwYdYLDR1soTbvd5nRP7oTH6tiNN2qZy1OsWfbdI+Qjaf9YHTKCEKcjCeU6
         A4FItrb2QAHKIGOpG1KKhRpkjxRgXUgI6CivpHTogcpyXtCtvCG9MzTtXXoRGVxdg/XR
         tIKppWK5abjrI4V/nDxJ+DYns9eQvtE0yTKNdLAQJKstrfWaY+KClwSq5yFvvAKV1NBt
         b3Dg==
MIME-Version: 1.0
X-Received: by 10.182.27.241 with SMTP id w17mr1674533obg.14.1422383710497;
 Tue, 27 Jan 2015 10:35:10 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Tue, 27 Jan 2015 10:35:10 -0800 (PST)
In-Reply-To: <DFDE5570-8E7B-49CE-8E8B-6C95D20360AE@webtrends.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
	<DFDE5570-8E7B-49CE-8E8B-6C95D20360AE@webtrends.com>
Date: Tue, 27 Jan 2015 10:35:10 -0800
Message-ID: <CABPQxssuLUAauGd9t3rxRrL2y83BN5sxY2osrZM8TzdgJOriLw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean McNamara <Sean.McNamara@webtrends.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

Right now we don't publish every 2.11 binary to avoid combinatorial
explosion of the number of build artifacts we publish (there are other
parameters such as whether hive is included, etc). We can revisit this
in future feature releases, but .1 releases like this are reserved for
bug fixes.

- Patrick

On Tue, Jan 27, 2015 at 10:31 AM, Sean McNamara
<Sean.McNamara@webtrends.com> wrote:
> We're using spark on scala 2.11 /w hadoop2.4.  Would it be practical / make sense to build a bin version of spark against scala 2.11 for versions other than just hadoop1 at this time?
>
> Cheers,
>
> Sean
>
>
>> On Jan 27, 2015, at 12:04 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>>
>> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1061/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.2.1!
>>
>> The vote is open until Friday, January 30, at 07:00 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.2.1
>> [ ] -1 Do not release this package because ...
>>
>> For a list of fixes in this release, see http://s.apache.org/Mpn.
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 18:45:25 2015
Return-Path: <dev-return-11307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 428C7104B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 18:45:25 +0000 (UTC)
Received: (qmail 59938 invoked by uid 500); 27 Jan 2015 18:45:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59842 invoked by uid 500); 27 Jan 2015 18:45:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59826 invoked by uid 99); 27 Jan 2015 18:45:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:45:24 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.23 as permitted sender)
Received: from [216.64.169.23] (HELO pdxmta02.webtrends.com) (216.64.169.23)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 18:44:58 +0000
Received: from pdxex1.webtrends.corp (Not Verified[10.61.2.220]) by pdxmta02.webtrends.com with MailMarshal (v7,2,3,6978) (using TLS: SSLv23)
	id <B54c7dc740000>; Tue, 27 Jan 2015 18:44:04 +0000
Received: from PDXEX2.WebTrends.corp ([172.27.3.221]) by pdxex1.webtrends.corp
 ([172.27.5.220]) with mapi id 14.03.0224.002; Tue, 27 Jan 2015 18:43:52 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: Sean McNamara <Sean.McNamara@Webtrends.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
Thread-Topic: [VOTE] Release Apache Spark 1.2.1 (RC1)
Thread-Index: AQHQOf9z2dNycqF5wE2MnM7j4sZ/x5zUSwEAgAAA9QCAAAJpgA==
Date: Tue, 27 Jan 2015 18:43:52 +0000
Message-ID: <8F75F2BB-D697-4346-A5C5-0A7317A6D989@webtrends.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
 <DFDE5570-8E7B-49CE-8E8B-6C95D20360AE@webtrends.com>
 <CABPQxssuLUAauGd9t3rxRrL2y83BN5sxY2osrZM8TzdgJOriLw@mail.gmail.com>
In-Reply-To: <CABPQxssuLUAauGd9t3rxRrL2y83BN5sxY2osrZM8TzdgJOriLw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="iso-8859-1"
Content-ID: <D3EAA06037CC0249989D1AE3DB555C89@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Sounds good, that makes sense.

Cheers,

Sean

> On Jan 27, 2015, at 11:35 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>=20
> Hey Sean,
>=20
> Right now we don't publish every 2.11 binary to avoid combinatorial
> explosion of the number of build artifacts we publish (there are other
> parameters such as whether hive is included, etc). We can revisit this
> in future feature releases, but .1 releases like this are reserved for
> bug fixes.
>=20
> - Patrick
>=20
> On Tue, Jan 27, 2015 at 10:31 AM, Sean McNamara
> <Sean.McNamara@webtrends.com> wrote:
>> We're using spark on scala 2.11 /w hadoop2.4.  Would it be practical / m=
ake sense to build a bin version of spark against scala 2.11 for versions o=
ther than just hadoop1 at this time?
>>=20
>> Cheers,
>>=20
>> Sean
>>=20
>>=20
>>> On Jan 27, 2015, at 12:04 AM, Patrick Wendell <pwendell@gmail.com> wrot=
e:
>>>=20
>>> Please vote on releasing the following candidate as Apache Spark versio=
n 1.2.1!
>>>=20
>>> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D3e=
2d7d310b76c293b9ac787f204e6880f508f6ec
>>>=20
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>>>=20
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>=20
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1061/
>>>=20
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>>>=20
>>> Please vote on releasing this package as Apache Spark 1.2.1!
>>>=20
>>> The vote is open until Friday, January 30, at 07:00 UTC and passes
>>> if a majority of at least 3 +1 PMC votes are cast.
>>>=20
>>> [ ] +1 Release this package as Apache Spark 1.2.1
>>> [ ] -1 Do not release this package because ...
>>>=20
>>> For a list of fixes in this release, see http://s.apache.org/Mpn.
>>>=20
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>=20
>>> - Patrick
>>>=20
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 19:15:17 2015
Return-Path: <dev-return-11308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 195011069F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 19:15:17 +0000 (UTC)
Received: (qmail 84576 invoked by uid 500); 27 Jan 2015 19:15:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84510 invoked by uid 500); 27 Jan 2015 19:15:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84452 invoked by uid 99); 27 Jan 2015 19:15:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:15:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:14:45 +0000
Received: by mail-oi0-f41.google.com with SMTP id z81so13971947oif.0
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 11:13:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=wQvGT7MAYa/Q8VZRcbbb8vhvQQAzHl4S9dToFqxkOrk=;
        b=drqy0VNwut3SyGFK7uGGbj02VzxyWKkWqJsLKWiGONmIF8MTilvKTz15GUXJW7eWEs
         RVuoB4NsTWwLPssTG0ltGmMAsr5CTgYGZ2V5PIjIOdY8KykmbxT0KmXXsd786KOJRwxt
         oUa1qlkuyggEGD05WxJfRKtROFDQMrLumZXTGAiRjv9JHYjlCXRm9uWTqb1T3wSSjg0V
         5AUszIAmsqPy4WZlJ31wOmV8NkwkMPY8+7/P9wc4FeWqjE4VyDrgpCm8Zic62b4WR+V0
         tM7uq3jA9675JG2t9YcrBiyPJpbcheZyDyh39nIW3+2S+0VMndG3NnY0AxhH2qd9ikHk
         2/tQ==
MIME-Version: 1.0
X-Received: by 10.202.223.3 with SMTP id w3mr1642733oig.82.1422386038336; Tue,
 27 Jan 2015 11:13:58 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Tue, 27 Jan 2015 11:13:58 -0800 (PST)
In-Reply-To: <8F75F2BB-D697-4346-A5C5-0A7317A6D989@webtrends.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
	<DFDE5570-8E7B-49CE-8E8B-6C95D20360AE@webtrends.com>
	<CABPQxssuLUAauGd9t3rxRrL2y83BN5sxY2osrZM8TzdgJOriLw@mail.gmail.com>
	<8F75F2BB-D697-4346-A5C5-0A7317A6D989@webtrends.com>
Date: Tue, 27 Jan 2015 11:13:58 -0800
Message-ID: <CABPQxsv7nCtst2p3yhLP8yZSMg6j4DHEFLp0_x+hC7DmhGoJig@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean McNamara <Sean.McNamara@webtrends.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Okay - we've resolved all issues with the signatures and keys.
However, I'll leave the current vote open for a bit to solicit
additional feedback.

On Tue, Jan 27, 2015 at 10:43 AM, Sean McNamara
<Sean.McNamara@webtrends.com> wrote:
> Sounds good, that makes sense.
>
> Cheers,
>
> Sean
>
>> On Jan 27, 2015, at 11:35 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> Hey Sean,
>>
>> Right now we don't publish every 2.11 binary to avoid combinatorial
>> explosion of the number of build artifacts we publish (there are other
>> parameters such as whether hive is included, etc). We can revisit this
>> in future feature releases, but .1 releases like this are reserved for
>> bug fixes.
>>
>> - Patrick
>>
>> On Tue, Jan 27, 2015 at 10:31 AM, Sean McNamara
>> <Sean.McNamara@webtrends.com> wrote:
>>> We're using spark on scala 2.11 /w hadoop2.4.  Would it be practical / make sense to build a bin version of spark against scala 2.11 for versions other than just hadoop1 at this time?
>>>
>>> Cheers,
>>>
>>> Sean
>>>
>>>
>>>> On Jan 27, 2015, at 12:04 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>
>>>> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>>>>
>>>> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>>>>
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>>>>
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1061/
>>>>
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>>>>
>>>> Please vote on releasing this package as Apache Spark 1.2.1!
>>>>
>>>> The vote is open until Friday, January 30, at 07:00 UTC and passes
>>>> if a majority of at least 3 +1 PMC votes are cast.
>>>>
>>>> [ ] +1 Release this package as Apache Spark 1.2.1
>>>> [ ] -1 Do not release this package because ...
>>>>
>>>> For a list of fixes in this release, see http://s.apache.org/Mpn.
>>>>
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>>
>>>> - Patrick
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 19:19:46 2015
Return-Path: <dev-return-11309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 974AD106F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 19:19:46 +0000 (UTC)
Received: (qmail 97043 invoked by uid 500); 27 Jan 2015 19:19:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96976 invoked by uid 500); 27 Jan 2015 19:19:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96965 invoked by uid 99); 27 Jan 2015 19:19:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:19:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:19:41 +0000
Received: by mail-qg0-f51.google.com with SMTP id z107so13219445qgd.10
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 11:19:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=AemFWCpwi8U5leY0p3uemWCtqDF09BNxVSA/c8xvKaI=;
        b=HDSOzKl1oGC0lOh5aazYzVmehIYS2+vFdwfx9CjvG1EgKsYZ7pN1OxLavQDaV+j1Mf
         Wz81yTW5iNDdv2HX8UHJ746Q3OjqBc9j/huS/TIbLJdTUNthoOCu1Jhm8ee+05GFsKAF
         kTtd7V4wLnGWwx++QEmS3pKrSErT4OakNHVnKubmGRXt6IR4Js50ZBTxtWCfYZjq5C5i
         fsjsB787k7nnqVzNrUYEGuUmSGC5Do9Fc8Yky4Dp4qatKo7sXmsZ0FYOsRjfkiK/8QH0
         AmSJm8n8NTyB+HmwhPVJmLlV32JqoabL0dmT4Gqb+butdR1MLONuTPrt+6fBq2DZy/Un
         Bq7w==
X-Gm-Message-State: ALoCoQnwt+AbwIGFK8iJ8jiWsg6xIfeUPGAUF7UigsrtHWfNLRupXfJ5FUSuidLWV66vhQBUzAqB
X-Received: by 10.224.129.202 with SMTP id p10mr4948731qas.54.1422386340057;
 Tue, 27 Jan 2015 11:19:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 27 Jan 2015 11:18:39 -0800 (PST)
In-Reply-To: <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com> <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 Jan 2015 11:18:39 -0800
Message-ID: <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c124886ac4050da72012
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c124886ac4050da72012
Content-Type: text/plain; charset=UTF-8

Dirceu,

That is not possible because one cannot overload return types.

SQLContext.parquetFile (and many other methods) needs to return some type,
and that type cannot be both SchemaRDD and DataFrame.

In 1.3, we will create a type alias for DataFrame called SchemaRDD to not
break source compatibility for Scala.


On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
dirceu.semighini@gmail.com> wrote:

> Can't the SchemaRDD remain the same, but deprecated, and be removed in the
> release 1.5(+/- 1)  for example, and the new code been added to DataFrame?
> With this, we don't impact in existing code for the next few releases.
>
>
>
> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>
> > I want to address the issue that Matei raised about the heavy lifting
> > required for a full SQL support. It is amazing that even after 30 years
> of
> > research there is not a single good open source columnar database like
> > Vertica. There is a column store option in MySQL, but it is not nearly as
> > sophisticated as Vertica or MonetDB. But there's a true need for such a
> > system. I wonder why so and it's high time to change that.
> > On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com> wrote:
> >
> > > Both SchemaRDD and DataFrame sound fine to me, though I like the former
> > > slightly better because it's more descriptive.
> > >
> > > Even if SchemaRDD's needs to rely on Spark SQL under the covers, it
> would
> > > be more clear from a user-facing perspective to at least choose a
> package
> > > name for it that omits "sql".
> > >
> > > I would also be in favor of adding a separate Spark Schema module for
> > Spark
> > > SQL to rely on, but I imagine that might be too large a change at this
> > > point?
> > >
> > > -Sandy
> > >
> > > On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> matei.zaharia@gmail.com>
> > > wrote:
> > >
> > > > (Actually when we designed Spark SQL we thought of giving it another
> > > name,
> > > > like Spark Schema, but we decided to stick with SQL since that was
> the
> > > most
> > > > obvious use case to many users.)
> > > >
> > > > Matei
> > > >
> > > > > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> matei.zaharia@gmail.com>
> > > > wrote:
> > > > >
> > > > > While it might be possible to move this concept to Spark Core
> > > long-term,
> > > > supporting structured data efficiently does require quite a bit of
> the
> > > > infrastructure in Spark SQL, such as query planning and columnar
> > storage.
> > > > The intent of Spark SQL though is to be more than a SQL server --
> it's
> > > > meant to be a library for manipulating structured data. Since this is
> > > > possible to build over the core API, it's pretty natural to organize
> it
> > > > that way, same as Spark Streaming is a library.
> > > > >
> > > > > Matei
> > > > >
> > > > >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
> > wrote:
> > > > >>
> > > > >> "The context is that SchemaRDD is becoming a common data format
> used
> > > for
> > > > >> bringing data into Spark from external systems, and used for
> various
> > > > >> components of Spark, e.g. MLlib's new pipeline API."
> > > > >>
> > > > >> i agree. this to me also implies it belongs in spark core, not sql
> > > > >>
> > > > >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > > > >> michaelmalak@yahoo.com.invalid> wrote:
> > > > >>
> > > > >>> And in the off chance that anyone hasn't seen it yet, the Jan. 13
> > Bay
> > > > Area
> > > > >>> Spark Meetup YouTube contained a wealth of background information
> > on
> > > > this
> > > > >>> idea (mostly from Patrick and Reynold :-).
> > > > >>>
> > > > >>> https://www.youtube.com/watch?v=YWppYPWznSQ
> > > > >>>
> > > > >>> ________________________________
> > > > >>> From: Patrick Wendell <pwendell@gmail.com>
> > > > >>> To: Reynold Xin <rxin@databricks.com>
> > > > >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> > > > >>> Sent: Monday, January 26, 2015 4:01 PM
> > > > >>> Subject: Re: renaming SchemaRDD -> DataFrame
> > > > >>>
> > > > >>>
> > > > >>> One thing potentially not clear from this e-mail, there will be a
> > 1:1
> > > > >>> correspondence where you can get an RDD to/from a DataFrame.
> > > > >>>
> > > > >>>
> > > > >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> rxin@databricks.com>
> > > > wrote:
> > > > >>>> Hi,
> > > > >>>>
> > > > >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
> > > wanted
> > > > to
> > > > >>>> get the community's opinion.
> > > > >>>>
> > > > >>>> The context is that SchemaRDD is becoming a common data format
> > used
> > > > for
> > > > >>>> bringing data into Spark from external systems, and used for
> > various
> > > > >>>> components of Spark, e.g. MLlib's new pipeline API. We also
> expect
> > > > more
> > > > >>> and
> > > > >>>> more users to be programming directly against SchemaRDD API
> rather
> > > > than
> > > > >>> the
> > > > >>>> core RDD API. SchemaRDD, through its less commonly used DSL
> > > originally
> > > > >>>> designed for writing test cases, always has the data-frame like
> > API.
> > > > In
> > > > >>>> 1.3, we are redesigning the API to make the API usable for end
> > > users.
> > > > >>>>
> > > > >>>>
> > > > >>>> There are two motivations for the renaming:
> > > > >>>>
> > > > >>>> 1. DataFrame seems to be a more self-evident name than
> SchemaRDD.
> > > > >>>>
> > > > >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
> anymore
> > > > (even
> > > > >>>> though it would contain some RDD functions like map, flatMap,
> > etc),
> > > > and
> > > > >>>> calling it Schema*RDD* while it is not an RDD is highly
> confusing.
> > > > >>> Instead.
> > > > >>>> DataFrame.rdd will return the underlying RDD for all RDD
> methods.
> > > > >>>>
> > > > >>>>
> > > > >>>> My understanding is that very few users program directly against
> > the
> > > > >>>> SchemaRDD API at the moment, because they are not well
> documented.
> > > > >>> However,
> > > > >>>> oo maintain backward compatibility, we can create a type alias
> > > > DataFrame
> > > > >>>> that is still named SchemaRDD. This will maintain source
> > > compatibility
> > > > >>> for
> > > > >>>> Scala. That said, we will have to update all existing materials
> to
> > > use
> > > > >>>> DataFrame rather than SchemaRDD.
> > > > >>>
> > > > >>>
> > ---------------------------------------------------------------------
> > > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > > > >>>
> > > > >>>
> > ---------------------------------------------------------------------
> > > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > > >>> For additional commands, e-mail: dev-help@spark.apache.org
> > > > >>>
> > > > >>>
> > > > >
> > > >
> > > >
> > > > ---------------------------------------------------------------------
> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > > For additional commands, e-mail: dev-help@spark.apache.org
> > > >
> > > >
> > >
> >
>

--001a11c2c124886ac4050da72012--

From dev-return-11310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 19:22:37 2015
Return-Path: <dev-return-11310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F07A1073B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 19:22:37 +0000 (UTC)
Received: (qmail 11354 invoked by uid 500); 27 Jan 2015 19:22:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11249 invoked by uid 500); 27 Jan 2015 19:22:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10483 invoked by uid 99); 27 Jan 2015 19:22:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:22:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:22:31 +0000
Received: by mail-qa0-f54.google.com with SMTP id w8so12878440qac.13
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 11:20:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=NcnseNzLIA2jN1eyMzS2X2Sh+LbNLydXlsNCJcfLAGQ=;
        b=bzpOYg5dl01FyOwzz4/3uzcE7pkIg58IH1eJmfPoMqTLM1dnZQQIUbcKt12cVFN+yz
         grdDOaMYDiup+RVdphODK5erM+4nzBtXwYhbARd+6z35sLAeS7j1iVwCdSqWUECJIv5M
         HTgVpc1qCWemj4lIZcgY7V/YFl3Tu/X/9l6K3ROIZrFUDUdpHLwH6ztlhVZKP8w34lq+
         Mzwems67Jkv+m0HtDDWqolREzHwzzc3muz/wRtXysxgvfUiDEK9Y3uYwfSzwA86YfktD
         1NnjDEb4JQ3I19cBhU0KEM9ck7aWq3keCFBDZdKpuYqb+hNU+e6EOzUIMYRtiHVM5GnH
         WFzQ==
X-Gm-Message-State: ALoCoQmdja6mO/7CrgFQqJVgG1Akltb8/HWjrnhVWRPrYzodSNChmjqljwoZQ7/3Ywyuss3VOxi6
X-Received: by 10.140.17.70 with SMTP id 64mr4701439qgc.53.1422386420312; Tue,
 27 Jan 2015 11:20:20 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 27 Jan 2015 11:19:59 -0800 (PST)
In-Reply-To: <CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 Jan 2015 11:19:59 -0800
Message-ID: <CAPh_B=bJck9r-Nvduk_Oc8H7t=1wUfEWv9k=ta67pVbP-m6VGA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Koert Kuipers <koert@tresata.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Michael Malak <michaelmalak@yahoo.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c0b2aa510c93050da72563
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c0b2aa510c93050da72563
Content-Type: text/plain; charset=UTF-8

Koert,

As Mark said, I have already refactored the API so that nothing is catalyst
is exposed (and users won't need them anyway). Data types, Row interfaces
are both outside catalyst package and in org.apache.spark.sql.

On Tue, Jan 27, 2015 at 9:08 AM, Koert Kuipers <koert@tresata.com> wrote:

> hey matei,
> i think that stuff such as SchemaRDD, columar storage and perhaps also
> query planning can be re-used by many systems that do analysis on
> structured data. i can imagine panda-like systems, but also datalog or
> scalding-like (which we use at tresata and i might rebase on SchemaRDD at
> some point). SchemaRDD should become the interface for all these. and
> columnar storage abstractions should be re-used between all these.
>
> currently the sql tie in is way beyond just the (perhaps unfortunate)
> naming convention. for example a core part of the SchemaRD abstraction is
> Row, which is org.apache.spark.sql.catalyst.expressions.Row, forcing anyone
> that want to build on top of SchemaRDD to dig into catalyst, a SQL Parser
> (if i understand it correctly, i have not used catalyst, but it looks
> neat). i should not need to include a SQL parser just to use structured
> data in say a panda-like framework.
>
> best, koert
>
>
> On Mon, Jan 26, 2015 at 8:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
>> While it might be possible to move this concept to Spark Core long-term,
>> supporting structured data efficiently does require quite a bit of the
>> infrastructure in Spark SQL, such as query planning and columnar storage.
>> The intent of Spark SQL though is to be more than a SQL server -- it's
>> meant to be a library for manipulating structured data. Since this is
>> possible to build over the core API, it's pretty natural to organize it
>> that way, same as Spark Streaming is a library.
>>
>> Matei
>>
>> > On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
>> >
>> > "The context is that SchemaRDD is becoming a common data format used for
>> > bringing data into Spark from external systems, and used for various
>> > components of Spark, e.g. MLlib's new pipeline API."
>> >
>> > i agree. this to me also implies it belongs in spark core, not sql
>> >
>> > On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> > michaelmalak@yahoo.com.invalid> wrote:
>> >
>> >> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
>> Area
>> >> Spark Meetup YouTube contained a wealth of background information on
>> this
>> >> idea (mostly from Patrick and Reynold :-).
>> >>
>> >> https://www.youtube.com/watch?v=YWppYPWznSQ
>> >>
>> >> ________________________________
>> >> From: Patrick Wendell <pwendell@gmail.com>
>> >> To: Reynold Xin <rxin@databricks.com>
>> >> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> >> Sent: Monday, January 26, 2015 4:01 PM
>> >> Subject: Re: renaming SchemaRDD -> DataFrame
>> >>
>> >>
>> >> One thing potentially not clear from this e-mail, there will be a 1:1
>> >> correspondence where you can get an RDD to/from a DataFrame.
>> >>
>> >>
>> >> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
>> wrote:
>> >>> Hi,
>> >>>
>> >>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted
>> to
>> >>> get the community's opinion.
>> >>>
>> >>> The context is that SchemaRDD is becoming a common data format used
>> for
>> >>> bringing data into Spark from external systems, and used for various
>> >>> components of Spark, e.g. MLlib's new pipeline API. We also expect
>> more
>> >> and
>> >>> more users to be programming directly against SchemaRDD API rather
>> than
>> >> the
>> >>> core RDD API. SchemaRDD, through its less commonly used DSL originally
>> >>> designed for writing test cases, always has the data-frame like API.
>> In
>> >>> 1.3, we are redesigning the API to make the API usable for end users.
>> >>>
>> >>>
>> >>> There are two motivations for the renaming:
>> >>>
>> >>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>> >>>
>> >>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
>> (even
>> >>> though it would contain some RDD functions like map, flatMap, etc),
>> and
>> >>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>> >> Instead.
>> >>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>> >>>
>> >>>
>> >>> My understanding is that very few users program directly against the
>> >>> SchemaRDD API at the moment, because they are not well documented.
>> >> However,
>> >>> oo maintain backward compatibility, we can create a type alias
>> DataFrame
>> >>> that is still named SchemaRDD. This will maintain source compatibility
>> >> for
>> >>> Scala. That said, we will have to update all existing materials to use
>> >>> DataFrame rather than SchemaRDD.
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >>
>>
>>
>

--001a11c0b2aa510c93050da72563--

From dev-return-11311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 19:31:33 2015
Return-Path: <dev-return-11311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19D0E10819
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 19:31:33 +0000 (UTC)
Received: (qmail 40812 invoked by uid 500); 27 Jan 2015 19:31:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40738 invoked by uid 500); 27 Jan 2015 19:31:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40726 invoked by uid 99); 27 Jan 2015 19:31:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:31:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 19:31:06 +0000
Received: by mail-wg0-f47.google.com with SMTP id n12so16634131wgh.6
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 11:29:59 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=95J8QW+KJ/4/fEzHx6YKmGnXXk789/KULB2MH0wrUv0=;
        b=GtjWDp2dyeVPqgXkZfh0WZiNxmspDEWKReyTQm9qPYFbSfhvBCMAFYb3XX1IU4fxtt
         oXje3J91pJLU1VuLvCgyf5oiZw5EZgZi0rhsxt1CJ7m9Ip8yg7DmffWT/4E9rGmD1TbU
         MdtiT2tlt6I3vG6AZrO5BbWcLuRxRwcraU2YlPEMay4tiVlY13P057teynAun0sLYi5H
         rDqpBX/KCnUrSEdYGd3GWpOZY1gX85AXn61FsJS8MAZKpav1CH8lHt/AtRTzdENgvRXf
         +MbgSe1HziUmmVMauL7Dmeb7ALrV/cSHr3z00G6Z7eFBaLnxneNcTFN/mZs0L3Nlb/ZK
         HG4Q==
X-Gm-Message-State: ALoCoQlw5qNmhVKJjLarLbzKV3UpP+QWALhNjJxcoewWdrJokicowmpkSG2IPY3rgXjisbH5qkT4
MIME-Version: 1.0
X-Received: by 10.180.79.233 with SMTP id m9mr46984533wix.15.1422386999711;
 Tue, 27 Jan 2015 11:29:59 -0800 (PST)
Received: by 10.217.122.200 with HTTP; Tue, 27 Jan 2015 11:29:59 -0800 (PST)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CAPh_B=bJck9r-Nvduk_Oc8H7t=1wUfEWv9k=ta67pVbP-m6VGA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<CANx3uAie2RO9725V+Dh9yzbrtMe86z6RXQGrhyki3ik38Kn6UQ@mail.gmail.com>
	<CAPh_B=bJck9r-Nvduk_Oc8H7t=1wUfEWv9k=ta67pVbP-m6VGA@mail.gmail.com>
Date: Tue, 27 Jan 2015 14:29:59 -0500
Message-ID: <CANx3uAgR=m94F4erG9gaE5MX3QdWDukQoysMyh2RVHgZBVAQMQ@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Koert Kuipers <koert@tresata.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Michael Malak <michaelmalak@yahoo.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04430418dab172050da747db
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04430418dab172050da747db
Content-Type: text/plain; charset=UTF-8

thats great. guess i was looking at a somewhat stale master branch...

On Tue, Jan 27, 2015 at 2:19 PM, Reynold Xin <rxin@databricks.com> wrote:

> Koert,
>
> As Mark said, I have already refactored the API so that nothing is
> catalyst is exposed (and users won't need them anyway). Data types, Row
> interfaces are both outside catalyst package and in org.apache.spark.sql.
>
> On Tue, Jan 27, 2015 at 9:08 AM, Koert Kuipers <koert@tresata.com> wrote:
>
>> hey matei,
>> i think that stuff such as SchemaRDD, columar storage and perhaps also
>> query planning can be re-used by many systems that do analysis on
>> structured data. i can imagine panda-like systems, but also datalog or
>> scalding-like (which we use at tresata and i might rebase on SchemaRDD at
>> some point). SchemaRDD should become the interface for all these. and
>> columnar storage abstractions should be re-used between all these.
>>
>> currently the sql tie in is way beyond just the (perhaps unfortunate)
>> naming convention. for example a core part of the SchemaRD abstraction is
>> Row, which is org.apache.spark.sql.catalyst.expressions.Row, forcing anyone
>> that want to build on top of SchemaRDD to dig into catalyst, a SQL Parser
>> (if i understand it correctly, i have not used catalyst, but it looks
>> neat). i should not need to include a SQL parser just to use structured
>> data in say a panda-like framework.
>>
>> best, koert
>>
>>
>> On Mon, Jan 26, 2015 at 8:31 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>
>>> While it might be possible to move this concept to Spark Core long-term,
>>> supporting structured data efficiently does require quite a bit of the
>>> infrastructure in Spark SQL, such as query planning and columnar storage.
>>> The intent of Spark SQL though is to be more than a SQL server -- it's
>>> meant to be a library for manipulating structured data. Since this is
>>> possible to build over the core API, it's pretty natural to organize it
>>> that way, same as Spark Streaming is a library.
>>>
>>> Matei
>>>
>>> > On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com> wrote:
>>> >
>>> > "The context is that SchemaRDD is becoming a common data format used
>>> for
>>> > bringing data into Spark from external systems, and used for various
>>> > components of Spark, e.g. MLlib's new pipeline API."
>>> >
>>> > i agree. this to me also implies it belongs in spark core, not sql
>>> >
>>> > On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>>> > michaelmalak@yahoo.com.invalid> wrote:
>>> >
>>> >> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay
>>> Area
>>> >> Spark Meetup YouTube contained a wealth of background information on
>>> this
>>> >> idea (mostly from Patrick and Reynold :-).
>>> >>
>>> >> https://www.youtube.com/watch?v=YWppYPWznSQ
>>> >>
>>> >> ________________________________
>>> >> From: Patrick Wendell <pwendell@gmail.com>
>>> >> To: Reynold Xin <rxin@databricks.com>
>>> >> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>>> >> Sent: Monday, January 26, 2015 4:01 PM
>>> >> Subject: Re: renaming SchemaRDD -> DataFrame
>>> >>
>>> >>
>>> >> One thing potentially not clear from this e-mail, there will be a 1:1
>>> >> correspondence where you can get an RDD to/from a DataFrame.
>>> >>
>>> >>
>>> >> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>> >>> Hi,
>>> >>>
>>> >>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
>>> wanted to
>>> >>> get the community's opinion.
>>> >>>
>>> >>> The context is that SchemaRDD is becoming a common data format used
>>> for
>>> >>> bringing data into Spark from external systems, and used for various
>>> >>> components of Spark, e.g. MLlib's new pipeline API. We also expect
>>> more
>>> >> and
>>> >>> more users to be programming directly against SchemaRDD API rather
>>> than
>>> >> the
>>> >>> core RDD API. SchemaRDD, through its less commonly used DSL
>>> originally
>>> >>> designed for writing test cases, always has the data-frame like API.
>>> In
>>> >>> 1.3, we are redesigning the API to make the API usable for end users.
>>> >>>
>>> >>>
>>> >>> There are two motivations for the renaming:
>>> >>>
>>> >>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>>> >>>
>>> >>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore
>>> (even
>>> >>> though it would contain some RDD functions like map, flatMap, etc),
>>> and
>>> >>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>>> >> Instead.
>>> >>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>>> >>>
>>> >>>
>>> >>> My understanding is that very few users program directly against the
>>> >>> SchemaRDD API at the moment, because they are not well documented.
>>> >> However,
>>> >>> oo maintain backward compatibility, we can create a type alias
>>> DataFrame
>>> >>> that is still named SchemaRDD. This will maintain source
>>> compatibility
>>> >> for
>>> >>> Scala. That said, we will have to update all existing materials to
>>> use
>>> >>> DataFrame rather than SchemaRDD.
>>> >>
>>> >> ---------------------------------------------------------------------
>>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> >> For additional commands, e-mail: dev-help@spark.apache.org
>>> >>
>>> >> ---------------------------------------------------------------------
>>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> >> For additional commands, e-mail: dev-help@spark.apache.org
>>> >>
>>> >>
>>>
>>>
>>
>

--f46d04430418dab172050da747db--

From dev-return-11312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 20:07:06 2015
Return-Path: <dev-return-11312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E257A10AC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 20:07:06 +0000 (UTC)
Received: (qmail 83865 invoked by uid 500); 27 Jan 2015 20:07:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83796 invoked by uid 500); 27 Jan 2015 20:07:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83779 invoked by uid 99); 27 Jan 2015 20:07:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:07:04 +0000
X-ASF-Spam-Status: No, hits=3.0 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.218.51 as permitted sender)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:06:39 +0000
Received: by mail-oi0-f51.google.com with SMTP id x69so14148753oia.10
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 12:05:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:cc
         :content-type;
        bh=4T4pMosTFmmTARsuAUkr6lxzriotO9UE1//RCyz1Cck=;
        b=bepDHW50NDhk/PhCVCbvRG4B+jN/QPFXJOKlSVuxc4+eVRZLKmyUTO5UMcqYikOMLT
         tGLYidNNPdV6lva51ROEUkcThlQ7Zb9o/4d12L+Nqmc6eE8XDcoze1G7bIQhJ4eET8Pk
         K3dfCbk56azpwx0jRiFMJ1kMEKK2QOu31mgXsKgbVNPa9RUYclVU/TuxXY1TnQS15WwP
         HkwURz8K7s1R3YKlnUu7RqlGT4xjn896aFmLX/RrvuquoH51uDsn9lqMrBgcp2Lbmb5t
         Zxy8OtsXWeP6zvq8qXPC3IdiHf9e0Z3y1RVnewl0xt4+iAjKhiqZmioeg6/mOQeq+fD/
         tNbw==
MIME-Version: 1.0
X-Received: by 10.202.225.214 with SMTP id y205mr1849110oig.60.1422389108048;
 Tue, 27 Jan 2015 12:05:08 -0800 (PST)
Received: by 10.76.98.230 with HTTP; Tue, 27 Jan 2015 12:05:07 -0800 (PST)
In-Reply-To: <CAPh_B=ZxU-fM4eSh_o-Z1puJbkhyMyf0BUEU8xfvgfooo_CL=g@mail.gmail.com>
References: <CAPh_B=ZxU-fM4eSh_o-Z1puJbkhyMyf0BUEU8xfvgfooo_CL=g@mail.gmail.com>
Date: Tue, 27 Jan 2015 12:05:07 -0800
Message-ID: <CAPud8Tr3mpyGeopSDjkM298zp5R8yhvd01SqmFjnL-p9JZE34A@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d2abc8489f4050da7c55c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d2abc8489f4050da7c55c
Content-Type: text/plain; charset=UTF-8

It has been pretty evident for some time that's what it is, hasn't it?

Yes that's a better name IMO.

On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <rxin@databricks.com> wrote:

> Hi,
>
> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
> get the community's opinion.
>
> The context is that SchemaRDD is becoming a common data format used for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API. We also expect more and
> more users to be programming directly against SchemaRDD API rather than the
> core RDD API. SchemaRDD, through its less commonly used DSL originally
> designed for writing test cases, always has the data-frame like API. In
> 1.3, we are redesigning the API to make the API usable for end users.
>
>
> There are two motivations for the renaming:
>
> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>
> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
> though it would contain some RDD functions like map, flatMap, etc), and
> calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
> DataFrame.rdd will return the underlying RDD for all RDD methods.
>
>
> My understanding is that very few users program directly against the
> SchemaRDD API at the moment, because they are not well documented. However,
> oo maintain backward compatibility, we can create a type alias DataFrame
> that is still named SchemaRDD. This will maintain source compatibility for
> Scala. That said, we will have to update all existing materials to use
> DataFrame rather than SchemaRDD.
>

--001a113d2abc8489f4050da7c55c--

From dev-return-11313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 20:11:11 2015
Return-Path: <dev-return-11313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7631F10AF2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 20:11:11 +0000 (UTC)
Received: (qmail 99293 invoked by uid 500); 27 Jan 2015 20:11:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99211 invoked by uid 500); 27 Jan 2015 20:11:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99199 invoked by uid 99); 27 Jan 2015 20:11:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:11:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:11:06 +0000
Received: by mail-ob0-f176.google.com with SMTP id va2so15566405obc.7
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 12:10:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=B3bf39RieTOs88nvYnW/7c6pUxVMMxel02P8ozi+cyk=;
        b=i1WEKCepk7wRm8XzByWFU0rhzgyHouXU7TqJb/xU8cXkz58lcyKgk8AxHDIzXvUrou
         3TkyWMF1olior1Hy74kxGYIyOu2ZulSogRZNLp30xDUGiCdQ8DOLrVknubyq8cCXpKbs
         6XkUTjqQ8C2vbFs4hxCW/D5D3dEbSowcahdb8yTPmyEzr1mzUzWVpx1TTVRMbrmcBrhP
         sSJA++Y8Xc+J51VVtVYwD0BjIx89I4nQa/EsaqSLJOiKs2ulELcIosjFreHKwyJrgc5O
         WaUrz1+ve2kRgbImQj92CeBjwpHLSqPGlShwAwt1eZS0thgU+M/+iPAU4qnZ1dJzokSU
         4g/w==
X-Received: by 10.182.71.39 with SMTP id r7mr1941042obu.42.1422389445925; Tue,
 27 Jan 2015 12:10:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.131 with HTTP; Tue, 27 Jan 2015 12:10:05 -0800 (PST)
In-Reply-To: <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com> <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Tue, 27 Jan 2015 18:10:05 -0200
Message-ID: <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8fb202aca8200a050da7d9b6
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb202aca8200a050da7d9b6
Content-Type: text/plain; charset=UTF-8

Reynold,
But with type alias we will have the same problem, right?
If the methods doesn't receive schemardd anymore, we will have to change
our code to migrade from schema to dataframe. Unless we have an implicit
conversion between DataFrame and SchemaRDD



2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:

> Dirceu,
>
> That is not possible because one cannot overload return types.
>
> SQLContext.parquetFile (and many other methods) needs to return some type,
> and that type cannot be both SchemaRDD and DataFrame.
>
> In 1.3, we will create a type alias for DataFrame called SchemaRDD to not
> break source compatibility for Scala.
>
>
> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> dirceu.semighini@gmail.com> wrote:
>
>> Can't the SchemaRDD remain the same, but deprecated, and be removed in the
>> release 1.5(+/- 1)  for example, and the new code been added to DataFrame?
>> With this, we don't impact in existing code for the next few releases.
>>
>>
>>
>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>>
>> > I want to address the issue that Matei raised about the heavy lifting
>> > required for a full SQL support. It is amazing that even after 30 years
>> of
>> > research there is not a single good open source columnar database like
>> > Vertica. There is a column store option in MySQL, but it is not nearly
>> as
>> > sophisticated as Vertica or MonetDB. But there's a true need for such a
>> > system. I wonder why so and it's high time to change that.
>> > On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com> wrote:
>> >
>> > > Both SchemaRDD and DataFrame sound fine to me, though I like the
>> former
>> > > slightly better because it's more descriptive.
>> > >
>> > > Even if SchemaRDD's needs to rely on Spark SQL under the covers, it
>> would
>> > > be more clear from a user-facing perspective to at least choose a
>> package
>> > > name for it that omits "sql".
>> > >
>> > > I would also be in favor of adding a separate Spark Schema module for
>> > Spark
>> > > SQL to rely on, but I imagine that might be too large a change at this
>> > > point?
>> > >
>> > > -Sandy
>> > >
>> > > On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>> matei.zaharia@gmail.com>
>> > > wrote:
>> > >
>> > > > (Actually when we designed Spark SQL we thought of giving it another
>> > > name,
>> > > > like Spark Schema, but we decided to stick with SQL since that was
>> the
>> > > most
>> > > > obvious use case to many users.)
>> > > >
>> > > > Matei
>> > > >
>> > > > > On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>> matei.zaharia@gmail.com>
>> > > > wrote:
>> > > > >
>> > > > > While it might be possible to move this concept to Spark Core
>> > > long-term,
>> > > > supporting structured data efficiently does require quite a bit of
>> the
>> > > > infrastructure in Spark SQL, such as query planning and columnar
>> > storage.
>> > > > The intent of Spark SQL though is to be more than a SQL server --
>> it's
>> > > > meant to be a library for manipulating structured data. Since this
>> is
>> > > > possible to build over the core API, it's pretty natural to
>> organize it
>> > > > that way, same as Spark Streaming is a library.
>> > > > >
>> > > > > Matei
>> > > > >
>> > > > >> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
>> > wrote:
>> > > > >>
>> > > > >> "The context is that SchemaRDD is becoming a common data format
>> used
>> > > for
>> > > > >> bringing data into Spark from external systems, and used for
>> various
>> > > > >> components of Spark, e.g. MLlib's new pipeline API."
>> > > > >>
>> > > > >> i agree. this to me also implies it belongs in spark core, not
>> sql
>> > > > >>
>> > > > >> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> > > > >> michaelmalak@yahoo.com.invalid> wrote:
>> > > > >>
>> > > > >>> And in the off chance that anyone hasn't seen it yet, the Jan.
>> 13
>> > Bay
>> > > > Area
>> > > > >>> Spark Meetup YouTube contained a wealth of background
>> information
>> > on
>> > > > this
>> > > > >>> idea (mostly from Patrick and Reynold :-).
>> > > > >>>
>> > > > >>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> > > > >>>
>> > > > >>> ________________________________
>> > > > >>> From: Patrick Wendell <pwendell@gmail.com>
>> > > > >>> To: Reynold Xin <rxin@databricks.com>
>> > > > >>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> > > > >>> Sent: Monday, January 26, 2015 4:01 PM
>> > > > >>> Subject: Re: renaming SchemaRDD -> DataFrame
>> > > > >>>
>> > > > >>>
>> > > > >>> One thing potentially not clear from this e-mail, there will be
>> a
>> > 1:1
>> > > > >>> correspondence where you can get an RDD to/from a DataFrame.
>> > > > >>>
>> > > > >>>
>> > > > >>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>> rxin@databricks.com>
>> > > > wrote:
>> > > > >>>> Hi,
>> > > > >>>>
>> > > > >>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
>> > > wanted
>> > > > to
>> > > > >>>> get the community's opinion.
>> > > > >>>>
>> > > > >>>> The context is that SchemaRDD is becoming a common data format
>> > used
>> > > > for
>> > > > >>>> bringing data into Spark from external systems, and used for
>> > various
>> > > > >>>> components of Spark, e.g. MLlib's new pipeline API. We also
>> expect
>> > > > more
>> > > > >>> and
>> > > > >>>> more users to be programming directly against SchemaRDD API
>> rather
>> > > > than
>> > > > >>> the
>> > > > >>>> core RDD API. SchemaRDD, through its less commonly used DSL
>> > > originally
>> > > > >>>> designed for writing test cases, always has the data-frame like
>> > API.
>> > > > In
>> > > > >>>> 1.3, we are redesigning the API to make the API usable for end
>> > > users.
>> > > > >>>>
>> > > > >>>>
>> > > > >>>> There are two motivations for the renaming:
>> > > > >>>>
>> > > > >>>> 1. DataFrame seems to be a more self-evident name than
>> SchemaRDD.
>> > > > >>>>
>> > > > >>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>> anymore
>> > > > (even
>> > > > >>>> though it would contain some RDD functions like map, flatMap,
>> > etc),
>> > > > and
>> > > > >>>> calling it Schema*RDD* while it is not an RDD is highly
>> confusing.
>> > > > >>> Instead.
>> > > > >>>> DataFrame.rdd will return the underlying RDD for all RDD
>> methods.
>> > > > >>>>
>> > > > >>>>
>> > > > >>>> My understanding is that very few users program directly
>> against
>> > the
>> > > > >>>> SchemaRDD API at the moment, because they are not well
>> documented.
>> > > > >>> However,
>> > > > >>>> oo maintain backward compatibility, we can create a type alias
>> > > > DataFrame
>> > > > >>>> that is still named SchemaRDD. This will maintain source
>> > > compatibility
>> > > > >>> for
>> > > > >>>> Scala. That said, we will have to update all existing
>> materials to
>> > > use
>> > > > >>>> DataFrame rather than SchemaRDD.
>> > > > >>>
>> > > > >>>
>> > ---------------------------------------------------------------------
>> > > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > > >>> For additional commands, e-mail: dev-help@spark.apache.org
>> > > > >>>
>> > > > >>>
>> > ---------------------------------------------------------------------
>> > > > >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > > >>> For additional commands, e-mail: dev-help@spark.apache.org
>> > > > >>>
>> > > > >>>
>> > > > >
>> > > >
>> > > >
>> > > >
>> ---------------------------------------------------------------------
>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> > > >
>> > > >
>> > >
>> >
>>
>
>

--e89a8fb202aca8200a050da7d9b6--

From dev-return-11314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 20:25:55 2015
Return-Path: <dev-return-11314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78F2010BE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 20:25:55 +0000 (UTC)
Received: (qmail 50068 invoked by uid 500); 27 Jan 2015 20:25:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49974 invoked by uid 500); 27 Jan 2015 20:25:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49944 invoked by uid 99); 27 Jan 2015 20:25:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:25:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:25:50 +0000
Received: by mail-pa0-f47.google.com with SMTP id lj1so20623206pab.6
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 12:25:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=bW8h+wIc0WHdi8AXcvhy360lFDEXZZMJJLtacBQ1sG0=;
        b=aar1ViDQWB1HIj+ENbWEqgXglfST0nfD9vC0hzQLfKipt7iABTkkNIyjAr31g3Aqsf
         zs1vnAksbBUp2hWTOcsDeQgiH5SPO3uLmkAFlvkRL90SxDQ2X4d200Ui4tRxkCaK/jod
         y3Hpq+/JyzzfCdsHw//lmXPpaijgVfUA10FQIA8zMSO+CgbvZLCDfCm6eIdGnpPLkjMi
         WNefsW/QTPMK0wuRRuLLp1/7TfUnhQwz6qUjiUSyZTOfKeIGg4f6QNGFthyrBwO8aDyf
         uXj5QMJoCx01Qsel1kKV5ixFhf7lveTjYJckt4ZSIrQa9zV6yDYodDcdLirxiMQBfT3q
         R+zQ==
X-Received: by 10.70.90.144 with SMTP id bw16mr4801918pdb.149.1422390329523;
        Tue, 27 Jan 2015 12:25:29 -0800 (PST)
Received: from [192.168.1.100] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id v2sm2425043pdm.77.2015.01.27.12.25.26
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 27 Jan 2015 12:25:28 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: renaming SchemaRDD -> DataFrame
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
Date: Tue, 27 Jan 2015 12:25:25 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com> <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com> <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com> <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com> <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com> <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com> <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com> <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com> <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
To: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

The type alias means your methods can specify either type and they will =
work. It's just another name for the same type. But Scaladocs and such =
will show DataFrame as the type.

Matei

> On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho =
<dirceu.semighini@gmail.com> wrote:
>=20
> Reynold,
> But with type alias we will have the same problem, right?
> If the methods doesn't receive schemardd anymore, we will have to =
change
> our code to migrade from schema to dataframe. Unless we have an =
implicit
> conversion between DataFrame and SchemaRDD
>=20
>=20
>=20
> 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>=20
>> Dirceu,
>>=20
>> That is not possible because one cannot overload return types.
>>=20
>> SQLContext.parquetFile (and many other methods) needs to return some =
type,
>> and that type cannot be both SchemaRDD and DataFrame.
>>=20
>> In 1.3, we will create a type alias for DataFrame called SchemaRDD to =
not
>> break source compatibility for Scala.
>>=20
>>=20
>> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>> dirceu.semighini@gmail.com> wrote:
>>=20
>>> Can't the SchemaRDD remain the same, but deprecated, and be removed =
in the
>>> release 1.5(+/- 1)  for example, and the new code been added to =
DataFrame?
>>> With this, we don't impact in existing code for the next few =
releases.
>>>=20
>>>=20
>>>=20
>>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>>>=20
>>>> I want to address the issue that Matei raised about the heavy =
lifting
>>>> required for a full SQL support. It is amazing that even after 30 =
years
>>> of
>>>> research there is not a single good open source columnar database =
like
>>>> Vertica. There is a column store option in MySQL, but it is not =
nearly
>>> as
>>>> sophisticated as Vertica or MonetDB. But there's a true need for =
such a
>>>> system. I wonder why so and it's high time to change that.
>>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com> =
wrote:
>>>>=20
>>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
>>> former
>>>>> slightly better because it's more descriptive.
>>>>>=20
>>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers, =
it
>>> would
>>>>> be more clear from a user-facing perspective to at least choose a
>>> package
>>>>> name for it that omits "sql".
>>>>>=20
>>>>> I would also be in favor of adding a separate Spark Schema module =
for
>>>> Spark
>>>>> SQL to rely on, but I imagine that might be too large a change at =
this
>>>>> point?
>>>>>=20
>>>>> -Sandy
>>>>>=20
>>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>>> matei.zaharia@gmail.com>
>>>>> wrote:
>>>>>=20
>>>>>> (Actually when we designed Spark SQL we thought of giving it =
another
>>>>> name,
>>>>>> like Spark Schema, but we decided to stick with SQL since that =
was
>>> the
>>>>> most
>>>>>> obvious use case to many users.)
>>>>>>=20
>>>>>> Matei
>>>>>>=20
>>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>>> matei.zaharia@gmail.com>
>>>>>> wrote:
>>>>>>>=20
>>>>>>> While it might be possible to move this concept to Spark Core
>>>>> long-term,
>>>>>> supporting structured data efficiently does require quite a bit =
of
>>> the
>>>>>> infrastructure in Spark SQL, such as query planning and columnar
>>>> storage.
>>>>>> The intent of Spark SQL though is to be more than a SQL server --
>>> it's
>>>>>> meant to be a library for manipulating structured data. Since =
this
>>> is
>>>>>> possible to build over the core API, it's pretty natural to
>>> organize it
>>>>>> that way, same as Spark Streaming is a library.
>>>>>>>=20
>>>>>>> Matei
>>>>>>>=20
>>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
>>>> wrote:
>>>>>>>>=20
>>>>>>>> "The context is that SchemaRDD is becoming a common data format
>>> used
>>>>> for
>>>>>>>> bringing data into Spark from external systems, and used for
>>> various
>>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>>>>>>>>=20
>>>>>>>> i agree. this to me also implies it belongs in spark core, not
>>> sql
>>>>>>>>=20
>>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>>>>>>>>=20
>>>>>>>>> And in the off chance that anyone hasn't seen it yet, the Jan.
>>> 13
>>>> Bay
>>>>>> Area
>>>>>>>>> Spark Meetup YouTube contained a wealth of background
>>> information
>>>> on
>>>>>> this
>>>>>>>>> idea (mostly from Patrick and Reynold :-).
>>>>>>>>>=20
>>>>>>>>> https://www.youtube.com/watch?v=3DYWppYPWznSQ
>>>>>>>>>=20
>>>>>>>>> ________________________________
>>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>>> One thing potentially not clear from this e-mail, there will =
be
>>> a
>>>> 1:1
>>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>>> rxin@databricks.com>
>>>>>> wrote:
>>>>>>>>>> Hi,
>>>>>>>>>>=20
>>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, =
and
>>>>> wanted
>>>>>> to
>>>>>>>>>> get the community's opinion.
>>>>>>>>>>=20
>>>>>>>>>> The context is that SchemaRDD is becoming a common data =
format
>>>> used
>>>>>> for
>>>>>>>>>> bringing data into Spark from external systems, and used for
>>>> various
>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
>>> expect
>>>>>> more
>>>>>>>>> and
>>>>>>>>>> more users to be programming directly against SchemaRDD API
>>> rather
>>>>>> than
>>>>>>>>> the
>>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
>>>>> originally
>>>>>>>>>> designed for writing test cases, always has the data-frame =
like
>>>> API.
>>>>>> In
>>>>>>>>>> 1.3, we are redesigning the API to make the API usable for =
end
>>>>> users.
>>>>>>>>>>=20
>>>>>>>>>>=20
>>>>>>>>>> There are two motivations for the renaming:
>>>>>>>>>>=20
>>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>>> SchemaRDD.
>>>>>>>>>>=20
>>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>>> anymore
>>>>>> (even
>>>>>>>>>> though it would contain some RDD functions like map, flatMap,
>>>> etc),
>>>>>> and
>>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>>> confusing.
>>>>>>>>> Instead.
>>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
>>> methods.
>>>>>>>>>>=20
>>>>>>>>>>=20
>>>>>>>>>> My understanding is that very few users program directly
>>> against
>>>> the
>>>>>>>>>> SchemaRDD API at the moment, because they are not well
>>> documented.
>>>>>>>>> However,
>>>>>>>>>> oo maintain backward compatibility, we can create a type =
alias
>>>>>> DataFrame
>>>>>>>>>> that is still named SchemaRDD. This will maintain source
>>>>> compatibility
>>>>>>>>> for
>>>>>>>>>> Scala. That said, we will have to update all existing
>>> materials to
>>>>> use
>>>>>>>>>> DataFrame rather than SchemaRDD.
>>>>>>>>>=20
>>>>>>>>>=20
>>>> =
---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>=20
>>>>>>>>>=20
>>>> =
---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>=20
>>>>>>=20
>>>>>>=20
>>>>>>=20
>>> =
---------------------------------------------------------------------
>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>=20
>>>>>>=20
>>>>>=20
>>>>=20
>>>=20
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 20:35:42 2015
Return-Path: <dev-return-11315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E478B10C8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 20:35:42 +0000 (UTC)
Received: (qmail 81644 invoked by uid 500); 27 Jan 2015 20:35:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81567 invoked by uid 500); 27 Jan 2015 20:35:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81550 invoked by uid 99); 27 Jan 2015 20:35:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:35:38 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 20:35:33 +0000
Received: by mail-pd0-f182.google.com with SMTP id z10so20889031pdj.13
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 12:35:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=iAimJ3sbD1j1avwtx4lZXvPCtcyLI3Jib/7ro/DaYN4=;
        b=Hign1T/xngqaPpF/iKYIPTloU55AUkAWiMzbQdy/oYqXO/eTuuziIyp2+fFvUR85T4
         aMc8NOntWyxPH3q3fvqo9f09n3/dylCx38bPPXzbohniaunrQ+mtTFJUkH/PbUprhpRR
         NnI+vasOKoCA8cAsLsLWHzArZ5PMUlBDDewdWEqH2hanfJ6rqvQvHLmW8FOOXkjGcUeq
         HjTgUZdNGcj6VFzxEMYGJmEhW8orS2ljKJ4ir5B+rlo3UV2Itn6ijiCBNXbNl4IPZFBm
         ep/ZnD/XD68zY5axzYNOthdLBZQQTSq1xPeDBWnjUSOrBl7NV1xAVNSaYa8uaGCPqc8L
         25XQ==
MIME-Version: 1.0
X-Received: by 10.66.63.106 with SMTP id f10mr5386105pas.0.1422390912650; Tue,
 27 Jan 2015 12:35:12 -0800 (PST)
Received: by 10.70.37.230 with HTTP; Tue, 27 Jan 2015 12:35:12 -0800 (PST)
In-Reply-To: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
Date: Tue, 27 Jan 2015 12:35:12 -0800
Message-ID: <CAOTBr2nHHgyWbDkUzSxASw5+M7aL_1m8QGd6X2CvSxw7W2byzA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Krishna Sankar <ksankar42@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136b10014cf7a050da8316a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136b10014cf7a050da8316a
Content-Type: text/plain; charset=UTF-8

+1
1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:55 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.0
2.1. statistics OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       Fixed : org.apache.spark.SparkException in zip !
2.5. rdd operations OK
       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. recommendation OK

Cheers
<k/>

On Mon, Jan 26, 2015 at 11:02 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1061/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until Friday, January 30, at 07:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1136b10014cf7a050da8316a--

From dev-return-11316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jan 27 23:58:10 2015
Return-Path: <dev-return-11316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7065176E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 Jan 2015 23:58:10 +0000 (UTC)
Received: (qmail 26865 invoked by uid 500); 27 Jan 2015 23:58:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26791 invoked by uid 500); 27 Jan 2015 23:58:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26333 invoked by uid 99); 27 Jan 2015 23:58:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 23:58:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.44 as permitted sender)
Received: from [209.85.218.44] (HELO mail-oi0-f44.google.com) (209.85.218.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 Jan 2015 23:57:43 +0000
Received: by mail-oi0-f44.google.com with SMTP id a3so15018004oib.3
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 15:56:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=g2O41av9BBLiXn4k0RbbymxA0cSKyBwsVrSL1n0QHZE=;
        b=CW96vNcqnmoIfrmfQHQ1Q07hbiL+xkGPpSy/taAI7c17JXE/2/qvTCGgCN+0YjMGeq
         eQbBSG7ppkvUPigJxwaffVloRbXjwU2ZRSlYOiLlq689Ryxo5zqgfrp3X9ui242GJrL4
         0lWviXbS7O4L77ya8XU6/Ub9t+hMbl+lsexM+kcOiBYceu0XY1a6MCtNYW2Y6Ec0qL2e
         i19F8dRYiyamL8P9gfaydo3u7i+CgZ/KL8u38o6T7hUeyaaB2tlakcp05YZ7lKPmbvsj
         gP4USpXtdI1UF8gToRFGjzBGA7tL9/Mn0Da8w9dH69ucNRHAwOUaEoiQXqjjYKgOQXmr
         J3Ew==
MIME-Version: 1.0
X-Received: by 10.202.185.198 with SMTP id j189mr382785oif.72.1422403017116;
 Tue, 27 Jan 2015 15:56:57 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Tue, 27 Jan 2015 15:56:57 -0800 (PST)
Date: Tue, 27 Jan 2015 15:56:57 -0800
Message-ID: <CABPQxstfvFRA-hOKuSH=b_pj=1PEABmgCCXrQdZ64BFy8oML_w@mail.gmail.com>
Subject: Friendly reminder/request to help with reviews!
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Just a reminder, as always around release time we have a very large
volume of patches show up near the deadline.

One thing that can help us maximize the number of patches we get in is
to have community involvement in performing code reviews. And in
particular, doing a thorough review and signing off on a patch with
LGTM can substantially increase the odds we can merge a patch
confidently.

If you are newer to Spark, finding a single area of the codebase to
focus on can still provide a lot of value to the project in the
reviewing process.

Cheers and good luck with everyone on work for this release.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 00:03:14 2015
Return-Path: <dev-return-11317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24DB817722
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 00:03:14 +0000 (UTC)
Received: (qmail 39567 invoked by uid 500); 28 Jan 2015 00:03:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39497 invoked by uid 500); 28 Jan 2015 00:03:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39485 invoked by uid 99); 28 Jan 2015 00:03:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:03:12 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of guruyvs@yahoo.com designates 216.109.114.174 as permitted sender)
Received: from [216.109.114.174] (HELO nm41-vm7.bullet.mail.bf1.yahoo.com) (216.109.114.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:02:44 +0000
Received: from [98.139.212.151] by nm41.bullet.mail.bf1.yahoo.com with NNFMP; 28 Jan 2015 00:02:42 -0000
Received: from [98.139.212.222] by tm8.bullet.mail.bf1.yahoo.com with NNFMP; 28 Jan 2015 00:02:42 -0000
Received: from [127.0.0.1] by omp1031.mail.bf1.yahoo.com with NNFMP; 28 Jan 2015 00:02:42 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 31396.44453.bm@omp1031.mail.bf1.yahoo.com
X-YMail-OSG: BpIILvgVM1lQqyjd_UP3f_wWLiQi0z8Xno2P8DkbBHiJ70tJaryyy4WDGlf76U7
 pMSb_ZZGZsAS9XxwIDkyqNTNY50hUNEL2qdC6wL..utGv3GU_NumSM3mPbEKvClfE14m4kAnLoLj
 kP5.Ptv4cmCOqw3ymFmxtbQvakiv9fffgaF26T_PXixepRLy6Kr0IGWCUCkEvWLZ8xlG5xdhgr1w
 GBFEYQcndwiAfbazpj9.382tuKFzfJt5FQ8dVFcmzXjX66AbAOVoAk26UU7Bvnm2pPbp2qWCOFhQ
 QIrDqWQM6AE8ReEiW_PAA6bjznc8xbZqTuze4__bzAt3lCPTYPsKIqoEh_I.fLOyURbYO5VDrk1L
 HN2iOtfmHW7bFn6In3zEVfLWn2jAFrE4hYOSiayvw3dQLzss6SvpMItAtdFuGSiQe8qIyLksN7wi
 NpHYuRmAIO_iqr2l8WqzYPUbI27_Hq6OAU1YyQnMC.ldepmeN8Bg.JgkWIRkXM4CLnQ6pH40e9xI
 p0_ZP
Received: by 66.196.80.117; Wed, 28 Jan 2015 00:02:41 +0000 
Date: Wed, 28 Jan 2015 00:02:40 +0000 (UTC)
From: Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>
Reply-To: Gurumurthy Yeleswarapu <guruyvs@yahoo.com>
To: Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <389795227.1497844.1422403361050.JavaMail.yahoo@mail.yahoo.com>
In-Reply-To: <CABPQxstfvFRA-hOKuSH=b_pj=1PEABmgCCXrQdZ64BFy8oML_w@mail.gmail.com>
References: <CABPQxstfvFRA-hOKuSH=b_pj=1PEABmgCCXrQdZ64BFy8oML_w@mail.gmail.com>
Subject: Re: Friendly reminder/request to help with reviews!
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_1497843_151677414.1422403361045"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_1497843_151677414.1422403361045
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi Patrick:
I would love to help reviewing in any way I can. Im fairly new here. Can you help with a pointer to get me started.
Thanks
      From: Patrick Wendell <pwendell@gmail.com>
 To: "dev@spark.apache.org" <dev@spark.apache.org> 
 Sent: Tuesday, January 27, 2015 3:56 PM
 Subject: Friendly reminder/request to help with reviews!
   
Hey All,

Just a reminder, as always around release time we have a very large
volume of patches show up near the deadline.

One thing that can help us maximize the number of patches we get in is
to have community involvement in performing code reviews. And in
particular, doing a thorough review and signing off on a patch with
LGTM can substantially increase the odds we can merge a patch
confidently.

If you are newer to Spark, finding a single area of the codebase to
focus on can still provide a lot of value to the project in the
reviewing process.

Cheers and good luck with everyone on work for this release.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org



   
------=_Part_1497843_151677414.1422403361045--

From dev-return-11318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 00:23:46 2015
Return-Path: <dev-return-11318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D9FBE17809
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 00:23:46 +0000 (UTC)
Received: (qmail 88191 invoked by uid 500); 28 Jan 2015 00:23:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88119 invoked by uid 500); 28 Jan 2015 00:23:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88108 invoked by uid 99); 28 Jan 2015 00:23:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:23:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:23:40 +0000
Received: by mail-qg0-f50.google.com with SMTP id f51so14431448qge.9
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 16:20:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=G+bDz5ml2ts1odXPmhhT6NNhQCgp0QQCydzlKP4c/Ak=;
        b=j1p80LmJ42HZfCymDSbaJcX3PhiX1/kY8r4VR40fzL64mTigLgKq5MCDU56XXj1ZJd
         HAFcZCamZta4j4pf5ModgAiu5N1TSs/JDx3GL4IYtu0008SKuaFRYlBA3AA6Op1xSKC5
         uzmrQdfa/jFk/UiYhXxp5l5n5Q7BLFRlZIamq0Ipzj5G4bDl4xzz/Tmo6mwWfkB9jnDS
         Xvm/+4kHNiuxNlKRO+4cl4/Wakg1dtO0T1mHnFRAnqJ1UrD+zPmeFCAVA8I1jKpvrnsp
         PW5iHq0Ryjyeo/VX02NilgcfumOk503HFLriybtrGKPdWNGF29bAjd5Vb7iIiONyrOot
         bbsw==
X-Gm-Message-State: ALoCoQljS2lVm7ywkzhd70qKp41w5WBhtze2v6MA/PAmLuNITxDQ3cpgAyke/LMNNaQ1YSqh1FPi
X-Received: by 10.140.86.233 with SMTP id p96mr7290824qgd.49.1422404443559;
 Tue, 27 Jan 2015 16:20:43 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 27 Jan 2015 16:20:23 -0800 (PST)
In-Reply-To: <CAOTBr2nHHgyWbDkUzSxASw5+M7aL_1m8QGd6X2CvSxw7W2byzA@mail.gmail.com>
References: <CABPQxssjOmLcNmCq1iWwvQ30WVxQ0OF11cmQfmAAgDxMHUOQHw@mail.gmail.com>
 <CAOTBr2nHHgyWbDkUzSxASw5+M7aL_1m8QGd6X2CvSxw7W2byzA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 Jan 2015 16:20:23 -0800
Message-ID: <CAPh_B=YMrFnZ0K+yfQdCceiFDB-V4ME4TypdfJptpp9QnYckSw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC1)
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c120c295eff0050dab57c5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c120c295eff0050dab57c5
Content-Type: text/plain; charset=UTF-8

+1

Tested on Mac OS X

On Tue, Jan 27, 2015 at 12:35 PM, Krishna Sankar <ksankar42@gmail.com>
wrote:

> +1
> 1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:55 min
>      mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
> -Dhadoop.version=2.6.0 -Phive -DskipTests
> 2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
> 1.2.0
> 2.1. statistics OK
> 2.2. Linear/Ridge/Laso Regression OK
> 2.3. Decision Tree, Naive Bayes OK
> 2.4. KMeans OK
>        Center And Scale OK
>        Fixed : org.apache.spark.SparkException in zip !
> 2.5. rdd operations OK
>        State of the Union Texts - MapReduce, Filter,sortByKey (word count)
> 2.6. recommendation OK
>
> Cheers
> <k/>
>
> On Mon, Jan 26, 2015 at 11:02 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.2.1!
> >
> > The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1061/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.2.1!
> >
> > The vote is open until Friday, January 30, at 07:00 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.1
> > [ ] -1 Do not release this package because ...
> >
> > For a list of fixes in this release, see http://s.apache.org/Mpn.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a11c120c295eff0050dab57c5--

From dev-return-11319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 00:37:10 2015
Return-Path: <dev-return-11319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3D711787A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 00:37:10 +0000 (UTC)
Received: (qmail 17920 invoked by uid 500); 28 Jan 2015 00:37:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17845 invoked by uid 500); 28 Jan 2015 00:37:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17834 invoked by uid 99); 28 Jan 2015 00:37:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:37:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 00:36:44 +0000
Received: by mail-qg0-f49.google.com with SMTP id i50so14458001qgf.8
        for <dev@spark.apache.org>; Tue, 27 Jan 2015 16:35:37 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=SbuSDKd4Y+kuvPUfz0OzK6kogWsmfnGeEPngJDiRCwQ=;
        b=D9SF+t5DOdVTavhRgoJWuFAbJhAEunGHN8NxcTIZ+551thb6Zou0MFoRRO/36hf7c8
         3QYfjKZlR6THt80zIlsuYH0OQMWVvZIzjkN4jJEHjqQ6TzfBt34pru8vnx4vtnYa4nzs
         whPeA6TD78HWRvgpjX0l6ztZHDvjAuu3utGR4d8r/pFebzL5yc0mWSMQ6PCHsLNI+voe
         9V8UROAbevxtnPNj9Fjeto9tIZ7LpW7b4nH4qCMWj0gbK3y92vAXCfACRR4GOBwS1AJ8
         pSOVDvSCJn4/PzHklNqm0oVP0FBvkgJ2iFdk6VTApE3l5uPKbCoUts4/M7g1RBMFMDJy
         +vRA==
X-Gm-Message-State: ALoCoQmiJY497Rzj20Cc13Km1luda2ApeoxbihIjbWOrvKZZG77z1e1WWyAdxYIvxfHP2JdFubD1
X-Received: by 10.224.129.202 with SMTP id p10mr7528635qas.54.1422405337121;
 Tue, 27 Jan 2015 16:35:37 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Tue, 27 Jan 2015 16:35:16 -0800 (PST)
In-Reply-To: <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
 <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
 <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com> <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 Jan 2015 16:35:16 -0800
Message-ID: <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Dirceu Semighini Filho <dirceu.semighini@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c124d89da3050dab8c34
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c124d89da3050dab8c34
Content-Type: text/plain; charset=UTF-8

Alright I have merged the patch ( https://github.com/apache/spark/pull/4173
) since I don't see any strong opinions against it (as a matter of fact
most were for it). We can still change it if somebody lays out a strong
argument.

On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> The type alias means your methods can specify either type and they will
> work. It's just another name for the same type. But Scaladocs and such will
> show DataFrame as the type.
>
> Matei
>
> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> dirceu.semighini@gmail.com> wrote:
> >
> > Reynold,
> > But with type alias we will have the same problem, right?
> > If the methods doesn't receive schemardd anymore, we will have to change
> > our code to migrade from schema to dataframe. Unless we have an implicit
> > conversion between DataFrame and SchemaRDD
> >
> >
> >
> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> >
> >> Dirceu,
> >>
> >> That is not possible because one cannot overload return types.
> >>
> >> SQLContext.parquetFile (and many other methods) needs to return some
> type,
> >> and that type cannot be both SchemaRDD and DataFrame.
> >>
> >> In 1.3, we will create a type alias for DataFrame called SchemaRDD to
> not
> >> break source compatibility for Scala.
> >>
> >>
> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> >> dirceu.semighini@gmail.com> wrote:
> >>
> >>> Can't the SchemaRDD remain the same, but deprecated, and be removed in
> the
> >>> release 1.5(+/- 1)  for example, and the new code been added to
> DataFrame?
> >>> With this, we don't impact in existing code for the next few releases.
> >>>
> >>>
> >>>
> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
> >>>
> >>>> I want to address the issue that Matei raised about the heavy lifting
> >>>> required for a full SQL support. It is amazing that even after 30
> years
> >>> of
> >>>> research there is not a single good open source columnar database like
> >>>> Vertica. There is a column store option in MySQL, but it is not nearly
> >>> as
> >>>> sophisticated as Vertica or MonetDB. But there's a true need for such
> a
> >>>> system. I wonder why so and it's high time to change that.
> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
> wrote:
> >>>>
> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
> >>> former
> >>>>> slightly better because it's more descriptive.
> >>>>>
> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers, it
> >>> would
> >>>>> be more clear from a user-facing perspective to at least choose a
> >>> package
> >>>>> name for it that omits "sql".
> >>>>>
> >>>>> I would also be in favor of adding a separate Spark Schema module for
> >>>> Spark
> >>>>> SQL to rely on, but I imagine that might be too large a change at
> this
> >>>>> point?
> >>>>>
> >>>>> -Sandy
> >>>>>
> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> >>> matei.zaharia@gmail.com>
> >>>>> wrote:
> >>>>>
> >>>>>> (Actually when we designed Spark SQL we thought of giving it another
> >>>>> name,
> >>>>>> like Spark Schema, but we decided to stick with SQL since that was
> >>> the
> >>>>> most
> >>>>>> obvious use case to many users.)
> >>>>>>
> >>>>>> Matei
> >>>>>>
> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> >>> matei.zaharia@gmail.com>
> >>>>>> wrote:
> >>>>>>>
> >>>>>>> While it might be possible to move this concept to Spark Core
> >>>>> long-term,
> >>>>>> supporting structured data efficiently does require quite a bit of
> >>> the
> >>>>>> infrastructure in Spark SQL, such as query planning and columnar
> >>>> storage.
> >>>>>> The intent of Spark SQL though is to be more than a SQL server --
> >>> it's
> >>>>>> meant to be a library for manipulating structured data. Since this
> >>> is
> >>>>>> possible to build over the core API, it's pretty natural to
> >>> organize it
> >>>>>> that way, same as Spark Streaming is a library.
> >>>>>>>
> >>>>>>> Matei
> >>>>>>>
> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
> >>>> wrote:
> >>>>>>>>
> >>>>>>>> "The context is that SchemaRDD is becoming a common data format
> >>> used
> >>>>> for
> >>>>>>>> bringing data into Spark from external systems, and used for
> >>> various
> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> >>>>>>>>
> >>>>>>>> i agree. this to me also implies it belongs in spark core, not
> >>> sql
> >>>>>>>>
> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> >>>>>>>>
> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the Jan.
> >>> 13
> >>>> Bay
> >>>>>> Area
> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> >>> information
> >>>> on
> >>>>>> this
> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> >>>>>>>>>
> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >>>>>>>>>
> >>>>>>>>> ________________________________
> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> One thing potentially not clear from this e-mail, there will be
> >>> a
> >>>> 1:1
> >>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> >>> rxin@databricks.com>
> >>>>>> wrote:
> >>>>>>>>>> Hi,
> >>>>>>>>>>
> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
> >>>>> wanted
> >>>>>> to
> >>>>>>>>>> get the community's opinion.
> >>>>>>>>>>
> >>>>>>>>>> The context is that SchemaRDD is becoming a common data format
> >>>> used
> >>>>>> for
> >>>>>>>>>> bringing data into Spark from external systems, and used for
> >>>> various
> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
> >>> expect
> >>>>>> more
> >>>>>>>>> and
> >>>>>>>>>> more users to be programming directly against SchemaRDD API
> >>> rather
> >>>>>> than
> >>>>>>>>> the
> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
> >>>>> originally
> >>>>>>>>>> designed for writing test cases, always has the data-frame like
> >>>> API.
> >>>>>> In
> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable for end
> >>>>> users.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> There are two motivations for the renaming:
> >>>>>>>>>>
> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
> >>> SchemaRDD.
> >>>>>>>>>>
> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
> >>> anymore
> >>>>>> (even
> >>>>>>>>>> though it would contain some RDD functions like map, flatMap,
> >>>> etc),
> >>>>>> and
> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
> >>> confusing.
> >>>>>>>>> Instead.
> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
> >>> methods.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> My understanding is that very few users program directly
> >>> against
> >>>> the
> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
> >>> documented.
> >>>>>>>>> However,
> >>>>>>>>>> oo maintain backward compatibility, we can create a type alias
> >>>>>> DataFrame
> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
> >>>>> compatibility
> >>>>>>>>> for
> >>>>>>>>>> Scala. That said, we will have to update all existing
> >>> materials to
> >>>>> use
> >>>>>>>>>> DataFrame rather than SchemaRDD.
> >>>>>>>>>
> >>>>>>>>>
> >>>> ---------------------------------------------------------------------
> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>>>>
> >>>>>>>>>
> >>>> ---------------------------------------------------------------------
> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>> ---------------------------------------------------------------------
> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>
> >>
> >>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c2c124d89da3050dab8c34--

From dev-return-11320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:04:32 2015
Return-Path: <dev-return-11320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2357617BA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:04:32 +0000 (UTC)
Received: (qmail 94587 invoked by uid 500); 28 Jan 2015 10:04:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94511 invoked by uid 500); 28 Jan 2015 10:04:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94496 invoked by uid 99); 28 Jan 2015 10:04:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:04:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:04:06 +0000
Received: by mail-oi0-f53.google.com with SMTP id i138so16730791oig.12
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:02:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:cc:content-type;
        bh=uHIav998reOXx4BfUPb7/ni60yCnDufYN9xne8PjCpc=;
        b=elKzdgtlW79kUuhNYpRI21UvpKSjukzb45OK2kCu+m4fCT8s7jBB9i4IFM7Qq1V8Tu
         4DfeWnv/c+Pm007HtSxdmQ/VWt1EU9bNq8YIA+pjKCXb7w5ou1T9QeUbkAkBy6sliNTN
         0cUHRp9x2Qz3nDQWMNigiwaBmtrvj+MNFnOgnGhK3mNplvTA2MguBLi5SKLApjUh2eZ5
         316ABv3/CZELKM1+Dq4iq3hac2KU1swembAwsULSnt4cozfOz/LdzZUdtxzR/uQkX3nC
         0tbXTcOOjCEPbVovH4TdNvUATs717WzngfCJrB/UGeR5nNu/UZjIl+sUXNd6Az1lfevw
         ZGMA==
MIME-Version: 1.0
X-Received: by 10.60.133.141 with SMTP id pc13mr1579542oeb.68.1422439354589;
 Wed, 28 Jan 2015 02:02:34 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Wed, 28 Jan 2015 02:02:34 -0800 (PST)
Date: Wed, 28 Jan 2015 02:02:34 -0800
Message-ID: <CABPQxsuz7StxUbQG+f+yCSCexo=_UR5JGVrZtd9Ae7S_DyU+Yw@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.2.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Krishna Sankar <ksankar42@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of RC2.

On Tue, Jan 27, 2015 at 4:20 PM, Reynold Xin <rxin@databricks.com> wrote:
> +1
>
> Tested on Mac OS X
>
> On Tue, Jan 27, 2015 at 12:35 PM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
>>
>> +1
>> 1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:55 min
>>      mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
>> -Dhadoop.version=2.6.0 -Phive -DskipTests
>> 2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
>> 1.2.0
>> 2.1. statistics OK
>> 2.2. Linear/Ridge/Laso Regression OK
>> 2.3. Decision Tree, Naive Bayes OK
>> 2.4. KMeans OK
>>        Center And Scale OK
>>        Fixed : org.apache.spark.SparkException in zip !
>> 2.5. rdd operations OK
>>        State of the Union Texts - MapReduce, Filter,sortByKey (word count)
>> 2.6. recommendation OK
>>
>> Cheers
>> <k/>
>>
>> On Mon, Jan 26, 2015 at 11:02 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > Please vote on releasing the following candidate as Apache Spark version
>> > 1.2.1!
>> >
>> > The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
>> >
>> >
>> > https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec
>> >
>> > The release files, including signatures, digests, etc. can be found at:
>> > http://people.apache.org/~pwendell/spark-1.2.1-rc1/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/pwendell.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1061/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/
>> >
>> > Please vote on releasing this package as Apache Spark 1.2.1!
>> >
>> > The vote is open until Friday, January 30, at 07:00 UTC and passes
>> > if a majority of at least 3 +1 PMC votes are cast.
>> >
>> > [ ] +1 Release this package as Apache Spark 1.2.1
>> > [ ] -1 Do not release this package because ...
>> >
>> > For a list of fixes in this release, see http://s.apache.org/Mpn.
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>> >
>> > - Patrick
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:07:22 2015
Return-Path: <dev-return-11321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 82EB717BB6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:07:22 +0000 (UTC)
Received: (qmail 1502 invoked by uid 500); 28 Jan 2015 10:07:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1416 invoked by uid 500); 28 Jan 2015 10:07:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1404 invoked by uid 99); 28 Jan 2015 10:07:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:07:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:07:16 +0000
Received: by mail-oi0-f50.google.com with SMTP id h136so16762529oig.9
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:06:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=qLCOzoeRByyCDv6pt1N/JE7eLAZw3PkGNpgTMUpQCiU=;
        b=bSiCZfznOiQeacaiwX0XDGhONyvsL98Wnr+aoLPtXq/2Hv7QX+nBBl98yRct0wPd58
         kZEdiGavwQ6Qdgo+2pt9TTmoTpaOoQ2vNP9WNdTcNpQGQB+vLRyBsxLZS1s8RFpmTSSU
         tCRnYrf8P6RffFgq7oEm8UsGBosM+t+xIuTKqFDVaoKAWAodR5s46kPpfgkN2hqe/eZY
         QROoggAE+sQ/NfaF0KIPXXvCTO01aNowhfW97Fu1DkWtAWhHkv7VjKj7jxQW41igqbl0
         4So23XLrbO5cZOnkVcCZmy4CyQt78UaeRshnmzjRXGqGZSeFCt9i/sPT6jC3lJeQ5Mau
         Iq4w==
MIME-Version: 1.0
X-Received: by 10.202.185.198 with SMTP id j189mr1498225oif.72.1422439570631;
 Wed, 28 Jan 2015 02:06:10 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Wed, 28 Jan 2015 02:06:10 -0800 (PST)
Date: Wed, 28 Jan 2015 02:06:10 -0800
Message-ID: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.2.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.2.1!

The tag to be voted on is v1.2.1-rc1 (commit b77f876):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1062/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/

Changes from rc1:
This has no code changes from RC1. Only minor changes to the release script.

Please vote on releasing this package as Apache Spark 1.2.1!

The vote is open until  Saturday, January 31, at 10:04 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.1
[ ] -1 Do not release this package because ...

For a list of fixes in this release, see http://s.apache.org/Mpn.

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:09:24 2015
Return-Path: <dev-return-11322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8931E17BD1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:09:24 +0000 (UTC)
Received: (qmail 10957 invoked by uid 500); 28 Jan 2015 10:09:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10885 invoked by uid 500); 28 Jan 2015 10:09:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10868 invoked by uid 99); 28 Jan 2015 10:09:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:09:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:08:57 +0000
Received: by mail-ob0-f178.google.com with SMTP id uz6so440434obc.9
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:08:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=dX5Y9MUKmImPPIShpWD/4aE4QsUqUWm0A35goqVaVRY=;
        b=UQKYUv3zL1bHGEG2gtGREBehwsMQk/ly3QZiIcVJbo4Pqk2TM/CVsK0yvZrNsOnp1d
         pHI5lLI+dzN//Ocykqg3hH6kAiQG41idCHzMZi8B3weQH0rjQ1vynl0vfe512162KFNv
         eELHkJ1ZO8VxCxEAY1Gty2S0ckK2migqgp+ZC1vLqbwVFcijejPB6f6lPWW11RCv7TMa
         ZdRVY6jbBuFH7yXVRRSeWZPzpBs0R/yYB4FpPzrsR9fnewAQw7lR2Zb+F9w38PQ+dGA6
         AMrXssNPqWpunOLnUEYH5QZBai9mYw0YE349PuoJYJSlVD/IfWEYg/Dm7sQ0lJtc/BVL
         mruA==
MIME-Version: 1.0
X-Received: by 10.202.196.137 with SMTP id u131mr1470674oif.78.1422439690993;
 Wed, 28 Jan 2015 02:08:10 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Wed, 28 Jan 2015 02:08:10 -0800 (PST)
In-Reply-To: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
Date: Wed, 28 Jan 2015 02:08:10 -0800
Message-ID: <CABPQxsukHr2V=D_SVEA71PuWfQqN8av9QzuCfkjrkmaHbTyBCg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Minor typo in the above e-mail - the tag is named v1.2.1-rc2 (not v1.2.1-rc1).

On Wed, Jan 28, 2015 at 2:06 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1062/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
>
> Changes from rc1:
> This has no code changes from RC1. Only minor changes to the release script.
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until  Saturday, January 31, at 10:04 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:14:06 2015
Return-Path: <dev-return-11323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4181D17BEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:14:06 +0000 (UTC)
Received: (qmail 20493 invoked by uid 500); 28 Jan 2015 10:14:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20421 invoked by uid 500); 28 Jan 2015 10:14:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20397 invoked by uid 99); 28 Jan 2015 10:14:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:14:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:13:35 +0000
Received: by mail-qc0-f176.google.com with SMTP id c9so15725121qcz.7
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:12:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=BvEZy15kdbKbGR0vqKJAGrzooMcJqkwkkRu+viw0zX4=;
        b=xmROqWPg9T1vYQcsptk4WRiv5pnduzditN6o7qMyx1bIyupPImiKDNLcoWdAa0fyP4
         nO0RfcM/Jaz+AOE9yYJuI9DmjxL7laWoHlr0LKacjnz3R8TjDqQbi/it+bwNm/iab5J5
         mgb/r/vkI8QEIZBmwu8/Rb8bB3PBoldZ+dUQ+NNcFppGUiDXk2RqciEp2LPCFAOiuvz9
         x/tLubWxEL/4i5jYCQ4qw/VzWqJU4shuDgkbXCiKtju/jnQWsvnKT3Mp72MyG8jtDlRa
         oUfbGOq4YmJ7xoPtMoqLTBYPQeZ1Q6a09iRPw4ScnzLlRDGMImr8/j4kLfj8FJ0Z+CXA
         FYXw==
X-Received: by 10.229.172.196 with SMTP id m4mr9476015qcz.19.1422439923990;
 Wed, 28 Jan 2015 02:12:03 -0800 (PST)
MIME-Version: 1.0
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Date: Wed, 28 Jan 2015 10:12:03 +0000
Message-ID: <CAJOb8bvJ1OiZBKQo6qHmdQ7kY2HcvF_=GPjrOGFsN_gyxH7tvw@mail.gmail.com>
Subject: Data source API | Support for dynamic schema
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01634ba26248fa050db39a12
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634ba26248fa050db39a12
Content-Type: text/plain; charset=UTF-8

I saw the talk on Spark data sources and looking at the interfaces, it
seems that the schema needs to be provided upfront. This works for many
data sources but I have a situation in which I would need to integrate a
system that supports schema evolutions by allowing users to change schema
without affecting existing rows. Basically, each row contains a schema hint
(id and version) and this allows developers to evolve schema over time and
perform migration at will. Since the schema needs to be specified upfront
in the data source API, one possible way would be to build a union of all
schema versions and handle populating row values appropriately. This works
in case columns have been added or deleted in the schema but doesn't work
if types have changed. I was wondering if it is possible to change the API
 to provide schema for each row instead of expecting data source to provide
schema upfront?

Thanks,
Aniket

--089e01634ba26248fa050db39a12--

From dev-return-11324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:18:31 2015
Return-Path: <dev-return-11324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24BC917C1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:18:31 +0000 (UTC)
Received: (qmail 34803 invoked by uid 500); 28 Jan 2015 10:18:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34718 invoked by uid 500); 28 Jan 2015 10:18:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34670 invoked by uid 99); 28 Jan 2015 10:18:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:18:29 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of aniket.bhatnagar@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:18:03 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 2685A120043E
	for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:17:02 -0800 (PST)
Date: Wed, 28 Jan 2015 03:17:00 -0700 (MST)
From: Aniket <aniket.bhatnagar@gmail.com>
To: dev@spark.apache.org
Message-ID: <CAJOb8bsjPeFDdcCZbHcbgPQiMNpfE9TwqJgnNom7zQGOP_6MPg@mail.gmail.com>
In-Reply-To: <CABPQxsukHr2V=D_SVEA71PuWfQqN8av9QzuCfkjrkmaHbTyBCg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com> <CABPQxsukHr2V=D_SVEA71PuWfQqN8av9QzuCfkjrkmaHbTyBCg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_85363_21391130.1422440220663"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_85363_21391130.1422440220663
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Patrick,

I am wondering if this version will address issues around certain artifacts
not getting published in 1.2 which are gating people to migrate to 1.2. One
such issue is https://issues.apache.org/jira/browse/SPARK-5144

Thanks,
Aniket

On Wed Jan 28 2015 at 15:39:43 Patrick Wendell [via Apache Spark Developers
List] <ml-node+s1001551n10318h3@n3.nabble.com> wrote:

> Minor typo in the above e-mail - the tag is named v1.2.1-rc2 (not
> v1.2.1-rc1).
>
> On Wed, Jan 28, 2015 at 2:06 AM, Patrick Wendell <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=10318&i=0>> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> 1.2.1!
> >
> > The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1062/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
> >
> > Changes from rc1:
> > This has no code changes from RC1. Only minor changes to the release
> script.
> >
> > Please vote on releasing this package as Apache Spark 1.2.1!
> >
> > The vote is open until  Saturday, January 31, at 10:04 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.1
> > [ ] -1 Do not release this package because ...
> >
> > For a list of fixes in this release, see http://s.apache.org/Mpn.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=10318&i=1>
> For additional commands, e-mail: [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=10318&i=2>
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10318.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h76@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10320.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_85363_21391130.1422440220663--

From dev-return-11325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:22:13 2015
Return-Path: <dev-return-11325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B7F717C4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:22:13 +0000 (UTC)
Received: (qmail 48590 invoked by uid 500); 28 Jan 2015 10:22:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48515 invoked by uid 500); 28 Jan 2015 10:22:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48502 invoked by uid 99); 28 Jan 2015 10:22:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:22:11 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:21:45 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so16811869oig.11
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:20:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=qMCuA9z/wk5GtqNbO4D+dW68KovHFal9v40LAIwK9bI=;
        b=iCwlgsqSS+dPDf6uA6miPBTUQXh96+ISp668l8VV4FrmaWa1Rc1L7iosMvd/PxbLp+
         eR9YYo73z+t28qKH1LHBYkm16lCz8WEgKng57iOhScg1v8XqdzLaxLBIFC0ZbUa7ByQf
         rMEAy4/roLp6fCk9iVOZn5RnYQx5ZhTnoVsS5W3Rs0F1uqi2Kx62T1YiFgJQ+Q5zRBAH
         dN/jWEt2Hl0HaraBBT+X1hz70S7VsqMYpLr96dn4vrNaT+MTJX46VC3owHfQG4aj79rH
         9AKlBXKHTZ57ZC5yIqmC1Q++mxkdxhGOdeWFYHsMbiK1KMzsFpVVYuzb9xYT5EqJrvbu
         PdNQ==
MIME-Version: 1.0
X-Received: by 10.182.213.38 with SMTP id np6mr1605191obc.34.1422440413938;
 Wed, 28 Jan 2015 02:20:13 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Wed, 28 Jan 2015 02:20:13 -0800 (PST)
In-Reply-To: <CAJOb8bsjPeFDdcCZbHcbgPQiMNpfE9TwqJgnNom7zQGOP_6MPg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
	<CABPQxsukHr2V=D_SVEA71PuWfQqN8av9QzuCfkjrkmaHbTyBCg@mail.gmail.com>
	<CAJOb8bsjPeFDdcCZbHcbgPQiMNpfE9TwqJgnNom7zQGOP_6MPg@mail.gmail.com>
Date: Wed, 28 Jan 2015 02:20:13 -0800
Message-ID: <CABPQxsvyMJnUPAbc9ZaRy=ZGp7DZx1hzZySbsOneMA3G5_JL8w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Aniket <aniket.bhatnagar@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yes - it fixes that issue.

On Wed, Jan 28, 2015 at 2:17 AM, Aniket <aniket.bhatnagar@gmail.com> wrote:
> Hi Patrick,
>
> I am wondering if this version will address issues around certain artifac=
ts
> not getting published in 1.2 which are gating people to migrate to 1.2. O=
ne
> such issue is https://issues.apache.org/jira/browse/SPARK-5144
>
> Thanks,
> Aniket
>
> On Wed Jan 28 2015 at 15:39:43 Patrick Wendell [via Apache Spark Develope=
rs
> List] <ml-node+s1001551n10318h3@n3.nabble.com> wrote:
>
>> Minor typo in the above e-mail - the tag is named v1.2.1-rc2 (not
>> v1.2.1-rc1).
>>
>> On Wed, Jan 28, 2015 at 2:06 AM, Patrick Wendell <[hidden email]
>> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10318&i=3D0>> wrote:
>>
>> > Please vote on releasing the following candidate as Apache Spark versi=
on
>> 1.2.1!
>> >
>> > The tag to be voted on is v1.2.1-rc1 (commit b77f876):
>> >
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Db77=
f87673d1f9f03d4c83cf583158227c551359b
>> >
>> > The release files, including signatures, digests, etc. can be found at=
:
>> > http://people.apache.org/~pwendell/spark-1.2.1-rc2/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/pwendell.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1062=
/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
>> >
>> > Changes from rc1:
>> > This has no code changes from RC1. Only minor changes to the release
>> script.
>> >
>> > Please vote on releasing this package as Apache Spark 1.2.1!
>> >
>> > The vote is open until  Saturday, January 31, at 10:04 UTC and passes
>> > if a majority of at least 3 +1 PMC votes are cast.
>> >
>> > [ ] +1 Release this package as Apache Spark 1.2.1
>> > [ ] -1 Do not release this package because ...
>> >
>> > For a list of fixes in this release, see http://s.apache.org/Mpn.
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: [hidden email]
>> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10318&i=3D1>
>> For additional commands, e-mail: [hidden email]
>> <http:///user/SendEmail.jtp?type=3Dnode&node=3D10318&i=3D2>
>>
>>
>>
>> ------------------------------
>>  If you reply to this email, your message will be added to the discussio=
n
>> below:
>>
>> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-A=
pache-Spark-1-2-1-RC2-tp10317p10318.html
>>  To start a new topic under Apache Spark Developers List, email
>> ml-node+s1001551n1h76@n3.nabble.com
>> To unsubscribe from Apache Spark Developers List, click here
>> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/Naml=
Servlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3DYW5pa2V0LmJoYXRuYWd=
hckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
>> .
>> NAML
>> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/Naml=
Servlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&ba=
se=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleN=
amespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nab=
bleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_su=
bscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_i=
nstant_email%21nabble%3Aemail.naml>
>>
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551=
.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10320.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble=
.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 10:42:07 2015
Return-Path: <dev-return-11326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7491B17CC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 10:42:07 +0000 (UTC)
Received: (qmail 81461 invoked by uid 500); 28 Jan 2015 10:42:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81388 invoked by uid 500); 28 Jan 2015 10:42:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81370 invoked by uid 99); 28 Jan 2015 10:42:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:42:00 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 10:41:55 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id B79D81200C1F
	for <dev@spark.apache.org>; Wed, 28 Jan 2015 02:41:15 -0800 (PST)
Date: Wed, 28 Jan 2015 03:41:14 -0700 (MST)
From: PierreB <pierre.borckmans@realimpactanalytics.com>
To: dev@spark.apache.org
Message-ID: <1422441674254-10322.post@n3.nabble.com>
In-Reply-To: <1422270942943-10269.post@n3.nabble.com>
References: <1422270942943-10269.post@n3.nabble.com>
Subject: Re: [SQL] Self join with ArrayType columns problems
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Should I file a JIRA for this?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/SQL-Self-join-with-ArrayType-columns-problems-tp10269p10322.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 13:18:24 2015
Return-Path: <dev-return-11327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5AE6117340
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 13:18:24 +0000 (UTC)
Received: (qmail 68886 invoked by uid 500); 28 Jan 2015 13:18:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68821 invoked by uid 500); 28 Jan 2015 13:18:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68806 invoked by uid 99); 28 Jan 2015 13:18:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:18:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.53 as permitted sender)
Received: from [74.125.82.53] (HELO mail-wg0-f53.google.com) (74.125.82.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:17:56 +0000
Received: by mail-wg0-f53.google.com with SMTP id a1so20443544wgh.12
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 05:17:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=7vX8FS99TLgvABEwPDrMlLB2ZBxXpWMXKtvOpFBmZ8A=;
        b=d41cRc3ncV64uz4PwArvgLsXiHGDb9FAAC2mL5sPk6od7RDGQZEtP1SyCCndRLDlzq
         6hXo9cuKJRrKSOckr9UGlRoKGJkDdY3sSp7h4f6NqSP0dYaFk9EYPv/J3VgKOY23hbYy
         NfRwql0gyJ96ZluawGAINFY96yHgXA5oS2QVHG63fPiPgOZjFlE5hNY0ycaH3d7Y54GL
         htaoMbyC6r8CKKkZBa07XDAVSNeyDo/VkOkRYowqM8UR0r5h5fu3ifJNtcASWxZkThtk
         RBA7DFBsB7VpnpXEdLwTQMEneBeCgfPX0DWaDFx1MG7sY3AYBKkvPxeqRLfJCGo44muY
         v2gw==
X-Gm-Message-State: ALoCoQnJYS0C8o3bS8M6c84vXJsuqOvdtln9w7QE/F+vE5s3Gm48+EVhP2+O1ddVlvRmvPkzh+x6
X-Received: by 10.194.88.131 with SMTP id bg3mr6929864wjb.99.1422451075591;
 Wed, 28 Jan 2015 05:17:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Wed, 28 Jan 2015 05:17:34 -0800 (PST)
In-Reply-To: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 28 Jan 2015 13:17:34 +0000
Message-ID: <CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (nonbinding). I verified that all the hash / signing items I
mentioned before are resolved.

The source package compiles on Ubuntu / Java 8. I ran tests and the
passed. Well, actually I see the same failure I've seeing locally on
OS X and on Ubuntu for a while, but I think nobody else has seen this?

MQTTStreamSuite:
- mqtt input stream *** FAILED ***
  org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in progress
  at org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientState.java:423)

Doesn't happen on Jenkins. If nobody else is seeing this, I suspect it
is something perhaps related to my env that I haven't figured out yet,
so should not be considered a blocker.

On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1062/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
>
> Changes from rc1:
> This has no code changes from RC1. Only minor changes to the release script.
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until  Saturday, January 31, at 10:04 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 13:22:41 2015
Return-Path: <dev-return-11328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 828DE17369
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 13:22:41 +0000 (UTC)
Received: (qmail 76184 invoked by uid 500); 28 Jan 2015 13:22:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76107 invoked by uid 500); 28 Jan 2015 13:22:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76093 invoked by uid 99); 28 Jan 2015 13:22:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:22:40 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of advancedxy@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:22:35 +0000
Received: by mail-pa0-f47.google.com with SMTP id lj1so25514756pab.6
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 05:22:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=tOIhh+YwDzrd7r0+qhgjTmU6yPnmIKdKHlf5D7/i66g=;
        b=0IN4PsHDXWycI90FPIMQbBFVjpFJoap3bG2BQrl7hggE878PV5XPUKy4rFJwjVt6gM
         blt5cbRG04/U8s9JHCfH68JcAq09C40d7GuL3yDrXq6pIyoIt5dil7djGl4SPAxkSBPX
         Gs2mtHF3DkAGRBwFDCGrLzb4MWtDAlP1iqewbtxyM4z9oQD9B7xhh11bffzbuFn8dC0A
         adj+82zeEbIf0rVnBSCym+TNX5xNVLf0t7rFQS0t8EGozAdOUDBtfYD5X9b7iyABC3ha
         dGod5icwCEfF1tGsJouUTsWa89r79OURN5vbZc4hcs62zh9UQ6Ozw1E4MlgX2nj3tYDX
         X3FQ==
X-Received: by 10.70.36.99 with SMTP id p3mr5545087pdj.81.1422451334946;
        Wed, 28 Jan 2015 05:22:14 -0800 (PST)
Received: from [172.16.100.171] (li580-54.members.linode.com. [106.186.22.54])
        by mx.google.com with ESMTPSA id cb9sm4797335pad.46.2015.01.28.05.22.12
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Wed, 28 Jan 2015 05:22:14 -0800 (PST)
Date: Wed, 28 Jan 2015 21:22:01 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <1B2447D4B1834F02B516CC01E6BA3B66@gmail.com>
In-Reply-To: <CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
 <CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54c8e279_515f007c_172"
X-Virus-Checked: Checked by ClamAV on apache.org

--54c8e279_515f007c_172
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Sean, =20
the MQRRStreamSuite is also failed for me on Mac OS X, Though I don=E2=80=
=99t have time to invest that.

-- =20
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/=3Fsig)


On Wednesday, January 28, 2015 at 9:17 PM, Sean Owen wrote:

> +1 (nonbinding). I verified that all the hash / signing items I
> mentioned before are resolved.
> =20
> The source package compiles on Ubuntu / Java 8. I ran tests and the
> passed. Well, actually I see the same failure I've seeing locally on
> OS X and on Ubuntu for a while, but I think nobody else has seen this=3F=

> =20
> MQTTStreamSuite:
> - mqtt input stream *** =46AILED ***
> org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in pro=
gress
> at org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientState=
.java:423)
> =20
> Doesn't happen on Jenkins. If nobody else is seeing this, I suspect it
> is something perhaps related to my env that I haven't figured out yet,
> so should not be considered a blocker.
> =20
> On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell <pwendell=40gmail.com=
 (mailto:pwendell=40gmail.com)> wrote:
> > Please vote on releasing the following candidate as Apache Spark vers=
ion 1.2.1=21
> > =20
> > The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> > https://git-wip-us.apache.org/repos/asf=3Fp=3Dspark.git;a=3Dcommit;h=3D=
b77f87673d1f9f03d4c83cf583158227c551359b
> > =20
> > The release files, including signatures, digests, etc. can be found a=
t:
> > http://people.apache.org/=7Epwendell/spark-1.2.1-rc2/
> > =20
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> > =20
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-106=
2/
> > =20
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/=7Epwendell/spark-1.2.1-rc2-docs/
> > =20
> > Changes from rc1:
> > This has no code changes from RC1. Only minor changes to the release =
script.
> > =20
> > Please vote on releasing this package as Apache Spark 1.2.1=21
> > =20
> > The vote is open until Saturday, January 31, at 10:04 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> > =20
> > =5B =5D +1 Release this package as Apache Spark 1.2.1
> > =5B =5D -1 Do not release this package because ...
> > =20
> > =46or a list of fixes in this release, see http://s.apache.org/Mpn.
> > =20
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> > =20
> > ---------------------------------------------------------------------=

> > To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org (mailto:de=
v-unsubscribe=40spark.apache.org)
> > =46or additional commands, e-mail: dev-help=40spark.apache.org (mailt=
o:dev-help=40spark.apache.org)
> > =20
> =20
> =20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org (mailto:dev-=
unsubscribe=40spark.apache.org)
> =46or additional commands, e-mail: dev-help=40spark.apache.org (mailto:=
dev-help=40spark.apache.org)
> =20
> =20



--54c8e279_515f007c_172--


From dev-return-11329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 13:47:01 2015
Return-Path: <dev-return-11329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBF1517466
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 13:47:01 +0000 (UTC)
Received: (qmail 28309 invoked by uid 500); 28 Jan 2015 13:47:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28243 invoked by uid 500); 28 Jan 2015 13:47:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28232 invoked by uid 99); 28 Jan 2015 13:47:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:47:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 13:46:34 +0000
Received: by mail-we0-f173.google.com with SMTP id w62so20881447wes.4
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 05:45:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=/cizNmf4WrJYKzwHCIKFhIkr9ZuyEZfqVDWARSs7hW8=;
        b=FfM/lILASay8dljmYfFxBdk0p8RbbDhzbN4n5ZauPgCnexGyTx+zhVdDujYBplwLfe
         Ihd7tQycjYQ6Vq2uA24gJ85HUJP3crvPdnQFiTSIxEKQufwlqQyWUIHS5+PWIKXUJXQu
         FfPFfPW8wK46VY7+3Vl4tOQ6arIE/yaHVXCsAUMEvDzLaVjyKJbIchWWLvNyghpCmZnO
         1V25lgJUDyagIz6X/6ROVbhU2wQpXt9vecgWhoBnmTvEv5hM8UIGRTTz6sDwmtLYJnqz
         Ta30Lz0UdRJSDdSBgOsIVL++i9fZ4KkwQodW8NNo933y9NgpJ6JXrQBEHhmTGJ0a1nAr
         Bm3Q==
X-Gm-Message-State: ALoCoQnc2c9wl0dej8WQKNvmff9YgooiVjT+ZOPvlnYP6w4LNgp8JoMqvdxur4NXy9YGYhtMaURB
X-Received: by 10.194.239.104 with SMTP id vr8mr7647958wjc.124.1422452703112;
 Wed, 28 Jan 2015 05:45:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Wed, 28 Jan 2015 05:44:41 -0800 (PST)
In-Reply-To: <1B2447D4B1834F02B516CC01E6BA3B66@gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
 <CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com> <1B2447D4B1834F02B516CC01E6BA3B66@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 28 Jan 2015 13:44:41 +0000
Message-ID: <CAMAsSd+nsPqpjuGVisqYpVeThqVGqqzW_-WqTm6S57i7uf8otw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
To: Ye Xianjin <advancedxy@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

We had both been using Java 8; Ye reports that it fails on Java 6 too.
We both believe this has been failing for a fair while, so I do not
think it's a regression. I'll make a JIRA though.

On Wed, Jan 28, 2015 at 1:22 PM, Ye Xianjin <advancedxy@gmail.com> wrote:
> Sean,
> the MQRRStreamSuite is also failed for me on Mac OS X, Though I don=E2=80=
=99t have
> time to invest that.
>
> --
> Ye Xianjin
> Sent with Sparrow
>
> On Wednesday, January 28, 2015 at 9:17 PM, Sean Owen wrote:
>
> +1 (nonbinding). I verified that all the hash / signing items I
> mentioned before are resolved.
>
> The source package compiles on Ubuntu / Java 8. I ran tests and the
> passed. Well, actually I see the same failure I've seeing locally on
> OS X and on Ubuntu for a while, but I think nobody else has seen this?
>
> MQTTStreamSuite:
> - mqtt input stream *** FAILED ***
> org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in progr=
ess
> at
> org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientState.java=
:423)
>
> Doesn't happen on Jenkins. If nobody else is seeing this, I suspect it
> is something perhaps related to my env that I haven't figured out yet,
> so should not be considered a blocker.
>
> On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Db77f=
87673d1f9f03d4c83cf583158227c551359b
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1062/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
>
> Changes from rc1:
> This has no code changes from RC1. Only minor changes to the release scri=
pt.
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until Saturday, January 31, at 10:04 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 14:54:34 2015
Return-Path: <dev-return-11330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 970741775E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 14:54:34 +0000 (UTC)
Received: (qmail 12061 invoked by uid 500); 28 Jan 2015 14:54:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11996 invoked by uid 500); 28 Jan 2015 14:54:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11982 invoked by uid 99); 28 Jan 2015 14:54:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 14:54:32 +0000
X-ASF-Spam-Status: No, hits=4.0 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 14:54:07 +0000
Received: by mail-oi0-f45.google.com with SMTP id g201so17838553oib.4
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 06:53:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:cc
         :content-type;
        bh=e8YvFED539YOJrfA8HhMCtlY5NlO6IUQmjIicg25XVc=;
        b=Ju4d4MIaurPwdJPbCuwOzbLC1MH1FrbNoJYuCOlwR6hJN8zMQjMS4z3D4HLsxcm8Ee
         W1N/l0w/ut4uuAjUQisD2YRwxa/rzobH2KS5Ml9qYqw996wWbYqj/sdphZpmSYhmT6Nr
         YMDqXPWjdiNNpW+3EfR5PjNWzQANy4WQOfwbxgdWWhOpLlSIip7mZ3uSyPAmrVuRobrG
         7d2En5DaNoxhEVO+dkHA3Yk5rEV3iIU/Kq5A9PYK1lghbloeracc0txDeHW9tNCLuW+Y
         3qwa89dTy2N8ar5EhyAzc3c5Mc+seMAPWTN+y2Z3B0lUGqjQCW7H9f30vhmyovHkO0P1
         Mp5A==
X-Received: by 10.202.97.130 with SMTP id v124mr2256504oib.34.1422456801035;
 Wed, 28 Jan 2015 06:53:21 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.131 with HTTP; Wed, 28 Jan 2015 06:52:40 -0800 (PST)
In-Reply-To: <CAMAsSd+zO+_QyvwRU8pfeFKBmDcutDTK23qeWXKVPXUyZa79dA@mail.gmail.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
 <1422357486167-10285.post@n3.nabble.com> <CAMAsSd+zO+_QyvwRU8pfeFKBmDcutDTK23qeWXKVPXUyZa79dA@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Wed, 28 Jan 2015 12:52:40 -0200
Message-ID: <CAO4-Pq-t57ZHHrw9hJ-u=YqYbofRj42Es=diKSpSJKKz+GBw+Q@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d5966558ce1050db788f9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d5966558ce1050db788f9
Content-Type: text/plain; charset=UTF-8

I was facing the same problem, and I fixed it by adding

<plugin>
<artifactId>maven-assembly-plugin</artifactId>
        <version>2.4.1</version>
        <configuration>
          <descriptors>
            <descriptor>assembly/src/main/assembly/assembly.xml</descriptor>
          </descriptors>
        </configuration>
</plugin>
 in the root pom.xml, following the maven assembly plugin docs
<http://maven.apache.org/plugins-archives/maven-assembly-plugin-2.4.1/examples/multimodule/module-source-inclusion-simple.html>

I can make a PR on this if you consider this an issue.

Now I'm facing this problem, is that what you have now?
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-assembly-plugin:2.4.1:single (default-cli)
on project spark-network-common_2.10: Failed to create assembly: Error
adding file 'org.apache.spark:spark-network-common_2.10:jar:1.3.0-SNAPSHOT'
to archive:
/home/dirceu/projects/spark/network/common/target/scala-2.10/classes isn't
a file. -> [Help 1]


2015-01-27 9:23 GMT-02:00 Sean Owen <sowen@cloudera.com>:

> You certainly do not need yo build Spark as root. It might clumsily
> overcome a permissions problem in your local env but probably causes other
> problems.
> On Jan 27, 2015 11:18 AM, "angel__" <angel.alvarez.pascua@gmail.com>
> wrote:
>
> > I had that problem when I tried to build Spark 1.2. I don't exactly know
> > what
> > is causing it, but I guess it might have something to do with user
> > permissions.
> >
> > I could finally fix this by building Spark as "root" user (now I'm
> dealing
> > with another problem, but ...that's another story...)
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Use-mvn-to-build-Spark-1-2-0-failed-tp9876p10285.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a113d5966558ce1050db788f9--

From dev-return-11331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 15:35:56 2015
Return-Path: <dev-return-11331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D03711798F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 15:35:56 +0000 (UTC)
Received: (qmail 36122 invoked by uid 500); 28 Jan 2015 15:35:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35997 invoked by uid 500); 28 Jan 2015 15:35:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35974 invoked by uid 99); 28 Jan 2015 15:35:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 15:35:55 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 15:35:51 +0000
Received: by mail-wi0-f180.google.com with SMTP id h11so12691899wiw.1
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 07:33:15 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=hIKWcCnmOLT57ZwALx7T4oJzw67S0ZGSnXDknWZDJfA=;
        b=BjIgIKaJjgzgcAe7YRHlYHkiZxj9rXWq9cfU8F/ndcd52dsdvXOD8Xi2UrnILCyPF9
         UYs3hRaZoSmjhcYGMFEhNgbpIK1UlEjy+mgtKRQUCf4QD9J1Bg/foizPFx/IJ8k6cAlE
         xKZjqI8zavYaFjpbkSWPkPkOGIEwYA8Z3CSoKMq/ndHKFZIR0NiBDN8LZOHkdZGuTyti
         yzf4XKtrexlggypkgY6bBKmIH4GlO4MvT8oqzYPSzx8uYqtcfL0GqvoTZ1xlNSfY6H7S
         91ABOy7ybn49uwoksZTHMfoEUKKi90a9gLua7VXgRHluqcBlSX4HAhCVPxiOKWC7Eh8d
         IpeA==
X-Gm-Message-State: ALoCoQl39qZ814vaAbqXfQLmrBiIViN3KGn+f0X7Fa6DMBx7uAFMYwzGlyrePrmhiOvbtivW17xq
X-Received: by 10.180.211.169 with SMTP id nd9mr8191810wic.4.1422459195430;
 Wed, 28 Jan 2015 07:33:15 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.76 with HTTP; Wed, 28 Jan 2015 07:32:55 -0800 (PST)
In-Reply-To: <CAO4-Pq-t57ZHHrw9hJ-u=YqYbofRj42Es=diKSpSJKKz+GBw+Q@mail.gmail.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
 <1422357486167-10285.post@n3.nabble.com> <CAMAsSd+zO+_QyvwRU8pfeFKBmDcutDTK23qeWXKVPXUyZa79dA@mail.gmail.com>
 <CAO4-Pq-t57ZHHrw9hJ-u=YqYbofRj42Es=diKSpSJKKz+GBw+Q@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 28 Jan 2015 15:32:55 +0000
Message-ID: <CAMAsSdLxDHN2nn7cnM-m4wicPxVp3WC9XYSd-+8B4y=gCuJrMg@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
To: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't see how this would relate to the problem in the OP? the
assemblies build fine already as far as I can tell.

Your new error may be introduced by your change.

On Wed, Jan 28, 2015 at 2:52 PM, Dirceu Semighini Filho
<dirceu.semighini@gmail.com> wrote:
> I was facing the same problem, and I fixed it by adding
>
> <plugin>
> <artifactId>maven-assembly-plugin</artifactId>
>         <version>2.4.1</version>
>         <configuration>
>           <descriptors>
>             <descriptor>assembly/src/main/assembly/assembly.xml</descriptor>
>           </descriptors>
>         </configuration>
> </plugin>
>  in the root pom.xml, following the maven assembly plugin docs
> <http://maven.apache.org/plugins-archives/maven-assembly-plugin-2.4.1/examples/multimodule/module-source-inclusion-simple.html>
>
> I can make a PR on this if you consider this an issue.
>
> Now I'm facing this problem, is that what you have now?
> [ERROR] Failed to execute goal
> org.apache.maven.plugins:maven-assembly-plugin:2.4.1:single (default-cli)
> on project spark-network-common_2.10: Failed to create assembly: Error
> adding file 'org.apache.spark:spark-network-common_2.10:jar:1.3.0-SNAPSHOT'
> to archive:
> /home/dirceu/projects/spark/network/common/target/scala-2.10/classes isn't
> a file. -> [Help 1]
>
>
> 2015-01-27 9:23 GMT-02:00 Sean Owen <sowen@cloudera.com>:
>
>> You certainly do not need yo build Spark as root. It might clumsily
>> overcome a permissions problem in your local env but probably causes other
>> problems.
>> On Jan 27, 2015 11:18 AM, "angel__" <angel.alvarez.pascua@gmail.com>
>> wrote:
>>
>> > I had that problem when I tried to build Spark 1.2. I don't exactly know
>> > what
>> > is causing it, but I guess it might have something to do with user
>> > permissions.
>> >
>> > I could finally fix this by building Spark as "root" user (now I'm
>> dealing
>> > with another problem, but ...that's another story...)
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Use-mvn-to-build-Spark-1-2-0-failed-tp9876p10285.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 16:19:54 2015
Return-Path: <dev-return-11332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0463717B4A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 16:19:54 +0000 (UTC)
Received: (qmail 66022 invoked by uid 500); 28 Jan 2015 16:19:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65951 invoked by uid 500); 28 Jan 2015 16:19:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65939 invoked by uid 99); 28 Jan 2015 16:19:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 16:19:52 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 16:19:48 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so18278119oig.11
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 08:17:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=0EhucoMRCskOYPPYcaVb0cR1n9jqk4BDAupkTrhOolI=;
        b=hXBWTEyXkZElyLTV4PYoXe7ju7BVDSkxPC465NGH4Emh4UbVDAj+G54r7C4J+90Dux
         YcrNpjA/gSxXoMeSfCvSlSXS58sdS52dC/zx0bKgT6QuS7qEOHz4XkgOV4vCQX/WlJ7X
         Z8u25sjmPR8/2GYYBN+SxI4li42H9d/Yk9xhqnKbL3iLW0QM/LTPDGzXMVYVRR3Re/7m
         EpwRBL64BkrzVFmH/dwL9H9jx3WBZ1AzQBMUkOAoyErrYbJ/L/5RB0JfgZJmJ9IRfTpq
         ZCp6KQzAIZtS4D6PazyRN+ymTdXIqlVsBSegX7Ut0Zu1GpPzgPTLboxhiQLmGgTTYYQr
         9qmQ==
X-Received: by 10.202.177.195 with SMTP id a186mr2506156oif.76.1422461878276;
 Wed, 28 Jan 2015 08:17:58 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.131 with HTTP; Wed, 28 Jan 2015 08:17:18 -0800 (PST)
In-Reply-To: <CAMAsSdLxDHN2nn7cnM-m4wicPxVp3WC9XYSd-+8B4y=gCuJrMg@mail.gmail.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
 <1422357486167-10285.post@n3.nabble.com> <CAMAsSd+zO+_QyvwRU8pfeFKBmDcutDTK23qeWXKVPXUyZa79dA@mail.gmail.com>
 <CAO4-Pq-t57ZHHrw9hJ-u=YqYbofRj42Es=diKSpSJKKz+GBw+Q@mail.gmail.com> <CAMAsSdLxDHN2nn7cnM-m4wicPxVp3WC9XYSd-+8B4y=gCuJrMg@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Wed, 28 Jan 2015 14:17:18 -0200
Message-ID: <CAO4-Pq90wTz0kUsC9z99Fu5xB=uXcMZkW8w4CMrNwTXzNBpdQw@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
To: Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113cddcef61868050db8b6f6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cddcef61868050db8b6f6
Content-Type: text/plain; charset=UTF-8

Before this I was facing the same problem, and fixed it adding the plugin
at the root pom.xml

Maybe this is related to the release, mine is:
Apache Maven 3.2.3 (33f8c3e1027c3ddde99d3cdebad2656a31e8fdf4;
2014-08-11T17:58:10-03:00)
Java version: 1.8.0_20, vendor: Oracle Corporation
OS name: "linux", version: "3.13.0-24-generic", arch: "amd64", family:
"unix"

Or the command that I'm using:
mvn -Dhadoop.version=2.0.0-mr1-cdh4.2.0 -DskipTests -Phive
-Phive-thriftserver clean compile assembly:single

I'm trying to build using the pr/1290
wyphao.2007 have you figured out how to complete the build?



2015-01-28 13:32 GMT-02:00 Sean Owen <sowen@cloudera.com>:

> I don't see how this would relate to the problem in the OP? the
> assemblies build fine already as far as I can tell.
>
> Your new error may be introduced by your change.
>
> On Wed, Jan 28, 2015 at 2:52 PM, Dirceu Semighini Filho
> <dirceu.semighini@gmail.com> wrote:
> > I was facing the same problem, and I fixed it by adding
> >
> > <plugin>
> > <artifactId>maven-assembly-plugin</artifactId>
> >         <version>2.4.1</version>
> >         <configuration>
> >           <descriptors>
> >
>  <descriptor>assembly/src/main/assembly/assembly.xml</descriptor>
> >           </descriptors>
> >         </configuration>
> > </plugin>
> >  in the root pom.xml, following the maven assembly plugin docs
> > <
> http://maven.apache.org/plugins-archives/maven-assembly-plugin-2.4.1/examples/multimodule/module-source-inclusion-simple.html
> >
> >
> > I can make a PR on this if you consider this an issue.
> >
> > Now I'm facing this problem, is that what you have now?
> > [ERROR] Failed to execute goal
> > org.apache.maven.plugins:maven-assembly-plugin:2.4.1:single (default-cli)
> > on project spark-network-common_2.10: Failed to create assembly: Error
> > adding file
> 'org.apache.spark:spark-network-common_2.10:jar:1.3.0-SNAPSHOT'
> > to archive:
> > /home/dirceu/projects/spark/network/common/target/scala-2.10/classes
> isn't
> > a file. -> [Help 1]
> >
> >
> > 2015-01-27 9:23 GMT-02:00 Sean Owen <sowen@cloudera.com>:
> >
> >> You certainly do not need yo build Spark as root. It might clumsily
> >> overcome a permissions problem in your local env but probably causes
> other
> >> problems.
> >> On Jan 27, 2015 11:18 AM, "angel__" <angel.alvarez.pascua@gmail.com>
> >> wrote:
> >>
> >> > I had that problem when I tried to build Spark 1.2. I don't exactly
> know
> >> > what
> >> > is causing it, but I guess it might have something to do with user
> >> > permissions.
> >> >
> >> > I could finally fix this by building Spark as "root" user (now I'm
> >> dealing
> >> > with another problem, but ...that's another story...)
> >> >
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >> >
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Use-mvn-to-build-Spark-1-2-0-failed-tp9876p10285.html
> >> > Sent from the Apache Spark Developers List mailing list archive at
> >> > Nabble.com.
> >> >
> >> > ---------------------------------------------------------------------
> >> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > For additional commands, e-mail: dev-help@spark.apache.org
> >> >
> >> >
> >>
>

--001a113cddcef61868050db8b6f6--

From dev-return-11333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 16:34:17 2015
Return-Path: <dev-return-11333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C48F17BE5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 16:34:17 +0000 (UTC)
Received: (qmail 15838 invoked by uid 500); 28 Jan 2015 16:34:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15770 invoked by uid 500); 28 Jan 2015 16:34:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15554 invoked by uid 99); 28 Jan 2015 16:34:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 16:34:11 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 16:33:45 +0000
Received: by mail-ig0-f177.google.com with SMTP id z20so12070719igj.4
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 08:33:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=iwpbG2Nk1uzaNXlY3MJwxXSjkV4zYrlVktbNe0K4DSM=;
        b=mH4ntwlIieauCzSPbzK0vSP0+NbBADiZd//IuARCiBKjTzakOB18lGqLEoElYAKUFD
         lh/x29CrphbmUZnXeKNnA/BK2JJ0sWmMl/QT8j3Zu1/k9GYQRrPPeXDFcI+M+AjYfvjW
         Lbbjty8yOEf+Hihb6D3h4roT3U7ZWfSXUo2Fzx6rypAbuKebgmFrfLia9byZNhKLQFNm
         hOMl17RdCzMMinlFwYW9yNMQGmJuBFx7kI5JLOiS1z8AmwXCGCG8AF9qPa31O9XB8To9
         nKVBnUm8qI22EjE/aojZpxzT6zIKIgYLyp8yZeo1ML5+MF/HNoD3XGffr0xXPI19/Qox
         Cs+g==
X-Received: by 10.107.157.67 with SMTP id g64mr4979547ioe.72.1422462823864;
 Wed, 28 Jan 2015 08:33:43 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzdhq=puUXZPyVtebS2Pn2n1CpqcJ4SpcZ6a_pRC87kjVQ@mail.gmail.com>
 <CALte62y7a6WyBDUFDcGUwbf8WCpttViE+PAo4pZOR+_-nB2UTw@mail.gmail.com>
 <CABPQxsvDNDNyfO86vUMHW622Tb7fkwK94M1O=u-8U2ANGA3CEQ@mail.gmail.com>
 <CAOhmDzeAevbfC_YYt+bBN6vO1CZYVVLWJ_XA_fuO0xTYLnNm7g@mail.gmail.com>
 <CAAswR-5EnHKb9SKk=E7T84-ckLDoXwdeqRSuz3KK5Yy2F=p6Jg@mail.gmail.com>
 <CAOhmDzeNVb4Gk2-xf3UV1d4doSzdZmB0TkUOR_QTm4U-NVER9A@mail.gmail.com>
 <542CCF41.8000105@gmail.com> <CAOhmDzfWXfdFn9DNKJvsh2psXhQ1NCH9i=tfTML0m5HO1CLNyA@mail.gmail.com>
 <CAOhmDzcrLkKeRSmSo7Huk-No4dt=AjRhMKdjUcrhrfnsugSacw@mail.gmail.com> <CAPh_B=YB6AXNaOG4TwNwr-+q7XN9sCjHr1_OtXjK2r2t0Gd-xA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 28 Jan 2015 16:33:43 +0000
Message-ID: <CAOhmDzdFqnm7mqD=-LchdckRRm3N0qooBuJTwOdYo_t1pgshQg@mail.gmail.com>
Subject: Re: Extending Scala style checks
To: Reynold Xin <rxin@databricks.com>
Cc: Cheng Lian <lian.cs.zju@gmail.com>, Michael Armbrust <michael@databricks.com>, 
	Patrick Wendell <pwendell@gmail.com>, Ted Yu <yuzhihong@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1140cd1e52a42b050db8ef8e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140cd1e52a42b050db8ef8e
Content-Type: text/plain; charset=UTF-8

FYI: scalastyle just merged in a patch to add support for external rules
<https://github.com/scalastyle/scalastyle/issues/25#ref-commit-9a576c3>.

I forget why I was following the linked issue, but I assume it's related to
this discussion.

Nick


On Thu Oct 09 2014 at 2:56:30 AM Reynold Xin <rxin@databricks.com> wrote:

> Thanks. I added one.
>
>
> On Wed, Oct 8, 2014 at 8:49 AM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> I've created SPARK-3849: Automate remaining Scala style rules
>>
> <https://issues.apache.org/jira/browse/SPARK-3849>.
>
>
>>
>> Please create sub-tasks on this issue for rules that we have not automated
>> and let's work through them as possible.
>>
>> I went ahead and created the first sub-task, SPARK-3850: Scala style:
>>
> Disallow trailing spaces <https://issues.apache.org/jira/browse/SPARK-3850
>> >.
>
>
>>
>> Nick
>>
>> On Tue, Oct 7, 2014 at 4:45 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com
>> > wrote:
>>
>> > For starters, do we have a list of all the Scala style rules that are
>> > currently not enforced automatically but are likely well-suited for
>> > automation?
>> >
>> > Let's put such a list together in a JIRA issue and work through
>> > implementing them.
>> >
>> > Nick
>> >
>> > On Thu, Oct 2, 2014 at 12:06 AM, Cheng Lian <lian.cs.zju@gmail.com>
>> wrote:
>> >
>> >> Since we can easily catch the list of all changed files in a PR, I
>> think
>> >> we can start with adding the no trailing space check for newly changed
>> >> files only?
>> >>
>> >>
>> >> On 10/2/14 9:24 AM, Nicholas Chammas wrote:
>> >>
>> >>> Yeah, I remember that hell when I added PEP 8 to the build checks and
>> >>> fixed
>> >>> all the outstanding Python style issues. I had to keep rebasing and
>> >>> resolving merge conflicts until the PR was merged.
>> >>>
>> >>> It's a rough process, but thankfully it's also a one-time process. I
>> >>> might
>> >>> be able to help with that in the next week or two if no-one else
>> wants to
>> >>> pick it up.
>> >>>
>> >>> Nick
>> >>>
>> >>> On Wed, Oct 1, 2014 at 9:20 PM, Michael Armbrust <
>> michael@databricks.com
>> >>> >
>> >>> wrote:
>> >>>
>> >>>  The hard part here is updating the existing code base... which is
>> going
>> >>>> to
>> >>>> create merge conflicts with like all of the open PRs...
>> >>>>
>> >>>> On Wed, Oct 1, 2014 at 6:13 PM, Nicholas Chammas <
>> >>>> nicholas.chammas@gmail.com> wrote:
>> >>>>
>> >>>>  Ah, since there appears to be a built-in rule for end-of-line
>> >>>>> whitespace,
>> >>>>> Michael and Cheng, y'all should be able to add this in pretty
>> easily.
>> >>>>>
>> >>>>> Nick
>> >>>>>
>> >>>>> On Wed, Oct 1, 2014 at 6:37 PM, Patrick Wendell <pwendell@gmail.com
>> >
>> >>>>> wrote:
>> >>>>>
>> >>>>>  Hey Nick,
>> >>>>>>
>> >>>>>> We can always take built-in rules. Back when we added this Prashant
>> >>>>>> Sharma actually did some great work that lets us write our own
>> style
>> >>>>>> rules in cases where rules don't exist.
>> >>>>>>
>> >>>>>> You can see some existing rules here:
>> >>>>>>
>> >>>>>>
>> >>>>>>  https://github.com/apache/spark/tree/master/project/
>> >>>>> spark-style/src/main/scala/org/apache/spark/scalastyle
>> >>>>>
>> >>>>>> Prashant has over time contributed a lot of our custom rules
>> upstream
>> >>>>>> to stalastyle, so now there are only a couple there.
>> >>>>>>
>> >>>>>> - Patrick
>> >>>>>>
>> >>>>>> On Wed, Oct 1, 2014 at 2:36 PM, Ted Yu <yuzhihong@gmail.com>
>> wrote:
>> >>>>>>
>> >>>>>>> Please take a look at WhitespaceEndOfLineChecker under:
>> >>>>>>> http://www.scalastyle.org/rules-0.1.0.html
>> >>>>>>>
>> >>>>>>> Cheers
>> >>>>>>>
>> >>>>>>> On Wed, Oct 1, 2014 at 2:01 PM, Nicholas Chammas <
>> >>>>>>>
>> >>>>>> nicholas.chammas@gmail.com
>> >>>>>>
>> >>>>>>> wrote:
>> >>>>>>>> As discussed here <https://github.com/apache/spark/pull/2619>,
>> it
>> >>>>>>>>
>> >>>>>>> would be
>> >>>>>>
>> >>>>>>> good to extend our Scala style checks to programmatically enforce
>> as
>> >>>>>>>>
>> >>>>>>> many
>> >>>>>>
>> >>>>>>> of our style rules as possible.
>> >>>>>>>>
>> >>>>>>>> Does anyone know if it's relatively straightforward to enforce
>> >>>>>>>>
>> >>>>>>> additional
>> >>>>>>
>> >>>>>>> rules like the "no trailing spaces" rule mentioned in the linked
>> PR?
>> >>>>>>>>
>> >>>>>>>> Nick
>> >>>>>>>>
>> >>>>>>>>
>> >>>>
>> >>
>> >
>>
>

--001a1140cd1e52a42b050db8ef8e--

From dev-return-11334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 19:26:40 2015
Return-Path: <dev-return-11334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 023EC1757E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 19:26:40 +0000 (UTC)
Received: (qmail 14139 invoked by uid 500); 28 Jan 2015 19:26:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14063 invoked by uid 500); 28 Jan 2015 19:26:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14051 invoked by uid 99); 28 Jan 2015 19:26:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 19:26:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.50 as permitted sender)
Received: from [209.85.220.50] (HELO mail-pa0-f50.google.com) (209.85.220.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 19:26:12 +0000
Received: by mail-pa0-f50.google.com with SMTP id rd3so28881630pab.9
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 11:26:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=RAnffXcnGUEULGkyPLq7rdkIzKu9fqUyOX7r9g/eIHo=;
        b=sUEgm4Y4nLK6S0S/cBMQtCMTldiMeeYOaVaiI9vYhjz4Na5MaXaXE7D9yO73MhNQeQ
         hlpSNQm8O4jHjKTw2q9rg611EXgR7A8DWiS1uhQ8lPperPJVHvm+QA2S8YXzH1Qsi4bv
         Skvlztp7kgXr6XN6cFL73QImZy7tB5OpKNZby4Ue+KbNXS5gS6sJCF4G1XnSyv8hGUUw
         +QP7em1N3wQJr7X5z3t3AarFuXiLL/ZIjFQsUzm5Pa0feFQuthxQwUU9nurxFy2RmjjD
         rpUag1BWAuo0Yu1KKX9V5xr7Xja7e39BRe7qTNk1sh4SyQQw4997Rb+FUxpogp+DnEh1
         y8Dg==
X-Received: by 10.70.134.97 with SMTP id pj1mr8195974pdb.125.1422473170402;
        Wed, 28 Jan 2015 11:26:10 -0800 (PST)
Received: from [192.168.1.168] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id ki4sm5508221pdb.34.2015.01.28.11.26.09
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 28 Jan 2015 11:26:09 -0800 (PST)
Message-ID: <54C937D0.1020000@gmail.com>
Date: Wed, 28 Jan 2015 11:26:08 -0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Data source API | Support for dynamic schema
References: <CAJOb8bvJ1OiZBKQo6qHmdQ7kY2HcvF_=GPjrOGFsN_gyxH7tvw@mail.gmail.com>
In-Reply-To: <CAJOb8bvJ1OiZBKQo6qHmdQ7kY2HcvF_=GPjrOGFsN_gyxH7tvw@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Aniket,

In general the schema of all rows in a single table must be same. This 
is a basic assumption made by Spark SQL. Schema union does make sense, 
and we're planning to support this for Parquet. But as you've mentioned, 
it doesn't help if types of different versions of a column differ from 
each other. Also, you need to reload the data source table after schema 
changes happen.

Cheng

On 1/28/15 2:12 AM, Aniket Bhatnagar wrote:
> I saw the talk on Spark data sources and looking at the interfaces, it
> seems that the schema needs to be provided upfront. This works for many
> data sources but I have a situation in which I would need to integrate a
> system that supports schema evolutions by allowing users to change schema
> without affecting existing rows. Basically, each row contains a schema hint
> (id and version) and this allows developers to evolve schema over time and
> perform migration at will. Since the schema needs to be specified upfront
> in the data source API, one possible way would be to build a union of all
> schema versions and handle populating row values appropriately. This works
> in case columns have been added or deleted in the schema but doesn't work
> if types have changed. I was wondering if it is possible to change the API
>   to provide schema for each row instead of expecting data source to provide
> schema upfront?
>
> Thanks,
> Aniket
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 19:40:10 2015
Return-Path: <dev-return-11335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D36EF17627
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 19:40:10 +0000 (UTC)
Received: (qmail 60664 invoked by uid 500); 28 Jan 2015 19:40:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60592 invoked by uid 500); 28 Jan 2015 19:40:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60571 invoked by uid 99); 28 Jan 2015 19:40:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 19:40:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 19:39:56 +0000
Received: by mail-qg0-f54.google.com with SMTP id q108so18343915qgd.13
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 11:39:15 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=nWmhnVJAx7wQTACbGwmPudFMbGQyCyjCSKP99snJh5A=;
        b=kQq+BEp/ZsLJVl+bB+wwqz5AZb8NEqZ/sZkfqh6TfqVEAZIOwjjNn66eY7aLZkwW3n
         7z2QGodDcj1hBGBJFTIfTHmExrJhDmkHvCj07cO1/RyLso1Lt+fDc2TB798wPF0FWh02
         0iIZYWZEczfJUb2UjD68meqkyXYt0CDg9vu66DvznsSbfEof9ZTXPeKhf2RDthlRITJq
         P3CwD+EuxjbR/vKZQ+9cDoM7fnY58xAwMtzep4+POYmXZrKL0OWYoOCaCypJpZ3De5kK
         3aYV6xfqYELMdfxORtjxc+r7EZoquLGKo7MY4xP+7oq1HvdCkFlgINZDcWk321m2qJwB
         qkcA==
X-Gm-Message-State: ALoCoQnbxZWh1koRfEvccqBlkqxxYTaDRpz8ILpT8wuPhijwnaPMuEts0J/ZV8wNtEmmNEfhM9z6
X-Received: by 10.224.126.133 with SMTP id c5mr16706632qas.62.1422473955478;
 Wed, 28 Jan 2015 11:39:15 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 28 Jan 2015 11:38:55 -0800 (PST)
In-Reply-To: <54C937D0.1020000@gmail.com>
References: <CAJOb8bvJ1OiZBKQo6qHmdQ7kY2HcvF_=GPjrOGFsN_gyxH7tvw@mail.gmail.com>
 <54C937D0.1020000@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 Jan 2015 11:38:55 -0800
Message-ID: <CAPh_B=Z7VjBrcEaauZ0kgfGd5kLUME5=uBV2WSa_+mVKnR1+pg@mail.gmail.com>
Subject: Re: Data source API | Support for dynamic schema
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ebf2d1a79f050dbb8690
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ebf2d1a79f050dbb8690
Content-Type: text/plain; charset=UTF-8

It's an interesting idea, but there are major challenges with per row
schema.

1. Performance - query optimizer and execution use assumptions about schema
and data to generate optimized query plans. Having to re-reason about
schema for each row can substantially slow down the engine, but due to
optimization and due to the overhead of schema information associated with
each row.

2. Data model: per-row schema is fundamentally a different data model. The
current relational model has gone through 40 years of research and have
very well defined semantics. I don't think there are well defined semantics
of a per-row schema data model. For example, what is the semantics of an
UDF function that is operating on a data cell that has incompatible schema?
Should we also coerce or convert the data type? If yes, will that lead to
conflicting semantics with some other rules? We need to answer questions
like this in order to have a robust data model.





On Wed, Jan 28, 2015 at 11:26 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

> Hi Aniket,
>
> In general the schema of all rows in a single table must be same. This is
> a basic assumption made by Spark SQL. Schema union does make sense, and
> we're planning to support this for Parquet. But as you've mentioned, it
> doesn't help if types of different versions of a column differ from each
> other. Also, you need to reload the data source table after schema changes
> happen.
>
> Cheng
>
>
> On 1/28/15 2:12 AM, Aniket Bhatnagar wrote:
>
>> I saw the talk on Spark data sources and looking at the interfaces, it
>> seems that the schema needs to be provided upfront. This works for many
>> data sources but I have a situation in which I would need to integrate a
>> system that supports schema evolutions by allowing users to change schema
>> without affecting existing rows. Basically, each row contains a schema
>> hint
>> (id and version) and this allows developers to evolve schema over time and
>> perform migration at will. Since the schema needs to be specified upfront
>> in the data source API, one possible way would be to build a union of all
>> schema versions and handle populating row values appropriately. This works
>> in case columns have been added or deleted in the schema but doesn't work
>> if types have changed. I was wondering if it is possible to change the API
>>   to provide schema for each row instead of expecting data source to
>> provide
>> schema upfront?
>>
>> Thanks,
>> Aniket
>>
>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c2ebf2d1a79f050dbb8690--

From dev-return-11336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 20:52:13 2015
Return-Path: <dev-return-11336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B843E17978
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 20:52:13 +0000 (UTC)
Received: (qmail 18816 invoked by uid 500); 28 Jan 2015 20:52:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18745 invoked by uid 500); 28 Jan 2015 20:52:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18733 invoked by uid 99); 28 Jan 2015 20:52:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 20:52:12 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 20:52:08 +0000
Received: by mail-pa0-f51.google.com with SMTP id fb1so29599101pad.10
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 12:51:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=nHl90TeBYPntkiPHuc9jtjhKQ4z/eSuY03CVjkmT+b4=;
        b=q9ZYkKQZ//dPT19r6vwJVqdCRzz/OT33Tan1/uXJUB4XejgzHk9xmKPWggsfBaLpDG
         5a07hD7LzX0LKiTwDINLZxibEHN/WeGnfUXnH2yXcw1U7xHXkJXVweR/zyLJSlXwuG2H
         14dfSnQO45nwj1R4JSEdVXzQsHv1bRIrZrRcH1IkOcAVVM79wTnpnHoCNmvLN6sOtdA7
         W3/2NxgNdjVqTRlCRvGPGFLhAYhLkS+aBgIS9Fi1iQi9hMoR6JnUybCwntkI6WoGsUNI
         wYsQSt7tJPzL/lSSAZ5nAuT7Qgb0C2be/ooqKGZzk9Uiq65tJ7DpccK4vogzCfS3MGyI
         Z6Wg==
MIME-Version: 1.0
X-Received: by 10.66.254.68 with SMTP id ag4mr8870400pad.39.1422478308070;
 Wed, 28 Jan 2015 12:51:48 -0800 (PST)
Received: by 10.70.37.230 with HTTP; Wed, 28 Jan 2015 12:51:47 -0800 (PST)
In-Reply-To: <CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
	<CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com>
Date: Wed, 28 Jan 2015 12:51:47 -0800
Message-ID: <CAOTBr2nJDtxGn__i2V_ugvKfEDp=EN8=4YUi0aK3gtk5BW4w-w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
From: Krishna Sankar <ksankar42@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b15fe6d40d8f6050dbc8aed
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b15fe6d40d8f6050dbc8aed
Content-Type: text/plain; charset=UTF-8

+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0
-Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.0
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       Fixed : org.apache.spark.SparkException in zip !
2.5. rdd operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lmbda) with itertools
OK

Cheers
<k/>

On Wed, Jan 28, 2015 at 5:17 AM, Sean Owen <sowen@cloudera.com> wrote:

> +1 (nonbinding). I verified that all the hash / signing items I
> mentioned before are resolved.
>
> The source package compiles on Ubuntu / Java 8. I ran tests and the
> passed. Well, actually I see the same failure I've seeing locally on
> OS X and on Ubuntu for a while, but I think nobody else has seen this?
>
> MQTTStreamSuite:
> - mqtt input stream *** FAILED ***
>   org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in
> progress
>   at
> org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientState.java:423)
>
> Doesn't happen on Jenkins. If nobody else is seeing this, I suspect it
> is something perhaps related to my env that I haven't figured out yet,
> so should not be considered a blocker.
>
> On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Please vote on releasing the following candidate as Apache Spark version
> 1.2.1!
> >
> > The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1062/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
> >
> > Changes from rc1:
> > This has no code changes from RC1. Only minor changes to the release
> script.
> >
> > Please vote on releasing this package as Apache Spark 1.2.1!
> >
> > The vote is open until  Saturday, January 31, at 10:04 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.1
> > [ ] -1 Do not release this package because ...
> >
> > For a list of fixes in this release, see http://s.apache.org/Mpn.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b15fe6d40d8f6050dbc8aed--

From dev-return-11337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 23:12:30 2015
Return-Path: <dev-return-11337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E13BB17F9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 23:12:29 +0000 (UTC)
Received: (qmail 79807 invoked by uid 500); 28 Jan 2015 23:12:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79735 invoked by uid 500); 28 Jan 2015 23:12:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79721 invoked by uid 99); 28 Jan 2015 23:12:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 23:12:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jayunit100.apache@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 23:12:03 +0000
Received: by mail-wi0-f178.google.com with SMTP id bs8so8484057wib.5
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 15:12:01 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=v8GgEzBVqCLSUPrAg71MFur2wp21jZGYYfbRgD1vQ3c=;
        b=HtGucK+7venv+9OsoE9Ne8p6JhOVuiWVVKhRQROxeNiFa3OFwjaXUBduHalhphv5uI
         5c3HJSGd8D1x2cLnFHssNdDD3QAJnua0WHfDAplYrSCmHaXQXMpi4OlkoLTpXSW6v3YP
         mxvfxRH1q15Pg2i+fpYcLYbGLLOAy8WgdzVss8di9g8WdmHu5NnLeV8NeV+bAcB391WK
         W9gnK2G19WVVbLJqS1h2usd7Kq4ZEXWJ6y/N/zW6JRyblYUGN1FTmnmL8pXVYHP21903
         eWhW/KxWX51t+/i8V6JU6rnc+MwqyIbyJfb1Y5k6IGevpivnhYnVcz0e75vLxVl0Gy8A
         iYvg==
MIME-Version: 1.0
X-Received: by 10.180.82.137 with SMTP id i9mr12166249wiy.38.1422486721879;
 Wed, 28 Jan 2015 15:12:01 -0800 (PST)
Received: by 10.27.175.144 with HTTP; Wed, 28 Jan 2015 15:12:01 -0800 (PST)
Date: Wed, 28 Jan 2015 18:12:01 -0500
Message-ID: <CACVCA=cy193cXxSrYOQSgbrYyQQMaECCoNe-gHTUct_Uwaqjpg@mail.gmail.com>
Subject: spark akka fork : is the source anywhere?
From: jay vyas <jayunit100.apache@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04182742c1669f050dbe7f3f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04182742c1669f050dbe7f3f
Content-Type: text/plain; charset=UTF-8

Hi spark. Where is akka coming from in spark ?

I see the distribution referenced is a spark artifact... but not in the
apache namespace.

     <akka.group>org.spark-project.akka</akka.group>
     <akka.version>2.3.4-spark</akka.version>

Clearly this is a deliberate thought out change (See SPARK-1812), but its
not clear where 2.3.4 spark is coming from and who is maintaining its
release?

-- 
jay vyas

PS

I've had some conversations with will benton as well about this, and its
clear that some modifications to akka are needed, or else a protobug error
occurs, which amount to serialization incompatibilities, hence if one wants
to build spark from sources, the patched akka is required (or else, manual
patching needs to be done)...

15/01/28 22:58:10 ERROR ActorSystemImpl: Uncaught fatal error from thread
[sparkWorker-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkWorker] java.lang.VerifyError: class
akka.remote.WireFormats$AkkaControlMessage overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;

--f46d04182742c1669f050dbe7f3f--

From dev-return-11338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jan 28 23:35:10 2015
Return-Path: <dev-return-11338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C899100C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 Jan 2015 23:35:10 +0000 (UTC)
Received: (qmail 61218 invoked by uid 500); 28 Jan 2015 23:35:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61143 invoked by uid 500); 28 Jan 2015 23:35:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61112 invoked by uid 99); 28 Jan 2015 23:35:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 23:35:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 Jan 2015 23:34:35 +0000
Received: by mail-qg0-f49.google.com with SMTP id i50so21183026qgf.8
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 15:34:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=+Ome2LcVwYtmExD/EXObHHhR38Bx9yml4Z3BN3z+aC0=;
        b=T3ZH7ItsAqlk/zdKrXHkyy9BD2kG72O/FSSJyYrIxVvr+TFsP9oFFG2ZyYhEofEr33
         /db+7jg0x4gJNQX0j+pZvKWJKiTDlda1+pcWf6/OA8hsnlB6BgiI3MUk8yWyqKcKBPWR
         cvouU1v0PHU1Cd4uJ+/OfutDbnheuk6YjMaEOCKz0Ur/iNrGRLLgcl1n4QIScX+kr1po
         7SUPDiHWluNMqmPAoBCivuEM+66vS8hzKRlnDqPKU6UpcEHE8koeVWgtHGlgkyQ7kdxy
         j52mXlXSQPcLc0Pu4S30UOKc6gVO3Y9q+bWWbLwBt23XrEbTxQapp9qcs9LQdN4hrIne
         MRzg==
X-Gm-Message-State: ALoCoQlL6gZ0cQuAoYGRot17I+C99gdZx6ag51pnR+ear4OcYamvrmwBgQ0UXsZOS6LSpPigeoBy
X-Received: by 10.140.23.199 with SMTP id 65mr17629822qgp.84.1422488053094;
 Wed, 28 Jan 2015 15:34:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 28 Jan 2015 15:33:52 -0800 (PST)
In-Reply-To: <CACVCA=cy193cXxSrYOQSgbrYyQQMaECCoNe-gHTUct_Uwaqjpg@mail.gmail.com>
References: <CACVCA=cy193cXxSrYOQSgbrYyQQMaECCoNe-gHTUct_Uwaqjpg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 Jan 2015 15:33:52 -0800
Message-ID: <CAPh_B=bA8QsJ3CD=XKzGhPodeB_T9EwOn-ZNw8iSCqMBu=8tjg@mail.gmail.com>
Subject: Re: spark akka fork : is the source anywhere?
To: jay vyas <jayunit100.apache@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c12fe81a3352050dbecf4b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12fe81a3352050dbecf4b
Content-Type: text/plain; charset=UTF-8

Hopefully problems like this will go away entirely in the next couple of
releases. https://issues.apache.org/jira/browse/SPARK-5293



On Wed, Jan 28, 2015 at 3:12 PM, jay vyas <jayunit100.apache@gmail.com>
wrote:

> Hi spark. Where is akka coming from in spark ?
>
> I see the distribution referenced is a spark artifact... but not in the
> apache namespace.
>
>      <akka.group>org.spark-project.akka</akka.group>
>      <akka.version>2.3.4-spark</akka.version>
>
> Clearly this is a deliberate thought out change (See SPARK-1812), but its
> not clear where 2.3.4 spark is coming from and who is maintaining its
> release?
>
> --
> jay vyas
>
> PS
>
> I've had some conversations with will benton as well about this, and its
> clear that some modifications to akka are needed, or else a protobug error
> occurs, which amount to serialization incompatibilities, hence if one wants
> to build spark from sources, the patched akka is required (or else, manual
> patching needs to be done)...
>
> 15/01/28 22:58:10 ERROR ActorSystemImpl: Uncaught fatal error from thread
> [sparkWorker-akka.remote.default-remote-dispatcher-6] shutting down
> ActorSystem [sparkWorker] java.lang.VerifyError: class
> akka.remote.WireFormats$AkkaControlMessage overrides final method
> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
>

--001a11c12fe81a3352050dbecf4b--

From dev-return-11339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 00:20:57 2015
Return-Path: <dev-return-11339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2552102DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 00:20:57 +0000 (UTC)
Received: (qmail 65372 invoked by uid 500); 29 Jan 2015 00:20:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65296 invoked by uid 500); 29 Jan 2015 00:20:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65267 invoked by uid 99); 29 Jan 2015 00:20:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:20:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of velvia.github@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:20:32 +0000
Received: by mail-wi0-f182.google.com with SMTP id n3so17391653wiv.3
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 16:20:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=r+ONWNEuoqGUdV4dyA1SWSxkpVohsimeT9Z2EQBX+Oc=;
        b=h/+93NyMLzAvtjs29e3w2VhLAhrZnywD7V80DCXGi3HMrzSTGWdqRt9BULI212Hksw
         9xMQfG1yWWFM2IpnTRbmbqag4hpjoQOv9BoiSc2BgjRDpg1gZokkmsR/AJhRryYEQy+Y
         853dtkljRwuDQ2vAU7Fqc22H7F4/iFx2gtXHEHn58NGVM6o8ibfLwq8RkFR0ZL400a+0
         It+bpham67ZJZuwdkztNx4VyE2WPk2BESdKVP/d9xuRmLlxMWktsU84afbQfh7kpEHSS
         6shEtZt9t/sfpRhppz4w/zV8504/gEvBmPEpPB70iQBalTAC/MGc0YevrY6bP+rkRH6p
         3VEA==
MIME-Version: 1.0
X-Received: by 10.180.160.166 with SMTP id xl6mr12494431wib.16.1422490830885;
 Wed, 28 Jan 2015 16:20:30 -0800 (PST)
Received: by 10.217.38.20 with HTTP; Wed, 28 Jan 2015 16:20:30 -0800 (PST)
In-Reply-To: <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
Date: Wed, 28 Jan 2015 16:20:30 -0800
Message-ID: <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Evan Chan <velvia.github@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey guys,

How does this impact the data sources API?  I was planning on using
this for a project.

+1 that many things from spark-sql / DataFrame is universally
desirable and useful.

By the way, one thing that prevents the columnar compression stuff in
Spark SQL from being more useful is, at least from previous talks with
Reynold and Michael et al., that the format was not designed for
persistence.

I have a new project that aims to change that.  It is a
zero-serialisation, high performance binary vector library, designed
from the outset to be a persistent storage friendly.  May be one day
it can replace the Spark SQL columnar compression.

Michael told me this would be a lot of work, and recreates parts of
Parquet, but I think it's worth it.  LMK if you'd like more details.

-Evan

On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com> wrote:
> Alright I have merged the patch ( https://github.com/apache/spark/pull/4173
> ) since I don't see any strong opinions against it (as a matter of fact
> most were for it). We can still change it if somebody lays out a strong
> argument.
>
> On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
>> The type alias means your methods can specify either type and they will
>> work. It's just another name for the same type. But Scaladocs and such will
>> show DataFrame as the type.
>>
>> Matei
>>
>> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>> dirceu.semighini@gmail.com> wrote:
>> >
>> > Reynold,
>> > But with type alias we will have the same problem, right?
>> > If the methods doesn't receive schemardd anymore, we will have to change
>> > our code to migrade from schema to dataframe. Unless we have an implicit
>> > conversion between DataFrame and SchemaRDD
>> >
>> >
>> >
>> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>> >
>> >> Dirceu,
>> >>
>> >> That is not possible because one cannot overload return types.
>> >>
>> >> SQLContext.parquetFile (and many other methods) needs to return some
>> type,
>> >> and that type cannot be both SchemaRDD and DataFrame.
>> >>
>> >> In 1.3, we will create a type alias for DataFrame called SchemaRDD to
>> not
>> >> break source compatibility for Scala.
>> >>
>> >>
>> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>> >> dirceu.semighini@gmail.com> wrote:
>> >>
>> >>> Can't the SchemaRDD remain the same, but deprecated, and be removed in
>> the
>> >>> release 1.5(+/- 1)  for example, and the new code been added to
>> DataFrame?
>> >>> With this, we don't impact in existing code for the next few releases.
>> >>>
>> >>>
>> >>>
>> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>> >>>
>> >>>> I want to address the issue that Matei raised about the heavy lifting
>> >>>> required for a full SQL support. It is amazing that even after 30
>> years
>> >>> of
>> >>>> research there is not a single good open source columnar database like
>> >>>> Vertica. There is a column store option in MySQL, but it is not nearly
>> >>> as
>> >>>> sophisticated as Vertica or MonetDB. But there's a true need for such
>> a
>> >>>> system. I wonder why so and it's high time to change that.
>> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
>> wrote:
>> >>>>
>> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
>> >>> former
>> >>>>> slightly better because it's more descriptive.
>> >>>>>
>> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers, it
>> >>> would
>> >>>>> be more clear from a user-facing perspective to at least choose a
>> >>> package
>> >>>>> name for it that omits "sql".
>> >>>>>
>> >>>>> I would also be in favor of adding a separate Spark Schema module for
>> >>>> Spark
>> >>>>> SQL to rely on, but I imagine that might be too large a change at
>> this
>> >>>>> point?
>> >>>>>
>> >>>>> -Sandy
>> >>>>>
>> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>> >>> matei.zaharia@gmail.com>
>> >>>>> wrote:
>> >>>>>
>> >>>>>> (Actually when we designed Spark SQL we thought of giving it another
>> >>>>> name,
>> >>>>>> like Spark Schema, but we decided to stick with SQL since that was
>> >>> the
>> >>>>> most
>> >>>>>> obvious use case to many users.)
>> >>>>>>
>> >>>>>> Matei
>> >>>>>>
>> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>> >>> matei.zaharia@gmail.com>
>> >>>>>> wrote:
>> >>>>>>>
>> >>>>>>> While it might be possible to move this concept to Spark Core
>> >>>>> long-term,
>> >>>>>> supporting structured data efficiently does require quite a bit of
>> >>> the
>> >>>>>> infrastructure in Spark SQL, such as query planning and columnar
>> >>>> storage.
>> >>>>>> The intent of Spark SQL though is to be more than a SQL server --
>> >>> it's
>> >>>>>> meant to be a library for manipulating structured data. Since this
>> >>> is
>> >>>>>> possible to build over the core API, it's pretty natural to
>> >>> organize it
>> >>>>>> that way, same as Spark Streaming is a library.
>> >>>>>>>
>> >>>>>>> Matei
>> >>>>>>>
>> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
>> >>>> wrote:
>> >>>>>>>>
>> >>>>>>>> "The context is that SchemaRDD is becoming a common data format
>> >>> used
>> >>>>> for
>> >>>>>>>> bringing data into Spark from external systems, and used for
>> >>> various
>> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>> >>>>>>>>
>> >>>>>>>> i agree. this to me also implies it belongs in spark core, not
>> >>> sql
>> >>>>>>>>
>> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>> >>>>>>>>
>> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the Jan.
>> >>> 13
>> >>>> Bay
>> >>>>>> Area
>> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
>> >>> information
>> >>>> on
>> >>>>>> this
>> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
>> >>>>>>>>>
>> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> >>>>>>>>>
>> >>>>>>>>> ________________________________
>> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>>>>>>> One thing potentially not clear from this e-mail, there will be
>> >>> a
>> >>>> 1:1
>> >>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>> >>> rxin@databricks.com>
>> >>>>>> wrote:
>> >>>>>>>>>> Hi,
>> >>>>>>>>>>
>> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
>> >>>>> wanted
>> >>>>>> to
>> >>>>>>>>>> get the community's opinion.
>> >>>>>>>>>>
>> >>>>>>>>>> The context is that SchemaRDD is becoming a common data format
>> >>>> used
>> >>>>>> for
>> >>>>>>>>>> bringing data into Spark from external systems, and used for
>> >>>> various
>> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
>> >>> expect
>> >>>>>> more
>> >>>>>>>>> and
>> >>>>>>>>>> more users to be programming directly against SchemaRDD API
>> >>> rather
>> >>>>>> than
>> >>>>>>>>> the
>> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
>> >>>>> originally
>> >>>>>>>>>> designed for writing test cases, always has the data-frame like
>> >>>> API.
>> >>>>>> In
>> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable for end
>> >>>>> users.
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> There are two motivations for the renaming:
>> >>>>>>>>>>
>> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>> >>> SchemaRDD.
>> >>>>>>>>>>
>> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>> >>> anymore
>> >>>>>> (even
>> >>>>>>>>>> though it would contain some RDD functions like map, flatMap,
>> >>>> etc),
>> >>>>>> and
>> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>> >>> confusing.
>> >>>>>>>>> Instead.
>> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
>> >>> methods.
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> My understanding is that very few users program directly
>> >>> against
>> >>>> the
>> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
>> >>> documented.
>> >>>>>>>>> However,
>> >>>>>>>>>> oo maintain backward compatibility, we can create a type alias
>> >>>>>> DataFrame
>> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
>> >>>>> compatibility
>> >>>>>>>>> for
>> >>>>>>>>>> Scala. That said, we will have to update all existing
>> >>> materials to
>> >>>>> use
>> >>>>>>>>>> DataFrame rather than SchemaRDD.
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>> ---------------------------------------------------------------------
>> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>> ---------------------------------------------------------------------
>> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>>>>>
>> >>>>>>
>> >>>>>>
>> >>>>>>
>> >>> ---------------------------------------------------------------------
>> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >>>>>>
>> >>>>>>
>> >>>>>
>> >>>>
>> >>>
>> >>
>> >>
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 00:25:37 2015
Return-Path: <dev-return-11340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B025102FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 00:25:37 +0000 (UTC)
Received: (qmail 72552 invoked by uid 500); 29 Jan 2015 00:25:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72477 invoked by uid 500); 29 Jan 2015 00:25:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72465 invoked by uid 99); 29 Jan 2015 00:25:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:25:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:25:11 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so21698037qgd.1
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 16:24:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=J15+vV/7EU8qij0SyHJA0kQQ/qtXnPqQplXpKz9l0GQ=;
        b=ToMBbU6lHXm9TnSfjKwxjk4palYiVj+Q765WIhBmuusFf1EkzLTpZiUzJ7kcI+ShUm
         Fv4KUkrpwqd2onaJrH/r7wsu1MiKovLC+BBBmXgkYicul5dgvBF/7rCNNeXY1mifw0DE
         mlW1OaZ2q9TLAVyx0c7jPXlNCioGYqIIZks3N53Kl0yNT+wumJccjtLVhf2OMb9tkyPT
         NAuwyHxIl5TBqZEeXeGWPKM+/mNkXIWvErdDWvDhy5c45v+NxQ+gC8TsnPvbWi9/ti6j
         CkQ95Gx68YZ9bO/QiiSJc0LxNcChbKBb+qQhVYH5CTGDodkUlWQHHsDxmq/E1rATSYqY
         HQpw==
X-Gm-Message-State: ALoCoQkxRui4pL4H+GiBu9Rzie++z9Cxfc7+GU8gaRo8XKmJfElV+excUvZAoYt2EJQqAJobs0AT
X-Received: by 10.224.63.70 with SMTP id a6mr1985745qai.42.1422491043583; Wed,
 28 Jan 2015 16:24:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 28 Jan 2015 16:23:43 -0800 (PST)
In-Reply-To: <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
 <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
 <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
 <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com> <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
 <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 Jan 2015 16:23:43 -0800
Message-ID: <CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Evan Chan <velvia.github@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdca100596eec050dbf81b6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdca100596eec050dbf81b6
Content-Type: text/plain; charset=UTF-8

It shouldn't change the data source api at all because data sources create
RDD[Row], and that gets converted into a DataFrame automatically
(previously to SchemaRDD).

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala

One thing that will break the data source API in 1.3 is the location of
types. Types were previously defined in sql.catalyst.types, and now moved
to sql.types. After 1.3, sql.catalyst is hidden from users, and all public
APIs have first class classes/objects defined in sql directly.



On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com> wrote:

> Hey guys,
>
> How does this impact the data sources API?  I was planning on using
> this for a project.
>
> +1 that many things from spark-sql / DataFrame is universally
> desirable and useful.
>
> By the way, one thing that prevents the columnar compression stuff in
> Spark SQL from being more useful is, at least from previous talks with
> Reynold and Michael et al., that the format was not designed for
> persistence.
>
> I have a new project that aims to change that.  It is a
> zero-serialisation, high performance binary vector library, designed
> from the outset to be a persistent storage friendly.  May be one day
> it can replace the Spark SQL columnar compression.
>
> Michael told me this would be a lot of work, and recreates parts of
> Parquet, but I think it's worth it.  LMK if you'd like more details.
>
> -Evan
>
> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com> wrote:
> > Alright I have merged the patch (
> https://github.com/apache/spark/pull/4173
> > ) since I don't see any strong opinions against it (as a matter of fact
> > most were for it). We can still change it if somebody lays out a strong
> > argument.
> >
> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia <matei.zaharia@gmail.com
> >
> > wrote:
> >
> >> The type alias means your methods can specify either type and they will
> >> work. It's just another name for the same type. But Scaladocs and such
> will
> >> show DataFrame as the type.
> >>
> >> Matei
> >>
> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> >> dirceu.semighini@gmail.com> wrote:
> >> >
> >> > Reynold,
> >> > But with type alias we will have the same problem, right?
> >> > If the methods doesn't receive schemardd anymore, we will have to
> change
> >> > our code to migrade from schema to dataframe. Unless we have an
> implicit
> >> > conversion between DataFrame and SchemaRDD
> >> >
> >> >
> >> >
> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> >> >
> >> >> Dirceu,
> >> >>
> >> >> That is not possible because one cannot overload return types.
> >> >>
> >> >> SQLContext.parquetFile (and many other methods) needs to return some
> >> type,
> >> >> and that type cannot be both SchemaRDD and DataFrame.
> >> >>
> >> >> In 1.3, we will create a type alias for DataFrame called SchemaRDD to
> >> not
> >> >> break source compatibility for Scala.
> >> >>
> >> >>
> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> >> >> dirceu.semighini@gmail.com> wrote:
> >> >>
> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be removed
> in
> >> the
> >> >>> release 1.5(+/- 1)  for example, and the new code been added to
> >> DataFrame?
> >> >>> With this, we don't impact in existing code for the next few
> releases.
> >> >>>
> >> >>>
> >> >>>
> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
> >> >>>
> >> >>>> I want to address the issue that Matei raised about the heavy
> lifting
> >> >>>> required for a full SQL support. It is amazing that even after 30
> >> years
> >> >>> of
> >> >>>> research there is not a single good open source columnar database
> like
> >> >>>> Vertica. There is a column store option in MySQL, but it is not
> nearly
> >> >>> as
> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need for
> such
> >> a
> >> >>>> system. I wonder why so and it's high time to change that.
> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
> >> wrote:
> >> >>>>
> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
> >> >>> former
> >> >>>>> slightly better because it's more descriptive.
> >> >>>>>
> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers,
> it
> >> >>> would
> >> >>>>> be more clear from a user-facing perspective to at least choose a
> >> >>> package
> >> >>>>> name for it that omits "sql".
> >> >>>>>
> >> >>>>> I would also be in favor of adding a separate Spark Schema module
> for
> >> >>>> Spark
> >> >>>>> SQL to rely on, but I imagine that might be too large a change at
> >> this
> >> >>>>> point?
> >> >>>>>
> >> >>>>> -Sandy
> >> >>>>>
> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> >> >>> matei.zaharia@gmail.com>
> >> >>>>> wrote:
> >> >>>>>
> >> >>>>>> (Actually when we designed Spark SQL we thought of giving it
> another
> >> >>>>> name,
> >> >>>>>> like Spark Schema, but we decided to stick with SQL since that
> was
> >> >>> the
> >> >>>>> most
> >> >>>>>> obvious use case to many users.)
> >> >>>>>>
> >> >>>>>> Matei
> >> >>>>>>
> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> >> >>> matei.zaharia@gmail.com>
> >> >>>>>> wrote:
> >> >>>>>>>
> >> >>>>>>> While it might be possible to move this concept to Spark Core
> >> >>>>> long-term,
> >> >>>>>> supporting structured data efficiently does require quite a bit
> of
> >> >>> the
> >> >>>>>> infrastructure in Spark SQL, such as query planning and columnar
> >> >>>> storage.
> >> >>>>>> The intent of Spark SQL though is to be more than a SQL server --
> >> >>> it's
> >> >>>>>> meant to be a library for manipulating structured data. Since
> this
> >> >>> is
> >> >>>>>> possible to build over the core API, it's pretty natural to
> >> >>> organize it
> >> >>>>>> that way, same as Spark Streaming is a library.
> >> >>>>>>>
> >> >>>>>>> Matei
> >> >>>>>>>
> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
> >> >>>> wrote:
> >> >>>>>>>>
> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data format
> >> >>> used
> >> >>>>> for
> >> >>>>>>>> bringing data into Spark from external systems, and used for
> >> >>> various
> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> >> >>>>>>>>
> >> >>>>>>>> i agree. this to me also implies it belongs in spark core, not
> >> >>> sql
> >> >>>>>>>>
> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> >> >>>>>>>>
> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the Jan.
> >> >>> 13
> >> >>>> Bay
> >> >>>>>> Area
> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> >> >>> information
> >> >>>> on
> >> >>>>>> this
> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> >> >>>>>>>>>
> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >> >>>>>>>>>
> >> >>>>>>>>> ________________________________
> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> >> >>>>>>>>>
> >> >>>>>>>>>
> >> >>>>>>>>> One thing potentially not clear from this e-mail, there will
> be
> >> >>> a
> >> >>>> 1:1
> >> >>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
> >> >>>>>>>>>
> >> >>>>>>>>>
> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> >> >>> rxin@databricks.com>
> >> >>>>>> wrote:
> >> >>>>>>>>>> Hi,
> >> >>>>>>>>>>
> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3,
> and
> >> >>>>> wanted
> >> >>>>>> to
> >> >>>>>>>>>> get the community's opinion.
> >> >>>>>>>>>>
> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
> format
> >> >>>> used
> >> >>>>>> for
> >> >>>>>>>>>> bringing data into Spark from external systems, and used for
> >> >>>> various
> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
> >> >>> expect
> >> >>>>>> more
> >> >>>>>>>>> and
> >> >>>>>>>>>> more users to be programming directly against SchemaRDD API
> >> >>> rather
> >> >>>>>> than
> >> >>>>>>>>> the
> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
> >> >>>>> originally
> >> >>>>>>>>>> designed for writing test cases, always has the data-frame
> like
> >> >>>> API.
> >> >>>>>> In
> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable for
> end
> >> >>>>> users.
> >> >>>>>>>>>>
> >> >>>>>>>>>>
> >> >>>>>>>>>> There are two motivations for the renaming:
> >> >>>>>>>>>>
> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
> >> >>> SchemaRDD.
> >> >>>>>>>>>>
> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
> >> >>> anymore
> >> >>>>>> (even
> >> >>>>>>>>>> though it would contain some RDD functions like map, flatMap,
> >> >>>> etc),
> >> >>>>>> and
> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
> >> >>> confusing.
> >> >>>>>>>>> Instead.
> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
> >> >>> methods.
> >> >>>>>>>>>>
> >> >>>>>>>>>>
> >> >>>>>>>>>> My understanding is that very few users program directly
> >> >>> against
> >> >>>> the
> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
> >> >>> documented.
> >> >>>>>>>>> However,
> >> >>>>>>>>>> oo maintain backward compatibility, we can create a type
> alias
> >> >>>>>> DataFrame
> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
> >> >>>>> compatibility
> >> >>>>>>>>> for
> >> >>>>>>>>>> Scala. That said, we will have to update all existing
> >> >>> materials to
> >> >>>>> use
> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
> >> >>>>>>>>>
> >> >>>>>>>>>
> >> >>>>
> ---------------------------------------------------------------------
> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >>>>>>>>>
> >> >>>>>>>>>
> >> >>>>
> ---------------------------------------------------------------------
> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >>>>>>>>>
> >> >>>>>>>>>
> >> >>>>>>>
> >> >>>>>>
> >> >>>>>>
> >> >>>>>>
> >> >>>
> ---------------------------------------------------------------------
> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >>>>>>
> >> >>>>>>
> >> >>>>>
> >> >>>>
> >> >>>
> >> >>
> >> >>
> >>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>

--047d7bdca100596eec050dbf81b6--

From dev-return-11341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 00:42:19 2015
Return-Path: <dev-return-11341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C12C0103E5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 00:42:19 +0000 (UTC)
Received: (qmail 5907 invoked by uid 500); 29 Jan 2015 00:42:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5821 invoked by uid 500); 29 Jan 2015 00:42:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5808 invoked by uid 99); 29 Jan 2015 00:42:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:42:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of velvia.github@gmail.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:42:14 +0000
Received: by mail-wi0-f174.google.com with SMTP id n3so17818575wiv.1
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 16:41:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=oVvOnw7GJJeSswgHoMhFyDTB4pRrhS1yHpsuWlv1row=;
        b=INnPXSH/Az7Zi5AjFHJfdoGTOzwXIvvukwOLxUBjPrGL590K20EB2ZYgNDxARDZ+Ef
         gGiU50uxy8yM+OFDlno1jclMNAZ/B9Ra5qvMt+Eu7j/cOITCciPBBuTozP0QsW9C3gi4
         9VPqshkOUpz3MnGxGHcqjpvQQFZGGMnNDQo/OiPUY3IRsF837NEToltu9fUFISMIlumd
         ym9JVToH53uYg0ctw0USAkXNHgMxOleEwx6cvj3fOAP9Bb5ho3Eerb3IVRqAVVY1wIdM
         zppGGYCEXzDwf+mLSAUb0KN9WBkDEs5f+cSS+yo3US7fih2rr0NcTu+bJt5GYAZjSJAv
         OESw==
MIME-Version: 1.0
X-Received: by 10.194.24.103 with SMTP id t7mr12943541wjf.15.1422492067936;
 Wed, 28 Jan 2015 16:41:07 -0800 (PST)
Received: by 10.217.38.20 with HTTP; Wed, 28 Jan 2015 16:41:07 -0800 (PST)
In-Reply-To: <CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
Date: Wed, 28 Jan 2015 16:41:07 -0800
Message-ID: <CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Evan Chan <velvia.github@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I believe that most DataFrame implementations out there, like Pandas,
supports the idea of missing values / NA, and some support the idea of
Not Meaningful as well.

Does Row support anything like that?  That is important for certain
applications.  I thought that Row worked by being a mutable object,
but haven't looked into the details in a while.

-Evan

On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com> wrote:
> It shouldn't change the data source api at all because data sources create
> RDD[Row], and that gets converted into a DataFrame automatically (previously
> to SchemaRDD).
>
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
>
> One thing that will break the data source API in 1.3 is the location of
> types. Types were previously defined in sql.catalyst.types, and now moved to
> sql.types. After 1.3, sql.catalyst is hidden from users, and all public APIs
> have first class classes/objects defined in sql directly.
>
>
>
> On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com> wrote:
>>
>> Hey guys,
>>
>> How does this impact the data sources API?  I was planning on using
>> this for a project.
>>
>> +1 that many things from spark-sql / DataFrame is universally
>> desirable and useful.
>>
>> By the way, one thing that prevents the columnar compression stuff in
>> Spark SQL from being more useful is, at least from previous talks with
>> Reynold and Michael et al., that the format was not designed for
>> persistence.
>>
>> I have a new project that aims to change that.  It is a
>> zero-serialisation, high performance binary vector library, designed
>> from the outset to be a persistent storage friendly.  May be one day
>> it can replace the Spark SQL columnar compression.
>>
>> Michael told me this would be a lot of work, and recreates parts of
>> Parquet, but I think it's worth it.  LMK if you'd like more details.
>>
>> -Evan
>>
>> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com> wrote:
>> > Alright I have merged the patch (
>> > https://github.com/apache/spark/pull/4173
>> > ) since I don't see any strong opinions against it (as a matter of fact
>> > most were for it). We can still change it if somebody lays out a strong
>> > argument.
>> >
>> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>> > <matei.zaharia@gmail.com>
>> > wrote:
>> >
>> >> The type alias means your methods can specify either type and they will
>> >> work. It's just another name for the same type. But Scaladocs and such
>> >> will
>> >> show DataFrame as the type.
>> >>
>> >> Matei
>> >>
>> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>> >> dirceu.semighini@gmail.com> wrote:
>> >> >
>> >> > Reynold,
>> >> > But with type alias we will have the same problem, right?
>> >> > If the methods doesn't receive schemardd anymore, we will have to
>> >> > change
>> >> > our code to migrade from schema to dataframe. Unless we have an
>> >> > implicit
>> >> > conversion between DataFrame and SchemaRDD
>> >> >
>> >> >
>> >> >
>> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>> >> >
>> >> >> Dirceu,
>> >> >>
>> >> >> That is not possible because one cannot overload return types.
>> >> >>
>> >> >> SQLContext.parquetFile (and many other methods) needs to return some
>> >> type,
>> >> >> and that type cannot be both SchemaRDD and DataFrame.
>> >> >>
>> >> >> In 1.3, we will create a type alias for DataFrame called SchemaRDD
>> >> >> to
>> >> not
>> >> >> break source compatibility for Scala.
>> >> >>
>> >> >>
>> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>> >> >> dirceu.semighini@gmail.com> wrote:
>> >> >>
>> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be removed
>> >> >>> in
>> >> the
>> >> >>> release 1.5(+/- 1)  for example, and the new code been added to
>> >> DataFrame?
>> >> >>> With this, we don't impact in existing code for the next few
>> >> >>> releases.
>> >> >>>
>> >> >>>
>> >> >>>
>> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>> >> >>>
>> >> >>>> I want to address the issue that Matei raised about the heavy
>> >> >>>> lifting
>> >> >>>> required for a full SQL support. It is amazing that even after 30
>> >> years
>> >> >>> of
>> >> >>>> research there is not a single good open source columnar database
>> >> >>>> like
>> >> >>>> Vertica. There is a column store option in MySQL, but it is not
>> >> >>>> nearly
>> >> >>> as
>> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need for
>> >> >>>> such
>> >> a
>> >> >>>> system. I wonder why so and it's high time to change that.
>> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
>> >> wrote:
>> >> >>>>
>> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
>> >> >>> former
>> >> >>>>> slightly better because it's more descriptive.
>> >> >>>>>
>> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers,
>> >> >>>>> it
>> >> >>> would
>> >> >>>>> be more clear from a user-facing perspective to at least choose a
>> >> >>> package
>> >> >>>>> name for it that omits "sql".
>> >> >>>>>
>> >> >>>>> I would also be in favor of adding a separate Spark Schema module
>> >> >>>>> for
>> >> >>>> Spark
>> >> >>>>> SQL to rely on, but I imagine that might be too large a change at
>> >> this
>> >> >>>>> point?
>> >> >>>>>
>> >> >>>>> -Sandy
>> >> >>>>>
>> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>> >> >>> matei.zaharia@gmail.com>
>> >> >>>>> wrote:
>> >> >>>>>
>> >> >>>>>> (Actually when we designed Spark SQL we thought of giving it
>> >> >>>>>> another
>> >> >>>>> name,
>> >> >>>>>> like Spark Schema, but we decided to stick with SQL since that
>> >> >>>>>> was
>> >> >>> the
>> >> >>>>> most
>> >> >>>>>> obvious use case to many users.)
>> >> >>>>>>
>> >> >>>>>> Matei
>> >> >>>>>>
>> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>> >> >>> matei.zaharia@gmail.com>
>> >> >>>>>> wrote:
>> >> >>>>>>>
>> >> >>>>>>> While it might be possible to move this concept to Spark Core
>> >> >>>>> long-term,
>> >> >>>>>> supporting structured data efficiently does require quite a bit
>> >> >>>>>> of
>> >> >>> the
>> >> >>>>>> infrastructure in Spark SQL, such as query planning and columnar
>> >> >>>> storage.
>> >> >>>>>> The intent of Spark SQL though is to be more than a SQL server
>> >> >>>>>> --
>> >> >>> it's
>> >> >>>>>> meant to be a library for manipulating structured data. Since
>> >> >>>>>> this
>> >> >>> is
>> >> >>>>>> possible to build over the core API, it's pretty natural to
>> >> >>> organize it
>> >> >>>>>> that way, same as Spark Streaming is a library.
>> >> >>>>>>>
>> >> >>>>>>> Matei
>> >> >>>>>>>
>> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <koert@tresata.com>
>> >> >>>> wrote:
>> >> >>>>>>>>
>> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
>> >> >>>>>>>> format
>> >> >>> used
>> >> >>>>> for
>> >> >>>>>>>> bringing data into Spark from external systems, and used for
>> >> >>> various
>> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>> >> >>>>>>>>
>> >> >>>>>>>> i agree. this to me also implies it belongs in spark core, not
>> >> >>> sql
>> >> >>>>>>>>
>> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>> >> >>>>>>>>
>> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the
>> >> >>>>>>>>> Jan.
>> >> >>> 13
>> >> >>>> Bay
>> >> >>>>>> Area
>> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
>> >> >>> information
>> >> >>>> on
>> >> >>>>>> this
>> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
>> >> >>>>>>>>>
>> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> >> >>>>>>>>>
>> >> >>>>>>>>> ________________________________
>> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>> >> >>>>>>>>>
>> >> >>>>>>>>>
>> >> >>>>>>>>> One thing potentially not clear from this e-mail, there will
>> >> >>>>>>>>> be
>> >> >>> a
>> >> >>>> 1:1
>> >> >>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
>> >> >>>>>>>>>
>> >> >>>>>>>>>
>> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>> >> >>> rxin@databricks.com>
>> >> >>>>>> wrote:
>> >> >>>>>>>>>> Hi,
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3,
>> >> >>>>>>>>>> and
>> >> >>>>> wanted
>> >> >>>>>> to
>> >> >>>>>>>>>> get the community's opinion.
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
>> >> >>>>>>>>>> format
>> >> >>>> used
>> >> >>>>>> for
>> >> >>>>>>>>>> bringing data into Spark from external systems, and used for
>> >> >>>> various
>> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
>> >> >>> expect
>> >> >>>>>> more
>> >> >>>>>>>>> and
>> >> >>>>>>>>>> more users to be programming directly against SchemaRDD API
>> >> >>> rather
>> >> >>>>>> than
>> >> >>>>>>>>> the
>> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
>> >> >>>>> originally
>> >> >>>>>>>>>> designed for writing test cases, always has the data-frame
>> >> >>>>>>>>>> like
>> >> >>>> API.
>> >> >>>>>> In
>> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable for
>> >> >>>>>>>>>> end
>> >> >>>>> users.
>> >> >>>>>>>>>>
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> There are two motivations for the renaming:
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>> >> >>> SchemaRDD.
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>> >> >>> anymore
>> >> >>>>>> (even
>> >> >>>>>>>>>> though it would contain some RDD functions like map,
>> >> >>>>>>>>>> flatMap,
>> >> >>>> etc),
>> >> >>>>>> and
>> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>> >> >>> confusing.
>> >> >>>>>>>>> Instead.
>> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
>> >> >>> methods.
>> >> >>>>>>>>>>
>> >> >>>>>>>>>>
>> >> >>>>>>>>>> My understanding is that very few users program directly
>> >> >>> against
>> >> >>>> the
>> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
>> >> >>> documented.
>> >> >>>>>>>>> However,
>> >> >>>>>>>>>> oo maintain backward compatibility, we can create a type
>> >> >>>>>>>>>> alias
>> >> >>>>>> DataFrame
>> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
>> >> >>>>> compatibility
>> >> >>>>>>>>> for
>> >> >>>>>>>>>> Scala. That said, we will have to update all existing
>> >> >>> materials to
>> >> >>>>> use
>> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
>> >> >>>>>>>>>
>> >> >>>>>>>>>
>> >> >>>>
>> >> >>>> ---------------------------------------------------------------------
>> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >>>>>>>>>
>> >> >>>>>>>>>
>> >> >>>>
>> >> >>>> ---------------------------------------------------------------------
>> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >>>>>>>>>
>> >> >>>>>>>>>
>> >> >>>>>>>
>> >> >>>>>>
>> >> >>>>>>
>> >> >>>>>>
>> >> >>>
>> >> >>> ---------------------------------------------------------------------
>> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >>>>>>
>> >> >>>>>>
>> >> >>>>>
>> >> >>>>
>> >> >>>
>> >> >>
>> >> >>
>> >>
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 00:44:06 2015
Return-Path: <dev-return-11342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87D8110401
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 00:44:06 +0000 (UTC)
Received: (qmail 10505 invoked by uid 500); 29 Jan 2015 00:44:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10431 invoked by uid 500); 29 Jan 2015 00:44:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10420 invoked by uid 99); 29 Jan 2015 00:44:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:44:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 00:43:39 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so21862530qgd.1
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 16:43:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=xEM3Z+UMJnkty8AeQraKSAfj0fmQl+Us7U9Jg9L+j5Y=;
        b=ZgEipGfQgSEw0DcgoELdUi9B4aqfg0MiTSZ71MerADdW0LfPluyNdskeKj3lcx3yqM
         AxSPxQdhV7Er7uNBKi8FmCZkEyw/SQ4f4XwBaFE3mbCEgVBXk+bci7VS4z9CHNG8BYzI
         LAJqDMjY9vKyPkCVq7SHb3I82kG8mRyp8rCEpqH5ECvt/gkLrH/45w7yNZ6b9RJHvp25
         6CooO0WkuTYoRdEqveLokz1eBzuwLPdUTHZsETLB7Et6g50OLcXMVvePIJedcIkyNYkv
         bc3MWVJkU/WAihVtMqQEw3FomzBcLVZBC03R8HHnbagi9mYZZSMzSS9GvmzedTvs77cY
         Pltw==
X-Gm-Message-State: ALoCoQloa9LCEEvWbLbRG63nBA7ZSsVJuIKIFtcWKki9iotH1KJPpe4Hre1+DNS9GODOPjAPG1sM
X-Received: by 10.229.172.196 with SMTP id m4mr2424675qcz.19.1422492197573;
 Wed, 28 Jan 2015 16:43:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 28 Jan 2015 16:42:56 -0800 (PST)
In-Reply-To: <CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
 <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
 <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
 <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com> <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
 <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
 <CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com> <CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 Jan 2015 16:42:56 -0800
Message-ID: <CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Evan Chan <velvia.github@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01634ba221eec4050dbfc612
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634ba221eec4050dbfc612
Content-Type: text/plain; charset=UTF-8

Isn't that just "null" in SQL?

On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com> wrote:

> I believe that most DataFrame implementations out there, like Pandas,
> supports the idea of missing values / NA, and some support the idea of
> Not Meaningful as well.
>
> Does Row support anything like that?  That is important for certain
> applications.  I thought that Row worked by being a mutable object,
> but haven't looked into the details in a while.
>
> -Evan
>
> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com> wrote:
> > It shouldn't change the data source api at all because data sources
> create
> > RDD[Row], and that gets converted into a DataFrame automatically
> (previously
> > to SchemaRDD).
> >
> >
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
> >
> > One thing that will break the data source API in 1.3 is the location of
> > types. Types were previously defined in sql.catalyst.types, and now
> moved to
> > sql.types. After 1.3, sql.catalyst is hidden from users, and all public
> APIs
> > have first class classes/objects defined in sql directly.
> >
> >
> >
> > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com>
> wrote:
> >>
> >> Hey guys,
> >>
> >> How does this impact the data sources API?  I was planning on using
> >> this for a project.
> >>
> >> +1 that many things from spark-sql / DataFrame is universally
> >> desirable and useful.
> >>
> >> By the way, one thing that prevents the columnar compression stuff in
> >> Spark SQL from being more useful is, at least from previous talks with
> >> Reynold and Michael et al., that the format was not designed for
> >> persistence.
> >>
> >> I have a new project that aims to change that.  It is a
> >> zero-serialisation, high performance binary vector library, designed
> >> from the outset to be a persistent storage friendly.  May be one day
> >> it can replace the Spark SQL columnar compression.
> >>
> >> Michael told me this would be a lot of work, and recreates parts of
> >> Parquet, but I think it's worth it.  LMK if you'd like more details.
> >>
> >> -Evan
> >>
> >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >> > Alright I have merged the patch (
> >> > https://github.com/apache/spark/pull/4173
> >> > ) since I don't see any strong opinions against it (as a matter of
> fact
> >> > most were for it). We can still change it if somebody lays out a
> strong
> >> > argument.
> >> >
> >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
> >> > <matei.zaharia@gmail.com>
> >> > wrote:
> >> >
> >> >> The type alias means your methods can specify either type and they
> will
> >> >> work. It's just another name for the same type. But Scaladocs and
> such
> >> >> will
> >> >> show DataFrame as the type.
> >> >>
> >> >> Matei
> >> >>
> >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> >> >> dirceu.semighini@gmail.com> wrote:
> >> >> >
> >> >> > Reynold,
> >> >> > But with type alias we will have the same problem, right?
> >> >> > If the methods doesn't receive schemardd anymore, we will have to
> >> >> > change
> >> >> > our code to migrade from schema to dataframe. Unless we have an
> >> >> > implicit
> >> >> > conversion between DataFrame and SchemaRDD
> >> >> >
> >> >> >
> >> >> >
> >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> >> >> >
> >> >> >> Dirceu,
> >> >> >>
> >> >> >> That is not possible because one cannot overload return types.
> >> >> >>
> >> >> >> SQLContext.parquetFile (and many other methods) needs to return
> some
> >> >> type,
> >> >> >> and that type cannot be both SchemaRDD and DataFrame.
> >> >> >>
> >> >> >> In 1.3, we will create a type alias for DataFrame called SchemaRDD
> >> >> >> to
> >> >> not
> >> >> >> break source compatibility for Scala.
> >> >> >>
> >> >> >>
> >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> >> >> >> dirceu.semighini@gmail.com> wrote:
> >> >> >>
> >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
> removed
> >> >> >>> in
> >> >> the
> >> >> >>> release 1.5(+/- 1)  for example, and the new code been added to
> >> >> DataFrame?
> >> >> >>> With this, we don't impact in existing code for the next few
> >> >> >>> releases.
> >> >> >>>
> >> >> >>>
> >> >> >>>
> >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
> >> >> >>>
> >> >> >>>> I want to address the issue that Matei raised about the heavy
> >> >> >>>> lifting
> >> >> >>>> required for a full SQL support. It is amazing that even after
> 30
> >> >> years
> >> >> >>> of
> >> >> >>>> research there is not a single good open source columnar
> database
> >> >> >>>> like
> >> >> >>>> Vertica. There is a column store option in MySQL, but it is not
> >> >> >>>> nearly
> >> >> >>> as
> >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need for
> >> >> >>>> such
> >> >> a
> >> >> >>>> system. I wonder why so and it's high time to change that.
> >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
> >> >> wrote:
> >> >> >>>>
> >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like
> the
> >> >> >>> former
> >> >> >>>>> slightly better because it's more descriptive.
> >> >> >>>>>
> >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
> covers,
> >> >> >>>>> it
> >> >> >>> would
> >> >> >>>>> be more clear from a user-facing perspective to at least
> choose a
> >> >> >>> package
> >> >> >>>>> name for it that omits "sql".
> >> >> >>>>>
> >> >> >>>>> I would also be in favor of adding a separate Spark Schema
> module
> >> >> >>>>> for
> >> >> >>>> Spark
> >> >> >>>>> SQL to rely on, but I imagine that might be too large a change
> at
> >> >> this
> >> >> >>>>> point?
> >> >> >>>>>
> >> >> >>>>> -Sandy
> >> >> >>>>>
> >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> >> >> >>> matei.zaharia@gmail.com>
> >> >> >>>>> wrote:
> >> >> >>>>>
> >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving it
> >> >> >>>>>> another
> >> >> >>>>> name,
> >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since that
> >> >> >>>>>> was
> >> >> >>> the
> >> >> >>>>> most
> >> >> >>>>>> obvious use case to many users.)
> >> >> >>>>>>
> >> >> >>>>>> Matei
> >> >> >>>>>>
> >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> >> >> >>> matei.zaharia@gmail.com>
> >> >> >>>>>> wrote:
> >> >> >>>>>>>
> >> >> >>>>>>> While it might be possible to move this concept to Spark Core
> >> >> >>>>> long-term,
> >> >> >>>>>> supporting structured data efficiently does require quite a
> bit
> >> >> >>>>>> of
> >> >> >>> the
> >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
> columnar
> >> >> >>>> storage.
> >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL server
> >> >> >>>>>> --
> >> >> >>> it's
> >> >> >>>>>> meant to be a library for manipulating structured data. Since
> >> >> >>>>>> this
> >> >> >>> is
> >> >> >>>>>> possible to build over the core API, it's pretty natural to
> >> >> >>> organize it
> >> >> >>>>>> that way, same as Spark Streaming is a library.
> >> >> >>>>>>>
> >> >> >>>>>>> Matei
> >> >> >>>>>>>
> >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
> koert@tresata.com>
> >> >> >>>> wrote:
> >> >> >>>>>>>>
> >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
> >> >> >>>>>>>> format
> >> >> >>> used
> >> >> >>>>> for
> >> >> >>>>>>>> bringing data into Spark from external systems, and used for
> >> >> >>> various
> >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> >> >> >>>>>>>>
> >> >> >>>>>>>> i agree. this to me also implies it belongs in spark core,
> not
> >> >> >>> sql
> >> >> >>>>>>>>
> >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> >> >> >>>>>>>>
> >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the
> >> >> >>>>>>>>> Jan.
> >> >> >>> 13
> >> >> >>>> Bay
> >> >> >>>>>> Area
> >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> >> >> >>> information
> >> >> >>>> on
> >> >> >>>>>> this
> >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> >> >> >>>>>>>>>
> >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >> >> >>>>>>>>>
> >> >> >>>>>>>>> ________________________________
> >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> >> >> >>>>>>>>>
> >> >> >>>>>>>>>
> >> >> >>>>>>>>> One thing potentially not clear from this e-mail, there
> will
> >> >> >>>>>>>>> be
> >> >> >>> a
> >> >> >>>> 1:1
> >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
> DataFrame.
> >> >> >>>>>>>>>
> >> >> >>>>>>>>>
> >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> >> >> >>> rxin@databricks.com>
> >> >> >>>>>> wrote:
> >> >> >>>>>>>>>> Hi,
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3,
> >> >> >>>>>>>>>> and
> >> >> >>>>> wanted
> >> >> >>>>>> to
> >> >> >>>>>>>>>> get the community's opinion.
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
> >> >> >>>>>>>>>> format
> >> >> >>>> used
> >> >> >>>>>> for
> >> >> >>>>>>>>>> bringing data into Spark from external systems, and used
> for
> >> >> >>>> various
> >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We
> also
> >> >> >>> expect
> >> >> >>>>>> more
> >> >> >>>>>>>>> and
> >> >> >>>>>>>>>> more users to be programming directly against SchemaRDD
> API
> >> >> >>> rather
> >> >> >>>>>> than
> >> >> >>>>>>>>> the
> >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used
> DSL
> >> >> >>>>> originally
> >> >> >>>>>>>>>> designed for writing test cases, always has the data-frame
> >> >> >>>>>>>>>> like
> >> >> >>>> API.
> >> >> >>>>>> In
> >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable for
> >> >> >>>>>>>>>> end
> >> >> >>>>> users.
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> There are two motivations for the renaming:
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
> >> >> >>> SchemaRDD.
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
> >> >> >>> anymore
> >> >> >>>>>> (even
> >> >> >>>>>>>>>> though it would contain some RDD functions like map,
> >> >> >>>>>>>>>> flatMap,
> >> >> >>>> etc),
> >> >> >>>>>> and
> >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
> >> >> >>> confusing.
> >> >> >>>>>>>>> Instead.
> >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
> >> >> >>> methods.
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>>
> >> >> >>>>>>>>>> My understanding is that very few users program directly
> >> >> >>> against
> >> >> >>>> the
> >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
> >> >> >>> documented.
> >> >> >>>>>>>>> However,
> >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a type
> >> >> >>>>>>>>>> alias
> >> >> >>>>>> DataFrame
> >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
> >> >> >>>>> compatibility
> >> >> >>>>>>>>> for
> >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
> >> >> >>> materials to
> >> >> >>>>> use
> >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
> >> >> >>>>>>>>>
> >> >> >>>>>>>>>
> >> >> >>>>
> >> >> >>>>
> ---------------------------------------------------------------------
> >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> >>>>>>>>>
> >> >> >>>>>>>>>
> >> >> >>>>
> >> >> >>>>
> ---------------------------------------------------------------------
> >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> >>>>>>>>>
> >> >> >>>>>>>>>
> >> >> >>>>>>>
> >> >> >>>>>>
> >> >> >>>>>>
> >> >> >>>>>>
> >> >> >>>
> >> >> >>>
> ---------------------------------------------------------------------
> >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> >>>>>>
> >> >> >>>>>>
> >> >> >>>>>
> >> >> >>>>
> >> >> >>>
> >> >> >>
> >> >> >>
> >> >>
> >> >>
> >> >> ---------------------------------------------------------------------
> >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> For additional commands, e-mail: dev-help@spark.apache.org
> >> >>
> >> >>
> >
> >
>

--089e01634ba221eec4050dbfc612--

From dev-return-11343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 01:50:14 2015
Return-Path: <dev-return-11343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00EAA1065F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 01:50:14 +0000 (UTC)
Received: (qmail 38437 invoked by uid 500); 29 Jan 2015 01:50:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38362 invoked by uid 500); 29 Jan 2015 01:50:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38346 invoked by uid 99); 29 Jan 2015 01:50:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 01:50:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of velvia.github@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 01:49:48 +0000
Received: by mail-wi0-f169.google.com with SMTP id h11so2653124wiw.0
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 17:49:01 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=R8ZBl3RXGWhr2UK5QnOaygmMOouV61wVXPSUwHYFhIc=;
        b=OBUH5q29ThoEhjivlWsu/OYXH+tKmt4KnCZAe1m6GOrbps6FzZi+AlQDXpU0itFAlb
         rQ0jZwc2CD4yk8w4aZFiTXPd3Up4cFjEoXyayeJku54W3SjKqZJQaTg/J4PejAQmxaHB
         AMIlCoJdXPoMyUfG7/CW4Kh1i0wgKei89YenYGcek75NBojbFoJEmitsvwhkjCNeNYak
         TzRGiEJkP7iij5LxnYstaUUvYjnD4ejJOgiqRUhqjX/WqOtah5OmGLGioOZKy5OgvG07
         H0rlJN122riMCdG7SAxfbCh2DxVy1ESq+H1W4BSJwfNtSrWAJvV5vV2HqF6U+g9gtYgZ
         Qy9g==
MIME-Version: 1.0
X-Received: by 10.194.203.234 with SMTP id kt10mr13711643wjc.88.1422496141863;
 Wed, 28 Jan 2015 17:49:01 -0800 (PST)
Received: by 10.217.38.20 with HTTP; Wed, 28 Jan 2015 17:49:01 -0800 (PST)
In-Reply-To: <CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
	<CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
	<CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
Date: Wed, 28 Jan 2015 17:49:01 -0800
Message-ID: <CAN6Vra1K1qiuMNuoDBUS5t9LqmFULWQLvqOimLjAfdK_AKypFg@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Evan Chan <velvia.github@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, it's "null".   I was worried you couldn't represent it in Row
because of primitive types like Int (unless you box the Int, which
would be a performance hit).  Anyways, I'll take another look at the
Row API again  :-p

On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com> wrote:
> Isn't that just "null" in SQL?
>
> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com> wrote:
>>
>> I believe that most DataFrame implementations out there, like Pandas,
>> supports the idea of missing values / NA, and some support the idea of
>> Not Meaningful as well.
>>
>> Does Row support anything like that?  That is important for certain
>> applications.  I thought that Row worked by being a mutable object,
>> but haven't looked into the details in a while.
>>
>> -Evan
>>
>> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com> wrote:
>> > It shouldn't change the data source api at all because data sources
>> > create
>> > RDD[Row], and that gets converted into a DataFrame automatically
>> > (previously
>> > to SchemaRDD).
>> >
>> >
>> > https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
>> >
>> > One thing that will break the data source API in 1.3 is the location of
>> > types. Types were previously defined in sql.catalyst.types, and now
>> > moved to
>> > sql.types. After 1.3, sql.catalyst is hidden from users, and all public
>> > APIs
>> > have first class classes/objects defined in sql directly.
>> >
>> >
>> >
>> > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com>
>> > wrote:
>> >>
>> >> Hey guys,
>> >>
>> >> How does this impact the data sources API?  I was planning on using
>> >> this for a project.
>> >>
>> >> +1 that many things from spark-sql / DataFrame is universally
>> >> desirable and useful.
>> >>
>> >> By the way, one thing that prevents the columnar compression stuff in
>> >> Spark SQL from being more useful is, at least from previous talks with
>> >> Reynold and Michael et al., that the format was not designed for
>> >> persistence.
>> >>
>> >> I have a new project that aims to change that.  It is a
>> >> zero-serialisation, high performance binary vector library, designed
>> >> from the outset to be a persistent storage friendly.  May be one day
>> >> it can replace the Spark SQL columnar compression.
>> >>
>> >> Michael told me this would be a lot of work, and recreates parts of
>> >> Parquet, but I think it's worth it.  LMK if you'd like more details.
>> >>
>> >> -Evan
>> >>
>> >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
>> >> wrote:
>> >> > Alright I have merged the patch (
>> >> > https://github.com/apache/spark/pull/4173
>> >> > ) since I don't see any strong opinions against it (as a matter of
>> >> > fact
>> >> > most were for it). We can still change it if somebody lays out a
>> >> > strong
>> >> > argument.
>> >> >
>> >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>> >> > <matei.zaharia@gmail.com>
>> >> > wrote:
>> >> >
>> >> >> The type alias means your methods can specify either type and they
>> >> >> will
>> >> >> work. It's just another name for the same type. But Scaladocs and
>> >> >> such
>> >> >> will
>> >> >> show DataFrame as the type.
>> >> >>
>> >> >> Matei
>> >> >>
>> >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>> >> >> dirceu.semighini@gmail.com> wrote:
>> >> >> >
>> >> >> > Reynold,
>> >> >> > But with type alias we will have the same problem, right?
>> >> >> > If the methods doesn't receive schemardd anymore, we will have to
>> >> >> > change
>> >> >> > our code to migrade from schema to dataframe. Unless we have an
>> >> >> > implicit
>> >> >> > conversion between DataFrame and SchemaRDD
>> >> >> >
>> >> >> >
>> >> >> >
>> >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>> >> >> >
>> >> >> >> Dirceu,
>> >> >> >>
>> >> >> >> That is not possible because one cannot overload return types.
>> >> >> >>
>> >> >> >> SQLContext.parquetFile (and many other methods) needs to return
>> >> >> >> some
>> >> >> type,
>> >> >> >> and that type cannot be both SchemaRDD and DataFrame.
>> >> >> >>
>> >> >> >> In 1.3, we will create a type alias for DataFrame called
>> >> >> >> SchemaRDD
>> >> >> >> to
>> >> >> not
>> >> >> >> break source compatibility for Scala.
>> >> >> >>
>> >> >> >>
>> >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>> >> >> >> dirceu.semighini@gmail.com> wrote:
>> >> >> >>
>> >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
>> >> >> >>> removed
>> >> >> >>> in
>> >> >> the
>> >> >> >>> release 1.5(+/- 1)  for example, and the new code been added to
>> >> >> DataFrame?
>> >> >> >>> With this, we don't impact in existing code for the next few
>> >> >> >>> releases.
>> >> >> >>>
>> >> >> >>>
>> >> >> >>>
>> >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:
>> >> >> >>>
>> >> >> >>>> I want to address the issue that Matei raised about the heavy
>> >> >> >>>> lifting
>> >> >> >>>> required for a full SQL support. It is amazing that even after
>> >> >> >>>> 30
>> >> >> years
>> >> >> >>> of
>> >> >> >>>> research there is not a single good open source columnar
>> >> >> >>>> database
>> >> >> >>>> like
>> >> >> >>>> Vertica. There is a column store option in MySQL, but it is not
>> >> >> >>>> nearly
>> >> >> >>> as
>> >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need
>> >> >> >>>> for
>> >> >> >>>> such
>> >> >> a
>> >> >> >>>> system. I wonder why so and it's high time to change that.
>> >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <sandy.ryza@cloudera.com>
>> >> >> wrote:
>> >> >> >>>>
>> >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like
>> >> >> >>>>> the
>> >> >> >>> former
>> >> >> >>>>> slightly better because it's more descriptive.
>> >> >> >>>>>
>> >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
>> >> >> >>>>> covers,
>> >> >> >>>>> it
>> >> >> >>> would
>> >> >> >>>>> be more clear from a user-facing perspective to at least
>> >> >> >>>>> choose a
>> >> >> >>> package
>> >> >> >>>>> name for it that omits "sql".
>> >> >> >>>>>
>> >> >> >>>>> I would also be in favor of adding a separate Spark Schema
>> >> >> >>>>> module
>> >> >> >>>>> for
>> >> >> >>>> Spark
>> >> >> >>>>> SQL to rely on, but I imagine that might be too large a change
>> >> >> >>>>> at
>> >> >> this
>> >> >> >>>>> point?
>> >> >> >>>>>
>> >> >> >>>>> -Sandy
>> >> >> >>>>>
>> >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>> >> >> >>> matei.zaharia@gmail.com>
>> >> >> >>>>> wrote:
>> >> >> >>>>>
>> >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving it
>> >> >> >>>>>> another
>> >> >> >>>>> name,
>> >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since
>> >> >> >>>>>> that
>> >> >> >>>>>> was
>> >> >> >>> the
>> >> >> >>>>> most
>> >> >> >>>>>> obvious use case to many users.)
>> >> >> >>>>>>
>> >> >> >>>>>> Matei
>> >> >> >>>>>>
>> >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>> >> >> >>> matei.zaharia@gmail.com>
>> >> >> >>>>>> wrote:
>> >> >> >>>>>>>
>> >> >> >>>>>>> While it might be possible to move this concept to Spark
>> >> >> >>>>>>> Core
>> >> >> >>>>> long-term,
>> >> >> >>>>>> supporting structured data efficiently does require quite a
>> >> >> >>>>>> bit
>> >> >> >>>>>> of
>> >> >> >>> the
>> >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
>> >> >> >>>>>> columnar
>> >> >> >>>> storage.
>> >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL
>> >> >> >>>>>> server
>> >> >> >>>>>> --
>> >> >> >>> it's
>> >> >> >>>>>> meant to be a library for manipulating structured data. Since
>> >> >> >>>>>> this
>> >> >> >>> is
>> >> >> >>>>>> possible to build over the core API, it's pretty natural to
>> >> >> >>> organize it
>> >> >> >>>>>> that way, same as Spark Streaming is a library.
>> >> >> >>>>>>>
>> >> >> >>>>>>> Matei
>> >> >> >>>>>>>
>> >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers
>> >> >> >>>>>>>> <koert@tresata.com>
>> >> >> >>>> wrote:
>> >> >> >>>>>>>>
>> >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
>> >> >> >>>>>>>> format
>> >> >> >>> used
>> >> >> >>>>> for
>> >> >> >>>>>>>> bringing data into Spark from external systems, and used
>> >> >> >>>>>>>> for
>> >> >> >>> various
>> >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>> >> >> >>>>>>>>
>> >> >> >>>>>>>> i agree. this to me also implies it belongs in spark core,
>> >> >> >>>>>>>> not
>> >> >> >>> sql
>> >> >> >>>>>>>>
>> >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>> >> >> >>>>>>>>
>> >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the
>> >> >> >>>>>>>>> Jan.
>> >> >> >>> 13
>> >> >> >>>> Bay
>> >> >> >>>>>> Area
>> >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
>> >> >> >>> information
>> >> >> >>>> on
>> >> >> >>>>>> this
>> >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>> ________________________________
>> >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>> >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>> >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>> >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>> One thing potentially not clear from this e-mail, there
>> >> >> >>>>>>>>> will
>> >> >> >>>>>>>>> be
>> >> >> >>> a
>> >> >> >>>> 1:1
>> >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
>> >> >> >>>>>>>>> DataFrame.
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>> >> >> >>> rxin@databricks.com>
>> >> >> >>>>>> wrote:
>> >> >> >>>>>>>>>> Hi,
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
>> >> >> >>>>>>>>>> 1.3,
>> >> >> >>>>>>>>>> and
>> >> >> >>>>> wanted
>> >> >> >>>>>> to
>> >> >> >>>>>>>>>> get the community's opinion.
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
>> >> >> >>>>>>>>>> format
>> >> >> >>>> used
>> >> >> >>>>>> for
>> >> >> >>>>>>>>>> bringing data into Spark from external systems, and used
>> >> >> >>>>>>>>>> for
>> >> >> >>>> various
>> >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We
>> >> >> >>>>>>>>>> also
>> >> >> >>> expect
>> >> >> >>>>>> more
>> >> >> >>>>>>>>> and
>> >> >> >>>>>>>>>> more users to be programming directly against SchemaRDD
>> >> >> >>>>>>>>>> API
>> >> >> >>> rather
>> >> >> >>>>>> than
>> >> >> >>>>>>>>> the
>> >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used
>> >> >> >>>>>>>>>> DSL
>> >> >> >>>>> originally
>> >> >> >>>>>>>>>> designed for writing test cases, always has the
>> >> >> >>>>>>>>>> data-frame
>> >> >> >>>>>>>>>> like
>> >> >> >>>> API.
>> >> >> >>>>>> In
>> >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable
>> >> >> >>>>>>>>>> for
>> >> >> >>>>>>>>>> end
>> >> >> >>>>> users.
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> There are two motivations for the renaming:
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>> >> >> >>> SchemaRDD.
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>> >> >> >>> anymore
>> >> >> >>>>>> (even
>> >> >> >>>>>>>>>> though it would contain some RDD functions like map,
>> >> >> >>>>>>>>>> flatMap,
>> >> >> >>>> etc),
>> >> >> >>>>>> and
>> >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>> >> >> >>> confusing.
>> >> >> >>>>>>>>> Instead.
>> >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
>> >> >> >>> methods.
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>>
>> >> >> >>>>>>>>>> My understanding is that very few users program directly
>> >> >> >>> against
>> >> >> >>>> the
>> >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
>> >> >> >>> documented.
>> >> >> >>>>>>>>> However,
>> >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a type
>> >> >> >>>>>>>>>> alias
>> >> >> >>>>>> DataFrame
>> >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
>> >> >> >>>>> compatibility
>> >> >> >>>>>>>>> for
>> >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
>> >> >> >>> materials to
>> >> >> >>>>> use
>> >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>>
>> >> >> >>>>
>> >> >> >>>>
>> >> >> >>>> ---------------------------------------------------------------------
>> >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>>
>> >> >> >>>>
>> >> >> >>>>
>> >> >> >>>> ---------------------------------------------------------------------
>> >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> >>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>>>
>> >> >> >>>>>>>
>> >> >> >>>>>>
>> >> >> >>>>>>
>> >> >> >>>>>>
>> >> >> >>>
>> >> >> >>>
>> >> >> >>> ---------------------------------------------------------------------
>> >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >> >>>>>>
>> >> >> >>>>>>
>> >> >> >>>>>
>> >> >> >>>>
>> >> >> >>>
>> >> >> >>
>> >> >> >>
>> >> >>
>> >> >>
>> >> >>
>> >> >> ---------------------------------------------------------------------
>> >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >> >>
>> >> >>
>> >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 01:56:40 2015
Return-Path: <dev-return-11344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55AB810678
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 01:56:40 +0000 (UTC)
Received: (qmail 48142 invoked by uid 500); 29 Jan 2015 01:56:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48071 invoked by uid 500); 29 Jan 2015 01:56:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48060 invoked by uid 99); 29 Jan 2015 01:56:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 01:56:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 01:56:35 +0000
Received: by mail-lb0-f172.google.com with SMTP id l4so23308221lbv.3
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 17:55:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/YUpUhu5/wAjejwxkSs0mpzi5sLMNSv0CPjNby1XoXQ=;
        b=kmXGh2lc0AX2svBmOWChzwxubNQLseccnt8RSSaF7yijWeCtNjzNE01vAonOL9neFo
         uix2VwZFH/22DiGToZlhmkweyri3blEn2DeLJn/AVKr3Y4wpUAVtQPWNOYW9aqpuWTRO
         wfDGaCLrB1lmAyXt03BrorGvw4pCEuzeXQfYUdUhBXv4yResDPvYix78VDGOcLPy0GlQ
         iJjvKsbSPJB0MllLFmL8NLQj2g+HXpJD8IAfqToQTEzndbkBU+O/PlqohKKhSlEbx5jT
         r+aHTf17aMhuLeDGS1+cmOoZp/w3tkT94F39RNOchntGoH755OZt5VIAza8FZ4Loaber
         qVlg==
X-Gm-Message-State: ALoCoQn99pn2CRYPGcSP2J6RSRL3+fHlU87XtSyuPEl+fXwkaibqmV2XZylM6gY2hpF9kTO2Jrya
X-Received: by 10.152.5.38 with SMTP id p6mr11601529lap.91.1422496552924; Wed,
 28 Jan 2015 17:55:52 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Wed, 28 Jan 2015 17:55:32 -0800 (PST)
In-Reply-To: <CAN6Vra1K1qiuMNuoDBUS5t9LqmFULWQLvqOimLjAfdK_AKypFg@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
 <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
 <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
 <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com> <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
 <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
 <CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
 <CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
 <CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com> <CAN6Vra1K1qiuMNuoDBUS5t9LqmFULWQLvqOimLjAfdK_AKypFg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 28 Jan 2015 17:55:32 -0800
Message-ID: <CAAswR-7fKf8AASOMjtQ+o-Bz9_9VbABMf2Vj9kVsK9D193RgYA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Evan Chan <velvia.github@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01419acebb56c2050dc0c9f2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01419acebb56c2050dc0c9f2
Content-Type: text/plain; charset=UTF-8

In particular the performance tricks are in SpecificMutableRow.

On Wed, Jan 28, 2015 at 5:49 PM, Evan Chan <velvia.github@gmail.com> wrote:

> Yeah, it's "null".   I was worried you couldn't represent it in Row
> because of primitive types like Int (unless you box the Int, which
> would be a performance hit).  Anyways, I'll take another look at the
> Row API again  :-p
>
> On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com> wrote:
> > Isn't that just "null" in SQL?
> >
> > On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
> wrote:
> >>
> >> I believe that most DataFrame implementations out there, like Pandas,
> >> supports the idea of missing values / NA, and some support the idea of
> >> Not Meaningful as well.
> >>
> >> Does Row support anything like that?  That is important for certain
> >> applications.  I thought that Row worked by being a mutable object,
> >> but haven't looked into the details in a while.
> >>
> >> -Evan
> >>
> >> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >> > It shouldn't change the data source api at all because data sources
> >> > create
> >> > RDD[Row], and that gets converted into a DataFrame automatically
> >> > (previously
> >> > to SchemaRDD).
> >> >
> >> >
> >> >
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
> >> >
> >> > One thing that will break the data source API in 1.3 is the location
> of
> >> > types. Types were previously defined in sql.catalyst.types, and now
> >> > moved to
> >> > sql.types. After 1.3, sql.catalyst is hidden from users, and all
> public
> >> > APIs
> >> > have first class classes/objects defined in sql directly.
> >> >
> >> >
> >> >
> >> > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com>
> >> > wrote:
> >> >>
> >> >> Hey guys,
> >> >>
> >> >> How does this impact the data sources API?  I was planning on using
> >> >> this for a project.
> >> >>
> >> >> +1 that many things from spark-sql / DataFrame is universally
> >> >> desirable and useful.
> >> >>
> >> >> By the way, one thing that prevents the columnar compression stuff in
> >> >> Spark SQL from being more useful is, at least from previous talks
> with
> >> >> Reynold and Michael et al., that the format was not designed for
> >> >> persistence.
> >> >>
> >> >> I have a new project that aims to change that.  It is a
> >> >> zero-serialisation, high performance binary vector library, designed
> >> >> from the outset to be a persistent storage friendly.  May be one day
> >> >> it can replace the Spark SQL columnar compression.
> >> >>
> >> >> Michael told me this would be a lot of work, and recreates parts of
> >> >> Parquet, but I think it's worth it.  LMK if you'd like more details.
> >> >>
> >> >> -Evan
> >> >>
> >> >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
> >> >> wrote:
> >> >> > Alright I have merged the patch (
> >> >> > https://github.com/apache/spark/pull/4173
> >> >> > ) since I don't see any strong opinions against it (as a matter of
> >> >> > fact
> >> >> > most were for it). We can still change it if somebody lays out a
> >> >> > strong
> >> >> > argument.
> >> >> >
> >> >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
> >> >> > <matei.zaharia@gmail.com>
> >> >> > wrote:
> >> >> >
> >> >> >> The type alias means your methods can specify either type and they
> >> >> >> will
> >> >> >> work. It's just another name for the same type. But Scaladocs and
> >> >> >> such
> >> >> >> will
> >> >> >> show DataFrame as the type.
> >> >> >>
> >> >> >> Matei
> >> >> >>
> >> >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> >> >> >> dirceu.semighini@gmail.com> wrote:
> >> >> >> >
> >> >> >> > Reynold,
> >> >> >> > But with type alias we will have the same problem, right?
> >> >> >> > If the methods doesn't receive schemardd anymore, we will have
> to
> >> >> >> > change
> >> >> >> > our code to migrade from schema to dataframe. Unless we have an
> >> >> >> > implicit
> >> >> >> > conversion between DataFrame and SchemaRDD
> >> >> >> >
> >> >> >> >
> >> >> >> >
> >> >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> >> >> >> >
> >> >> >> >> Dirceu,
> >> >> >> >>
> >> >> >> >> That is not possible because one cannot overload return types.
> >> >> >> >>
> >> >> >> >> SQLContext.parquetFile (and many other methods) needs to return
> >> >> >> >> some
> >> >> >> type,
> >> >> >> >> and that type cannot be both SchemaRDD and DataFrame.
> >> >> >> >>
> >> >> >> >> In 1.3, we will create a type alias for DataFrame called
> >> >> >> >> SchemaRDD
> >> >> >> >> to
> >> >> >> not
> >> >> >> >> break source compatibility for Scala.
> >> >> >> >>
> >> >> >> >>
> >> >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> >> >> >> >> dirceu.semighini@gmail.com> wrote:
> >> >> >> >>
> >> >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
> >> >> >> >>> removed
> >> >> >> >>> in
> >> >> >> the
> >> >> >> >>> release 1.5(+/- 1)  for example, and the new code been added
> to
> >> >> >> DataFrame?
> >> >> >> >>> With this, we don't impact in existing code for the next few
> >> >> >> >>> releases.
> >> >> >> >>>
> >> >> >> >>>
> >> >> >> >>>
> >> >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <
> kushal.datta@gmail.com>:
> >> >> >> >>>
> >> >> >> >>>> I want to address the issue that Matei raised about the heavy
> >> >> >> >>>> lifting
> >> >> >> >>>> required for a full SQL support. It is amazing that even
> after
> >> >> >> >>>> 30
> >> >> >> years
> >> >> >> >>> of
> >> >> >> >>>> research there is not a single good open source columnar
> >> >> >> >>>> database
> >> >> >> >>>> like
> >> >> >> >>>> Vertica. There is a column store option in MySQL, but it is
> not
> >> >> >> >>>> nearly
> >> >> >> >>> as
> >> >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need
> >> >> >> >>>> for
> >> >> >> >>>> such
> >> >> >> a
> >> >> >> >>>> system. I wonder why so and it's high time to change that.
> >> >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <
> sandy.ryza@cloudera.com>
> >> >> >> wrote:
> >> >> >> >>>>
> >> >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like
> >> >> >> >>>>> the
> >> >> >> >>> former
> >> >> >> >>>>> slightly better because it's more descriptive.
> >> >> >> >>>>>
> >> >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
> >> >> >> >>>>> covers,
> >> >> >> >>>>> it
> >> >> >> >>> would
> >> >> >> >>>>> be more clear from a user-facing perspective to at least
> >> >> >> >>>>> choose a
> >> >> >> >>> package
> >> >> >> >>>>> name for it that omits "sql".
> >> >> >> >>>>>
> >> >> >> >>>>> I would also be in favor of adding a separate Spark Schema
> >> >> >> >>>>> module
> >> >> >> >>>>> for
> >> >> >> >>>> Spark
> >> >> >> >>>>> SQL to rely on, but I imagine that might be too large a
> change
> >> >> >> >>>>> at
> >> >> >> this
> >> >> >> >>>>> point?
> >> >> >> >>>>>
> >> >> >> >>>>> -Sandy
> >> >> >> >>>>>
> >> >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> >> >> >> >>> matei.zaharia@gmail.com>
> >> >> >> >>>>> wrote:
> >> >> >> >>>>>
> >> >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving
> it
> >> >> >> >>>>>> another
> >> >> >> >>>>> name,
> >> >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since
> >> >> >> >>>>>> that
> >> >> >> >>>>>> was
> >> >> >> >>> the
> >> >> >> >>>>> most
> >> >> >> >>>>>> obvious use case to many users.)
> >> >> >> >>>>>>
> >> >> >> >>>>>> Matei
> >> >> >> >>>>>>
> >> >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> >> >> >> >>> matei.zaharia@gmail.com>
> >> >> >> >>>>>> wrote:
> >> >> >> >>>>>>>
> >> >> >> >>>>>>> While it might be possible to move this concept to Spark
> >> >> >> >>>>>>> Core
> >> >> >> >>>>> long-term,
> >> >> >> >>>>>> supporting structured data efficiently does require quite a
> >> >> >> >>>>>> bit
> >> >> >> >>>>>> of
> >> >> >> >>> the
> >> >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
> >> >> >> >>>>>> columnar
> >> >> >> >>>> storage.
> >> >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL
> >> >> >> >>>>>> server
> >> >> >> >>>>>> --
> >> >> >> >>> it's
> >> >> >> >>>>>> meant to be a library for manipulating structured data.
> Since
> >> >> >> >>>>>> this
> >> >> >> >>> is
> >> >> >> >>>>>> possible to build over the core API, it's pretty natural to
> >> >> >> >>> organize it
> >> >> >> >>>>>> that way, same as Spark Streaming is a library.
> >> >> >> >>>>>>>
> >> >> >> >>>>>>> Matei
> >> >> >> >>>>>>>
> >> >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers
> >> >> >> >>>>>>>> <koert@tresata.com>
> >> >> >> >>>> wrote:
> >> >> >> >>>>>>>>
> >> >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
> >> >> >> >>>>>>>> format
> >> >> >> >>> used
> >> >> >> >>>>> for
> >> >> >> >>>>>>>> bringing data into Spark from external systems, and used
> >> >> >> >>>>>>>> for
> >> >> >> >>> various
> >> >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> >> >> >> >>>>>>>>
> >> >> >> >>>>>>>> i agree. this to me also implies it belongs in spark
> core,
> >> >> >> >>>>>>>> not
> >> >> >> >>> sql
> >> >> >> >>>>>>>>
> >> >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> >> >> >> >>>>>>>>
> >> >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet,
> the
> >> >> >> >>>>>>>>> Jan.
> >> >> >> >>> 13
> >> >> >> >>>> Bay
> >> >> >> >>>>>> Area
> >> >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> >> >> >> >>> information
> >> >> >> >>>> on
> >> >> >> >>>>>> this
> >> >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>> ________________________________
> >> >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> >> >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> >> >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >> >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> >> >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>> One thing potentially not clear from this e-mail, there
> >> >> >> >>>>>>>>> will
> >> >> >> >>>>>>>>> be
> >> >> >> >>> a
> >> >> >> >>>> 1:1
> >> >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
> >> >> >> >>>>>>>>> DataFrame.
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> >> >> >> >>> rxin@databricks.com>
> >> >> >> >>>>>> wrote:
> >> >> >> >>>>>>>>>> Hi,
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
> >> >> >> >>>>>>>>>> 1.3,
> >> >> >> >>>>>>>>>> and
> >> >> >> >>>>> wanted
> >> >> >> >>>>>> to
> >> >> >> >>>>>>>>>> get the community's opinion.
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
> >> >> >> >>>>>>>>>> format
> >> >> >> >>>> used
> >> >> >> >>>>>> for
> >> >> >> >>>>>>>>>> bringing data into Spark from external systems, and
> used
> >> >> >> >>>>>>>>>> for
> >> >> >> >>>> various
> >> >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We
> >> >> >> >>>>>>>>>> also
> >> >> >> >>> expect
> >> >> >> >>>>>> more
> >> >> >> >>>>>>>>> and
> >> >> >> >>>>>>>>>> more users to be programming directly against SchemaRDD
> >> >> >> >>>>>>>>>> API
> >> >> >> >>> rather
> >> >> >> >>>>>> than
> >> >> >> >>>>>>>>> the
> >> >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used
> >> >> >> >>>>>>>>>> DSL
> >> >> >> >>>>> originally
> >> >> >> >>>>>>>>>> designed for writing test cases, always has the
> >> >> >> >>>>>>>>>> data-frame
> >> >> >> >>>>>>>>>> like
> >> >> >> >>>> API.
> >> >> >> >>>>>> In
> >> >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable
> >> >> >> >>>>>>>>>> for
> >> >> >> >>>>>>>>>> end
> >> >> >> >>>>> users.
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> There are two motivations for the renaming:
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
> >> >> >> >>> SchemaRDD.
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
> RDD
> >> >> >> >>> anymore
> >> >> >> >>>>>> (even
> >> >> >> >>>>>>>>>> though it would contain some RDD functions like map,
> >> >> >> >>>>>>>>>> flatMap,
> >> >> >> >>>> etc),
> >> >> >> >>>>>> and
> >> >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
> >> >> >> >>> confusing.
> >> >> >> >>>>>>>>> Instead.
> >> >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
> RDD
> >> >> >> >>> methods.
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>>
> >> >> >> >>>>>>>>>> My understanding is that very few users program
> directly
> >> >> >> >>> against
> >> >> >> >>>> the
> >> >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
> >> >> >> >>> documented.
> >> >> >> >>>>>>>>> However,
> >> >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a
> type
> >> >> >> >>>>>>>>>> alias
> >> >> >> >>>>>> DataFrame
> >> >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain
> source
> >> >> >> >>>>> compatibility
> >> >> >> >>>>>>>>> for
> >> >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
> >> >> >> >>> materials to
> >> >> >> >>>>> use
> >> >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>>
> >> >> >> >>>>
> >> >> >> >>>>
> >> >> >> >>>>
> ---------------------------------------------------------------------
> >> >> >> >>>>>>>>> To unsubscribe, e-mail:
> dev-unsubscribe@spark.apache.org
> >> >> >> >>>>>>>>> For additional commands, e-mail:
> dev-help@spark.apache.org
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>>
> >> >> >> >>>>
> >> >> >> >>>>
> >> >> >> >>>>
> ---------------------------------------------------------------------
> >> >> >> >>>>>>>>> To unsubscribe, e-mail:
> dev-unsubscribe@spark.apache.org
> >> >> >> >>>>>>>>> For additional commands, e-mail:
> dev-help@spark.apache.org
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>>>
> >> >> >> >>>>>>>
> >> >> >> >>>>>>
> >> >> >> >>>>>>
> >> >> >> >>>>>>
> >> >> >> >>>
> >> >> >> >>>
> >> >> >> >>>
> ---------------------------------------------------------------------
> >> >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> >> >>>>>>
> >> >> >> >>>>>>
> >> >> >> >>>>>
> >> >> >> >>>>
> >> >> >> >>>
> >> >> >> >>
> >> >> >> >>
> >> >> >>
> >> >> >>
> >> >> >>
> >> >> >>
> ---------------------------------------------------------------------
> >> >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >> >> For additional commands, e-mail: dev-help@spark.apache.org
> >> >> >>
> >> >> >>
> >> >
> >> >
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e01419acebb56c2050dc0c9f2--

From dev-return-11345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 02:01:48 2015
Return-Path: <dev-return-11345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 749AC106AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 02:01:48 +0000 (UTC)
Received: (qmail 59996 invoked by uid 500); 29 Jan 2015 02:01:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59923 invoked by uid 500); 29 Jan 2015 02:01:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59911 invoked by uid 99); 29 Jan 2015 02:01:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 02:01:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 02:01:43 +0000
Received: by mail-oi0-f47.google.com with SMTP id a141so22325141oig.6
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 18:01:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=XlWoJDJkvMMvm0z9/FYe0BW5rBd3n3YQHXzbd18I/aM=;
        b=niOSlfQfhBE2RptizEcDJQUux4WLZVBwU2jZxn6xXdqBEKsAPeq0IxWeVFBEXV1587
         rEejzlXGeZE7LWAWPBD+g2rwsQAYK6IgJY5RPG4DtZRLIFRYZwSN47nFNyizFY1j2Kg4
         5yv0IRzfAtwMvYG4jj9xZJ9uEUBUOzVNumeBzJ7Uw53EVQYSZ5tCWjAupF8K0G7j/+yc
         cqFWf2BBDrCUNRs7Zak7RIRvjv/fGytQRE7Sm017rCX/XoLhTAFofqTKN8CQxmr8sldU
         zWDP8cqLCX5/WX1M0f87pEMwSb+Nqriz7nqwyuSNWJmW0KzpFTj6A2tIGUVvMB1FcVGE
         Tsng==
MIME-Version: 1.0
X-Received: by 10.182.27.241 with SMTP id w17mr4102066obg.14.1422496882928;
 Wed, 28 Jan 2015 18:01:22 -0800 (PST)
Received: by 10.202.198.67 with HTTP; Wed, 28 Jan 2015 18:01:22 -0800 (PST)
In-Reply-To: <CAPh_B=bA8QsJ3CD=XKzGhPodeB_T9EwOn-ZNw8iSCqMBu=8tjg@mail.gmail.com>
References: <CACVCA=cy193cXxSrYOQSgbrYyQQMaECCoNe-gHTUct_Uwaqjpg@mail.gmail.com>
	<CAPh_B=bA8QsJ3CD=XKzGhPodeB_T9EwOn-ZNw8iSCqMBu=8tjg@mail.gmail.com>
Date: Wed, 28 Jan 2015 18:01:22 -0800
Message-ID: <CABPQxssBsOj4KPGgYNP9GZsQm_Gds7F8ZPfgG=uaVmujcS0BOg@mail.gmail.com>
Subject: Re: spark akka fork : is the source anywhere?
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: jay vyas <jayunit100.apache@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

It's maintained here:

https://github.com/pwendell/akka/tree/2.2.3-shaded-proto

Over time, this is something that would be great to get rid of, per rxin

On Wed, Jan 28, 2015 at 3:33 PM, Reynold Xin <rxin@databricks.com> wrote:
> Hopefully problems like this will go away entirely in the next couple of
> releases. https://issues.apache.org/jira/browse/SPARK-5293
>
>
>
> On Wed, Jan 28, 2015 at 3:12 PM, jay vyas <jayunit100.apache@gmail.com>
> wrote:
>
>> Hi spark. Where is akka coming from in spark ?
>>
>> I see the distribution referenced is a spark artifact... but not in the
>> apache namespace.
>>
>>      <akka.group>org.spark-project.akka</akka.group>
>>      <akka.version>2.3.4-spark</akka.version>
>>
>> Clearly this is a deliberate thought out change (See SPARK-1812), but its
>> not clear where 2.3.4 spark is coming from and who is maintaining its
>> release?
>>
>> --
>> jay vyas
>>
>> PS
>>
>> I've had some conversations with will benton as well about this, and its
>> clear that some modifications to akka are needed, or else a protobug error
>> occurs, which amount to serialization incompatibilities, hence if one wants
>> to build spark from sources, the patched akka is required (or else, manual
>> patching needs to be done)...
>>
>> 15/01/28 22:58:10 ERROR ActorSystemImpl: Uncaught fatal error from thread
>> [sparkWorker-akka.remote.default-remote-dispatcher-6] shutting down
>> ActorSystem [sparkWorker] java.lang.VerifyError: class
>> akka.remote.WireFormats$AkkaControlMessage overrides final method
>> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 02:11:39 2015
Return-Path: <dev-return-11346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EFC4610732
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 02:11:39 +0000 (UTC)
Received: (qmail 79519 invoked by uid 500); 29 Jan 2015 02:11:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79439 invoked by uid 500); 29 Jan 2015 02:11:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79427 invoked by uid 99); 29 Jan 2015 02:11:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 02:11:38 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 02:11:13 +0000
Received: by mail-vc0-f181.google.com with SMTP id id10so7987106vcb.12
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 18:11:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=DCF9RieuBLArsYznvbaru/oK3QlETaVXteM8Juo4CKg=;
        b=Bbws7sKR//ywxiGgaZ4Fj1b1hKR1/Y3k/r3icSor1fef+Alu5r13nTsDUTqa/VrtBn
         Oaror7ZCoAQa2w4y+DCtTiL2MPxV+YhtDymJj4PQvrfg6UDxwh2VPiieqoXcxiYMicSg
         tEzDGsg1Q/mRngZfHKrAOxth9CwpeYPk8cVAl6FKxXVqjACOzojhzpUads0YN7g8bViS
         nGA/ZTLmpxkvwKG20vDuK1GD0ZtqO1MEUdbYXc3anS5n4ck8hMOrzDcqs7B5d4PGl6hy
         1UGYRVa4l10sHFhSYvj4+CQtTfn4uVFawPP4r2grFuLGydbu7IuU7rQWF8bsVRmhRbKS
         TJgQ==
X-Received: by 10.52.139.7 with SMTP id qu7mr2818182vdb.68.1422497471227; Wed,
 28 Jan 2015 18:11:11 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Wed, 28 Jan 2015 18:10:51 -0800 (PST)
In-Reply-To: <CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
 <1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
 <CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
 <7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com> <1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
 <CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
 <CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
 <CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
 <CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
 <CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
 <D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com> <CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
 <CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
 <CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
 <CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com> <CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Wed, 28 Jan 2015 18:10:51 -0800
Message-ID: <CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
To: Reynold Xin <rxin@databricks.com>
Cc: Evan Chan <velvia.github@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec52d53ab7770c0050dc1007e
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec52d53ab7770c0050dc1007e
Content-Type: text/plain; charset=UTF-8

You've got to be a little bit careful here. "NA" in systems like R or
pandas may have special meaning that is distinct from "null".

See, e.g. http://www.r-bloggers.com/r-na-vs-null/



On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com> wrote:

> Isn't that just "null" in SQL?
>
> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
> wrote:
>
> > I believe that most DataFrame implementations out there, like Pandas,
> > supports the idea of missing values / NA, and some support the idea of
> > Not Meaningful as well.
> >
> > Does Row support anything like that?  That is important for certain
> > applications.  I thought that Row worked by being a mutable object,
> > but haven't looked into the details in a while.
> >
> > -Evan
> >
> > On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> > > It shouldn't change the data source api at all because data sources
> > create
> > > RDD[Row], and that gets converted into a DataFrame automatically
> > (previously
> > > to SchemaRDD).
> > >
> > >
> >
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
> > >
> > > One thing that will break the data source API in 1.3 is the location of
> > > types. Types were previously defined in sql.catalyst.types, and now
> > moved to
> > > sql.types. After 1.3, sql.catalyst is hidden from users, and all public
> > APIs
> > > have first class classes/objects defined in sql directly.
> > >
> > >
> > >
> > > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com>
> > wrote:
> > >>
> > >> Hey guys,
> > >>
> > >> How does this impact the data sources API?  I was planning on using
> > >> this for a project.
> > >>
> > >> +1 that many things from spark-sql / DataFrame is universally
> > >> desirable and useful.
> > >>
> > >> By the way, one thing that prevents the columnar compression stuff in
> > >> Spark SQL from being more useful is, at least from previous talks with
> > >> Reynold and Michael et al., that the format was not designed for
> > >> persistence.
> > >>
> > >> I have a new project that aims to change that.  It is a
> > >> zero-serialisation, high performance binary vector library, designed
> > >> from the outset to be a persistent storage friendly.  May be one day
> > >> it can replace the Spark SQL columnar compression.
> > >>
> > >> Michael told me this would be a lot of work, and recreates parts of
> > >> Parquet, but I think it's worth it.  LMK if you'd like more details.
> > >>
> > >> -Evan
> > >>
> > >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >> > Alright I have merged the patch (
> > >> > https://github.com/apache/spark/pull/4173
> > >> > ) since I don't see any strong opinions against it (as a matter of
> > fact
> > >> > most were for it). We can still change it if somebody lays out a
> > strong
> > >> > argument.
> > >> >
> > >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
> > >> > <matei.zaharia@gmail.com>
> > >> > wrote:
> > >> >
> > >> >> The type alias means your methods can specify either type and they
> > will
> > >> >> work. It's just another name for the same type. But Scaladocs and
> > such
> > >> >> will
> > >> >> show DataFrame as the type.
> > >> >>
> > >> >> Matei
> > >> >>
> > >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> > >> >> dirceu.semighini@gmail.com> wrote:
> > >> >> >
> > >> >> > Reynold,
> > >> >> > But with type alias we will have the same problem, right?
> > >> >> > If the methods doesn't receive schemardd anymore, we will have to
> > >> >> > change
> > >> >> > our code to migrade from schema to dataframe. Unless we have an
> > >> >> > implicit
> > >> >> > conversion between DataFrame and SchemaRDD
> > >> >> >
> > >> >> >
> > >> >> >
> > >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> > >> >> >
> > >> >> >> Dirceu,
> > >> >> >>
> > >> >> >> That is not possible because one cannot overload return types.
> > >> >> >>
> > >> >> >> SQLContext.parquetFile (and many other methods) needs to return
> > some
> > >> >> type,
> > >> >> >> and that type cannot be both SchemaRDD and DataFrame.
> > >> >> >>
> > >> >> >> In 1.3, we will create a type alias for DataFrame called
> SchemaRDD
> > >> >> >> to
> > >> >> not
> > >> >> >> break source compatibility for Scala.
> > >> >> >>
> > >> >> >>
> > >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> > >> >> >> dirceu.semighini@gmail.com> wrote:
> > >> >> >>
> > >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
> > removed
> > >> >> >>> in
> > >> >> the
> > >> >> >>> release 1.5(+/- 1)  for example, and the new code been added to
> > >> >> DataFrame?
> > >> >> >>> With this, we don't impact in existing code for the next few
> > >> >> >>> releases.
> > >> >> >>>
> > >> >> >>>
> > >> >> >>>
> > >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com
> >:
> > >> >> >>>
> > >> >> >>>> I want to address the issue that Matei raised about the heavy
> > >> >> >>>> lifting
> > >> >> >>>> required for a full SQL support. It is amazing that even after
> > 30
> > >> >> years
> > >> >> >>> of
> > >> >> >>>> research there is not a single good open source columnar
> > database
> > >> >> >>>> like
> > >> >> >>>> Vertica. There is a column store option in MySQL, but it is
> not
> > >> >> >>>> nearly
> > >> >> >>> as
> > >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need
> for
> > >> >> >>>> such
> > >> >> a
> > >> >> >>>> system. I wonder why so and it's high time to change that.
> > >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza" <
> sandy.ryza@cloudera.com>
> > >> >> wrote:
> > >> >> >>>>
> > >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like
> > the
> > >> >> >>> former
> > >> >> >>>>> slightly better because it's more descriptive.
> > >> >> >>>>>
> > >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
> > covers,
> > >> >> >>>>> it
> > >> >> >>> would
> > >> >> >>>>> be more clear from a user-facing perspective to at least
> > choose a
> > >> >> >>> package
> > >> >> >>>>> name for it that omits "sql".
> > >> >> >>>>>
> > >> >> >>>>> I would also be in favor of adding a separate Spark Schema
> > module
> > >> >> >>>>> for
> > >> >> >>>> Spark
> > >> >> >>>>> SQL to rely on, but I imagine that might be too large a
> change
> > at
> > >> >> this
> > >> >> >>>>> point?
> > >> >> >>>>>
> > >> >> >>>>> -Sandy
> > >> >> >>>>>
> > >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> > >> >> >>> matei.zaharia@gmail.com>
> > >> >> >>>>> wrote:
> > >> >> >>>>>
> > >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving it
> > >> >> >>>>>> another
> > >> >> >>>>> name,
> > >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since
> that
> > >> >> >>>>>> was
> > >> >> >>> the
> > >> >> >>>>> most
> > >> >> >>>>>> obvious use case to many users.)
> > >> >> >>>>>>
> > >> >> >>>>>> Matei
> > >> >> >>>>>>
> > >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> > >> >> >>> matei.zaharia@gmail.com>
> > >> >> >>>>>> wrote:
> > >> >> >>>>>>>
> > >> >> >>>>>>> While it might be possible to move this concept to Spark
> Core
> > >> >> >>>>> long-term,
> > >> >> >>>>>> supporting structured data efficiently does require quite a
> > bit
> > >> >> >>>>>> of
> > >> >> >>> the
> > >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
> > columnar
> > >> >> >>>> storage.
> > >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL
> server
> > >> >> >>>>>> --
> > >> >> >>> it's
> > >> >> >>>>>> meant to be a library for manipulating structured data.
> Since
> > >> >> >>>>>> this
> > >> >> >>> is
> > >> >> >>>>>> possible to build over the core API, it's pretty natural to
> > >> >> >>> organize it
> > >> >> >>>>>> that way, same as Spark Streaming is a library.
> > >> >> >>>>>>>
> > >> >> >>>>>>> Matei
> > >> >> >>>>>>>
> > >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
> > koert@tresata.com>
> > >> >> >>>> wrote:
> > >> >> >>>>>>>>
> > >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
> > >> >> >>>>>>>> format
> > >> >> >>> used
> > >> >> >>>>> for
> > >> >> >>>>>>>> bringing data into Spark from external systems, and used
> for
> > >> >> >>> various
> > >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> > >> >> >>>>>>>>
> > >> >> >>>>>>>> i agree. this to me also implies it belongs in spark core,
> > not
> > >> >> >>> sql
> > >> >> >>>>>>>>
> > >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> > >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> > >> >> >>>>>>>>
> > >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet, the
> > >> >> >>>>>>>>> Jan.
> > >> >> >>> 13
> > >> >> >>>> Bay
> > >> >> >>>>>> Area
> > >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> > >> >> >>> information
> > >> >> >>>> on
> > >> >> >>>>>> this
> > >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>> ________________________________
> > >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> > >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> > >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> > >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> > >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>> One thing potentially not clear from this e-mail, there
> > will
> > >> >> >>>>>>>>> be
> > >> >> >>> a
> > >> >> >>>> 1:1
> > >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
> > DataFrame.
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> > >> >> >>> rxin@databricks.com>
> > >> >> >>>>>> wrote:
> > >> >> >>>>>>>>>> Hi,
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
> 1.3,
> > >> >> >>>>>>>>>> and
> > >> >> >>>>> wanted
> > >> >> >>>>>> to
> > >> >> >>>>>>>>>> get the community's opinion.
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
> > >> >> >>>>>>>>>> format
> > >> >> >>>> used
> > >> >> >>>>>> for
> > >> >> >>>>>>>>>> bringing data into Spark from external systems, and used
> > for
> > >> >> >>>> various
> > >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We
> > also
> > >> >> >>> expect
> > >> >> >>>>>> more
> > >> >> >>>>>>>>> and
> > >> >> >>>>>>>>>> more users to be programming directly against SchemaRDD
> > API
> > >> >> >>> rather
> > >> >> >>>>>> than
> > >> >> >>>>>>>>> the
> > >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used
> > DSL
> > >> >> >>>>> originally
> > >> >> >>>>>>>>>> designed for writing test cases, always has the
> data-frame
> > >> >> >>>>>>>>>> like
> > >> >> >>>> API.
> > >> >> >>>>>> In
> > >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable
> for
> > >> >> >>>>>>>>>> end
> > >> >> >>>>> users.
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> There are two motivations for the renaming:
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
> > >> >> >>> SchemaRDD.
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
> RDD
> > >> >> >>> anymore
> > >> >> >>>>>> (even
> > >> >> >>>>>>>>>> though it would contain some RDD functions like map,
> > >> >> >>>>>>>>>> flatMap,
> > >> >> >>>> etc),
> > >> >> >>>>>> and
> > >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
> > >> >> >>> confusing.
> > >> >> >>>>>>>>> Instead.
> > >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
> > >> >> >>> methods.
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>>
> > >> >> >>>>>>>>>> My understanding is that very few users program directly
> > >> >> >>> against
> > >> >> >>>> the
> > >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
> > >> >> >>> documented.
> > >> >> >>>>>>>>> However,
> > >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a type
> > >> >> >>>>>>>>>> alias
> > >> >> >>>>>> DataFrame
> > >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain source
> > >> >> >>>>> compatibility
> > >> >> >>>>>>>>> for
> > >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
> > >> >> >>> materials to
> > >> >> >>>>> use
> > >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>>
> > >> >> >>>>
> > >> >> >>>>
> > ---------------------------------------------------------------------
> > >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> >> >>>>>>>>> For additional commands, e-mail:
> dev-help@spark.apache.org
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>>
> > >> >> >>>>
> > >> >> >>>>
> > ---------------------------------------------------------------------
> > >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> >> >>>>>>>>> For additional commands, e-mail:
> dev-help@spark.apache.org
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>>>
> > >> >> >>>>>>>
> > >> >> >>>>>>
> > >> >> >>>>>>
> > >> >> >>>>>>
> > >> >> >>>
> > >> >> >>>
> > ---------------------------------------------------------------------
> > >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> > >> >> >>>>>>
> > >> >> >>>>>>
> > >> >> >>>>>
> > >> >> >>>>
> > >> >> >>>
> > >> >> >>
> > >> >> >>
> > >> >>
> > >> >>
> > >> >>
> ---------------------------------------------------------------------
> > >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> >> For additional commands, e-mail: dev-help@spark.apache.org
> > >> >>
> > >> >>
> > >
> > >
> >
>

--bcaec52d53ab7770c0050dc1007e--

From dev-return-11347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 05:57:46 2015
Return-Path: <dev-return-11347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A82D10CA4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 05:57:46 +0000 (UTC)
Received: (qmail 85865 invoked by uid 500); 29 Jan 2015 05:57:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85799 invoked by uid 500); 29 Jan 2015 05:57:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85537 invoked by uid 99); 29 Jan 2015 05:57:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 05:57:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.174 as permitted sender)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 05:57:31 +0000
Received: by mail-lb0-f174.google.com with SMTP id f15so24504425lbj.5
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 21:57:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=JzJic9MQRvvsJRNXsWH2uUg5MOCZpg8fghFDHN6d8lA=;
        b=RqEUyAmo2I7LjL67lanoeuWQ0jzDsbSwznq6ADPEk6by/F429oHXRG68bHixm5h+9J
         1yoD69uk3NuDGLwnD13EtPnYPOriptoqbrb+TNPtsRl8eQou44A2vlsHhL2becrUQHI3
         qNwurPnLhhlZZB3e0hTnlBuyB5LOAomilACv6SsyT2wnpF0josfCCvuEZjHPJPG80hBL
         jKBJ6Dlu1lzpZqE3eBcEQcz4CW5oRQWVaf403msIljNatamFJ3fALFTbAWQmtCEuyQK2
         VWm1c5kMnBbCIHF6IuWVVYk8kb+He4BLprovget8Py58wpEH46PFAF9SDcJO4V9KIaZV
         HYpg==
X-Gm-Message-State: ALoCoQlKU/ItUsSuyCaJuH1d2+K9IIxttGt8jqoCUib4Y90jFgcY6oMfCGM4Yh6EAbyayKk/RFBR
X-Received: by 10.112.199.69 with SMTP id ji5mr480698lbc.45.1422511028748;
 Wed, 28 Jan 2015 21:57:08 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.185.97 with HTTP; Wed, 28 Jan 2015 21:56:48 -0800 (PST)
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 28 Jan 2015 21:56:48 -0800
Message-ID: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
Subject: emergency jenkins restart soon
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c239088eced3050dc4285e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c239088eced3050dc4285e
Content-Type: text/plain; charset=UTF-8

the spark master builds stopped triggering ~yesterday and the logs don't
show anything.  i'm going to give the current batch of spark pull request
builder jobs a little more time (~30 mins) to finish, then kill whatever is
left and restart jenkins.  anything that was queued or killed will be
retriggered once jenkins is back up.

sorry for the inconvenience, we'll get this sorted asap.

thanks,

shane

--001a11c239088eced3050dc4285e--

From dev-return-11348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 06:30:37 2015
Return-Path: <dev-return-11348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EFE9710D73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 06:30:36 +0000 (UTC)
Received: (qmail 35398 invoked by uid 500); 29 Jan 2015 06:30:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35327 invoked by uid 500); 29 Jan 2015 06:30:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35315 invoked by uid 99); 29 Jan 2015 06:30:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:30:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:30:11 +0000
Received: by mail-lb0-f172.google.com with SMTP id l4so24700445lbv.3
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 22:29:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=zaugYeNIwHI6gNoUnpnb0Wn608RxLaGLq2Xl3Ljnlac=;
        b=Aalsg4caFJkuYGK/Iw2oQrPIktTwlrtEG4Krfwb5td79OsTSyZnFZAtoQVf4LiqYch
         gzyYuDkBJ1ZNCQWopXMqiU7RiPV/3P5h//hyyY8fnqd1RawzCqnVhjx45WtjJGPUje9/
         9Z0F9KKXhMecrzSAkiTbe/P7CSxwOWyWWbmk543gTeY+HI7PxSMrzqp7A2PiVXnjpX7O
         np1RMkTUN9v2FeuV/BR9zSrSuIEFf67On7bEzPV5/aymGqOtdAORx9ddLyJl1oXDOwew
         Vdu+aBpVAgTez6lPD8xVS9JRMpY9mT2LkWqDV+qUtalvX+2H67rJCQJBoMrnMxmOn204
         bb5Q==
X-Gm-Message-State: ALoCoQl5Acv8qqFWK2WkaaedDIJk0AqMZbFkxt8dFMZ7Fpq+/JnJchhfs9yVdI+tcrBlVjBKE70t
X-Received: by 10.152.27.228 with SMTP id w4mr3092936lag.75.1422512964206;
 Wed, 28 Jan 2015 22:29:24 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.185.97 with HTTP; Wed, 28 Jan 2015 22:29:03 -0800 (PST)
In-Reply-To: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
References: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 28 Jan 2015 22:29:03 -0800
Message-ID: <CACdU-dRVcq9dqPTZtwRtovTy+HYo8GfrhOuWzVOOEQtagGBVMg@mail.gmail.com>
Subject: Re: emergency jenkins restart soon
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160a3b6eb94f2050dc49bac
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a3b6eb94f2050dc49bac
Content-Type: text/plain; charset=UTF-8

jenkins is back up and all builds have been retriggered...  things are
building and looking good, and i'll keep an eye on the spark master builds
tonite and tomorrow.

On Wed, Jan 28, 2015 at 9:56 PM, shane knapp <sknapp@berkeley.edu> wrote:

> the spark master builds stopped triggering ~yesterday and the logs don't
> show anything.  i'm going to give the current batch of spark pull request
> builder jobs a little more time (~30 mins) to finish, then kill whatever is
> left and restart jenkins.  anything that was queued or killed will be
> retriggered once jenkins is back up.
>
> sorry for the inconvenience, we'll get this sorted asap.
>
> thanks,
>
> shane
>

--089e0160a3b6eb94f2050dc49bac--

From dev-return-11349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 06:33:09 2015
Return-Path: <dev-return-11349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0D54410D80
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 06:33:09 +0000 (UTC)
Received: (qmail 39020 invoked by uid 500); 29 Jan 2015 06:33:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38940 invoked by uid 500); 29 Jan 2015 06:33:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38413 invoked by uid 99); 29 Jan 2015 06:33:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:33:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:33:01 +0000
Received: by mail-qg0-f48.google.com with SMTP id z60so24377006qgd.7
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 22:31:35 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1Mw3YxmBl7PXfsClynGnakLJMtNu322sNp86ayiauAs=;
        b=e18pguOpRqh1hGthYhgSdWv2wJCkcH53VEiHXI5VLnzrrNqcCWJgv0RmJwj4PYDakE
         w8W6Ko2hXKHl8tkcJqntYzxcnIna2HzjUyGknJPwmEiFsaUSHqK21y0FydvX6Se7am2E
         d3y9fYBpqi6w8L5RV7EZYqmwUFXU7vFDPq9Ta2LSJgxrL2g3JTe/tDhZb2TrTquDSNyx
         dWmcLQixw3WwOeefSbGqIxhw5CfDr3o4SOUWfQHLtQEdx+uZW2tpmH4rzCXQnFsKPfuv
         trCe0RayWu3NaOTQTYWlEW0PERz8x+1Qzc2/wSYFxgrYUbV5kd475qpz8/EphyMnaTWK
         R0KA==
X-Gm-Message-State: ALoCoQnge86lj+/RgVZ1vyPMg9+0eFlVuVczUxj5ubO2S2iwIk8lTAn9mCL43QqccEUXs5eQT39p
X-Received: by 10.224.86.74 with SMTP id r10mr3889940qal.45.1422513095089;
 Wed, 28 Jan 2015 22:31:35 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Wed, 28 Jan 2015 22:31:14 -0800 (PST)
In-Reply-To: <CACdU-dRVcq9dqPTZtwRtovTy+HYo8GfrhOuWzVOOEQtagGBVMg@mail.gmail.com>
References: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
 <CACdU-dRVcq9dqPTZtwRtovTy+HYo8GfrhOuWzVOOEQtagGBVMg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 Jan 2015 22:31:14 -0800
Message-ID: <CAPh_B=YOTcZ+2RegjcroAC5BxEC4jAtR45+BvW4vXXHZqy2kDQ@mail.gmail.com>
Subject: Re: emergency jenkins restart soon
To: shane knapp <sknapp@berkeley.edu>
Cc: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ff74b8b2d8050dc4a343
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ff74b8b2d8050dc4a343
Content-Type: text/plain; charset=UTF-8

Thanks for doing that, Shane!


On Wed, Jan 28, 2015 at 10:29 PM, shane knapp <sknapp@berkeley.edu> wrote:

> jenkins is back up and all builds have been retriggered...  things are
> building and looking good, and i'll keep an eye on the spark master builds
> tonite and tomorrow.
>
> On Wed, Jan 28, 2015 at 9:56 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
> > the spark master builds stopped triggering ~yesterday and the logs don't
> > show anything.  i'm going to give the current batch of spark pull request
> > builder jobs a little more time (~30 mins) to finish, then kill whatever
> is
> > left and restart jenkins.  anything that was queued or killed will be
> > retriggered once jenkins is back up.
> >
> > sorry for the inconvenience, we'll get this sorted asap.
> >
> > thanks,
> >
> > shane
> >
>

--001a11c2ff74b8b2d8050dc4a343--

From dev-return-11350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 06:33:51 2015
Return-Path: <dev-return-11350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED86610D81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 06:33:50 +0000 (UTC)
Received: (qmail 40629 invoked by uid 500); 29 Jan 2015 06:33:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40552 invoked by uid 500); 29 Jan 2015 06:33:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40541 invoked by uid 99); 29 Jan 2015 06:33:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:33:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 06:33:25 +0000
Received: by mail-lb0-f170.google.com with SMTP id w7so24661858lbi.1
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 22:32:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6lvcWaf+M1ynYHTTgB6GGY5J+p8jIzBbsU3n3XXPyQ8=;
        b=minX/ppdbqKk462azRfOhxmsklhOhwstGtaonN31JiCY4S2ccjZuVfw7gIUWRMkVus
         ocW8xccu8B06k0rzVNXepwerSdUy0fcHfAhG2qhVrFpmt3PnYLZzGlzH8okh/vP98IN2
         Ku0WKaOmqe6t8wivApTlViZ/E8QD5tw2FfvXJMy0gg1twup08mtm67BYFwoBSY6zBMQI
         9Yt7su5MSEWys1PoGiac1rK23dG1urlU3PkZWmpk3LpopUFCIXbebmUSK64f97xPDFZn
         TTo2ntbT/cL64D9UAvnQA2lrvqUzQbar+yipxSkTa3o3sUvtR9sFOMfwRYiu5gCkWRKJ
         NR6Q==
X-Gm-Message-State: ALoCoQnm0tIP8m5FICtBub7iaKj0ADcxcN68FYy5AmYecNXCNWp+Kju9nrJKFA7Ryr5C0O9yYo5f
X-Received: by 10.152.28.37 with SMTP id y5mr12418809lag.55.1422513158923;
 Wed, 28 Jan 2015 22:32:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.185.97 with HTTP; Wed, 28 Jan 2015 22:32:18 -0800 (PST)
In-Reply-To: <CAPh_B=YOTcZ+2RegjcroAC5BxEC4jAtR45+BvW4vXXHZqy2kDQ@mail.gmail.com>
References: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
 <CACdU-dRVcq9dqPTZtwRtovTy+HYo8GfrhOuWzVOOEQtagGBVMg@mail.gmail.com> <CAPh_B=YOTcZ+2RegjcroAC5BxEC4jAtR45+BvW4vXXHZqy2kDQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 28 Jan 2015 22:32:18 -0800
Message-ID: <CACdU-dRCJBHxwWt7-0qhufTjtmW-YyhpbD1OiHhU+8VLmrYxJw@mail.gmail.com>
Subject: Re: emergency jenkins restart soon
To: Reynold Xin <rxin@databricks.com>
Cc: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158ca0286b97e050dc4a72b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158ca0286b97e050dc4a72b
Content-Type: text/plain; charset=UTF-8

np!  the master builds haven't triggered yet, but let's give the rube
goldberg machine a minute to get it's bearings.

On Wed, Jan 28, 2015 at 10:31 PM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for doing that, Shane!
>
>
> On Wed, Jan 28, 2015 at 10:29 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> jenkins is back up and all builds have been retriggered...  things are
>> building and looking good, and i'll keep an eye on the spark master builds
>> tonite and tomorrow.
>>
>> On Wed, Jan 28, 2015 at 9:56 PM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>> > the spark master builds stopped triggering ~yesterday and the logs don't
>> > show anything.  i'm going to give the current batch of spark pull
>> request
>> > builder jobs a little more time (~30 mins) to finish, then kill
>> whatever is
>> > left and restart jenkins.  anything that was queued or killed will be
>> > retriggered once jenkins is back up.
>> >
>> > sorry for the inconvenience, we'll get this sorted asap.
>> >
>> > thanks,
>> >
>> > shane
>> >
>>
>
>

--089e0158ca0286b97e050dc4a72b--

From dev-return-11351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 07:20:15 2015
Return-Path: <dev-return-11351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3002C10F09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 07:20:15 +0000 (UTC)
Received: (qmail 22133 invoked by uid 500); 29 Jan 2015 07:20:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22059 invoked by uid 500); 29 Jan 2015 07:20:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22035 invoked by uid 99); 29 Jan 2015 07:20:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 07:20:13 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 07:19:49 +0000
Received: by mail-qg0-f47.google.com with SMTP id z60so24692812qgd.6
        for <dev@spark.apache.org>; Wed, 28 Jan 2015 23:18:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type
         :content-transfer-encoding;
        bh=Li03u8VQbOzTxX/9pXcSsH8cS6wldGUyBQpMHyBntqA=;
        b=b4CL8xuHL1dBLlyKAHn2TtUhQa6moUEfZHrp2osdVKP07/+n4S3Dq+zKc43FnaJ2zJ
         JWcuNITBzmYRkulhfiJaeJlFhTFlSvANj1ju75iEp1cyBDiXhjlmcWycXa8bLVJLmKYN
         i1pZxADV9StkXYQhy9W3FvwLuy/uamBkDbZUcxH28ZU8LHQHOZlCbMf3hb+b8dnYgOJm
         0BAV7Sdd5JZAK3dl8nmX8E1zZxvOhhEa0H5kENpS0YuI2MdCHXBWjK7R53eyJg6TYGxY
         1K9dAbCUdEqVeqbUwgZYgsHqkELPWl4IuybHuTPlcWliYtC+Uz9eUexa7xQXG/n/i/op
         wBZQ==
X-Gm-Message-State: ALoCoQnyt0Y1PGo7qU41qX3A/8bTUWpKBgA7NZMnvxNDPO+udg5O116VtaKlCMEEXs/rzh2lpG+X
MIME-Version: 1.0
X-Received: by 10.224.37.138 with SMTP id x10mr22039664qad.4.1422515922918;
 Wed, 28 Jan 2015 23:18:42 -0800 (PST)
Received: by 10.229.2.136 with HTTP; Wed, 28 Jan 2015 23:18:42 -0800 (PST)
In-Reply-To: <56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
References: <1421340401649-10127.post@n3.nabble.com>
	<56A0D9D8-2F61-4640-91F4-3251081A5DA2@xense.co.uk>
Date: Wed, 28 Jan 2015 23:18:42 -0800
Message-ID: <CAEYYnxYQRCY0qQQ-hHdjaBR8XMikSmTwFx+ssSCbmhVDfedgXA@mail.gmail.com>
Subject: Re: LinearRegressionWithSGD accuracy
From: DB Tsai <dbtsai@dbtsai.com>
To: Robin East <robin.east@xense.co.uk>
Cc: "devl.development" <devl.development@gmail.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Robin,

You can try this PR out. This has built-in features scaling, and has
ElasticNet regularization (L1/L2 mix). This implementation can stably
converge to model from R's glmnet package.

https://github.com/apache/spark/pull/4259

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



On Thu, Jan 15, 2015 at 9:42 AM, Robin East <robin.east@xense.co.uk> wrote:
> -dev, +user
>
> You=E2=80=99ll need to set the gradient descent step size to something sm=
all - a bit of trial and error shows that 0.00000001 works.
>
> You=E2=80=99ll need to create a LinearRegressionWithSGD instance and set =
the step size explicitly:
>
> val lr =3D new LinearRegressionWithSGD()
> lr.optimizer.setStepSize(0.00000001)
> lr.optimizer.setNumIterations(100)
> val model =3D lr.run(parsedData)
>
> On 15 Jan 2015, at 16:46, devl.development <devl.development@gmail.com> w=
rote:
>
>> From what I gather, you use LinearRegressionWithSGD to predict y or the
>> response variable given a feature vector x.
>>
>> In a simple example I used a perfectly linear dataset such that x=3Dy
>> y,x
>> 1,1
>> 2,2
>> ...
>>
>> 10000,10000
>>
>> Using the out-of-box example from the website (with and without scaling)=
:
>>
>> val data =3D sc.textFile(file)
>>
>>    val parsedData =3D data.map { line =3D>
>>      val parts =3D line.split(',')
>>     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) //=
y
>> and x
>>
>>    }
>>    val scaler =3D new StandardScaler(withMean =3D true, withStd =3D true=
)
>>      .fit(parsedData.map(x =3D> x.features))
>>    val scaledData =3D parsedData
>>      .map(x =3D>
>>      LabeledPoint(x.label,
>>        scaler.transform(Vectors.dense(x.features.toArray))))
>>
>>    // Building the model
>>    val numIterations =3D 100
>>    val model =3D LinearRegressionWithSGD.train(parsedData, numIterations=
)
>>
>>    // Evaluate model on training examples and compute training error *
>> tried using both scaledData and parsedData
>>    val valuesAndPreds =3D scaledData.map { point =3D>
>>      val prediction =3D model.predict(point.features)
>>      (point.label, prediction)
>>    }
>>    val MSE =3D valuesAndPreds.map{case(v, p) =3D> math.pow((v - p), 2)}.=
mean()
>>    println("training Mean Squared Error =3D " + MSE)
>>
>> Both scaled and unscaled attempts give:
>>
>> training Mean Squared Error =3D NaN
>>
>> I've even tried x, y+(sample noise from normal with mean 0 and stddev 1)
>> still comes up with the same thing.
>>
>> Is this not supposed to work for x and y or 2 dimensional plots? Is ther=
e
>> something I'm missing or wrong in the code above? Or is there a limitati=
on
>> in the method?
>>
>> Thanks for any advice.
>>
>>
>>
>> --
>> View this message in context: http://apache-spark-developers-list.100155=
1.n3.nabble.com/LinearRegressionWithSGD-accuracy-tp10127.html
>> Sent from the Apache Spark Developers List mailing list archive at Nabbl=
e.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 15:38:01 2015
Return-Path: <dev-return-11352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 67A5817368
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 15:38:01 +0000 (UTC)
Received: (qmail 31411 invoked by uid 500); 29 Jan 2015 15:38:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31331 invoked by uid 500); 29 Jan 2015 15:38:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31319 invoked by uid 99); 29 Jan 2015 15:37:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 15:37:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of aniket.bhatnagar@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 15:37:55 +0000
Received: by mail-qg0-f53.google.com with SMTP id a108so29685815qge.12
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 07:37:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=dmEB1Hcmxxtl27bAFlYHJC8c8vl7SNpsuo9TDzkg5Ig=;
        b=zpnvgHaki6CmL5u7NnTqSgzFPfleThG9u8dbyfDLOGnQK354XX8DDO7sUFwrhtRkhC
         oY/135ic955Aq6DDiXID43tE0L3NNp2RyUnTPewcr0EhwwRvn5/x6bl1MuTuKhgJ3St+
         6PnKxfCPT8eDepiC0Hl0mW1C/d2LOmy6IHTH9R6R9XdaaAwk4kdnNLGw99/LOB1/BQGz
         K4+pZh6hn8eCGUYnscfse38rN2KJDzvQclaxhCz1k3trS+RwqELvWUyfRKjE+sAMTwOO
         oiyq9XyQtbgxo8NyfUjnfb2rVH39FAKOw49B+3OoOwWI1u4AgjXTXF3nFDvPvQlvVYQp
         oEiA==
X-Received: by 10.140.20.83 with SMTP id 77mr2166618qgi.40.1422545854712; Thu,
 29 Jan 2015 07:37:34 -0800 (PST)
MIME-Version: 1.0
References: <CAJOb8bvJ1OiZBKQo6qHmdQ7kY2HcvF_=GPjrOGFsN_gyxH7tvw@mail.gmail.com>
 <54C937D0.1020000@gmail.com> <CAPh_B=Z7VjBrcEaauZ0kgfGd5kLUME5=uBV2WSa_+mVKnR1+pg@mail.gmail.com>
From: Aniket Bhatnagar <aniket.bhatnagar@gmail.com>
Date: Thu, 29 Jan 2015 15:37:34 +0000
Message-ID: <CAJOb8btmPsj9nhb4Ac7kpTwV4xn_vOqLD6phj+B3ey20p_gj8Q@mail.gmail.com>
Subject: Re: Data source API | Support for dynamic schema
To: Reynold Xin <rxin@databricks.com>, Cheng Lian <lian.cs.zju@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c12bca58d3c0050dcc4482
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12bca58d3c0050dcc4482
Content-Type: text/plain; charset=UTF-8

Thanks Reynold and Cheng. It does seem quiet a bit of heavy lifting to have
schema per row. I will for now settle with having to do a union schema of
all the schema versions and complain any incompatibilities :-)

Looking forward to do great things with the API!

Thanks,
Aniket

On Thu Jan 29 2015 at 01:09:15 Reynold Xin <rxin@databricks.com> wrote:

> It's an interesting idea, but there are major challenges with per row
> schema.
>
> 1. Performance - query optimizer and execution use assumptions about
> schema and data to generate optimized query plans. Having to re-reason
> about schema for each row can substantially slow down the engine, but due
> to optimization and due to the overhead of schema information associated
> with each row.
>
> 2. Data model: per-row schema is fundamentally a different data model. The
> current relational model has gone through 40 years of research and have
> very well defined semantics. I don't think there are well defined semantics
> of a per-row schema data model. For example, what is the semantics of an
> UDF function that is operating on a data cell that has incompatible schema?
> Should we also coerce or convert the data type? If yes, will that lead to
> conflicting semantics with some other rules? We need to answer questions
> like this in order to have a robust data model.
>
>
>
>
>
> On Wed, Jan 28, 2015 at 11:26 AM, Cheng Lian <lian.cs.zju@gmail.com>
> wrote:
>
>> Hi Aniket,
>>
>> In general the schema of all rows in a single table must be same. This is
>> a basic assumption made by Spark SQL. Schema union does make sense, and
>> we're planning to support this for Parquet. But as you've mentioned, it
>> doesn't help if types of different versions of a column differ from each
>> other. Also, you need to reload the data source table after schema changes
>> happen.
>>
>> Cheng
>>
>>
>> On 1/28/15 2:12 AM, Aniket Bhatnagar wrote:
>>
>>> I saw the talk on Spark data sources and looking at the interfaces, it
>>> seems that the schema needs to be provided upfront. This works for many
>>> data sources but I have a situation in which I would need to integrate a
>>> system that supports schema evolutions by allowing users to change schema
>>> without affecting existing rows. Basically, each row contains a schema
>>> hint
>>> (id and version) and this allows developers to evolve schema over time
>>> and
>>> perform migration at will. Since the schema needs to be specified upfront
>>> in the data source API, one possible way would be to build a union of all
>>> schema versions and handle populating row values appropriately. This
>>> works
>>> in case columns have been added or deleted in the schema but doesn't work
>>> if types have changed. I was wondering if it is possible to change the
>>> API
>>>   to provide schema for each row instead of expecting data source to
>>> provide
>>> schema upfront?
>>>
>>> Thanks,
>>> Aniket
>>>
>>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a11c12bca58d3c0050dcc4482--

From dev-return-11353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 16:35:54 2015
Return-Path: <dev-return-11353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F3444175DC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 16:35:53 +0000 (UTC)
Received: (qmail 50723 invoked by uid 500); 29 Jan 2015 16:35:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50653 invoked by uid 500); 29 Jan 2015 16:35:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50642 invoked by uid 99); 29 Jan 2015 16:35:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 16:35:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 16:35:28 +0000
Received: by mail-lb0-f177.google.com with SMTP id p9so30028834lbv.8
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 08:33:11 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=AMsxjR9sL1MmDzkdB/hicWmNlE8SI5FB76ZVpbQHLqA=;
        b=ffqrsAXqSBZN/v7i2Ux8bRp5Be+oUC79Wd3gU7GzK0jLqNx7tZPtKt0UpprHeQqr5n
         BnOxmjyWATSqIipAlIUiiTnDy6MTbiMzNkwScHEFD8f6ECyLmCz4G2h4yn1BoU3hRnr3
         lWhHZqroKMF7Zq1C/bw5oG6XyPSp+k6CL170mEjnUYFYHiD0Q/ORsMYXqznH5sSgr2LR
         Pq+ozFXCi4XIby036KxxigdCN2CTx1Wn9qnmfCOxsMzeszAtesUAzuacY/1NnzRJqxyD
         ymb2fJX8fnjwi81aD+Jl6gqkwaYiEgPy4gNJJljLIYROiuf6OOx2aX0OfsDc/zW2c/jk
         ioMw==
X-Gm-Message-State: ALoCoQmKNV1ljLTgBSr7YCoN/cGFUeUBbny+ngd27hSP2I21la3jGrfbUHQ6VotUp/vzsA1vRnP4
X-Received: by 10.112.12.134 with SMTP id y6mr1975416lbb.34.1422549191080;
 Thu, 29 Jan 2015 08:33:11 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.185.97 with HTTP; Thu, 29 Jan 2015 08:32:50 -0800 (PST)
In-Reply-To: <CACdU-dRCJBHxwWt7-0qhufTjtmW-YyhpbD1OiHhU+8VLmrYxJw@mail.gmail.com>
References: <CACdU-dQ0eUQayW+WLeJAUmt8fzfQPja9P3EDNk==nQBqXnNfRA@mail.gmail.com>
 <CACdU-dRVcq9dqPTZtwRtovTy+HYo8GfrhOuWzVOOEQtagGBVMg@mail.gmail.com>
 <CAPh_B=YOTcZ+2RegjcroAC5BxEC4jAtR45+BvW4vXXHZqy2kDQ@mail.gmail.com> <CACdU-dRCJBHxwWt7-0qhufTjtmW-YyhpbD1OiHhU+8VLmrYxJw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 29 Jan 2015 08:32:50 -0800
Message-ID: <CACdU-dToC2qiu5nZqvhwQ9166E5DbK7K8+CkKLU7o0CeGkqd4g@mail.gmail.com>
Subject: Re: emergency jenkins restart soon
To: Reynold Xin <rxin@databricks.com>
Cc: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3a0e835d5e6050dcd0b90
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3a0e835d5e6050dcd0b90
Content-Type: text/plain; charset=UTF-8

the master builds triggered around ~1am last night (according to the logs),
so it looks like we're back in business.

On Wed, Jan 28, 2015 at 10:32 PM, shane knapp <sknapp@berkeley.edu> wrote:

> np!  the master builds haven't triggered yet, but let's give the rube
> goldberg machine a minute to get it's bearings.
>
> On Wed, Jan 28, 2015 at 10:31 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Thanks for doing that, Shane!
>>
>>
>> On Wed, Jan 28, 2015 at 10:29 PM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>>
>>> jenkins is back up and all builds have been retriggered...  things are
>>> building and looking good, and i'll keep an eye on the spark master
>>> builds
>>> tonite and tomorrow.
>>>
>>> On Wed, Jan 28, 2015 at 9:56 PM, shane knapp <sknapp@berkeley.edu>
>>> wrote:
>>>
>>> > the spark master builds stopped triggering ~yesterday and the logs
>>> don't
>>> > show anything.  i'm going to give the current batch of spark pull
>>> request
>>> > builder jobs a little more time (~30 mins) to finish, then kill
>>> whatever is
>>> > left and restart jenkins.  anything that was queued or killed will be
>>> > retriggered once jenkins is back up.
>>> >
>>> > sorry for the inconvenience, we'll get this sorted asap.
>>> >
>>> > thanks,
>>> >
>>> > shane
>>> >
>>>
>>
>>
>

--001a11c3a0e835d5e6050dcd0b90--

From dev-return-11354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 17:26:37 2015
Return-Path: <dev-return-11354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5970E1778B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 17:26:37 +0000 (UTC)
Received: (qmail 87530 invoked by uid 500); 29 Jan 2015 17:26:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87459 invoked by uid 500); 29 Jan 2015 17:26:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87089 invoked by uid 99); 29 Jan 2015 17:26:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 17:26:35 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS,TVD_FW_GRAPHIC_NAME_MID
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rcsenkbe@us.ibm.com designates 32.97.110.150 as permitted sender)
Received: from [32.97.110.150] (HELO e32.co.us.ibm.com) (32.97.110.150)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 17:26:29 +0000
Received: from /spool/local
	by e32.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <rcsenkbe@us.ibm.com>;
	Thu, 29 Jan 2015 10:26:08 -0700
Received: from d03dlp01.boulder.ibm.com (9.17.202.177)
	by e32.co.us.ibm.com (192.168.1.132) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Thu, 29 Jan 2015 10:26:05 -0700
Received: from b03cxnp07029.gho.boulder.ibm.com (b03cxnp07029.gho.boulder.ibm.com [9.17.130.16])
	by d03dlp01.boulder.ibm.com (Postfix) with ESMTP id 173091FF0040
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 10:17:17 -0700 (MST)
Received: from d03av03.boulder.ibm.com (d03av03.boulder.ibm.com [9.17.195.169])
	by b03cxnp07029.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id t0THPueJ34996466
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 10:26:05 -0700
Received: from d03av03.boulder.ibm.com (localhost [127.0.0.1])
	by d03av03.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id t0THPWQG031340
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 10:25:32 -0700
Received: from d03nm127.boulder.ibm.com (d03nm127.boulder.ibm.com [9.63.33.48])
	by d03av03.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id t0THPWmU030324
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 10:25:32 -0700
In-Reply-To: <CAOTBr2nJDtxGn__i2V_ugvKfEDp=EN8=4YUi0aK3gtk5BW4w-w@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>	<CAMAsSd+EShmsiVec3AvxbhHqm4uvyQMiVd7JFQ866HEYE9adSw@mail.gmail.com> <CAOTBr2nJDtxGn__i2V_ugvKfEDp=EN8=4YUi0aK3gtk5BW4w-w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
X-KeepSent: 220160F5:4806E851-87257DDC:005F0354;
 type=4; name=$KeepSent
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: IBM Notes Release 9.0.1 October 14, 2013
Message-ID: <OF220160F5.4806E851-ON87257DDC.005F0354-86257DDC.005FAED8@us.ibm.com>
From: Robert C Senkbeil <rcsenkbe@us.ibm.com>
Date: Thu, 29 Jan 2015 11:25:03 -0600
X-MIMETrack: Serialize by Router on D03NM127/03/M/IBM(Release 9.0.1FP1|April  03, 2014) at
 01/29/2015 10:25:29
MIME-Version: 1.0
Content-type: multipart/related; 
	Boundary="0__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4"
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15012917-0005-0000-0000-0000086702D1
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4
Content-type: multipart/alternative; 
	Boundary="1__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4"

--1__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4
Content-type: text/plain; charset=US-ASCII
Content-transfer-encoding: quoted-printable


+1

I verified that the REPL jars published work fine with the Spark Kernel=

project (can build/test against them).

Signed,
Chip Senkbeil



From:	Krishna Sankar <ksankar42@gmail.com>
To:	Sean Owen <sowen@cloudera.com>
Cc:	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org"
            <dev@spark.apache.org>
Date:	01/28/2015 02:52 PM
Subject:	Re: [VOTE] Release Apache Spark 1.2.1 (RC2)



+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min
     mvn clean package -Pyarn -Dyarn.version=3D2.6.0 -Phadoop-2.4
-Dhadoop.version=3D2.6.0
-Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.1.x=
 &
1.2.0
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       Fixed : org.apache.spark.SparkException in zip !
2.5. rdd operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word coun=
t)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lmbda) with iterto=
ols
OK

Cheers
<k/>

On Wed, Jan 28, 2015 at 5:17 AM, Sean Owen <sowen@cloudera.com> wrote:

> +1 (nonbinding). I verified that all the hash / signing items I
> mentioned before are resolved.
>
> The source package compiles on Ubuntu / Java 8. I ran tests and the
> passed. Well, actually I see the same failure I've seeing locally on
> OS X and on Ubuntu for a while, but I think nobody else has seen this=
?
>
> MQTTStreamSuite:
> - mqtt input stream *** FAILED ***
>   org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in=

> progress
>   at
> org.eclipse.paho.client.mqttv3.internal.ClientState.send
(ClientState.java:423)
>
> Doesn't happen on Jenkins. If nobody else is seeing this, I suspect i=
t
> is something perhaps related to my env that I haven't figured out yet=
,
> so should not be considered a blocker.
>
> On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell <pwendell@gmail.com=
>
> wrote:
> > Please vote on releasing the following candidate as Apache Spark
version
> 1.2.1!
> >
> > The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> >
>
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Db7=
7f87673d1f9f03d4c83cf583158227c551359b

> >
> > The release files, including signatures, digests, etc. can be found=
 at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1=
062/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
> >
> > Changes from rc1:
> > This has no code changes from RC1. Only minor changes to the releas=
e
> script.
> >
> > Please vote on releasing this package as Apache Spark 1.2.1!
> >
> > The vote is open until  Saturday, January 31, at 10:04 UTC and pass=
es
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.1
> > [ ] -1 Do not release this package because ...
> >
> > For a list of fixes in this release, see http://s.apache.org/Mpn.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > -------------------------------------------------------------------=
--
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------=

> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
=

--1__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4
Content-type: text/html; charset=US-ASCII
Content-Disposition: inline
Content-transfer-encoding: quoted-printable

<html><body>
<p><font size=3D"2" face=3D"sans-serif">+1</font><br>
<br>
<font size=3D"2" face=3D"sans-serif">I verified that the REPL jars publ=
ished work fine with the Spark Kernel project (can build/test against t=
hem).</font><br>
<br>
<font size=3D"2" face=3D"sans-serif">Signed,</font><br>
<font size=3D"2" face=3D"sans-serif">Chip Senkbeil</font><br>
<br>
<img width=3D"16" height=3D"16" src=3D"cid:1__=3D08BBF74FDFCC85C48f9e8a=
93df938@us.ibm.com" border=3D"0" alt=3D"Inactive hide details for Krish=
na Sankar ---01/28/2015 02:52:14 PM---+1 (non-binding, of course) 1. Co=
mpiled OSX 10.10 (Yosemit"><font size=3D"2" color=3D"#424282" face=3D"s=
ans-serif">Krishna Sankar ---01/28/2015 02:52:14 PM---+1 (non-binding, =
of course) 1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min</f=
ont><br>
<br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">From:	</font><fo=
nt size=3D"1" face=3D"sans-serif">Krishna Sankar &lt;ksankar42@gmail.co=
m&gt;</font><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">To:	</font><font=
 size=3D"1" face=3D"sans-serif">Sean Owen &lt;sowen@cloudera.com&gt;</f=
ont><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">Cc:	</font><font=
 size=3D"1" face=3D"sans-serif">Patrick Wendell &lt;pwendell@gmail.com&=
gt;, &quot;dev@spark.apache.org&quot; &lt;dev@spark.apache.org&gt;</fon=
t><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">Date:	</font><fo=
nt size=3D"1" face=3D"sans-serif">01/28/2015 02:52 PM</font><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">Subject:	</font>=
<font size=3D"1" face=3D"sans-serif">Re: [VOTE] Release Apache Spark 1.=
2.1 (RC2)</font><br>
<hr width=3D"100%" size=3D"2" align=3D"left" noshade style=3D"color:#80=
91A5; "><br>
<br>
<br>
<tt><font size=3D"2">+1 (non-binding, of course)<br>
<br>
1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min<br>
 &nbsp; &nbsp; mvn clean package -Pyarn -Dyarn.version=3D2.6.0 -Phadoop=
-2.4<br>
-Dhadoop.version=3D2.6.0<br>
-Phive -DskipTests<br>
2. Tested pyspark, mlib - running as well as compare results with 1.1.x=
 &amp;<br>
1.2.0<br>
2.1. statistics (min,max,mean,Pearson,Spearman) OK<br>
2.2. Linear/Ridge/Laso Regression OK<br>
2.3. Decision Tree, Naive Bayes OK<br>
2.4. KMeans OK<br>
 &nbsp; &nbsp; &nbsp; Center And Scale OK<br>
 &nbsp; &nbsp; &nbsp; Fixed : org.apache.spark.SparkException in zip !<=
br>
2.5. rdd operations OK<br>
 &nbsp; &nbsp; &nbsp;State of the Union Texts - MapReduce, Filter,sortB=
yKey (word count)<br>
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK<br>
 &nbsp; &nbsp; &nbsp; Model evaluation/optimization (rank, numIter, lmb=
da) with itertools<br>
OK<br>
<br>
Cheers<br>
&lt;k/&gt;<br>
<br>
On Wed, Jan 28, 2015 at 5:17 AM, Sean Owen &lt;sowen@cloudera.com&gt; w=
rote:<br>
<br>
&gt; +1 (nonbinding). I verified that all the hash / signing items I<br=
>
&gt; mentioned before are resolved.<br>
&gt;<br>
&gt; The source package compiles on Ubuntu / Java 8. I ran tests and th=
e<br>
&gt; passed. Well, actually I see the same failure I've seeing locally =
on<br>
&gt; OS X and on Ubuntu for a while, but I think nobody else has seen t=
his?<br>
&gt;<br>
&gt; MQTTStreamSuite:<br>
&gt; - mqtt input stream *** FAILED ***<br>
&gt; &nbsp; org.eclipse.paho.client.mqttv3.MqttException: Too many publ=
ishes in<br>
&gt; progress<br>
&gt; &nbsp; at<br>
&gt; org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientSta=
te.java:423)<br>
&gt;<br>
&gt; Doesn't happen on Jenkins. If nobody else is seeing this, I suspec=
t it<br>
&gt; is something perhaps related to my env that I haven't figured out =
yet,<br>
&gt; so should not be considered a blocker.<br>
&gt;<br>
&gt; On Wed, Jan 28, 2015 at 10:06 AM, Patrick Wendell &lt;pwendell@gma=
il.com&gt;<br>
&gt; wrote:<br>
&gt; &gt; Please vote on releasing the following candidate as Apache Sp=
ark version<br>
&gt; 1.2.1!<br>
&gt; &gt;<br>
&gt; &gt; The tag to be voted on is v1.2.1-rc1 (commit b77f876):<br>
&gt; &gt;<br>
&gt; </font></tt><tt><font size=3D"2"><a href=3D"https://git-wip-us.apa=
che.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Db77f87673d1f9f03d4c83cf5=
83158227c551359b">https://git-wip-us.apache.org/repos/asf?p=3Dspark.git=
;a=3Dcommit;h=3Db77f87673d1f9f03d4c83cf583158227c551359b</a></font></tt=
><tt><font size=3D"2"><br>
&gt; &gt;<br>
&gt; &gt; The release files, including signatures, digests, etc. can be=
 found at:<br>
&gt; &gt; </font></tt><tt><font size=3D"2"><a href=3D"http://people.apa=
che.org/~pwendell/spark-1.2.1-rc2/">http://people.apache.org/~pwendell/=
spark-1.2.1-rc2/</a></font></tt><tt><font size=3D"2"><br>
&gt; &gt;<br>
&gt; &gt; Release artifacts are signed with the following key:<br>
&gt; &gt; </font></tt><tt><font size=3D"2"><a href=3D"https://people.ap=
ache.org/keys/committer/pwendell.asc">https://people.apache.org/keys/co=
mmitter/pwendell.asc</a></font></tt><tt><font size=3D"2"><br>
&gt; &gt;<br>
&gt; &gt; The staging repository for this release can be found at:<br>
&gt; &gt; </font></tt><tt><font size=3D"2"><a href=3D"https://repositor=
y.apache.org/content/repositories/orgapachespark-1062/">https://reposit=
ory.apache.org/content/repositories/orgapachespark-1062/</a></font></tt=
><tt><font size=3D"2"><br>
&gt; &gt;<br>
&gt; &gt; The documentation corresponding to this release can be found =
at:<br>
&gt; &gt; </font></tt><tt><font size=3D"2"><a href=3D"http://people.apa=
che.org/~pwendell/spark-1.2.1-rc2-docs/">http://people.apache.org/~pwen=
dell/spark-1.2.1-rc2-docs/</a></font></tt><tt><font size=3D"2"><br>
&gt; &gt;<br>
&gt; &gt; Changes from rc1:<br>
&gt; &gt; This has no code changes from RC1. Only minor changes to the =
release<br>
&gt; script.<br>
&gt; &gt;<br>
&gt; &gt; Please vote on releasing this package as Apache Spark 1.2.1!<=
br>
&gt; &gt;<br>
&gt; &gt; The vote is open until &nbsp;Saturday, January 31, at 10:04 U=
TC and passes<br>
&gt; &gt; if a majority of at least 3 +1 PMC votes are cast.<br>
&gt; &gt;<br>
&gt; &gt; [ ] +1 Release this package as Apache Spark 1.2.1<br>
&gt; &gt; [ ] -1 Do not release this package because ...<br>
&gt; &gt;<br>
&gt; &gt; For a list of fixes in this release, see </font></tt><tt><fon=
t size=3D"2"><a href=3D"http://s.apache.org/Mpn">http://s.apache.org/Mp=
n</a></font></tt><tt><font size=3D"2">.<br>
&gt; &gt;<br>
&gt; &gt; To learn more about Apache Spark, please see<br>
&gt; &gt; </font></tt><tt><font size=3D"2"><a href=3D"http://spark.apac=
he.org/">http://spark.apache.org/</a></font></tt><tt><font size=3D"2"><=
br>
&gt; &gt;<br>
&gt; &gt; -------------------------------------------------------------=
--------<br>
&gt; &gt; To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<br>
&gt; &gt; For additional commands, e-mail: dev-help@spark.apache.org<br=
>
&gt; &gt;<br>
&gt;<br>
&gt; ------------------------------------------------------------------=
---<br>
&gt; To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<br>
&gt; For additional commands, e-mail: dev-help@spark.apache.org<br>
&gt;<br>
&gt;<br>
</font></tt><br>
</body></html>=


--1__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4--


--0__=08BBF74FDFCC85C48f9e8a93df938690918c08BBF74FDFCC85C4--


From dev-return-11355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 19:00:28 2015
Return-Path: <dev-return-11355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC73917B0C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 19:00:28 +0000 (UTC)
Received: (qmail 64924 invoked by uid 500); 29 Jan 2015 19:00:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64853 invoked by uid 500); 29 Jan 2015 19:00:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63755 invoked by uid 99); 29 Jan 2015 19:00:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:00:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mohitjaggi@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:00:01 +0000
Received: by mail-pa0-f47.google.com with SMTP id lj1so42369079pab.6;
        Thu, 29 Jan 2015 10:58:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=+saTlYMndZSwf/i1UilG1wFeyuKgDR3hVf+WMpWXFvw=;
        b=J1uHGHsgxnXh1cfJQr61ShGX7nUhTIkBEo7lkwD1OQLzIxG/2zyry/aKsO0aUJxZaA
         m+kJemuAjzOne2TYTszA2hjyu2rBM1wup03gQYt/G1xVfqTsRE9vJ0yWnzTzWeixWJac
         c8K0EEBqHSjaeXRNFkhit6x2fM31veFD9CmWYk+JGGzsnm8iS4Ed2q3CBq04C8C11J5s
         B5THyVTRpYDqQ7GdqFQi9p/t/cgWji0f88RgGnqT0wuxPV3yEoFnsIPCp/WBPmYmXuB1
         fGajVE6luPCPNLv8ycrM+M8/B2J+ye3mfIEPCcgTDFbYiY/p5ZyKYnxBZ6TDCucC7hsw
         RB6Q==
X-Received: by 10.68.57.205 with SMTP id k13mr2744043pbq.87.1422557907795;
        Thu, 29 Jan 2015 10:58:27 -0800 (PST)
Received: from [192.168.21.194] ([66.201.45.138])
        by mx.google.com with ESMTPSA id kl11sm8438841pbd.89.2015.01.29.10.58.26
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 29 Jan 2015 10:58:26 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_309788C8-3875-40AB-B90E-3EE6388C2DB0"
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: RDD.combineBy without intermediate (k,v) pair allocation
From: Mohit Jaggi <mohitjaggi@gmail.com>
In-Reply-To: <1422395569281.bee71011@Nodemailer>
Date: Thu, 29 Jan 2015 09:52:00 -0800
Cc: user@spark.apache.org,
 dev@spark.apache.org
Message-Id: <BB4E15B0-C16B-4C38-8F63-315D16348742@gmail.com>
References: <C4508C79-8231-4C36-B492-CDFC0F0DAC3D@gmail.com> <1422395569281.bee71011@Nodemailer>
To: francois.garillot@typesafe.com
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_309788C8-3875-40AB-B90E-3EE6388C2DB0
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Francois,
RDD.aggregate() does not support aggregation by key. But, indeed, that =
is the kind of implementation I am looking for, one that does not =
allocate intermediate space for storing (K,V) pairs. When working with =
large datasets this type of intermediate memory allocation wrecks havoc =
with garbage collection, not to mention unnecessarily increases the =
working memory requirement of the program.

I wonder if someone has already noticed this and there is an effort =
underway to optimize this. If not, I will take a shot at adding this =
functionality.

Mohit.

> On Jan 27, 2015, at 1:52 PM, francois.garillot@typesafe.com wrote:
>=20
> Have you looked at the `aggregate` function in the RDD API ?=20
>=20
> If your way of extracting the =E2=80=9Ckey=E2=80=9D (identifier) and =
=E2=80=9Cvalue=E2=80=9D (payload) parts of the RDD elements is uniform =
(a function), it=E2=80=99s unclear to me how this would be more =
efficient that extracting key and value and then using combine, however.
>=20
> =E2=80=94
> FG
>=20
>=20
> On Tue, Jan 27, 2015 at 10:17 PM, Mohit Jaggi <mohitjaggi@gmail.com =
<mailto:mohitjaggi@gmail.com>> wrote:
>=20
> Hi All,=20
> I have a use case where I have an RDD (not a k,v pair) where I want to =
do a combineByKey() operation. I can do that by creating an intermediate =
RDD of k,v pairs and using PairRDDFunctions.combineByKey(). However, I =
believe it will be more efficient if I can avoid this intermediate RDD. =
Is there a way I can do this by passing in a function that extracts the =
key, like in RDD.groupBy()? [oops, RDD.groupBy seems to create the =
intermediate RDD anyway, maybe a better implementation is possible for =
that too?]=20
> If not, is it worth adding to the Spark API?=20
>=20
> Mohit.=20
> ---------------------------------------------------------------------=20=

> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org=20
> For additional commands, e-mail: user-help@spark.apache.org=20
>=20
>=20
>=20


--Apple-Mail=_309788C8-3875-40AB-B90E-3EE6388C2DB0--

From dev-return-11356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 19:33:21 2015
Return-Path: <dev-return-11356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEB7517CA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 19:33:21 +0000 (UTC)
Received: (qmail 69484 invoked by uid 500); 29 Jan 2015 19:33:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69414 invoked by uid 500); 29 Jan 2015 19:33:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68853 invoked by uid 99); 29 Jan 2015 19:33:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:33:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of velvia.github@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:32:54 +0000
Received: by mail-wi0-f173.google.com with SMTP id r20so31103763wiv.0
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 11:32:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=fNUeHVq6gt0Rck5h298mWdlM45pdZzbkuoygrI2g1hc=;
        b=vcft65QTOKpby/dV4k8CFgY8E1encqj4nZoZmedOoNJmZfwg1lVGaTb2cZsBKSIV93
         aw+y7Hu9RbmmzIaot92gyAFeWUIQ+Vj4M7LZvmPJFuGh14DMTXdni9fYn/xpo52F2u89
         VfzVPo5EJr99xjsoa0ELuKC3kbf+Ao3PXUEpkz6qNs4/4DDMy4ltpU6fi68s2f0eX6wI
         fnZD0zplRx+O3ALEEBBrCW+0UhHq7CoCQ2aAtBFnZDjKnHOB1m5L08UEt6lAPzi1Vh99
         3AIQyitHK23wmqTcVrkP5mAqtrnJZsYnMEifTtMEl9OIcJwWIJaKtG1gUIpRmXFJMDsC
         hf3A==
MIME-Version: 1.0
X-Received: by 10.180.73.241 with SMTP id o17mr4427917wiv.16.1422559973408;
 Thu, 29 Jan 2015 11:32:53 -0800 (PST)
Received: by 10.217.38.20 with HTTP; Thu, 29 Jan 2015 11:32:53 -0800 (PST)
In-Reply-To: <CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
	<CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
	<CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
	<CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>
Date: Thu, 29 Jan 2015 11:32:53 -0800
Message-ID: <CAN6Vra1Pr9ouShTMzp8fmDfaK_E51=gAkxhnGhdC1GpJDYO54w@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Evan Chan <velvia.github@gmail.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1.... having proper NA support is much cleaner than using null, at
least the Java null.

On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks <evan.sparks@gmail.com> wrote:
> You've got to be a little bit careful here. "NA" in systems like R or pandas
> may have special meaning that is distinct from "null".
>
> See, e.g. http://www.r-bloggers.com/r-na-vs-null/
>
>
>
> On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> Isn't that just "null" in SQL?
>>
>> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
>> wrote:
>>
>> > I believe that most DataFrame implementations out there, like Pandas,
>> > supports the idea of missing values / NA, and some support the idea of
>> > Not Meaningful as well.
>> >
>> > Does Row support anything like that?  That is important for certain
>> > applications.  I thought that Row worked by being a mutable object,
>> > but haven't looked into the details in a while.
>> >
>> > -Evan
>> >
>> > On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
>> > wrote:
>> > > It shouldn't change the data source api at all because data sources
>> > create
>> > > RDD[Row], and that gets converted into a DataFrame automatically
>> > (previously
>> > > to SchemaRDD).
>> > >
>> > >
>> >
>> > https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
>> > >
>> > > One thing that will break the data source API in 1.3 is the location
>> > > of
>> > > types. Types were previously defined in sql.catalyst.types, and now
>> > moved to
>> > > sql.types. After 1.3, sql.catalyst is hidden from users, and all
>> > > public
>> > APIs
>> > > have first class classes/objects defined in sql directly.
>> > >
>> > >
>> > >
>> > > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com>
>> > wrote:
>> > >>
>> > >> Hey guys,
>> > >>
>> > >> How does this impact the data sources API?  I was planning on using
>> > >> this for a project.
>> > >>
>> > >> +1 that many things from spark-sql / DataFrame is universally
>> > >> desirable and useful.
>> > >>
>> > >> By the way, one thing that prevents the columnar compression stuff in
>> > >> Spark SQL from being more useful is, at least from previous talks
>> > >> with
>> > >> Reynold and Michael et al., that the format was not designed for
>> > >> persistence.
>> > >>
>> > >> I have a new project that aims to change that.  It is a
>> > >> zero-serialisation, high performance binary vector library, designed
>> > >> from the outset to be a persistent storage friendly.  May be one day
>> > >> it can replace the Spark SQL columnar compression.
>> > >>
>> > >> Michael told me this would be a lot of work, and recreates parts of
>> > >> Parquet, but I think it's worth it.  LMK if you'd like more details.
>> > >>
>> > >> -Evan
>> > >>
>> > >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
>> > wrote:
>> > >> > Alright I have merged the patch (
>> > >> > https://github.com/apache/spark/pull/4173
>> > >> > ) since I don't see any strong opinions against it (as a matter of
>> > fact
>> > >> > most were for it). We can still change it if somebody lays out a
>> > strong
>> > >> > argument.
>> > >> >
>> > >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>> > >> > <matei.zaharia@gmail.com>
>> > >> > wrote:
>> > >> >
>> > >> >> The type alias means your methods can specify either type and they
>> > will
>> > >> >> work. It's just another name for the same type. But Scaladocs and
>> > such
>> > >> >> will
>> > >> >> show DataFrame as the type.
>> > >> >>
>> > >> >> Matei
>> > >> >>
>> > >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>> > >> >> dirceu.semighini@gmail.com> wrote:
>> > >> >> >
>> > >> >> > Reynold,
>> > >> >> > But with type alias we will have the same problem, right?
>> > >> >> > If the methods doesn't receive schemardd anymore, we will have
>> > >> >> > to
>> > >> >> > change
>> > >> >> > our code to migrade from schema to dataframe. Unless we have an
>> > >> >> > implicit
>> > >> >> > conversion between DataFrame and SchemaRDD
>> > >> >> >
>> > >> >> >
>> > >> >> >
>> > >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>> > >> >> >
>> > >> >> >> Dirceu,
>> > >> >> >>
>> > >> >> >> That is not possible because one cannot overload return types.
>> > >> >> >>
>> > >> >> >> SQLContext.parquetFile (and many other methods) needs to return
>> > some
>> > >> >> type,
>> > >> >> >> and that type cannot be both SchemaRDD and DataFrame.
>> > >> >> >>
>> > >> >> >> In 1.3, we will create a type alias for DataFrame called
>> > >> >> >> SchemaRDD
>> > >> >> >> to
>> > >> >> not
>> > >> >> >> break source compatibility for Scala.
>> > >> >> >>
>> > >> >> >>
>> > >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>> > >> >> >> dirceu.semighini@gmail.com> wrote:
>> > >> >> >>
>> > >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
>> > removed
>> > >> >> >>> in
>> > >> >> the
>> > >> >> >>> release 1.5(+/- 1)  for example, and the new code been added
>> > >> >> >>> to
>> > >> >> DataFrame?
>> > >> >> >>> With this, we don't impact in existing code for the next few
>> > >> >> >>> releases.
>> > >> >> >>>
>> > >> >> >>>
>> > >> >> >>>
>> > >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta
>> > >> >> >>> <kushal.datta@gmail.com>:
>> > >> >> >>>
>> > >> >> >>>> I want to address the issue that Matei raised about the heavy
>> > >> >> >>>> lifting
>> > >> >> >>>> required for a full SQL support. It is amazing that even
>> > >> >> >>>> after
>> > 30
>> > >> >> years
>> > >> >> >>> of
>> > >> >> >>>> research there is not a single good open source columnar
>> > database
>> > >> >> >>>> like
>> > >> >> >>>> Vertica. There is a column store option in MySQL, but it is
>> > >> >> >>>> not
>> > >> >> >>>> nearly
>> > >> >> >>> as
>> > >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true need
>> > >> >> >>>> for
>> > >> >> >>>> such
>> > >> >> a
>> > >> >> >>>> system. I wonder why so and it's high time to change that.
>> > >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"
>> > >> >> >>>> <sandy.ryza@cloudera.com>
>> > >> >> wrote:
>> > >> >> >>>>
>> > >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I like
>> > the
>> > >> >> >>> former
>> > >> >> >>>>> slightly better because it's more descriptive.
>> > >> >> >>>>>
>> > >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
>> > covers,
>> > >> >> >>>>> it
>> > >> >> >>> would
>> > >> >> >>>>> be more clear from a user-facing perspective to at least
>> > choose a
>> > >> >> >>> package
>> > >> >> >>>>> name for it that omits "sql".
>> > >> >> >>>>>
>> > >> >> >>>>> I would also be in favor of adding a separate Spark Schema
>> > module
>> > >> >> >>>>> for
>> > >> >> >>>> Spark
>> > >> >> >>>>> SQL to rely on, but I imagine that might be too large a
>> > >> >> >>>>> change
>> > at
>> > >> >> this
>> > >> >> >>>>> point?
>> > >> >> >>>>>
>> > >> >> >>>>> -Sandy
>> > >> >> >>>>>
>> > >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>> > >> >> >>> matei.zaharia@gmail.com>
>> > >> >> >>>>> wrote:
>> > >> >> >>>>>
>> > >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving
>> > >> >> >>>>>> it
>> > >> >> >>>>>> another
>> > >> >> >>>>> name,
>> > >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since
>> > >> >> >>>>>> that
>> > >> >> >>>>>> was
>> > >> >> >>> the
>> > >> >> >>>>> most
>> > >> >> >>>>>> obvious use case to many users.)
>> > >> >> >>>>>>
>> > >> >> >>>>>> Matei
>> > >> >> >>>>>>
>> > >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>> > >> >> >>> matei.zaharia@gmail.com>
>> > >> >> >>>>>> wrote:
>> > >> >> >>>>>>>
>> > >> >> >>>>>>> While it might be possible to move this concept to Spark
>> > >> >> >>>>>>> Core
>> > >> >> >>>>> long-term,
>> > >> >> >>>>>> supporting structured data efficiently does require quite a
>> > bit
>> > >> >> >>>>>> of
>> > >> >> >>> the
>> > >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
>> > columnar
>> > >> >> >>>> storage.
>> > >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL
>> > >> >> >>>>>> server
>> > >> >> >>>>>> --
>> > >> >> >>> it's
>> > >> >> >>>>>> meant to be a library for manipulating structured data.
>> > >> >> >>>>>> Since
>> > >> >> >>>>>> this
>> > >> >> >>> is
>> > >> >> >>>>>> possible to build over the core API, it's pretty natural to
>> > >> >> >>> organize it
>> > >> >> >>>>>> that way, same as Spark Streaming is a library.
>> > >> >> >>>>>>>
>> > >> >> >>>>>>> Matei
>> > >> >> >>>>>>>
>> > >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
>> > koert@tresata.com>
>> > >> >> >>>> wrote:
>> > >> >> >>>>>>>>
>> > >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common data
>> > >> >> >>>>>>>> format
>> > >> >> >>> used
>> > >> >> >>>>> for
>> > >> >> >>>>>>>> bringing data into Spark from external systems, and used
>> > >> >> >>>>>>>> for
>> > >> >> >>> various
>> > >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>> > >> >> >>>>>>>>
>> > >> >> >>>>>>>> i agree. this to me also implies it belongs in spark
>> > >> >> >>>>>>>> core,
>> > not
>> > >> >> >>> sql
>> > >> >> >>>>>>>>
>> > >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>> > >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>> > >> >> >>>>>>>>
>> > >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet,
>> > >> >> >>>>>>>>> the
>> > >> >> >>>>>>>>> Jan.
>> > >> >> >>> 13
>> > >> >> >>>> Bay
>> > >> >> >>>>>> Area
>> > >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
>> > >> >> >>> information
>> > >> >> >>>> on
>> > >> >> >>>>>> this
>> > >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>> ________________________________
>> > >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>> > >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>> > >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>> > >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>> > >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>> One thing potentially not clear from this e-mail, there
>> > will
>> > >> >> >>>>>>>>> be
>> > >> >> >>> a
>> > >> >> >>>> 1:1
>> > >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
>> > DataFrame.
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>> > >> >> >>> rxin@databricks.com>
>> > >> >> >>>>>> wrote:
>> > >> >> >>>>>>>>>> Hi,
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
>> > >> >> >>>>>>>>>> 1.3,
>> > >> >> >>>>>>>>>> and
>> > >> >> >>>>> wanted
>> > >> >> >>>>>> to
>> > >> >> >>>>>>>>>> get the community's opinion.
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common data
>> > >> >> >>>>>>>>>> format
>> > >> >> >>>> used
>> > >> >> >>>>>> for
>> > >> >> >>>>>>>>>> bringing data into Spark from external systems, and
>> > >> >> >>>>>>>>>> used
>> > for
>> > >> >> >>>> various
>> > >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We
>> > also
>> > >> >> >>> expect
>> > >> >> >>>>>> more
>> > >> >> >>>>>>>>> and
>> > >> >> >>>>>>>>>> more users to be programming directly against SchemaRDD
>> > API
>> > >> >> >>> rather
>> > >> >> >>>>>> than
>> > >> >> >>>>>>>>> the
>> > >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used
>> > DSL
>> > >> >> >>>>> originally
>> > >> >> >>>>>>>>>> designed for writing test cases, always has the
>> > >> >> >>>>>>>>>> data-frame
>> > >> >> >>>>>>>>>> like
>> > >> >> >>>> API.
>> > >> >> >>>>>> In
>> > >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API usable
>> > >> >> >>>>>>>>>> for
>> > >> >> >>>>>>>>>> end
>> > >> >> >>>>> users.
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> There are two motivations for the renaming:
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>> > >> >> >>> SchemaRDD.
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
>> > >> >> >>>>>>>>>> RDD
>> > >> >> >>> anymore
>> > >> >> >>>>>> (even
>> > >> >> >>>>>>>>>> though it would contain some RDD functions like map,
>> > >> >> >>>>>>>>>> flatMap,
>> > >> >> >>>> etc),
>> > >> >> >>>>>> and
>> > >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>> > >> >> >>> confusing.
>> > >> >> >>>>>>>>> Instead.
>> > >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
>> > >> >> >>>>>>>>>> RDD
>> > >> >> >>> methods.
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>>
>> > >> >> >>>>>>>>>> My understanding is that very few users program
>> > >> >> >>>>>>>>>> directly
>> > >> >> >>> against
>> > >> >> >>>> the
>> > >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not well
>> > >> >> >>> documented.
>> > >> >> >>>>>>>>> However,
>> > >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a
>> > >> >> >>>>>>>>>> type
>> > >> >> >>>>>>>>>> alias
>> > >> >> >>>>>> DataFrame
>> > >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain
>> > >> >> >>>>>>>>>> source
>> > >> >> >>>>> compatibility
>> > >> >> >>>>>>>>> for
>> > >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
>> > >> >> >>> materials to
>> > >> >> >>>>> use
>> > >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>>
>> > >> >> >>>>
>> > >> >> >>>>
>> > ---------------------------------------------------------------------
>> > >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> >> >>>>>>>>> For additional commands, e-mail:
>> > >> >> >>>>>>>>> dev-help@spark.apache.org
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>>
>> > >> >> >>>>
>> > >> >> >>>>
>> > ---------------------------------------------------------------------
>> > >> >> >>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> >> >>>>>>>>> For additional commands, e-mail:
>> > >> >> >>>>>>>>> dev-help@spark.apache.org
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>>>
>> > >> >> >>>>>>>
>> > >> >> >>>>>>
>> > >> >> >>>>>>
>> > >> >> >>>>>>
>> > >> >> >>>
>> > >> >> >>>
>> > ---------------------------------------------------------------------
>> > >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> >> >>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>> > >> >> >>>>>>
>> > >> >> >>>>>>
>> > >> >> >>>>>
>> > >> >> >>>>
>> > >> >> >>>
>> > >> >> >>
>> > >> >> >>
>> > >> >>
>> > >> >>
>> > >> >>
>> > >> >> ---------------------------------------------------------------------
>> > >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> >> For additional commands, e-mail: dev-help@spark.apache.org
>> > >> >>
>> > >> >>
>> > >
>> > >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 19:54:27 2015
Return-Path: <dev-return-11357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3239717D91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 19:54:27 +0000 (UTC)
Received: (qmail 34933 invoked by uid 500); 29 Jan 2015 19:54:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34866 invoked by uid 500); 29 Jan 2015 19:54:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34851 invoked by uid 99); 29 Jan 2015 19:54:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:54:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.218.51 as permitted sender)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 19:54:20 +0000
Received: by mail-oi0-f51.google.com with SMTP id x69so30440181oia.10
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 11:53:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=h6bbYduS9jr4Kgk/rRFLDusMbytGKDazhhja78K2qt4=;
        b=OL1Q0WV5qHwxZWI+Gc+HcNZoUkhuC0X/Lew09eLN/ClYPoVM3+ajkXmixhZmQ18z8j
         6zBd9K8AnnHwDDzQXFxkK4/toh/WctbSMZ35RVYmPONPfUbzPiKu/uvPXSMc3nHVrIYM
         5I4RglfHN7J9rij+Yt44VSJ0dxcfyeeknb4zPhZ8Z2FzHu/BllRKNZdZ1eVDXUUAXVtz
         xAj/hQvMKOrIJSUtHFDG5LHyFyuS6O9gAft+tCIRbn2LcWueubqtyWssz3lLCnKnNMTh
         xyN/EWR3IldZhjm5uE3V4WK7ZtVOrRh9bKIPKdXlYl3S9p+rJJF/BxHch9A9FbdsVkbz
         QLFg==
X-Received: by 10.202.95.7 with SMTP id t7mr1445249oib.104.1422561195057; Thu,
 29 Jan 2015 11:53:15 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.77.131 with HTTP; Thu, 29 Jan 2015 11:52:34 -0800 (PST)
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Thu, 29 Jan 2015 17:52:34 -0200
Message-ID: <CAO4-Pq_3bMJ4BbCwEESScxDUMZnT0ZkKiypDG7gGjwNE0Khw6w@mail.gmail.com>
Subject: TimeoutException on tests
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113cdf70b3e9ec050dcfd699
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cdf70b3e9ec050dcfd699
Content-Type: text/plain; charset=UTF-8

Hi All,
I'm trying to use a local build spark, adding the pr 1290 to the 1.2.0
build and after I do the build, I my tests start to fail.
 should create labeledpoint *** FAILED *** (10 seconds, 50 milliseconds)
[info]   java.util.concurrent.TimeoutException: Futures timed out after
[10000 milliseconds]

It seems that this is related to a netty problem, I've already tried to
change the netty version but it didn't solved my problem (migrated from
3.4.0.Final, to 3.10.0.Final, does anyone here know how to fix it?

Kind Regards,
Dirceu

--001a113cdf70b3e9ec050dcfd699--

From dev-return-11358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 20:41:06 2015
Return-Path: <dev-return-11358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A796C17F99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 20:41:06 +0000 (UTC)
Received: (qmail 68289 invoked by uid 500); 29 Jan 2015 20:41:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68211 invoked by uid 500); 29 Jan 2015 20:41:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68200 invoked by uid 99); 29 Jan 2015 20:41:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 20:41:05 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of ogeagla@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 20:40:40 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 916661232203
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 12:40:38 -0800 (PST)
Date: Thu, 29 Jan 2015 13:40:38 -0700 (MST)
From: Octavian Geagla <ogeagla@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422564038193-10355.post@n3.nabble.com>
In-Reply-To: <CABjXkq6GOQKq9e4-XELKM0VZbJEAfrFFqqFSZUevyaHGvfz17A@mail.gmail.com>
References: <1422116785722-10265.post@n3.nabble.com> <CAJgQjQ_zr+xqKCmM0kySqaRnGU18DhUvz-Raqc99919-7Mqn5A@mail.gmail.com> <CABjXkq6GOQKq9e4-XELKM0VZbJEAfrFFqqFSZUevyaHGvfz17A@mail.gmail.com>
Subject: Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for the responses.  How would something like HadamardProduct or
similar be in order to keep it explicit?  Would still be a VectorTransformer
so the name and trait would hopefully lead to a somewhat self-documenting
class.  

Xiangrui, do you mean Hadamard product or transform?  My initial proposal
was only a vector-vector product, but I can extend this to matrices. The
transform would require a bit more work, which I'm willing to do, but I'm
not sure where FFT comes in, can you elaborate?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Any-interest-in-weighting-VectorTransformer-which-does-component-wise-scaling-tp10265p10355.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 20:45:47 2015
Return-Path: <dev-return-11359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E6DC617FF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 20:45:46 +0000 (UTC)
Received: (qmail 82720 invoked by uid 500); 29 Jan 2015 20:45:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82644 invoked by uid 500); 29 Jan 2015 20:45:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82600 invoked by uid 99); 29 Jan 2015 20:45:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 20:45:46 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of pastuszka.przemyslaw@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 20:45:40 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 6027512323B1
	for <dev@spark.apache.org>; Thu, 29 Jan 2015 12:45:20 -0800 (PST)
Date: Thu, 29 Jan 2015 13:45:20 -0700 (MST)
From: rtshadow <pastuszka.przemyslaw@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422564320008-10356.post@n3.nabble.com>
Subject: How to speed PySpark to match Scala/Java performance
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

In my company, we've been trying to use PySpark to run ETLs on our data.
Alas, it turned out to be terribly slow compared to Java or Scala API (which
we ended up using to meet performance criteria). 

To be more quantitative, let's consider simple case:
I've generated test file (848MB): /seq 1 100000000 > /tmp/test/

and tried to run simple computation on it, which includes three steps: read
-> multiply each row by 2 -> take max
Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/

Here are the results of this simple benchmark:
CPython - 59s
PyPy - 26s
Scala version - 7s

I didn't dig into what exactly contributes to execution times of CPython /
PyPy, but it seems that serialization / deserialization, when sending data
to the worker may be the issue. 
I know some guys already have been asking about using Jython
(http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658,
http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html),
but it seems, that no one have really done this with Spark.
It looks like performance gain from using jython can be huge - you wouldn't
need to spawn PythonWorkers, all the code would be just executed inside
SparkExecutor JVM, using python code compiled to java bytecode. Do you think
that's possible to achieve? Do you see any obvious obstacles? Of course,
jython doesn't have C extensions, but if one doesn't need them, then it
should fit here nicely.

I'm willing to try to marry Spark with Jython and see how it goes.

What do you think about this?





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 21:10:46 2015
Return-Path: <dev-return-11360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB80B17342
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 21:10:46 +0000 (UTC)
Received: (qmail 50177 invoked by uid 500); 29 Jan 2015 21:10:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50101 invoked by uid 500); 29 Jan 2015 21:10:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50090 invoked by uid 99); 29 Jan 2015 21:10:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:10:45 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:10:20 +0000
Received: by mail-lb0-f177.google.com with SMTP id p9so32687331lbv.8
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 13:08:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=fegdFi5E23aCOvFUxxOMtPymDK/Zd2HMwLm0IRDv4pU=;
        b=URmZrD50dvCXcN8FNubc/GsJ6iuOD/LxhUmAh6EzHn2M8MSePc1LLwSG/hJD4Ud+3w
         G8XtS/X0TMXaVS9PsMNxF+IVuyDjE4ZHAM5PI6k00aGxUxryAkJoN/IhFkjxznAFU64o
         siv3IRBtTzIRrhGWZBigS26lrEx70EQdzSOGuUkaZ0b2T+0ozfQEAsaaublcEXt6u6fw
         kyFe5OmtFJC0mZUZWHByjwMAVZn4EUigBbQIXxYsSYzomwEYKeA+/iYH8i1jjsbN/mmQ
         eqYs3Zri2QO51NkExzOMkOJEqKXC9IOQ71IXm7bq2nuQNFcKtdY6gXKFN8KONN984CUO
         JlXw==
X-Gm-Message-State: ALoCoQlhlSs0quZ+PiRj+Wmk/eC81j26/6j5ouTFvou1QmXe4EEDFyTQwBsVt2mIIwBDbEGMVnKe
MIME-Version: 1.0
X-Received: by 10.112.252.42 with SMTP id zp10mr3190031lbc.51.1422565709110;
 Thu, 29 Jan 2015 13:08:29 -0800 (PST)
Received: by 10.25.166.136 with HTTP; Thu, 29 Jan 2015 13:08:29 -0800 (PST)
In-Reply-To: <1422564320008-10356.post@n3.nabble.com>
References: <1422564320008-10356.post@n3.nabble.com>
Date: Thu, 29 Jan 2015 13:08:29 -0800
Message-ID: <CA+2Pv=hNBFMuGCXf0zq_u3LhjXCQUeZacD29L5-nBhN78p9XGQ@mail.gmail.com>
Subject: Re: How to speed PySpark to match Scala/Java performance
From: Davies Liu <davies@databricks.com>
To: rtshadow <pastuszka.przemyslaw@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey,

Without having Python as fast as Scala/Java, I think it's impossible to similar
performance in PySpark as in Scala/Java. Jython is also much slower than
Scala/Java.

With Jython, we can avoid the cost of manage multiple process and RPC,
we may still need to do the data conversion between Java and Python.
Given that fact that Jython is not widely used in production, it may introduce
more troubles than the performance gain.

Spark jobs can be easily speed up by scaling out (by adding more resources).
I think the most advantage of PySpark is that it let you do fast prototype.
Once you got your ETL finalized, it's not that hard to translate your
pure Python
jobs into Scala to reduce the cost(it's optional).

Now days, engineer time is much more expensive than CPU time, I think we
should be more focus on the former.

That's my 2 cents.

Davies

On Thu, Jan 29, 2015 at 12:45 PM, rtshadow
<pastuszka.przemyslaw@gmail.com> wrote:
> Hi,
>
> In my company, we've been trying to use PySpark to run ETLs on our data.
> Alas, it turned out to be terribly slow compared to Java or Scala API (which
> we ended up using to meet performance criteria).
>
> To be more quantitative, let's consider simple case:
> I've generated test file (848MB): /seq 1 100000000 > /tmp/test/
>
> and tried to run simple computation on it, which includes three steps: read
> -> multiply each row by 2 -> take max
> Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
> Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/
>
> Here are the results of this simple benchmark:
> CPython - 59s
> PyPy - 26s
> Scala version - 7s
>
> I didn't dig into what exactly contributes to execution times of CPython /
> PyPy, but it seems that serialization / deserialization, when sending data
> to the worker may be the issue.
> I know some guys already have been asking about using Jython
> (http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658,
> http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html),
> but it seems, that no one have really done this with Spark.
> It looks like performance gain from using jython can be huge - you wouldn't
> need to spawn PythonWorkers, all the code would be just executed inside
> SparkExecutor JVM, using python code compiled to java bytecode. Do you think
> that's possible to achieve? Do you see any obvious obstacles? Of course,
> jython doesn't have C extensions, but if one doesn't need them, then it
> should fit here nicely.
>
> I'm willing to try to marry Spark with Jython and see how it goes.
>
> What do you think about this?
>
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 21:15:30 2015
Return-Path: <dev-return-11361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9947217363
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 21:15:30 +0000 (UTC)
Received: (qmail 64511 invoked by uid 500); 29 Jan 2015 21:15:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64432 invoked by uid 500); 29 Jan 2015 21:15:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64406 invoked by uid 99); 29 Jan 2015 21:15:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:15:29 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:15:25 +0000
Received: by mail-qg0-f50.google.com with SMTP id f51so34379949qge.9
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 13:13:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=7qnl3G3b3wK8pljGYIkzKnVTx5ktet1Wo/1ZvMoDa+w=;
        b=f/ULGjhrtFgrKe531EMlK+BYGqg1JBXp3qIoyvp12fm0NAm0279MXw/brf1wdq/7fg
         hE0IwKoRZF1ymW1w5dv10+MT2wN47KvVygGFePOhs+AruyvEeMrZngy36aNkQlVpBJXE
         vfE14fbjfJ4svFRPmFOFQmg6rkCji9yP05jMwNYEnWf3ggnI6d7foxJEuf/DX8o7jLgW
         W9B4cVJsWJdDIVh4RFj3HjsidOIcNUC/NogI8HEjA5XoV5fWhxxkWrvLZHpd3QW4pnIY
         VCNHaKDWfUw8z43oGlyCLiXG1Ognh9Gx6FcE76lLrSXn8M4S8C0aGIKmufSVNAFwb7ia
         VNAw==
X-Gm-Message-State: ALoCoQlhsSziUPDcbmGWrmJ8z+Wn/1g1NeAjbNsO9imzxI1JWBjm3M4BP1jIMMlqyhoRSUS40C/x
X-Received: by 10.224.63.70 with SMTP id a6mr5541955qai.42.1422565983741; Thu,
 29 Jan 2015 13:13:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 29 Jan 2015 13:12:43 -0800 (PST)
In-Reply-To: <1422564320008-10356.post@n3.nabble.com>
References: <1422564320008-10356.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 29 Jan 2015 13:12:43 -0800
Message-ID: <CAPh_B=ZH2KV7C7B6WRyJFmZWarfJpVjCPYHcHBzJHvmeTa5okg@mail.gmail.com>
Subject: Re: How to speed PySpark to match Scala/Java performance
To: rtshadow <pastuszka.przemyslaw@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdca1002184ec050dd0f451
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdca1002184ec050dd0f451
Content-Type: text/plain; charset=UTF-8

Once the data frame API is released for 1.3, you can write your thing in
Python and get the same performance. It can't express everything, but for
basic things like projection, filter, join, aggregate and simple numeric
computation, it should work pretty well.


On Thu, Jan 29, 2015 at 12:45 PM, rtshadow <pastuszka.przemyslaw@gmail.com>
wrote:

> Hi,
>
> In my company, we've been trying to use PySpark to run ETLs on our data.
> Alas, it turned out to be terribly slow compared to Java or Scala API
> (which
> we ended up using to meet performance criteria).
>
> To be more quantitative, let's consider simple case:
> I've generated test file (848MB): /seq 1 100000000 > /tmp/test/
>
> and tried to run simple computation on it, which includes three steps: read
> -> multiply each row by 2 -> take max
> Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
> Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/
>
> Here are the results of this simple benchmark:
> CPython - 59s
> PyPy - 26s
> Scala version - 7s
>
> I didn't dig into what exactly contributes to execution times of CPython /
> PyPy, but it seems that serialization / deserialization, when sending data
> to the worker may be the issue.
> I know some guys already have been asking about using Jython
> (
> http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658
> ,
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html
> ),
> but it seems, that no one have really done this with Spark.
> It looks like performance gain from using jython can be huge - you wouldn't
> need to spawn PythonWorkers, all the code would be just executed inside
> SparkExecutor JVM, using python code compiled to java bytecode. Do you
> think
> that's possible to achieve? Do you see any obvious obstacles? Of course,
> jython doesn't have C extensions, but if one doesn't need them, then it
> should fit here nicely.
>
> I'm willing to try to marry Spark with Jython and see how it goes.
>
> What do you think about this?
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bdca1002184ec050dd0f451--

From dev-return-11362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 21:25:34 2015
Return-Path: <dev-return-11362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5FE9C173C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 21:25:34 +0000 (UTC)
Received: (qmail 95661 invoked by uid 500); 29 Jan 2015 21:25:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95591 invoked by uid 500); 29 Jan 2015 21:25:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95580 invoked by uid 99); 29 Jan 2015 21:25:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:25:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 21:25:06 +0000
Received: by mail-wi0-f181.google.com with SMTP id fb4so30461258wid.2
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 13:24:45 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=7+YRpOKvbRsF0Vi1m5p7dr0tEZq0WwE8s5HXz+7u7jo=;
        b=Ou3Jv1NbVbox6kvfYb4VShMyuorVE7jWXVWL06ywWoIfELZqmRO75vHhlhZ2ggRDrC
         EXz6Z66tsjYBElVGZyeCrxdpQMq/0Yi27qNkcT+NcwaufM0VJOgdTZLER+FzUQ1zmtC8
         nTTNtF0tn2ZUCg5uOEW4CwickYknB/+RCJGYGlheeP6U5zR6NpvJkwQGhO8Km5iBEokM
         9F1z/FGuJN722T95WvwVHDQasBa+SUPvnKIO6qLm5FJBJBF2ZPHZbh8tdmYIGBw2Zt1s
         dNspsfn6E/7SNooYQ7a0hCKM3RrefXpSqLRNBfeadUeomf6TICqBxhk+fLdqgavZGnIG
         ZU9A==
X-Gm-Message-State: ALoCoQld2CFuq6A0ZyXbbVL4rF1MFdKyDqSd0y+1ekbLwOsdFtj0eEZ3BFbgJ7yDkOcE2o/+NSKg
MIME-Version: 1.0
X-Received: by 10.194.201.137 with SMTP id ka9mr5561207wjc.66.1422566685433;
 Thu, 29 Jan 2015 13:24:45 -0800 (PST)
Received: by 10.217.122.200 with HTTP; Thu, 29 Jan 2015 13:24:45 -0800 (PST)
X-Originating-IP: [204.148.13.62]
In-Reply-To: <CAN6Vra1Pr9ouShTMzp8fmDfaK_E51=gAkxhnGhdC1GpJDYO54w@mail.gmail.com>
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>
	<1523658604.850253.1422313882707.JavaMail.yahoo@mail.yahoo.com>
	<CANx3uAikOPchQi1VkVtUa-xTCYEZXb8r=jJTnz6uQkKC0FmkhA@mail.gmail.com>
	<7D454179-D2DC-4D23-9F0F-F4881DF44628@gmail.com>
	<1C10BA8E-D6C5-4722-B43F-D7F29C12A6F6@gmail.com>
	<CACBYxKLTC4tBEMQ=qJhMdrPzAfN3aSxgZKmN-v-2fAcm=ybgww@mail.gmail.com>
	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>
	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>
	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>
	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>
	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>
	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>
	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>
	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>
	<CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>
	<CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>
	<CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>
	<CAN6Vra1Pr9ouShTMzp8fmDfaK_E51=gAkxhnGhdC1GpJDYO54w@mail.gmail.com>
Date: Thu, 29 Jan 2015 16:24:45 -0500
Message-ID: <CANx3uAjOvdKeUx1cuadSW3vy77=1juykbQRGbVbDt36hjif+vg@mail.gmail.com>
Subject: Re: renaming SchemaRDD -> DataFrame
From: Koert Kuipers <koert@tresata.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b873d5ef478b7050dd11dbd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b873d5ef478b7050dd11dbd
Content-Type: text/plain; charset=UTF-8

to me the word DataFrame does come with certain expectations. one of them
is that the data is stored columnar. in R data.frame internally uses a list
of sequences i think, but since lists can have labels its more like a
SortedMap[String, Array[_]]. this makes certain operations very cheap (such
as adding a column).

in Spark the closest thing would be a data structure where per Partition
the data is also stored columnar. does spark SQL already use something like
that? Evan mentioned "Spark SQL columnar compression", which sounds like
it. where can i find that?

thanks

On Thu, Jan 29, 2015 at 2:32 PM, Evan Chan <velvia.github@gmail.com> wrote:

> +1.... having proper NA support is much cleaner than using null, at
> least the Java null.
>
> On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks <evan.sparks@gmail.com>
> wrote:
> > You've got to be a little bit careful here. "NA" in systems like R or
> pandas
> > may have special meaning that is distinct from "null".
> >
> > See, e.g. http://www.r-bloggers.com/r-na-vs-null/
> >
> >
> >
> > On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>
> >> Isn't that just "null" in SQL?
> >>
> >> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
> >> wrote:
> >>
> >> > I believe that most DataFrame implementations out there, like Pandas,
> >> > supports the idea of missing values / NA, and some support the idea of
> >> > Not Meaningful as well.
> >> >
> >> > Does Row support anything like that?  That is important for certain
> >> > applications.  I thought that Row worked by being a mutable object,
> >> > but haven't looked into the details in a while.
> >> >
> >> > -Evan
> >> >
> >> > On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
> >> > wrote:
> >> > > It shouldn't change the data source api at all because data sources
> >> > create
> >> > > RDD[Row], and that gets converted into a DataFrame automatically
> >> > (previously
> >> > > to SchemaRDD).
> >> > >
> >> > >
> >> >
> >> >
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
> >> > >
> >> > > One thing that will break the data source API in 1.3 is the location
> >> > > of
> >> > > types. Types were previously defined in sql.catalyst.types, and now
> >> > moved to
> >> > > sql.types. After 1.3, sql.catalyst is hidden from users, and all
> >> > > public
> >> > APIs
> >> > > have first class classes/objects defined in sql directly.
> >> > >
> >> > >
> >> > >
> >> > > On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com
> >
> >> > wrote:
> >> > >>
> >> > >> Hey guys,
> >> > >>
> >> > >> How does this impact the data sources API?  I was planning on using
> >> > >> this for a project.
> >> > >>
> >> > >> +1 that many things from spark-sql / DataFrame is universally
> >> > >> desirable and useful.
> >> > >>
> >> > >> By the way, one thing that prevents the columnar compression stuff
> in
> >> > >> Spark SQL from being more useful is, at least from previous talks
> >> > >> with
> >> > >> Reynold and Michael et al., that the format was not designed for
> >> > >> persistence.
> >> > >>
> >> > >> I have a new project that aims to change that.  It is a
> >> > >> zero-serialisation, high performance binary vector library,
> designed
> >> > >> from the outset to be a persistent storage friendly.  May be one
> day
> >> > >> it can replace the Spark SQL columnar compression.
> >> > >>
> >> > >> Michael told me this would be a lot of work, and recreates parts of
> >> > >> Parquet, but I think it's worth it.  LMK if you'd like more
> details.
> >> > >>
> >> > >> -Evan
> >> > >>
> >> > >> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
> >> > wrote:
> >> > >> > Alright I have merged the patch (
> >> > >> > https://github.com/apache/spark/pull/4173
> >> > >> > ) since I don't see any strong opinions against it (as a matter
> of
> >> > fact
> >> > >> > most were for it). We can still change it if somebody lays out a
> >> > strong
> >> > >> > argument.
> >> > >> >
> >> > >> > On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
> >> > >> > <matei.zaharia@gmail.com>
> >> > >> > wrote:
> >> > >> >
> >> > >> >> The type alias means your methods can specify either type and
> they
> >> > will
> >> > >> >> work. It's just another name for the same type. But Scaladocs
> and
> >> > such
> >> > >> >> will
> >> > >> >> show DataFrame as the type.
> >> > >> >>
> >> > >> >> Matei
> >> > >> >>
> >> > >> >> > On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
> >> > >> >> dirceu.semighini@gmail.com> wrote:
> >> > >> >> >
> >> > >> >> > Reynold,
> >> > >> >> > But with type alias we will have the same problem, right?
> >> > >> >> > If the methods doesn't receive schemardd anymore, we will have
> >> > >> >> > to
> >> > >> >> > change
> >> > >> >> > our code to migrade from schema to dataframe. Unless we have
> an
> >> > >> >> > implicit
> >> > >> >> > conversion between DataFrame and SchemaRDD
> >> > >> >> >
> >> > >> >> >
> >> > >> >> >
> >> > >> >> > 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
> >> > >> >> >
> >> > >> >> >> Dirceu,
> >> > >> >> >>
> >> > >> >> >> That is not possible because one cannot overload return
> types.
> >> > >> >> >>
> >> > >> >> >> SQLContext.parquetFile (and many other methods) needs to
> return
> >> > some
> >> > >> >> type,
> >> > >> >> >> and that type cannot be both SchemaRDD and DataFrame.
> >> > >> >> >>
> >> > >> >> >> In 1.3, we will create a type alias for DataFrame called
> >> > >> >> >> SchemaRDD
> >> > >> >> >> to
> >> > >> >> not
> >> > >> >> >> break source compatibility for Scala.
> >> > >> >> >>
> >> > >> >> >>
> >> > >> >> >> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
> >> > >> >> >> dirceu.semighini@gmail.com> wrote:
> >> > >> >> >>
> >> > >> >> >>> Can't the SchemaRDD remain the same, but deprecated, and be
> >> > removed
> >> > >> >> >>> in
> >> > >> >> the
> >> > >> >> >>> release 1.5(+/- 1)  for example, and the new code been added
> >> > >> >> >>> to
> >> > >> >> DataFrame?
> >> > >> >> >>> With this, we don't impact in existing code for the next few
> >> > >> >> >>> releases.
> >> > >> >> >>>
> >> > >> >> >>>
> >> > >> >> >>>
> >> > >> >> >>> 2015-01-27 0:02 GMT-02:00 Kushal Datta
> >> > >> >> >>> <kushal.datta@gmail.com>:
> >> > >> >> >>>
> >> > >> >> >>>> I want to address the issue that Matei raised about the
> heavy
> >> > >> >> >>>> lifting
> >> > >> >> >>>> required for a full SQL support. It is amazing that even
> >> > >> >> >>>> after
> >> > 30
> >> > >> >> years
> >> > >> >> >>> of
> >> > >> >> >>>> research there is not a single good open source columnar
> >> > database
> >> > >> >> >>>> like
> >> > >> >> >>>> Vertica. There is a column store option in MySQL, but it is
> >> > >> >> >>>> not
> >> > >> >> >>>> nearly
> >> > >> >> >>> as
> >> > >> >> >>>> sophisticated as Vertica or MonetDB. But there's a true
> need
> >> > >> >> >>>> for
> >> > >> >> >>>> such
> >> > >> >> a
> >> > >> >> >>>> system. I wonder why so and it's high time to change that.
> >> > >> >> >>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"
> >> > >> >> >>>> <sandy.ryza@cloudera.com>
> >> > >> >> wrote:
> >> > >> >> >>>>
> >> > >> >> >>>>> Both SchemaRDD and DataFrame sound fine to me, though I
> like
> >> > the
> >> > >> >> >>> former
> >> > >> >> >>>>> slightly better because it's more descriptive.
> >> > >> >> >>>>>
> >> > >> >> >>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
> >> > covers,
> >> > >> >> >>>>> it
> >> > >> >> >>> would
> >> > >> >> >>>>> be more clear from a user-facing perspective to at least
> >> > choose a
> >> > >> >> >>> package
> >> > >> >> >>>>> name for it that omits "sql".
> >> > >> >> >>>>>
> >> > >> >> >>>>> I would also be in favor of adding a separate Spark Schema
> >> > module
> >> > >> >> >>>>> for
> >> > >> >> >>>> Spark
> >> > >> >> >>>>> SQL to rely on, but I imagine that might be too large a
> >> > >> >> >>>>> change
> >> > at
> >> > >> >> this
> >> > >> >> >>>>> point?
> >> > >> >> >>>>>
> >> > >> >> >>>>> -Sandy
> >> > >> >> >>>>>
> >> > >> >> >>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
> >> > >> >> >>> matei.zaharia@gmail.com>
> >> > >> >> >>>>> wrote:
> >> > >> >> >>>>>
> >> > >> >> >>>>>> (Actually when we designed Spark SQL we thought of giving
> >> > >> >> >>>>>> it
> >> > >> >> >>>>>> another
> >> > >> >> >>>>> name,
> >> > >> >> >>>>>> like Spark Schema, but we decided to stick with SQL since
> >> > >> >> >>>>>> that
> >> > >> >> >>>>>> was
> >> > >> >> >>> the
> >> > >> >> >>>>> most
> >> > >> >> >>>>>> obvious use case to many users.)
> >> > >> >> >>>>>>
> >> > >> >> >>>>>> Matei
> >> > >> >> >>>>>>
> >> > >> >> >>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
> >> > >> >> >>> matei.zaharia@gmail.com>
> >> > >> >> >>>>>> wrote:
> >> > >> >> >>>>>>>
> >> > >> >> >>>>>>> While it might be possible to move this concept to Spark
> >> > >> >> >>>>>>> Core
> >> > >> >> >>>>> long-term,
> >> > >> >> >>>>>> supporting structured data efficiently does require
> quite a
> >> > bit
> >> > >> >> >>>>>> of
> >> > >> >> >>> the
> >> > >> >> >>>>>> infrastructure in Spark SQL, such as query planning and
> >> > columnar
> >> > >> >> >>>> storage.
> >> > >> >> >>>>>> The intent of Spark SQL though is to be more than a SQL
> >> > >> >> >>>>>> server
> >> > >> >> >>>>>> --
> >> > >> >> >>> it's
> >> > >> >> >>>>>> meant to be a library for manipulating structured data.
> >> > >> >> >>>>>> Since
> >> > >> >> >>>>>> this
> >> > >> >> >>> is
> >> > >> >> >>>>>> possible to build over the core API, it's pretty natural
> to
> >> > >> >> >>> organize it
> >> > >> >> >>>>>> that way, same as Spark Streaming is a library.
> >> > >> >> >>>>>>>
> >> > >> >> >>>>>>> Matei
> >> > >> >> >>>>>>>
> >> > >> >> >>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
> >> > koert@tresata.com>
> >> > >> >> >>>> wrote:
> >> > >> >> >>>>>>>>
> >> > >> >> >>>>>>>> "The context is that SchemaRDD is becoming a common
> data
> >> > >> >> >>>>>>>> format
> >> > >> >> >>> used
> >> > >> >> >>>>> for
> >> > >> >> >>>>>>>> bringing data into Spark from external systems, and
> used
> >> > >> >> >>>>>>>> for
> >> > >> >> >>> various
> >> > >> >> >>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
> >> > >> >> >>>>>>>>
> >> > >> >> >>>>>>>> i agree. this to me also implies it belongs in spark
> >> > >> >> >>>>>>>> core,
> >> > not
> >> > >> >> >>> sql
> >> > >> >> >>>>>>>>
> >> > >> >> >>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
> >> > >> >> >>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
> >> > >> >> >>>>>>>>
> >> > >> >> >>>>>>>>> And in the off chance that anyone hasn't seen it yet,
> >> > >> >> >>>>>>>>> the
> >> > >> >> >>>>>>>>> Jan.
> >> > >> >> >>> 13
> >> > >> >> >>>> Bay
> >> > >> >> >>>>>> Area
> >> > >> >> >>>>>>>>> Spark Meetup YouTube contained a wealth of background
> >> > >> >> >>> information
> >> > >> >> >>>> on
> >> > >> >> >>>>>> this
> >> > >> >> >>>>>>>>> idea (mostly from Patrick and Reynold :-).
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>> ________________________________
> >> > >> >> >>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
> >> > >> >> >>>>>>>>> To: Reynold Xin <rxin@databricks.com>
> >> > >> >> >>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
> >> > >> >> >>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
> >> > >> >> >>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>> One thing potentially not clear from this e-mail,
> there
> >> > will
> >> > >> >> >>>>>>>>> be
> >> > >> >> >>> a
> >> > >> >> >>>> 1:1
> >> > >> >> >>>>>>>>> correspondence where you can get an RDD to/from a
> >> > DataFrame.
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
> >> > >> >> >>> rxin@databricks.com>
> >> > >> >> >>>>>> wrote:
> >> > >> >> >>>>>>>>>> Hi,
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
> >> > >> >> >>>>>>>>>> 1.3,
> >> > >> >> >>>>>>>>>> and
> >> > >> >> >>>>> wanted
> >> > >> >> >>>>>> to
> >> > >> >> >>>>>>>>>> get the community's opinion.
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> The context is that SchemaRDD is becoming a common
> data
> >> > >> >> >>>>>>>>>> format
> >> > >> >> >>>> used
> >> > >> >> >>>>>> for
> >> > >> >> >>>>>>>>>> bringing data into Spark from external systems, and
> >> > >> >> >>>>>>>>>> used
> >> > for
> >> > >> >> >>>> various
> >> > >> >> >>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API.
> We
> >> > also
> >> > >> >> >>> expect
> >> > >> >> >>>>>> more
> >> > >> >> >>>>>>>>> and
> >> > >> >> >>>>>>>>>> more users to be programming directly against
> SchemaRDD
> >> > API
> >> > >> >> >>> rather
> >> > >> >> >>>>>> than
> >> > >> >> >>>>>>>>> the
> >> > >> >> >>>>>>>>>> core RDD API. SchemaRDD, through its less commonly
> used
> >> > DSL
> >> > >> >> >>>>> originally
> >> > >> >> >>>>>>>>>> designed for writing test cases, always has the
> >> > >> >> >>>>>>>>>> data-frame
> >> > >> >> >>>>>>>>>> like
> >> > >> >> >>>> API.
> >> > >> >> >>>>>> In
> >> > >> >> >>>>>>>>>> 1.3, we are redesigning the API to make the API
> usable
> >> > >> >> >>>>>>>>>> for
> >> > >> >> >>>>>>>>>> end
> >> > >> >> >>>>> users.
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> There are two motivations for the renaming:
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> 1. DataFrame seems to be a more self-evident name
> than
> >> > >> >> >>> SchemaRDD.
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
> >> > >> >> >>>>>>>>>> RDD
> >> > >> >> >>> anymore
> >> > >> >> >>>>>> (even
> >> > >> >> >>>>>>>>>> though it would contain some RDD functions like map,
> >> > >> >> >>>>>>>>>> flatMap,
> >> > >> >> >>>> etc),
> >> > >> >> >>>>>> and
> >> > >> >> >>>>>>>>>> calling it Schema*RDD* while it is not an RDD is
> highly
> >> > >> >> >>> confusing.
> >> > >> >> >>>>>>>>> Instead.
> >> > >> >> >>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
> >> > >> >> >>>>>>>>>> RDD
> >> > >> >> >>> methods.
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>>
> >> > >> >> >>>>>>>>>> My understanding is that very few users program
> >> > >> >> >>>>>>>>>> directly
> >> > >> >> >>> against
> >> > >> >> >>>> the
> >> > >> >> >>>>>>>>>> SchemaRDD API at the moment, because they are not
> well
> >> > >> >> >>> documented.
> >> > >> >> >>>>>>>>> However,
> >> > >> >> >>>>>>>>>> oo maintain backward compatibility, we can create a
> >> > >> >> >>>>>>>>>> type
> >> > >> >> >>>>>>>>>> alias
> >> > >> >> >>>>>> DataFrame
> >> > >> >> >>>>>>>>>> that is still named SchemaRDD. This will maintain
> >> > >> >> >>>>>>>>>> source
> >> > >> >> >>>>> compatibility
> >> > >> >> >>>>>>>>> for
> >> > >> >> >>>>>>>>>> Scala. That said, we will have to update all existing
> >> > >> >> >>> materials to
> >> > >> >> >>>>> use
> >> > >> >> >>>>>>>>>> DataFrame rather than SchemaRDD.
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>
> >> > >> >> >>>>
> >> > ---------------------------------------------------------------------
> >> > >> >> >>>>>>>>> To unsubscribe, e-mail:
> dev-unsubscribe@spark.apache.org
> >> > >> >> >>>>>>>>> For additional commands, e-mail:
> >> > >> >> >>>>>>>>> dev-help@spark.apache.org
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>
> >> > >> >> >>>>
> >> > ---------------------------------------------------------------------
> >> > >> >> >>>>>>>>> To unsubscribe, e-mail:
> dev-unsubscribe@spark.apache.org
> >> > >> >> >>>>>>>>> For additional commands, e-mail:
> >> > >> >> >>>>>>>>> dev-help@spark.apache.org
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>>>
> >> > >> >> >>>>>>>
> >> > >> >> >>>>>>
> >> > >> >> >>>>>>
> >> > >> >> >>>>>>
> >> > >> >> >>>
> >> > >> >> >>>
> >> > ---------------------------------------------------------------------
> >> > >> >> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > >> >> >>>>>> For additional commands, e-mail:
> dev-help@spark.apache.org
> >> > >> >> >>>>>>
> >> > >> >> >>>>>>
> >> > >> >> >>>>>
> >> > >> >> >>>>
> >> > >> >> >>>
> >> > >> >> >>
> >> > >> >> >>
> >> > >> >>
> >> > >> >>
> >> > >> >>
> >> > >> >>
> ---------------------------------------------------------------------
> >> > >> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > >> >> For additional commands, e-mail: dev-help@spark.apache.org
> >> > >> >>
> >> > >> >>
> >> > >
> >> > >
> >> >
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b873d5ef478b7050dd11dbd--

From dev-return-11363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 22:01:12 2015
Return-Path: <dev-return-11363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 878BD17569
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 22:01:12 +0000 (UTC)
Received: (qmail 6330 invoked by uid 500); 29 Jan 2015 22:01:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6247 invoked by uid 500); 29 Jan 2015 22:01:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6230 invoked by uid 99); 29 Jan 2015 22:01:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 22:01:11 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.223.176 as permitted sender)
Received: from [209.85.223.176] (HELO mail-ie0-f176.google.com) (209.85.223.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 22:00:43 +0000
Received: by mail-ie0-f176.google.com with SMTP id at20so194570iec.7
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 14:00:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type;
        bh=xemMuzOhTnPoCcmuTGxRO0fqCMu5BhFuaYNBHMqdXNk=;
        b=TN7QY9bQj75C30vGvnIlMaZhCyZYC3BXFS2JrE4IAgWUHvypig0ao4U1zTbPZyjUON
         xo+3Vci6oFnnBPsB/jKpev0AqySaUvU6Dj91p4oqJ+WusO/aOtkW6oLKh8m33JGYLkvF
         C/BxclkJ/HVlXAHFvikQljXnAvrWRKSQa9adurVfrEOCtzfhnC+3vkXK5B7jO7/wHJ6C
         mbbX2fZurvQVjR0JG89WZEYLsGTEa//F+1nBzxPsDwdHYzyIAdoV1aMTEunD01pz+BoS
         6jzMHA961K/oEkuMRDdsS+bdRE9SwBw7lV3xJlN8VLImCV+hkC1l7BHkUyYGAAU+z63C
         s+ZA==
X-Received: by 10.70.34.208 with SMTP id b16mr3692494pdj.141.1422568840459;
        Thu, 29 Jan 2015 14:00:40 -0800 (PST)
Received: from [192.168.1.168] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id pf10sm8669129pbc.82.2015.01.29.14.00.36
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 29 Jan 2015 14:00:39 -0800 (PST)
Message-ID: <54CAAD83.80600@gmail.com>
Date: Thu, 29 Jan 2015 14:00:35 -0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: Koert Kuipers <koert@tresata.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: renaming SchemaRDD -> DataFrame
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>	<CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>	<CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>	<CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>	<CAN6Vra1Pr9ouShTMzp8fmDfaK_E51=gAkxhnGhdC1GpJDYO54w@mail.gmail.com> <CANx3uAjOvdKeUx1cuadSW3vy77=1juykbQRGbVbDt36hjif+vg@mail.gmail.com> <54CAAD2D.8030407@gmail.com>
In-Reply-To: <54CAAD2D.8030407@gmail.com>
Content-Type: multipart/alternative;
 boundary="------------040105060102050604080703"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------040105060102050604080703
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Forgot to mention that you can find it here 
<https://github.com/apache/spark/blob/f9e569452e2f0ae69037644170d8aa79ac6b4ccf/sql/core/src/main/scala/org/apache/spark/sql/columnar/InMemoryColumnarTableScan.scala>.

On 1/29/15 1:59 PM, Cheng Lian wrote:

> Yes, when a DataFrame is cached in memory, it's stored in an efficient 
> columnar format. And you can also easily persist it on disk using 
> Parquet, which is also columnar.
>
> Cheng
>
> On 1/29/15 1:24 PM, Koert Kuipers wrote:
>> to me the word DataFrame does come with certain expectations. one of 
>> them
>> is that the data is stored columnar. in R data.frame internally uses 
>> a list
>> of sequences i think, but since lists can have labels its more like a
>> SortedMap[String, Array[_]]. this makes certain operations very cheap 
>> (such
>> as adding a column).
>>
>> in Spark the closest thing would be a data structure where per Partition
>> the data is also stored columnar. does spark SQL already use 
>> something like
>> that? Evan mentioned "Spark SQL columnar compression", which sounds like
>> it. where can i find that?
>>
>> thanks
>>
>> On Thu, Jan 29, 2015 at 2:32 PM, Evan Chan <velvia.github@gmail.com> 
>> wrote:
>>
>>> +1.... having proper NA support is much cleaner than using null, at
>>> least the Java null.
>>>
>>> On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks <evan.sparks@gmail.com>
>>> wrote:
>>>> You've got to be a little bit careful here. "NA" in systems like R or
>>> pandas
>>>> may have special meaning that is distinct from "null".
>>>>
>>>> See, e.g. http://www.r-bloggers.com/r-na-vs-null/
>>>>
>>>>
>>>>
>>>> On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>>>> Isn't that just "null" in SQL?
>>>>>
>>>>> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> I believe that most DataFrame implementations out there, like 
>>>>>> Pandas,
>>>>>> supports the idea of missing values / NA, and some support the 
>>>>>> idea of
>>>>>> Not Meaningful as well.
>>>>>>
>>>>>> Does Row support anything like that?  That is important for certain
>>>>>> applications.  I thought that Row worked by being a mutable object,
>>>>>> but haven't looked into the details in a while.
>>>>>>
>>>>>> -Evan
>>>>>>
>>>>>> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
>>>>>> wrote:
>>>>>>> It shouldn't change the data source api at all because data sources
>>>>>> create
>>>>>>> RDD[Row], and that gets converted into a DataFrame automatically
>>>>>> (previously
>>>>>>> to SchemaRDD).
>>>>>>>
>>>>>>>
>>>>>>
>>> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala 
>>>
>>>>>>> One thing that will break the data source API in 1.3 is the 
>>>>>>> location
>>>>>>> of
>>>>>>> types. Types were previously defined in sql.catalyst.types, and now
>>>>>> moved to
>>>>>>> sql.types. After 1.3, sql.catalyst is hidden from users, and all
>>>>>>> public
>>>>>> APIs
>>>>>>> have first class classes/objects defined in sql directly.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com
>>>>>> wrote:
>>>>>>>> Hey guys,
>>>>>>>>
>>>>>>>> How does this impact the data sources API?  I was planning on 
>>>>>>>> using
>>>>>>>> this for a project.
>>>>>>>>
>>>>>>>> +1 that many things from spark-sql / DataFrame is universally
>>>>>>>> desirable and useful.
>>>>>>>>
>>>>>>>> By the way, one thing that prevents the columnar compression stuff
>>> in
>>>>>>>> Spark SQL from being more useful is, at least from previous talks
>>>>>>>> with
>>>>>>>> Reynold and Michael et al., that the format was not designed for
>>>>>>>> persistence.
>>>>>>>>
>>>>>>>> I have a new project that aims to change that. It is a
>>>>>>>> zero-serialisation, high performance binary vector library,
>>> designed
>>>>>>>> from the outset to be a persistent storage friendly.  May be one
>>> day
>>>>>>>> it can replace the Spark SQL columnar compression.
>>>>>>>>
>>>>>>>> Michael told me this would be a lot of work, and recreates 
>>>>>>>> parts of
>>>>>>>> Parquet, but I think it's worth it.  LMK if you'd like more
>>> details.
>>>>>>>> -Evan
>>>>>>>>
>>>>>>>> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
>>>>>> wrote:
>>>>>>>>> Alright I have merged the patch (
>>>>>>>>> https://github.com/apache/spark/pull/4173
>>>>>>>>> ) since I don't see any strong opinions against it (as a matter
>>> of
>>>>>> fact
>>>>>>>>> most were for it). We can still change it if somebody lays out a
>>>>>> strong
>>>>>>>>> argument.
>>>>>>>>>
>>>>>>>>> On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>>>>>>>>> <matei.zaharia@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> The type alias means your methods can specify either type and
>>> they
>>>>>> will
>>>>>>>>>> work. It's just another name for the same type. But Scaladocs
>>> and
>>>>>> such
>>>>>>>>>> will
>>>>>>>>>> show DataFrame as the type.
>>>>>>>>>>
>>>>>>>>>> Matei
>>>>>>>>>>
>>>>>>>>>>> On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>>>>>>>>>> dirceu.semighini@gmail.com> wrote:
>>>>>>>>>>> Reynold,
>>>>>>>>>>> But with type alias we will have the same problem, right?
>>>>>>>>>>> If the methods doesn't receive schemardd anymore, we will have
>>>>>>>>>>> to
>>>>>>>>>>> change
>>>>>>>>>>> our code to migrade from schema to dataframe. Unless we have
>>> an
>>>>>>>>>>> implicit
>>>>>>>>>>> conversion between DataFrame and SchemaRDD
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>>>>>>>>>>>
>>>>>>>>>>>> Dirceu,
>>>>>>>>>>>>
>>>>>>>>>>>> That is not possible because one cannot overload return
>>> types.
>>>>>>>>>>>> SQLContext.parquetFile (and many other methods) needs to
>>> return
>>>>>> some
>>>>>>>>>> type,
>>>>>>>>>>>> and that type cannot be both SchemaRDD and DataFrame.
>>>>>>>>>>>>
>>>>>>>>>>>> In 1.3, we will create a type alias for DataFrame called
>>>>>>>>>>>> SchemaRDD
>>>>>>>>>>>> to
>>>>>>>>>> not
>>>>>>>>>>>> break source compatibility for Scala.
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>>>>>>>>>>>> dirceu.semighini@gmail.com> wrote:
>>>>>>>>>>>>
>>>>>>>>>>>>> Can't the SchemaRDD remain the same, but deprecated, and be
>>>>>> removed
>>>>>>>>>>>>> in
>>>>>>>>>> the
>>>>>>>>>>>>> release 1.5(+/- 1)  for example, and the new code been added
>>>>>>>>>>>>> to
>>>>>>>>>> DataFrame?
>>>>>>>>>>>>> With this, we don't impact in existing code for the next few
>>>>>>>>>>>>> releases.
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> 2015-01-27 0:02 GMT-02:00 Kushal Datta
>>>>>>>>>>>>> <kushal.datta@gmail.com>:
>>>>>>>>>>>>>
>>>>>>>>>>>>>> I want to address the issue that Matei raised about the
>>> heavy
>>>>>>>>>>>>>> lifting
>>>>>>>>>>>>>> required for a full SQL support. It is amazing that even
>>>>>>>>>>>>>> after
>>>>>> 30
>>>>>>>>>> years
>>>>>>>>>>>>> of
>>>>>>>>>>>>>> research there is not a single good open source columnar
>>>>>> database
>>>>>>>>>>>>>> like
>>>>>>>>>>>>>> Vertica. There is a column store option in MySQL, but it is
>>>>>>>>>>>>>> not
>>>>>>>>>>>>>> nearly
>>>>>>>>>>>>> as
>>>>>>>>>>>>>> sophisticated as Vertica or MonetDB. But there's a true
>>> need
>>>>>>>>>>>>>> for
>>>>>>>>>>>>>> such
>>>>>>>>>> a
>>>>>>>>>>>>>> system. I wonder why so and it's high time to change that.
>>>>>>>>>>>>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"
>>>>>>>>>>>>>> <sandy.ryza@cloudera.com>
>>>>>>>>>> wrote:
>>>>>>>>>>>>>>> Both SchemaRDD and DataFrame sound fine to me, though I
>>> like
>>>>>> the
>>>>>>>>>>>>> former
>>>>>>>>>>>>>>> slightly better because it's more descriptive.
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
>>>>>> covers,
>>>>>>>>>>>>>>> it
>>>>>>>>>>>>> would
>>>>>>>>>>>>>>> be more clear from a user-facing perspective to at least
>>>>>> choose a
>>>>>>>>>>>>> package
>>>>>>>>>>>>>>> name for it that omits "sql".
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> I would also be in favor of adding a separate Spark Schema
>>>>>> module
>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>> Spark
>>>>>>>>>>>>>>> SQL to rely on, but I imagine that might be too large a
>>>>>>>>>>>>>>> change
>>>>>> at
>>>>>>>>>> this
>>>>>>>>>>>>>>> point?
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> -Sandy
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> (Actually when we designed Spark SQL we thought of giving
>>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>>> another
>>>>>>>>>>>>>>> name,
>>>>>>>>>>>>>>>> like Spark Schema, but we decided to stick with SQL since
>>>>>>>>>>>>>>>> that
>>>>>>>>>>>>>>>> was
>>>>>>>>>>>>> the
>>>>>>>>>>>>>>> most
>>>>>>>>>>>>>>>> obvious use case to many users.)
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>> While it might be possible to move this concept to Spark
>>>>>>>>>>>>>>>>> Core
>>>>>>>>>>>>>>> long-term,
>>>>>>>>>>>>>>>> supporting structured data efficiently does require
>>> quite a
>>>>>> bit
>>>>>>>>>>>>>>>> of
>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>> infrastructure in Spark SQL, such as query planning and
>>>>>> columnar
>>>>>>>>>>>>>> storage.
>>>>>>>>>>>>>>>> The intent of Spark SQL though is to be more than a SQL
>>>>>>>>>>>>>>>> server
>>>>>>>>>>>>>>>> -- 
>>>>>>>>>>>>> it's
>>>>>>>>>>>>>>>> meant to be a library for manipulating structured data.
>>>>>>>>>>>>>>>> Since
>>>>>>>>>>>>>>>> this
>>>>>>>>>>>>> is
>>>>>>>>>>>>>>>> possible to build over the core API, it's pretty natural
>>> to
>>>>>>>>>>>>> organize it
>>>>>>>>>>>>>>>> that way, same as Spark Streaming is a library.
>>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
>>>>>> koert@tresata.com>
>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>> "The context is that SchemaRDD is becoming a common
>>> data
>>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>>> used
>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>>> used
>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> i agree. this to me also implies it belongs in spark
>>>>>>>>>>>>>>>>>> core,
>>>>>> not
>>>>>>>>>>>>> sql
>>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>>>>>>>>>>>>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> And in the off chance that anyone hasn't seen it yet,
>>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>> Jan.
>>>>>>>>>>>>> 13
>>>>>>>>>>>>>> Bay
>>>>>>>>>>>>>>>> Area
>>>>>>>>>>>>>>>>>>> Spark Meetup YouTube contained a wealth of background
>>>>>>>>>>>>> information
>>>>>>>>>>>>>> on
>>>>>>>>>>>>>>>> this
>>>>>>>>>>>>>>>>>>> idea (mostly from Patrick and Reynold :-).
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> ________________________________
>>>>>>>>>>>>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>>>>>>>>>>>>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>>>>>>>>>>>>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>>>>>>>>>>>>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>>>>>>>>>>>>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> One thing potentially not clear from this e-mail,
>>> there
>>>>>> will
>>>>>>>>>>>>>>>>>>> be
>>>>>>>>>>>>> a
>>>>>>>>>>>>>> 1:1
>>>>>>>>>>>>>>>>>>> correspondence where you can get an RDD to/from a
>>>>>> DataFrame.
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>>>>>>>>>>>>> rxin@databricks.com>
>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>>> Hi,
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
>>>>>>>>>>>>>>>>>>>> 1.3,
>>>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>> wanted
>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>>>>> get the community's opinion.
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> The context is that SchemaRDD is becoming a common
>>> data
>>>>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>>>> used
>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>>>>>>>>>>>>>>>>>>>> used
>>>>>> for
>>>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API.
>>> We
>>>>>> also
>>>>>>>>>>>>> expect
>>>>>>>>>>>>>>>> more
>>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>>> more users to be programming directly against
>>> SchemaRDD
>>>>>> API
>>>>>>>>>>>>> rather
>>>>>>>>>>>>>>>> than
>>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>>> core RDD API. SchemaRDD, through its less commonly
>>> used
>>>>>> DSL
>>>>>>>>>>>>>>> originally
>>>>>>>>>>>>>>>>>>>> designed for writing test cases, always has the
>>>>>>>>>>>>>>>>>>>> data-frame
>>>>>>>>>>>>>>>>>>>> like
>>>>>>>>>>>>>> API.
>>>>>>>>>>>>>>>> In
>>>>>>>>>>>>>>>>>>>> 1.3, we are redesigning the API to make the API
>>> usable
>>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>> end
>>>>>>>>>>>>>>> users.
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> There are two motivations for the renaming:
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> 1. DataFrame seems to be a more self-evident name
>>> than
>>>>>>>>>>>>> SchemaRDD.
>>>>>>>>>>>>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
>>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>>> anymore
>>>>>>>>>>>>>>>> (even
>>>>>>>>>>>>>>>>>>>> though it would contain some RDD functions like map,
>>>>>>>>>>>>>>>>>>>> flatMap,
>>>>>>>>>>>>>> etc),
>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>>> calling it Schema*RDD* while it is not an RDD is
>>> highly
>>>>>>>>>>>>> confusing.
>>>>>>>>>>>>>>>>>>> Instead.
>>>>>>>>>>>>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
>>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>>> methods.
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> My understanding is that very few users program
>>>>>>>>>>>>>>>>>>>> directly
>>>>>>>>>>>>> against
>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>>> SchemaRDD API at the moment, because they are not
>>> well
>>>>>>>>>>>>> documented.
>>>>>>>>>>>>>>>>>>> However,
>>>>>>>>>>>>>>>>>>>> oo maintain backward compatibility, we can create a
>>>>>>>>>>>>>>>>>>>> type
>>>>>>>>>>>>>>>>>>>> alias
>>>>>>>>>>>>>>>> DataFrame
>>>>>>>>>>>>>>>>>>>> that is still named SchemaRDD. This will maintain
>>>>>>>>>>>>>>>>>>>> source
>>>>>>>>>>>>>>> compatibility
>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>> Scala. That said, we will have to update all existing
>>>>>>>>>>>>> materials to
>>>>>>>>>>>>>>> use
>>>>>>>>>>>>>>>>>>>> DataFrame rather than SchemaRDD.
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>> --------------------------------------------------------------------- 
>>>>>>
>>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>> --------------------------------------------------------------------- 
>>>>>>
>>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>> --------------------------------------------------------------------- 
>>>>>>
>>>>>>>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>> ---------------------------------------------------------------------
>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>
>>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>
​

--------------040105060102050604080703--

From dev-return-11364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jan 29 22:01:56 2015
Return-Path: <dev-return-11364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13BE217574
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 Jan 2015 22:01:56 +0000 (UTC)
Received: (qmail 10882 invoked by uid 500); 29 Jan 2015 22:01:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10804 invoked by uid 500); 29 Jan 2015 22:01:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10792 invoked by uid 99); 29 Jan 2015 22:01:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 22:01:54 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 Jan 2015 22:01:29 +0000
Received: by mail-ie0-f174.google.com with SMTP id vy18so194732iec.5
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 13:59:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=Y3R3JJh7ZXQjLtDy5MmMGMlhSps0gDoHVV/ZXBN7u0I=;
        b=Eyr5Vn5g3wk56OF25wQ2NsUnGzA47hv4Vb16zG1uBVarrZkBecMv8q57zH7QxvPam5
         Llv48rIwHxzOcse7rW8ITYNxxfqWGl7cQNj1iJMevvkq+QGa5sxagzxQ0iq9jVnmTWd2
         Zfx5JtERo3424zY/LczQNfkW5o4Hnmm8pvgKw2ksCMGGGCyUjDLoZufi3FczEvsQbtcj
         TIl9n6QyPQmW46eOxTNyCykkZl0CSR7typx14FAq7AAPrdzGVm78HejoAiLRGHTIln7e
         ImFw6FJG7xbkD2V1b/Fmg/0X3Jmm3cODtzF+d3VrAPo53kHBhjgOIPjfZVNLMxwQgB0o
         fN0g==
X-Received: by 10.66.224.199 with SMTP id re7mr3900364pac.65.1422568751699;
        Thu, 29 Jan 2015 13:59:11 -0800 (PST)
Received: from [192.168.1.168] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id sl5sm8704037pbc.45.2015.01.29.13.59.10
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 29 Jan 2015 13:59:10 -0800 (PST)
Message-ID: <54CAAD2D.8030407@gmail.com>
Date: Thu, 29 Jan 2015 13:59:09 -0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: Koert Kuipers <koert@tresata.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: renaming SchemaRDD -> DataFrame
References: <CABPQxsutcmwwZOgWUj36HcWsUPz+S9003MNZZoxcNZTx6Fnwaw@mail.gmail.com>	<CANjHi9rj8aFrZWohEoNU+-gmmRDsRp8OQdUOLxjnEuf3ZS+Tug@mail.gmail.com>	<CAO4-Pq9-+sVSf-R4EcdcH_2ytb74fGLVevY_T3OCezt1UXiJCA@mail.gmail.com>	<CAPh_B=bvP4upD7SNg-vbYRQGhjfLYTtzOSfysq4-g_DHxyaS7g@mail.gmail.com>	<CAO4-Pq9G1KbsM3UR2O_cfUKruKcGcfVsDG5HxYjEPR1eD4VSww@mail.gmail.com>	<D4F5E5C6-14C9-4566-806D-E94AA7D5B0C7@gmail.com>	<CAPh_B=ZdTZv_8zFjE3YH0qPvEqnaSfaRW=DgbjEy-V9BjvxiBA@mail.gmail.com>	<CAN6Vra04d2x+aP6-L4eC-bx3_hEiJZCok7DhtJTGYsD2VE1=Ew@mail.gmail.com>	<CAPh_B=anVaSM5=bEBvNpV+0PLonW9y4NG3tsOpeXjxTJVRRAOA@mail.gmail.com>	<CAN6Vra05sqS3GBccyj6GUj6iCWqxNwG7BDc1j3+s9wZTjeiTDw@mail.gmail.com>	<CAPh_B=YtiASt6Cue+9dEFHCuTv7HnVx3i9BjTaCEPRx82QoTPA@mail.gmail.com>	<CABjXkq6f+5L507810fE51Y9v0UsAqSYMVCih0agSw2epiJ-ijA@mail.gmail.com>	<CAN6Vra1Pr9ouShTMzp8fmDfaK_E51=gAkxhnGhdC1GpJDYO54w@mail.gmail.com> <CANx3uAjOvdKeUx1cuadSW3vy77=1juykbQRGbVbDt36hjif+vg@mail.gmail.com>
In-Reply-To: <CANx3uAjOvdKeUx1cuadSW3vy77=1juykbQRGbVbDt36hjif+vg@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, when a DataFrame is cached in memory, it's stored in an efficient 
columnar format. And you can also easily persist it on disk using 
Parquet, which is also columnar.

Cheng

On 1/29/15 1:24 PM, Koert Kuipers wrote:
> to me the word DataFrame does come with certain expectations. one of them
> is that the data is stored columnar. in R data.frame internally uses a list
> of sequences i think, but since lists can have labels its more like a
> SortedMap[String, Array[_]]. this makes certain operations very cheap (such
> as adding a column).
>
> in Spark the closest thing would be a data structure where per Partition
> the data is also stored columnar. does spark SQL already use something like
> that? Evan mentioned "Spark SQL columnar compression", which sounds like
> it. where can i find that?
>
> thanks
>
> On Thu, Jan 29, 2015 at 2:32 PM, Evan Chan <velvia.github@gmail.com> wrote:
>
>> +1.... having proper NA support is much cleaner than using null, at
>> least the Java null.
>>
>> On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks <evan.sparks@gmail.com>
>> wrote:
>>> You've got to be a little bit careful here. "NA" in systems like R or
>> pandas
>>> may have special meaning that is distinct from "null".
>>>
>>> See, e.g. http://www.r-bloggers.com/r-na-vs-null/
>>>
>>>
>>>
>>> On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>>> Isn't that just "null" in SQL?
>>>>
>>>> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan <velvia.github@gmail.com>
>>>> wrote:
>>>>
>>>>> I believe that most DataFrame implementations out there, like Pandas,
>>>>> supports the idea of missing values / NA, and some support the idea of
>>>>> Not Meaningful as well.
>>>>>
>>>>> Does Row support anything like that?  That is important for certain
>>>>> applications.  I thought that Row worked by being a mutable object,
>>>>> but haven't looked into the details in a while.
>>>>>
>>>>> -Evan
>>>>>
>>>>> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin <rxin@databricks.com>
>>>>> wrote:
>>>>>> It shouldn't change the data source api at all because data sources
>>>>> create
>>>>>> RDD[Row], and that gets converted into a DataFrame automatically
>>>>> (previously
>>>>>> to SchemaRDD).
>>>>>>
>>>>>>
>>>>>
>> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
>>>>>> One thing that will break the data source API in 1.3 is the location
>>>>>> of
>>>>>> types. Types were previously defined in sql.catalyst.types, and now
>>>>> moved to
>>>>>> sql.types. After 1.3, sql.catalyst is hidden from users, and all
>>>>>> public
>>>>> APIs
>>>>>> have first class classes/objects defined in sql directly.
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan <velvia.github@gmail.com
>>>>> wrote:
>>>>>>> Hey guys,
>>>>>>>
>>>>>>> How does this impact the data sources API?  I was planning on using
>>>>>>> this for a project.
>>>>>>>
>>>>>>> +1 that many things from spark-sql / DataFrame is universally
>>>>>>> desirable and useful.
>>>>>>>
>>>>>>> By the way, one thing that prevents the columnar compression stuff
>> in
>>>>>>> Spark SQL from being more useful is, at least from previous talks
>>>>>>> with
>>>>>>> Reynold and Michael et al., that the format was not designed for
>>>>>>> persistence.
>>>>>>>
>>>>>>> I have a new project that aims to change that.  It is a
>>>>>>> zero-serialisation, high performance binary vector library,
>> designed
>>>>>>> from the outset to be a persistent storage friendly.  May be one
>> day
>>>>>>> it can replace the Spark SQL columnar compression.
>>>>>>>
>>>>>>> Michael told me this would be a lot of work, and recreates parts of
>>>>>>> Parquet, but I think it's worth it.  LMK if you'd like more
>> details.
>>>>>>> -Evan
>>>>>>>
>>>>>>> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin <rxin@databricks.com>
>>>>> wrote:
>>>>>>>> Alright I have merged the patch (
>>>>>>>> https://github.com/apache/spark/pull/4173
>>>>>>>> ) since I don't see any strong opinions against it (as a matter
>> of
>>>>> fact
>>>>>>>> most were for it). We can still change it if somebody lays out a
>>>>> strong
>>>>>>>> argument.
>>>>>>>>
>>>>>>>> On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>>>>>>>> <matei.zaharia@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> The type alias means your methods can specify either type and
>> they
>>>>> will
>>>>>>>>> work. It's just another name for the same type. But Scaladocs
>> and
>>>>> such
>>>>>>>>> will
>>>>>>>>> show DataFrame as the type.
>>>>>>>>>
>>>>>>>>> Matei
>>>>>>>>>
>>>>>>>>>> On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho <
>>>>>>>>> dirceu.semighini@gmail.com> wrote:
>>>>>>>>>> Reynold,
>>>>>>>>>> But with type alias we will have the same problem, right?
>>>>>>>>>> If the methods doesn't receive schemardd anymore, we will have
>>>>>>>>>> to
>>>>>>>>>> change
>>>>>>>>>> our code to migrade from schema to dataframe. Unless we have
>> an
>>>>>>>>>> implicit
>>>>>>>>>> conversion between DataFrame and SchemaRDD
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> 2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:
>>>>>>>>>>
>>>>>>>>>>> Dirceu,
>>>>>>>>>>>
>>>>>>>>>>> That is not possible because one cannot overload return
>> types.
>>>>>>>>>>> SQLContext.parquetFile (and many other methods) needs to
>> return
>>>>> some
>>>>>>>>> type,
>>>>>>>>>>> and that type cannot be both SchemaRDD and DataFrame.
>>>>>>>>>>>
>>>>>>>>>>> In 1.3, we will create a type alias for DataFrame called
>>>>>>>>>>> SchemaRDD
>>>>>>>>>>> to
>>>>>>>>> not
>>>>>>>>>>> break source compatibility for Scala.
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho <
>>>>>>>>>>> dirceu.semighini@gmail.com> wrote:
>>>>>>>>>>>
>>>>>>>>>>>> Can't the SchemaRDD remain the same, but deprecated, and be
>>>>> removed
>>>>>>>>>>>> in
>>>>>>>>> the
>>>>>>>>>>>> release 1.5(+/- 1)  for example, and the new code been added
>>>>>>>>>>>> to
>>>>>>>>> DataFrame?
>>>>>>>>>>>> With this, we don't impact in existing code for the next few
>>>>>>>>>>>> releases.
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> 2015-01-27 0:02 GMT-02:00 Kushal Datta
>>>>>>>>>>>> <kushal.datta@gmail.com>:
>>>>>>>>>>>>
>>>>>>>>>>>>> I want to address the issue that Matei raised about the
>> heavy
>>>>>>>>>>>>> lifting
>>>>>>>>>>>>> required for a full SQL support. It is amazing that even
>>>>>>>>>>>>> after
>>>>> 30
>>>>>>>>> years
>>>>>>>>>>>> of
>>>>>>>>>>>>> research there is not a single good open source columnar
>>>>> database
>>>>>>>>>>>>> like
>>>>>>>>>>>>> Vertica. There is a column store option in MySQL, but it is
>>>>>>>>>>>>> not
>>>>>>>>>>>>> nearly
>>>>>>>>>>>> as
>>>>>>>>>>>>> sophisticated as Vertica or MonetDB. But there's a true
>> need
>>>>>>>>>>>>> for
>>>>>>>>>>>>> such
>>>>>>>>> a
>>>>>>>>>>>>> system. I wonder why so and it's high time to change that.
>>>>>>>>>>>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"
>>>>>>>>>>>>> <sandy.ryza@cloudera.com>
>>>>>>>>> wrote:
>>>>>>>>>>>>>> Both SchemaRDD and DataFrame sound fine to me, though I
>> like
>>>>> the
>>>>>>>>>>>> former
>>>>>>>>>>>>>> slightly better because it's more descriptive.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
>>>>> covers,
>>>>>>>>>>>>>> it
>>>>>>>>>>>> would
>>>>>>>>>>>>>> be more clear from a user-facing perspective to at least
>>>>> choose a
>>>>>>>>>>>> package
>>>>>>>>>>>>>> name for it that omits "sql".
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> I would also be in favor of adding a separate Spark Schema
>>>>> module
>>>>>>>>>>>>>> for
>>>>>>>>>>>>> Spark
>>>>>>>>>>>>>> SQL to rely on, but I imagine that might be too large a
>>>>>>>>>>>>>> change
>>>>> at
>>>>>>>>> this
>>>>>>>>>>>>>> point?
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> -Sandy
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia <
>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> (Actually when we designed Spark SQL we thought of giving
>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>> another
>>>>>>>>>>>>>> name,
>>>>>>>>>>>>>>> like Spark Schema, but we decided to stick with SQL since
>>>>>>>>>>>>>>> that
>>>>>>>>>>>>>>> was
>>>>>>>>>>>> the
>>>>>>>>>>>>>> most
>>>>>>>>>>>>>>> obvious use case to many users.)
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia <
>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>> While it might be possible to move this concept to Spark
>>>>>>>>>>>>>>>> Core
>>>>>>>>>>>>>> long-term,
>>>>>>>>>>>>>>> supporting structured data efficiently does require
>> quite a
>>>>> bit
>>>>>>>>>>>>>>> of
>>>>>>>>>>>> the
>>>>>>>>>>>>>>> infrastructure in Spark SQL, such as query planning and
>>>>> columnar
>>>>>>>>>>>>> storage.
>>>>>>>>>>>>>>> The intent of Spark SQL though is to be more than a SQL
>>>>>>>>>>>>>>> server
>>>>>>>>>>>>>>> --
>>>>>>>>>>>> it's
>>>>>>>>>>>>>>> meant to be a library for manipulating structured data.
>>>>>>>>>>>>>>> Since
>>>>>>>>>>>>>>> this
>>>>>>>>>>>> is
>>>>>>>>>>>>>>> possible to build over the core API, it's pretty natural
>> to
>>>>>>>>>>>> organize it
>>>>>>>>>>>>>>> that way, same as Spark Streaming is a library.
>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers <
>>>>> koert@tresata.com>
>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>> "The context is that SchemaRDD is becoming a common
>> data
>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>> used
>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>> used
>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>> i agree. this to me also implies it belongs in spark
>>>>>>>>>>>>>>>>> core,
>>>>> not
>>>>>>>>>>>> sql
>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak <
>>>>>>>>>>>>>>>>> michaelmalak@yahoo.com.invalid> wrote:
>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> And in the off chance that anyone hasn't seen it yet,
>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>> Jan.
>>>>>>>>>>>> 13
>>>>>>>>>>>>> Bay
>>>>>>>>>>>>>>> Area
>>>>>>>>>>>>>>>>>> Spark Meetup YouTube contained a wealth of background
>>>>>>>>>>>> information
>>>>>>>>>>>>> on
>>>>>>>>>>>>>>> this
>>>>>>>>>>>>>>>>>> idea (mostly from Patrick and Reynold :-).
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> ________________________________
>>>>>>>>>>>>>>>>>> From: Patrick Wendell <pwendell@gmail.com>
>>>>>>>>>>>>>>>>>> To: Reynold Xin <rxin@databricks.com>
>>>>>>>>>>>>>>>>>> Cc: "dev@spark.apache.org" <dev@spark.apache.org>
>>>>>>>>>>>>>>>>>> Sent: Monday, January 26, 2015 4:01 PM
>>>>>>>>>>>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> One thing potentially not clear from this e-mail,
>> there
>>>>> will
>>>>>>>>>>>>>>>>>> be
>>>>>>>>>>>> a
>>>>>>>>>>>>> 1:1
>>>>>>>>>>>>>>>>>> correspondence where you can get an RDD to/from a
>>>>> DataFrame.
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin <
>>>>>>>>>>>> rxin@databricks.com>
>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>> Hi,
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
>>>>>>>>>>>>>>>>>>> 1.3,
>>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>> wanted
>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>>>> get the community's opinion.
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> The context is that SchemaRDD is becoming a common
>> data
>>>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>>> used
>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>>>>>>>>>>>>>>>>>>> used
>>>>> for
>>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API.
>> We
>>>>> also
>>>>>>>>>>>> expect
>>>>>>>>>>>>>>> more
>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>> more users to be programming directly against
>> SchemaRDD
>>>>> API
>>>>>>>>>>>> rather
>>>>>>>>>>>>>>> than
>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>> core RDD API. SchemaRDD, through its less commonly
>> used
>>>>> DSL
>>>>>>>>>>>>>> originally
>>>>>>>>>>>>>>>>>>> designed for writing test cases, always has the
>>>>>>>>>>>>>>>>>>> data-frame
>>>>>>>>>>>>>>>>>>> like
>>>>>>>>>>>>> API.
>>>>>>>>>>>>>>> In
>>>>>>>>>>>>>>>>>>> 1.3, we are redesigning the API to make the API
>> usable
>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>> end
>>>>>>>>>>>>>> users.
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> There are two motivations for the renaming:
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> 1. DataFrame seems to be a more self-evident name
>> than
>>>>>>>>>>>> SchemaRDD.
>>>>>>>>>>>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>> anymore
>>>>>>>>>>>>>>> (even
>>>>>>>>>>>>>>>>>>> though it would contain some RDD functions like map,
>>>>>>>>>>>>>>>>>>> flatMap,
>>>>>>>>>>>>> etc),
>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>> calling it Schema*RDD* while it is not an RDD is
>> highly
>>>>>>>>>>>> confusing.
>>>>>>>>>>>>>>>>>> Instead.
>>>>>>>>>>>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>> methods.
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> My understanding is that very few users program
>>>>>>>>>>>>>>>>>>> directly
>>>>>>>>>>>> against
>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>> SchemaRDD API at the moment, because they are not
>> well
>>>>>>>>>>>> documented.
>>>>>>>>>>>>>>>>>> However,
>>>>>>>>>>>>>>>>>>> oo maintain backward compatibility, we can create a
>>>>>>>>>>>>>>>>>>> type
>>>>>>>>>>>>>>>>>>> alias
>>>>>>>>>>>>>>> DataFrame
>>>>>>>>>>>>>>>>>>> that is still named SchemaRDD. This will maintain
>>>>>>>>>>>>>>>>>>> source
>>>>>>>>>>>>>> compatibility
>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>> Scala. That said, we will have to update all existing
>>>>>>>>>>>> materials to
>>>>>>>>>>>>>> use
>>>>>>>>>>>>>>>>>>> DataFrame rather than SchemaRDD.
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>> For additional commands, e-mail:
>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>> ---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>
>>>>>>>>>
>>>>>>
>>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 30 00:32:10 2015
Return-Path: <dev-return-11365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1362817A73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 Jan 2015 00:32:10 +0000 (UTC)
Received: (qmail 35296 invoked by uid 500); 30 Jan 2015 00:32:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35220 invoked by uid 500); 30 Jan 2015 00:32:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35203 invoked by uid 99); 30 Jan 2015 00:32:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:32:08 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of skacanski@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:31:43 +0000
Received: by mail-qg0-f41.google.com with SMTP id q108so36057006qgd.0
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 16:31:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lezkibQCbYFaaA3YQVFUOcrjko1bfbGEDxkHRaNf7hs=;
        b=csv0Hb6uILNBbZL5oTtrPXSFTu2rNNZCjMWbr0jQ0qOUdzy+n/4hvggPQS825MmPRm
         if+AcRAOu2uToGsQh/KLdlaVOULSIT/urfb7s8cMDWBwRqGOFJ5P8CaO7ucggRY5stGz
         ViAc2Oc+aAuVsOiD4Lp31SSppwh7CNu2zSsie2RHn05zLGqAH2lJyMDUbrpKAk85zmVV
         CbaBfFlDhnVogqahDrsdZYhUy6B2X+S3H1Wl4rgpbN6hOim23j838w4h8ym4cO84rWE+
         +KeaAYbKspz+jFkcDdlBARdKZrxOMYd9/Qzx7eMSm0QwlusQgwQNN4bwIQ4o3SEsVvih
         57SA==
MIME-Version: 1.0
X-Received: by 10.140.108.134 with SMTP id j6mr3213656qgf.68.1422577901681;
 Thu, 29 Jan 2015 16:31:41 -0800 (PST)
Received: by 10.96.35.225 with HTTP; Thu, 29 Jan 2015 16:31:41 -0800 (PST)
In-Reply-To: <CAPh_B=ZH2KV7C7B6WRyJFmZWarfJpVjCPYHcHBzJHvmeTa5okg@mail.gmail.com>
References: <1422564320008-10356.post@n3.nabble.com>
	<CAPh_B=ZH2KV7C7B6WRyJFmZWarfJpVjCPYHcHBzJHvmeTa5okg@mail.gmail.com>
Date: Thu, 29 Jan 2015 19:31:41 -0500
Message-ID: <CAFWiikqUd=fL7jFaQfgk4O=Ch-MsF1R9gFd-FJbaR0E74chQQg@mail.gmail.com>
Subject: Re: How to speed PySpark to match Scala/Java performance
From: Sasha Kacanski <skacanski@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: rtshadow <pastuszka.przemyslaw@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a53587ec063050dd3ba36
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a53587ec063050dd3ba36
Content-Type: text/plain; charset=UTF-8

Hi Reynold,
In my project I want to use Python API too.
When you mention DF's are we talking about pandas or this is something
internal to spark py api.
If you could elaborate a bit on this or point me to alternate documentation.
Thanks much --sasha

On Thu, Jan 29, 2015 at 4:12 PM, Reynold Xin <rxin@databricks.com> wrote:

> Once the data frame API is released for 1.3, you can write your thing in
> Python and get the same performance. It can't express everything, but for
> basic things like projection, filter, join, aggregate and simple numeric
> computation, it should work pretty well.
>
>
> On Thu, Jan 29, 2015 at 12:45 PM, rtshadow <pastuszka.przemyslaw@gmail.com
> >
> wrote:
>
> > Hi,
> >
> > In my company, we've been trying to use PySpark to run ETLs on our data.
> > Alas, it turned out to be terribly slow compared to Java or Scala API
> > (which
> > we ended up using to meet performance criteria).
> >
> > To be more quantitative, let's consider simple case:
> > I've generated test file (848MB): /seq 1 100000000 > /tmp/test/
> >
> > and tried to run simple computation on it, which includes three steps:
> read
> > -> multiply each row by 2 -> take max
> > Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
> > Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/
> >
> > Here are the results of this simple benchmark:
> > CPython - 59s
> > PyPy - 26s
> > Scala version - 7s
> >
> > I didn't dig into what exactly contributes to execution times of CPython
> /
> > PyPy, but it seems that serialization / deserialization, when sending
> data
> > to the worker may be the issue.
> > I know some guys already have been asking about using Jython
> > (
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658
> > ,
> >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html
> > ),
> > but it seems, that no one have really done this with Spark.
> > It looks like performance gain from using jython can be huge - you
> wouldn't
> > need to spawn PythonWorkers, all the code would be just executed inside
> > SparkExecutor JVM, using python code compiled to java bytecode. Do you
> > think
> > that's possible to achieve? Do you see any obvious obstacles? Of course,
> > jython doesn't have C extensions, but if one doesn't need them, then it
> > should fit here nicely.
> >
> > I'm willing to try to marry Spark with Jython and see how it goes.
> >
> > What do you think about this?
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>



-- 
Aleksandar Kacanski

--001a113a53587ec063050dd3ba36--

From dev-return-11366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 30 00:44:12 2015
Return-Path: <dev-return-11366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE80317ACC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 Jan 2015 00:44:12 +0000 (UTC)
Received: (qmail 69443 invoked by uid 500); 30 Jan 2015 00:44:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69366 invoked by uid 500); 30 Jan 2015 00:44:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69351 invoked by uid 99); 30 Jan 2015 00:44:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:44:10 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:44:06 +0000
Received: by mail-qa0-f41.google.com with SMTP id bm13so17908305qab.0
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 16:41:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=f72MYjNmjHNIm2e97xKS0YRxNiru4lPWTfOZYi4P6hU=;
        b=iH+5jplgJZKI6BC6cEdozuzgjG58F+pitrFYFOcUgNLXGrtGUL7Vh3q8dSbY42jB4C
         Ubph5WtFsDP5z/G9xh/vsc/U0xRADIAUnXXzHoXnRHb3nhxGzw7cng5WcR35GC9GIEK0
         ZhPQzhWs2UtlICzq7dYKAswxfLPpU3htlBNLQ2T0CXjqe9wyZPmt7azvvypU6zWPHpTa
         gvrQwzkRw5n7c+8gD5anZzw4tumTW7mRmj+FS69Z4Z6v8GGyvJZMOpx4rz2rRT5oe4R0
         cvZ1h3kF1gjLYd2jWYDrhWCe58ZXTtR2NtVevQWrU4MNpdjtKPM1R/HqbFdDs6qwK0Ih
         8q6w==
X-Gm-Message-State: ALoCoQka+kSS0I4FxLsel2XLq8H4yHr2OJxBxd9RDBPceUsZ4icjlqCj2E4hPRpvEUvQi7AXbYM3
X-Received: by 10.140.39.84 with SMTP id u78mr3305588qgu.63.1422578515433;
 Thu, 29 Jan 2015 16:41:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.155.132 with HTTP; Thu, 29 Jan 2015 16:41:35 -0800 (PST)
In-Reply-To: <CAFWiikqUd=fL7jFaQfgk4O=Ch-MsF1R9gFd-FJbaR0E74chQQg@mail.gmail.com>
References: <1422564320008-10356.post@n3.nabble.com> <CAPh_B=ZH2KV7C7B6WRyJFmZWarfJpVjCPYHcHBzJHvmeTa5okg@mail.gmail.com>
 <CAFWiikqUd=fL7jFaQfgk4O=Ch-MsF1R9gFd-FJbaR0E74chQQg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 29 Jan 2015 16:41:35 -0800
Message-ID: <CAPh_B=aJHbRkkmZsWoNxGjZXFMRgv+rGsx2q=MAmQcr0pAhBBw@mail.gmail.com>
Subject: Re: How to speed PySpark to match Scala/Java performance
To: Sasha Kacanski <skacanski@gmail.com>
Cc: rtshadow <pastuszka.przemyslaw@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1296813f8db050dd3df6a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1296813f8db050dd3df6a
Content-Type: text/plain; charset=UTF-8

It is something like this: https://issues.apache.org/jira/browse/SPARK-5097

On the master branch, we have a Pandas like API already.


On Thu, Jan 29, 2015 at 4:31 PM, Sasha Kacanski <skacanski@gmail.com> wrote:

> Hi Reynold,
> In my project I want to use Python API too.
> When you mention DF's are we talking about pandas or this is something
> internal to spark py api.
> If you could elaborate a bit on this or point me to alternate
> documentation.
> Thanks much --sasha
>
> On Thu, Jan 29, 2015 at 4:12 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Once the data frame API is released for 1.3, you can write your thing in
>> Python and get the same performance. It can't express everything, but for
>> basic things like projection, filter, join, aggregate and simple numeric
>> computation, it should work pretty well.
>>
>>
>> On Thu, Jan 29, 2015 at 12:45 PM, rtshadow <
>> pastuszka.przemyslaw@gmail.com>
>> wrote:
>>
>> > Hi,
>> >
>> > In my company, we've been trying to use PySpark to run ETLs on our data.
>> > Alas, it turned out to be terribly slow compared to Java or Scala API
>> > (which
>> > we ended up using to meet performance criteria).
>> >
>> > To be more quantitative, let's consider simple case:
>> > I've generated test file (848MB): /seq 1 100000000 > /tmp/test/
>> >
>> > and tried to run simple computation on it, which includes three steps:
>> read
>> > -> multiply each row by 2 -> take max
>> > Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
>> > Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/
>> >
>> > Here are the results of this simple benchmark:
>> > CPython - 59s
>> > PyPy - 26s
>> > Scala version - 7s
>> >
>> > I didn't dig into what exactly contributes to execution times of
>> CPython /
>> > PyPy, but it seems that serialization / deserialization, when sending
>> data
>> > to the worker may be the issue.
>> > I know some guys already have been asking about using Jython
>> > (
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658
>> > ,
>> >
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html
>> > ),
>> > but it seems, that no one have really done this with Spark.
>> > It looks like performance gain from using jython can be huge - you
>> wouldn't
>> > need to spawn PythonWorkers, all the code would be just executed inside
>> > SparkExecutor JVM, using python code compiled to java bytecode. Do you
>> > think
>> > that's possible to achieve? Do you see any obvious obstacles? Of course,
>> > jython doesn't have C extensions, but if one doesn't need them, then it
>> > should fit here nicely.
>> >
>> > I'm willing to try to marry Spark with Jython and see how it goes.
>> >
>> > What do you think about this?
>> >
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>
>
>
> --
> Aleksandar Kacanski
>

--001a11c1296813f8db050dd3df6a--

From dev-return-11367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jan 30 00:46:21 2015
Return-Path: <dev-return-11367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C938B17ADB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 Jan 2015 00:46:21 +0000 (UTC)
Received: (qmail 74026 invoked by uid 500); 30 Jan 2015 00:46:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73949 invoked by uid 500); 30 Jan 2015 00:46:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73937 invoked by uid 99); 30 Jan 2015 00:46:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:46:20 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of skacanski@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 Jan 2015 00:46:16 +0000
Received: by mail-qa0-f48.google.com with SMTP id v8so17831982qal.7
        for <dev@spark.apache.org>; Thu, 29 Jan 2015 16:44:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0bu7sD09DU8VDJ9TE9RZYqnwS+HQCsxcCTc9T2DX1oY=;
        b=zEy88Zi4dSNUODiRsYgP3XCAIGub+ZzqwcRuqtq7Y5goQZM5XwdlRKP5LcTHydloyk
         HL8BT03U45cz27W+sLfR+0WlPpaP1bnPgEXxLK5zhKyH26abYFo0ELDQyQg2wJSH8aGu
         Ki2r5YVdm5PFqzx2QZmjP5cTkNqxGoRKqIfi/KmdR7v5+jV8ALMTMtKV+PXi8Tmh2k2J
         2hOlsGhPN4VD5Xw4WQftaeZq2K6XK1Flj9AX8Eab4HdTFLwozimkfr1jPrbb9P0GmgQQ
         /RyA3qNqWj5IyZe8qonOciS8Q/qpDJf/lsYVJtB3VJe6YB+BSLE70IamkAsm9W6bTIMD
         soGA==
MIME-Version: 1.0
X-Received: by 10.224.95.71 with SMTP id c7mr7328250qan.70.1422578665588; Thu,
 29 Jan 2015 16:44:25 -0800 (PST)
Received: by 10.96.35.225 with HTTP; Thu, 29 Jan 2015 16:44:25 -0800 (PST)
In-Reply-To: <CAPh_B=aJHbRkkmZsWoNxGjZXFMRgv+rGsx2q=MAmQcr0pAhBBw@mail.gmail.com>
References: <1422564320008-10356.post@n3.nabble.com>
	<CAPh_B=ZH2KV7C7B6WRyJFmZWarfJpVjCPYHcHBzJHvmeTa5okg@mail.gmail.com>
	<CAFWiikqUd=fL7jFaQfgk4O=Ch-MsF1R9gFd-FJbaR0E74chQQg@mail.gmail.com>
	<CAPh_B=aJHbRkkmZsWoNxGjZXFMRgv+rGsx2q=MAmQcr0pAhBBw@mail.gmail.com>
Date: Thu, 29 Jan 2015 19:44:25 -0500
Message-ID: <CAFWiikq4FMVU97DD=vO_OWETsaN74TOLVJ1kanJT5xga19+vpA@mail.gmail.com>
Subject: Re: How to speed PySpark to match Scala/Java performance
From: Sasha Kacanski <skacanski@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: rtshadow <pastuszka.przemyslaw@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1bba4070d96050dd3e845
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bba4070d96050dd3e845
Content-Type: text/plain; charset=UTF-8

thanks for quick reply, I will check the link.
Hopefully, with conversion to py3, or 3.4 we could take advantage of
asyncio and other cool new stuff ...

On Thu, Jan 29, 2015 at 7:41 PM, Reynold Xin <rxin@databricks.com> wrote:

> It is something like this:
> https://issues.apache.org/jira/browse/SPARK-5097
>
> On the master branch, we have a Pandas like API already.
>
>
> On Thu, Jan 29, 2015 at 4:31 PM, Sasha Kacanski <skacanski@gmail.com>
> wrote:
>
>> Hi Reynold,
>> In my project I want to use Python API too.
>> When you mention DF's are we talking about pandas or this is something
>> internal to spark py api.
>> If you could elaborate a bit on this or point me to alternate
>> documentation.
>> Thanks much --sasha
>>
>> On Thu, Jan 29, 2015 at 4:12 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Once the data frame API is released for 1.3, you can write your thing in
>>> Python and get the same performance. It can't express everything, but for
>>> basic things like projection, filter, join, aggregate and simple numeric
>>> computation, it should work pretty well.
>>>
>>>
>>> On Thu, Jan 29, 2015 at 12:45 PM, rtshadow <
>>> pastuszka.przemyslaw@gmail.com>
>>> wrote:
>>>
>>> > Hi,
>>> >
>>> > In my company, we've been trying to use PySpark to run ETLs on our
>>> data.
>>> > Alas, it turned out to be terribly slow compared to Java or Scala API
>>> > (which
>>> > we ended up using to meet performance criteria).
>>> >
>>> > To be more quantitative, let's consider simple case:
>>> > I've generated test file (848MB): /seq 1 100000000 > /tmp/test/
>>> >
>>> > and tried to run simple computation on it, which includes three steps:
>>> read
>>> > -> multiply each row by 2 -> take max
>>> > Code in python: /sc.textFile("/tmp/test").map(lambda x: x * 2).max()/
>>> > Code in scala: /sc.textFile("/tmp/test").map(x => x * 2).max()/
>>> >
>>> > Here are the results of this simple benchmark:
>>> > CPython - 59s
>>> > PyPy - 26s
>>> > Scala version - 7s
>>> >
>>> > I didn't dig into what exactly contributes to execution times of
>>> CPython /
>>> > PyPy, but it seems that serialization / deserialization, when sending
>>> data
>>> > to the worker may be the issue.
>>> > I know some guys already have been asking about using Jython
>>> > (
>>> >
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658
>>> > ,
>>> >
>>> >
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html
>>> > ),
>>> > but it seems, that no one have really done this with Spark.
>>> > It looks like performance gain from using jython can be huge - you
>>> wouldn't
>>> > need to spawn PythonWorkers, all the code would be just executed inside
>>> > SparkExecutor JVM, using python code compiled to java bytecode. Do you
>>> > think
>>> > that's possible to achieve? Do you see any obvious obstacles? Of
>>> course,
>>> > jython doesn't have C extensions, but if one doesn't need them, then it
>>> > should fit here nicely.
>>> >
>>> > I'm willing to try to marry Spark with Jython and see how it goes.
>>> >
>>> > What do you think about this?
>>> >
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> >
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-speed-PySpark-to-match-Scala-Java-performance-tp10356.html
>>> > Sent from the Apache Spark Developers List mailing list archive at
>>> > Nabble.com.
>>> >
>>> > ---------------------------------------------------------------------
>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>> >
>>> >
>>>
>>
>>
>>
>> --
>> Aleksandar Kacanski
>>
>
>


-- 
Aleksandar Kacanski

--001a11c1bba4070d96050dd3e845--

From dev-return-11368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 00:07:15 2015
Return-Path: <dev-return-11368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C30217F84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 00:07:15 +0000 (UTC)
Received: (qmail 71230 invoked by uid 500); 31 Jan 2015 00:07:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71153 invoked by uid 500); 31 Jan 2015 00:07:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71140 invoked by uid 99); 31 Jan 2015 00:07:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 00:07:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 00:07:09 +0000
Received: by mail-ig0-f180.google.com with SMTP id b16so6491715igk.1
        for <dev@spark.apache.org>; Fri, 30 Jan 2015 16:06:48 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=mvpK+pJFvmPh/loIawuPGXaaNE4wm0ergfrpKlfirJ8=;
        b=Pcatoq7l81Ls928hZQsKrcwxkCdE/0Y1C4HlZxiOTxsOYWzn6eYKk+hoePV5SlOBqp
         xnDRNk2dLSZWKZEw2K8A7RNeru62BP6YOMdqPkacHKIqGN8sT3gEfMK/eI8lj8QRmq4R
         qEYJQ0LwOVH2QMr9bd38K2cEu7sBamn6nLRjjXVHsqSg6ryUpAEaZPKNmIwwXFvkM9A3
         LZAdmwUWMBEDg2BIsBa5rtlNu/U/OQLq24NVQP15M+SXJyyv9vR08OP6wqFcDLxp3YKp
         wc3uAZhwR36I5bDPaP49I3ZaNAWEeYEJo7oMQkyVGnG/GribiTg6eOAu16iFaoSavHdw
         S4wg==
X-Gm-Message-State: ALoCoQnOaRsuvAbSX28KeNcHQ50WgGPELqVnudAVKw5VG8Q79PBqkNUT2XS7AdNGSEL48/0QvtQM
MIME-Version: 1.0
X-Received: by 10.43.144.9 with SMTP id jo9mr8743331icc.74.1422662808456; Fri,
 30 Jan 2015 16:06:48 -0800 (PST)
Received: by 10.36.118.135 with HTTP; Fri, 30 Jan 2015 16:06:48 -0800 (PST)
In-Reply-To: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
Date: Fri, 30 Jan 2015 16:06:48 -0800
Message-ID: <CAAOnQ7vsxJeatD3jO+mv=HkxZNX84c8o-LaxwE1daucDn0T8Kg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (non-binding)

Ran spark-shell and Scala jobs on top of yarn (using the hadoop-2.4 tarball).

There's a very slight behavioral change in the API. This code now throws an NPE:

  new SparkConf().setIfMissing("foo", null)

It worked before. It's probably fine, though, since `SparkConf.set`
would throw an NPE before for the same arguments, so it's unlikely
anyone was relying on that behavior.


On Wed, Jan 28, 2015 at 2:06 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
>
> The tag to be voted on is v1.2.1-rc1 (commit b77f876):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1062/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/
>
> Changes from rc1:
> This has no code changes from RC1. Only minor changes to the release script.
>
> Please vote on releasing this package as Apache Spark 1.2.1!
>
> The vote is open until  Saturday, January 31, at 10:04 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
>
> For a list of fixes in this release, see http://s.apache.org/Mpn.
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 02:16:52 2015
Return-Path: <dev-return-11369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87DD517530
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 02:16:52 +0000 (UTC)
Received: (qmail 4130 invoked by uid 500); 31 Jan 2015 02:16:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4048 invoked by uid 500); 31 Jan 2015 02:16:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4032 invoked by uid 99); 31 Jan 2015 02:16:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 02:16:51 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ankitsoni9@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 02:16:46 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id D3347125D1F8
	for <dev@spark.apache.org>; Fri, 30 Jan 2015 18:15:56 -0800 (PST)
Date: Fri, 30 Jan 2015 19:15:55 -0700 (MST)
From: ankits <ankitsoni9@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422670555626-10366.post@n3.nabble.com>
Subject: Get size of rdd in memory
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I want to benchmark the memory savings by using the in-memory columnar
storage for schemardds (using cacheTable) vs caching the SchemaRDD directly.
It would be really helpful to be able to query this from the spark-shell or
jobs directly. Could a dev point me to the way to do this? From what I
understand i will need a reference to the block manager, or something like
RDDInfo.fromRdd(rdd).memSize.

I could use reflection or whatever to override the private access modifiers.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Get-size-of-rdd-in-memory-tp10366.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 03:55:39 2015
Return-Path: <dev-return-11370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9517F17756
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 03:55:39 +0000 (UTC)
Received: (qmail 645 invoked by uid 500); 31 Jan 2015 03:55:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 576 invoked by uid 500); 31 Jan 2015 03:55:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 561 invoked by uid 99); 31 Jan 2015 03:55:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 03:55:38 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 03:55:33 +0000
Received: by mail-pa0-f43.google.com with SMTP id eu11so59807043pac.2
        for <dev@spark.apache.org>; Fri, 30 Jan 2015 19:54:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type;
        bh=y6YfBjn4BDxkakbFd8UczptUihsKvMXmoxkSr2KRuH8=;
        b=e3rh9vSrUuZt8zRmVVBmCWJDiAG4BfjnvVfQJZdMf9pDd9YYAp7H2iJAWvOIsTm9fs
         KgYgLSI1EW5krtYmUGBeCfc5PTM0fJnu4W8r8LpSnHKwqz2wz7HOqq7wwaxmPJJPoUir
         R8IuI8+QrzdmapfFEqZZJEyglO+lqgqDyNcLw30IsZv6SsDsh9aJGKoo+cjhJVCWHG5K
         O3V7CDuT3mhf45BAxtJte3bGVnaYPhV7V4DwiINytMBPyGr/Tj0Jx4bYGwuId0J6nWhf
         MOHVyru6jm9xT2DMEt+nEyLbWTcR6JEk8N0k2VvUBLwxpGjv/Y7d55BuerBjbJqZVmpA
         U/UA==
X-Received: by 10.70.134.142 with SMTP id pk14mr13629830pdb.17.1422676467606;
        Fri, 30 Jan 2015 19:54:27 -0800 (PST)
Received: from [192.168.1.168] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id tn2sm4404755pbc.15.2015.01.30.19.54.26
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 30 Jan 2015 19:54:26 -0800 (PST)
Message-ID: <54CC51F1.3030604@gmail.com>
Date: Fri, 30 Jan 2015 19:54:25 -0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: ankits <ankitsoni9@gmail.com>, dev@spark.apache.org
Subject: Re: Get size of rdd in memory
References: <1422670555626-10366.post@n3.nabble.com>
In-Reply-To: <1422670555626-10366.post@n3.nabble.com>
Content-Type: multipart/alternative;
 boundary="------------010206080805040605050701"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------010206080805040605050701
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

Here is a toy |spark-shell| session snippet that can show the memory 
consumption difference:

|import  org.apache.spark.sql.SQLContext
import  sc._

val  sqlContext  =  new  SQLContext(sc)
import  sqlContext._

setConf("spark.sql.shuffle.partitions","1")

case  class  KV(key:Int, value:String)

parallelize(1  to1024).map(i =>KV(i, i.toString)).toSchemaRDD.cache().count()
parallelize(1  to1024).map(i =>KV(i, i.toString)).cache().count()
|

You may see the result from the storage page of the web UI. It suggests 
the in-memory columnar version uses 11.6KB while the raw RDD version 
uses 76.6KB on my machine.

Not quite sure how to do the comparison programmatically. You can track 
the data source of the “Size in Memory” field showed in the web UI 
storage tab.

Cheng

On 1/30/15 6:15 PM, ankits wrote:

> Hi,
>
> I want to benchmark the memory savings by using the in-memory columnar
> storage for schemardds (using cacheTable) vs caching the SchemaRDD directly.
> It would be really helpful to be able to query this from the spark-shell or
> jobs directly. Could a dev point me to the way to do this? From what I
> understand i will need a reference to the block manager, or something like
> RDDInfo.fromRdd(rdd).memSize.
>
> I could use reflection or whatever to override the private access modifiers.
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Get-size-of-rdd-in-memory-tp10366.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
​

--------------010206080805040605050701--

From dev-return-11371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 08:56:37 2015
Return-Path: <dev-return-11371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47F6917B10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 08:56:37 +0000 (UTC)
Received: (qmail 37377 invoked by uid 500); 31 Jan 2015 08:56:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37308 invoked by uid 500); 31 Jan 2015 08:56:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37296 invoked by uid 99); 31 Jan 2015 08:56:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 08:56:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lafernando@gmail.com designates 209.85.215.52 as permitted sender)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 08:56:30 +0000
Received: by mail-la0-f52.google.com with SMTP id ge10so27889298lab.11
        for <dev@spark.apache.org>; Sat, 31 Jan 2015 00:55:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=FcX1oND8N7pdZCu6sxDrjP8WH+RC4rdITM753N6o1kA=;
        b=tAIHEljqISawofubOOkWXfXGKYHhoENVnIPnlTvVNv5ofNQfcrqRV6sXqt1+YzUTP/
         TQ/PaJbFlEMoLWSaKGjBog3O2gWBlCBYooQrjF0S7jVt/fE4KmOMR472Fep8u+wAJQOf
         KYSjbmY9WNBDdUk/wHx1q+vzfNz3YfnKZcNDuzJIcaNdaz7vA8vQ1rdAfel1TKR9ylz8
         NZAKQjb8Gbb/++Bu0tpQhnNYz3zopsFWxTbkcQmj7K9w5ZdtPq9vXpJf8CYHj2BL4Qyn
         7EKh4RNVhNrIl3ULi8hEtNmo2iBfhhnQUJDZ50lfmVToEhNBJ8R4/VqGqB/Zd4nDAGTf
         ACsg==
MIME-Version: 1.0
X-Received: by 10.152.5.38 with SMTP id p6mr10169862lap.91.1422694524840; Sat,
 31 Jan 2015 00:55:24 -0800 (PST)
Received: by 10.114.3.177 with HTTP; Sat, 31 Jan 2015 00:55:24 -0800 (PST)
Date: Sat, 31 Jan 2015 14:25:24 +0530
Message-ID: <CACpu_pP5i7v8wQjrD4iknektzKbSdVHShAWrOeRewYy4_boS9g@mail.gmail.com>
Subject: Custom Cluster Managers / Standalone Recovery Mode in Spark
From: Anjana Fernando <lafernando@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01419acec6f2e6050deee1af
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01419acec6f2e6050deee1af
Content-Type: text/plain; charset=UTF-8

Hi everyone,

I've been experimenting, and somewhat of a newbie for Spark. I was
wondering, if there is any way, that I can use a custom cluster manager
implementation with Spark. Basically, as I understood, at the moment, the
inbuilt modes supported are with standalone, Mesos and  Yarn. My
requirement is basically a simple clustering solution with high
availability of the master. I don't want to use a separate Zookeeper
cluster, since this would complicate my deployment, but rather, I would
like to use something like Hazelcast, which has a peer-to-peer cluster
coordination implementation.

I found that, there is already this JIRA [1], which requests for a custom
persistence engine, I guess for storing state information. So basically,
what I would want to do is, use Hazelcast to use for leader election, to
make an existing node the master, and to lookup the state information from
the distributed memory. Appreciate any help on how to archive this. And if
it useful for a wider audience, hopefully I can contribute this back to the
project.

[1] https://issues.apache.org/jira/browse/SPARK-1180

Cheers,
Anjana.

--089e01419acec6f2e6050deee1af--

From dev-return-11372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 09:03:06 2015
Return-Path: <dev-return-11372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BBE1317B2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 09:03:06 +0000 (UTC)
Received: (qmail 40859 invoked by uid 500); 31 Jan 2015 09:03:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40782 invoked by uid 500); 31 Jan 2015 09:03:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40771 invoked by uid 99); 31 Jan 2015 09:03:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 09:03:05 +0000
X-ASF-Spam-Status: No, hits=4.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of neilp23@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 09:03:01 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 3D3171261B4B
	for <dev@spark.apache.org>; Sat, 31 Jan 2015 01:02:12 -0800 (PST)
Date: Sat, 31 Jan 2015 02:02:10 -0700 (MST)
From: nl32 <neilp23@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422694930812-10369.post@n3.nabble.com>
Subject: Disabling eviction warnings when using sbt
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_60726_1817733600.1422694930813"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_60726_1817733600.1422694930813
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

I am trying to disabling eviction warnings when using sbt, such as these:
[warn] There may be incompatibilities among your library dependencies.[warn]
Here are some of the libraries that were evicted:[warn] 	*
com.typesafe.sbt:sbt-git:0.6.1 -> 0.6.2[warn] 	*
com.typesafe.sbt:sbt-site:0.7.0 -> 0.7.1[warn] Run 'evicted' to see detailed
eviction warnings
I am using this line to disable eviction warnings:
evictionWarningOptions in update :=
EvictionWarningOptions.default.withWarnTransitiveEvictions(false).withWarnDirectEvictions(false).withWarnScalaVersionEviction(false)
as found here https://github.com/sbt/sbt/issues/1636#issuecomment-57498141.
But I am not sure where it belongs in project/ScalaBuild.scalaI've tried
putting it in SparkBuild.sharedSettings, but it doesn't work. Can anyone
help?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Disabling-eviction-warnings-when-using-sbt-tp10369.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_60726_1817733600.1422694930813--

From dev-return-11373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jan 31 19:58:10 2015
Return-Path: <dev-return-11373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A1B017759
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 Jan 2015 19:58:10 +0000 (UTC)
Received: (qmail 15343 invoked by uid 500); 31 Jan 2015 19:58:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15268 invoked by uid 500); 31 Jan 2015 19:58:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15255 invoked by uid 99); 31 Jan 2015 19:58:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 19:58:09 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of martin.weindel@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 Jan 2015 19:58:04 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id B93FA126A7B3
	for <dev@spark.apache.org>; Sat, 31 Jan 2015 11:57:15 -0800 (PST)
Date: Sat, 31 Jan 2015 12:57:14 -0700 (MST)
From: MartinWeindel <martin.weindel@gmail.com>
To: dev@spark.apache.org
Message-ID: <1422734234030-10370.post@n3.nabble.com>
In-Reply-To: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
References: <CABPQxsuvbeCK53qqggYfK0PsuhnmNgprHKf3v7ZT85eKOcKVBg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.1 (RC2)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

FYI: Spark 1.2.1rc2 does not work on Windows!

On creating a Spark context you get following log output on my Windows
machine:
INFO  org.apache.spark.SparkEnv:59 - Registering BlockManagerMaster
ERROR org.apache.spark.util.Utils:75 - Failed to create local root dir in
C:\Users\mweindel\AppData\Local\Temp\. Ignoring this directory.
ERROR org.apache.spark.storage.DiskBlockManager:75 - Failed to create any
local dir.

I have already located the cause. A newly added function chmod700() in
org.apache.util.Utils uses functionality which only works on a Unix file
system.

See also pull request [https://github.com/apache/spark/pull/4299] for my
suggestion how to resolve the issue.

Best regards,

Martin Weindel



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10370.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


