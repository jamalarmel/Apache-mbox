From dev-return-10582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 00:26:32 2014
Return-Path: <dev-return-10582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4166D1065F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 00:26:32 +0000 (UTC)
Received: (qmail 55496 invoked by uid 500); 1 Dec 2014 00:26:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55440 invoked by uid 500); 1 Dec 2014 00:26:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55428 invoked by uid 99); 1 Dec 2014 00:26:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 00:26:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 00:26:03 +0000
Received: by mail-ie0-f181.google.com with SMTP id tp5so8436989ieb.40
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 16:24:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=uuhh4txAQgVlB95j+TX3fvcwgkaL3Py93t1ljiypdOk=;
        b=KoPjyPyImM0MNFRHHPI7dMtrDVc7TcRDVeJ3jUlcasVAPyXDOPW5rnwvgUbspSr9iv
         8cGEMjyZKx2dvwWk0Rgbxd+Cvl1PDY5Yhm9i5VCpgAGf434Ojp1UrE14pLCfYL8OTLw9
         cRZCwCNoLMu29uAxn95C3bbSmoQe1YuCeSGrF8KADL5aP3LvMVahLx4xvelrN5H3XQ8t
         RrDcbvICizI9bJB/lHIo7On9m9AdA2ytbWMqYTqf1+L3JGwjSDxKe/12nZLlt3EPFXPH
         Kh9WmIh3LMl495nL7OncmBhY3q/c34NaU6INsi2E6S96fwavh4SO2bCzw2AZFtPPgy/B
         MtGQ==
X-Received: by 10.51.17.107 with SMTP id gd11mr1440392igd.45.1417393471949;
 Sun, 30 Nov 2014 16:24:31 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
 <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 01 Dec 2014 00:24:31 +0000
Message-ID: <CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Patrick Wendell <pwendell@gmail.com>, Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136000e67107e05091ca2e9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136000e67107e05091ca2e9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

   - currently the docs only contain information about building with maven,
   and even then don=E2=80=99t cover many important cases

 All other points aside, I just want to point out that the docs document
both how to use Maven and SBT and clearly state
<https://github.com/apache/spark/blob/master/docs/building-spark.md#buildin=
g-with-sbt>
that Maven is the =E2=80=9Cbuild of reference=E2=80=9D while SBT may be pre=
ferable for
day-to-day development.

I believe the main reason most people miss this documentation is that,
though it=E2=80=99s up-to-date on GitHub, it has=E2=80=99t been published y=
et to the docs
site. It should go out with the 1.2 release.

Improvements to the documentation on building Spark belong here:
https://github.com/apache/spark/blob/master/docs/building-spark.md

If there are clear recommendations that come out of this thread but are not
in that doc, they should be added in there. Other, less important details
may possibly be better suited for the Contributing to Spark
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
guide.

Nick
=E2=80=8B

On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com> wrote=
:

> Hey Ryan,
>
> A few more things here. You should feel free to send patches to
> Jenkins to test them, since this is the reference environment in which
> we regularly run tests. This is the normal workflow for most
> developers and we spend a lot of effort provisioning/maintaining a
> very large jenkins cluster to allow developers access this resource. A
> common development approach is to locally run tests that you've added
> in a patch, then send it to jenkins for the full run, and then try to
> debug locally if you see specific unanticipated test failures.
>
> One challenge we have is that given the proliferation of OS versions,
> Java versions, Python versions, ulimits, etc. there is a combinatorial
> number of environments in which tests could be run. It is very hard in
> some cases to figure out post-hoc why a given test is not working in a
> specific environment. I think a good solution here would be to use a
> standardized docker container for running Spark tests and asking folks
> to use that locally if they are trying to run all of the hundreds of
> Spark tests.
>
> Another solution would be to mock out every system interaction in
> Spark's tests including e.g. filesystem interactions to try and reduce
> variance across environments. However, that seems difficult.
>
> As the number of developers of Spark increases, it's definitely a good
> idea for us to invest in developer infrastructure including things
> like snapshot releases, better documentation, etc. Thanks for bringing
> this up as a pain point.
>
> - Patrick
>
>
> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
> <ryan.blake.williams@gmail.com> wrote:
> > thanks for the info, Matei and Brennon. I will try to switch my workflo=
w
> to
> > using sbt. Other potential action items:
> >
> > - currently the docs only contain information about building with maven=
,
> > and even then don't cover many important cases, as I described in my
> > previous email. If SBT is as much better as you've described then that
> > should be made much more obvious. Wasn't it the case recently that ther=
e
> > was only a page about building with SBT, and not one about building wit=
h
> > maven? Clearer messaging around this needs to exist in the documentatio=
n,
> > not just on the mailing list, imho.
> >
> > - +1 to better distinguishing between unit and integration tests, havin=
g
> > separate scripts for each, improving documentation around common
> workflows,
> > expectations of brittleness with each kind of test, advisability of jus=
t
> > relying on Jenkins for certain kinds of tests to not waste too much tim=
e,
> > etc. Things like the compiler crash should be discussed in the
> > documentation, not just in the mailing list archives, if new contributo=
rs
> > are likely to run into them through no fault of their own.
> >
> > - What is the algorithm you use to decide what tests you might have
> broken?
> > Can we codify it in some scripts that other people can use?
> >
> >
> >
> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.com=
>
> > wrote:
> >
> >> Hi Ryan,
> >>
> >> As a tip (and maybe this isn't documented well), I normally use SBT fo=
r
> >> development to avoid the slow build process, and use its interactive
> >> console to run only specific tests. The nice advantage is that SBT can
> keep
> >> the Scala compiler loaded and JITed across builds, making it faster to
> >> iterate. To use it, you can do the following:
> >>
> >> - Start the SBT interactive console with sbt/sbt
> >> - Build your assembly by running the "assembly" target in the assembly
> >> project: assembly/assembly
> >> - Run all the tests in one module: core/test
> >> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
> (this
> >> also supports tab completion)
> >>
> >> Running all the tests does take a while, and I usually just rely on
> >> Jenkins for that once I've run the tests for the things I believed my
> patch
> >> could break. But this is because some of them are integration tests
> (e.g.
> >> DistributedSuite, which creates multi-process mini-clusters). Many of
> the
> >> individual suites run fast without requiring this, however, so you can
> pick
> >> the ones you want. Perhaps we should find a way to tag them so people
> can
> >> do a "quick-test" that skips the integration ones.
> >>
> >> The assembly builds are annoying but they only take about a minute for
> me
> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
> required
> >> for some of the "integration" tests (which launch new processes), but
> I'd
> >> recommend doing it all the time anyway since it would be very confusin=
g
> to
> >> run those with an old assembly. The Scala compiler crash issue can als=
o
> be
> >> a problem, but I don't see it very often with SBT. If it happens, I ex=
it
> >> SBT and do sbt clean.
> >>
> >> Anyway, this is useful feedback and I think we should try to improve
> some
> >> of these suites, but hopefully you can also try the faster SBT process=
.
> At
> >> the end of the day, if we want integration tests, the whole test proce=
ss
> >> will take an hour, but most of the developers I know leave that to
> Jenkins
> >> and only run individual tests locally before submitting a patch.
> >>
> >> Matei
> >>
> >>
> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
> >> ryan.blake.williams@gmail.com> wrote:
> >> >
> >> > In the course of trying to make contributions to Spark, I have had a
> lot
> >> of
> >> > trouble running Spark's tests successfully. The main pain points I'v=
e
> >> > experienced are:
> >> >
> >> >    1) frequent, spurious test failures
> >> >    2) high latency of running tests
> >> >    3) difficulty running specific tests in an iterative fashion
> >> >
> >> > Here is an example series of failures that I encountered this weeken=
d
> >> > (along with footnote links to the console output from each and
> >> > approximately how long each took):
> >> >
> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not see=
n
> >> > before.
> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
> >> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]:
> BroadcastSuite
> >> > passed, but scala compiler crashed on the "catalyst" project.
> >> > - `mvn clean`: some attempts to run earlier commands (that previousl=
y
> >> > didn't crash the compiler) all result in the same compiler crash.
> >> Previous
> >> > discussion on this list implies this can only be solved by a `mvn
> clean`
> >> > [4].
> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-cle=
an,
> >> > BroadcastSuite can't run because assembly is not built.
> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages abo=
ut
> >> > version mismatches and python 2.6. The machine this ran on has pytho=
n
> >> 2.7,
> >> > so I don't know what that's about.
> >> > - `./dev/run-tests` again [7]: "too many open files" errors in sever=
al
> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
> this is
> >> > not enough, but only some of the time? I increased it to 8192 and
> tried
> >> > again.
> >> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
> seems
> >> to
> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
> October
> >> 14;
> >> > not sure how I'm seeing it now. In any case, switched to Python 2.6
> and
> >> > installed unittest2, and python/run-tests seems to be unblocked.
> >> > - `./dev/run-tests` again [10]: finally passes!
> >> >
> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
> trivial
> >> > changes added on (that I wanted to test before sending out a PR), on=
 a
> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
> >> >
> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
> >> commands
> >> > from the same repo state:
> >> >
> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
> seen
> >> > this one before on this machine and am guessing it actually occurs
> every
> >> > time.
> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
> more
> >> > time from ceb6281, and saw the same failure.
> >> >
> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
> >> narrow
> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on =
my
> >> mac,
> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
> >> used),
> >> > and it passed [16], so the failure seems specific to my linux
> >> machine/arch.
> >> >
> >> > At this point I believe that my changes don't break any tests (the
> >> > YarnClusterSuite failure on my linux presumably not being... "real")=
,
> >> and I
> >> > am ready to send out a PR. Whew!
> >> >
> >> > However, reflecting on the 5 or 6 distinct failure-modes represented
> >> above:
> >> >
> >> > - One of them (too many files open), is something I can (and did,
> >> > hopefully) fix once and for all. It cost me an ~hour this time
> >> (approximate
> >> > time of running ./dev/run-tests) and a few hours other times when I
> >> didn't
> >> > fully understand/fix it. It doesn't happen deterministically (why?),
> but
> >> > does happen somewhat frequently to people, having been discussed on
> the
> >> > user list multiple times [17] and on SO [18]. Maybe some note in the
> >> > documentation advising people to check their ulimit makes sense?
> >> > - One of them (unittest2 must be installed for python 2.6) was
> supposedly
> >> > fixed upstream of the commits I tested here; I don't know why I'm
> still
> >> > running into it. This cost me a few hours of running `./dev/run-test=
s`
> >> > multiple times to see if it was transient, plus some time researchin=
g
> and
> >> > working around it.
> >> > - The original BroadcastSuite failure cost me a few hours and went
> away
> >> > before I'd even run `mvn clean`.
> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a f=
ew
> >> > hours of running `./dev/run-tests` in different ways before deciding
> >> that,
> >> > as usual, there was no way around it and that I'd need to run `mvn
> clean`
> >> > and start running tests from scratch.
> >> > - The YarnClusterSuite failures on my linux box have cost me hours o=
f
> >> > trying to figure out whether they're my fault. I've seen them many
> times
> >> > over the past weeks/months, plus or minus other failures that have
> come
> >> and
> >> > gone, and was especially befuddled by them when I was seeing a
> disjoint
> >> set
> >> > of reproducible failures on my mac [19] (the triaging of which
> involved
> >> > dozens of runs of `./dev/run-tests`).
> >> >
> >> > While I'm interested in digging into each of these issues, I also
> want to
> >> > discuss the frequency with which I've run into issues like these.
> This is
> >> > unfortunately not the first time in recent months that I've spent da=
ys
> >> > playing spurious-test-failure whack-a-mole with a 60-90min
> dev/run-tests
> >> > iteration time, which is no fun! So I am wondering/thinking:
> >> >
> >> > - Do other people experience this level of flakiness from spark test=
s?
> >> > - Do other people bother running dev/run-tests locally, or just let
> >> Jenkins
> >> > do it during the CR process?
> >> > - Needing to run a full assembly post-clean just to continue running
> one
> >> > specific test case feels especially wasteful, and the failure output
> when
> >> > naively attempting to run a specific test without having built an
> >> assembly
> >> > jar is not always clear about what the issue is or how to fix it; ev=
en
> >> the
> >> > fact that certain tests require "building the world" is not somethin=
g
> I
> >> > would have expected, and has cost me hours of confusion.
> >> >    - Should a person running spark tests assume that they must build
> an
> >> > assembly JAR before running anything?
> >> >    - Are there some proper "unit" tests that are actually
> self-contained
> >> /
> >> > able to be run without building an assembly jar?
> >> >    - Can we better document/demarcate which tests have which
> >> dependencies?
> >> >    - Is there something finer-grained than building an assembly JAR
> that
> >> > is sufficient in some cases?
> >> >        - If so, can we document that?
> >> >        - If not, can we move to a world of finer-grained dependencie=
s
> for
> >> > some of these?
> >> > - Leaving all of these spurious failures aside, the process of
> assembling
> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
> >> typically,
> >> > respectively). I would guess that there are dozens (hundreds?) of
> people
> >> > who build a Spark assembly from various ToTs on any given day, and w=
ho
> >> all
> >> > wait on the exact same compilation / assembly steps to occur.
> Expanding
> >> on
> >> > the recent work to publish nightly snapshots [20], can we do a bette=
r
> job
> >> > caching/sharing compilation artifacts at a more granular level
> (pre-built
> >> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
> more
> >> > granular maven modules, plus the previous two?), or otherwise save
> some
> >> of
> >> > the considerable amount of redundant compilation work that I had to =
do
> >> over
> >> > the course of my odyssey this weekend?
> >> >
> >> > Ramping up on most projects involves some amount of supplementing th=
e
> >> > documentation with trial and error to figure out what to run, which
> >> > "errors" are real errors and which can be ignored, etc., but
> navigating
> >> > that minefield on Spark has proved especially challenging and
> >> > time-consuming for me. Some of that comes directly from scala's
> >> relatively
> >> > slow compilation times and immature build-tooling ecosystem, but tha=
t
> is
> >> > the world we live in and it would be nice if Spark took the
> alleviation
> >> of
> >> > the resulting pain more seriously, as one of the more interesting an=
d
> >> > well-known large scala projects around right now. The official
> >> > documentation around how to build different subsets of the codebase =
is
> >> > somewhat sparse [21], and there have been many mixed [22] accounts
> [23]
> >> on
> >> > this mailing list about preferred ways to build on mvn vs. sbt (none
> of
> >> > which has made it into official documentation, as far as I've seen).
> >> > Expecting new contributors to piece together all of this received
> >> > folk-wisdom about how to build/test in a sane way by trawling mailin=
g
> >> list
> >> > archives seems suboptimal.
> >> >
> >> > Thanks for reading, looking forward to hearing your ideas!
> >> >
> >> > -Ryan
> >> >
> >> > P.S. Is "best practice" for emailing this list to not incorporate an=
y
> >> HTML
> >> > in the body? It seems like all of the archives I've seen strip it ou=
t,
> >> but
> >> > other people have used it and gmail displays it.
> >> >
> >> >
> >> > [1]
> >> > https://gist.githubusercontent.com/ryan-
> williams/8a162367c4dc157d2479/
> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
> >> > (57 mins)
> >> > [2]
> >> > https://gist.githubusercontent.com/ryan-
> williams/8a162367c4dc157d2479/
> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> >> > (6 mins)
> >> > [3]
> >> > https://gist.githubusercontent.com/ryan-
> williams/8a162367c4dc157d2479/
> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
> >> 20pass%20test,%20fail%20subsequent%20compile
> >> > (4 mins)
> >> > [4]
> >> > https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dweb=
&
> >> cd=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-
> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
> >> DataTypeConversions-scala-td17083.html&ei=3DaRF6VJrpNKr-
> >> iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=3D
> >> zDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
> >> > [5]
> >> > https://gist.githubusercontent.com/ryan-
> williams/8a162367c4dc157d2479/
> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
> >> 20clean,%20need%20dependencies%20built
> >> > [6]
> >> > https://gist.githubusercontent.com/ryan-
> williams/8a162367c4dc157d2479/
> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
> >> 20post%20clean
> >> > (50 mins)
> >> > [7]
> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
> >> > (1hr)
> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> >> > [12]
> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
> >> file-gistfile1-txt-L853
> >> > (~90 mins)
> >> > [13]
> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
> >> file-gistfile1-txt-L852
> >> > (91 mins)
> >> > [14]
> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
> >> file-gistfile1-txt-L854
> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> >> > [17]
> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
> >> > [18]
> >> > http://stackoverflow.com/questions/25707629/why-does-
> >> spark-job-fail-with-too-many-open-files
> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
> >> > [21]
> >> > https://spark.apache.org/docs/latest/building-with-maven.
> >> html#spark-tests-in-maven
> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
> >> > [23]
> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3=
E
> >>
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1136000e67107e05091ca2e9--

From dev-return-10583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 01:37:28 2014
Return-Path: <dev-return-10583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8730610788
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 01:37:28 +0000 (UTC)
Received: (qmail 13248 invoked by uid 500); 1 Dec 2014 01:37:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13173 invoked by uid 500); 1 Dec 2014 01:37:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13161 invoked by uid 99); 1 Dec 2014 01:37:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 01:37:27 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 01:37:23 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so23038033wiv.11
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 17:36:16 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=cVl+izz/5yhOn/eyQXcSf4mpMzy6AbUNW8YIWwJNqQo=;
        b=fJI1vRGYU7HzYHrT2QlAkntNZOfpILipotL6caYSgEthBjyX9zT8mm0c6vtdsc/v50
         xUz5LbIBS8OYXJJA4+LZw6km3p0f3XSuLnNEiGvtzhWdH6RDMPpIti0RghHIvjJGlrRM
         vAPsNAnUBE6MQmnMbuSM4dRwEqHx9qbvlJs39lwnf8YeQ8uA7WTlDm2SPByHTIwMkBV7
         DeFx43hQ7Wx4bRuy1qGHoM2lLUa2NbSNYjGWAKyQfhz97pBAJNItuhJc44gbIoC3U+XW
         bs+zFJCEX7CMgmz45YGrx6p5KxRZ87qWLJmAeBv4QI+GqW4tu+WN0hSkHIBlyXEoNP/3
         Hesw==
X-Gm-Message-State: ALoCoQmY6Aq+IA7MEDnpOHv0YVVshFGsoa3aW5UJ7gbiykctemqACvDCa1Scs86Z7a9V+HmFK+dm
MIME-Version: 1.0
X-Received: by 10.180.96.162 with SMTP id dt2mr80612456wib.66.1417397776028;
 Sun, 30 Nov 2014 17:36:16 -0800 (PST)
Received: by 10.216.68.137 with HTTP; Sun, 30 Nov 2014 17:36:15 -0800 (PST)
In-Reply-To: <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
Date: Sun, 30 Nov 2014 17:36:15 -0800
Message-ID: <CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Mark Hamstra <mark@clearstorydata.com>
To: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0442866ef22d0505091da2ae
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442866ef22d0505091da2ae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

>
> - Start the SBT interactive console with sbt/sbt
> - Build your assembly by running the "assembly" target in the assembly
> project: assembly/assembly
> - Run all the tests in one module: core/test
> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite (thi=
s
> also supports tab completion)


The equivalent using Maven:

- Start zinc
- Build your assembly using the mvn "package" or "install" target
("install" is actually the equivalent of SBT's "publishLocal") -- this step
is the first step in
http://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in=
-maven
- Run all the tests in one module: mvn -pl core test
- Run a specific suite: mvn -pl core
-DwildcardSuites=3Dorg.apache.spark.rdd.RDDSuite test (the -pl option isn't
strictly necessary if you don't mind waiting for Maven to scan through all
the other sub-projects only to do nothing; and, of course, it needs to be
something other than "core" if the test you want to run is in another
sub-project.)

You also typically want to carry along in each subsequent step any relevant
command line options you added in the "package"/"install" step.

On Sun, Nov 30, 2014 at 3:06 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Hi Ryan,
>
> As a tip (and maybe this isn't documented well), I normally use SBT for
> development to avoid the slow build process, and use its interactive
> console to run only specific tests. The nice advantage is that SBT can ke=
ep
> the Scala compiler loaded and JITed across builds, making it faster to
> iterate. To use it, you can do the following:
>
> - Start the SBT interactive console with sbt/sbt
> - Build your assembly by running the "assembly" target in the assembly
> project: assembly/assembly
> - Run all the tests in one module: core/test
> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite (thi=
s
> also supports tab completion)
>
> Running all the tests does take a while, and I usually just rely on
> Jenkins for that once I've run the tests for the things I believed my pat=
ch
> could break. But this is because some of them are integration tests (e.g.
> DistributedSuite, which creates multi-process mini-clusters). Many of the
> individual suites run fast without requiring this, however, so you can pi=
ck
> the ones you want. Perhaps we should find a way to tag them so people  ca=
n
> do a "quick-test" that skips the integration ones.
>
> The assembly builds are annoying but they only take about a minute for me
> on a MacBook Pro with SBT warmed up. The assembly is actually only requir=
ed
> for some of the "integration" tests (which launch new processes), but I'd
> recommend doing it all the time anyway since it would be very confusing t=
o
> run those with an old assembly. The Scala compiler crash issue can also b=
e
> a problem, but I don't see it very often with SBT. If it happens, I exit
> SBT and do sbt clean.
>
> Anyway, this is useful feedback and I think we should try to improve some
> of these suites, but hopefully you can also try the faster SBT process. A=
t
> the end of the day, if we want integration tests, the whole test process
> will take an hour, but most of the developers I know leave that to Jenkin=
s
> and only run individual tests locally before submitting a patch.
>
> Matei
>
>
> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
> ryan.blake.williams@gmail.com> wrote:
> >
> > In the course of trying to make contributions to Spark, I have had a lo=
t
> of
> > trouble running Spark's tests successfully. The main pain points I've
> > experienced are:
> >
> >    1) frequent, spurious test failures
> >    2) high latency of running tests
> >    3) difficulty running specific tests in an iterative fashion
> >
> > Here is an example series of failures that I encountered this weekend
> > (along with footnote links to the console output from each and
> > approximately how long each took):
> >
> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
> > before.
> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]: BroadcastS=
uite
> > passed, but scala compiler crashed on the "catalyst" project.
> > - `mvn clean`: some attempts to run earlier commands (that previously
> > didn't crash the compiler) all result in the same compiler crash.
> Previous
> > discussion on this list implies this can only be solved by a `mvn clean=
`
> > [4].
> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-clean,
> > BroadcastSuite can't run because assembly is not built.
> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages about
> > version mismatches and python 2.6. The machine this ran on has python
> 2.7,
> > so I don't know what that's about.
> > - `./dev/run-tests` again [7]: "too many open files" errors in several
> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this =
is
> > not enough, but only some of the time? I increased it to 8192 and tried
> > again.
> > - `./dev/run-tests` again [8]: same pyspark errors as before. This seem=
s
> to
> > be the issue from SPARK-3867 [9], which was supposedly fixed on October
> 14;
> > not sure how I'm seeing it now. In any case, switched to Python 2.6 and
> > installed unittest2, and python/run-tests seems to be unblocked.
> > - `./dev/run-tests` again [10]: finally passes!
> >
> > This was on a spark checkout at ceb6281 (ToT Friday), with a few trivia=
l
> > changes added on (that I wanted to test before sending out a PR), on a
> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
> >
> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
> commands
> > from the same repo state:
> >
> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've se=
en
> > this one before on this machine and am guessing it actually occurs ever=
y
> > time.
> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one mo=
re
> > time from ceb6281, and saw the same failure.
> >
> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
> narrow
> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my
> mac,
> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
> used),
> > and it passed [16], so the failure seems specific to my linux
> machine/arch.
> >
> > At this point I believe that my changes don't break any tests (the
> > YarnClusterSuite failure on my linux presumably not being... "real"),
> and I
> > am ready to send out a PR. Whew!
> >
> > However, reflecting on the 5 or 6 distinct failure-modes represented
> above:
> >
> > - One of them (too many files open), is something I can (and did,
> > hopefully) fix once and for all. It cost me an ~hour this time
> (approximate
> > time of running ./dev/run-tests) and a few hours other times when I
> didn't
> > fully understand/fix it. It doesn't happen deterministically (why?), bu=
t
> > does happen somewhat frequently to people, having been discussed on the
> > user list multiple times [17] and on SO [18]. Maybe some note in the
> > documentation advising people to check their ulimit makes sense?
> > - One of them (unittest2 must be installed for python 2.6) was supposed=
ly
> > fixed upstream of the commits I tested here; I don't know why I'm still
> > running into it. This cost me a few hours of running `./dev/run-tests`
> > multiple times to see if it was transient, plus some time researching a=
nd
> > working around it.
> > - The original BroadcastSuite failure cost me a few hours and went away
> > before I'd even run `mvn clean`.
> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a few
> > hours of running `./dev/run-tests` in different ways before deciding
> that,
> > as usual, there was no way around it and that I'd need to run `mvn clea=
n`
> > and start running tests from scratch.
> > - The YarnClusterSuite failures on my linux box have cost me hours of
> > trying to figure out whether they're my fault. I've seen them many time=
s
> > over the past weeks/months, plus or minus other failures that have come
> and
> > gone, and was especially befuddled by them when I was seeing a disjoint
> set
> > of reproducible failures on my mac [19] (the triaging of which involved
> > dozens of runs of `./dev/run-tests`).
> >
> > While I'm interested in digging into each of these issues, I also want =
to
> > discuss the frequency with which I've run into issues like these. This =
is
> > unfortunately not the first time in recent months that I've spent days
> > playing spurious-test-failure whack-a-mole with a 60-90min dev/run-test=
s
> > iteration time, which is no fun! So I am wondering/thinking:
> >
> > - Do other people experience this level of flakiness from spark tests?
> > - Do other people bother running dev/run-tests locally, or just let
> Jenkins
> > do it during the CR process?
> > - Needing to run a full assembly post-clean just to continue running on=
e
> > specific test case feels especially wasteful, and the failure output wh=
en
> > naively attempting to run a specific test without having built an
> assembly
> > jar is not always clear about what the issue is or how to fix it; even
> the
> > fact that certain tests require "building the world" is not something I
> > would have expected, and has cost me hours of confusion.
> >    - Should a person running spark tests assume that they must build an
> > assembly JAR before running anything?
> >    - Are there some proper "unit" tests that are actually self-containe=
d
> /
> > able to be run without building an assembly jar?
> >    - Can we better document/demarcate which tests have which
> dependencies?
> >    - Is there something finer-grained than building an assembly JAR tha=
t
> > is sufficient in some cases?
> >        - If so, can we document that?
> >        - If not, can we move to a world of finer-grained dependencies f=
or
> > some of these?
> > - Leaving all of these spurious failures aside, the process of assembli=
ng
> > and testing a new JAR is not a quick one (40 and 60 mins for me
> typically,
> > respectively). I would guess that there are dozens (hundreds?) of peopl=
e
> > who build a Spark assembly from various ToTs on any given day, and who
> all
> > wait on the exact same compilation / assembly steps to occur. Expanding
> on
> > the recent work to publish nightly snapshots [20], can we do a better j=
ob
> > caching/sharing compilation artifacts at a more granular level (pre-bui=
lt
> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA? mo=
re
> > granular maven modules, plus the previous two?), or otherwise save some
> of
> > the considerable amount of redundant compilation work that I had to do
> over
> > the course of my odyssey this weekend?
> >
> > Ramping up on most projects involves some amount of supplementing the
> > documentation with trial and error to figure out what to run, which
> > "errors" are real errors and which can be ignored, etc., but navigating
> > that minefield on Spark has proved especially challenging and
> > time-consuming for me. Some of that comes directly from scala's
> relatively
> > slow compilation times and immature build-tooling ecosystem, but that i=
s
> > the world we live in and it would be nice if Spark took the alleviation
> of
> > the resulting pain more seriously, as one of the more interesting and
> > well-known large scala projects around right now. The official
> > documentation around how to build different subsets of the codebase is
> > somewhat sparse [21], and there have been many mixed [22] accounts [23]
> on
> > this mailing list about preferred ways to build on mvn vs. sbt (none of
> > which has made it into official documentation, as far as I've seen).
> > Expecting new contributors to piece together all of this received
> > folk-wisdom about how to build/test in a sane way by trawling mailing
> list
> > archives seems suboptimal.
> >
> > Thanks for reading, looking forward to hearing your ideas!
> >
> > -Ryan
> >
> > P.S. Is "best practice" for emailing this list to not incorporate any
> HTML
> > in the body? It seems like all of the archives I've seen strip it out,
> but
> > other people have used it and gmail displays it.
> >
> >
> > [1]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
> > (57 mins)
> > [2]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> > (6 mins)
> > [3]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,=
%20fail%20subsequent%20compile
> > (4 mins)
> > [4]
> >
> https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dweb&cd=
=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-list.1001560.n3.n=
abble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.h=
tml&ei=3DaRF6VJrpNKr-iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&si=
g2=3DzDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
> > [5]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20nee=
d%20dependencies%20built
> > [6]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
> > (50 mins)
> > [7]
> >
> https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#file-dev-run-t=
ests-failure-too-many-files-open-then-hang-L5260
> > (1hr)
> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
> > [9] https://issues.apache.org/jira/browse/SPARK-3867
> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> > [12]
> >
> https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#file-gistfile1=
-txt-L853
> > (~90 mins)
> > [13]
> >
> https://gist.github.com/ryan-williams/718f6324af358819b496#file-gistfile1=
-txt-L852
> > (91 mins)
> > [14]
> >
> https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#file-gistfile1=
-txt-L854
> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> > [17]
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/quot-Too-many-open-fi=
les-quot-exception-on-reduceByKey-td2462.html
> > [18]
> >
> http://stackoverflow.com/questions/25707629/why-does-spark-job-fail-with-=
too-many-open-files
> > [19] https://issues.apache.org/jira/browse/SPARK-4002
> > [20] https://issues.apache.org/jira/browse/SPARK-4542
> > [21]
> >
> https://spark.apache.org/docs/latest/building-with-maven.html#spark-tests=
-in-maven
> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
> > [23]
> >
> http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%3CCAOhmDz=
eUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--f46d0442866ef22d0505091da2ae--

From dev-return-10584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 01:56:41 2014
Return-Path: <dev-return-10584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 196E3107DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 01:56:41 +0000 (UTC)
Received: (qmail 26923 invoked by uid 500); 1 Dec 2014 01:56:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26855 invoked by uid 500); 1 Dec 2014 01:56:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26841 invoked by uid 99); 1 Dec 2014 01:56:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 01:56:39 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.213.53 as permitted sender)
Received: from [209.85.213.53] (HELO mail-yh0-f53.google.com) (209.85.213.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 01:56:35 +0000
Received: by mail-yh0-f53.google.com with SMTP id i57so4413259yha.40
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 17:53:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=f1BSQddOorNZ2o+KuKv1j5BG3XLHzTVHHHrkbdkXQ1Q=;
        b=GH3Cf0pH2AYgh0b2iJUkx6OU62uC2Vt8VrM/Mdyv9II1obB5m7AyNTlFaT22ykf80s
         RFgQNjrxFYLM2E6NZjxc9nxIzJzTIbuYBLOawaK1L66qv1RcGktr994NMZsSOg2Jocts
         EfZoOdxvXIRHAnJqPKQQOtcQYwv+aW1hdYFIwV7xiJOHuz8UvIEg7xiXv4c5FRH9jcwA
         Z1aJotOtlVv70xKwbTBROItvRPMlQON9VyuzITqa9Q3Mj5HM3DVstTH8yHJDlt9aXPPp
         Nk78iJMxnqCl8TAJPimjuf8Xz3KM8i3jMby9XCWaoZnJg22XNyjfZm1Q2If0f2A/J4bJ
         9l3Q==
X-Received: by 10.170.209.208 with SMTP id a199mr62298532ykf.120.1417398839524;
 Sun, 30 Nov 2014 17:53:59 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
 <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com> <CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Mon, 01 Dec 2014 01:53:57 +0000
Message-ID: <CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113983ca55c9b905091de29c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113983ca55c9b905091de29c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks Nicholas, glad to hear that some of this info will be pushed to the
main site soon, but this brings up yet another point of confusion that I've
struggled with, namely whether the documentation on github or that on
spark.apache.org should be considered the primary reference for people
seeking to learn about best practices for developing Spark.

Trying to read docs starting from
https://github.com/apache/spark/blob/master/docs/index.md right now, I find
that all of the links to other parts of the documentation are broken: they
point to relative paths that end in ".html", which will work when published
on the docs-site, but that would have to end in ".md" if a person was to be
able to navigate them on github.

So expecting people to use the up-to-date docs on github (where all
internal URLs 404 and the main github README suggests that the "latest
Spark documentation" can be found on the actually-months-old docs-site
<https://github.com/apache/spark#online-documentation>) is not a good
solution. On the other hand, consulting months-old docs on the site is also
problematic, as this thread and your last email have borne out.  The result
is that there is no good place on the internet to learn about the most
up-to-date best practices for using/developing Spark.

Why not build http://spark.apache.org/docs/latest/ nightly (or every
commit) off of what's in github, rather than having that URL point to the
last release's docs (up to ~3 months old)? This way, casual users who want
the docs for the released version they happen to be using (which is already
frequently !=3D "/latest" today, for many Spark users) can (still) find the=
m
at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
point people to a site (/latest) that actually has up-to-date docs that
reflect ToT and whose links work.

If there are concerns about existing semantics around "/latest" URLs being
broken, some new URL could be used, like
http://spark.apache.org/docs/snapshot/, but given that everything under
http://spark.apache.org/docs/latest/ is in a state of
planned-backwards-incompatible-changes every ~3mos, that doesn't sound like
that serious an issue to me; anyone sending around permanent links to
things under /latest is already going to have those links break / not make
sense in the near future.


On Sun Nov 30 2014 at 5:24:33 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

>
>    - currently the docs only contain information about building with
>    maven,
>    and even then don=E2=80=99t cover many important cases
>
>  All other points aside, I just want to point out that the docs document
> both how to use Maven and SBT and clearly state
> <https://github.com/apache/spark/blob/master/docs/building-spark.md#build=
ing-with-sbt>
> that Maven is the =E2=80=9Cbuild of reference=E2=80=9D while SBT may be p=
referable for
> day-to-day development.
>
> I believe the main reason most people miss this documentation is that,
> though it=E2=80=99s up-to-date on GitHub, it has=E2=80=99t been published=
 yet to the docs
> site. It should go out with the 1.2 release.
>
> Improvements to the documentation on building Spark belong here:
> https://github.com/apache/spark/blob/master/docs/building-spark.md
>
> If there are clear recommendations that come out of this thread but are
> not in that doc, they should be added in there. Other, less important
> details may possibly be better suited for the Contributing to Spark
> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
> guide.
>
> Nick
> =E2=80=8B
>
> On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey Ryan,
>>
>> A few more things here. You should feel free to send patches to
>> Jenkins to test them, since this is the reference environment in which
>> we regularly run tests. This is the normal workflow for most
>> developers and we spend a lot of effort provisioning/maintaining a
>> very large jenkins cluster to allow developers access this resource. A
>> common development approach is to locally run tests that you've added
>> in a patch, then send it to jenkins for the full run, and then try to
>> debug locally if you see specific unanticipated test failures.
>>
>> One challenge we have is that given the proliferation of OS versions,
>> Java versions, Python versions, ulimits, etc. there is a combinatorial
>> number of environments in which tests could be run. It is very hard in
>> some cases to figure out post-hoc why a given test is not working in a
>> specific environment. I think a good solution here would be to use a
>> standardized docker container for running Spark tests and asking folks
>> to use that locally if they are trying to run all of the hundreds of
>> Spark tests.
>>
>> Another solution would be to mock out every system interaction in
>> Spark's tests including e.g. filesystem interactions to try and reduce
>> variance across environments. However, that seems difficult.
>>
>> As the number of developers of Spark increases, it's definitely a good
>> idea for us to invest in developer infrastructure including things
>> like snapshot releases, better documentation, etc. Thanks for bringing
>> this up as a pain point.
>>
>> - Patrick
>>
>>
>> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
>> <ryan.blake.williams@gmail.com> wrote:
>> > thanks for the info, Matei and Brennon. I will try to switch my
>> workflow to
>> > using sbt. Other potential action items:
>> >
>> > - currently the docs only contain information about building with mave=
n,
>> > and even then don't cover many important cases, as I described in my
>> > previous email. If SBT is as much better as you've described then that
>> > should be made much more obvious. Wasn't it the case recently that the=
re
>> > was only a page about building with SBT, and not one about building wi=
th
>> > maven? Clearer messaging around this needs to exist in the
>> documentation,
>> > not just on the mailing list, imho.
>> >
>> > - +1 to better distinguishing between unit and integration tests, havi=
ng
>> > separate scripts for each, improving documentation around common
>> workflows,
>> > expectations of brittleness with each kind of test, advisability of ju=
st
>> > relying on Jenkins for certain kinds of tests to not waste too much
>> time,
>> > etc. Things like the compiler crash should be discussed in the
>> > documentation, not just in the mailing list archives, if new
>> contributors
>> > are likely to run into them through no fault of their own.
>> >
>> > - What is the algorithm you use to decide what tests you might have
>> broken?
>> > Can we codify it in some scripts that other people can use?
>> >
>> >
>> >
>> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.co=
m
>> >
>> > wrote:
>> >
>> >> Hi Ryan,
>> >>
>> >> As a tip (and maybe this isn't documented well), I normally use SBT f=
or
>> >> development to avoid the slow build process, and use its interactive
>> >> console to run only specific tests. The nice advantage is that SBT ca=
n
>> keep
>> >> the Scala compiler loaded and JITed across builds, making it faster t=
o
>> >> iterate. To use it, you can do the following:
>> >>
>> >> - Start the SBT interactive console with sbt/sbt
>> >> - Build your assembly by running the "assembly" target in the assembl=
y
>> >> project: assembly/assembly
>> >> - Run all the tests in one module: core/test
>> >> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>> (this
>> >> also supports tab completion)
>> >>
>> >> Running all the tests does take a while, and I usually just rely on
>> >> Jenkins for that once I've run the tests for the things I believed my
>> patch
>> >> could break. But this is because some of them are integration tests
>> (e.g.
>> >> DistributedSuite, which creates multi-process mini-clusters). Many of
>> the
>> >> individual suites run fast without requiring this, however, so you ca=
n
>> pick
>> >> the ones you want. Perhaps we should find a way to tag them so people
>> can
>> >> do a "quick-test" that skips the integration ones.
>> >>
>> >> The assembly builds are annoying but they only take about a minute fo=
r
>> me
>> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
>> required
>> >> for some of the "integration" tests (which launch new processes), but
>> I'd
>> >> recommend doing it all the time anyway since it would be very
>> confusing to
>> >> run those with an old assembly. The Scala compiler crash issue can
>> also be
>> >> a problem, but I don't see it very often with SBT. If it happens, I
>> exit
>> >> SBT and do sbt clean.
>> >>
>> >> Anyway, this is useful feedback and I think we should try to improve
>> some
>> >> of these suites, but hopefully you can also try the faster SBT
>> process. At
>> >> the end of the day, if we want integration tests, the whole test
>> process
>> >> will take an hour, but most of the developers I know leave that to
>> Jenkins
>> >> and only run individual tests locally before submitting a patch.
>> >>
>> >> Matei
>> >>
>> >>
>> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>> >> ryan.blake.williams@gmail.com> wrote:
>> >> >
>> >> > In the course of trying to make contributions to Spark, I have had =
a
>> lot
>> >> of
>> >> > trouble running Spark's tests successfully. The main pain points I'=
ve
>> >> > experienced are:
>> >> >
>> >> >    1) frequent, spurious test failures
>> >> >    2) high latency of running tests
>> >> >    3) difficulty running specific tests in an iterative fashion
>> >> >
>> >> > Here is an example series of failures that I encountered this weeke=
nd
>> >> > (along with footnote links to the console output from each and
>> >> > approximately how long each took):
>> >> >
>> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not se=
en
>> >> > before.
>> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
>> >> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]:
>> BroadcastSuite
>> >> > passed, but scala compiler crashed on the "catalyst" project.
>> >> > - `mvn clean`: some attempts to run earlier commands (that previous=
ly
>> >> > didn't crash the compiler) all result in the same compiler crash.
>> >> Previous
>> >> > discussion on this list implies this can only be solved by a `mvn
>> clean`
>> >> > [4].
>> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-cl=
ean,
>> >> > BroadcastSuite can't run because assembly is not built.
>> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>> about
>> >> > version mismatches and python 2.6. The machine this ran on has pyth=
on
>> >> 2.7,
>> >> > so I don't know what that's about.
>> >> > - `./dev/run-tests` again [7]: "too many open files" errors in
>> several
>> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>> this is
>> >> > not enough, but only some of the time? I increased it to 8192 and
>> tried
>> >> > again.
>> >> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>> seems
>> >> to
>> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>> October
>> >> 14;
>> >> > not sure how I'm seeing it now. In any case, switched to Python 2.6
>> and
>> >> > installed unittest2, and python/run-tests seems to be unblocked.
>> >> > - `./dev/run-tests` again [10]: finally passes!
>> >> >
>> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>> trivial
>> >> > changes added on (that I wanted to test before sending out a PR), o=
n
>> a
>> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11]=
.
>> >> >
>> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>> >> commands
>> >> > from the same repo state:
>> >> >
>> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I'v=
e
>> seen
>> >> > this one before on this machine and am guessing it actually occurs
>> every
>> >> > time.
>> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran on=
e
>> more
>> >> > time from ceb6281, and saw the same failure.
>> >> >
>> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt t=
o
>> >> narrow
>> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on
>> my
>> >> mac,
>> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous run=
s
>> >> used),
>> >> > and it passed [16], so the failure seems specific to my linux
>> >> machine/arch.
>> >> >
>> >> > At this point I believe that my changes don't break any tests (the
>> >> > YarnClusterSuite failure on my linux presumably not being... "real"=
),
>> >> and I
>> >> > am ready to send out a PR. Whew!
>> >> >
>> >> > However, reflecting on the 5 or 6 distinct failure-modes represente=
d
>> >> above:
>> >> >
>> >> > - One of them (too many files open), is something I can (and did,
>> >> > hopefully) fix once and for all. It cost me an ~hour this time
>> >> (approximate
>> >> > time of running ./dev/run-tests) and a few hours other times when I
>> >> didn't
>> >> > fully understand/fix it. It doesn't happen deterministically (why?)=
,
>> but
>> >> > does happen somewhat frequently to people, having been discussed on
>> the
>> >> > user list multiple times [17] and on SO [18]. Maybe some note in th=
e
>> >> > documentation advising people to check their ulimit makes sense?
>> >> > - One of them (unittest2 must be installed for python 2.6) was
>> supposedly
>> >> > fixed upstream of the commits I tested here; I don't know why I'm
>> still
>> >> > running into it. This cost me a few hours of running
>> `./dev/run-tests`
>> >> > multiple times to see if it was transient, plus some time
>> researching and
>> >> > working around it.
>> >> > - The original BroadcastSuite failure cost me a few hours and went
>> away
>> >> > before I'd even run `mvn clean`.
>> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>> few
>> >> > hours of running `./dev/run-tests` in different ways before decidin=
g
>> >> that,
>> >> > as usual, there was no way around it and that I'd need to run `mvn
>> clean`
>> >> > and start running tests from scratch.
>> >> > - The YarnClusterSuite failures on my linux box have cost me hours =
of
>> >> > trying to figure out whether they're my fault. I've seen them many
>> times
>> >> > over the past weeks/months, plus or minus other failures that have
>> come
>> >> and
>> >> > gone, and was especially befuddled by them when I was seeing a
>> disjoint
>> >> set
>> >> > of reproducible failures on my mac [19] (the triaging of which
>> involved
>> >> > dozens of runs of `./dev/run-tests`).
>> >> >
>> >> > While I'm interested in digging into each of these issues, I also
>> want to
>> >> > discuss the frequency with which I've run into issues like these.
>> This is
>> >> > unfortunately not the first time in recent months that I've spent
>> days
>> >> > playing spurious-test-failure whack-a-mole with a 60-90min
>> dev/run-tests
>> >> > iteration time, which is no fun! So I am wondering/thinking:
>> >> >
>> >> > - Do other people experience this level of flakiness from spark
>> tests?
>> >> > - Do other people bother running dev/run-tests locally, or just let
>> >> Jenkins
>> >> > do it during the CR process?
>> >> > - Needing to run a full assembly post-clean just to continue runnin=
g
>> one
>> >> > specific test case feels especially wasteful, and the failure outpu=
t
>> when
>> >> > naively attempting to run a specific test without having built an
>> >> assembly
>> >> > jar is not always clear about what the issue is or how to fix it;
>> even
>> >> the
>> >> > fact that certain tests require "building the world" is not
>> something I
>> >> > would have expected, and has cost me hours of confusion.
>> >> >    - Should a person running spark tests assume that they must buil=
d
>> an
>> >> > assembly JAR before running anything?
>> >> >    - Are there some proper "unit" tests that are actually
>> self-contained
>> >> /
>> >> > able to be run without building an assembly jar?
>> >> >    - Can we better document/demarcate which tests have which
>> >> dependencies?
>> >> >    - Is there something finer-grained than building an assembly JAR
>> that
>> >> > is sufficient in some cases?
>> >> >        - If so, can we document that?
>> >> >        - If not, can we move to a world of finer-grained
>> dependencies for
>> >> > some of these?
>> >> > - Leaving all of these spurious failures aside, the process of
>> assembling
>> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
>> >> typically,
>> >> > respectively). I would guess that there are dozens (hundreds?) of
>> people
>> >> > who build a Spark assembly from various ToTs on any given day, and
>> who
>> >> all
>> >> > wait on the exact same compilation / assembly steps to occur.
>> Expanding
>> >> on
>> >> > the recent work to publish nightly snapshots [20], can we do a
>> better job
>> >> > caching/sharing compilation artifacts at a more granular level
>> (pre-built
>> >> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA=
?
>> more
>> >> > granular maven modules, plus the previous two?), or otherwise save
>> some
>> >> of
>> >> > the considerable amount of redundant compilation work that I had to
>> do
>> >> over
>> >> > the course of my odyssey this weekend?
>> >> >
>> >> > Ramping up on most projects involves some amount of supplementing t=
he
>> >> > documentation with trial and error to figure out what to run, which
>> >> > "errors" are real errors and which can be ignored, etc., but
>> navigating
>> >> > that minefield on Spark has proved especially challenging and
>> >> > time-consuming for me. Some of that comes directly from scala's
>> >> relatively
>> >> > slow compilation times and immature build-tooling ecosystem, but
>> that is
>> >> > the world we live in and it would be nice if Spark took the
>> alleviation
>> >> of
>> >> > the resulting pain more seriously, as one of the more interesting a=
nd
>> >> > well-known large scala projects around right now. The official
>> >> > documentation around how to build different subsets of the codebase
>> is
>> >> > somewhat sparse [21], and there have been many mixed [22] accounts
>> [23]
>> >> on
>> >> > this mailing list about preferred ways to build on mvn vs. sbt (non=
e
>> of
>> >> > which has made it into official documentation, as far as I've seen)=
.
>> >> > Expecting new contributors to piece together all of this received
>> >> > folk-wisdom about how to build/test in a sane way by trawling maili=
ng
>> >> list
>> >> > archives seems suboptimal.
>> >> >
>> >> > Thanks for reading, looking forward to hearing your ideas!
>> >> >
>> >> > -Ryan
>> >> >
>> >> > P.S. Is "best practice" for emailing this list to not incorporate a=
ny
>> >> HTML
>> >> > in the body? It seems like all of the archives I've seen strip it
>> out,
>> >> but
>> >> > other people have used it and gmail displays it.
>> >> >
>> >> >
>> >> > [1]
>> >> > https://gist.githubusercontent.com/ryan-williams/
>> 8a162367c4dc157d2479/
>> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>> >> > (57 mins)
>> >> > [2]
>> >> > https://gist.githubusercontent.com/ryan-williams/
>> 8a162367c4dc157d2479/
>> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>> >> > (6 mins)
>> >> > [3]
>> >> > https://gist.githubusercontent.com/ryan-williams/
>> 8a162367c4dc157d2479/
>> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>> >> 20pass%20test,%20fail%20subsequent%20compile
>> >> > (4 mins)
>> >> > [4]
>> >> > https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dwe=
b&
>> >> cd=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-
>> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>> >> DataTypeConversions-scala-td17083.html&ei=3DaRF6VJrpNKr-
>> >> iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=3D
>> >> zDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
>> >> > [5]
>> >> > https://gist.githubusercontent.com/ryan-williams/
>> 8a162367c4dc157d2479/
>> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>> >> 20clean,%20need%20dependencies%20built
>> >> > [6]
>> >> > https://gist.githubusercontent.com/ryan-williams/
>> 8a162367c4dc157d2479/
>> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>> >> 20post%20clean
>> >> > (50 mins)
>> >> > [7]
>> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>> >> > (1hr)
>> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr=
)
>> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>> >> > [12]
>> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>> >> file-gistfile1-txt-L853
>> >> > (~90 mins)
>> >> > [13]
>> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>> >> file-gistfile1-txt-L852
>> >> > (91 mins)
>> >> > [14]
>> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>> >> file-gistfile1-txt-L854
>> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>> >> > [17]
>> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>> >> > [18]
>> >> > http://stackoverflow.com/questions/25707629/why-does-
>> >> spark-job-fail-with-too-many-open-files
>> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>> >> > [21]
>> >> > https://spark.apache.org/docs/latest/building-with-maven.
>> >> html#spark-tests-in-maven
>> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.htm=
l
>> >> > [23]
>> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com
>> %3E
>> >>
>> >>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

--001a113983ca55c9b905091de29c--

From dev-return-10585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 02:15:31 2014
Return-Path: <dev-return-10585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB7E710839
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 02:15:31 +0000 (UTC)
Received: (qmail 40778 invoked by uid 500); 1 Dec 2014 02:15:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40701 invoked by uid 500); 1 Dec 2014 02:15:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40689 invoked by uid 99); 1 Dec 2014 02:15:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:15:30 +0000
X-ASF-Spam-Status: No, hits=1.6 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:15:26 +0000
Received: by mail-oi0-f48.google.com with SMTP id u20so6681039oif.35
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 18:15:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=9iNCXH1nsdgCbXWgJxlmaofCSUghBzUoB9Mlv9Cj+IY=;
        b=yp3XuHqzXdhjmPc50R456d5Miwx5GSmWC/HHpmsAVqd7SP2DczERIrjcIjJemY1TBe
         20L4LzAhRIPnWpjNNXFUXPLfwbqzygBNJoT9cQausZT6f4zni1qxOQLWSQCH3Gi7kuF/
         fzD4bIYYyPmS93+DKaLMfUS4mUYEu3XbHKOExO3pV35604o3gxiwpTjCQ5L4I6YNz+vQ
         QPxBwL7J9W4QEdEeAnFqFPEQK5T3oZ2ZM8rRB6TyHVqb0LtUhEvwR1xWRpdmScZykpva
         d1Zwf+mAhKN4iT8u5c41OXfTWGQqVvvqlZzf7Ja+ATdtz+BxzaYAZZ2ve7YsT/yuy75H
         OGUQ==
MIME-Version: 1.0
X-Received: by 10.60.78.167 with SMTP id c7mr34979176oex.18.1417400105510;
 Sun, 30 Nov 2014 18:15:05 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Sun, 30 Nov 2014 18:15:05 -0800 (PST)
In-Reply-To: <CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
	<CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
	<CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com>
	<CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
Date: Sun, 30 Nov 2014 18:15:05 -0800
Message-ID: <CABPQxsswV3OiN8xLMThEXTCB3Q6KjGt5OpwYk5urZFB0csCn-g@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Patrick Wendell <pwendell@gmail.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Ryan,

The existing JIRA also covers publishing nightly docs:
https://issues.apache.org/jira/browse/SPARK-1517

- Patrick

On Sun, Nov 30, 2014 at 5:53 PM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
> Thanks Nicholas, glad to hear that some of this info will be pushed to the
> main site soon, but this brings up yet another point of confusion that I've
> struggled with, namely whether the documentation on github or that on
> spark.apache.org should be considered the primary reference for people
> seeking to learn about best practices for developing Spark.
>
> Trying to read docs starting from
> https://github.com/apache/spark/blob/master/docs/index.md right now, I find
> that all of the links to other parts of the documentation are broken: they
> point to relative paths that end in ".html", which will work when published
> on the docs-site, but that would have to end in ".md" if a person was to be
> able to navigate them on github.
>
> So expecting people to use the up-to-date docs on github (where all
> internal URLs 404 and the main github README suggests that the "latest
> Spark documentation" can be found on the actually-months-old docs-site
> <https://github.com/apache/spark#online-documentation>) is not a good
> solution. On the other hand, consulting months-old docs on the site is also
> problematic, as this thread and your last email have borne out.  The result
> is that there is no good place on the internet to learn about the most
> up-to-date best practices for using/developing Spark.
>
> Why not build http://spark.apache.org/docs/latest/ nightly (or every
> commit) off of what's in github, rather than having that URL point to the
> last release's docs (up to ~3 months old)? This way, casual users who want
> the docs for the released version they happen to be using (which is already
> frequently != "/latest" today, for many Spark users) can (still) find them
> at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
> point people to a site (/latest) that actually has up-to-date docs that
> reflect ToT and whose links work.
>
> If there are concerns about existing semantics around "/latest" URLs being
> broken, some new URL could be used, like
> http://spark.apache.org/docs/snapshot/, but given that everything under
> http://spark.apache.org/docs/latest/ is in a state of
> planned-backwards-incompatible-changes every ~3mos, that doesn't sound like
> that serious an issue to me; anyone sending around permanent links to
> things under /latest is already going to have those links break / not make
> sense in the near future.
>
>
> On Sun Nov 30 2014 at 5:24:33 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>>
>>    - currently the docs only contain information about building with
>>    maven,
>>    and even then don't cover many important cases
>>
>>  All other points aside, I just want to point out that the docs document
>> both how to use Maven and SBT and clearly state
>> <https://github.com/apache/spark/blob/master/docs/building-spark.md#building-with-sbt>
>> that Maven is the "build of reference" while SBT may be preferable for
>> day-to-day development.
>>
>> I believe the main reason most people miss this documentation is that,
>> though it's up-to-date on GitHub, it has't been published yet to the docs
>> site. It should go out with the 1.2 release.
>>
>> Improvements to the documentation on building Spark belong here:
>> https://github.com/apache/spark/blob/master/docs/building-spark.md
>>
>> If there are clear recommendations that come out of this thread but are
>> not in that doc, they should be added in there. Other, less important
>> details may possibly be better suited for the Contributing to Spark
>> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
>> guide.
>>
>> Nick
>>
>>
>> On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Hey Ryan,
>>>
>>> A few more things here. You should feel free to send patches to
>>> Jenkins to test them, since this is the reference environment in which
>>> we regularly run tests. This is the normal workflow for most
>>> developers and we spend a lot of effort provisioning/maintaining a
>>> very large jenkins cluster to allow developers access this resource. A
>>> common development approach is to locally run tests that you've added
>>> in a patch, then send it to jenkins for the full run, and then try to
>>> debug locally if you see specific unanticipated test failures.
>>>
>>> One challenge we have is that given the proliferation of OS versions,
>>> Java versions, Python versions, ulimits, etc. there is a combinatorial
>>> number of environments in which tests could be run. It is very hard in
>>> some cases to figure out post-hoc why a given test is not working in a
>>> specific environment. I think a good solution here would be to use a
>>> standardized docker container for running Spark tests and asking folks
>>> to use that locally if they are trying to run all of the hundreds of
>>> Spark tests.
>>>
>>> Another solution would be to mock out every system interaction in
>>> Spark's tests including e.g. filesystem interactions to try and reduce
>>> variance across environments. However, that seems difficult.
>>>
>>> As the number of developers of Spark increases, it's definitely a good
>>> idea for us to invest in developer infrastructure including things
>>> like snapshot releases, better documentation, etc. Thanks for bringing
>>> this up as a pain point.
>>>
>>> - Patrick
>>>
>>>
>>> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
>>> <ryan.blake.williams@gmail.com> wrote:
>>> > thanks for the info, Matei and Brennon. I will try to switch my
>>> workflow to
>>> > using sbt. Other potential action items:
>>> >
>>> > - currently the docs only contain information about building with maven,
>>> > and even then don't cover many important cases, as I described in my
>>> > previous email. If SBT is as much better as you've described then that
>>> > should be made much more obvious. Wasn't it the case recently that there
>>> > was only a page about building with SBT, and not one about building with
>>> > maven? Clearer messaging around this needs to exist in the
>>> documentation,
>>> > not just on the mailing list, imho.
>>> >
>>> > - +1 to better distinguishing between unit and integration tests, having
>>> > separate scripts for each, improving documentation around common
>>> workflows,
>>> > expectations of brittleness with each kind of test, advisability of just
>>> > relying on Jenkins for certain kinds of tests to not waste too much
>>> time,
>>> > etc. Things like the compiler crash should be discussed in the
>>> > documentation, not just in the mailing list archives, if new
>>> contributors
>>> > are likely to run into them through no fault of their own.
>>> >
>>> > - What is the algorithm you use to decide what tests you might have
>>> broken?
>>> > Can we codify it in some scripts that other people can use?
>>> >
>>> >
>>> >
>>> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.com
>>> >
>>> > wrote:
>>> >
>>> >> Hi Ryan,
>>> >>
>>> >> As a tip (and maybe this isn't documented well), I normally use SBT for
>>> >> development to avoid the slow build process, and use its interactive
>>> >> console to run only specific tests. The nice advantage is that SBT can
>>> keep
>>> >> the Scala compiler loaded and JITed across builds, making it faster to
>>> >> iterate. To use it, you can do the following:
>>> >>
>>> >> - Start the SBT interactive console with sbt/sbt
>>> >> - Build your assembly by running the "assembly" target in the assembly
>>> >> project: assembly/assembly
>>> >> - Run all the tests in one module: core/test
>>> >> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>>> (this
>>> >> also supports tab completion)
>>> >>
>>> >> Running all the tests does take a while, and I usually just rely on
>>> >> Jenkins for that once I've run the tests for the things I believed my
>>> patch
>>> >> could break. But this is because some of them are integration tests
>>> (e.g.
>>> >> DistributedSuite, which creates multi-process mini-clusters). Many of
>>> the
>>> >> individual suites run fast without requiring this, however, so you can
>>> pick
>>> >> the ones you want. Perhaps we should find a way to tag them so people
>>> can
>>> >> do a "quick-test" that skips the integration ones.
>>> >>
>>> >> The assembly builds are annoying but they only take about a minute for
>>> me
>>> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
>>> required
>>> >> for some of the "integration" tests (which launch new processes), but
>>> I'd
>>> >> recommend doing it all the time anyway since it would be very
>>> confusing to
>>> >> run those with an old assembly. The Scala compiler crash issue can
>>> also be
>>> >> a problem, but I don't see it very often with SBT. If it happens, I
>>> exit
>>> >> SBT and do sbt clean.
>>> >>
>>> >> Anyway, this is useful feedback and I think we should try to improve
>>> some
>>> >> of these suites, but hopefully you can also try the faster SBT
>>> process. At
>>> >> the end of the day, if we want integration tests, the whole test
>>> process
>>> >> will take an hour, but most of the developers I know leave that to
>>> Jenkins
>>> >> and only run individual tests locally before submitting a patch.
>>> >>
>>> >> Matei
>>> >>
>>> >>
>>> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>>> >> ryan.blake.williams@gmail.com> wrote:
>>> >> >
>>> >> > In the course of trying to make contributions to Spark, I have had a
>>> lot
>>> >> of
>>> >> > trouble running Spark's tests successfully. The main pain points I've
>>> >> > experienced are:
>>> >> >
>>> >> >    1) frequent, spurious test failures
>>> >> >    2) high latency of running tests
>>> >> >    3) difficulty running specific tests in an iterative fashion
>>> >> >
>>> >> > Here is an example series of failures that I encountered this weekend
>>> >> > (along with footnote links to the console output from each and
>>> >> > approximately how long each took):
>>> >> >
>>> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
>>> >> > before.
>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
>>> >> > - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]:
>>> BroadcastSuite
>>> >> > passed, but scala compiler crashed on the "catalyst" project.
>>> >> > - `mvn clean`: some attempts to run earlier commands (that previously
>>> >> > didn't crash the compiler) all result in the same compiler crash.
>>> >> Previous
>>> >> > discussion on this list implies this can only be solved by a `mvn
>>> clean`
>>> >> > [4].
>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
>>> >> > BroadcastSuite can't run because assembly is not built.
>>> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>>> about
>>> >> > version mismatches and python 2.6. The machine this ran on has python
>>> >> 2.7,
>>> >> > so I don't know what that's about.
>>> >> > - `./dev/run-tests` again [7]: "too many open files" errors in
>>> several
>>> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>>> this is
>>> >> > not enough, but only some of the time? I increased it to 8192 and
>>> tried
>>> >> > again.
>>> >> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>>> seems
>>> >> to
>>> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>>> October
>>> >> 14;
>>> >> > not sure how I'm seeing it now. In any case, switched to Python 2.6
>>> and
>>> >> > installed unittest2, and python/run-tests seems to be unblocked.
>>> >> > - `./dev/run-tests` again [10]: finally passes!
>>> >> >
>>> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>>> trivial
>>> >> > changes added on (that I wanted to test before sending out a PR), on
>>> a
>>> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
>>> >> >
>>> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>>> >> commands
>>> >> > from the same repo state:
>>> >> >
>>> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>>> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
>>> seen
>>> >> > this one before on this machine and am guessing it actually occurs
>>> every
>>> >> > time.
>>> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
>>> more
>>> >> > time from ceb6281, and saw the same failure.
>>> >> >
>>> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
>>> >> narrow
>>> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on
>>> my
>>> >> mac,
>>> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
>>> >> used),
>>> >> > and it passed [16], so the failure seems specific to my linux
>>> >> machine/arch.
>>> >> >
>>> >> > At this point I believe that my changes don't break any tests (the
>>> >> > YarnClusterSuite failure on my linux presumably not being... "real"),
>>> >> and I
>>> >> > am ready to send out a PR. Whew!
>>> >> >
>>> >> > However, reflecting on the 5 or 6 distinct failure-modes represented
>>> >> above:
>>> >> >
>>> >> > - One of them (too many files open), is something I can (and did,
>>> >> > hopefully) fix once and for all. It cost me an ~hour this time
>>> >> (approximate
>>> >> > time of running ./dev/run-tests) and a few hours other times when I
>>> >> didn't
>>> >> > fully understand/fix it. It doesn't happen deterministically (why?),
>>> but
>>> >> > does happen somewhat frequently to people, having been discussed on
>>> the
>>> >> > user list multiple times [17] and on SO [18]. Maybe some note in the
>>> >> > documentation advising people to check their ulimit makes sense?
>>> >> > - One of them (unittest2 must be installed for python 2.6) was
>>> supposedly
>>> >> > fixed upstream of the commits I tested here; I don't know why I'm
>>> still
>>> >> > running into it. This cost me a few hours of running
>>> `./dev/run-tests`
>>> >> > multiple times to see if it was transient, plus some time
>>> researching and
>>> >> > working around it.
>>> >> > - The original BroadcastSuite failure cost me a few hours and went
>>> away
>>> >> > before I'd even run `mvn clean`.
>>> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>>> few
>>> >> > hours of running `./dev/run-tests` in different ways before deciding
>>> >> that,
>>> >> > as usual, there was no way around it and that I'd need to run `mvn
>>> clean`
>>> >> > and start running tests from scratch.
>>> >> > - The YarnClusterSuite failures on my linux box have cost me hours of
>>> >> > trying to figure out whether they're my fault. I've seen them many
>>> times
>>> >> > over the past weeks/months, plus or minus other failures that have
>>> come
>>> >> and
>>> >> > gone, and was especially befuddled by them when I was seeing a
>>> disjoint
>>> >> set
>>> >> > of reproducible failures on my mac [19] (the triaging of which
>>> involved
>>> >> > dozens of runs of `./dev/run-tests`).
>>> >> >
>>> >> > While I'm interested in digging into each of these issues, I also
>>> want to
>>> >> > discuss the frequency with which I've run into issues like these.
>>> This is
>>> >> > unfortunately not the first time in recent months that I've spent
>>> days
>>> >> > playing spurious-test-failure whack-a-mole with a 60-90min
>>> dev/run-tests
>>> >> > iteration time, which is no fun! So I am wondering/thinking:
>>> >> >
>>> >> > - Do other people experience this level of flakiness from spark
>>> tests?
>>> >> > - Do other people bother running dev/run-tests locally, or just let
>>> >> Jenkins
>>> >> > do it during the CR process?
>>> >> > - Needing to run a full assembly post-clean just to continue running
>>> one
>>> >> > specific test case feels especially wasteful, and the failure output
>>> when
>>> >> > naively attempting to run a specific test without having built an
>>> >> assembly
>>> >> > jar is not always clear about what the issue is or how to fix it;
>>> even
>>> >> the
>>> >> > fact that certain tests require "building the world" is not
>>> something I
>>> >> > would have expected, and has cost me hours of confusion.
>>> >> >    - Should a person running spark tests assume that they must build
>>> an
>>> >> > assembly JAR before running anything?
>>> >> >    - Are there some proper "unit" tests that are actually
>>> self-contained
>>> >> /
>>> >> > able to be run without building an assembly jar?
>>> >> >    - Can we better document/demarcate which tests have which
>>> >> dependencies?
>>> >> >    - Is there something finer-grained than building an assembly JAR
>>> that
>>> >> > is sufficient in some cases?
>>> >> >        - If so, can we document that?
>>> >> >        - If not, can we move to a world of finer-grained
>>> dependencies for
>>> >> > some of these?
>>> >> > - Leaving all of these spurious failures aside, the process of
>>> assembling
>>> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
>>> >> typically,
>>> >> > respectively). I would guess that there are dozens (hundreds?) of
>>> people
>>> >> > who build a Spark assembly from various ToTs on any given day, and
>>> who
>>> >> all
>>> >> > wait on the exact same compilation / assembly steps to occur.
>>> Expanding
>>> >> on
>>> >> > the recent work to publish nightly snapshots [20], can we do a
>>> better job
>>> >> > caching/sharing compilation artifacts at a more granular level
>>> (pre-built
>>> >> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
>>> more
>>> >> > granular maven modules, plus the previous two?), or otherwise save
>>> some
>>> >> of
>>> >> > the considerable amount of redundant compilation work that I had to
>>> do
>>> >> over
>>> >> > the course of my odyssey this weekend?
>>> >> >
>>> >> > Ramping up on most projects involves some amount of supplementing the
>>> >> > documentation with trial and error to figure out what to run, which
>>> >> > "errors" are real errors and which can be ignored, etc., but
>>> navigating
>>> >> > that minefield on Spark has proved especially challenging and
>>> >> > time-consuming for me. Some of that comes directly from scala's
>>> >> relatively
>>> >> > slow compilation times and immature build-tooling ecosystem, but
>>> that is
>>> >> > the world we live in and it would be nice if Spark took the
>>> alleviation
>>> >> of
>>> >> > the resulting pain more seriously, as one of the more interesting and
>>> >> > well-known large scala projects around right now. The official
>>> >> > documentation around how to build different subsets of the codebase
>>> is
>>> >> > somewhat sparse [21], and there have been many mixed [22] accounts
>>> [23]
>>> >> on
>>> >> > this mailing list about preferred ways to build on mvn vs. sbt (none
>>> of
>>> >> > which has made it into official documentation, as far as I've seen).
>>> >> > Expecting new contributors to piece together all of this received
>>> >> > folk-wisdom about how to build/test in a sane way by trawling mailing
>>> >> list
>>> >> > archives seems suboptimal.
>>> >> >
>>> >> > Thanks for reading, looking forward to hearing your ideas!
>>> >> >
>>> >> > -Ryan
>>> >> >
>>> >> > P.S. Is "best practice" for emailing this list to not incorporate any
>>> >> HTML
>>> >> > in the body? It seems like all of the archives I've seen strip it
>>> out,
>>> >> but
>>> >> > other people have used it and gmail displays it.
>>> >> >
>>> >> >
>>> >> > [1]
>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>> 8a162367c4dc157d2479/
>>> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>>> >> > (57 mins)
>>> >> > [2]
>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>> 8a162367c4dc157d2479/
>>> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>>> >> > (6 mins)
>>> >> > [3]
>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>> 8a162367c4dc157d2479/
>>> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>>> >> 20pass%20test,%20fail%20subsequent%20compile
>>> >> > (4 mins)
>>> >> > [4]
>>> >> > https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&
>>> >> cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-
>>> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>>> >> DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-
>>> >> iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=
>>> >> zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
>>> >> > [5]
>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>> 8a162367c4dc157d2479/
>>> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>>> >> 20clean,%20need%20dependencies%20built
>>> >> > [6]
>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>> 8a162367c4dc157d2479/
>>> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>>> >> 20post%20clean
>>> >> > (50 mins)
>>> >> > [7]
>>> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>>> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>>> >> > (1hr)
>>> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
>>> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>>> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>>> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>>> >> > [12]
>>> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>>> >> file-gistfile1-txt-L853
>>> >> > (~90 mins)
>>> >> > [13]
>>> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>>> >> file-gistfile1-txt-L852
>>> >> > (91 mins)
>>> >> > [14]
>>> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>>> >> file-gistfile1-txt-L854
>>> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>>> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>>> >> > [17]
>>> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>>> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>>> >> > [18]
>>> >> > http://stackoverflow.com/questions/25707629/why-does-
>>> >> spark-job-fail-with-too-many-open-files
>>> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>>> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>>> >> > [21]
>>> >> > https://spark.apache.org/docs/latest/building-with-maven.
>>> >> html#spark-tests-in-maven
>>> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
>>> >> > [23]
>>> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>>> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com
>>> %3E
>>> >>
>>> >>
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 02:21:29 2014
Return-Path: <dev-return-10586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63E9710859
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 02:21:29 +0000 (UTC)
Received: (qmail 50045 invoked by uid 500); 1 Dec 2014 02:21:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49973 invoked by uid 500); 1 Dec 2014 02:21:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49961 invoked by uid 99); 1 Dec 2014 02:21:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:21:27 +0000
X-ASF-Spam-Status: No, hits=1.6 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.180 as permitted sender)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:21:24 +0000
Received: by mail-ob0-f180.google.com with SMTP id wp4so7178890obc.39
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 18:19:33 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BqVLAhg5XXc77NcrXwgzHdmrCk59X1mYQvpVui5PkKw=;
        b=zle8eJo4iX7hmE+jRz21ldtbhmwL2ennO11OHqEzCxh7N/Wb5CP6pLOkedI3yE5bZT
         6xeA8cXeUaLr04MJLXItkLuXO50K7tr4CTVQEMnKbl+Riv9/zhEWs9Qardl7WWRPlN99
         /nS3uF93ZJkl2mHzu6TcMLzOZMjHcqPz5dZT1WNmHKTeMnCz7VCJJ5UYON2H9azvGGss
         or4c7mWOtiscUFk6bqlCmLi4TmQqoZJnDCbTHbb1jTHkWVXIGRDzhVtNz9giX3oDO2uX
         9wGrGpiW8BhEK0BcLwUzs7k/tlSU2Ts4KqqbrjotvxEmyRS/jm8emwXlYUW/CE8/+502
         UGVg==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr31302489oie.78.1417400373091;
 Sun, 30 Nov 2014 18:19:33 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Sun, 30 Nov 2014 18:19:32 -0800 (PST)
In-Reply-To: <CABPQxsswV3OiN8xLMThEXTCB3Q6KjGt5OpwYk5urZFB0csCn-g@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
	<CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
	<CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com>
	<CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
	<CABPQxsswV3OiN8xLMThEXTCB3Q6KjGt5OpwYk5urZFB0csCn-g@mail.gmail.com>
Date: Sun, 30 Nov 2014 18:19:32 -0800
Message-ID: <CABPQxsvyH5vmbdvZzBjPGRCOBG3tg+F-z=VkgjOvVkSGRm__WQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Patrick Wendell <pwendell@gmail.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Btw - the documnetation on github represents the source code of our
docs, which is versioned with each release. Unfortunately github will
always try to render ".md" files so it could look to a passerby like
this is supposed to represent published docs. This is a feature
limitation of github, AFAIK we cannot disable it.

The official published docs are associated with each release and
available on the apache.org website. I think "/latest" is a common
convention for referring to the latest *published release* docs, so
probably we can't change that (the audience for /latest is orders of
magnitude larger than for snapshot docs). However we could just add
/snapshot and publish docs there.

- Patrick

On Sun, Nov 30, 2014 at 6:15 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Ryan,
>
> The existing JIRA also covers publishing nightly docs:
> https://issues.apache.org/jira/browse/SPARK-1517
>
> - Patrick
>
> On Sun, Nov 30, 2014 at 5:53 PM, Ryan Williams
> <ryan.blake.williams@gmail.com> wrote:
>> Thanks Nicholas, glad to hear that some of this info will be pushed to the
>> main site soon, but this brings up yet another point of confusion that I've
>> struggled with, namely whether the documentation on github or that on
>> spark.apache.org should be considered the primary reference for people
>> seeking to learn about best practices for developing Spark.
>>
>> Trying to read docs starting from
>> https://github.com/apache/spark/blob/master/docs/index.md right now, I find
>> that all of the links to other parts of the documentation are broken: they
>> point to relative paths that end in ".html", which will work when published
>> on the docs-site, but that would have to end in ".md" if a person was to be
>> able to navigate them on github.
>>
>> So expecting people to use the up-to-date docs on github (where all
>> internal URLs 404 and the main github README suggests that the "latest
>> Spark documentation" can be found on the actually-months-old docs-site
>> <https://github.com/apache/spark#online-documentation>) is not a good
>> solution. On the other hand, consulting months-old docs on the site is also
>> problematic, as this thread and your last email have borne out.  The result
>> is that there is no good place on the internet to learn about the most
>> up-to-date best practices for using/developing Spark.
>>
>> Why not build http://spark.apache.org/docs/latest/ nightly (or every
>> commit) off of what's in github, rather than having that URL point to the
>> last release's docs (up to ~3 months old)? This way, casual users who want
>> the docs for the released version they happen to be using (which is already
>> frequently != "/latest" today, for many Spark users) can (still) find them
>> at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
>> point people to a site (/latest) that actually has up-to-date docs that
>> reflect ToT and whose links work.
>>
>> If there are concerns about existing semantics around "/latest" URLs being
>> broken, some new URL could be used, like
>> http://spark.apache.org/docs/snapshot/, but given that everything under
>> http://spark.apache.org/docs/latest/ is in a state of
>> planned-backwards-incompatible-changes every ~3mos, that doesn't sound like
>> that serious an issue to me; anyone sending around permanent links to
>> things under /latest is already going to have those links break / not make
>> sense in the near future.
>>
>>
>> On Sun Nov 30 2014 at 5:24:33 PM Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>>
>>>    - currently the docs only contain information about building with
>>>    maven,
>>>    and even then don't cover many important cases
>>>
>>>  All other points aside, I just want to point out that the docs document
>>> both how to use Maven and SBT and clearly state
>>> <https://github.com/apache/spark/blob/master/docs/building-spark.md#building-with-sbt>
>>> that Maven is the "build of reference" while SBT may be preferable for
>>> day-to-day development.
>>>
>>> I believe the main reason most people miss this documentation is that,
>>> though it's up-to-date on GitHub, it has't been published yet to the docs
>>> site. It should go out with the 1.2 release.
>>>
>>> Improvements to the documentation on building Spark belong here:
>>> https://github.com/apache/spark/blob/master/docs/building-spark.md
>>>
>>> If there are clear recommendations that come out of this thread but are
>>> not in that doc, they should be added in there. Other, less important
>>> details may possibly be better suited for the Contributing to Spark
>>> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
>>> guide.
>>>
>>> Nick
>>>
>>>
>>> On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>>> Hey Ryan,
>>>>
>>>> A few more things here. You should feel free to send patches to
>>>> Jenkins to test them, since this is the reference environment in which
>>>> we regularly run tests. This is the normal workflow for most
>>>> developers and we spend a lot of effort provisioning/maintaining a
>>>> very large jenkins cluster to allow developers access this resource. A
>>>> common development approach is to locally run tests that you've added
>>>> in a patch, then send it to jenkins for the full run, and then try to
>>>> debug locally if you see specific unanticipated test failures.
>>>>
>>>> One challenge we have is that given the proliferation of OS versions,
>>>> Java versions, Python versions, ulimits, etc. there is a combinatorial
>>>> number of environments in which tests could be run. It is very hard in
>>>> some cases to figure out post-hoc why a given test is not working in a
>>>> specific environment. I think a good solution here would be to use a
>>>> standardized docker container for running Spark tests and asking folks
>>>> to use that locally if they are trying to run all of the hundreds of
>>>> Spark tests.
>>>>
>>>> Another solution would be to mock out every system interaction in
>>>> Spark's tests including e.g. filesystem interactions to try and reduce
>>>> variance across environments. However, that seems difficult.
>>>>
>>>> As the number of developers of Spark increases, it's definitely a good
>>>> idea for us to invest in developer infrastructure including things
>>>> like snapshot releases, better documentation, etc. Thanks for bringing
>>>> this up as a pain point.
>>>>
>>>> - Patrick
>>>>
>>>>
>>>> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
>>>> <ryan.blake.williams@gmail.com> wrote:
>>>> > thanks for the info, Matei and Brennon. I will try to switch my
>>>> workflow to
>>>> > using sbt. Other potential action items:
>>>> >
>>>> > - currently the docs only contain information about building with maven,
>>>> > and even then don't cover many important cases, as I described in my
>>>> > previous email. If SBT is as much better as you've described then that
>>>> > should be made much more obvious. Wasn't it the case recently that there
>>>> > was only a page about building with SBT, and not one about building with
>>>> > maven? Clearer messaging around this needs to exist in the
>>>> documentation,
>>>> > not just on the mailing list, imho.
>>>> >
>>>> > - +1 to better distinguishing between unit and integration tests, having
>>>> > separate scripts for each, improving documentation around common
>>>> workflows,
>>>> > expectations of brittleness with each kind of test, advisability of just
>>>> > relying on Jenkins for certain kinds of tests to not waste too much
>>>> time,
>>>> > etc. Things like the compiler crash should be discussed in the
>>>> > documentation, not just in the mailing list archives, if new
>>>> contributors
>>>> > are likely to run into them through no fault of their own.
>>>> >
>>>> > - What is the algorithm you use to decide what tests you might have
>>>> broken?
>>>> > Can we codify it in some scripts that other people can use?
>>>> >
>>>> >
>>>> >
>>>> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.com
>>>> >
>>>> > wrote:
>>>> >
>>>> >> Hi Ryan,
>>>> >>
>>>> >> As a tip (and maybe this isn't documented well), I normally use SBT for
>>>> >> development to avoid the slow build process, and use its interactive
>>>> >> console to run only specific tests. The nice advantage is that SBT can
>>>> keep
>>>> >> the Scala compiler loaded and JITed across builds, making it faster to
>>>> >> iterate. To use it, you can do the following:
>>>> >>
>>>> >> - Start the SBT interactive console with sbt/sbt
>>>> >> - Build your assembly by running the "assembly" target in the assembly
>>>> >> project: assembly/assembly
>>>> >> - Run all the tests in one module: core/test
>>>> >> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>>>> (this
>>>> >> also supports tab completion)
>>>> >>
>>>> >> Running all the tests does take a while, and I usually just rely on
>>>> >> Jenkins for that once I've run the tests for the things I believed my
>>>> patch
>>>> >> could break. But this is because some of them are integration tests
>>>> (e.g.
>>>> >> DistributedSuite, which creates multi-process mini-clusters). Many of
>>>> the
>>>> >> individual suites run fast without requiring this, however, so you can
>>>> pick
>>>> >> the ones you want. Perhaps we should find a way to tag them so people
>>>> can
>>>> >> do a "quick-test" that skips the integration ones.
>>>> >>
>>>> >> The assembly builds are annoying but they only take about a minute for
>>>> me
>>>> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
>>>> required
>>>> >> for some of the "integration" tests (which launch new processes), but
>>>> I'd
>>>> >> recommend doing it all the time anyway since it would be very
>>>> confusing to
>>>> >> run those with an old assembly. The Scala compiler crash issue can
>>>> also be
>>>> >> a problem, but I don't see it very often with SBT. If it happens, I
>>>> exit
>>>> >> SBT and do sbt clean.
>>>> >>
>>>> >> Anyway, this is useful feedback and I think we should try to improve
>>>> some
>>>> >> of these suites, but hopefully you can also try the faster SBT
>>>> process. At
>>>> >> the end of the day, if we want integration tests, the whole test
>>>> process
>>>> >> will take an hour, but most of the developers I know leave that to
>>>> Jenkins
>>>> >> and only run individual tests locally before submitting a patch.
>>>> >>
>>>> >> Matei
>>>> >>
>>>> >>
>>>> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>>>> >> ryan.blake.williams@gmail.com> wrote:
>>>> >> >
>>>> >> > In the course of trying to make contributions to Spark, I have had a
>>>> lot
>>>> >> of
>>>> >> > trouble running Spark's tests successfully. The main pain points I've
>>>> >> > experienced are:
>>>> >> >
>>>> >> >    1) frequent, spurious test failures
>>>> >> >    2) high latency of running tests
>>>> >> >    3) difficulty running specific tests in an iterative fashion
>>>> >> >
>>>> >> > Here is an example series of failures that I encountered this weekend
>>>> >> > (along with footnote links to the console output from each and
>>>> >> > approximately how long each took):
>>>> >> >
>>>> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
>>>> >> > before.
>>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
>>>> >> > - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]:
>>>> BroadcastSuite
>>>> >> > passed, but scala compiler crashed on the "catalyst" project.
>>>> >> > - `mvn clean`: some attempts to run earlier commands (that previously
>>>> >> > didn't crash the compiler) all result in the same compiler crash.
>>>> >> Previous
>>>> >> > discussion on this list implies this can only be solved by a `mvn
>>>> clean`
>>>> >> > [4].
>>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
>>>> >> > BroadcastSuite can't run because assembly is not built.
>>>> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>>>> about
>>>> >> > version mismatches and python 2.6. The machine this ran on has python
>>>> >> 2.7,
>>>> >> > so I don't know what that's about.
>>>> >> > - `./dev/run-tests` again [7]: "too many open files" errors in
>>>> several
>>>> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>>>> this is
>>>> >> > not enough, but only some of the time? I increased it to 8192 and
>>>> tried
>>>> >> > again.
>>>> >> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>>>> seems
>>>> >> to
>>>> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>>>> October
>>>> >> 14;
>>>> >> > not sure how I'm seeing it now. In any case, switched to Python 2.6
>>>> and
>>>> >> > installed unittest2, and python/run-tests seems to be unblocked.
>>>> >> > - `./dev/run-tests` again [10]: finally passes!
>>>> >> >
>>>> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>>>> trivial
>>>> >> > changes added on (that I wanted to test before sending out a PR), on
>>>> a
>>>> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
>>>> >> >
>>>> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>>>> >> commands
>>>> >> > from the same repo state:
>>>> >> >
>>>> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>>>> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
>>>> seen
>>>> >> > this one before on this machine and am guessing it actually occurs
>>>> every
>>>> >> > time.
>>>> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
>>>> more
>>>> >> > time from ceb6281, and saw the same failure.
>>>> >> >
>>>> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
>>>> >> narrow
>>>> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on
>>>> my
>>>> >> mac,
>>>> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
>>>> >> used),
>>>> >> > and it passed [16], so the failure seems specific to my linux
>>>> >> machine/arch.
>>>> >> >
>>>> >> > At this point I believe that my changes don't break any tests (the
>>>> >> > YarnClusterSuite failure on my linux presumably not being... "real"),
>>>> >> and I
>>>> >> > am ready to send out a PR. Whew!
>>>> >> >
>>>> >> > However, reflecting on the 5 or 6 distinct failure-modes represented
>>>> >> above:
>>>> >> >
>>>> >> > - One of them (too many files open), is something I can (and did,
>>>> >> > hopefully) fix once and for all. It cost me an ~hour this time
>>>> >> (approximate
>>>> >> > time of running ./dev/run-tests) and a few hours other times when I
>>>> >> didn't
>>>> >> > fully understand/fix it. It doesn't happen deterministically (why?),
>>>> but
>>>> >> > does happen somewhat frequently to people, having been discussed on
>>>> the
>>>> >> > user list multiple times [17] and on SO [18]. Maybe some note in the
>>>> >> > documentation advising people to check their ulimit makes sense?
>>>> >> > - One of them (unittest2 must be installed for python 2.6) was
>>>> supposedly
>>>> >> > fixed upstream of the commits I tested here; I don't know why I'm
>>>> still
>>>> >> > running into it. This cost me a few hours of running
>>>> `./dev/run-tests`
>>>> >> > multiple times to see if it was transient, plus some time
>>>> researching and
>>>> >> > working around it.
>>>> >> > - The original BroadcastSuite failure cost me a few hours and went
>>>> away
>>>> >> > before I'd even run `mvn clean`.
>>>> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>>>> few
>>>> >> > hours of running `./dev/run-tests` in different ways before deciding
>>>> >> that,
>>>> >> > as usual, there was no way around it and that I'd need to run `mvn
>>>> clean`
>>>> >> > and start running tests from scratch.
>>>> >> > - The YarnClusterSuite failures on my linux box have cost me hours of
>>>> >> > trying to figure out whether they're my fault. I've seen them many
>>>> times
>>>> >> > over the past weeks/months, plus or minus other failures that have
>>>> come
>>>> >> and
>>>> >> > gone, and was especially befuddled by them when I was seeing a
>>>> disjoint
>>>> >> set
>>>> >> > of reproducible failures on my mac [19] (the triaging of which
>>>> involved
>>>> >> > dozens of runs of `./dev/run-tests`).
>>>> >> >
>>>> >> > While I'm interested in digging into each of these issues, I also
>>>> want to
>>>> >> > discuss the frequency with which I've run into issues like these.
>>>> This is
>>>> >> > unfortunately not the first time in recent months that I've spent
>>>> days
>>>> >> > playing spurious-test-failure whack-a-mole with a 60-90min
>>>> dev/run-tests
>>>> >> > iteration time, which is no fun! So I am wondering/thinking:
>>>> >> >
>>>> >> > - Do other people experience this level of flakiness from spark
>>>> tests?
>>>> >> > - Do other people bother running dev/run-tests locally, or just let
>>>> >> Jenkins
>>>> >> > do it during the CR process?
>>>> >> > - Needing to run a full assembly post-clean just to continue running
>>>> one
>>>> >> > specific test case feels especially wasteful, and the failure output
>>>> when
>>>> >> > naively attempting to run a specific test without having built an
>>>> >> assembly
>>>> >> > jar is not always clear about what the issue is or how to fix it;
>>>> even
>>>> >> the
>>>> >> > fact that certain tests require "building the world" is not
>>>> something I
>>>> >> > would have expected, and has cost me hours of confusion.
>>>> >> >    - Should a person running spark tests assume that they must build
>>>> an
>>>> >> > assembly JAR before running anything?
>>>> >> >    - Are there some proper "unit" tests that are actually
>>>> self-contained
>>>> >> /
>>>> >> > able to be run without building an assembly jar?
>>>> >> >    - Can we better document/demarcate which tests have which
>>>> >> dependencies?
>>>> >> >    - Is there something finer-grained than building an assembly JAR
>>>> that
>>>> >> > is sufficient in some cases?
>>>> >> >        - If so, can we document that?
>>>> >> >        - If not, can we move to a world of finer-grained
>>>> dependencies for
>>>> >> > some of these?
>>>> >> > - Leaving all of these spurious failures aside, the process of
>>>> assembling
>>>> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
>>>> >> typically,
>>>> >> > respectively). I would guess that there are dozens (hundreds?) of
>>>> people
>>>> >> > who build a Spark assembly from various ToTs on any given day, and
>>>> who
>>>> >> all
>>>> >> > wait on the exact same compilation / assembly steps to occur.
>>>> Expanding
>>>> >> on
>>>> >> > the recent work to publish nightly snapshots [20], can we do a
>>>> better job
>>>> >> > caching/sharing compilation artifacts at a more granular level
>>>> (pre-built
>>>> >> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
>>>> more
>>>> >> > granular maven modules, plus the previous two?), or otherwise save
>>>> some
>>>> >> of
>>>> >> > the considerable amount of redundant compilation work that I had to
>>>> do
>>>> >> over
>>>> >> > the course of my odyssey this weekend?
>>>> >> >
>>>> >> > Ramping up on most projects involves some amount of supplementing the
>>>> >> > documentation with trial and error to figure out what to run, which
>>>> >> > "errors" are real errors and which can be ignored, etc., but
>>>> navigating
>>>> >> > that minefield on Spark has proved especially challenging and
>>>> >> > time-consuming for me. Some of that comes directly from scala's
>>>> >> relatively
>>>> >> > slow compilation times and immature build-tooling ecosystem, but
>>>> that is
>>>> >> > the world we live in and it would be nice if Spark took the
>>>> alleviation
>>>> >> of
>>>> >> > the resulting pain more seriously, as one of the more interesting and
>>>> >> > well-known large scala projects around right now. The official
>>>> >> > documentation around how to build different subsets of the codebase
>>>> is
>>>> >> > somewhat sparse [21], and there have been many mixed [22] accounts
>>>> [23]
>>>> >> on
>>>> >> > this mailing list about preferred ways to build on mvn vs. sbt (none
>>>> of
>>>> >> > which has made it into official documentation, as far as I've seen).
>>>> >> > Expecting new contributors to piece together all of this received
>>>> >> > folk-wisdom about how to build/test in a sane way by trawling mailing
>>>> >> list
>>>> >> > archives seems suboptimal.
>>>> >> >
>>>> >> > Thanks for reading, looking forward to hearing your ideas!
>>>> >> >
>>>> >> > -Ryan
>>>> >> >
>>>> >> > P.S. Is "best practice" for emailing this list to not incorporate any
>>>> >> HTML
>>>> >> > in the body? It seems like all of the archives I've seen strip it
>>>> out,
>>>> >> but
>>>> >> > other people have used it and gmail displays it.
>>>> >> >
>>>> >> >
>>>> >> > [1]
>>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>>> 8a162367c4dc157d2479/
>>>> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>>>> >> > (57 mins)
>>>> >> > [2]
>>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>>> 8a162367c4dc157d2479/
>>>> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>>>> >> > (6 mins)
>>>> >> > [3]
>>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>>> 8a162367c4dc157d2479/
>>>> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>>>> >> 20pass%20test,%20fail%20subsequent%20compile
>>>> >> > (4 mins)
>>>> >> > [4]
>>>> >> > https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&
>>>> >> cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-
>>>> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>>>> >> DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-
>>>> >> iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=
>>>> >> zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
>>>> >> > [5]
>>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>>> 8a162367c4dc157d2479/
>>>> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>>>> >> 20clean,%20need%20dependencies%20built
>>>> >> > [6]
>>>> >> > https://gist.githubusercontent.com/ryan-williams/
>>>> 8a162367c4dc157d2479/
>>>> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>>>> >> 20post%20clean
>>>> >> > (50 mins)
>>>> >> > [7]
>>>> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>>>> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>>>> >> > (1hr)
>>>> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
>>>> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>>>> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>>>> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>>>> >> > [12]
>>>> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>>>> >> file-gistfile1-txt-L853
>>>> >> > (~90 mins)
>>>> >> > [13]
>>>> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>>>> >> file-gistfile1-txt-L852
>>>> >> > (91 mins)
>>>> >> > [14]
>>>> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>>>> >> file-gistfile1-txt-L854
>>>> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>>>> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>>>> >> > [17]
>>>> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>>>> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>>>> >> > [18]
>>>> >> > http://stackoverflow.com/questions/25707629/why-does-
>>>> >> spark-job-fail-with-too-many-open-files
>>>> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>>>> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>>>> >> > [21]
>>>> >> > https://spark.apache.org/docs/latest/building-with-maven.
>>>> >> html#spark-tests-in-maven
>>>> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
>>>> >> > [23]
>>>> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>>>> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com
>>>> %3E
>>>> >>
>>>> >>
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 02:32:57 2014
Return-Path: <dev-return-10587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0D41108B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 02:32:57 +0000 (UTC)
Received: (qmail 68395 invoked by uid 500); 1 Dec 2014 02:32:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68311 invoked by uid 500); 1 Dec 2014 02:32:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68299 invoked by uid 99); 1 Dec 2014 02:32:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:32:56 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.180 as permitted sender)
Received: from [209.85.160.180] (HELO mail-yk0-f180.google.com) (209.85.160.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:32:52 +0000
Received: by mail-yk0-f180.google.com with SMTP id 9so4302830ykp.25
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 18:31:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=sFvMxjvifM8QxnaLM3FkHKGZacnjXzGrTqliOR2db1g=;
        b=HwIjHNQKBqnLsg84ItKNKyfjM8E70PkaSIbsGDhKUk/Kka6bBxv3u/RUfeDceUsmQJ
         Cgx08WxEExqzcioan2JorS64RGDLbYK5+1Azyzp8e6YKosDRGStaBmXVXVjJQDtBD7Pj
         en9HiUphenTmhgFMDteZ/vkxCWYGAVbQHvccUdjOMMHNeOuC37zD7iJ9qn+RpxL5z2fB
         lm2c8/FooVqcRwobVsdsAI9OUY1AY/hxccU1KHXBmN+qWxl1QQCQyjEFUIk5Tpq6WKFo
         qhxZUr/+nni4c7YqGdDFTjNOsbhgLfabARffdMjlikVMLvnAOmxR/U4RmTlcip9YSgrp
         u/Pw==
X-Received: by 10.236.110.35 with SMTP id t23mr56706814yhg.126.1417401106226;
 Sun, 30 Nov 2014 18:31:46 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
 <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
 <CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com> <CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Mon, 01 Dec 2014 02:31:43 +0000
Message-ID: <CANeJXFNuajJQC_TTW+gHVgaBDXHcoU9VMUDTKGNpabyDhzSBeQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133358a70e74405091e6931
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133358a70e74405091e6931
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks Mark, most of those commands are things I've been using and used in
my original post except for "Start zinc". I now see the section about it on
the "unpublished" building-spark
<https://github.com/apache/spark/blob/master/docs/building-spark.md#speedin=
g-up-compilation-with-zinc>
page and will try using it.

Even so, finding those commands took a nontrivial amount of trial and
error, I've not seen them very-well-documented outside of this list (your
and Matei's emails (and previous emails to this list) each have more info
about building/testing with Maven and SBT (resp.) than building-spark
<https://github.com/apache/spark/blob/master/docs/building-spark.md#spark-t=
ests-in-maven>
does),
the per-suite invocation is still subject to requiring assembly in some
cases ("without warning" from my perspective, having not read up on the
names of all Spark integration tests), spurious failures still abound,
there's no good way to run only the things that a given change actually
could have broken, etc.

Anyway, hopefully zinc brings me to the world of ~minute iteration times
that have been reported on this thread.


On Sun Nov 30 2014 at 6:53:57 PM Ryan Williams <
ryan.blake.williams@gmail.com> wrote:

> Thanks Nicholas, glad to hear that some of this info will be pushed to th=
e
> main site soon, but this brings up yet another point of confusion that I'=
ve
> struggled with, namely whether the documentation on github or that on
> spark.apache.org should be considered the primary reference for people
> seeking to learn about best practices for developing Spark.
>
> Trying to read docs starting from
> https://github.com/apache/spark/blob/master/docs/index.md right now, I
> find that all of the links to other parts of the documentation are broken=
:
> they point to relative paths that end in ".html", which will work when
> published on the docs-site, but that would have to end in ".md" if a pers=
on
> was to be able to navigate them on github.
>
> So expecting people to use the up-to-date docs on github (where all
> internal URLs 404 and the main github README suggests that the "latest
> Spark documentation" can be found on the actually-months-old docs-site
> <https://github.com/apache/spark#online-documentation>) is not a good
> solution. On the other hand, consulting months-old docs on the site is al=
so
> problematic, as this thread and your last email have borne out.  The resu=
lt
> is that there is no good place on the internet to learn about the most
> up-to-date best practices for using/developing Spark.
>
> Why not build http://spark.apache.org/docs/latest/ nightly (or every
> commit) off of what's in github, rather than having that URL point to the
> last release's docs (up to ~3 months old)? This way, casual users who wan=
t
> the docs for the released version they happen to be using (which is alrea=
dy
> frequently !=3D "/latest" today, for many Spark users) can (still) find t=
hem
> at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
> point people to a site (/latest) that actually has up-to-date docs that
> reflect ToT and whose links work.
>
> If there are concerns about existing semantics around "/latest" URLs bein=
g
> broken, some new URL could be used, like
> http://spark.apache.org/docs/snapshot/, but given that everything under
> http://spark.apache.org/docs/latest/ is in a state of
> planned-backwards-incompatible-changes every ~3mos, that doesn't sound li=
ke
> that serious an issue to me; anyone sending around permanent links to
> things under /latest is already going to have those links break / not mak=
e
> sense in the near future.
>
>
> On Sun Nov 30 2014 at 5:24:33 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>>
>>    - currently the docs only contain information about building with
>>    maven,
>>    and even then don=E2=80=99t cover many important cases
>>
>>  All other points aside, I just want to point out that the docs document
>> both how to use Maven and SBT and clearly state
>> <https://github.com/apache/spark/blob/master/docs/building-spark.md#buil=
ding-with-sbt>
>> that Maven is the =E2=80=9Cbuild of reference=E2=80=9D while SBT may be =
preferable for
>> day-to-day development.
>>
>> I believe the main reason most people miss this documentation is that,
>> though it=E2=80=99s up-to-date on GitHub, it has=E2=80=99t been publishe=
d yet to the docs
>> site. It should go out with the 1.2 release.
>>
>> Improvements to the documentation on building Spark belong here:
>> https://github.com/apache/spark/blob/master/docs/building-spark.md
>>
>> If there are clear recommendations that come out of this thread but are
>> not in that doc, they should be added in there. Other, less important
>> details may possibly be better suited for the Contributing to Spark
>> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark=
>
>> guide.
>>
>> Nick
>> =E2=80=8B
>>
>> On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Hey Ryan,
>>>
>>> A few more things here. You should feel free to send patches to
>>> Jenkins to test them, since this is the reference environment in which
>>> we regularly run tests. This is the normal workflow for most
>>> developers and we spend a lot of effort provisioning/maintaining a
>>> very large jenkins cluster to allow developers access this resource. A
>>> common development approach is to locally run tests that you've added
>>> in a patch, then send it to jenkins for the full run, and then try to
>>> debug locally if you see specific unanticipated test failures.
>>>
>>> One challenge we have is that given the proliferation of OS versions,
>>> Java versions, Python versions, ulimits, etc. there is a combinatorial
>>> number of environments in which tests could be run. It is very hard in
>>> some cases to figure out post-hoc why a given test is not working in a
>>> specific environment. I think a good solution here would be to use a
>>> standardized docker container for running Spark tests and asking folks
>>> to use that locally if they are trying to run all of the hundreds of
>>> Spark tests.
>>>
>>> Another solution would be to mock out every system interaction in
>>> Spark's tests including e.g. filesystem interactions to try and reduce
>>> variance across environments. However, that seems difficult.
>>>
>>> As the number of developers of Spark increases, it's definitely a good
>>> idea for us to invest in developer infrastructure including things
>>> like snapshot releases, better documentation, etc. Thanks for bringing
>>> this up as a pain point.
>>>
>>> - Patrick
>>>
>>>
>>> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
>>> <ryan.blake.williams@gmail.com> wrote:
>>> > thanks for the info, Matei and Brennon. I will try to switch my
>>> workflow to
>>> > using sbt. Other potential action items:
>>> >
>>> > - currently the docs only contain information about building with
>>> maven,
>>> > and even then don't cover many important cases, as I described in my
>>> > previous email. If SBT is as much better as you've described then tha=
t
>>> > should be made much more obvious. Wasn't it the case recently that
>>> there
>>> > was only a page about building with SBT, and not one about building
>>> with
>>> > maven? Clearer messaging around this needs to exist in the
>>> documentation,
>>> > not just on the mailing list, imho.
>>> >
>>> > - +1 to better distinguishing between unit and integration tests,
>>> having
>>> > separate scripts for each, improving documentation around common
>>> workflows,
>>> > expectations of brittleness with each kind of test, advisability of
>>> just
>>> > relying on Jenkins for certain kinds of tests to not waste too much
>>> time,
>>> > etc. Things like the compiler crash should be discussed in the
>>> > documentation, not just in the mailing list archives, if new
>>> contributors
>>> > are likely to run into them through no fault of their own.
>>> >
>>> > - What is the algorithm you use to decide what tests you might have
>>> broken?
>>> > Can we codify it in some scripts that other people can use?
>>> >
>>> >
>>> >
>>> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <
>>> matei.zaharia@gmail.com>
>>> > wrote:
>>> >
>>> >> Hi Ryan,
>>> >>
>>> >> As a tip (and maybe this isn't documented well), I normally use SBT
>>> for
>>> >> development to avoid the slow build process, and use its interactive
>>> >> console to run only specific tests. The nice advantage is that SBT
>>> can keep
>>> >> the Scala compiler loaded and JITed across builds, making it faster =
to
>>> >> iterate. To use it, you can do the following:
>>> >>
>>> >> - Start the SBT interactive console with sbt/sbt
>>> >> - Build your assembly by running the "assembly" target in the assemb=
ly
>>> >> project: assembly/assembly
>>> >> - Run all the tests in one module: core/test
>>> >> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>>> (this
>>> >> also supports tab completion)
>>> >>
>>> >> Running all the tests does take a while, and I usually just rely on
>>> >> Jenkins for that once I've run the tests for the things I believed m=
y
>>> patch
>>> >> could break. But this is because some of them are integration tests
>>> (e.g.
>>> >> DistributedSuite, which creates multi-process mini-clusters). Many o=
f
>>> the
>>> >> individual suites run fast without requiring this, however, so you
>>> can pick
>>> >> the ones you want. Perhaps we should find a way to tag them so
>>> people  can
>>> >> do a "quick-test" that skips the integration ones.
>>> >>
>>> >> The assembly builds are annoying but they only take about a minute
>>> for me
>>> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
>>> required
>>> >> for some of the "integration" tests (which launch new processes), bu=
t
>>> I'd
>>> >> recommend doing it all the time anyway since it would be very
>>> confusing to
>>> >> run those with an old assembly. The Scala compiler crash issue can
>>> also be
>>> >> a problem, but I don't see it very often with SBT. If it happens, I
>>> exit
>>> >> SBT and do sbt clean.
>>> >>
>>> >> Anyway, this is useful feedback and I think we should try to improve
>>> some
>>> >> of these suites, but hopefully you can also try the faster SBT
>>> process. At
>>> >> the end of the day, if we want integration tests, the whole test
>>> process
>>> >> will take an hour, but most of the developers I know leave that to
>>> Jenkins
>>> >> and only run individual tests locally before submitting a patch.
>>> >>
>>> >> Matei
>>> >>
>>> >>
>>> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>>> >> ryan.blake.williams@gmail.com> wrote:
>>> >> >
>>> >> > In the course of trying to make contributions to Spark, I have had
>>> a lot
>>> >> of
>>> >> > trouble running Spark's tests successfully. The main pain points
>>> I've
>>> >> > experienced are:
>>> >> >
>>> >> >    1) frequent, spurious test failures
>>> >> >    2) high latency of running tests
>>> >> >    3) difficulty running specific tests in an iterative fashion
>>> >> >
>>> >> > Here is an example series of failures that I encountered this
>>> weekend
>>> >> > (along with footnote links to the console output from each and
>>> >> > approximately how long each took):
>>> >> >
>>> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not
>>> seen
>>> >> > before.
>>> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
>>> >> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]:
>>> BroadcastSuite
>>> >> > passed, but scala compiler crashed on the "catalyst" project.
>>> >> > - `mvn clean`: some attempts to run earlier commands (that
>>> previously
>>> >> > didn't crash the compiler) all result in the same compiler crash.
>>> >> Previous
>>> >> > discussion on this list implies this can only be solved by a `mvn
>>> clean`
>>> >> > [4].
>>> >> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately
>>> post-clean,
>>> >> > BroadcastSuite can't run because assembly is not built.
>>> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>>> about
>>> >> > version mismatches and python 2.6. The machine this ran on has
>>> python
>>> >> 2.7,
>>> >> > so I don't know what that's about.
>>> >> > - `./dev/run-tests` again [7]: "too many open files" errors in
>>> several
>>> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>>> this is
>>> >> > not enough, but only some of the time? I increased it to 8192 and
>>> tried
>>> >> > again.
>>> >> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>>> seems
>>> >> to
>>> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>>> October
>>> >> 14;
>>> >> > not sure how I'm seeing it now. In any case, switched to Python 2.=
6
>>> and
>>> >> > installed unittest2, and python/run-tests seems to be unblocked.
>>> >> > - `./dev/run-tests` again [10]: finally passes!
>>> >> >
>>> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>>> trivial
>>> >> > changes added on (that I wanted to test before sending out a PR),
>>> on a
>>> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11=
].
>>> >> >
>>> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>>> >> commands
>>> >> > from the same repo state:
>>> >> >
>>> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>>> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know
>>> I've seen
>>> >> > this one before on this machine and am guessing it actually occurs
>>> every
>>> >> > time.
>>> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran
>>> one more
>>> >> > time from ceb6281, and saw the same failure.
>>> >> >
>>> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt =
to
>>> >> narrow
>>> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` o=
n
>>> my
>>> >> mac,
>>> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous ru=
ns
>>> >> used),
>>> >> > and it passed [16], so the failure seems specific to my linux
>>> >> machine/arch.
>>> >> >
>>> >> > At this point I believe that my changes don't break any tests (the
>>> >> > YarnClusterSuite failure on my linux presumably not being...
>>> "real"),
>>> >> and I
>>> >> > am ready to send out a PR. Whew!
>>> >> >
>>> >> > However, reflecting on the 5 or 6 distinct failure-modes represent=
ed
>>> >> above:
>>> >> >
>>> >> > - One of them (too many files open), is something I can (and did,
>>> >> > hopefully) fix once and for all. It cost me an ~hour this time
>>> >> (approximate
>>> >> > time of running ./dev/run-tests) and a few hours other times when =
I
>>> >> didn't
>>> >> > fully understand/fix it. It doesn't happen deterministically
>>> (why?), but
>>> >> > does happen somewhat frequently to people, having been discussed o=
n
>>> the
>>> >> > user list multiple times [17] and on SO [18]. Maybe some note in t=
he
>>> >> > documentation advising people to check their ulimit makes sense?
>>> >> > - One of them (unittest2 must be installed for python 2.6) was
>>> supposedly
>>> >> > fixed upstream of the commits I tested here; I don't know why I'm
>>> still
>>> >> > running into it. This cost me a few hours of running
>>> `./dev/run-tests`
>>> >> > multiple times to see if it was transient, plus some time
>>> researching and
>>> >> > working around it.
>>> >> > - The original BroadcastSuite failure cost me a few hours and went
>>> away
>>> >> > before I'd even run `mvn clean`.
>>> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>>> few
>>> >> > hours of running `./dev/run-tests` in different ways before decidi=
ng
>>> >> that,
>>> >> > as usual, there was no way around it and that I'd need to run `mvn
>>> clean`
>>> >> > and start running tests from scratch.
>>> >> > - The YarnClusterSuite failures on my linux box have cost me hours
>>> of
>>> >> > trying to figure out whether they're my fault. I've seen them many
>>> times
>>> >> > over the past weeks/months, plus or minus other failures that have
>>> come
>>> >> and
>>> >> > gone, and was especially befuddled by them when I was seeing a
>>> disjoint
>>> >> set
>>> >> > of reproducible failures on my mac [19] (the triaging of which
>>> involved
>>> >> > dozens of runs of `./dev/run-tests`).
>>> >> >
>>> >> > While I'm interested in digging into each of these issues, I also
>>> want to
>>> >> > discuss the frequency with which I've run into issues like these.
>>> This is
>>> >> > unfortunately not the first time in recent months that I've spent
>>> days
>>> >> > playing spurious-test-failure whack-a-mole with a 60-90min
>>> dev/run-tests
>>> >> > iteration time, which is no fun! So I am wondering/thinking:
>>> >> >
>>> >> > - Do other people experience this level of flakiness from spark
>>> tests?
>>> >> > - Do other people bother running dev/run-tests locally, or just le=
t
>>> >> Jenkins
>>> >> > do it during the CR process?
>>> >> > - Needing to run a full assembly post-clean just to continue
>>> running one
>>> >> > specific test case feels especially wasteful, and the failure
>>> output when
>>> >> > naively attempting to run a specific test without having built an
>>> >> assembly
>>> >> > jar is not always clear about what the issue is or how to fix it;
>>> even
>>> >> the
>>> >> > fact that certain tests require "building the world" is not
>>> something I
>>> >> > would have expected, and has cost me hours of confusion.
>>> >> >    - Should a person running spark tests assume that they must
>>> build an
>>> >> > assembly JAR before running anything?
>>> >> >    - Are there some proper "unit" tests that are actually
>>> self-contained
>>> >> /
>>> >> > able to be run without building an assembly jar?
>>> >> >    - Can we better document/demarcate which tests have which
>>> >> dependencies?
>>> >> >    - Is there something finer-grained than building an assembly JA=
R
>>> that
>>> >> > is sufficient in some cases?
>>> >> >        - If so, can we document that?
>>> >> >        - If not, can we move to a world of finer-grained
>>> dependencies for
>>> >> > some of these?
>>> >> > - Leaving all of these spurious failures aside, the process of
>>> assembling
>>> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
>>> >> typically,
>>> >> > respectively). I would guess that there are dozens (hundreds?) of
>>> people
>>> >> > who build a Spark assembly from various ToTs on any given day, and
>>> who
>>> >> all
>>> >> > wait on the exact same compilation / assembly steps to occur.
>>> Expanding
>>> >> on
>>> >> > the recent work to publish nightly snapshots [20], can we do a
>>> better job
>>> >> > caching/sharing compilation artifacts at a more granular level
>>> (pre-built
>>> >> > assembly JARs at each SHA? pre-built JARs per-maven-module,
>>> per-SHA? more
>>> >> > granular maven modules, plus the previous two?), or otherwise save
>>> some
>>> >> of
>>> >> > the considerable amount of redundant compilation work that I had t=
o
>>> do
>>> >> over
>>> >> > the course of my odyssey this weekend?
>>> >> >
>>> >> > Ramping up on most projects involves some amount of supplementing
>>> the
>>> >> > documentation with trial and error to figure out what to run, whic=
h
>>> >> > "errors" are real errors and which can be ignored, etc., but
>>> navigating
>>> >> > that minefield on Spark has proved especially challenging and
>>> >> > time-consuming for me. Some of that comes directly from scala's
>>> >> relatively
>>> >> > slow compilation times and immature build-tooling ecosystem, but
>>> that is
>>> >> > the world we live in and it would be nice if Spark took the
>>> alleviation
>>> >> of
>>> >> > the resulting pain more seriously, as one of the more interesting
>>> and
>>> >> > well-known large scala projects around right now. The official
>>> >> > documentation around how to build different subsets of the codebas=
e
>>> is
>>> >> > somewhat sparse [21], and there have been many mixed [22] accounts
>>> [23]
>>> >> on
>>> >> > this mailing list about preferred ways to build on mvn vs. sbt
>>> (none of
>>> >> > which has made it into official documentation, as far as I've seen=
).
>>> >> > Expecting new contributors to piece together all of this received
>>> >> > folk-wisdom about how to build/test in a sane way by trawling
>>> mailing
>>> >> list
>>> >> > archives seems suboptimal.
>>> >> >
>>> >> > Thanks for reading, looking forward to hearing your ideas!
>>> >> >
>>> >> > -Ryan
>>> >> >
>>> >> > P.S. Is "best practice" for emailing this list to not incorporate
>>> any
>>> >> HTML
>>> >> > in the body? It seems like all of the archives I've seen strip it
>>> out,
>>> >> but
>>> >> > other people have used it and gmail displays it.
>>> >> >
>>> >> >
>>> >> > [1]
>>> >> > https://gist.githubusercontent.com/ryan-williams/8a162367c4d
>>> c157d2479/
>>> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>>> >> > (57 mins)
>>> >> > [2]
>>> >> > https://gist.githubusercontent.com/ryan-williams/8a162367c4d
>>> c157d2479/
>>> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>>> >> > (6 mins)
>>> >> > [3]
>>> >> > https://gist.githubusercontent.com/ryan-williams/8a162367c4d
>>> c157d2479/
>>> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>>> >> 20pass%20test,%20fail%20subsequent%20compile
>>> >> > (4 mins)
>>> >> > [4]
>>> >> > https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dw=
eb&
>>> >> cd=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-
>>> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>>> >> DataTypeConversions-scala-td17083.html&ei=3DaRF6VJrpNKr-
>>> >> iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=3D
>>> >> zDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
>>> >> > [5]
>>> >> > https://gist.githubusercontent.com/ryan-williams/8a162367c4d
>>> c157d2479/
>>> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>>> >> 20clean,%20need%20dependencies%20built
>>> >> > [6]
>>> >> > https://gist.githubusercontent.com/ryan-williams/8a162367c4d
>>> c157d2479/
>>> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>>> >> 20post%20clean
>>> >> > (50 mins)
>>> >> > [7]
>>> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>>> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>>> >> > (1hr)
>>> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f
>>> (1hr)
>>> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>>> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>>> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>>> >> > [12]
>>> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>>> >> file-gistfile1-txt-L853
>>> >> > (~90 mins)
>>> >> > [13]
>>> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>>> >> file-gistfile1-txt-L852
>>> >> > (91 mins)
>>> >> > [14]
>>> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>>> >> file-gistfile1-txt-L854
>>> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>>> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>>> >> > [17]
>>> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>>> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>>> >> > [18]
>>> >> > http://stackoverflow.com/questions/25707629/why-does-
>>> >> spark-job-fail-with-too-many-open-files
>>> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>>> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>>> >> > [21]
>>> >> > https://spark.apache.org/docs/latest/building-with-maven.
>>> >> html#spark-tests-in-maven
>>> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.h
>>> tml
>>> >> > [23]
>>> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>>> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com
>>> %3E
>>> >>
>>> >>
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>

--001a1133358a70e74405091e6931--

From dev-return-10588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 02:40:34 2014
Return-Path: <dev-return-10588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B58510913
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 02:40:34 +0000 (UTC)
Received: (qmail 1340 invoked by uid 500); 1 Dec 2014 02:40:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1269 invoked by uid 500); 1 Dec 2014 02:40:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1257 invoked by uid 99); 1 Dec 2014 02:40:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:40:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:39:59 +0000
Received: by mail-pa0-f49.google.com with SMTP id eu11so10047549pac.22
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 18:39:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=0sDScrRx7+QSxtl8gcl6R3+e6hdJZwZhAr7OCOCVKCk=;
        b=Ev37qCaiVV+M2ROZXVLMWVi23N6U+Jma0JwL2JXhRUwMvU4+b1hrs+9ZE1o8O2qsLa
         1a677/WpQ/cFyeIqtljrz5DgfJ9MS6oyVIOG89VjiIu0/qQsdt9ujKVnLZzg2tjem3Px
         Za71/5wcUO9uR45PKLtfZMNHRQPE1ZisZfThjNh6wcoDHsLpKzwQhkdJCp8kUwm5lmeD
         NLBkv0+MAAPl4LzqUUhwHW8Z6PgU3/McTzaeZAgZB3mAOT4cnakJIWyuSNoJIcOUQZRm
         nDZOjfBQ7jpqMOaHvt39sQ0EJhXRZOqv7sG9SkyHgOBVC7/8t5GAdD5dA93BH25wt258
         HQww==
X-Received: by 10.68.230.10 with SMTP id su10mr38856394pbc.129.1417401553407;
        Sun, 30 Nov 2014 18:39:13 -0800 (PST)
Received: from [192.168.1.100] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id c9sm15998946pdn.81.2014.11.30.18.39.12
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 30 Nov 2014 18:39:12 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: [RESULT] [VOTE] Designating maintainers for some Spark components
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <24609BD1-BEEB-453B-A8F2-2B4D0077636E@gmail.com>
Date: Sun, 30 Nov 2014 18:39:11 -0800
Content-Transfer-Encoding: quoted-printable
Message-Id: <0523F6C4-C3BD-4CAF-A482-F1E72307AEC5@gmail.com>
References: <24609BD1-BEEB-453B-A8F2-2B4D0077636E@gmail.com>
To: dev <dev@spark.apache.org>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

An update on this: After adding the initial maintainer list, we got =
feedback to add more maintainers for some components, so we added four =
others (Josh Rosen for core API, Mark Hamstra for scheduler, Shivaram =
Venkataraman for MLlib and Xiangrui Meng for Python). We also decided to =
lower the "timeout" for waiting for a maintainer to a week. Hopefully =
this will provide more options for reviewing in these components.

The complete list is available at =
https://cwiki.apache.org/confluence/display/SPARK/Committers.

Matei

> On Nov 8, 2014, at 7:28 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:
>=20
> Thanks everyone for voting on this. With all of the PMC votes being =
for, the vote passes, but there were some concerns that I wanted to =
address for everyone who brought them up, as well as in the wording we =
will use for this policy.
>=20
> First, like every Apache project, Spark follows the Apache voting =
process (http://www.apache.org/foundation/voting.html), wherein all code =
changes are done by consensus. This means that any PMC member can block =
a code change on technical grounds, and thus that there is consensus =
when something goes in. It's absolutely true that every PMC member is =
responsible for the whole codebase, as Greg said (not least due to legal =
reasons, e.g. making sure it complies to licensing rules), and this idea =
will not change that. To make this clear, I will include that in the =
wording on the project page, to make sure new committers and other =
community members are all aware of it.
>=20
> What the maintainer model does, instead, is to change the review =
process, by having a required review from some people on some types of =
code changes (assuming those people respond in time). Projects can have =
their own diverse review processes (e.g. some do commit-then-review and =
others do review-then-commit, some point people to specific reviewers, =
etc). This kind of process seems useful to try (and to refine) as the =
project grows. We will of course evaluate how it goes and respond to any =
problems.
>=20
> So to summarize,
>=20
> - Every committer is responsible for, and more than welcome to review =
and vote on, every code change. In fact all community members are =
welcome to do this, and lots are doing it.
> - Everyone has the same voting rights on these code changes (namely =
consensus as described at http://www.apache.org/foundation/voting.html)
> - Committers will be asked to run patches that are making =
architectural and API changes by the maintainers before merging.
>=20
> In practice, none of this matters too much because we are not exactly =
a hot-well of discord ;), and even in the case of discord, the point of =
the ASF voting process is to create consensus. The goal is just to have =
a better structure for reviewing and minimize the chance of errors.
>=20
> Here is a tally of the votes:
>=20
> Binding votes (from PMC): 17 +1, no 0 or -1
>=20
> Matei Zaharia
> Michael Armbrust
> Reynold Xin
> Patrick Wendell
> Andrew Or
> Prashant Sharma
> Mark Hamstra
> Xiangrui Meng
> Ankur Dave
> Imran Rashid
> Jason Dai
> Tom Graves
> Sean McNamara
> Nick Pentreath
> Josh Rosen
> Kay Ousterhout
> Tathagata Das
>=20
> Non-binding votes: 18 +1, one +0, one -1
>=20
> +1:
> Nan Zhu
> Nicholas Chammas
> Denny Lee
> Cheng Lian
> Timothy Chen
> Jeremy Freeman
> Cheng Hao
> Jackylk Likun
> Kousuke Saruta
> Reza Zadeh
> Xuefeng Wu
> Witgo
> Manoj Babu
> Ravindra Pesala
> Liquan Pei
> Kushal Datta
> Davies Liu
> Vaquar Khan
>=20
> +0: Corey Nolet
>=20
> -1: Greg Stein
>=20
> I'll send another email when I have a more detailed writeup of this on =
the website.
>=20
> Matei


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 02:43:50 2014
Return-Path: <dev-return-10589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A40751092D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 02:43:50 +0000 (UTC)
Received: (qmail 11159 invoked by uid 500); 1 Dec 2014 02:43:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11078 invoked by uid 500); 1 Dec 2014 02:43:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11066 invoked by uid 99); 1 Dec 2014 02:43:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:43:49 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.169 as permitted sender)
Received: from [209.85.160.169] (HELO mail-yk0-f169.google.com) (209.85.160.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 02:43:45 +0000
Received: by mail-yk0-f169.google.com with SMTP id 79so4372408ykr.0
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 18:41:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=F4SYOUEZjhIhxzJFG8cHxzdI1gu163c+csaNLbu/RF4=;
        b=lcx+LQizOIxCLZc1eQn3CN+jE/sYchUM7CVWgQAs3nYG1E0JsSaDaSnqOuMcxqrdYX
         A8zS3tW+HSaiJeCQpCPoqwirnBMSGWA9Pfw/HfGouWyFAnq2TQrVa/9dtXEQh0TCc4Gu
         +pUwrs0NrUEbQca3GmsbwdX+kYNgThZ/rb5JrBwA1ogKqyTnHzzDzR1E+RI9o4LqOUO1
         0oLv/N887Ey+Cd+YCRX0Nz2Sr+uTPWXotBHyHxuoKi1vtLTPqjJ1mMAUQcdWcRU65q3k
         vyLof33goSJJW7VBBQm15yRBL1fOeN70KrZv2KRgwDzsZLnfQWaf5ht5W1voRf7gJNXl
         dMng==
X-Received: by 10.236.110.35 with SMTP id t23mr56734585yhg.126.1417401713968;
 Sun, 30 Nov 2014 18:41:53 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
 <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
 <CAOhmDzcNd-ukWPK5qtwuZ2hocrwE5PmvBYAJn4gyGjYpxgjXag@mail.gmail.com>
 <CANeJXFMem-rHgbUyEuDzzgnO2A4WMj4uJUBiR+0bsjnnKCDbkQ@mail.gmail.com>
 <CABPQxsswV3OiN8xLMThEXTCB3Q6KjGt5OpwYk5urZFB0csCn-g@mail.gmail.com> <CABPQxsvyH5vmbdvZzBjPGRCOBG3tg+F-z=VkgjOvVkSGRm__WQ@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Mon, 01 Dec 2014 02:41:52 +0000
Message-ID: <CANeJXFM8L8TsXGqoVsSBBV93ixw9HucmKUBYCq+M_m_oStfyiw@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133358aaa4ec905091e8db2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133358aaa4ec905091e8db2
Content-Type: text/plain; charset=UTF-8

Thanks Patrick, great to hear that docs-snapshots-via-jenkins is already
JIRA'd; you can interpret some of this thread as a gigantic +1 from me on
prioritizing that, which it looks like you are doing :)

I do understand the limitations of the "github vs. official site" status
quo; I was mostly responding to a perceived implication that I should have
been getting building/testing-spark advice from the github .md files
instead of from /latest. I agree that neither one works very well
currently, and that docs-snapshots-via-jenkins is the right solution. Per
my other email, leaving /latest as-is sounds reasonable, as long as jenkins
is putting the latest docs *somewhere*.

On Sun Nov 30 2014 at 7:19:33 PM Patrick Wendell <pwendell@gmail.com> wrote:

> Btw - the documnetation on github represents the source code of our
> docs, which is versioned with each release. Unfortunately github will
> always try to render ".md" files so it could look to a passerby like
> this is supposed to represent published docs. This is a feature
> limitation of github, AFAIK we cannot disable it.
>
> The official published docs are associated with each release and
> available on the apache.org website. I think "/latest" is a common
> convention for referring to the latest *published release* docs, so
> probably we can't change that (the audience for /latest is orders of
> magnitude larger than for snapshot docs). However we could just add
> /snapshot and publish docs there.
>
> - Patrick
>
> On Sun, Nov 30, 2014 at 6:15 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Hey Ryan,
> >
> > The existing JIRA also covers publishing nightly docs:
> > https://issues.apache.org/jira/browse/SPARK-1517
> >
> > - Patrick
> >
> > On Sun, Nov 30, 2014 at 5:53 PM, Ryan Williams
> > <ryan.blake.williams@gmail.com> wrote:
> >> Thanks Nicholas, glad to hear that some of this info will be pushed to
> the
> >> main site soon, but this brings up yet another point of confusion that
> I've
> >> struggled with, namely whether the documentation on github or that on
> >> spark.apache.org should be considered the primary reference for people
> >> seeking to learn about best practices for developing Spark.
> >>
> >> Trying to read docs starting from
> >> https://github.com/apache/spark/blob/master/docs/index.md right now, I
> find
> >> that all of the links to other parts of the documentation are broken:
> they
> >> point to relative paths that end in ".html", which will work when
> published
> >> on the docs-site, but that would have to end in ".md" if a person was
> to be
> >> able to navigate them on github.
> >>
> >> So expecting people to use the up-to-date docs on github (where all
> >> internal URLs 404 and the main github README suggests that the "latest
> >> Spark documentation" can be found on the actually-months-old docs-site
> >> <https://github.com/apache/spark#online-documentation>) is not a good
> >> solution. On the other hand, consulting months-old docs on the site is
> also
> >> problematic, as this thread and your last email have borne out.  The
> result
> >> is that there is no good place on the internet to learn about the most
> >> up-to-date best practices for using/developing Spark.
> >>
> >> Why not build http://spark.apache.org/docs/latest/ nightly (or every
> >> commit) off of what's in github, rather than having that URL point to
> the
> >> last release's docs (up to ~3 months old)? This way, casual users who
> want
> >> the docs for the released version they happen to be using (which is
> already
> >> frequently != "/latest" today, for many Spark users) can (still) find
> them
> >> at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
> >> point people to a site (/latest) that actually has up-to-date docs that
> >> reflect ToT and whose links work.
> >>
> >> If there are concerns about existing semantics around "/latest" URLs
> being
> >> broken, some new URL could be used, like
> >> http://spark.apache.org/docs/snapshot/, but given that everything under
> >> http://spark.apache.org/docs/latest/ is in a state of
> >> planned-backwards-incompatible-changes every ~3mos, that doesn't sound
> like
> >> that serious an issue to me; anyone sending around permanent links to
> >> things under /latest is already going to have those links break / not
> make
> >> sense in the near future.
> >>
> >>
> >> On Sun Nov 30 2014 at 5:24:33 PM Nicholas Chammas <
> >> nicholas.chammas@gmail.com> wrote:
> >>
> >>>
> >>>    - currently the docs only contain information about building with
> >>>    maven,
> >>>    and even then don't cover many important cases
> >>>
> >>>  All other points aside, I just want to point out that the docs
> document
> >>> both how to use Maven and SBT and clearly state
> >>> <https://github.com/apache/spark/blob/master/docs/
> building-spark.md#building-with-sbt>
> >>> that Maven is the "build of reference" while SBT may be preferable for
> >>> day-to-day development.
> >>>
> >>> I believe the main reason most people miss this documentation is that,
> >>> though it's up-to-date on GitHub, it has't been published yet to the
> docs
> >>> site. It should go out with the 1.2 release.
> >>>
> >>> Improvements to the documentation on building Spark belong here:
> >>> https://github.com/apache/spark/blob/master/docs/building-spark.md
> >>>
> >>> If there are clear recommendations that come out of this thread but are
> >>> not in that doc, they should be added in there. Other, less important
> >>> details may possibly be better suited for the Contributing to Spark
> >>> <https://cwiki.apache.org/confluence/display/SPARK/
> Contributing+to+Spark>
> >>> guide.
> >>>
> >>> Nick
> >>>
> >>>
> >>> On Sun Nov 30 2014 at 6:50:55 PM Patrick Wendell <pwendell@gmail.com>
> >>> wrote:
> >>>
> >>>> Hey Ryan,
> >>>>
> >>>> A few more things here. You should feel free to send patches to
> >>>> Jenkins to test them, since this is the reference environment in which
> >>>> we regularly run tests. This is the normal workflow for most
> >>>> developers and we spend a lot of effort provisioning/maintaining a
> >>>> very large jenkins cluster to allow developers access this resource. A
> >>>> common development approach is to locally run tests that you've added
> >>>> in a patch, then send it to jenkins for the full run, and then try to
> >>>> debug locally if you see specific unanticipated test failures.
> >>>>
> >>>> One challenge we have is that given the proliferation of OS versions,
> >>>> Java versions, Python versions, ulimits, etc. there is a combinatorial
> >>>> number of environments in which tests could be run. It is very hard in
> >>>> some cases to figure out post-hoc why a given test is not working in a
> >>>> specific environment. I think a good solution here would be to use a
> >>>> standardized docker container for running Spark tests and asking folks
> >>>> to use that locally if they are trying to run all of the hundreds of
> >>>> Spark tests.
> >>>>
> >>>> Another solution would be to mock out every system interaction in
> >>>> Spark's tests including e.g. filesystem interactions to try and reduce
> >>>> variance across environments. However, that seems difficult.
> >>>>
> >>>> As the number of developers of Spark increases, it's definitely a good
> >>>> idea for us to invest in developer infrastructure including things
> >>>> like snapshot releases, better documentation, etc. Thanks for bringing
> >>>> this up as a pain point.
> >>>>
> >>>> - Patrick
> >>>>
> >>>>
> >>>> On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
> >>>> <ryan.blake.williams@gmail.com> wrote:
> >>>> > thanks for the info, Matei and Brennon. I will try to switch my
> >>>> workflow to
> >>>> > using sbt. Other potential action items:
> >>>> >
> >>>> > - currently the docs only contain information about building with
> maven,
> >>>> > and even then don't cover many important cases, as I described in my
> >>>> > previous email. If SBT is as much better as you've described then
> that
> >>>> > should be made much more obvious. Wasn't it the case recently that
> there
> >>>> > was only a page about building with SBT, and not one about building
> with
> >>>> > maven? Clearer messaging around this needs to exist in the
> >>>> documentation,
> >>>> > not just on the mailing list, imho.
> >>>> >
> >>>> > - +1 to better distinguishing between unit and integration tests,
> having
> >>>> > separate scripts for each, improving documentation around common
> >>>> workflows,
> >>>> > expectations of brittleness with each kind of test, advisability of
> just
> >>>> > relying on Jenkins for certain kinds of tests to not waste too much
> >>>> time,
> >>>> > etc. Things like the compiler crash should be discussed in the
> >>>> > documentation, not just in the mailing list archives, if new
> >>>> contributors
> >>>> > are likely to run into them through no fault of their own.
> >>>> >
> >>>> > - What is the algorithm you use to decide what tests you might have
> >>>> broken?
> >>>> > Can we codify it in some scripts that other people can use?
> >>>> >
> >>>> >
> >>>> >
> >>>> > On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <
> matei.zaharia@gmail.com
> >>>> >
> >>>> > wrote:
> >>>> >
> >>>> >> Hi Ryan,
> >>>> >>
> >>>> >> As a tip (and maybe this isn't documented well), I normally use
> SBT for
> >>>> >> development to avoid the slow build process, and use its
> interactive
> >>>> >> console to run only specific tests. The nice advantage is that SBT
> can
> >>>> keep
> >>>> >> the Scala compiler loaded and JITed across builds, making it
> faster to
> >>>> >> iterate. To use it, you can do the following:
> >>>> >>
> >>>> >> - Start the SBT interactive console with sbt/sbt
> >>>> >> - Build your assembly by running the "assembly" target in the
> assembly
> >>>> >> project: assembly/assembly
> >>>> >> - Run all the tests in one module: core/test
> >>>> >> - Run a specific suite: core/test-only
> org.apache.spark.rdd.RDDSuite
> >>>> (this
> >>>> >> also supports tab completion)
> >>>> >>
> >>>> >> Running all the tests does take a while, and I usually just rely on
> >>>> >> Jenkins for that once I've run the tests for the things I believed
> my
> >>>> patch
> >>>> >> could break. But this is because some of them are integration tests
> >>>> (e.g.
> >>>> >> DistributedSuite, which creates multi-process mini-clusters). Many
> of
> >>>> the
> >>>> >> individual suites run fast without requiring this, however, so you
> can
> >>>> pick
> >>>> >> the ones you want. Perhaps we should find a way to tag them so
> people
> >>>> can
> >>>> >> do a "quick-test" that skips the integration ones.
> >>>> >>
> >>>> >> The assembly builds are annoying but they only take about a minute
> for
> >>>> me
> >>>> >> on a MacBook Pro with SBT warmed up. The assembly is actually only
> >>>> required
> >>>> >> for some of the "integration" tests (which launch new processes),
> but
> >>>> I'd
> >>>> >> recommend doing it all the time anyway since it would be very
> >>>> confusing to
> >>>> >> run those with an old assembly. The Scala compiler crash issue can
> >>>> also be
> >>>> >> a problem, but I don't see it very often with SBT. If it happens, I
> >>>> exit
> >>>> >> SBT and do sbt clean.
> >>>> >>
> >>>> >> Anyway, this is useful feedback and I think we should try to
> improve
> >>>> some
> >>>> >> of these suites, but hopefully you can also try the faster SBT
> >>>> process. At
> >>>> >> the end of the day, if we want integration tests, the whole test
> >>>> process
> >>>> >> will take an hour, but most of the developers I know leave that to
> >>>> Jenkins
> >>>> >> and only run individual tests locally before submitting a patch.
> >>>> >>
> >>>> >> Matei
> >>>> >>
> >>>> >>
> >>>> >> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
> >>>> >> ryan.blake.williams@gmail.com> wrote:
> >>>> >> >
> >>>> >> > In the course of trying to make contributions to Spark, I have
> had a
> >>>> lot
> >>>> >> of
> >>>> >> > trouble running Spark's tests successfully. The main pain points
> I've
> >>>> >> > experienced are:
> >>>> >> >
> >>>> >> >    1) frequent, spurious test failures
> >>>> >> >    2) high latency of running tests
> >>>> >> >    3) difficulty running specific tests in an iterative fashion
> >>>> >> >
> >>>> >> > Here is an example series of failures that I encountered this
> weekend
> >>>> >> > (along with footnote links to the console output from each and
> >>>> >> > approximately how long each took):
> >>>> >> >
> >>>> >> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not
> seen
> >>>> >> > before.
> >>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
> >>>> >> > - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]:
> >>>> BroadcastSuite
> >>>> >> > passed, but scala compiler crashed on the "catalyst" project.
> >>>> >> > - `mvn clean`: some attempts to run earlier commands (that
> previously
> >>>> >> > didn't crash the compiler) all result in the same compiler crash.
> >>>> >> Previous
> >>>> >> > discussion on this list implies this can only be solved by a `mvn
> >>>> clean`
> >>>> >> > [4].
> >>>> >> > - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately
> post-clean,
> >>>> >> > BroadcastSuite can't run because assembly is not built.
> >>>> >> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
> >>>> about
> >>>> >> > version mismatches and python 2.6. The machine this ran on has
> python
> >>>> >> 2.7,
> >>>> >> > so I don't know what that's about.
> >>>> >> > - `./dev/run-tests` again [7]: "too many open files" errors in
> >>>> several
> >>>> >> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
> >>>> this is
> >>>> >> > not enough, but only some of the time? I increased it to 8192 and
> >>>> tried
> >>>> >> > again.
> >>>> >> > - `./dev/run-tests` again [8]: same pyspark errors as before.
> This
> >>>> seems
> >>>> >> to
> >>>> >> > be the issue from SPARK-3867 [9], which was supposedly fixed on
> >>>> October
> >>>> >> 14;
> >>>> >> > not sure how I'm seeing it now. In any case, switched to Python
> 2.6
> >>>> and
> >>>> >> > installed unittest2, and python/run-tests seems to be unblocked.
> >>>> >> > - `./dev/run-tests` again [10]: finally passes!
> >>>> >> >
> >>>> >> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
> >>>> trivial
> >>>> >> > changes added on (that I wanted to test before sending out a
> PR), on
> >>>> a
> >>>> >> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3
> [11].
> >>>> >> >
> >>>> >> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried
> similar
> >>>> >> commands
> >>>> >> > from the same repo state:
> >>>> >> >
> >>>> >> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
> >>>> >> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know
> I've
> >>>> seen
> >>>> >> > this one before on this machine and am guessing it actually
> occurs
> >>>> every
> >>>> >> > time.
> >>>> >> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran
> one
> >>>> more
> >>>> >> > time from ceb6281, and saw the same failure.
> >>>> >> >
> >>>> >> > This was with java 1.7 and maven 3.2.3 [15]. In one final
> attempt to
> >>>> >> narrow
> >>>> >> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests`
> on
> >>>> my
> >>>> >> mac,
> >>>> >> > from ceb6281, with java 1.7 (instead of 1.8, which the previous
> runs
> >>>> >> used),
> >>>> >> > and it passed [16], so the failure seems specific to my linux
> >>>> >> machine/arch.
> >>>> >> >
> >>>> >> > At this point I believe that my changes don't break any tests
> (the
> >>>> >> > YarnClusterSuite failure on my linux presumably not being...
> "real"),
> >>>> >> and I
> >>>> >> > am ready to send out a PR. Whew!
> >>>> >> >
> >>>> >> > However, reflecting on the 5 or 6 distinct failure-modes
> represented
> >>>> >> above:
> >>>> >> >
> >>>> >> > - One of them (too many files open), is something I can (and did,
> >>>> >> > hopefully) fix once and for all. It cost me an ~hour this time
> >>>> >> (approximate
> >>>> >> > time of running ./dev/run-tests) and a few hours other times
> when I
> >>>> >> didn't
> >>>> >> > fully understand/fix it. It doesn't happen deterministically
> (why?),
> >>>> but
> >>>> >> > does happen somewhat frequently to people, having been discussed
> on
> >>>> the
> >>>> >> > user list multiple times [17] and on SO [18]. Maybe some note in
> the
> >>>> >> > documentation advising people to check their ulimit makes sense?
> >>>> >> > - One of them (unittest2 must be installed for python 2.6) was
> >>>> supposedly
> >>>> >> > fixed upstream of the commits I tested here; I don't know why I'm
> >>>> still
> >>>> >> > running into it. This cost me a few hours of running
> >>>> `./dev/run-tests`
> >>>> >> > multiple times to see if it was transient, plus some time
> >>>> researching and
> >>>> >> > working around it.
> >>>> >> > - The original BroadcastSuite failure cost me a few hours and
> went
> >>>> away
> >>>> >> > before I'd even run `mvn clean`.
> >>>> >> > - A new incarnation of the sbt-compiler-crash phenomenon cost me
> a
> >>>> few
> >>>> >> > hours of running `./dev/run-tests` in different ways before
> deciding
> >>>> >> that,
> >>>> >> > as usual, there was no way around it and that I'd need to run
> `mvn
> >>>> clean`
> >>>> >> > and start running tests from scratch.
> >>>> >> > - The YarnClusterSuite failures on my linux box have cost me
> hours of
> >>>> >> > trying to figure out whether they're my fault. I've seen them
> many
> >>>> times
> >>>> >> > over the past weeks/months, plus or minus other failures that
> have
> >>>> come
> >>>> >> and
> >>>> >> > gone, and was especially befuddled by them when I was seeing a
> >>>> disjoint
> >>>> >> set
> >>>> >> > of reproducible failures on my mac [19] (the triaging of which
> >>>> involved
> >>>> >> > dozens of runs of `./dev/run-tests`).
> >>>> >> >
> >>>> >> > While I'm interested in digging into each of these issues, I also
> >>>> want to
> >>>> >> > discuss the frequency with which I've run into issues like these.
> >>>> This is
> >>>> >> > unfortunately not the first time in recent months that I've spent
> >>>> days
> >>>> >> > playing spurious-test-failure whack-a-mole with a 60-90min
> >>>> dev/run-tests
> >>>> >> > iteration time, which is no fun! So I am wondering/thinking:
> >>>> >> >
> >>>> >> > - Do other people experience this level of flakiness from spark
> >>>> tests?
> >>>> >> > - Do other people bother running dev/run-tests locally, or just
> let
> >>>> >> Jenkins
> >>>> >> > do it during the CR process?
> >>>> >> > - Needing to run a full assembly post-clean just to continue
> running
> >>>> one
> >>>> >> > specific test case feels especially wasteful, and the failure
> output
> >>>> when
> >>>> >> > naively attempting to run a specific test without having built an
> >>>> >> assembly
> >>>> >> > jar is not always clear about what the issue is or how to fix it;
> >>>> even
> >>>> >> the
> >>>> >> > fact that certain tests require "building the world" is not
> >>>> something I
> >>>> >> > would have expected, and has cost me hours of confusion.
> >>>> >> >    - Should a person running spark tests assume that they must
> build
> >>>> an
> >>>> >> > assembly JAR before running anything?
> >>>> >> >    - Are there some proper "unit" tests that are actually
> >>>> self-contained
> >>>> >> /
> >>>> >> > able to be run without building an assembly jar?
> >>>> >> >    - Can we better document/demarcate which tests have which
> >>>> >> dependencies?
> >>>> >> >    - Is there something finer-grained than building an assembly
> JAR
> >>>> that
> >>>> >> > is sufficient in some cases?
> >>>> >> >        - If so, can we document that?
> >>>> >> >        - If not, can we move to a world of finer-grained
> >>>> dependencies for
> >>>> >> > some of these?
> >>>> >> > - Leaving all of these spurious failures aside, the process of
> >>>> assembling
> >>>> >> > and testing a new JAR is not a quick one (40 and 60 mins for me
> >>>> >> typically,
> >>>> >> > respectively). I would guess that there are dozens (hundreds?) of
> >>>> people
> >>>> >> > who build a Spark assembly from various ToTs on any given day,
> and
> >>>> who
> >>>> >> all
> >>>> >> > wait on the exact same compilation / assembly steps to occur.
> >>>> Expanding
> >>>> >> on
> >>>> >> > the recent work to publish nightly snapshots [20], can we do a
> >>>> better job
> >>>> >> > caching/sharing compilation artifacts at a more granular level
> >>>> (pre-built
> >>>> >> > assembly JARs at each SHA? pre-built JARs per-maven-module,
> per-SHA?
> >>>> more
> >>>> >> > granular maven modules, plus the previous two?), or otherwise
> save
> >>>> some
> >>>> >> of
> >>>> >> > the considerable amount of redundant compilation work that I had
> to
> >>>> do
> >>>> >> over
> >>>> >> > the course of my odyssey this weekend?
> >>>> >> >
> >>>> >> > Ramping up on most projects involves some amount of
> supplementing the
> >>>> >> > documentation with trial and error to figure out what to run,
> which
> >>>> >> > "errors" are real errors and which can be ignored, etc., but
> >>>> navigating
> >>>> >> > that minefield on Spark has proved especially challenging and
> >>>> >> > time-consuming for me. Some of that comes directly from scala's
> >>>> >> relatively
> >>>> >> > slow compilation times and immature build-tooling ecosystem, but
> >>>> that is
> >>>> >> > the world we live in and it would be nice if Spark took the
> >>>> alleviation
> >>>> >> of
> >>>> >> > the resulting pain more seriously, as one of the more
> interesting and
> >>>> >> > well-known large scala projects around right now. The official
> >>>> >> > documentation around how to build different subsets of the
> codebase
> >>>> is
> >>>> >> > somewhat sparse [21], and there have been many mixed [22]
> accounts
> >>>> [23]
> >>>> >> on
> >>>> >> > this mailing list about preferred ways to build on mvn vs. sbt
> (none
> >>>> of
> >>>> >> > which has made it into official documentation, as far as I've
> seen).
> >>>> >> > Expecting new contributors to piece together all of this received
> >>>> >> > folk-wisdom about how to build/test in a sane way by trawling
> mailing
> >>>> >> list
> >>>> >> > archives seems suboptimal.
> >>>> >> >
> >>>> >> > Thanks for reading, looking forward to hearing your ideas!
> >>>> >> >
> >>>> >> > -Ryan
> >>>> >> >
> >>>> >> > P.S. Is "best practice" for emailing this list to not
> incorporate any
> >>>> >> HTML
> >>>> >> > in the body? It seems like all of the archives I've seen strip it
> >>>> out,
> >>>> >> but
> >>>> >> > other people have used it and gmail displays it.
> >>>> >> >
> >>>> >> >
> >>>> >> > [1]
> >>>> >> > https://gist.githubusercontent.com/ryan-williams/
> >>>> 8a162367c4dc157d2479/
> >>>> >> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%
> 20fail
> >>>> >> > (57 mins)
> >>>> >> > [2]
> >>>> >> > https://gist.githubusercontent.com/ryan-williams/
> >>>> 8a162367c4dc157d2479/
> >>>> >> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> >>>> >> > (6 mins)
> >>>> >> > [3]
> >>>> >> > https://gist.githubusercontent.com/ryan-williams/
> >>>> 8a162367c4dc157d2479/
> >>>> >> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
> >>>> >> 20pass%20test,%20fail%20subsequent%20compile
> >>>> >> > (4 mins)
> >>>> >> > [4]
> >>>> >> > https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&
> >>>> >> cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-
> >>>> >> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
> >>>> >> DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-
> >>>> >> iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=
> >>>> >> zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
> >>>> >> > [5]
> >>>> >> > https://gist.githubusercontent.com/ryan-williams/
> >>>> 8a162367c4dc157d2479/
> >>>> >> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
> >>>> >> 20clean,%20need%20dependencies%20built
> >>>> >> > [6]
> >>>> >> > https://gist.githubusercontent.com/ryan-williams/
> >>>> 8a162367c4dc157d2479/
> >>>> >> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
> >>>> >> 20post%20clean
> >>>> >> > (50 mins)
> >>>> >> > [7]
> >>>> >> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
> >>>> >> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
> >>>> >> > (1hr)
> >>>> >> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f
> (1hr)
> >>>> >> > [9] https://issues.apache.org/jira/browse/SPARK-3867
> >>>> >> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> >>>> >> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> >>>> >> > [12]
> >>>> >> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
> >>>> >> file-gistfile1-txt-L853
> >>>> >> > (~90 mins)
> >>>> >> > [13]
> >>>> >> > https://gist.github.com/ryan-williams/718f6324af358819b496#
> >>>> >> file-gistfile1-txt-L852
> >>>> >> > (91 mins)
> >>>> >> > [14]
> >>>> >> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
> >>>> >> file-gistfile1-txt-L854
> >>>> >> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> >>>> >> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> >>>> >> > [17]
> >>>> >> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
> >>>> >> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
> >>>> >> > [18]
> >>>> >> > http://stackoverflow.com/questions/25707629/why-does-
> >>>> >> spark-job-fail-with-too-many-open-files
> >>>> >> > [19] https://issues.apache.org/jira/browse/SPARK-4002
> >>>> >> > [20] https://issues.apache.org/jira/browse/SPARK-4542
> >>>> >> > [21]
> >>>> >> > https://spark.apache.org/docs/latest/building-with-maven.
> >>>> >> html#spark-tests-in-maven
> >>>> >> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.
> html
> >>>> >> > [23]
> >>>> >> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
> >>>> >> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.
> gmail.com
> >>>> %3E
> >>>> >>
> >>>> >>
> >>>>
> >>>> ---------------------------------------------------------------------
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>
> >>>>
>

--001a1133358aaa4ec905091e8db2--

From dev-return-10590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 04:59:20 2014
Return-Path: <dev-return-10590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 657F510C5C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 04:59:20 +0000 (UTC)
Received: (qmail 76852 invoked by uid 500); 1 Dec 2014 04:59:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76775 invoked by uid 500); 1 Dec 2014 04:59:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76762 invoked by uid 99); 1 Dec 2014 04:59:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 04:59:18 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail02.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 04:58:53 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=19334; q=dns/txt; s=SM2048Apr2013K;
  t=1417409953; x=1417496353;
  h=from:to:cc:date:subject:message-id:references:
   in-reply-to:mime-version:content-transfer-encoding;
  bh=tfVFlknMzQ5E+Z6toeV2yBMsNGQAc2FPj/6akmr4n7U=;
  b=G/LyirNnd+X2kaRiPXvEncu6v4Avgyo0xbDICBWv4kiqaUr/ieoxs05E
   uRRRlBQzcQIvzhc/5HTwO54lOvaIb7e54LadMCdp5CGKHdJ5X67uhgq8K
   1AaZ9BMsvfvMJVFKbbvVjGoAOycAgCNSj+dRhHJwYGc3mjZFGFuilEaM8
   M/4ZsCcq17x4AemB8IPhgRigCmdseB1ygzC38G9nwuHh8f50iBMvgxi0H
   eSVg459T8gZpg8xYHqPjiArp+pD0ws16jZCnXTaf6GoOhPYJcWIKnXA4m
   uPON4ansZs6GAfk4epdO8YsMWnmrC/QC43FElyeU1GGek74TSIfuYZ59+
   w==;
X-IronPort-AV: E=McAfee;i="5600,1067,7638"; a="183969139"
X-IronPort-AV: E=Sophos;i="5.07,489,1413259200"; 
   d="scan'208";a="183969139"
X-OSD_Exception: TRUE
Received: from kdcpexcasht03.cof.ds.capitalone.com ([10.37.194.13])
  by komail02.kdc.capitalone.com with ESMTP; 30 Nov 2014 23:57:51 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 kdcpexcasht03.cof.ds.capitalone.com ([10.37.194.13]) with mapi; Sun, 30 Nov
 2014 23:57:51 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: Patrick Wendell <pwendell@gmail.com>, Ryan Williams
	<ryan.blake.williams@gmail.com>
CC: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Date: Sun, 30 Nov 2014 23:57:14 -0500
Subject: Re: Spurious test failures, testing best practices
Thread-Topic: Spurious test failures, testing best practices
Thread-Index: AdANI1udiQsk0pyOQm2NChNf/LYOuw==
Message-ID: <D0A15F29.5EE9%ilya.ganelin@capitalone.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
 <CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
 <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
In-Reply-To: <CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.3.140616
acceptlanguage: en-US
Content-Type: text/plain; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, Patrick - with regards to testing on Jenkins, is the process for this
to submit a pull request for the branch or is there another interface we
can use to submit a build to Jenkins for testing?

On 11/30/14, 6:49 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:

>Hey Ryan,
>
>A few more things here. You should feel free to send patches to
>Jenkins to test them, since this is the reference environment in which
>we regularly run tests. This is the normal workflow for most
>developers and we spend a lot of effort provisioning/maintaining a
>very large jenkins cluster to allow developers access this resource. A
>common development approach is to locally run tests that you've added
>in a patch, then send it to jenkins for the full run, and then try to
>debug locally if you see specific unanticipated test failures.
>
>One challenge we have is that given the proliferation of OS versions,
>Java versions, Python versions, ulimits, etc. there is a combinatorial
>number of environments in which tests could be run. It is very hard in
>some cases to figure out post-hoc why a given test is not working in a
>specific environment. I think a good solution here would be to use a
>standardized docker container for running Spark tests and asking folks
>to use that locally if they are trying to run all of the hundreds of
>Spark tests.
>
>Another solution would be to mock out every system interaction in
>Spark's tests including e.g. filesystem interactions to try and reduce
>variance across environments. However, that seems difficult.
>
>As the number of developers of Spark increases, it's definitely a good
>idea for us to invest in developer infrastructure including things
>like snapshot releases, better documentation, etc. Thanks for bringing
>this up as a pain point.
>
>- Patrick
>
>
>On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
><ryan.blake.williams@gmail.com> wrote:
>> thanks for the info, Matei and Brennon. I will try to switch my
>>workflow to
>> using sbt. Other potential action items:
>>
>> - currently the docs only contain information about building with maven,
>> and even then don't cover many important cases, as I described in my
>> previous email. If SBT is as much better as you've described then that
>> should be made much more obvious. Wasn't it the case recently that there
>> was only a page about building with SBT, and not one about building with
>> maven? Clearer messaging around this needs to exist in the
>>documentation,
>> not just on the mailing list, imho.
>>
>> - +1 to better distinguishing between unit and integration tests, having
>> separate scripts for each, improving documentation around common
>>workflows,
>> expectations of brittleness with each kind of test, advisability of just
>> relying on Jenkins for certain kinds of tests to not waste too much
>>time,
>> etc. Things like the compiler crash should be discussed in the
>> documentation, not just in the mailing list archives, if new
>>contributors
>> are likely to run into them through no fault of their own.
>>
>> - What is the algorithm you use to decide what tests you might have
>>broken?
>> Can we codify it in some scripts that other people can use?
>>
>>
>>
>> On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>
>>> Hi Ryan,
>>>
>>> As a tip (and maybe this isn't documented well), I normally use SBT for
>>> development to avoid the slow build process, and use its interactive
>>> console to run only specific tests. The nice advantage is that SBT can
>>>keep
>>> the Scala compiler loaded and JITed across builds, making it faster to
>>> iterate. To use it, you can do the following:
>>>
>>> - Start the SBT interactive console with sbt/sbt
>>> - Build your assembly by running the "assembly" target in the assembly
>>> project: assembly/assembly
>>> - Run all the tests in one module: core/test
>>> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>>>(this
>>> also supports tab completion)
>>>
>>> Running all the tests does take a while, and I usually just rely on
>>> Jenkins for that once I've run the tests for the things I believed my
>>>patch
>>> could break. But this is because some of them are integration tests
>>>(e.g.
>>> DistributedSuite, which creates multi-process mini-clusters). Many of
>>>the
>>> individual suites run fast without requiring this, however, so you can
>>>pick
>>> the ones you want. Perhaps we should find a way to tag them so people
>>>can
>>> do a "quick-test" that skips the integration ones.
>>>
>>> The assembly builds are annoying but they only take about a minute for
>>>me
>>> on a MacBook Pro with SBT warmed up. The assembly is actually only
>>>required
>>> for some of the "integration" tests (which launch new processes), but
>>>I'd
>>> recommend doing it all the time anyway since it would be very
>>>confusing to
>>> run those with an old assembly. The Scala compiler crash issue can
>>>also be
>>> a problem, but I don't see it very often with SBT. If it happens, I
>>>exit
>>> SBT and do sbt clean.
>>>
>>> Anyway, this is useful feedback and I think we should try to improve
>>>some
>>> of these suites, but hopefully you can also try the faster SBT
>>>process. At
>>> the end of the day, if we want integration tests, the whole test
>>>process
>>> will take an hour, but most of the developers I know leave that to
>>>Jenkins
>>> and only run individual tests locally before submitting a patch.
>>>
>>> Matei
>>>
>>>
>>> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>>> ryan.blake.williams@gmail.com> wrote:
>>> >
>>> > In the course of trying to make contributions to Spark, I have had a
>>>lot
>>> of
>>> > trouble running Spark's tests successfully. The main pain points I've
>>> > experienced are:
>>> >
>>> >    1) frequent, spurious test failures
>>> >    2) high latency of running tests
>>> >    3) difficulty running specific tests in an iterative fashion
>>> >
>>> > Here is an example series of failures that I encountered this weekend
>>> > (along with footnote links to the console output from each and
>>> > approximately how long each took):
>>> >
>>> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
>>> > before.
>>> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
>>> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]:
>>>BroadcastSuite
>>> > passed, but scala compiler crashed on the "catalyst" project.
>>> > - `mvn clean`: some attempts to run earlier commands (that previously
>>> > didn't crash the compiler) all result in the same compiler crash.
>>> Previous
>>> > discussion on this list implies this can only be solved by a `mvn
>>>clean`
>>> > [4].
>>> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-clea=
n,
>>> > BroadcastSuite can't run because assembly is not built.
>>> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>>>about
>>> > version mismatches and python 2.6. The machine this ran on has python
>>> 2.7,
>>> > so I don't know what that's about.
>>> > - `./dev/run-tests` again [7]: "too many open files" errors in
>>>several
>>> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>>>this is
>>> > not enough, but only some of the time? I increased it to 8192 and
>>>tried
>>> > again.
>>> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>>>seems
>>> to
>>> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>>>October
>>> 14;
>>> > not sure how I'm seeing it now. In any case, switched to Python 2.6
>>>and
>>> > installed unittest2, and python/run-tests seems to be unblocked.
>>> > - `./dev/run-tests` again [10]: finally passes!
>>> >
>>> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>>>trivial
>>> > changes added on (that I wanted to test before sending out a PR), on
>>>a
>>> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
>>> >
>>> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>>> commands
>>> > from the same repo state:
>>> >
>>> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>>> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
>>>seen
>>> > this one before on this machine and am guessing it actually occurs
>>>every
>>> > time.
>>> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
>>>more
>>> > time from ceb6281, and saw the same failure.
>>> >
>>> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
>>> narrow
>>> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on
>>>my
>>> mac,
>>> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
>>> used),
>>> > and it passed [16], so the failure seems specific to my linux
>>> machine/arch.
>>> >
>>> > At this point I believe that my changes don't break any tests (the
>>> > YarnClusterSuite failure on my linux presumably not being... "real"),
>>> and I
>>> > am ready to send out a PR. Whew!
>>> >
>>> > However, reflecting on the 5 or 6 distinct failure-modes represented
>>> above:
>>> >
>>> > - One of them (too many files open), is something I can (and did,
>>> > hopefully) fix once and for all. It cost me an ~hour this time
>>> (approximate
>>> > time of running ./dev/run-tests) and a few hours other times when I
>>> didn't
>>> > fully understand/fix it. It doesn't happen deterministically (why?),
>>>but
>>> > does happen somewhat frequently to people, having been discussed on
>>>the
>>> > user list multiple times [17] and on SO [18]. Maybe some note in the
>>> > documentation advising people to check their ulimit makes sense?
>>> > - One of them (unittest2 must be installed for python 2.6) was
>>>supposedly
>>> > fixed upstream of the commits I tested here; I don't know why I'm
>>>still
>>> > running into it. This cost me a few hours of running
>>>`./dev/run-tests`
>>> > multiple times to see if it was transient, plus some time
>>>researching and
>>> > working around it.
>>> > - The original BroadcastSuite failure cost me a few hours and went
>>>away
>>> > before I'd even run `mvn clean`.
>>> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>>>few
>>> > hours of running `./dev/run-tests` in different ways before deciding
>>> that,
>>> > as usual, there was no way around it and that I'd need to run `mvn
>>>clean`
>>> > and start running tests from scratch.
>>> > - The YarnClusterSuite failures on my linux box have cost me hours of
>>> > trying to figure out whether they're my fault. I've seen them many
>>>times
>>> > over the past weeks/months, plus or minus other failures that have
>>>come
>>> and
>>> > gone, and was especially befuddled by them when I was seeing a
>>>disjoint
>>> set
>>> > of reproducible failures on my mac [19] (the triaging of which
>>>involved
>>> > dozens of runs of `./dev/run-tests`).
>>> >
>>> > While I'm interested in digging into each of these issues, I also
>>>want to
>>> > discuss the frequency with which I've run into issues like these.
>>>This is
>>> > unfortunately not the first time in recent months that I've spent
>>>days
>>> > playing spurious-test-failure whack-a-mole with a 60-90min
>>>dev/run-tests
>>> > iteration time, which is no fun! So I am wondering/thinking:
>>> >
>>> > - Do other people experience this level of flakiness from spark
>>>tests?
>>> > - Do other people bother running dev/run-tests locally, or just let
>>> Jenkins
>>> > do it during the CR process?
>>> > - Needing to run a full assembly post-clean just to continue running
>>>one
>>> > specific test case feels especially wasteful, and the failure output
>>>when
>>> > naively attempting to run a specific test without having built an
>>> assembly
>>> > jar is not always clear about what the issue is or how to fix it;
>>>even
>>> the
>>> > fact that certain tests require "building the world" is not
>>>something I
>>> > would have expected, and has cost me hours of confusion.
>>> >    - Should a person running spark tests assume that they must build
>>>an
>>> > assembly JAR before running anything?
>>> >    - Are there some proper "unit" tests that are actually
>>>self-contained
>>> /
>>> > able to be run without building an assembly jar?
>>> >    - Can we better document/demarcate which tests have which
>>> dependencies?
>>> >    - Is there something finer-grained than building an assembly JAR
>>>that
>>> > is sufficient in some cases?
>>> >        - If so, can we document that?
>>> >        - If not, can we move to a world of finer-grained
>>>dependencies for
>>> > some of these?
>>> > - Leaving all of these spurious failures aside, the process of
>>>assembling
>>> > and testing a new JAR is not a quick one (40 and 60 mins for me
>>> typically,
>>> > respectively). I would guess that there are dozens (hundreds?) of
>>>people
>>> > who build a Spark assembly from various ToTs on any given day, and
>>>who
>>> all
>>> > wait on the exact same compilation / assembly steps to occur.
>>>Expanding
>>> on
>>> > the recent work to publish nightly snapshots [20], can we do a
>>>better job
>>> > caching/sharing compilation artifacts at a more granular level
>>>(pre-built
>>> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
>>>more
>>> > granular maven modules, plus the previous two?), or otherwise save
>>>some
>>> of
>>> > the considerable amount of redundant compilation work that I had to
>>>do
>>> over
>>> > the course of my odyssey this weekend?
>>> >
>>> > Ramping up on most projects involves some amount of supplementing the
>>> > documentation with trial and error to figure out what to run, which
>>> > "errors" are real errors and which can be ignored, etc., but
>>>navigating
>>> > that minefield on Spark has proved especially challenging and
>>> > time-consuming for me. Some of that comes directly from scala's
>>> relatively
>>> > slow compilation times and immature build-tooling ecosystem, but
>>>that is
>>> > the world we live in and it would be nice if Spark took the
>>>alleviation
>>> of
>>> > the resulting pain more seriously, as one of the more interesting and
>>> > well-known large scala projects around right now. The official
>>> > documentation around how to build different subsets of the codebase
>>>is
>>> > somewhat sparse [21], and there have been many mixed [22] accounts
>>>[23]
>>> on
>>> > this mailing list about preferred ways to build on mvn vs. sbt (none
>>>of
>>> > which has made it into official documentation, as far as I've seen).
>>> > Expecting new contributors to piece together all of this received
>>> > folk-wisdom about how to build/test in a sane way by trawling mailing
>>> list
>>> > archives seems suboptimal.
>>> >
>>> > Thanks for reading, looking forward to hearing your ideas!
>>> >
>>> > -Ryan
>>> >
>>> > P.S. Is "best practice" for emailing this list to not incorporate any
>>> HTML
>>> > in the body? It seems like all of the archives I've seen strip it
>>>out,
>>> but
>>> > other people have used it and gmail displays it.
>>> >
>>> >
>>> > [1]
>>> >
>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>>> > (57 mins)
>>> > [2]
>>> >
>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>>> > (6 mins)
>>> > [3]
>>> >
>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>>> 20pass%20test,%20fail%20subsequent%20compile
>>> > (4 mins)
>>> > [4]
>>> > https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dweb&
>>> cd=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-
>>> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>>> DataTypeConversions-scala-td17083.html&ei=3DaRF6VJrpNKr-
>>> iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=3D
>>> zDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
>>> > [5]
>>> >
>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>>> 20clean,%20need%20dependencies%20built
>>> > [6]
>>> >
>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>>> 20post%20clean
>>> > (50 mins)
>>> > [7]
>>> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>>> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>>> > (1hr)
>>> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
>>> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>>> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>>> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>>> > [12]
>>> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>>> file-gistfile1-txt-L853
>>> > (~90 mins)
>>> > [13]
>>> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>>> file-gistfile1-txt-L852
>>> > (91 mins)
>>> > [14]
>>> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>>> file-gistfile1-txt-L854
>>> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>>> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>>> > [17]
>>> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>>> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>>> > [18]
>>> > http://stackoverflow.com/questions/25707629/why-does-
>>> spark-job-fail-with-too-many-open-files
>>> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>>> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>>> > [21]
>>> > https://spark.apache.org/docs/latest/building-with-maven.
>>> html#spark-tests-in-maven
>>> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
>>> > [23]
>>> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>>> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
>>>
>>>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 05:02:50 2014
Return-Path: <dev-return-10591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D2FCC10CA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 05:02:50 +0000 (UTC)
Received: (qmail 84559 invoked by uid 500); 1 Dec 2014 05:02:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84483 invoked by uid 500); 1 Dec 2014 05:02:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84466 invoked by uid 99); 1 Dec 2014 05:02:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 05:02:49 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 05:02:23 +0000
Received: by mail-ob0-f173.google.com with SMTP id uy5so7436218obc.32
        for <dev@spark.apache.org>; Sun, 30 Nov 2014 21:01:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=8c6sMj+2wiVKqT7PNNT9T/Pi+l0eDKSFC/3z1iRtaX8=;
        b=ke0ZRqlGJ+znSrqzDtzuk1fADZlHqDy0rX0Om910MYpe89FhKXibYji0ykrCrXfDw3
         S7cD0BeC9PieLkCiuuS72WSZncBbeIuParudxPxQQyP+lGFvN2/70awn/vKfb6q4kAag
         6HifMQyraD82ia3/2msiDq05yrLryuwiX7+eBN0e7zPM+4Aeu3mPHBi5O5nhXp/ZyMep
         OiiE09AnM4M9R+R6DasHpNGh+Ra21+ZXaHRR4i142FGGwdOL9xq40Zslt2oik4qus/ER
         /d3kYbDKapsTkxw0Da6VNzFef00TYs2C1VPY+arjYRJYEP2i38BzZ9r37xsNGBf8Mtin
         ezzQ==
MIME-Version: 1.0
X-Received: by 10.60.56.78 with SMTP id y14mr33251913oep.52.1417410097208;
 Sun, 30 Nov 2014 21:01:37 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Sun, 30 Nov 2014 21:01:37 -0800 (PST)
In-Reply-To: <D0A15F29.5EE9%ilya.ganelin@capitalone.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CANeJXFNvCjXU=wgEFm52dQ7stxJz+ZuvvjVqVnWyjpBa8EdCZw@mail.gmail.com>
	<CABPQxsuo3WvfsEYV=tzx94biU4-M3B45uLiNtQQbnXVBJbG3pA@mail.gmail.com>
	<D0A15F29.5EE9%ilya.ganelin@capitalone.com>
Date: Sun, 30 Nov 2014 21:01:37 -0800
Message-ID: <CABPQxsuiTc3yXDAFnp4rBm4MbNi9Sn_WyXRn6kL4kJuHS5P9NQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Patrick Wendell <pwendell@gmail.com>
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: Ryan Williams <ryan.blake.williams@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Ilya - you can just submit a pull request and the way we test them
is to run it through jenkins. You don't need to do anything special.

On Sun, Nov 30, 2014 at 8:57 PM, Ganelin, Ilya
<Ilya.Ganelin@capitalone.com> wrote:
> Hi, Patrick - with regards to testing on Jenkins, is the process for this
> to submit a pull request for the branch or is there another interface we
> can use to submit a build to Jenkins for testing?
>
> On 11/30/14, 6:49 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>>Hey Ryan,
>>
>>A few more things here. You should feel free to send patches to
>>Jenkins to test them, since this is the reference environment in which
>>we regularly run tests. This is the normal workflow for most
>>developers and we spend a lot of effort provisioning/maintaining a
>>very large jenkins cluster to allow developers access this resource. A
>>common development approach is to locally run tests that you've added
>>in a patch, then send it to jenkins for the full run, and then try to
>>debug locally if you see specific unanticipated test failures.
>>
>>One challenge we have is that given the proliferation of OS versions,
>>Java versions, Python versions, ulimits, etc. there is a combinatorial
>>number of environments in which tests could be run. It is very hard in
>>some cases to figure out post-hoc why a given test is not working in a
>>specific environment. I think a good solution here would be to use a
>>standardized docker container for running Spark tests and asking folks
>>to use that locally if they are trying to run all of the hundreds of
>>Spark tests.
>>
>>Another solution would be to mock out every system interaction in
>>Spark's tests including e.g. filesystem interactions to try and reduce
>>variance across environments. However, that seems difficult.
>>
>>As the number of developers of Spark increases, it's definitely a good
>>idea for us to invest in developer infrastructure including things
>>like snapshot releases, better documentation, etc. Thanks for bringing
>>this up as a pain point.
>>
>>- Patrick
>>
>>
>>On Sun, Nov 30, 2014 at 3:35 PM, Ryan Williams
>><ryan.blake.williams@gmail.com> wrote:
>>> thanks for the info, Matei and Brennon. I will try to switch my
>>>workflow to
>>> using sbt. Other potential action items:
>>>
>>> - currently the docs only contain information about building with maven=
,
>>> and even then don't cover many important cases, as I described in my
>>> previous email. If SBT is as much better as you've described then that
>>> should be made much more obvious. Wasn't it the case recently that ther=
e
>>> was only a page about building with SBT, and not one about building wit=
h
>>> maven? Clearer messaging around this needs to exist in the
>>>documentation,
>>> not just on the mailing list, imho.
>>>
>>> - +1 to better distinguishing between unit and integration tests, havin=
g
>>> separate scripts for each, improving documentation around common
>>>workflows,
>>> expectations of brittleness with each kind of test, advisability of jus=
t
>>> relying on Jenkins for certain kinds of tests to not waste too much
>>>time,
>>> etc. Things like the compiler crash should be discussed in the
>>> documentation, not just in the mailing list archives, if new
>>>contributors
>>> are likely to run into them through no fault of their own.
>>>
>>> - What is the algorithm you use to decide what tests you might have
>>>broken?
>>> Can we codify it in some scripts that other people can use?
>>>
>>>
>>>
>>> On Sun Nov 30 2014 at 4:06:41 PM Matei Zaharia <matei.zaharia@gmail.com=
>
>>> wrote:
>>>
>>>> Hi Ryan,
>>>>
>>>> As a tip (and maybe this isn't documented well), I normally use SBT fo=
r
>>>> development to avoid the slow build process, and use its interactive
>>>> console to run only specific tests. The nice advantage is that SBT can
>>>>keep
>>>> the Scala compiler loaded and JITed across builds, making it faster to
>>>> iterate. To use it, you can do the following:
>>>>
>>>> - Start the SBT interactive console with sbt/sbt
>>>> - Build your assembly by running the "assembly" target in the assembly
>>>> project: assembly/assembly
>>>> - Run all the tests in one module: core/test
>>>> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>>>>(this
>>>> also supports tab completion)
>>>>
>>>> Running all the tests does take a while, and I usually just rely on
>>>> Jenkins for that once I've run the tests for the things I believed my
>>>>patch
>>>> could break. But this is because some of them are integration tests
>>>>(e.g.
>>>> DistributedSuite, which creates multi-process mini-clusters). Many of
>>>>the
>>>> individual suites run fast without requiring this, however, so you can
>>>>pick
>>>> the ones you want. Perhaps we should find a way to tag them so people
>>>>can
>>>> do a "quick-test" that skips the integration ones.
>>>>
>>>> The assembly builds are annoying but they only take about a minute for
>>>>me
>>>> on a MacBook Pro with SBT warmed up. The assembly is actually only
>>>>required
>>>> for some of the "integration" tests (which launch new processes), but
>>>>I'd
>>>> recommend doing it all the time anyway since it would be very
>>>>confusing to
>>>> run those with an old assembly. The Scala compiler crash issue can
>>>>also be
>>>> a problem, but I don't see it very often with SBT. If it happens, I
>>>>exit
>>>> SBT and do sbt clean.
>>>>
>>>> Anyway, this is useful feedback and I think we should try to improve
>>>>some
>>>> of these suites, but hopefully you can also try the faster SBT
>>>>process. At
>>>> the end of the day, if we want integration tests, the whole test
>>>>process
>>>> will take an hour, but most of the developers I know leave that to
>>>>Jenkins
>>>> and only run individual tests locally before submitting a patch.
>>>>
>>>> Matei
>>>>
>>>>
>>>> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>>>> ryan.blake.williams@gmail.com> wrote:
>>>> >
>>>> > In the course of trying to make contributions to Spark, I have had a
>>>>lot
>>>> of
>>>> > trouble running Spark's tests successfully. The main pain points I'v=
e
>>>> > experienced are:
>>>> >
>>>> >    1) frequent, spurious test failures
>>>> >    2) high latency of running tests
>>>> >    3) difficulty running specific tests in an iterative fashion
>>>> >
>>>> > Here is an example series of failures that I encountered this weeken=
d
>>>> > (along with footnote links to the console output from each and
>>>> > approximately how long each took):
>>>> >
>>>> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not see=
n
>>>> > before.
>>>> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
>>>> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]:
>>>>BroadcastSuite
>>>> > passed, but scala compiler crashed on the "catalyst" project.
>>>> > - `mvn clean`: some attempts to run earlier commands (that previousl=
y
>>>> > didn't crash the compiler) all result in the same compiler crash.
>>>> Previous
>>>> > discussion on this list implies this can only be solved by a `mvn
>>>>clean`
>>>> > [4].
>>>> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-cle=
an,
>>>> > BroadcastSuite can't run because assembly is not built.
>>>> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages
>>>>about
>>>> > version mismatches and python 2.6. The machine this ran on has pytho=
n
>>>> 2.7,
>>>> > so I don't know what that's about.
>>>> > - `./dev/run-tests` again [7]: "too many open files" errors in
>>>>several
>>>> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently
>>>>this is
>>>> > not enough, but only some of the time? I increased it to 8192 and
>>>>tried
>>>> > again.
>>>> > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>>>>seems
>>>> to
>>>> > be the issue from SPARK-3867 [9], which was supposedly fixed on
>>>>October
>>>> 14;
>>>> > not sure how I'm seeing it now. In any case, switched to Python 2.6
>>>>and
>>>> > installed unittest2, and python/run-tests seems to be unblocked.
>>>> > - `./dev/run-tests` again [10]: finally passes!
>>>> >
>>>> > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>>>>trivial
>>>> > changes added on (that I wanted to test before sending out a PR), on
>>>>a
>>>> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
>>>> >
>>>> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>>>> commands
>>>> > from the same repo state:
>>>> >
>>>> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>>>> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
>>>>seen
>>>> > this one before on this machine and am guessing it actually occurs
>>>>every
>>>> > time.
>>>> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
>>>>more
>>>> > time from ceb6281, and saw the same failure.
>>>> >
>>>> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
>>>> narrow
>>>> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on
>>>>my
>>>> mac,
>>>> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
>>>> used),
>>>> > and it passed [16], so the failure seems specific to my linux
>>>> machine/arch.
>>>> >
>>>> > At this point I believe that my changes don't break any tests (the
>>>> > YarnClusterSuite failure on my linux presumably not being... "real")=
,
>>>> and I
>>>> > am ready to send out a PR. Whew!
>>>> >
>>>> > However, reflecting on the 5 or 6 distinct failure-modes represented
>>>> above:
>>>> >
>>>> > - One of them (too many files open), is something I can (and did,
>>>> > hopefully) fix once and for all. It cost me an ~hour this time
>>>> (approximate
>>>> > time of running ./dev/run-tests) and a few hours other times when I
>>>> didn't
>>>> > fully understand/fix it. It doesn't happen deterministically (why?),
>>>>but
>>>> > does happen somewhat frequently to people, having been discussed on
>>>>the
>>>> > user list multiple times [17] and on SO [18]. Maybe some note in the
>>>> > documentation advising people to check their ulimit makes sense?
>>>> > - One of them (unittest2 must be installed for python 2.6) was
>>>>supposedly
>>>> > fixed upstream of the commits I tested here; I don't know why I'm
>>>>still
>>>> > running into it. This cost me a few hours of running
>>>>`./dev/run-tests`
>>>> > multiple times to see if it was transient, plus some time
>>>>researching and
>>>> > working around it.
>>>> > - The original BroadcastSuite failure cost me a few hours and went
>>>>away
>>>> > before I'd even run `mvn clean`.
>>>> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a
>>>>few
>>>> > hours of running `./dev/run-tests` in different ways before deciding
>>>> that,
>>>> > as usual, there was no way around it and that I'd need to run `mvn
>>>>clean`
>>>> > and start running tests from scratch.
>>>> > - The YarnClusterSuite failures on my linux box have cost me hours o=
f
>>>> > trying to figure out whether they're my fault. I've seen them many
>>>>times
>>>> > over the past weeks/months, plus or minus other failures that have
>>>>come
>>>> and
>>>> > gone, and was especially befuddled by them when I was seeing a
>>>>disjoint
>>>> set
>>>> > of reproducible failures on my mac [19] (the triaging of which
>>>>involved
>>>> > dozens of runs of `./dev/run-tests`).
>>>> >
>>>> > While I'm interested in digging into each of these issues, I also
>>>>want to
>>>> > discuss the frequency with which I've run into issues like these.
>>>>This is
>>>> > unfortunately not the first time in recent months that I've spent
>>>>days
>>>> > playing spurious-test-failure whack-a-mole with a 60-90min
>>>>dev/run-tests
>>>> > iteration time, which is no fun! So I am wondering/thinking:
>>>> >
>>>> > - Do other people experience this level of flakiness from spark
>>>>tests?
>>>> > - Do other people bother running dev/run-tests locally, or just let
>>>> Jenkins
>>>> > do it during the CR process?
>>>> > - Needing to run a full assembly post-clean just to continue running
>>>>one
>>>> > specific test case feels especially wasteful, and the failure output
>>>>when
>>>> > naively attempting to run a specific test without having built an
>>>> assembly
>>>> > jar is not always clear about what the issue is or how to fix it;
>>>>even
>>>> the
>>>> > fact that certain tests require "building the world" is not
>>>>something I
>>>> > would have expected, and has cost me hours of confusion.
>>>> >    - Should a person running spark tests assume that they must build
>>>>an
>>>> > assembly JAR before running anything?
>>>> >    - Are there some proper "unit" tests that are actually
>>>>self-contained
>>>> /
>>>> > able to be run without building an assembly jar?
>>>> >    - Can we better document/demarcate which tests have which
>>>> dependencies?
>>>> >    - Is there something finer-grained than building an assembly JAR
>>>>that
>>>> > is sufficient in some cases?
>>>> >        - If so, can we document that?
>>>> >        - If not, can we move to a world of finer-grained
>>>>dependencies for
>>>> > some of these?
>>>> > - Leaving all of these spurious failures aside, the process of
>>>>assembling
>>>> > and testing a new JAR is not a quick one (40 and 60 mins for me
>>>> typically,
>>>> > respectively). I would guess that there are dozens (hundreds?) of
>>>>people
>>>> > who build a Spark assembly from various ToTs on any given day, and
>>>>who
>>>> all
>>>> > wait on the exact same compilation / assembly steps to occur.
>>>>Expanding
>>>> on
>>>> > the recent work to publish nightly snapshots [20], can we do a
>>>>better job
>>>> > caching/sharing compilation artifacts at a more granular level
>>>>(pre-built
>>>> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
>>>>more
>>>> > granular maven modules, plus the previous two?), or otherwise save
>>>>some
>>>> of
>>>> > the considerable amount of redundant compilation work that I had to
>>>>do
>>>> over
>>>> > the course of my odyssey this weekend?
>>>> >
>>>> > Ramping up on most projects involves some amount of supplementing th=
e
>>>> > documentation with trial and error to figure out what to run, which
>>>> > "errors" are real errors and which can be ignored, etc., but
>>>>navigating
>>>> > that minefield on Spark has proved especially challenging and
>>>> > time-consuming for me. Some of that comes directly from scala's
>>>> relatively
>>>> > slow compilation times and immature build-tooling ecosystem, but
>>>>that is
>>>> > the world we live in and it would be nice if Spark took the
>>>>alleviation
>>>> of
>>>> > the resulting pain more seriously, as one of the more interesting an=
d
>>>> > well-known large scala projects around right now. The official
>>>> > documentation around how to build different subsets of the codebase
>>>>is
>>>> > somewhat sparse [21], and there have been many mixed [22] accounts
>>>>[23]
>>>> on
>>>> > this mailing list about preferred ways to build on mvn vs. sbt (none
>>>>of
>>>> > which has made it into official documentation, as far as I've seen).
>>>> > Expecting new contributors to piece together all of this received
>>>> > folk-wisdom about how to build/test in a sane way by trawling mailin=
g
>>>> list
>>>> > archives seems suboptimal.
>>>> >
>>>> > Thanks for reading, looking forward to hearing your ideas!
>>>> >
>>>> > -Ryan
>>>> >
>>>> > P.S. Is "best practice" for emailing this list to not incorporate an=
y
>>>> HTML
>>>> > in the body? It seems like all of the archives I've seen strip it
>>>>out,
>>>> but
>>>> > other people have used it and gmail displays it.
>>>> >
>>>> >
>>>> > [1]
>>>> >
>>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>>> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>>>> > (57 mins)
>>>> > [2]
>>>> >
>>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>>> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>>>> > (6 mins)
>>>> > [3]
>>>> >
>>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>>> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>>>> 20pass%20test,%20fail%20subsequent%20compile
>>>> > (4 mins)
>>>> > [4]
>>>> > https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dweb=
&
>>>> cd=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-
>>>> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>>>> DataTypeConversions-scala-td17083.html&ei=3DaRF6VJrpNKr-
>>>> iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=3D
>>>> zDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
>>>> > [5]
>>>> >
>>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>>> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>>>> 20clean,%20need%20dependencies%20built
>>>> > [6]
>>>> >
>>>>https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>>>> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>>>> 20post%20clean
>>>> > (50 mins)
>>>> > [7]
>>>> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>>>> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>>>> > (1hr)
>>>> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
>>>> > [9] https://issues.apache.org/jira/browse/SPARK-3867
>>>> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>>>> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>>>> > [12]
>>>> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>>>> file-gistfile1-txt-L853
>>>> > (~90 mins)
>>>> > [13]
>>>> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>>>> file-gistfile1-txt-L852
>>>> > (91 mins)
>>>> > [14]
>>>> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>>>> file-gistfile1-txt-L854
>>>> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>>>> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>>>> > [17]
>>>> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>>>> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>>>> > [18]
>>>> > http://stackoverflow.com/questions/25707629/why-does-
>>>> spark-job-fail-with-too-many-open-files
>>>> > [19] https://issues.apache.org/jira/browse/SPARK-4002
>>>> > [20] https://issues.apache.org/jira/browse/SPARK-4542
>>>> > [21]
>>>> > https://spark.apache.org/docs/latest/building-with-maven.
>>>> html#spark-tests-in-maven
>>>> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
>>>> > [23]
>>>> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>>>> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3=
E
>>>>
>>>>
>>
>>---------------------------------------------------------------------
>>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>For additional commands, e-mail: dev-help@spark.apache.org
>>
>
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or proprieta=
ry to Capital One and/or its affiliates. The information transmitted herewi=
th is intended only for use by the individual or entity to which it is addr=
essed.  If the reader of this message is not the intended recipient, you ar=
e hereby notified that any review, retransmission, dissemination, distribut=
ion, copying or other use of, or taking of any action in reliance upon this=
 information is strictly prohibited. If you have received this communicatio=
n in error, please contact the sender and delete the material from your com=
puter.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 05:33:02 2014
Return-Path: <dev-return-10592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3781A10D61
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 05:33:02 +0000 (UTC)
Received: (qmail 22618 invoked by uid 500); 1 Dec 2014 05:33:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22565 invoked by uid 500); 1 Dec 2014 05:33:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22542 invoked by uid 99); 1 Dec 2014 05:33:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 05:33:00 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 05:32:55 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 97375BC069A
	for <dev@spark.incubator.apache.org>; Sun, 30 Nov 2014 21:32:36 -0800 (PST)
Date: Sun, 30 Nov 2014 22:32:35 -0700 (MST)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417411955227-9575.post@n3.nabble.com>
In-Reply-To: <CAF7ADNpECZoQ-u-rE881j4G083ajjYMb4KYnAAxDXxVetP_azw@mail.gmail.com>
References: <1417099318158-9540.post@n3.nabble.com> <CAF7ADNpECZoQ-u-rE881j4G083ajjYMb4KYnAAxDXxVetP_azw@mail.gmail.com>
Subject: Re: [mllib] Which is the correct package to add a new algorithm?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Joseph, 

Thank you for your nice work and telling us the draft!

> During the next development cycle, new algorithms should be contributed to 
> spark.mllib.  Optionally, wrappers for new (and old) algorithms can be 
> contributed to spark.ml. 

I understand that we should contribute new algorithms to spark.mllib.
thanks, 
Yu



-----
-- Yu Ishikawa
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Which-is-the-correct-package-to-add-a-new-algorithm-tp9540p9575.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 08:17:33 2014
Return-Path: <dev-return-10593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B17910154
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 08:17:33 +0000 (UTC)
Received: (qmail 22474 invoked by uid 500); 1 Dec 2014 08:17:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22396 invoked by uid 500); 1 Dec 2014 08:17:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22385 invoked by uid 99); 1 Dec 2014 08:17:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 08:17:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 08:17:27 +0000
Received: by mail-qa0-f54.google.com with SMTP id i13so6756999qae.41
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 00:17:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=F/RmB03NAxB94gE8IxMrKuPYZNINYT3A5C2J2imEE28=;
        b=a2iXXghrZyMUG/U/qQPLi8nbbLcEBh7ovhZrUOjFHQdd+2DuP3BSPq/HHYGtnFH1d7
         vFHzVNWtFWMsff9SQJdDjF2YU7kWtR6v3ZpSLjoYHuAonLotj9QHe1dxX9/hnV5Xa/So
         kQfIkpVY3bV0TObIYLGcad9lB67WmtkeRwEmaHXoFoVYgy4o864yRvoSiUAhB89jMGXC
         li+AVcJXguCS86rBcZ0l48p096R7c2zubTpK401z2UEv1FKqCE/ra2b7/vrBW/cMQ5p0
         of3NxR5xr+3oQPva9rAU2yjyfO9+gLWXL1a/jQni6+FBcCg8Ka9AbHK+HZ4NnUpHeXCq
         rO+A==
X-Gm-Message-State: ALoCoQn3cAsdvywJQbjtv0qqtDurW3FUbgQFtZ1c8KntmVmdHHYw5VolFNVPCw6oIF9H6OOd0/Hz
MIME-Version: 1.0
X-Received: by 10.140.92.215 with SMTP id b81mr84573382qge.5.1417421827225;
 Mon, 01 Dec 2014 00:17:07 -0800 (PST)
Received: by 10.140.102.113 with HTTP; Mon, 1 Dec 2014 00:17:07 -0800 (PST)
In-Reply-To: <tencent_316F9A915869AD127D13FA6A@qq.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<tencent_316F9A915869AD127D13FA6A@qq.com>
Date: Mon, 1 Dec 2014 00:17:07 -0800
Message-ID: <CACBYxKK8FUHm8LJ=Skd=nFLfFPmZQSZ0KpOcnuVSOoxry-8dOw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: GuoQiang Li <witgo@qq.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139126c8263480509233c5d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139126c8263480509233c5d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1 (non-binding)

built from source
fired up a spark-shell against YARN cluster
ran some jobs using parallelize
ran some jobs that read files
clicked around the web UI


On Sun, Nov 30, 2014 at 1:10 AM, GuoQiang Li <witgo@qq.com> wrote:

> +1 (non-binding=E2=80=8D)
>
>
>
>
> ------------------ Original ------------------
> From:  "Patrick Wendell";<pwendell@gmail.com>;
> Date:  Sat, Nov 29, 2014 01:16 PM
> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>
> Subject:  [VOTE] Release Apache Spark 1.2.0 (RC1)
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.2.0!
>
> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D1056=
e9ec13203d0c51564265e94d77a054498fdb
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1048/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.0!
>
> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> =3D=3D What justifies a -1 vote for this release? =3D=3D
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
>
> =3D=3D What default changes should I be aware of? =3D=3D
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
>
> 2. The default value of "spark.shuffle.manager" has been changed to "sort=
".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> "hash".
>
> =3D=3D Other notes =3D=3D
> Because this vote is occurring over a weekend, I will likely extend
> the vote if this RC survives until the end of the vote period.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

--001a1139126c8263480509233c5d--

From dev-return-10594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 09:35:44 2014
Return-Path: <dev-return-10594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A41910400
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 09:35:44 +0000 (UTC)
Received: (qmail 32740 invoked by uid 500); 1 Dec 2014 09:35:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32658 invoked by uid 500); 1 Dec 2014 09:35:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32645 invoked by uid 99); 1 Dec 2014 09:35:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 09:35:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of niranda@wso2.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 09:35:16 +0000
Received: by mail-wi0-f169.google.com with SMTP id r20so26135267wiv.0
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 01:35:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=wso2.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=HW6EjJazXY7NaZwrISdRTklpDG1v7Sh9UD8CNJEKF34=;
        b=Z/J1Fyk5LaV9HQtjDTCpyRuLaB6J3YV+IvqLThMPJ++9yN2w/feton1Gjsbm1eL7Ic
         yVgO06hj20czjY4HPQJrQjEU3yf1DTrnjDNzeA/ASrnKi05iapF/iWlQs2IsrL2pLrZz
         vf8Ed0O23Y9ymnrmot9U1M6I5MldxELB1TJWA=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HW6EjJazXY7NaZwrISdRTklpDG1v7Sh9UD8CNJEKF34=;
        b=lhYQ1GruMnl996kPfNMjsfbVf8k9sjSp/Lzr2f2rsL3IteEVGvoPPu9YA+546VllUc
         lANvqdTQjRv+4SgS9cpaxH1lpuoz1a1CSNf6N7GL6INODbGUqWbjlAULLeRAXp+q5ITa
         FV2429BO7fzzX/RJdbkAHgkk+xybjj7lAD1SxeJkv3Ug6Ez1vJrrvSzJ2rB/bE7hP/H2
         8mmt6uP09KLi9LueJBj/QpsPXZWl8+bwIQYtwrm7s8mdQ7bUDoXiiZJLN5kr3HjyLV8F
         CDLIzQodG5yS4uHVFHRs5qjOs1aN1kq6GUUz5nb/yNtCDv8mMmMKg0C/liAlSmSdLzLB
         aDyw==
X-Gm-Message-State: ALoCoQmLQE9pEDdYTEQZqKl9jJZQQeEJgzDAD+C6EZEk6QPepWmIIdfIHm0JpaFbrlr3yBpFFnv7
X-Received: by 10.180.11.140 with SMTP id q12mr83120126wib.45.1417426515959;
 Mon, 01 Dec 2014 01:35:15 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.174.66 with HTTP; Mon, 1 Dec 2014 01:34:55 -0800 (PST)
In-Reply-To: <CAAswR-5YLQo_Kvm3KLyo+d3dczhALhsW51ovEMJhWQsX+G1MWA@mail.gmail.com>
References: <CADz3zK3eu-uU8Po529DGw0FSSCjLbEbpc9+HnPj8xoDijhiKVA@mail.gmail.com>
 <CAAswR-5YLQo_Kvm3KLyo+d3dczhALhsW51ovEMJhWQsX+G1MWA@mail.gmail.com>
From: Niranda Perera <niranda@wso2.com>
Date: Mon, 1 Dec 2014 15:04:55 +0530
Message-ID: <CADz3zK0xjngrBpLGWraDEMgdPOTCrgqHinWeSPbW3+DS0hpAcQ@mail.gmail.com>
Subject: Re: Creating a SchemaRDD from an existing API
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2aeeafaba2105092453f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2aeeafaba2105092453f8
Content-Type: text/plain; charset=UTF-8

Hi Michael,

About this new data source API, what type of data sources would it support?
Does it have to be RDBMS necessarily?

Cheers

On Sat, Nov 29, 2014 at 12:57 AM, Michael Armbrust <michael@databricks.com>
wrote:

> You probably don't need to create a new kind of SchemaRDD.  Instead I'd
> suggest taking a look at the data sources API that we are adding in Spark
> 1.2.  There is not a ton of documentation, but the test cases show how to
> implement the various interfaces
> <https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/sources>,
> and there is an example library for reading Avro data
> <https://github.com/databricks/spark-avro>.
>
> On Thu, Nov 27, 2014 at 10:31 PM, Niranda Perera <niranda@wso2.com> wrote:
>
>> Hi,
>>
>> I am evaluating Spark for an analytic component where we do batch
>> processing of data using SQL.
>>
>> So, I am particularly interested in Spark SQL and in creating a SchemaRDD
>> from an existing API [1].
>>
>> This API exposes elements in a database as datasources. Using the methods
>> allowed by this data source, we can access and edit data.
>>
>> So, I want to create a custom SchemaRDD using the methods and provisions
>> of
>> this API. I tried going through Spark documentation and the Java Docs, but
>> unfortunately, I was unable to come to a final conclusion if this was
>> actually possible.
>>
>> I would like to ask the Spark Devs,
>> 1. As of the current Spark release, can we make a custom SchemaRDD?
>> 2. What is the extension point to a custom SchemaRDD? or are there
>> particular interfaces?
>> 3. Could you please point me the specific docs regarding this matter?
>>
>> Your help in this regard is highly appreciated.
>>
>> Cheers
>>
>> [1]
>>
>> https://github.com/wso2-dev/carbon-analytics/tree/master/components/xanalytics
>>
>> --
>> *Niranda Perera*
>> Software Engineer, WSO2 Inc.
>> Mobile: +94-71-554-8430
>> Twitter: @n1r44 <https://twitter.com/N1R44>
>>
>
>


-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>

--001a11c2aeeafaba2105092453f8--

From dev-return-10595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 11:00:05 2014
Return-Path: <dev-return-10595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 040B910774
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 11:00:05 +0000 (UTC)
Received: (qmail 2357 invoked by uid 500); 1 Dec 2014 11:00:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2276 invoked by uid 500); 1 Dec 2014 11:00:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2261 invoked by uid 99); 1 Dec 2014 11:00:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 11:00:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lochanac@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 10:59:36 +0000
Received: by mail-pa0-f47.google.com with SMTP id kq14so10863558pab.34
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 02:58:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject
         :content-type:content-transfer-encoding;
        bh=LvUISGEmFLjb/CVncNXrFfg5Cyd/vtO0YbUSQ/KEByo=;
        b=tUPxZ4l4ycRacq2vA9jqaXy6p+1/uN2/b5B0eJC5OPMoH5v54LEKPzwO1mPiWIDKZh
         dPStGuje7ckRLsB/gnF+fgdGB75BW55WLg1GPb8CqHL2hD9d90BfXj3uWWm2CoP8zhIN
         rNy9ifB/BG5M/MUh9S8PggxDJldfipqQAdDXHNUioAA8S7lggZBktIJ5G1V7eWXl03QL
         sS0NWIPqQ2KSfbick2jvBZbp6pmidEUiD/th8b3TK1GGZTS/JRknRxhguSSECloDLaGa
         oG3pT8AjGfEi0rxBqz89PTrdiYpVB0NwOuJI26b8JPnwXYb9rvMuzmApAvFW8M+XTKo3
         trsg==
X-Received: by 10.68.69.48 with SMTP id b16mr97264132pbu.59.1417431528301;
        Mon, 01 Dec 2014 02:58:48 -0800 (PST)
Received: from Lochanas-MacBook-Pro.local ([203.94.95.4])
        by mx.google.com with ESMTPSA id th7sm17357601pac.47.2014.12.01.02.58.46
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 01 Dec 2014 02:58:47 -0800 (PST)
Message-ID: <547C49EA.9040804@gmail.com>
Date: Mon, 01 Dec 2014 16:28:50 +0530
From: Lochana Menikarachchi <lochanac@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.2.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: packaging spark run time with osgi service
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have spark core and mllib as dependencies for a spark based osgi 
service. When I call the model building method through a unit test 
(without osgi) it works OK. When I call it through the osgi service, 
nothing happens. I tried adding spark assembly jar. Now it throws 
following error..

An error occurred while building supervised machine learning model: No 
configuration setting found for key 'akka.version'
com.typesafe.config.ConfigException$Missing: No configuration setting 
found for key 'akka.version'
     at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:136)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:142)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:150)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:155)
     at 
com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:197)

What is the correct way to include spark runtime dependencies to osgi 
service.. Thanks.

Lochana

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 17:53:19 2014
Return-Path: <dev-return-10596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3ED45107F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 17:53:19 +0000 (UTC)
Received: (qmail 84272 invoked by uid 500); 1 Dec 2014 17:53:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83975 invoked by uid 500); 1 Dec 2014 17:53:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83318 invoked by uid 99); 1 Dec 2014 17:53:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 17:53:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of scottwalent@gmail.com designates 209.85.215.42 as permitted sender)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 17:52:49 +0000
Received: by mail-la0-f42.google.com with SMTP id s18so9367253lam.1
        for <multiple recipients>; Mon, 01 Dec 2014 09:52:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=dEuBGh2ojX7EgExE32teO5kXLCQhlBXqx0Q+EjAJGVw=;
        b=cBgtDGq6qn1GVR1Qsz1/ztcR/l6LnoJRYBhZpF6WUZGys5BIU/1pmhmcrXlCtWAUDy
         5r9FmNEmDB1bvTrrmOcKEPJTG/wbAlAhjX+bBUZCbEu6iDAQK0L0giAMgBckEn3DE5tS
         2+A3DOy0R6XNzfIaQ9WZoLqT4QGzPfLnlU9bXIHrgvRn9yVzAIeILoIuJqjOSmphyhSX
         sxDNQMIc8rVOPk0BgWbvE9Ojle0FzSvB5qFDnNl/i/OgIfcz0LnKLjM3Em9PrIZJq5hf
         5EftoGfj8pHZhne9nHydgPZnLd/++asefXzhZFvE28l5pzC9+ZDojt8/z4Cw+jG1QOj6
         JLAA==
MIME-Version: 1.0
X-Received: by 10.152.120.167 with SMTP id ld7mr57247131lab.77.1417456368680;
 Mon, 01 Dec 2014 09:52:48 -0800 (PST)
Received: by 10.114.199.196 with HTTP; Mon, 1 Dec 2014 09:52:48 -0800 (PST)
Date: Mon, 1 Dec 2014 09:52:48 -0800
Message-ID: <CAP7HBy2XQ5pyKj_D3DULKmayq5jDANULcjZERu7jTw7A2Yj31g@mail.gmail.com>
Subject: Spark Summit East CFP - 5 days until deadline
From: Scott walent <scottwalent@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=089e012290f257050005092b477a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e012290f257050005092b477a
Content-Type: text/plain; charset=UTF-8

The inaugural Spark Summit East (spark-summit.org/east), an event to bring
the Apache Spark community together, will be in New York City on March 18,
2015. The call for submissions is currently open, but will close this
Friday December 5, at 11:59pm PST.   The summit is looking for talks that
will cover topics including applications, development, research, and data
science.

At the Summit you can look forward to hearing from committers, developers,
CEOs, and companies who are solving real-world big data challenges with
Spark.

All submissions will be reviewed by a Program Committee that is made up of
the creators, top committers and individuals who have heavily contributed
to the Spark project. No speaker slots are being sold to sponsors in an
effort to to keep the Summit a community driven event.

To submit your abstracts please visit: spark-summit.org/east/2015/cfp

Looking forward to seeing you there!

Best,
Scott & The Spark Summit Organizers

--089e012290f257050005092b477a--

From dev-return-10597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 19:19:34 2014
Return-Path: <dev-return-10597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5792910D32
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 19:19:34 +0000 (UTC)
Received: (qmail 65618 invoked by uid 500); 1 Dec 2014 19:19:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65549 invoked by uid 500); 1 Dec 2014 19:19:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65537 invoked by uid 99); 1 Dec 2014 19:19:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 19:19:32 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 19:19:04 +0000
Received: by mail-pa0-f44.google.com with SMTP id et14so11673007pad.31
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 11:18:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=hvqt33atsz/EuDZDlE0sDVEf3N49Sp+TOqXY3Ezc6eA=;
        b=KmKfzn7JesdAFOa3kJoacqfFChJH1CYUbegEivHCHY5QU/9B/0Y1X78AZzkhi2LC/s
         hno6ml1iSoF7d/mScbmx17emRx+PtCIjJzrPuaZL3b7E/rJ2ajo/kGwg3VH43MHZRPCV
         8SsaEMYVHrgxc50D+CpHOnqlmBfWYvUtSLn8zTVonLvAB21eotRksnFJHNa5sQidsFTC
         PkGXDdukIR1y5NRYKmsW4n4lNKHPEmvwwVbyx7umHBTqh1h+KQKoVHzC8srdw55CUC1j
         1K5lA8/Ko3aB8c4uvw6fFyc5o0QuAfcaS2BkxHWri7uvmeB0t7TXUl3dfRw9gtUn/M6e
         4HFA==
X-Received: by 10.70.89.105 with SMTP id bn9mr20169964pdb.155.1417461498338;
        Mon, 01 Dec 2014 11:18:18 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id op10sm10777602pbc.90.2014.12.01.11.18.17
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Mon, 01 Dec 2014 11:18:17 -0800 (PST)
Date: Mon, 1 Dec 2014 11:18:16 -0800
From: Josh Rosen <rosenville@gmail.com>
To: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Cc: harry.brundage@gmail.com
Message-ID: <etPan.547cbef8.3352255a.1313e@joshs-mbp>
In-Reply-To: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
X-Mailer: Airmail Beta (269)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="547cbef8_109cf92e_1313e"
X-Virus-Checked: Checked by ClamAV on apache.org

--547cbef8_109cf92e_1313e
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi everyone,

There=E2=80=99s an open bug report related to Spark standalone which coul=
d be a potential release-blocker (pending investigation / a bug fix):=C2=A0=
https://issues.apache.org/jira/browse/SPARK-4498. =C2=A0This issue seems =
non-deterministc and only affects long-running Spark standalone deploymen=
ts, so it may be hard to reproduce. =C2=A0I=E2=80=99m going to work on a =
patch to add additional logging in order to help with debugging.

I just wanted to give an early head=E2=80=99s up about this issue and to =
get more eyes on it in case anyone else has run into it or wants to help =
with debugging.

- Josh

On November 28, 2014 at 9:18:09 PM, Patrick Wendell (pwendell=40gmail.com=
) wrote:

Please vote on releasing the following candidate as Apache Spark version =
1.2.0=21 =20

The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1): =20
https://git-wip-us.apache.org/repos/asf=3Fp=3Dspark.git;a=3Dcommit;h=3D10=
56e9ec13203d0c51564265e94d77a054498fdb =20

The release files, including signatures, digests, etc. can be found at: =20
http://people.apache.org/=7Epwendell/spark-1.2.0-rc1/ =20

Release artifacts are signed with the following key: =20
https://people.apache.org/keys/committer/pwendell.asc =20

The staging repository for this release can be found at: =20
https://repository.apache.org/content/repositories/orgapachespark-1048/ =20

The documentation corresponding to this release can be found at: =20
http://people.apache.org/=7Epwendell/spark-1.2.0-rc1-docs/ =20

Please vote on releasing this package as Apache Spark 1.2.0=21 =20

The vote is open until Tuesday, December 02, at 05:15 UTC and passes =20
if a majority of at least 3 +1 PMC votes are cast. =20

=5B =5D +1 Release this package as Apache Spark 1.1.0 =20
=5B =5D -1 Do not release this package because ... =20

To learn more about Apache Spark, please see =20
http://spark.apache.org/ =20

=3D=3D What justifies a -1 vote for this release=3F =3D=3D =20
This vote is happening very late into the QA period compared with =20
previous votes, so -1 votes should only occur for significant =20
regressions from 1.0.2. Bugs already present in 1.1.X, minor =20
regressions, or bugs related to new features will not block this =20
release. =20

=3D=3D What default changes should I be aware of=3F =3D=3D =20
1. The default value of =22spark.shuffle.blockTransferService=22 has been=
 =20
changed to =22netty=22 =20
--> Old behavior can be restored by switching to =22nio=22 =20

2. The default value of =22spark.shuffle.manager=22 has been changed to =22=
sort=22. =20
--> Old behavior can be restored by setting =22spark.shuffle.manager=22 t=
o =22hash=22. =20

=3D=3D Other notes =3D=3D =20
Because this vote is occurring over a weekend, I will likely extend =20
the vote if this RC survives until the end of the vote period. =20

- Patrick =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--547cbef8_109cf92e_1313e--


From dev-return-10598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 19:21:24 2014
Return-Path: <dev-return-10598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9555D10D36
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 19:21:24 +0000 (UTC)
Received: (qmail 67859 invoked by uid 500); 1 Dec 2014 19:21:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67791 invoked by uid 500); 1 Dec 2014 19:21:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67777 invoked by uid 99); 1 Dec 2014 19:21:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 19:21:23 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 19:20:57 +0000
Received: by mail-la0-f45.google.com with SMTP id gq15so9401130lab.32
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 11:20:56 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=+ywYiyf964L6MCETnDhcJZzcJdnQ/vBDKpxQofwxwqY=;
        b=KQFKXJJhLs+CMBUQx4tPx94yg7TD6x576SD5V25jfJTocuxlNc58XL0SKpCXX9tel7
         CWfBbu4/iVOm+klcaeRhNU4pBMmGXycZ3efb44CwCdiAx3lNa+l1GLhsTTQW4PUWUSMJ
         5c8kS0JhEVSC9l+gkrQeHWlzRiLss5IOifKtchsBuLFTa2o1cUvIYFbB/Vc7D+0G9RbB
         qt8oPRrqcl+CWRLuT25daULPgwfdLdNldPn2zLbcRT2mtprbwdgbQyrok36twm0jcm0P
         +HJd3l1SIPoQ1Gjhv6cB779wVHE7YaR+EUcM5us1GyvpIX3It2HA+2MUphthAnS+cGf7
         0dHw==
X-Gm-Message-State: ALoCoQl5QH/YGLl0Lzd67AyByFKNSRmfdZDq9N+gvJqiR8KJ7+D4cvB2iwde64tRhoRuYLI1gM5Z
X-Received: by 10.152.29.97 with SMTP id j1mr51580327lah.3.1417461655975; Mon,
 01 Dec 2014 11:20:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 1 Dec 2014 11:20:35 -0800 (PST)
In-Reply-To: <CADz3zK0xjngrBpLGWraDEMgdPOTCrgqHinWeSPbW3+DS0hpAcQ@mail.gmail.com>
References: <CADz3zK3eu-uU8Po529DGw0FSSCjLbEbpc9+HnPj8xoDijhiKVA@mail.gmail.com>
 <CAAswR-5YLQo_Kvm3KLyo+d3dczhALhsW51ovEMJhWQsX+G1MWA@mail.gmail.com> <CADz3zK0xjngrBpLGWraDEMgdPOTCrgqHinWeSPbW3+DS0hpAcQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 1 Dec 2014 11:20:35 -0800
Message-ID: <CAAswR-7usoSRnZxcfqHmu8MhQH-fjYx1C6L_14sO8WFYUdxHOQ@mail.gmail.com>
Subject: Re: Creating a SchemaRDD from an existing API
To: Niranda Perera <niranda@wso2.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160bdaa7cce1a05092c82c9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bdaa7cce1a05092c82c9
Content-Type: text/plain; charset=UTF-8

No, it should support any data source that has a schema and can produce
rows.

On Mon, Dec 1, 2014 at 1:34 AM, Niranda Perera <niranda@wso2.com> wrote:

> Hi Michael,
>
> About this new data source API, what type of data sources would it
> support? Does it have to be RDBMS necessarily?
>
> Cheers
>
> On Sat, Nov 29, 2014 at 12:57 AM, Michael Armbrust <michael@databricks.com
> > wrote:
>
>> You probably don't need to create a new kind of SchemaRDD.  Instead I'd
>> suggest taking a look at the data sources API that we are adding in Spark
>> 1.2.  There is not a ton of documentation, but the test cases show how
>> to implement the various interfaces
>> <https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/sources>,
>> and there is an example library for reading Avro data
>> <https://github.com/databricks/spark-avro>.
>>
>> On Thu, Nov 27, 2014 at 10:31 PM, Niranda Perera <niranda@wso2.com>
>> wrote:
>>
>>> Hi,
>>>
>>> I am evaluating Spark for an analytic component where we do batch
>>> processing of data using SQL.
>>>
>>> So, I am particularly interested in Spark SQL and in creating a SchemaRDD
>>> from an existing API [1].
>>>
>>> This API exposes elements in a database as datasources. Using the methods
>>> allowed by this data source, we can access and edit data.
>>>
>>> So, I want to create a custom SchemaRDD using the methods and provisions
>>> of
>>> this API. I tried going through Spark documentation and the Java Docs,
>>> but
>>> unfortunately, I was unable to come to a final conclusion if this was
>>> actually possible.
>>>
>>> I would like to ask the Spark Devs,
>>> 1. As of the current Spark release, can we make a custom SchemaRDD?
>>> 2. What is the extension point to a custom SchemaRDD? or are there
>>> particular interfaces?
>>> 3. Could you please point me the specific docs regarding this matter?
>>>
>>> Your help in this regard is highly appreciated.
>>>
>>> Cheers
>>>
>>> [1]
>>>
>>> https://github.com/wso2-dev/carbon-analytics/tree/master/components/xanalytics
>>>
>>> --
>>> *Niranda Perera*
>>> Software Engineer, WSO2 Inc.
>>> Mobile: +94-71-554-8430
>>> Twitter: @n1r44 <https://twitter.com/N1R44>
>>>
>>
>>
>
>
> --
> *Niranda Perera*
> Software Engineer, WSO2 Inc.
> Mobile: +94-71-554-8430
> Twitter: @n1r44 <https://twitter.com/N1R44>
>

--089e0160bdaa7cce1a05092c82c9--

From dev-return-10599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  1 23:31:29 2014
Return-Path: <dev-return-10599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2FD3410B52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  1 Dec 2014 23:31:29 +0000 (UTC)
Received: (qmail 90432 invoked by uid 500); 1 Dec 2014 23:31:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90361 invoked by uid 500); 1 Dec 2014 23:31:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90348 invoked by uid 99); 1 Dec 2014 23:31:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 23:31:27 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 01 Dec 2014 23:31:21 +0000
Received: by mail-pd0-f172.google.com with SMTP id y13so11843261pdi.31
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 15:30:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=tjfwyXxO5mknO4cxxsjTw9hqD9+VBg08wEsnEmYuy98=;
        b=nle0NWij4hsvndWKucYwDHvfspvvcU4pKhVZbfFi5pP23ZbF1QeupPt+FRenckN+Rr
         WqNbfboNX42bjVBt/ilR9gXinqXkyUFP98SCkZaLQx71TfF5pfCSeCXaYlA2pCjW092I
         JhX0twaIssswcUdyIxD8EhN1JNaPaNKnhrVGueyyAfKYeopv5tyMvtFuB30q53HrO1+T
         KxAz47OJXxNKxc84PFnUWq2v1Wnq8KGF2XSetyOMTmISBinnMHQIUG+GEMYYMAPWes17
         VTfGp9XxOD8n1iFikS/9P2FFhPQ71daIsAoYZuU+DQLVAlZBhSDNrjJdltz5q2E25u3b
         kaqA==
X-Received: by 10.66.145.234 with SMTP id sx10mr105157666pab.130.1417476615870;
        Mon, 01 Dec 2014 15:30:15 -0800 (PST)
Received: from [192.168.1.100] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ye3sm18403933pbb.93.2014.12.01.15.30.14
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 01 Dec 2014 15:30:15 -0800 (PST)
Content-Type: text/plain; charset=utf-8
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <etPan.547cbef8.3352255a.1313e@joshs-mbp>
Date: Mon, 1 Dec 2014 15:30:13 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
 harry.brundage@gmail.com
Content-Transfer-Encoding: quoted-printable
Message-Id: <5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com> <etPan.547cbef8.3352255a.1313e@joshs-mbp>
To: Josh Rosen <rosenville@gmail.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

+0.9 from me. Tested it on Mac and Windows (someone has to do it) and =
while things work, I noticed a few recent scripts don't have Windows =
equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683 and =
https://issues.apache.org/jira/browse/SPARK-4684. The first one at least =
would be good to fix if we do another RC. Not blocking the release but =
useful to fix in docs is =
https://issues.apache.org/jira/browse/SPARK-4685.

Matei


> On Dec 1, 2014, at 11:18 AM, Josh Rosen <rosenville@gmail.com> wrote:
>=20
> Hi everyone,
>=20
> There=E2=80=99s an open bug report related to Spark standalone which =
could be a potential release-blocker (pending investigation / a bug =
fix): https://issues.apache.org/jira/browse/SPARK-4498.  This issue =
seems non-deterministc and only affects long-running Spark standalone =
deployments, so it may be hard to reproduce.  I=E2=80=99m going to work =
on a patch to add additional logging in order to help with debugging.
>=20
> I just wanted to give an early head=E2=80=99s up about this issue and =
to get more eyes on it in case anyone else has run into it or wants to =
help with debugging.
>=20
> - Josh
>=20
> On November 28, 2014 at 9:18:09 PM, Patrick Wendell =
(pwendell@gmail.com) wrote:
>=20
> Please vote on releasing the following candidate as Apache Spark =
version 1.2.0! =20
>=20
> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1): =20
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D1056e=
9ec13203d0c51564265e94d77a054498fdb =20
>=20
> The release files, including signatures, digests, etc. can be found =
at: =20
> http://people.apache.org/~pwendell/spark-1.2.0-rc1/ =20
>=20
> Release artifacts are signed with the following key: =20
> https://people.apache.org/keys/committer/pwendell.asc =20
>=20
> The staging repository for this release can be found at: =20
> =
https://repository.apache.org/content/repositories/orgapachespark-1048/ =20=

>=20
> The documentation corresponding to this release can be found at: =20
> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/ =20
>=20
> Please vote on releasing this package as Apache Spark 1.2.0! =20
>=20
> The vote is open until Tuesday, December 02, at 05:15 UTC and passes =20=

> if a majority of at least 3 +1 PMC votes are cast. =20
>=20
> [ ] +1 Release this package as Apache Spark 1.1.0 =20
> [ ] -1 Do not release this package because ... =20
>=20
> To learn more about Apache Spark, please see =20
> http://spark.apache.org/ =20
>=20
> =3D=3D What justifies a -1 vote for this release? =3D=3D =20
> This vote is happening very late into the QA period compared with =20
> previous votes, so -1 votes should only occur for significant =20
> regressions from 1.0.2. Bugs already present in 1.1.X, minor =20
> regressions, or bugs related to new features will not block this =20
> release. =20
>=20
> =3D=3D What default changes should I be aware of? =3D=3D =20
> 1. The default value of "spark.shuffle.blockTransferService" has been =20=

> changed to "netty" =20
> --> Old behavior can be restored by switching to "nio" =20
>=20
> 2. The default value of "spark.shuffle.manager" has been changed to =
"sort". =20
> --> Old behavior can be restored by setting "spark.shuffle.manager" to =
"hash". =20
>=20
> =3D=3D Other notes =3D=3D =20
> Because this vote is occurring over a weekend, I will likely extend =20=

> the vote if this RC survives until the end of the vote period. =20
>=20
> - Patrick =20
>=20
> --------------------------------------------------------------------- =20=

> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org =20
> For additional commands, e-mail: dev-help@spark.apache.org =20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 01:11:48 2014
Return-Path: <dev-return-10600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DFD6610F63
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 01:11:48 +0000 (UTC)
Received: (qmail 89558 invoked by uid 500); 2 Dec 2014 01:11:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89483 invoked by uid 500); 2 Dec 2014 01:11:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89471 invoked by uid 99); 2 Dec 2014 01:11:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 01:11:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 01:11:42 +0000
Received: by mail-la0-f53.google.com with SMTP id gm9so9744655lab.26
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 17:10:35 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=DOmUInbRODkxz3DlgLri5i7bfKC1nt9w9M3xVQHTUIo=;
        b=SgwzX7pphIyCT26hqcgUGF10FP8mDeTGvuEJ5XcYuGK0xqV3CD7mjsimrr+Ek8w0E/
         3fdSmaaD6oERxJ9Ai9i+tYzyHMLT6e6SqLETxajjvbS6s4wN9HmGb9O7NxYK7P2jdGGE
         U4PZTHwXyf3k++YqXktYYPYJ+WSPIHHQkvCwOB0k/McegrJGfl3Od0V2BugmfYlOhRi4
         We7uNJPG9un9xgjCljlyJXeUwFNirz7lDP0wXFheZGIJZFClmb5pcMqEvsoxXd/897BI
         Qt1oGGTVL4pEUFvhEMDUEflYImY+Bh+MEa4kwL9G/0bsu0i8HZ40RafSq1WPYse1o4P8
         /s2A==
X-Gm-Message-State: ALoCoQktoK+KKQYgbt5lZ86HgPot5qFnh0mn/4Sk4brp6d6yk66K2vlh7Mjxoe6VTaqnj+HIi5Rm
X-Received: by 10.153.11.133 with SMTP id ei5mr59973523lad.75.1417482635789;
 Mon, 01 Dec 2014 17:10:35 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Mon, 1 Dec 2014 17:10:15 -0800 (PST)
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 1 Dec 2014 17:10:15 -0800
Message-ID: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
Subject: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11347120fb56cf05093164da
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11347120fb56cf05093164da
Content-Type: text/plain; charset=UTF-8

i'll send out a reminder next week, but i wanted to give a heads up:  i'll
be bringing down the entire jenkins infrastructure for reboots and system
updates.

please let me know if there are any conflicts with this, thanks!

shane

--001a11347120fb56cf05093164da--

From dev-return-10601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 02:42:53 2014
Return-Path: <dev-return-10601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 37B42C31C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 02:42:53 +0000 (UTC)
Received: (qmail 40322 invoked by uid 500); 2 Dec 2014 02:42:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40252 invoked by uid 500); 2 Dec 2014 02:42:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40239 invoked by uid 99); 2 Dec 2014 02:42:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:42:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:42:45 +0000
Received: by mail-vc0-f178.google.com with SMTP id hq11so5443884vcb.9
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 18:42:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Qu4p47/ibJf+e9d9Y0Z0j1hv293X33CD6U8zUEAxzQ8=;
        b=yw+FKdCuoDLZdSBdQfGDjKmM8Ui+06wayRltj/2JeZFZwsMKzGrg0SnnF2lidM5see
         q77lA4kPFqFT7KAZlqsW199XlmPdbkPp5CNS0UPk4J4SSw94JK0meIAtjDe6hldj7T1/
         wSoNEQOiRaBqPs4WfGHCmovTLN4Snf/NClD5oh+P4mbDZBg7CGSkq+ZaG9tZM63DFbM/
         jHv6UTuShQrq5CEySby1c2lVHmVnTfPKVr0s3yAVwYV0/Dp4OMJU51O20ajEkTuuABMF
         YMDx4wZaQL90aCpTCITHyP+NmzLIBT+SDP4xiAWnJcctTiMoFCE09o2KbZRcNDv3iWn3
         t98A==
MIME-Version: 1.0
X-Received: by 10.52.165.77 with SMTP id yw13mr16106021vdb.62.1417488145224;
 Mon, 01 Dec 2014 18:42:25 -0800 (PST)
Received: by 10.31.166.137 with HTTP; Mon, 1 Dec 2014 18:42:25 -0800 (PST)
Date: Mon, 1 Dec 2014 18:42:25 -0800
Message-ID: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
Subject: Required file not found in building
From: Stephen Boesch <javadba@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c21d145e9af2050932ad2b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c21d145e9af2050932ad2b
Content-Type: text/plain; charset=UTF-8

It seems there were some additional settings required to build spark now .
This should be a snap for most of you ot there about what I am missing.
Here is the command line I have traditionally used:

   mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests

That command line is however failing with the lastest from HEAD:

INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
spark-network-common_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)

*[error] Required file not found: scala-compiler-2.10.4.jar*

*[error] See zinc -help for information about locating necessary files*

[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [4.077s]
[INFO] Spark Project Networking .......................... FAILURE [0.445s]


OK let's try "zinc -help":

18:38:00/spark2 $*zinc -help*
Nailgun server running with 1 cached compiler

Version = 0.3.5.1

Zinc compiler cache limit = 5
Resident scalac cache limit = 0
Analysis cache limit = 5

Compiler(Scala 2.10.4) [74ff364f]
Setup = {
*   scala compiler =
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
   scala library =
/Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
   scala extra = {

/Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
      /shared/zinc-0.3.5.1/lib/scala-reflect.jar
   }
   sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
   compiler interface sources =
/shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
   java home =
   fork java = false
   cache directory = /Users/steve/.zinc/0.3.5.1
}

Does that compiler jar exist?  Yes!

18:39:34/spark2 $ll
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
-rw-r--r--  1 steve  staff  14445780 Apr  9  2014
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar

--001a11c21d145e9af2050932ad2b--

From dev-return-10602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 02:50:14 2014
Return-Path: <dev-return-10602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90B04C352
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 02:50:14 +0000 (UTC)
Received: (qmail 53274 invoked by uid 500); 2 Dec 2014 02:50:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53150 invoked by uid 500); 2 Dec 2014 02:50:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52528 invoked by uid 99); 2 Dec 2014 02:50:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:50:12 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dineshjweerakkody@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:50:08 +0000
Received: by mail-ie0-f181.google.com with SMTP id tp5so10770728ieb.12
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 18:47:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=PMtsQOBcBjQ59FMniYxnsr5tH8Zr0nlo8JaRYNRmFVg=;
        b=DAwy0WzWq9Gi8E2l6yqW78E+8D2GG6ZXKdGHTOfTK9WrjHjmROAzEoBjSF9Jivm+Hz
         9utwY+JIdf3G4DjkPApmuWOP/cdRqsuR/UsbZgIFLp+sI3aDXTAEN23IHFqzrXeWTNrP
         WP4+Eth/5O6rv2eG9+KjM+1fyczvdfJHKpib/rLTqaI0GoNno7rqFcIbot3X+eqCWWyp
         o0NyHpZtwUq7eFG+55GC32BRjj7Wb/A35HVjhx45hG2aXqwsjjxcCZwj4ccihO/6BwyI
         kRDlYINMz4kaCkgebk7EA2U0YkSSgvBSEJlgPNM6sOZn3cW05vjvJJWa5956yh0p8xW5
         tybg==
MIME-Version: 1.0
X-Received: by 10.42.153.131 with SMTP id m3mr930433icw.28.1417488452778; Mon,
 01 Dec 2014 18:47:32 -0800 (PST)
Received: by 10.107.3.80 with HTTP; Mon, 1 Dec 2014 18:47:32 -0800 (PST)
In-Reply-To: <547C49EA.9040804@gmail.com>
References: <547C49EA.9040804@gmail.com>
Date: Tue, 2 Dec 2014 08:17:32 +0530
Message-ID: <CAGC4hZtkqqVV49RSJ0hCMeE4r8=ri2c1A24-q57_RFU1dxpMhA@mail.gmail.com>
Subject: Re: packaging spark run time with osgi service
From: "Dinesh J. Weerakkody" <dineshjweerakkody@gmail.com>
To: Lochana Menikarachchi <lochanac@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba1efc30b386f6050932bf88
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba1efc30b386f6050932bf88
Content-Type: text/plain; charset=UTF-8

Hi Lochana,

can you please go through this mail thread [1]. I haven't tried but can be
useful.

[1]
http://apache-spark-user-list.1001560.n3.nabble.com/Packaging-a-spark-job-using-maven-td5615.html

On Mon, Dec 1, 2014 at 4:28 PM, Lochana Menikarachchi <lochanac@gmail.com>
wrote:

> I have spark core and mllib as dependencies for a spark based osgi
> service. When I call the model building method through a unit test (without
> osgi) it works OK. When I call it through the osgi service, nothing
> happens. I tried adding spark assembly jar. Now it throws following error..
>
> An error occurred while building supervised machine learning model: No
> configuration setting found for key 'akka.version'
> com.typesafe.config.ConfigException$Missing: No configuration setting
> found for key 'akka.version'
>     at com.typesafe.config.impl.SimpleConfig.findKey(
> SimpleConfig.java:115)
>     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:136)
>     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:142)
>     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:150)
>     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:155)
>     at com.typesafe.config.impl.SimpleConfig.getString(
> SimpleConfig.java:197)
>
> What is the correct way to include spark runtime dependencies to osgi
> service.. Thanks.
>
> Lochana
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
*www.dineshjweerakkody.com <http://www.dineshjweerakkody.com>*

--90e6ba1efc30b386f6050932bf88--

From dev-return-10603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 02:52:14 2014
Return-Path: <dev-return-10603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E8B59C368
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 02:52:14 +0000 (UTC)
Received: (qmail 57897 invoked by uid 500); 2 Dec 2014 02:52:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57831 invoked by uid 500); 2 Dec 2014 02:52:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57818 invoked by uid 99); 2 Dec 2014 02:52:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:52:13 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lochanac@gmail.com designates 209.85.220.50 as permitted sender)
Received: from [209.85.220.50] (HELO mail-pa0-f50.google.com) (209.85.220.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:51:47 +0000
Received: by mail-pa0-f50.google.com with SMTP id bj1so12423898pad.37
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 18:51:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type;
        bh=taV/56xSjSYfXA1JKhqwJDM0A52CvhI7U/s+frLCQx4=;
        b=byHUUtM2YZTlc0QEoG07tUSqWvB3oZOXKa2+p6ZpfwDcKW5XHmIHphV0ZKZv3iwUfn
         G85JuAlnzWLots51fjoUBassZb0j25hTHfTdy9zCks04yYC8C3idjT4coiPTUsSSpgBS
         FLA+wgFri6BdfGzmjTF7NS1xUxuvFVB7tHDzq7KTL+u1lCbi5Z7GTmHWFHUb4pAZX9an
         U69twjM4zsXkCl2K783AM27UVKXV40Xe4tOS1L91oDMYnRoH9iO5KZK/a6TsLMgHhjNo
         RcIP8KgZh6V+HimHy47jNegQVIwqtfMbWyaL6duPIipNWqp42LJK88ioSQczwduii5Cx
         09zA==
X-Received: by 10.68.130.136 with SMTP id oe8mr1347541pbb.121.1417488706113;
        Mon, 01 Dec 2014 18:51:46 -0800 (PST)
Received: from Lochanas-MacBook-Pro.local ([203.94.95.4])
        by mx.google.com with ESMTPSA id w1sm1770176pdf.38.2014.12.01.18.51.44
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 01 Dec 2014 18:51:45 -0800 (PST)
Message-ID: <547D2940.9090004@gmail.com>
Date: Tue, 02 Dec 2014 08:21:44 +0530
From: Lochana Menikarachchi <lochanac@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.2.0
MIME-Version: 1.0
To: "Dinesh J. Weerakkody" <dineshjweerakkody@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: packaging spark run time with osgi service
References: <547C49EA.9040804@gmail.com> <CAGC4hZtkqqVV49RSJ0hCMeE4r8=ri2c1A24-q57_RFU1dxpMhA@mail.gmail.com>
In-Reply-To: <CAGC4hZtkqqVV49RSJ0hCMeE4r8=ri2c1A24-q57_RFU1dxpMhA@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------030008030208060507030209"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------030008030208060507030209
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Already tried the solutions they provided.. Did not workout..
On 12/2/14 8:17 AM, Dinesh J. Weerakkody wrote:
> Hi Lochana,
>
> can you please go through this mail thread [1]. I haven't tried but 
> can be useful.
>
> [1] 
> http://apache-spark-user-list.1001560.n3.nabble.com/Packaging-a-spark-job-using-maven-td5615.html 
>
>
> On Mon, Dec 1, 2014 at 4:28 PM, Lochana Menikarachchi 
> <lochanac@gmail.com <mailto:lochanac@gmail.com>> wrote:
>
>     I have spark core and mllib as dependencies for a spark based osgi
>     service. When I call the model building method through a unit test
>     (without osgi) it works OK. When I call it through the osgi
>     service, nothing happens. I tried adding spark assembly jar. Now
>     it throws following error..
>
>     An error occurred while building supervised machine learning
>     model: No configuration setting found for key 'akka.version'
>     com.typesafe.config.ConfigException$Missing: No configuration
>     setting found for key 'akka.version'
>         at
>     com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:136)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:142)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:150)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:155)
>         at
>     com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:197)
>
>     What is the correct way to include spark runtime dependencies to
>     osgi service.. Thanks.
>
>     Lochana
>
>     ---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>
>
>
> -- 
> Thanks & Best Regards,
>
> *Dinesh J. Weerakkody*
> /www.dineshjweerakkody.com <http://www.dineshjweerakkody.com>/


--------------030008030208060507030209--

From dev-return-10604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 02:57:06 2014
Return-Path: <dev-return-10604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47523C391
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 02:57:06 +0000 (UTC)
Received: (qmail 63642 invoked by uid 500); 2 Dec 2014 02:57:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63566 invoked by uid 500); 2 Dec 2014 02:57:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63552 invoked by uid 99); 2 Dec 2014 02:57:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:57:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.52 as permitted sender)
Received: from [209.85.213.52] (HELO mail-yh0-f52.google.com) (209.85.213.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 02:57:00 +0000
Received: by mail-yh0-f52.google.com with SMTP id z6so5608519yhz.11
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 18:55:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Vz/OthIvDlow0wKs+NvD5KaU6jlGYTlRJHioyt2E6lk=;
        b=xYFnUrCUjb498Vnr0Q3/BWBSpDq0XrsOek6jHCY7bykzhAHFxTaXWAXRBuix6PKjEG
         0Q1Fip8tuRDSmvQeGNMRAkPjIBk2YIYKg1sJR6yVvIjVNnZ4PLNPun7KywaoE4kNwKMp
         6uV9VCfDpsPUHkw6VeP1NmHDOXDCWW7VMo6MlhUfcAlh1QLCTCALFK79jsxnTGAtr8iZ
         oqQwUzCZLUnt6FfRAbF3Il6NhrjspSu9hRYPFzCRL/knjvI1EMrYGbI9PB2CtS1fwZ2f
         Og6rJLEmIPRynEU9PY7uI47KXQczgQQF1goXVX7NUKkwuFsO5VMEgVPYP3B/DsanM+lN
         2G1Q==
MIME-Version: 1.0
X-Received: by 10.236.61.6 with SMTP id v6mr25727322yhc.44.1417488954367; Mon,
 01 Dec 2014 18:55:54 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Mon, 1 Dec 2014 18:55:54 -0800 (PST)
In-Reply-To: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
Date: Mon, 1 Dec 2014 18:55:54 -0800
Message-ID: <CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
Subject: Re: Required file not found in building
From: Ted Yu <yuzhihong@gmail.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160a63e9935c0050932dd49
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a63e9935c0050932dd49
Content-Type: text/plain; charset=UTF-8

I tried the same command on MacBook and didn't experience the same error.

Which OS are you using ?

Cheers

On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com> wrote:

> It seems there were some additional settings required to build spark now .
> This should be a snap for most of you ot there about what I am missing.
> Here is the command line I have traditionally used:
>
>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>
> That command line is however failing with the lastest from HEAD:
>
> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
> spark-network-common_2.10 ---
> [INFO] Using zinc server for incremental compilation
> [INFO] compiler plugin:
> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>
> *[error] Required file not found: scala-compiler-2.10.4.jar*
>
> *[error] See zinc -help for information about locating necessary files*
>
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM .......................... SUCCESS [4.077s]
> [INFO] Spark Project Networking .......................... FAILURE [0.445s]
>
>
> OK let's try "zinc -help":
>
> 18:38:00/spark2 $*zinc -help*
> Nailgun server running with 1 cached compiler
>
> Version = 0.3.5.1
>
> Zinc compiler cache limit = 5
> Resident scalac cache limit = 0
> Analysis cache limit = 5
>
> Compiler(Scala 2.10.4) [74ff364f]
> Setup = {
> *   scala compiler =
>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>    scala library =
>
> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>    scala extra = {
>
>
> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>    }
>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>    compiler interface sources =
> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>    java home =
>    fork java = false
>    cache directory = /Users/steve/.zinc/0.3.5.1
> }
>
> Does that compiler jar exist?  Yes!
>
> 18:39:34/spark2 $ll
>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>

--089e0160a63e9935c0050932dd49--

From dev-return-10605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 03:04:28 2014
Return-Path: <dev-return-10605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B61DC416
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 03:04:28 +0000 (UTC)
Received: (qmail 84277 invoked by uid 500); 2 Dec 2014 03:04:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84207 invoked by uid 500); 2 Dec 2014 03:04:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84190 invoked by uid 99); 2 Dec 2014 03:04:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 03:04:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 03:04:21 +0000
Received: by mail-vc0-f181.google.com with SMTP id le20so5392897vcb.12
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 19:02:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lLChPEK3i0gHwFEi18/5ngOKa2Uq9JNq6PVqvIaPko0=;
        b=cdvXP1IVDdzAixu2K+Lw/+Z9VL2tqb1gaNzwxqbJAQhgoKcmmJJNYX9Ek/GoPlzTkF
         WOD5hkKecNmjmr8Bqa8hWhHX8A1/RLx2vG/YmaK1o9jfusD3y8HdMPpPlFdcN5IVrYF4
         6HHUvNSAuGIJUB41T670Ai9v7dUBDnwoehYN7nVYR+f99tmNP/jQbt60EhnAO9i8ivCd
         D3s4K0pHVKcjJMNFFbH6/HSNN4FfD0RMwsjz/OoRenl9hgEndglXBsVdZAphGdYYAW0y
         RkZuvfDVFIVu7hzxy+cY+MOD0+B07R04rUQOFEVzMRw68WVu7gi5QZ15S6jQ4tpTd1+a
         2CKg==
MIME-Version: 1.0
X-Received: by 10.52.65.227 with SMTP id a3mr5541043vdt.32.1417489350859; Mon,
 01 Dec 2014 19:02:30 -0800 (PST)
Received: by 10.31.166.137 with HTTP; Mon, 1 Dec 2014 19:02:30 -0800 (PST)
In-Reply-To: <CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
Date: Mon, 1 Dec 2014 19:02:30 -0800
Message-ID: <CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
Subject: Re: Required file not found in building
From: Stephen Boesch <javadba@gmail.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf307f337a3b23a6050932f551
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307f337a3b23a6050932f551
Content-Type: text/plain; charset=UTF-8

Mac as well.  Just found the problem:  I had created an alias to zinc a
couple of months back. Apparently that is not happy with the build anymore.
No problem now that the issue has been isolated - just need to fix my zinc
alias.

2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:

> I tried the same command on MacBook and didn't experience the same error.
>
> Which OS are you using ?
>
> Cheers
>
> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
>> It seems there were some additional settings required to build spark now .
>> This should be a snap for most of you ot there about what I am missing.
>> Here is the command line I have traditionally used:
>>
>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>
>> That command line is however failing with the lastest from HEAD:
>>
>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>> spark-network-common_2.10 ---
>> [INFO] Using zinc server for incremental compilation
>> [INFO] compiler plugin:
>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>
>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>
>> *[error] See zinc -help for information about locating necessary files*
>>
>> [INFO]
>> ------------------------------------------------------------------------
>> [INFO] Reactor Summary:
>> [INFO]
>> [INFO] Spark Project Parent POM .......................... SUCCESS
>> [4.077s]
>> [INFO] Spark Project Networking .......................... FAILURE
>> [0.445s]
>>
>>
>> OK let's try "zinc -help":
>>
>> 18:38:00/spark2 $*zinc -help*
>> Nailgun server running with 1 cached compiler
>>
>> Version = 0.3.5.1
>>
>> Zinc compiler cache limit = 5
>> Resident scalac cache limit = 0
>> Analysis cache limit = 5
>>
>> Compiler(Scala 2.10.4) [74ff364f]
>> Setup = {
>> *   scala compiler =
>>
>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>    scala library =
>>
>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>    scala extra = {
>>
>>
>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>    }
>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>    compiler interface sources =
>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>    java home =
>>    fork java = false
>>    cache directory = /Users/steve/.zinc/0.3.5.1
>> }
>>
>> Does that compiler jar exist?  Yes!
>>
>> 18:39:34/spark2 $ll
>>
>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>
>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>
>
>

--20cf307f337a3b23a6050932f551--

From dev-return-10606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 04:01:01 2014
Return-Path: <dev-return-10606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BC20C5B5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 04:01:01 +0000 (UTC)
Received: (qmail 70955 invoked by uid 500); 2 Dec 2014 04:00:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70880 invoked by uid 500); 2 Dec 2014 04:00:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70868 invoked by uid 99); 2 Dec 2014 04:00:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:00:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.220.175 as permitted sender)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:00:32 +0000
Received: by mail-vc0-f175.google.com with SMTP id hy10so5404223vcb.34
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 20:00:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ekqBFgTl8D2dpLZIfPf3lyhuVlrJIGedaOLtZAI/l/Q=;
        b=GAcrxTD5azK1snq68tI6TbZ7mwJ+OiUluj/f8TE4Z4EG9IhKBfBoTEXvtOy5ylh0Pe
         0azQwYlQHiRna51STsSvE7WBzGsQgbtULb38KsIlMUceyTwu53KxehuAxxWJGHqs2FtB
         WutSEkxGXRptibaNvjDELlavndjmX4cbjQ+EnPV8XfeONN/8AQbqWOD4vGMd81FrP+RZ
         l5UcT0PbEFObHH6wj3VCLc4Ut97h5si5YMuCSRhcLH1Km8qNMNla4ZxPrYL1cCCYEgB8
         mR1/f/JVcRcuFcW37MZaT4eq77E+pQtgBqW0nofHlIlKT1bSpEbNP6fynq/5TnVG1Sg6
         xC9Q==
MIME-Version: 1.0
X-Received: by 10.220.158.137 with SMTP id f9mr30692927vcx.34.1417492831715;
 Mon, 01 Dec 2014 20:00:31 -0800 (PST)
Received: by 10.31.166.137 with HTTP; Mon, 1 Dec 2014 20:00:31 -0800 (PST)
In-Reply-To: <CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
	<CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
Date: Mon, 1 Dec 2014 20:00:31 -0800
Message-ID: <CACkSZy2XUw9GP1qa=YSvPFPBeq=5xAQShM-jSmB=ZvbaZ4yQ2g@mail.gmail.com>
Subject: Re: Required file not found in building
From: Stephen Boesch <javadba@gmail.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c28b16b4c9d2050933c421
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28b16b4c9d2050933c421
Content-Type: text/plain; charset=UTF-8

Anyone maybe can assist on how to run zinc with the latest maven build?

I am starting zinc as follows:

/shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home
$SCALA_HOME -nailed -start

The pertinent env vars are:


19:58:11/lib $echo $SCALA_HOME
/shared/scala
19:58:14/lib $which scala
/shared/scala/bin/scala
19:58:16/lib $scala -version
Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL


When I do *not *start zinc then the maven build works .. but v slowly since
no incremental compiler available.

When zinc is started as shown above then the error occurs on all of the
modules except parent:


[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[error] Required file not found: scala-compiler-2.10.4.jar
[error] See zinc -help for information about locating necessary files

2014-12-01 19:02 GMT-08:00 Stephen Boesch <javadba@gmail.com>:

> Mac as well.  Just found the problem:  I had created an alias to zinc a
> couple of months back. Apparently that is not happy with the build anymore.
> No problem now that the issue has been isolated - just need to fix my zinc
> alias.
>
> 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>
> I tried the same command on MacBook and didn't experience the same error.
>>
>> Which OS are you using ?
>>
>> Cheers
>>
>> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com> wrote:
>>
>>> It seems there were some additional settings required to build spark now
>>> .
>>> This should be a snap for most of you ot there about what I am missing.
>>> Here is the command line I have traditionally used:
>>>
>>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>>
>>> That command line is however failing with the lastest from HEAD:
>>>
>>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>>> spark-network-common_2.10 ---
>>> [INFO] Using zinc server for incremental compilation
>>> [INFO] compiler plugin:
>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>>
>>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>>
>>> *[error] See zinc -help for information about locating necessary files*
>>>
>>> [INFO]
>>> ------------------------------------------------------------------------
>>> [INFO] Reactor Summary:
>>> [INFO]
>>> [INFO] Spark Project Parent POM .......................... SUCCESS
>>> [4.077s]
>>> [INFO] Spark Project Networking .......................... FAILURE
>>> [0.445s]
>>>
>>>
>>> OK let's try "zinc -help":
>>>
>>> 18:38:00/spark2 $*zinc -help*
>>> Nailgun server running with 1 cached compiler
>>>
>>> Version = 0.3.5.1
>>>
>>> Zinc compiler cache limit = 5
>>> Resident scalac cache limit = 0
>>> Analysis cache limit = 5
>>>
>>> Compiler(Scala 2.10.4) [74ff364f]
>>> Setup = {
>>> *   scala compiler =
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>>    scala library =
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>>    scala extra = {
>>>
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>>    }
>>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>>    compiler interface sources =
>>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>>    java home =
>>>    fork java = false
>>>    cache directory = /Users/steve/.zinc/0.3.5.1
>>> }
>>>
>>> Does that compiler jar exist?  Yes!
>>>
>>> 18:39:34/spark2 $ll
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>
>>
>>
>

--001a11c28b16b4c9d2050933c421--

From dev-return-10607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 04:14:50 2014
Return-Path: <dev-return-10607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5F85C649
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 04:14:50 +0000 (UTC)
Received: (qmail 93135 invoked by uid 500); 2 Dec 2014 04:14:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93046 invoked by uid 500); 2 Dec 2014 04:14:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93028 invoked by uid 99); 2 Dec 2014 04:14:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:14:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.53 as permitted sender)
Received: from [209.85.213.53] (HELO mail-yh0-f53.google.com) (209.85.213.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:14:44 +0000
Received: by mail-yh0-f53.google.com with SMTP id i57so5560381yha.26
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 20:12:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4DM8+/tOXt3kWldE6BTsQMdLtzuNYWDF2bUSWNYLUjY=;
        b=cjNAY3uSodEIbHwJ/nk4orRF9KqnVA1klZEah7DNGePh5dqPJhHYv9DNNLXWK9jfE6
         bQ1eoA7xCLDpKSHNVwp1VFkloIgT8TIdM8HHrOItG+zpSXPpa5VnDzaZHtGD4K+80opA
         4lQ0f/ZtTc72vek9PiVsMfTqz051MQg5/nBv7+Vu8f0QS0OfSw9gu8is3YVKhY490keJ
         dk9x0mSNXQbAfqmSAy3iy+QpiVplLEtMFiwr/P2x9oDwx41Qo/kihPOpdI2TeVTAlEFI
         pXzH7Z/Viu7DedpRFzab6yMEXImKRvdBTvCbNwRZ5T0Vg7PoRGGv5pSw9SQImbAshrJJ
         /mcw==
MIME-Version: 1.0
X-Received: by 10.236.206.8 with SMTP id k8mr9602839yho.23.1417493528721; Mon,
 01 Dec 2014 20:12:08 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Mon, 1 Dec 2014 20:12:08 -0800 (PST)
In-Reply-To: <CACkSZy2XUw9GP1qa=YSvPFPBeq=5xAQShM-jSmB=ZvbaZ4yQ2g@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
	<CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
	<CACkSZy2XUw9GP1qa=YSvPFPBeq=5xAQShM-jSmB=ZvbaZ4yQ2g@mail.gmail.com>
Date: Mon, 1 Dec 2014 20:12:08 -0800
Message-ID: <CALte62x9F=4VtjEYNci7C8ey7q4sXUi5_rAg=mdUXfnhhAU2sA@mail.gmail.com>
Subject: Re: Required file not found in building
From: Ted Yu <yuzhihong@gmail.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011616a8403e95050933ee46
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011616a8403e95050933ee46
Content-Type: text/plain; charset=UTF-8

I use zinc 0.2.0 and started zinc with the same command shown below.

I don't observe such error.

How did you install zinc-0.3.5.3 ?

Cheers

On Mon, Dec 1, 2014 at 8:00 PM, Stephen Boesch <javadba@gmail.com> wrote:

>
> Anyone maybe can assist on how to run zinc with the latest maven build?
>
> I am starting zinc as follows:
>
> /shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home
> $SCALA_HOME -nailed -start
>
> The pertinent env vars are:
>
>
> 19:58:11/lib $echo $SCALA_HOME
> /shared/scala
> 19:58:14/lib $which scala
> /shared/scala/bin/scala
> 19:58:16/lib $scala -version
> Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL
>
>
> When I do *not *start zinc then the maven build works .. but v slowly
> since no incremental compiler available.
>
> When zinc is started as shown above then the error occurs on all of the
> modules except parent:
>
>
> [INFO] Using zinc server for incremental compilation
> [INFO] compiler plugin:
> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
> [error] Required file not found: scala-compiler-2.10.4.jar
> [error] See zinc -help for information about locating necessary files
>
> 2014-12-01 19:02 GMT-08:00 Stephen Boesch <javadba@gmail.com>:
>
> Mac as well.  Just found the problem:  I had created an alias to zinc a
>> couple of months back. Apparently that is not happy with the build anymore.
>> No problem now that the issue has been isolated - just need to fix my zinc
>> alias.
>>
>> 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>>
>> I tried the same command on MacBook and didn't experience the same error.
>>>
>>> Which OS are you using ?
>>>
>>> Cheers
>>>
>>> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com>
>>> wrote:
>>>
>>>> It seems there were some additional settings required to build spark
>>>> now .
>>>> This should be a snap for most of you ot there about what I am missing.
>>>> Here is the command line I have traditionally used:
>>>>
>>>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>>>
>>>> That command line is however failing with the lastest from HEAD:
>>>>
>>>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>>>> spark-network-common_2.10 ---
>>>> [INFO] Using zinc server for incremental compilation
>>>> [INFO] compiler plugin:
>>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>>>
>>>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>>>
>>>> *[error] See zinc -help for information about locating necessary files*
>>>>
>>>> [INFO]
>>>> ------------------------------------------------------------------------
>>>> [INFO] Reactor Summary:
>>>> [INFO]
>>>> [INFO] Spark Project Parent POM .......................... SUCCESS
>>>> [4.077s]
>>>> [INFO] Spark Project Networking .......................... FAILURE
>>>> [0.445s]
>>>>
>>>>
>>>> OK let's try "zinc -help":
>>>>
>>>> 18:38:00/spark2 $*zinc -help*
>>>> Nailgun server running with 1 cached compiler
>>>>
>>>> Version = 0.3.5.1
>>>>
>>>> Zinc compiler cache limit = 5
>>>> Resident scalac cache limit = 0
>>>> Analysis cache limit = 5
>>>>
>>>> Compiler(Scala 2.10.4) [74ff364f]
>>>> Setup = {
>>>> *   scala compiler =
>>>>
>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>>>    scala library =
>>>>
>>>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>>>    scala extra = {
>>>>
>>>>
>>>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>>>    }
>>>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>>>    compiler interface sources =
>>>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>>>    java home =
>>>>    fork java = false
>>>>    cache directory = /Users/steve/.zinc/0.3.5.1
>>>> }
>>>>
>>>> Does that compiler jar exist?  Yes!
>>>>
>>>> 18:39:34/spark2 $ll
>>>>
>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>>>
>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>>
>>>
>>>
>>
>

--089e011616a8403e95050933ee46--

From dev-return-10608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 04:19:18 2014
Return-Path: <dev-return-10608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5BBC5C668
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 04:19:18 +0000 (UTC)
Received: (qmail 667 invoked by uid 500); 2 Dec 2014 04:19:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 594 invoked by uid 500); 2 Dec 2014 04:19:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 581 invoked by uid 99); 2 Dec 2014 04:19:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:19:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.171 as permitted sender)
Received: from [209.85.220.171] (HELO mail-vc0-f171.google.com) (209.85.220.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 04:19:11 +0000
Received: by mail-vc0-f171.google.com with SMTP id hy4so5339103vcb.16
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 20:18:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qnqH2cdHz7CrRTq4kh0TFtgW8f4sSuw5hHcWiFHtOko=;
        b=Rs1pj5ekd7DboATT4udzl+2RviHxkXVSo/j7GFumC7D9N4OQ1mYAiVW7ECqTITA7fh
         n1LQUqG0ao6raKNTlUZ/hAC8R4/FZ2A+zF6szFSLHLJx4OE6VdauWkzAYAZYAkBo4TpY
         XdJr5DDO/dTbVKM1qUxNxcdDtqvnZSS1J5IVlKdYIdrqi/HbF3zbMn2p/wDS+1x7xhp6
         ddWRthcgRV1aPwaeaZbkAP+NQWm/hZiB2k8lQdzUugVriSdI0iaXpAe/qTa/e8YtKURa
         KymCj+BnqFf5cxYv/+zjD7JkvyCi2IuEi8jkMfu6Limc2oYN2pLUTkTSegaqQn/PqClr
         RKWQ==
MIME-Version: 1.0
X-Received: by 10.220.158.137 with SMTP id f9mr30720254vcx.34.1417493931024;
 Mon, 01 Dec 2014 20:18:51 -0800 (PST)
Received: by 10.31.166.137 with HTTP; Mon, 1 Dec 2014 20:18:50 -0800 (PST)
In-Reply-To: <CALte62x9F=4VtjEYNci7C8ey7q4sXUi5_rAg=mdUXfnhhAU2sA@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
	<CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
	<CACkSZy2XUw9GP1qa=YSvPFPBeq=5xAQShM-jSmB=ZvbaZ4yQ2g@mail.gmail.com>
	<CALte62x9F=4VtjEYNci7C8ey7q4sXUi5_rAg=mdUXfnhhAU2sA@mail.gmail.com>
Date: Mon, 1 Dec 2014 20:18:50 -0800
Message-ID: <CACkSZy3FMKAxJEAv=vmYkOBpVARLrMC__ioRBQvCpa9=FfUs+g@mail.gmail.com>
Subject: Re: Required file not found in building
From: Stephen Boesch <javadba@gmail.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c28b163ae64105093406ac
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28b163ae64105093406ac
Content-Type: text/plain; charset=UTF-8

The zinc src zip for  0.3.5.3 was  downloaded  and exploded. Then I  ran
sbt dist/create .  zinc is being launched from
dist/target/zinc-0.3.5.3/bin/zinc

2014-12-01 20:12 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:

> I use zinc 0.2.0 and started zinc with the same command shown below.
>
> I don't observe such error.
>
> How did you install zinc-0.3.5.3 ?
>
> Cheers
>
> On Mon, Dec 1, 2014 at 8:00 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
>>
>> Anyone maybe can assist on how to run zinc with the latest maven build?
>>
>> I am starting zinc as follows:
>>
>> /shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home
>> $SCALA_HOME -nailed -start
>>
>> The pertinent env vars are:
>>
>>
>> 19:58:11/lib $echo $SCALA_HOME
>> /shared/scala
>> 19:58:14/lib $which scala
>> /shared/scala/bin/scala
>> 19:58:16/lib $scala -version
>> Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL
>>
>>
>> When I do *not *start zinc then the maven build works .. but v slowly
>> since no incremental compiler available.
>>
>> When zinc is started as shown above then the error occurs on all of the
>> modules except parent:
>>
>>
>> [INFO] Using zinc server for incremental compilation
>> [INFO] compiler plugin:
>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>> [error] Required file not found: scala-compiler-2.10.4.jar
>> [error] See zinc -help for information about locating necessary files
>>
>> 2014-12-01 19:02 GMT-08:00 Stephen Boesch <javadba@gmail.com>:
>>
>> Mac as well.  Just found the problem:  I had created an alias to zinc a
>>> couple of months back. Apparently that is not happy with the build anymore.
>>> No problem now that the issue has been isolated - just need to fix my zinc
>>> alias.
>>>
>>> 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>>>
>>> I tried the same command on MacBook and didn't experience the same error.
>>>>
>>>> Which OS are you using ?
>>>>
>>>> Cheers
>>>>
>>>> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com>
>>>> wrote:
>>>>
>>>>> It seems there were some additional settings required to build spark
>>>>> now .
>>>>> This should be a snap for most of you ot there about what I am missing.
>>>>> Here is the command line I have traditionally used:
>>>>>
>>>>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>>>>
>>>>> That command line is however failing with the lastest from HEAD:
>>>>>
>>>>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>>>>> spark-network-common_2.10 ---
>>>>> [INFO] Using zinc server for incremental compilation
>>>>> [INFO] compiler plugin:
>>>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>>>>
>>>>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>>>>
>>>>> *[error] See zinc -help for information about locating necessary files*
>>>>>
>>>>> [INFO]
>>>>>
>>>>> ------------------------------------------------------------------------
>>>>> [INFO] Reactor Summary:
>>>>> [INFO]
>>>>> [INFO] Spark Project Parent POM .......................... SUCCESS
>>>>> [4.077s]
>>>>> [INFO] Spark Project Networking .......................... FAILURE
>>>>> [0.445s]
>>>>>
>>>>>
>>>>> OK let's try "zinc -help":
>>>>>
>>>>> 18:38:00/spark2 $*zinc -help*
>>>>> Nailgun server running with 1 cached compiler
>>>>>
>>>>> Version = 0.3.5.1
>>>>>
>>>>> Zinc compiler cache limit = 5
>>>>> Resident scalac cache limit = 0
>>>>> Analysis cache limit = 5
>>>>>
>>>>> Compiler(Scala 2.10.4) [74ff364f]
>>>>> Setup = {
>>>>> *   scala compiler =
>>>>>
>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>>>>    scala library =
>>>>>
>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>>>>    scala extra = {
>>>>>
>>>>>
>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>>>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>>>>    }
>>>>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>>>>    compiler interface sources =
>>>>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>>>>    java home =
>>>>>    fork java = false
>>>>>    cache directory = /Users/steve/.zinc/0.3.5.1
>>>>> }
>>>>>
>>>>> Does that compiler jar exist?  Yes!
>>>>>
>>>>> 18:39:34/spark2 $ll
>>>>>
>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>>>>
>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>>>
>>>>
>>>>
>>>
>>
>

--001a11c28b163ae64105093406ac--

From dev-return-10609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 05:16:45 2014
Return-Path: <dev-return-10609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 775C6C7F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 05:16:45 +0000 (UTC)
Received: (qmail 69323 invoked by uid 500); 2 Dec 2014 05:16:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69264 invoked by uid 500); 2 Dec 2014 05:16:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69251 invoked by uid 99); 2 Dec 2014 05:16:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:16:43 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pop1998@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:16:17 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so26827503wiv.11
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 21:16:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=nopVehepf5zPUOB9XoB3FeSu8OZeIiQEwI+3OgJz7YE=;
        b=e9haZmETJX6IY4klcqUA0zrWc1aWyH1cBJvq5WyYHm2HUbXpz4Vyk70awBeGmVX4gG
         7F41pyJYiGVknSyflc01FWPJ1n4iVU1ok4hhaZPrbtVry3k2DmdrLVLx+7vxJsj34klv
         uZshpBftpPo7kAHhFmN9J3lOeeJ41mocmCXf5Ls4xLwDyyHcJTGdSeXv8UuJc91xC/iH
         8FjbUgwvPm0ixXUZ8Cr6yppZBxJbX2uoj7dmY8+dQP9dvG6H80/0mg8hXSMhyTdjy6By
         daaz/3ftATyiuMUkQR9Fhd5Y/wZxtlkGbItuzVE4GOnvPqmWusM6tJmTaYHr9Q296TrY
         p7lw==
X-Received: by 10.181.12.17 with SMTP id em17mr90338048wid.45.1417497376837;
 Mon, 01 Dec 2014 21:16:16 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.178.146 with HTTP; Mon, 1 Dec 2014 21:15:56 -0800 (PST)
From: Isca Harmatz <pop1998@gmail.com>
Date: Tue, 2 Dec 2014 07:15:56 +0200
Message-ID: <CAATMUCitui9fdVxcqM45PS++kxjdnoJFTX1SFukNf+UL8FX02g@mail.gmail.com>
Subject: test
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043893c39dd4d4050934d332
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043893c39dd4d4050934d332
Content-Type: text/plain; charset=UTF-8



--f46d043893c39dd4d4050934d332--

From dev-return-10610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 05:18:03 2014
Return-Path: <dev-return-10610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7635BC7F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 05:18:03 +0000 (UTC)
Received: (qmail 71895 invoked by uid 500); 2 Dec 2014 05:18:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71820 invoked by uid 500); 2 Dec 2014 05:18:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71806 invoked by uid 99); 2 Dec 2014 05:18:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:18:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.179 as permitted sender)
Received: from [209.85.160.179] (HELO mail-yk0-f179.google.com) (209.85.160.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:17:57 +0000
Received: by mail-yk0-f179.google.com with SMTP id 19so5493514ykq.38
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 21:16:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mNxNRoEmcd3gtov5/M83UwGFk2Q1eFDEVOIsZ/SkQG8=;
        b=iGXlB0DaDRldoWInWZ/waeGT6+IwlC6kQVYJDORrTPq4K7QAXoftZsktgy/sUH2HVX
         nLNlSWuPNij9gLAa15azbRQT17jo5epPyVOiqNk6BJ90F+Upc6P3jvNybqSawcSv48//
         hGcpre0muwbIBERxh39v+3FY0aN6m8YL38uPX0jhB4YzO8k9Z73Keks7mfTUP4oFUqYI
         Mf91ddxTCkIbr16vyMiFV5Us+V136zBRAEszGePdzDzrfg63SlMZuiBRCsjyIr0dWLs3
         sV0S8uJB47MQp0KsJy2rINEAEDnqe2X4c2lNpwmvVnk6x9oxRgCoDROJJy7U7o5pcrxO
         at5w==
MIME-Version: 1.0
X-Received: by 10.170.66.207 with SMTP id i198mr19463204yki.26.1417497411374;
 Mon, 01 Dec 2014 21:16:51 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Mon, 1 Dec 2014 21:16:51 -0800 (PST)
In-Reply-To: <CACkSZy3FMKAxJEAv=vmYkOBpVARLrMC__ioRBQvCpa9=FfUs+g@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
	<CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
	<CACkSZy2XUw9GP1qa=YSvPFPBeq=5xAQShM-jSmB=ZvbaZ4yQ2g@mail.gmail.com>
	<CALte62x9F=4VtjEYNci7C8ey7q4sXUi5_rAg=mdUXfnhhAU2sA@mail.gmail.com>
	<CACkSZy3FMKAxJEAv=vmYkOBpVARLrMC__ioRBQvCpa9=FfUs+g@mail.gmail.com>
Date: Mon, 1 Dec 2014 21:16:51 -0800
Message-ID: <CALte62yY_w7ZPT47SOWD+VzwpCeVPVLcxw+eGGy8qXwD6Hr=9w@mail.gmail.com>
Subject: Re: Required file not found in building
From: Ted Yu <yuzhihong@gmail.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134f9a0ace16e050934d5af
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134f9a0ace16e050934d5af
Content-Type: text/plain; charset=UTF-8

I used the following for brew:
http://repo.typesafe.com/typesafe/zinc/com/typesafe/zinc/dist/0.3.0/zinc-0.3.0.tgz

After starting zinc, I issued the same mvn command but didn't encounter the
error you saw.

FYI

On Mon, Dec 1, 2014 at 8:18 PM, Stephen Boesch <javadba@gmail.com> wrote:

> The zinc src zip for  0.3.5.3 was  downloaded  and exploded. Then I  ran
> sbt dist/create .  zinc is being launched from
> dist/target/zinc-0.3.5.3/bin/zinc
>
> 2014-12-01 20:12 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>
> I use zinc 0.2.0 and started zinc with the same command shown below.
>>
>> I don't observe such error.
>>
>> How did you install zinc-0.3.5.3 ?
>>
>> Cheers
>>
>> On Mon, Dec 1, 2014 at 8:00 PM, Stephen Boesch <javadba@gmail.com> wrote:
>>
>>>
>>> Anyone maybe can assist on how to run zinc with the latest maven build?
>>>
>>> I am starting zinc as follows:
>>>
>>> /shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home
>>> $SCALA_HOME -nailed -start
>>>
>>> The pertinent env vars are:
>>>
>>>
>>> 19:58:11/lib $echo $SCALA_HOME
>>> /shared/scala
>>> 19:58:14/lib $which scala
>>> /shared/scala/bin/scala
>>> 19:58:16/lib $scala -version
>>> Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL
>>>
>>>
>>> When I do *not *start zinc then the maven build works .. but v slowly
>>> since no incremental compiler available.
>>>
>>> When zinc is started as shown above then the error occurs on all of the
>>> modules except parent:
>>>
>>>
>>> [INFO] Using zinc server for incremental compilation
>>> [INFO] compiler plugin:
>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>> [error] Required file not found: scala-compiler-2.10.4.jar
>>> [error] See zinc -help for information about locating necessary files
>>>
>>> 2014-12-01 19:02 GMT-08:00 Stephen Boesch <javadba@gmail.com>:
>>>
>>> Mac as well.  Just found the problem:  I had created an alias to zinc a
>>>> couple of months back. Apparently that is not happy with the build anymore.
>>>> No problem now that the issue has been isolated - just need to fix my zinc
>>>> alias.
>>>>
>>>> 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>>>>
>>>> I tried the same command on MacBook and didn't experience the same
>>>>> error.
>>>>>
>>>>> Which OS are you using ?
>>>>>
>>>>> Cheers
>>>>>
>>>>> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> It seems there were some additional settings required to build spark
>>>>>> now .
>>>>>> This should be a snap for most of you ot there about what I am
>>>>>> missing.
>>>>>> Here is the command line I have traditionally used:
>>>>>>
>>>>>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>>>>>
>>>>>> That command line is however failing with the lastest from HEAD:
>>>>>>
>>>>>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>>>>>> spark-network-common_2.10 ---
>>>>>> [INFO] Using zinc server for incremental compilation
>>>>>> [INFO] compiler plugin:
>>>>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>>>>>
>>>>>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>>>>>
>>>>>> *[error] See zinc -help for information about locating necessary
>>>>>> files*
>>>>>>
>>>>>> [INFO]
>>>>>>
>>>>>> ------------------------------------------------------------------------
>>>>>> [INFO] Reactor Summary:
>>>>>> [INFO]
>>>>>> [INFO] Spark Project Parent POM .......................... SUCCESS
>>>>>> [4.077s]
>>>>>> [INFO] Spark Project Networking .......................... FAILURE
>>>>>> [0.445s]
>>>>>>
>>>>>>
>>>>>> OK let's try "zinc -help":
>>>>>>
>>>>>> 18:38:00/spark2 $*zinc -help*
>>>>>> Nailgun server running with 1 cached compiler
>>>>>>
>>>>>> Version = 0.3.5.1
>>>>>>
>>>>>> Zinc compiler cache limit = 5
>>>>>> Resident scalac cache limit = 0
>>>>>> Analysis cache limit = 5
>>>>>>
>>>>>> Compiler(Scala 2.10.4) [74ff364f]
>>>>>> Setup = {
>>>>>> *   scala compiler =
>>>>>>
>>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>>>>>    scala library =
>>>>>>
>>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>>>>>    scala extra = {
>>>>>>
>>>>>>
>>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>>>>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>>>>>    }
>>>>>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>>>>>    compiler interface sources =
>>>>>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>>>>>    java home =
>>>>>>    fork java = false
>>>>>>    cache directory = /Users/steve/.zinc/0.3.5.1
>>>>>> }
>>>>>>
>>>>>> Does that compiler jar exist?  Yes!
>>>>>>
>>>>>> 18:39:34/spark2 $ll
>>>>>>
>>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>>>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>>>>>
>>>>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--001a1134f9a0ace16e050934d5af--

From dev-return-10611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 05:20:23 2014
Return-Path: <dev-return-10611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3CAD3C80B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 05:20:23 +0000 (UTC)
Received: (qmail 78513 invoked by uid 500); 2 Dec 2014 05:20:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78444 invoked by uid 500); 2 Dec 2014 05:20:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78431 invoked by uid 99); 2 Dec 2014 05:20:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:20:21 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pop1998@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:20:16 +0000
Received: by mail-wi0-f179.google.com with SMTP id ex7so19707521wid.6
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 21:18:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=XlpMgWXnXwg6GQg8039OJmqfFoAg8AE1Qx92728bRjA=;
        b=C7bAlqPsYQ9FTB70JNqAhjV7T57+AaWp0qKxKzOwguFEl8yxWg/ldDQNsBv1LO0gbB
         jyvkzuL06lA4t4vl7CwcWQbStyKePpJv1pM8RTFpX/UrSn6s6ETMgD0d57BJTLm92i7O
         2iI6L3Scbw3y+CmNsB7FNh4+G8rbGZo4Vaq9QyTAAI8aGuM8z60j+xZdrFpljumXGXCF
         LpwWwa0PATpKzXozozDbDdSrfMP4IVM6RxFrbNyWePuWvw0g6CNJ2PBx4uFLzsbvq2/T
         QICVMjGgQSU9XNcg4V47b+ndYTVthfN4pz3IwroayLhviUurXZVMvhpK31v61cwMVTq/
         qvXA==
X-Received: by 10.194.59.166 with SMTP id a6mr37065892wjr.49.1417497505209;
 Mon, 01 Dec 2014 21:18:25 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.178.146 with HTTP; Mon, 1 Dec 2014 21:18:05 -0800 (PST)
From: Isca Harmatz <pop1998@gmail.com>
Date: Tue, 2 Dec 2014 07:18:05 +0200
Message-ID: <CAATMUCgmvGW=wGP8vbVLS9YZ=XupwAHomh+_n3UMRDkhe5-ASQ@mail.gmail.com>
Subject: Monitoring Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86dc6644a2bc050934db5b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86dc6644a2bc050934db5b
Content-Type: text/plain; charset=UTF-8

hello,

im running spark on a cluster and i want to monitor how many nodes/ cores
are active in different (specific) points of the program.

is there any way to do this?

thanks,
  Isca

--047d7b86dc6644a2bc050934db5b--

From dev-return-10612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 05:24:18 2014
Return-Path: <dev-return-10612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02AF8C818
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 05:24:18 +0000 (UTC)
Received: (qmail 85820 invoked by uid 500); 2 Dec 2014 05:24:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85757 invoked by uid 500); 2 Dec 2014 05:24:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85745 invoked by uid 99); 2 Dec 2014 05:24:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:24:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda@wso2.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 05:24:12 +0000
Received: by mail-wi0-f182.google.com with SMTP id h11so19696045wiw.15
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 21:23:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=wso2.com; s=google;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=QXKRzoghnb4IelwnxRl8KSsMItujbYlIcEzJd11SX6Q=;
        b=hZS5KkI97Mnn2UsGohO0uDal1tSQqXdY6mbl9PomzUubPLxRo8Sfst01UL516jS7Ny
         Rz8p0wvf21cE2Zujcj9WBtU0/7licgSCpUwqnnynFOsBMw1QE3TPXUh8MrZP2oPpQzSe
         ii3KvoBGP5UNdy4CQG18yDyjumYO98dgzuwWU=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=QXKRzoghnb4IelwnxRl8KSsMItujbYlIcEzJd11SX6Q=;
        b=UaIGUs1Tx+Nh6KGOVMy2PuCLCiOWEkZkjoTpmhKvzYjQsxDZ7fL5RJ7Qg0qPPMPisF
         e3XorN4SmJzcaul2HhwyfWGdetvN1vIhEXMiSANmCxmm7tp2ABaf2YF2V0AR0eMJVqsQ
         V8nRD4ygtRCnXImoVQeZqYJmArZlvFVd9+r58ICDnCi7XaKiCxV9vuXNqbTWBPG2XsTP
         i1iHhd5LETLVmYpydhW7jBoGeRVTuJk0SldQEkJ4EaDGWTEqkAdAC2skvKXG4WxfhRA0
         hoOMC+GThGumWLSiYJlrphc/E6xtSPhN24klUgdO/PImuFOEGJVzIpetPFHrqcMplOti
         +lUQ==
X-Gm-Message-State: ALoCoQnc41NpFi6Q9bwOeiMduXTkaXW/goQEUo2cUhGg0STYpM5arNy1y/s0DL82e/axt9c2zXtL
X-Received: by 10.194.193.38 with SMTP id hl6mr54715177wjc.38.1417497785733;
 Mon, 01 Dec 2014 21:23:05 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.174.66 with HTTP; Mon, 1 Dec 2014 21:22:45 -0800 (PST)
From: Niranda Perera <niranda@wso2.com>
Date: Tue, 2 Dec 2014 10:52:45 +0530
Message-ID: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
Subject: Can the Scala classes in the spark source code, be inherited in Java classes?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb03f08fd2f12050934eb8b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03f08fd2f12050934eb8b
Content-Type: text/plain; charset=UTF-8

Hi,

Can the Scala classes in the spark source code, be inherited (and other OOP
concepts) in Java classes?

I want to customize some part of the code, but I would like to do it in a
Java environment.

Rgds

-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>

--047d7bb03f08fd2f12050934eb8b--

From dev-return-10613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 06:40:06 2014
Return-Path: <dev-return-10613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A084CA03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 06:40:06 +0000 (UTC)
Received: (qmail 94758 invoked by uid 500); 2 Dec 2014 06:40:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94677 invoked by uid 500); 2 Dec 2014 06:40:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94666 invoked by uid 99); 2 Dec 2014 06:40:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:40:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:39:59 +0000
Received: by mail-wg0-f46.google.com with SMTP id a1so7876636wgh.19
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 22:39:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=GtLfy4waQOaRBj4eCI/EoFK4lKRjjWqzomInDP3fy7I=;
        b=N9PvzQC8N0CgGRUmIIzr4YMoye2CaPI5shJFPiBpKe64Nf7Lw0e7NOGqkaLNfDlDpL
         DrrSXXmokYNZhIxxiOyAinCvpZVHO4Sy5WaBdoh45B+rrOCleKUGcNlpQuniMd/RXAC0
         LPYrBRiv5PjAOACQNVFwIn5P2nSxz8N2P/OrYAA5MK1bRID48cvrrArawehdBXbtSfFH
         EqRBRuphHbeQ2YhUbOJpwcZcUvs/V9cjssDTA8nBUj7gCiPsrW2dYjRRSDtJ9RLN9TCH
         WnrD0D86Ps6eqCghccEHsKXXztzd9c4vEv6STM8YnJmWHaHFkyCIKqIA2kXIMtb3adCn
         6a2g==
X-Gm-Message-State: ALoCoQnAqvQnRJ8mcYVTrz/Y9MjgL51MTC7uDM3OsTeYzfCWLFsy60VKl7mQ7v6zVrs585YblugQ
X-Received: by 10.180.76.144 with SMTP id k16mr2432701wiw.3.1417502378824;
 Mon, 01 Dec 2014 22:39:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Mon, 1 Dec 2014 22:39:18 -0800 (PST)
In-Reply-To: <CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
 <CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com> <CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 2 Dec 2014 06:39:18 +0000
Message-ID: <CAMAsSdJ6cNkDyMgeGVD+pZj+RMGn8SnOgazrOY_WbM9x31uD=w@mail.gmail.com>
Subject: Re: Required file not found in building
To: Stephen Boesch <javadba@gmail.com>
Cc: Ted Yu <yuzhihong@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm having no problems with the build or zinc on my Mac. I use zinc
from "brew install zinc".

On Tue, Dec 2, 2014 at 3:02 AM, Stephen Boesch <javadba@gmail.com> wrote:
> Mac as well.  Just found the problem:  I had created an alias to zinc a
> couple of months back. Apparently that is not happy with the build anymore.
> No problem now that the issue has been isolated - just need to fix my zinc
> alias.
>
> 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
>
>> I tried the same command on MacBook and didn't experience the same error.
>>
>> Which OS are you using ?
>>
>> Cheers
>>
>> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com> wrote:
>>
>>> It seems there were some additional settings required to build spark now .
>>> This should be a snap for most of you ot there about what I am missing.
>>> Here is the command line I have traditionally used:
>>>
>>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
>>>
>>> That command line is however failing with the lastest from HEAD:
>>>
>>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>>> spark-network-common_2.10 ---
>>> [INFO] Using zinc server for incremental compilation
>>> [INFO] compiler plugin:
>>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>>>
>>> *[error] Required file not found: scala-compiler-2.10.4.jar*
>>>
>>> *[error] See zinc -help for information about locating necessary files*
>>>
>>> [INFO]
>>> ------------------------------------------------------------------------
>>> [INFO] Reactor Summary:
>>> [INFO]
>>> [INFO] Spark Project Parent POM .......................... SUCCESS
>>> [4.077s]
>>> [INFO] Spark Project Networking .......................... FAILURE
>>> [0.445s]
>>>
>>>
>>> OK let's try "zinc -help":
>>>
>>> 18:38:00/spark2 $*zinc -help*
>>> Nailgun server running with 1 cached compiler
>>>
>>> Version = 0.3.5.1
>>>
>>> Zinc compiler cache limit = 5
>>> Resident scalac cache limit = 0
>>> Analysis cache limit = 5
>>>
>>> Compiler(Scala 2.10.4) [74ff364f]
>>> Setup = {
>>> *   scala compiler =
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
>>>    scala library =
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
>>>    scala extra = {
>>>
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
>>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
>>>    }
>>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
>>>    compiler interface sources =
>>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
>>>    java home =
>>>    fork java = false
>>>    cache directory = /Users/steve/.zinc/0.3.5.1
>>> }
>>>
>>> Does that compiler jar exist?  Yes!
>>>
>>> 18:39:34/spark2 $ll
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
>>>
>>> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
>>>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 06:41:48 2014
Return-Path: <dev-return-10614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92A87CA13
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 06:41:48 +0000 (UTC)
Received: (qmail 99312 invoked by uid 500); 2 Dec 2014 06:41:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99237 invoked by uid 500); 2 Dec 2014 06:41:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99226 invoked by uid 99); 2 Dec 2014 06:41:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:41:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:41:21 +0000
Received: by mail-wi0-f175.google.com with SMTP id l15so27040460wiw.2
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 22:39:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=fWEvH7ueE+G1FIRGJU3q09GyPIoA7X2IOXXnVj9UkO0=;
        b=cKTVrnMVk0Gyr9yp6u0X8bIfVzQEyZ6thn+dcn3P431h7zdhPlvfMxPaPohMrQ/A+K
         YJ5mpfPw++o0IwPijW/ZNA21kF53OthyHdBF8zH0GPz2V3ZdYkYV98dRVtIdOxFSeQsY
         aCPf1+nHFkI9b9t1ODfvGhRWL15i95w/9PHtEqbgr4hItCH1t/7UmmmH8t5kur8DuFuf
         vK34tdTPE1CuDly3tw/M5s/e415dqhBGFUK4wtmRkHy4vWVtD3h6UXAP/VnQrhYo29aI
         8ra16eoVHUCY5E7YSCJjSqaTs+H4gMD1kXTWPKb0OwedDMMu5Ac/0azBgjfnV1bT4sA0
         JTzg==
X-Gm-Message-State: ALoCoQlHlVXJGvEAQ2R2NvGe0tgxBvn0CezAkVihz3X8zVf8Df/X1DKZsD67cmiskp706TiKRS2N
X-Received: by 10.180.186.40 with SMTP id fh8mr2572445wic.40.1417502345976;
 Mon, 01 Dec 2014 22:39:05 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Mon, 1 Dec 2014 22:38:45 -0800 (PST)
In-Reply-To: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
References: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 2 Dec 2014 06:38:45 +0000
Message-ID: <CAMAsSdK4_g=E=jCPmzvWuLxQJ2fWR=HKSgoXLodfPyi6OjJ96A@mail.gmail.com>
Subject: Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?
To: Niranda Perera <niranda@wso2.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, they are compiled to classes in JVM bytecode just the same. You
may find the generated code from Scala looks a bit strange and uses
Scala-specific classes, but it's certainly possible to treat them like
other Java classes.

On Tue, Dec 2, 2014 at 5:22 AM, Niranda Perera <niranda@wso2.com> wrote:
> Hi,
>
> Can the Scala classes in the spark source code, be inherited (and other OOP
> concepts) in Java classes?
>
> I want to customize some part of the code, but I would like to do it in a
> Java environment.
>
> Rgds
>
> --
> *Niranda Perera*
> Software Engineer, WSO2 Inc.
> Mobile: +94-71-554-8430
> Twitter: @n1r44 <https://twitter.com/N1R44>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 06:49:34 2014
Return-Path: <dev-return-10615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0A37CA37
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 06:49:33 +0000 (UTC)
Received: (qmail 11704 invoked by uid 500); 2 Dec 2014 06:49:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11612 invoked by uid 500); 2 Dec 2014 06:49:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11601 invoked by uid 99); 2 Dec 2014 06:49:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:49:32 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 06:49:07 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so8691753qge.4
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 22:48:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6ULmXXlboQD4l8fqf3boYbzKgZMkWLJID2Fi2dqP+dA=;
        b=Zv+ZADG/HHrBV8qh4L813jfojunlv4Vy+dIvGLHL86mf4YBlhx1TEnMA5B5qn1rHO0
         X5LXjB9wvlp8KGi1jjkaJhg1mhd7Hsclamg3DlWqtMISMFbQTOVyxVHVddyrbD0Nl0bx
         cVuM+byd+5PXgIZFy1nAMnTaSKUjaVa4qb4C/srbb4PuZQ1IYfqtS/rjTIXQgciraj+a
         8l5o2+wGM8Ah1wl+uvAREOAokkVoG/PHKnzNObg6VmKtCR7dxgZvL44YGCoZr2yz5PDL
         LKLGWJdIzD/NDDcZJNdxA8TWs0VXKR4kt3IsNZK9GgmedmIKibqiv7kjhg0rXb2NaO4G
         G6CA==
X-Gm-Message-State: ALoCoQkY80aQk5cj3g/jB3IXFDbOcl/b9Lv4VpEVGpQoOGXkfMOgQYnElDAV6pKUfORq4/xn2rrc
X-Received: by 10.229.49.69 with SMTP id u5mr91170736qcf.19.1417502900392;
 Mon, 01 Dec 2014 22:48:20 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.47.8 with HTTP; Mon, 1 Dec 2014 22:48:00 -0800 (PST)
In-Reply-To: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
References: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 1 Dec 2014 22:48:00 -0800
Message-ID: <CAPh_B=ZZrBiBOWbhit+T3hGXp3iey_WHVezvqKGYx=SGBE5P_g@mail.gmail.com>
Subject: Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?
To: Niranda Perera <niranda@wso2.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11330eccd8ab130509361cea
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11330eccd8ab130509361cea
Content-Type: text/plain; charset=UTF-8

Oops my previous response wasn't sent properly to the dev list. Here you go
for archiving.


Yes you can. Scala classes are compiled down to classes in bytecode. Take a
look at this: https://twitter.github.io/scala_school/java.html

Note that questions like this are not exactly what this dev list is meant
for  ...

On Mon, Dec 1, 2014 at 9:22 PM, Niranda Perera <niranda@wso2.com> wrote:

> Hi,
>
> Can the Scala classes in the spark source code, be inherited (and other OOP
> concepts) in Java classes?
>
> I want to customize some part of the code, but I would like to do it in a
> Java environment.
>
> Rgds
>
> --
> *Niranda Perera*
> Software Engineer, WSO2 Inc.
> Mobile: +94-71-554-8430
> Twitter: @n1r44 <https://twitter.com/N1R44>
>

--001a11330eccd8ab130509361cea--

From dev-return-10616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 07:09:27 2014
Return-Path: <dev-return-10616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA111CABF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 07:09:26 +0000 (UTC)
Received: (qmail 36274 invoked by uid 500); 2 Dec 2014 07:09:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36203 invoked by uid 500); 2 Dec 2014 07:09:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36191 invoked by uid 99); 2 Dec 2014 07:09:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 07:09:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 07:09:20 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so8727319oib.30
        for <dev@spark.apache.org>; Mon, 01 Dec 2014 23:07:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=7aEQX6FqKDf5uO8StLKOD8K0f9UA2UtnQ37dKGifmRk=;
        b=y9xbEDpMQzbPeG8uFiLT5h16bcsmjdgHN2AAOx/RCtfU8wCn/nTNzxYBFffz2ncFv6
         5SaZRaK+8A/EjZaerEPWb3z7vhU9TY7hLuMI9xCTa5oPIeGrOlKkdjKCwh14hCfePVEX
         a0Qnvi8WlBefWhT0wv0DHtB08MIfXK6cbIw7b83UWGX14Rlmu1kansCDAXYTL+JgOxQQ
         6RBViFkPhAuEQBnS9Kxsi6Kx0upkqexHAM/JvAaJ6f7fJBFYkLBz/gywOZ93LKM0e/hD
         KrtI/R7KzIUYuvtfbRWdCyyT4qCmS8cezrKJ8Var95uJ6Q9iYboymEwfvt58LP0Eyuc+
         YFFQ==
MIME-Version: 1.0
X-Received: by 10.182.33.67 with SMTP id p3mr39706749obi.15.1417504049098;
 Mon, 01 Dec 2014 23:07:29 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Mon, 1 Dec 2014 23:07:28 -0800 (PST)
In-Reply-To: <5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<etPan.547cbef8.3352255a.1313e@joshs-mbp>
	<5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com>
Date: Mon, 1 Dec 2014 23:07:28 -0800
Message-ID: <CABPQxsu28jK5OY9r+jOv=jpnHqetsNn1z=mmQVMkbcSZ4Cx6Dw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Josh Rosen <rosenville@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, harry.brundage@gmail.com
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Just an update. Josh, Andrew, and others are working to reproduce
SPARK-4498 and fix it. Other than that issue no serious regressions
have been reported so far. If we are able to get a fix in for that
soon, we'll likely cut another RC with the patch.

Continued testing of RC1 is definitely appreciated!

I'll leave this vote open to allow folks to continue posting comments.
It's fine to still give "+1" from your own testing... i.e. you can
assume at this point SPARK-4498 will be fixed before releasing.

- Patrick

On Mon, Dec 1, 2014 at 3:30 PM, Matei Zaharia <matei.zaharia@gmail.com> wro=
te:
> +0.9 from me. Tested it on Mac and Windows (someone has to do it) and whi=
le things work, I noticed a few recent scripts don't have Windows equivalen=
ts, namely https://issues.apache.org/jira/browse/SPARK-4683 and https://iss=
ues.apache.org/jira/browse/SPARK-4684. The first one at least would be good=
 to fix if we do another RC. Not blocking the release but useful to fix in =
docs is https://issues.apache.org/jira/browse/SPARK-4685.
>
> Matei
>
>
>> On Dec 1, 2014, at 11:18 AM, Josh Rosen <rosenville@gmail.com> wrote:
>>
>> Hi everyone,
>>
>> There's an open bug report related to Spark standalone which could be a =
potential release-blocker (pending investigation / a bug fix): https://issu=
es.apache.org/jira/browse/SPARK-4498.  This issue seems non-deterministc an=
d only affects long-running Spark standalone deployments, so it may be hard=
 to reproduce.  I'm going to work on a patch to add additional logging in o=
rder to help with debugging.
>>
>> I just wanted to give an early head's up about this issue and to get mor=
e eyes on it in case anyone else has run into it or wants to help with debu=
gging.
>>
>> - Josh
>>
>> On November 28, 2014 at 9:18:09 PM, Patrick Wendell (pwendell@gmail.com)=
 wrote:
>>
>> Please vote on releasing the following candidate as Apache Spark version=
 1.2.0!
>>
>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D105=
6e9ec13203d0c51564265e94d77a054498fdb
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1048/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.2.0!
>>
>> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>> regressions, or bugs related to new features will not block this
>> release.
>>
>> =3D=3D What default changes should I be aware of? =3D=3D
>> 1. The default value of "spark.shuffle.blockTransferService" has been
>> changed to "netty"
>> --> Old behavior can be restored by switching to "nio"
>>
>> 2. The default value of "spark.shuffle.manager" has been changed to "sor=
t".
>> --> Old behavior can be restored by setting "spark.shuffle.manager" to "=
hash".
>>
>> =3D=3D Other notes =3D=3D
>> Because this vote is occurring over a weekend, I will likely extend
>> the vote if this RC survives until the end of the vote period.
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 08:24:12 2014
Return-Path: <dev-return-10617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE15DCD45
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 08:24:12 +0000 (UTC)
Received: (qmail 64239 invoked by uid 500); 2 Dec 2014 08:24:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64170 invoked by uid 500); 2 Dec 2014 08:24:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64153 invoked by uid 99); 2 Dec 2014 08:24:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 08:24:11 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of denny.g.lee@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 08:24:05 +0000
Received: by mail-ie0-f174.google.com with SMTP id rl12so11167313iec.19
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 00:23:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=PJXxNQZ4M88+xCckTtft71itiAmBzrmzk7kuHDzi6kw=;
        b=mioKkN2+G8BsecnZqUN2QFrIpAnj/oGvs5HrZXgpKHdR362uegXgJTX4NftRlqCZI9
         pquedsahuq+2m3qVqBvQk/I5l9fuFQ8hC6waPw/YGAlB9vB0G5OfHehpETOFpKiYB4s6
         f5vm3UrHYB1FnuBDyLrPGMW1rYgIo0EqgmKut/XD1kdRUAsmto+MKItrpCstGAONEZ2K
         eGhf02Wwc8frhM1ZH6lk98OrAGT4Qi2LlSdMiP1LVqbIahthArXsRLpVlMtOeL57LXnM
         e0+/52HxlPn+wcVhozKgNpDZD/Sji1wD0boDEyhRSxckYpr0lFGwmM3uvRCxQ+2dOIiP
         JDpA==
X-Received: by 10.107.15.15 with SMTP id x15mr55137286ioi.61.1417508580113;
 Tue, 02 Dec 2014 00:23:00 -0800 (PST)
MIME-Version: 1.0
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
 <etPan.547cbef8.3352255a.1313e@joshs-mbp> <5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com>
 <CABPQxsu28jK5OY9r+jOv=jpnHqetsNn1z=mmQVMkbcSZ4Cx6Dw@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Tue, 02 Dec 2014 08:22:59 +0000
Message-ID: <CABjYQ3-ai9VPvHH-K5pvP9EWGds3Zte7JAO5rYJqvDOhrQuwtA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
To: Patrick Wendell <pwendell@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>
Cc: Josh Rosen <rosenville@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, harry.brundage@gmail.com
Content-Type: multipart/alternative; boundary=001a113f24f662475b0509376fc4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f24f662475b0509376fc4
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

Verified on OSX 10.10.2, built from source,
spark-shell / spark-submit jobs
ran various simple Spark / Scala queries
ran various SparkSQL queries (including HiveContext)
ran ThriftServer service and connected via beeline
ran SparkSVD


On Mon Dec 01 2014 at 11:09:26 PM Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey All,
>
> Just an update. Josh, Andrew, and others are working to reproduce
> SPARK-4498 and fix it. Other than that issue no serious regressions
> have been reported so far. If we are able to get a fix in for that
> soon, we'll likely cut another RC with the patch.
>
> Continued testing of RC1 is definitely appreciated!
>
> I'll leave this vote open to allow folks to continue posting comments.
> It's fine to still give "+1" from your own testing... i.e. you can
> assume at this point SPARK-4498 will be fixed before releasing.
>
> - Patrick
>
> On Mon, Dec 1, 2014 at 3:30 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > +0.9 from me. Tested it on Mac and Windows (someone has to do it) and
> while things work, I noticed a few recent scripts don't have Windows
> equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683 and
> https://issues.apache.org/jira/browse/SPARK-4684. The first one at least
> would be good to fix if we do another RC. Not blocking the release but
> useful to fix in docs is https://issues.apache.org/jira/browse/SPARK-4685.
> >
> > Matei
> >
> >
> >> On Dec 1, 2014, at 11:18 AM, Josh Rosen <rosenville@gmail.com> wrote:
> >>
> >> Hi everyone,
> >>
> >> There's an open bug report related to Spark standalone which could be a
> potential release-blocker (pending investigation / a bug fix):
> https://issues.apache.org/jira/browse/SPARK-4498.  This issue seems
> non-deterministc and only affects long-running Spark standalone
> deployments, so it may be hard to reproduce.  I'm going to work on a patch
> to add additional logging in order to help with debugging.
> >>
> >> I just wanted to give an early head's up about this issue and to get
> more eyes on it in case anyone else has run into it or wants to help with
> debugging.
> >>
> >> - Josh
> >>
> >> On November 28, 2014 at 9:18:09 PM, Patrick Wendell (pwendell@gmail.com)
> wrote:
> >>
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.2.0!
> >>
> >> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
> >> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
> 1056e9ec13203d0c51564265e94d77a054498fdb
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1048/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.2.0!
> >>
> >> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> >> if a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.1.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening very late into the QA period compared with
> >> previous votes, so -1 votes should only occur for significant
> >> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> >> regressions, or bugs related to new features will not block this
> >> release.
> >>
> >> == What default changes should I be aware of? ==
> >> 1. The default value of "spark.shuffle.blockTransferService" has been
> >> changed to "netty"
> >> --> Old behavior can be restored by switching to "nio"
> >>
> >> 2. The default value of "spark.shuffle.manager" has been changed to
> "sort".
> >> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> "hash".
> >>
> >> == Other notes ==
> >> Because this vote is occurring over a weekend, I will likely extend
> >> the vote if this RC survives until the end of the vote period.
> >>
> >> - Patrick
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113f24f662475b0509376fc4--

From dev-return-10618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 08:49:36 2014
Return-Path: <dev-return-10618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F241FCDF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 08:49:36 +0000 (UTC)
Received: (qmail 16471 invoked by uid 500); 2 Dec 2014 08:49:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16391 invoked by uid 500); 2 Dec 2014 08:49:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16378 invoked by uid 99); 2 Dec 2014 08:49:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 08:49:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda@wso2.com designates 74.125.82.53 as permitted sender)
Received: from [74.125.82.53] (HELO mail-wg0-f53.google.com) (74.125.82.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 08:49:31 +0000
Received: by mail-wg0-f53.google.com with SMTP id l18so16109989wgh.26
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 00:49:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=wso2.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=w20CgsIz2yjdx1Gu7OWODI/hQ2PXojpwHQpwr3ofTm8=;
        b=AziuygXkuskN7JzcEdRk4nhPXqEN+hdvnyaumv4zABNexnBUTtSzBStfqhmIUXtjPf
         UpQ92hW75/mBF5LAYhAipGSwRYIxyQuMlep65jSHOEkUjU/pgyHz8/gGeYOKlK2wqUil
         W8vdaLhCUC2V9R/C/91/iY9j/hzJT9m5V0qv4=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=w20CgsIz2yjdx1Gu7OWODI/hQ2PXojpwHQpwr3ofTm8=;
        b=ApD75cY54nuShOWh4LQ1g4ASwUFFB0gFvYQ822atBaeGJZ41es/KRJMgBNsnq6GFws
         CnlIYpNjPJhV+RMbS4bp8ymz/xbMS9pMVzFEE15CdiMGH9I1QeMqq+Fv/LOSz/uDnpO+
         BHItn1jA1CM1avEH4cP1AXFQjTQjq2IvDUbA9iS9EQfydTL3lqQOrElMOxRXfLblcDIZ
         FoTjL52quO9Dg+xBfw7nbz3V1BLNvRZ1Q/BcBMSAdj0+tFZVRUgzTRRPsonatNzBn6Hj
         iaB+vy9DoAx27GPzTLQ6r+hKq0sBSv5E3xCC7xiNmmYskCX517Ylo2A/MwsPwQQJd0L1
         EJWA==
X-Gm-Message-State: ALoCoQneKtBnpVqSFsLQ2+XgLG+uPc/NQAMXbPHd0HEUdtez8GliyZiX/Fslpg5f+WKFtb42rUXX
X-Received: by 10.180.11.140 with SMTP id q12mr3267768wib.45.1417510149909;
 Tue, 02 Dec 2014 00:49:09 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.174.66 with HTTP; Tue, 2 Dec 2014 00:48:49 -0800 (PST)
In-Reply-To: <CAPh_B=ZZrBiBOWbhit+T3hGXp3iey_WHVezvqKGYx=SGBE5P_g@mail.gmail.com>
References: <CADz3zK2peXD=-fB9jTQ7Qer+rdp4DUn_5sUc_5DJNwyjjCyjEg@mail.gmail.com>
 <CAPh_B=ZZrBiBOWbhit+T3hGXp3iey_WHVezvqKGYx=SGBE5P_g@mail.gmail.com>
From: Niranda Perera <niranda@wso2.com>
Date: Tue, 2 Dec 2014 14:18:49 +0530
Message-ID: <CADz3zK0Bxq7kdELoq2GcyQBQkdMDtP1+MkOAVNN=mZPoh4pCzA@mail.gmail.com>
Subject: Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2aeeaf384f8050937ccd0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2aeeaf384f8050937ccd0
Content-Type: text/plain; charset=UTF-8

Thanks.

And @Reynold, sorry my bad, Guess I should have used something like
Stackoverflow!

On Tue, Dec 2, 2014 at 12:18 PM, Reynold Xin <rxin@databricks.com> wrote:

> Oops my previous response wasn't sent properly to the dev list. Here you
> go for archiving.
>
>
> Yes you can. Scala classes are compiled down to classes in bytecode. Take
> a look at this: https://twitter.github.io/scala_school/java.html
>
> Note that questions like this are not exactly what this dev list is meant
> for  ...
>
> On Mon, Dec 1, 2014 at 9:22 PM, Niranda Perera <niranda@wso2.com> wrote:
>
>> Hi,
>>
>> Can the Scala classes in the spark source code, be inherited (and other
>> OOP
>> concepts) in Java classes?
>>
>> I want to customize some part of the code, but I would like to do it in a
>> Java environment.
>>
>> Rgds
>>
>> --
>> *Niranda Perera*
>> Software Engineer, WSO2 Inc.
>> Mobile: +94-71-554-8430
>> Twitter: @n1r44 <https://twitter.com/N1R44>
>>
>
>


-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>

--001a11c2aeeaf384f8050937ccd0--

From dev-return-10619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 16:29:05 2014
Return-Path: <dev-return-10619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A4A410204
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 16:29:05 +0000 (UTC)
Received: (qmail 51722 invoked by uid 500); 2 Dec 2014 16:29:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51651 invoked by uid 500); 2 Dec 2014 16:29:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51639 invoked by uid 99); 2 Dec 2014 16:29:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 16:29:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.220.170 as permitted sender)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 16:28:37 +0000
Received: by mail-vc0-f170.google.com with SMTP id hy4so5981820vcb.1
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 08:28:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=HIfVa5HZZ2sojY7rk9hvu4xX8cptPVJvt7ViFYmVjsc=;
        b=BWQs5PFvc2NfK1Yo2NWSn+FBAbRPBA6zdBTnm/yMGQcYSRS+efsn+kz/Hyaj8ujrmq
         3C2GmKq8nDX+6hHgIenOwaiECX1l8ucdCsAdABziu1pkwq6xsNG17Ibf6glrBPu2EO+v
         XlcHHFDNxLUfzW5C7fPBsnXOUmVB+qVc1xjEp+YykQ6MexItiQZyQr5ACl5XMAvth50R
         dPrGxgMZE0zKZ4rKxP86eihVDaTSYXiPxHwVn1eBHUB4v5CqIoZhX4bkXaSxXZQeqymH
         M/ZsobUgfzH4Yho8kYuVec/aBEeSR5nhwKZuMrxWTCW0hKQqV3ER9UKt2E+RwFJUTqrb
         oJOA==
MIME-Version: 1.0
X-Received: by 10.220.82.199 with SMTP id c7mr35903vcl.61.1417537716602; Tue,
 02 Dec 2014 08:28:36 -0800 (PST)
Received: by 10.31.166.137 with HTTP; Tue, 2 Dec 2014 08:28:36 -0800 (PST)
In-Reply-To: <CAMAsSdJ6cNkDyMgeGVD+pZj+RMGn8SnOgazrOY_WbM9x31uD=w@mail.gmail.com>
References: <CACkSZy0v47Ez2O57dv1UcAQgdRok1_9C49U+9ocb_90ncJ_eLQ@mail.gmail.com>
	<CALte62x2QWDiOjS8Ab+648U-J3Bhxtvs73B1ZHGAfzM4wU5Luw@mail.gmail.com>
	<CACkSZy3Vd5NjJjnOyfbVe-Y19Np6+MH8OU0NvwuT8jZdZ8E_Pg@mail.gmail.com>
	<CAMAsSdJ6cNkDyMgeGVD+pZj+RMGn8SnOgazrOY_WbM9x31uD=w@mail.gmail.com>
Date: Tue, 2 Dec 2014 08:28:36 -0800
Message-ID: <CACkSZy0_aVAvOhAOAp7AL-rP0q_FW=5_4+oSFs=WSJ3Qp3A_Dg@mail.gmail.com>
Subject: Re: Required file not found in building
From: Stephen Boesch <javadba@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Ted Yu <yuzhihong@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e016340380dd0c705093e3829
X-Virus-Checked: Checked by ClamAV on apache.org

--089e016340380dd0c705093e3829
Content-Type: text/plain; charset=UTF-8

Thanks Sean, I followed suit (brew install zinc) and that is working.

2014-12-01 22:39 GMT-08:00 Sean Owen <sowen@cloudera.com>:

> I'm having no problems with the build or zinc on my Mac. I use zinc
> from "brew install zinc".
>
> On Tue, Dec 2, 2014 at 3:02 AM, Stephen Boesch <javadba@gmail.com> wrote:
> > Mac as well.  Just found the problem:  I had created an alias to zinc a
> > couple of months back. Apparently that is not happy with the build
> anymore.
> > No problem now that the issue has been isolated - just need to fix my
> zinc
> > alias.
> >
> > 2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:
> >
> >> I tried the same command on MacBook and didn't experience the same
> error.
> >>
> >> Which OS are you using ?
> >>
> >> Cheers
> >>
> >> On Mon, Dec 1, 2014 at 6:42 PM, Stephen Boesch <javadba@gmail.com>
> wrote:
> >>
> >>> It seems there were some additional settings required to build spark
> now .
> >>> This should be a snap for most of you ot there about what I am missing.
> >>> Here is the command line I have traditionally used:
> >>>
> >>>    mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests
> >>>
> >>> That command line is however failing with the lastest from HEAD:
> >>>
> >>> INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
> >>> spark-network-common_2.10 ---
> >>> [INFO] Using zinc server for incremental compilation
> >>> [INFO] compiler plugin:
> >>> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
> >>>
> >>> *[error] Required file not found: scala-compiler-2.10.4.jar*
> >>>
> >>> *[error] See zinc -help for information about locating necessary files*
> >>>
> >>> [INFO]
> >>>
> ------------------------------------------------------------------------
> >>> [INFO] Reactor Summary:
> >>> [INFO]
> >>> [INFO] Spark Project Parent POM .......................... SUCCESS
> >>> [4.077s]
> >>> [INFO] Spark Project Networking .......................... FAILURE
> >>> [0.445s]
> >>>
> >>>
> >>> OK let's try "zinc -help":
> >>>
> >>> 18:38:00/spark2 $*zinc -help*
> >>> Nailgun server running with 1 cached compiler
> >>>
> >>> Version = 0.3.5.1
> >>>
> >>> Zinc compiler cache limit = 5
> >>> Resident scalac cache limit = 0
> >>> Analysis cache limit = 5
> >>>
> >>> Compiler(Scala 2.10.4) [74ff364f]
> >>> Setup = {
> >>> *   scala compiler =
> >>>
> >>>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
> >>>    scala library =
> >>>
> >>>
> /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
> >>>    scala extra = {
> >>>
> >>>
> >>>
> /Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
> >>>       /shared/zinc-0.3.5.1/lib/scala-reflect.jar
> >>>    }
> >>>    sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
> >>>    compiler interface sources =
> >>> /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
> >>>    java home =
> >>>    fork java = false
> >>>    cache directory = /Users/steve/.zinc/0.3.5.1
> >>> }
> >>>
> >>> Does that compiler jar exist?  Yes!
> >>>
> >>> 18:39:34/spark2 $ll
> >>>
> >>>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
> >>> -rw-r--r--  1 steve  staff  14445780 Apr  9  2014
> >>>
> >>>
> /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
> >>>
> >>
> >>
>

--089e016340380dd0c705093e3829--

From dev-return-10620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 17:33:01 2014
Return-Path: <dev-return-10620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 393A8106D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 17:33:01 +0000 (UTC)
Received: (qmail 47927 invoked by uid 500); 2 Dec 2014 17:33:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47853 invoked by uid 500); 2 Dec 2014 17:32:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47839 invoked by uid 99); 2 Dec 2014 17:32:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 17:32:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yana.kadiyska@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 17:32:33 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so17606238wgh.11
        for <dev@spark.incubator.apache.org>; Tue, 02 Dec 2014 09:31:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:date:message-id:subject:from:to:content-type;
        bh=15B3erQUUol5eyJR66wMQV2UK/5cPhLoDvLaUlF+1sA=;
        b=Wj/PMzjsq0w0IMX4wCPAe+MtqzYWvKtA4koqu9qVZtlvdKHfeoqq8kr1y10MhuzV3S
         HyR2FEo9amEwbI0EnJAEICLRGuRrMgeQn6XY/mSH1cXoQKQeC+/04p+VxyHBcAkOOJ19
         b884rt9tFFF4yIkegpDJl9JQTTVYdeWekJUrcuI5B8CvIoIeCbvnE1/vekbuNTSoQeiB
         aoU7jN6SPUlSSuty0hWE/TYVOhU1X7Ni0NCoEwc4XOIkrmDZIDsfNjqarpSaEUYmqYk1
         Twn2sJsBAZuymDFi5Zzo/S9XS93UAwuqPyCOKE6WEd3w/b3+JO7AFJ1FNbr51YBu12VD
         WA0A==
MIME-Version: 1.0
X-Received: by 10.180.99.1 with SMTP id em1mr7021786wib.29.1417541505783; Tue,
 02 Dec 2014 09:31:45 -0800 (PST)
Received: by 10.216.212.199 with HTTP; Tue, 2 Dec 2014 09:31:45 -0800 (PST)
Reply-To: yana.kadiyska@gmail.com
Date: Tue, 2 Dec 2014 12:31:45 -0500
Message-ID: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
Subject: Fwd: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe
From: Yana Kadiyska <yana.kadiyska@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=f46d0418258ae81e9505093f19c8
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0418258ae81e9505093f19c8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Apologies if people get this more than once -- I sent mail to dev@spark
last night and don't see it in the archives. Trying the incubator list
now...wanted to make sure it doesn't get lost in case it's a bug...

---------- Forwarded message ----------
From: Yana Kadiyska <yana.kadiyska@gmail.com>
Date: Mon, Dec 1, 2014 at 8:10 PM
Subject: [Thrift,1.2 RC] what happened to
parquet.hive.serde.ParquetHiveSerDe
To: dev@spark.apache.org


Hi all, apologies if this is not a question for the dev list -- figured
User list might not be appropriate since I'm having trouble with the RC tag=
.

I just tried deploying the RC and running ThriftServer. I see the following
error:

14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionException
as:anonymous (auth:SIMPLE)
cause:org.apache.hive.service.cli.HiveSQLException:
java.lang.RuntimeException:
MetaException(message:java.lang.ClassNotFoundException Class
parquet.hive.serde.ParquetHiveSerDe not found)
14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException:
MetaException(message:java.lang.ClassNotFoundException Class
parquet.hive.serde.ParquetHiveSerDe not found)
at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(S=
him13.scala:192)
at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInterna=
l(HiveSessionImpl.java:231)
at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSe=
ssionImpl.java:212)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:5=
7)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImp=
l.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProx=
y.java:79)
at
org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSession=
Proxy.java:37)
at
org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy=
.java:64)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
=E2=80=8B


I looked at a working installation that I have(build master a few weeks
ago) and this class used to be included in spark-assembly:

ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
matches

but with the RC build it's not there?

I tried both the prebuilt CDH drop and later manually built the tag with
the following command:

 ./make-distribution.sh --tgz -Phive -Dhadoop.version=3D2.0.0-mr1-cdh4.2.0
-Phive-thriftserver
$JAVA_HOME/bin/jar -tvf spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.jar
|grep parquet.hive.serde.ParquetHiveSerDe

comes back empty...

--f46d0418258ae81e9505093f19c8--

From dev-return-10621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 18:17:01 2014
Return-Path: <dev-return-10621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED74710986
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 18:17:00 +0000 (UTC)
Received: (qmail 88752 invoked by uid 500); 2 Dec 2014 18:16:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88678 invoked by uid 500); 2 Dec 2014 18:16:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88666 invoked by uid 99); 2 Dec 2014 18:16:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 18:16:58 +0000
X-ASF-Spam-Status: No, hits=3.6 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,TRACKER_ID
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 18:16:52 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so9714157qge.18
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 10:14:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=aKIXbXKVum3MFWbL2cgSVQpgzxla0EanTaR4JIq2o74=;
        b=0fQ+3vpgKI8BkCeUPoxpD9fqVCkGlPPjHhgDOgI5pJUV+xf2XHrbePgAndLVdPjKQP
         V7HKLY8bYs480rGsFKOOB9mp1/lHju9jJcu1JrGjxssE5xU9Gj4qavplEfyUbsBF/JcV
         Ob9pBtvlo+0LmBYYUQ50hJdKNTFmhTp1bDkDqMghSosvIt9b7VRylyjg7ihjmywYH73I
         ACiNU9coLL75T8LTfISm+i6MUscJzeLEUOD6pWBQWQUgYbkkjfVe1xb4AodCgR57IYG9
         iC/k7WI6p+6MyZCRHaxx44hSWYSGLO7Y4ziwzw6GX1gtpoROPTjKGLyzTdXrwog/xqmr
         h58w==
X-Received: by 10.224.23.71 with SMTP id q7mr889089qab.22.1417544056982;
        Tue, 02 Dec 2014 10:14:16 -0800 (PST)
Received: from [10.102.20.112] (simcoe.janelia.org. [206.241.0.254])
        by mx.google.com with ESMTPSA id f65sm21018745qga.9.2014.12.02.10.14.15
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 02 Dec 2014 10:14:16 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_FD67F4C9-E3C5-47B8-A185-D63833EAF6E5"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <CABjYQ3-ai9VPvHH-K5pvP9EWGds3Zte7JAO5rYJqvDOhrQuwtA@mail.gmail.com>
Date: Tue, 2 Dec 2014 13:16:02 -0500
Cc: Patrick Wendell <pwendell@gmail.com>,
 Matei Zaharia <matei.zaharia@gmail.com>,
 Josh Rosen <rosenville@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>,
 harry.brundage@gmail.com
Message-Id: <F47D30E7-3439-4E63-BCB0-74041FBE1EF1@gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com> <etPan.547cbef8.3352255a.1313e@joshs-mbp> <5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com> <CABPQxsu28jK5OY9r+jOv=jpnHqetsNn1z=mmQVMkbcSZ4Cx6Dw@mail.gmail.com> <CABjYQ3-ai9VPvHH-K5pvP9EWGds3Zte7JAO5rYJqvDOhrQuwtA@mail.gmail.com>
To: Denny Lee <denny.g.lee@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_FD67F4C9-E3C5-47B8-A185-D63833EAF6E5
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

+1 (non-binding)

Installed version pre-built for Hadoop on a private HPC
ran PySpark shell w/ iPython
loaded data using custom Hadoop input formats
ran MLlib routines in PySpark
ran custom workflows in PySpark
browsed the web UI

Noticeable improvements in stability and performance during large =
shuffles (as well as the elimination of frequent but unpredictable =
=93FileNotFound / too many open files=94 errors).

We initially hit errors during large collects that ran fine in 1.1, but =
setting the new spark.driver.maxResultSize to 0 preserved the old =
behavior. Definitely worth highlighting this setting in the release =
notes, as the new default may be too small for some users and workloads.

=97 Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab

On Dec 2, 2014, at 3:22 AM, Denny Lee <denny.g.lee@gmail.com> wrote:

> +1 (non-binding)
>=20
> Verified on OSX 10.10.2, built from source,
> spark-shell / spark-submit jobs
> ran various simple Spark / Scala queries
> ran various SparkSQL queries (including HiveContext)
> ran ThriftServer service and connected via beeline
> ran SparkSVD
>=20
>=20
> On Mon Dec 01 2014 at 11:09:26 PM Patrick Wendell <pwendell@gmail.com>
> wrote:
>=20
>> Hey All,
>>=20
>> Just an update. Josh, Andrew, and others are working to reproduce
>> SPARK-4498 and fix it. Other than that issue no serious regressions
>> have been reported so far. If we are able to get a fix in for that
>> soon, we'll likely cut another RC with the patch.
>>=20
>> Continued testing of RC1 is definitely appreciated!
>>=20
>> I'll leave this vote open to allow folks to continue posting =
comments.
>> It's fine to still give "+1" from your own testing... i.e. you can
>> assume at this point SPARK-4498 will be fixed before releasing.
>>=20
>> - Patrick
>>=20
>> On Mon, Dec 1, 2014 at 3:30 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
>> wrote:
>>> +0.9 from me. Tested it on Mac and Windows (someone has to do it) =
and
>> while things work, I noticed a few recent scripts don't have Windows
>> equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683 =
and
>> https://issues.apache.org/jira/browse/SPARK-4684. The first one at =
least
>> would be good to fix if we do another RC. Not blocking the release =
but
>> useful to fix in docs is =
https://issues.apache.org/jira/browse/SPARK-4685.
>>>=20
>>> Matei
>>>=20
>>>=20
>>>> On Dec 1, 2014, at 11:18 AM, Josh Rosen <rosenville@gmail.com> =
wrote:
>>>>=20
>>>> Hi everyone,
>>>>=20
>>>> There's an open bug report related to Spark standalone which could =
be a
>> potential release-blocker (pending investigation / a bug fix):
>> https://issues.apache.org/jira/browse/SPARK-4498.  This issue seems
>> non-deterministc and only affects long-running Spark standalone
>> deployments, so it may be hard to reproduce.  I'm going to work on a =
patch
>> to add additional logging in order to help with debugging.
>>>>=20
>>>> I just wanted to give an early head's up about this issue and to =
get
>> more eyes on it in case anyone else has run into it or wants to help =
with
>> debugging.
>>>>=20
>>>> - Josh
>>>>=20
>>>> On November 28, 2014 at 9:18:09 PM, Patrick Wendell =
(pwendell@gmail.com)
>> wrote:
>>>>=20
>>>> Please vote on releasing the following candidate as Apache Spark
>> version 1.2.0!
>>>>=20
>>>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D=

>> 1056e9ec13203d0c51564265e94d77a054498fdb
>>>>=20
>>>> The release files, including signatures, digests, etc. can be found =
at:
>>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>>>>=20
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>=20
>>>> The staging repository for this release can be found at:
>>>> =
https://repository.apache.org/content/repositories/orgapachespark-1048/
>>>>=20
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>>>>=20
>>>> Please vote on releasing this package as Apache Spark 1.2.0!
>>>>=20
>>>> The vote is open until Tuesday, December 02, at 05:15 UTC and =
passes
>>>> if a majority of at least 3 +1 PMC votes are cast.
>>>>=20
>>>> [ ] +1 Release this package as Apache Spark 1.1.0
>>>> [ ] -1 Do not release this package because ...
>>>>=20
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>>=20
>>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>>>> This vote is happening very late into the QA period compared with
>>>> previous votes, so -1 votes should only occur for significant
>>>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>>>> regressions, or bugs related to new features will not block this
>>>> release.
>>>>=20
>>>> =3D=3D What default changes should I be aware of? =3D=3D
>>>> 1. The default value of "spark.shuffle.blockTransferService" has =
been
>>>> changed to "netty"
>>>> --> Old behavior can be restored by switching to "nio"
>>>>=20
>>>> 2. The default value of "spark.shuffle.manager" has been changed to
>> "sort".
>>>> --> Old behavior can be restored by setting "spark.shuffle.manager" =
to
>> "hash".
>>>>=20
>>>> =3D=3D Other notes =3D=3D
>>>> Because this vote is occurring over a weekend, I will likely extend
>>>> the vote if this RC survives until the end of the vote period.
>>>>=20
>>>> - Patrick
>>>>=20
>>>> =
---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>=20
>>>=20
>>>=20
>>> =
---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20


--Apple-Mail=_FD67F4C9-E3C5-47B8-A185-D63833EAF6E5--

From dev-return-10622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 20:25:54 2014
Return-Path: <dev-return-10622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47294100C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 20:25:54 +0000 (UTC)
Received: (qmail 71945 invoked by uid 500); 2 Dec 2014 20:25:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71892 invoked by uid 500); 2 Dec 2014 20:25:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71878 invoked by uid 99); 2 Dec 2014 20:25:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:25:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kayousterhout@gmail.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:25:25 +0000
Received: by mail-yh0-f41.google.com with SMTP id a41so6448244yho.0
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 12:23:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=WibL9ui4wbboDrUe5efviKfMiv7LX5vmSQkF2+rfDd4=;
        b=aanmuu3hHnNnWgYmh4kWU6kWKtARNwSDdYlILWLm9OFmuqzYSNPYxF6pcTGl75QvSK
         2Fkn8E5wiTAcdzw9hsUukBiquZlI6S1zpteiN0xQXtc5d06DsdQnrYzcrIkJmuOPI4QC
         HdTAT30sqcTwLOdQNQuGQ8U0vf+qon36L7wZ+JKlcSvc8/Cy1tTFwDbQzOEpEyfQlsxf
         nTim04pC3/Rec2BuX7Ktcyxi9T03ir3AxirZ/y3nEYLVC7aWvwDFgAJ5ognywjKDUllP
         v9itVtKvqdKjFvlHBRYjsyz1eZJZAGJpbU4uD00AGx7SOUqx6QbsR/HG+nXEeTKAQY6O
         c41g==
MIME-Version: 1.0
X-Received: by 10.170.80.138 with SMTP id w132mr2018016ykw.118.1417551789178;
 Tue, 02 Dec 2014 12:23:09 -0800 (PST)
Received: by 10.170.174.6 with HTTP; Tue, 2 Dec 2014 12:23:09 -0800 (PST)
Date: Tue, 2 Dec 2014 12:23:09 -0800
Message-ID: <CAKJXNjHdX0Rqpgn7XJ_6GF2Chz4yE=oh_RY-UqbsWDeTuky6=Q@mail.gmail.com>
Subject: keeping PR titles / descriptions up to date
From: Kay Ousterhout <kayousterhout@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a32fcd845f60509417ea1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a32fcd845f60509417ea1
Content-Type: text/plain; charset=UTF-8

Hi all,

I've noticed a bunch of times lately where a pull request changes to be
pretty different from the original pull request, and the title /
description never get updated.  Because the pull request title and
description are used as the commit message, the incorrect description lives
on forever, making it harder to understand the reason behind a particular
commit without going back and reading the entire conversation on the pull
request.  If folks could try to keep these up to date (and committers, try
to remember to verify that the title and description are correct before
making merging pull requests), that would be awesome.

-Kay

--001a113a32fcd845f60509417ea1--

From dev-return-10623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 20:50:58 2014
Return-Path: <dev-return-10623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABA0E101E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 20:50:58 +0000 (UTC)
Received: (qmail 37311 invoked by uid 500); 2 Dec 2014 20:50:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37240 invoked by uid 500); 2 Dec 2014 20:50:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37228 invoked by uid 99); 2 Dec 2014 20:50:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:50:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:50:25 +0000
Received: by mail-qc0-f176.google.com with SMTP id i17so9988931qcy.7
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 12:50:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=So72uI9m86/u4Xh/ChDVWzZWY1dL7Vn1Ds65P7kJzsw=;
        b=w+D42yasNbu/9aRoiHamCjlWzQY4mayqE10uXhCaJ+JZilEvIutt7t5htukMOswMcy
         A45+HcGPv13jwJ02lK9bqOgMA2371qZZfK7x6pAQk9JmCtUIbZGOUs9WJWqjIdIRg0lM
         fOn543hFoqGaPKq788RkxZ+pCVZbn0DcEVxh36GZCP0bVDPMo7CwmX8Au7OyF4IreGXr
         kfZn2lYzmleFVAIwH6TD0eroqbzZjNT+2/dlnCdNkt0VE/OC4DN5oQL0I7mDeweySRKD
         0SxsMoJVpidF3glHq6hDqIsYcngwVHrirb2QHZULpixcZVA4P6OGPvkFtCB5WFtR7uyj
         Q77g==
MIME-Version: 1.0
X-Received: by 10.224.65.134 with SMTP id j6mr1793902qai.90.1417553424929;
 Tue, 02 Dec 2014 12:50:24 -0800 (PST)
Received: by 10.140.98.101 with HTTP; Tue, 2 Dec 2014 12:50:24 -0800 (PST)
In-Reply-To: <CAKJXNjHdX0Rqpgn7XJ_6GF2Chz4yE=oh_RY-UqbsWDeTuky6=Q@mail.gmail.com>
References: <CAKJXNjHdX0Rqpgn7XJ_6GF2Chz4yE=oh_RY-UqbsWDeTuky6=Q@mail.gmail.com>
Date: Wed, 3 Dec 2014 02:20:24 +0530
Message-ID: <CAJiQeYLEYppjbvUsF_ww1Ezeye=qU1C6nPa_yDueiQzXE_haOw@mail.gmail.com>
Subject: Re: keeping PR titles / descriptions up to date
From: Mridul Muralidharan <mridul@gmail.com>
To: Kay Ousterhout <kayousterhout@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I second that !
Would also be great if the JIRA was updated accordingly too.

Regards,
Mridul


On Wed, Dec 3, 2014 at 1:53 AM, Kay Ousterhout <kayousterhout@gmail.com> wrote:
> Hi all,
>
> I've noticed a bunch of times lately where a pull request changes to be
> pretty different from the original pull request, and the title /
> description never get updated.  Because the pull request title and
> description are used as the commit message, the incorrect description lives
> on forever, making it harder to understand the reason behind a particular
> commit without going back and reading the entire conversation on the pull
> request.  If folks could try to keep these up to date (and committers, try
> to remember to verify that the title and description are correct before
> making merging pull requests), that would be awesome.
>
> -Kay

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 20:59:40 2014
Return-Path: <dev-return-10624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C38510233
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 20:59:40 +0000 (UTC)
Received: (qmail 66293 invoked by uid 500); 2 Dec 2014 20:59:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66211 invoked by uid 500); 2 Dec 2014 20:59:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66200 invoked by uid 99); 2 Dec 2014 20:59:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:59:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.175] (HELO mail-lb0-f175.google.com) (209.85.217.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 20:59:34 +0000
Received: by mail-lb0-f175.google.com with SMTP id u10so10986741lbd.20
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 12:58:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wq69wFdtmaxN02MUdcY5SPjqZrJVaylXaIQHIM2C/KI=;
        b=gbOFbH9RfGIsGtOeQn3JsNip40gCmi+OT3O40nB6h2wInmeIvAq4c6QXOu3mgZmqTR
         Bh1j7g6bwnLyGWKsr4jJaudSYq4uNL4eks4L+rOJllaKXON8PDeLn0Kz+5lX1lSdDGSu
         S4Mb0x9QjXn+hhSJrtOjCBbdMHwNi1U/oExuGM65CIVPogLoTMzwLq1/U4nayJfZYWhk
         CiSbY1NFu7AYt6xfSdu45yX5KM+rFRmskZ58C4xa+y2TBNkauOzalQcvkFz/urK2b2fu
         azrEb0FPxsWY2PJPJXAY1dG4bb4zwFCV7KesD8jxWwyRA2K/M3tQNEgui7CaXktbYEFo
         jsMQ==
X-Gm-Message-State: ALoCoQmOC/9vkzB211Mj9WIg3epHqy4+tPcrMLxYLhE8+7CBg+CVNvZHNMNOC1fvJVF+OMaNfnr5
MIME-Version: 1.0
X-Received: by 10.112.219.227 with SMTP id pr3mr1041895lbc.63.1417553907561;
 Tue, 02 Dec 2014 12:58:27 -0800 (PST)
Received: by 10.112.112.66 with HTTP; Tue, 2 Dec 2014 12:58:27 -0800 (PST)
In-Reply-To: <F47D30E7-3439-4E63-BCB0-74041FBE1EF1@gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<etPan.547cbef8.3352255a.1313e@joshs-mbp>
	<5FE3F2F9-FD5C-48CC-A76C-A99B45661A8C@gmail.com>
	<CABPQxsu28jK5OY9r+jOv=jpnHqetsNn1z=mmQVMkbcSZ4Cx6Dw@mail.gmail.com>
	<CABjYQ3-ai9VPvHH-K5pvP9EWGds3Zte7JAO5rYJqvDOhrQuwtA@mail.gmail.com>
	<F47D30E7-3439-4E63-BCB0-74041FBE1EF1@gmail.com>
Date: Tue, 2 Dec 2014 12:58:27 -0800
Message-ID: <CAMJOb8=5b2GG0hOgV2BYgd45qs=O5P2Q_YwKQdq-fgvM6gK-cg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Andrew Or <andrew@databricks.com>
To: Jeremy Freeman <freeman.jeremy@gmail.com>
Cc: Denny Lee <denny.g.lee@gmail.com>, Patrick Wendell <pwendell@gmail.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>, Josh Rosen <rosenville@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, harry.brundage@gmail.com
Content-Type: multipart/alternative; boundary=001a11c334121c5922050941fd97
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c334121c5922050941fd97
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1. I also tested on Windows just in case, with jars referring other jars
and python files referring other python files. Path resolution still works.

2014-12-02 10:16 GMT-08:00 Jeremy Freeman <freeman.jeremy@gmail.com>:

> +1 (non-binding)
>
> Installed version pre-built for Hadoop on a private HPC
> ran PySpark shell w/ iPython
> loaded data using custom Hadoop input formats
> ran MLlib routines in PySpark
> ran custom workflows in PySpark
> browsed the web UI
>
> Noticeable improvements in stability and performance during large shuffle=
s
> (as well as the elimination of frequent but unpredictable =E2=80=9CFileNo=
tFound /
> too many open files=E2=80=9D errors).
>
> We initially hit errors during large collects that ran fine in 1.1, but
> setting the new spark.driver.maxResultSize to 0 preserved the old behavio=
r.
> Definitely worth highlighting this setting in the release notes, as the n=
ew
> default may be too small for some users and workloads.
>
> =E2=80=94 Jeremy
>
> -------------------------
> jeremyfreeman.net
> @thefreemanlab
>
> On Dec 2, 2014, at 3:22 AM, Denny Lee <denny.g.lee@gmail.com> wrote:
>
> > +1 (non-binding)
> >
> > Verified on OSX 10.10.2, built from source,
> > spark-shell / spark-submit jobs
> > ran various simple Spark / Scala queries
> > ran various SparkSQL queries (including HiveContext)
> > ran ThriftServer service and connected via beeline
> > ran SparkSVD
> >
> >
> > On Mon Dec 01 2014 at 11:09:26 PM Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> >> Hey All,
> >>
> >> Just an update. Josh, Andrew, and others are working to reproduce
> >> SPARK-4498 and fix it. Other than that issue no serious regressions
> >> have been reported so far. If we are able to get a fix in for that
> >> soon, we'll likely cut another RC with the patch.
> >>
> >> Continued testing of RC1 is definitely appreciated!
> >>
> >> I'll leave this vote open to allow folks to continue posting comments.
> >> It's fine to still give "+1" from your own testing... i.e. you can
> >> assume at this point SPARK-4498 will be fixed before releasing.
> >>
> >> - Patrick
> >>
> >> On Mon, Dec 1, 2014 at 3:30 PM, Matei Zaharia <matei.zaharia@gmail.com=
>
> >> wrote:
> >>> +0.9 from me. Tested it on Mac and Windows (someone has to do it) and
> >> while things work, I noticed a few recent scripts don't have Windows
> >> equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683
> and
> >> https://issues.apache.org/jira/browse/SPARK-4684. The first one at
> least
> >> would be good to fix if we do another RC. Not blocking the release but
> >> useful to fix in docs is
> https://issues.apache.org/jira/browse/SPARK-4685.
> >>>
> >>> Matei
> >>>
> >>>
> >>>> On Dec 1, 2014, at 11:18 AM, Josh Rosen <rosenville@gmail.com> wrote=
:
> >>>>
> >>>> Hi everyone,
> >>>>
> >>>> There's an open bug report related to Spark standalone which could b=
e
> a
> >> potential release-blocker (pending investigation / a bug fix):
> >> https://issues.apache.org/jira/browse/SPARK-4498.  This issue seems
> >> non-deterministc and only affects long-running Spark standalone
> >> deployments, so it may be hard to reproduce.  I'm going to work on a
> patch
> >> to add additional logging in order to help with debugging.
> >>>>
> >>>> I just wanted to give an early head's up about this issue and to get
> >> more eyes on it in case anyone else has run into it or wants to help
> with
> >> debugging.
> >>>>
> >>>> - Josh
> >>>>
> >>>> On November 28, 2014 at 9:18:09 PM, Patrick Wendell (
> pwendell@gmail.com)
> >> wrote:
> >>>>
> >>>> Please vote on releasing the following candidate as Apache Spark
> >> version 1.2.0!
> >>>>
> >>>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
> >>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=
=3D
> >> 1056e9ec13203d0c51564265e94d77a054498fdb
> >>>>
> >>>> The release files, including signatures, digests, etc. can be found
> at:
> >>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
> >>>>
> >>>> Release artifacts are signed with the following key:
> >>>> https://people.apache.org/keys/committer/pwendell.asc
> >>>>
> >>>> The staging repository for this release can be found at:
> >>>>
> https://repository.apache.org/content/repositories/orgapachespark-1048/
> >>>>
> >>>> The documentation corresponding to this release can be found at:
> >>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
> >>>>
> >>>> Please vote on releasing this package as Apache Spark 1.2.0!
> >>>>
> >>>> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> >>>> if a majority of at least 3 +1 PMC votes are cast.
> >>>>
> >>>> [ ] +1 Release this package as Apache Spark 1.1.0
> >>>> [ ] -1 Do not release this package because ...
> >>>>
> >>>> To learn more about Apache Spark, please see
> >>>> http://spark.apache.org/
> >>>>
> >>>> =3D=3D What justifies a -1 vote for this release? =3D=3D
> >>>> This vote is happening very late into the QA period compared with
> >>>> previous votes, so -1 votes should only occur for significant
> >>>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> >>>> regressions, or bugs related to new features will not block this
> >>>> release.
> >>>>
> >>>> =3D=3D What default changes should I be aware of? =3D=3D
> >>>> 1. The default value of "spark.shuffle.blockTransferService" has bee=
n
> >>>> changed to "netty"
> >>>> --> Old behavior can be restored by switching to "nio"
> >>>>
> >>>> 2. The default value of "spark.shuffle.manager" has been changed to
> >> "sort".
> >>>> --> Old behavior can be restored by setting "spark.shuffle.manager" =
to
> >> "hash".
> >>>>
> >>>> =3D=3D Other notes =3D=3D
> >>>> Because this vote is occurring over a weekend, I will likely extend
> >>>> the vote if this RC survives until the end of the vote period.
> >>>>
> >>>> - Patrick
> >>>>
> >>>> --------------------------------------------------------------------=
-
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>
> >>>
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>
>

--001a11c334121c5922050941fd97--

From dev-return-10625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 21:07:25 2014
Return-Path: <dev-return-10625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAACA1028E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 21:07:25 +0000 (UTC)
Received: (qmail 84845 invoked by uid 500); 2 Dec 2014 21:07:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84770 invoked by uid 500); 2 Dec 2014 21:07:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84758 invoked by uid 99); 2 Dec 2014 21:07:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 21:07:24 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 21:07:19 +0000
Received: by mail-oi0-f47.google.com with SMTP id v63so9947657oia.34
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 13:05:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=MzUCUvpJOiBRbYYuVNTZxWnJZr+Bjy8KpqpO3JknH1Q=;
        b=FuZMZpZRpv0fkrfh8tLlyuKYhdkumy6vCWk6v5Gysxel56es5DZeYzkdfaHWwRGdg9
         X9KSAstZuvD3lBKzBsvs2ObJb/79JSpye/zGNDAJ57j6KtKMPR5FQpLVtXDsnMpuP3Vu
         xgu5m9wioFlSGXcg7cAp2xSGwY13Y5EafW2GoE5LBqC2j9o69EFLEk8nhG0Lh7AJE37b
         Ws5mqnhiPaHfd1hzgaf2AN2PWChx0cXNJH3Ysc284h3Qv1O0hunWCzhDsnedSTPZYoWv
         2tDFu4Muv7XKvJK+YrZxArUgrexGLGRQHlr20Qoox2NNP/LorMoqnwnzN1pM4mynMMOZ
         e6uw==
MIME-Version: 1.0
X-Received: by 10.202.218.2 with SMTP id r2mr842710oig.82.1417554328324; Tue,
 02 Dec 2014 13:05:28 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Tue, 2 Dec 2014 13:05:28 -0800 (PST)
In-Reply-To: <CAJiQeYLEYppjbvUsF_ww1Ezeye=qU1C6nPa_yDueiQzXE_haOw@mail.gmail.com>
References: <CAKJXNjHdX0Rqpgn7XJ_6GF2Chz4yE=oh_RY-UqbsWDeTuky6=Q@mail.gmail.com>
	<CAJiQeYLEYppjbvUsF_ww1Ezeye=qU1C6nPa_yDueiQzXE_haOw@mail.gmail.com>
Date: Tue, 2 Dec 2014 13:05:28 -0800
Message-ID: <CABPQxsuXEPu-oXHVcN9azSH=1B5eeACoB6dLDjXdnEH8A2Tw5g@mail.gmail.com>
Subject: Re: keeping PR titles / descriptions up to date
From: Patrick Wendell <pwendell@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: Kay Ousterhout <kayousterhout@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Also a note on this for committers - it's possible to re-word the
title during merging, by just running "git commit -a --amend" before
you push the PR.

- Patrick

On Tue, Dec 2, 2014 at 12:50 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> I second that !
> Would also be great if the JIRA was updated accordingly too.
>
> Regards,
> Mridul
>
>
> On Wed, Dec 3, 2014 at 1:53 AM, Kay Ousterhout <kayousterhout@gmail.com> wrote:
>> Hi all,
>>
>> I've noticed a bunch of times lately where a pull request changes to be
>> pretty different from the original pull request, and the title /
>> description never get updated.  Because the pull request title and
>> description are used as the commit message, the incorrect description lives
>> on forever, making it harder to understand the reason behind a particular
>> commit without going back and reading the entire conversation on the pull
>> request.  If folks could try to keep these up to date (and committers, try
>> to remember to verify that the title and description are correct before
>> making merging pull requests), that would be awesome.
>>
>> -Kay
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 21:37:23 2014
Return-Path: <dev-return-10626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB269104C4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 21:37:23 +0000 (UTC)
Received: (qmail 72758 invoked by uid 500); 2 Dec 2014 21:37:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72525 invoked by uid 500); 2 Dec 2014 21:37:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72000 invoked by uid 99); 2 Dec 2014 21:37:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 21:37:14 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 21:37:09 +0000
Received: by mail-la0-f45.google.com with SMTP id gq15so11363292lab.4
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 13:36:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=+FslzmGsJ9Y+sk/fi+BfAQuY2+4BR3AdBVoPRoznWG0=;
        b=ghp6ZVRTOSC1GNeTsSzGEdCxKeSLO4e4xavZn5+TfEGO4lavg7y5POyRO33WqYo5QG
         xr5DMrc4zsUswApi4q8cdofxAJhk1wl1tkm8vYLarhLlbry/YenXFaQyAyyjHOhutzJl
         2WZa7AmR9dUf9xFI7g9VH8oJlh0ss2hyXBZyrZSiw9lRMPgt4FTkGabbxDyK5pVjwPGE
         iCJqA+IREA4hedkiYPiA/6YzT/SREap0g+G04wsXZEegJLKARA1qdEoDN5RYY+wG2r7Q
         c9QozdUlBayeEZi6LA1Ow9hWLlgl9Wk8Dy0VjCcpeEnyc/TJ1WDhkgeQYeBrcNYqtT9x
         2NSA==
X-Gm-Message-State: ALoCoQmpJdBH1k25GBO0nFgtFi+4EtysAbILm1trGC90ZXMkhGN8f/o/+D0prPIxOSEI/q04hTE+
MIME-Version: 1.0
X-Received: by 10.152.25.168 with SMTP id d8mr1225873lag.41.1417556207389;
 Tue, 02 Dec 2014 13:36:47 -0800 (PST)
Received: by 10.112.112.66 with HTTP; Tue, 2 Dec 2014 13:36:47 -0800 (PST)
Date: Tue, 2 Dec 2014 13:36:47 -0800
Message-ID: <CAMJOb8mCN8FWEywe5V4yd7MD-F5Qod_StV4uNZeeNn4JuWzgbQ@mail.gmail.com>
Subject: Announcing Spark 1.1.1!
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160a5d430e56b0509428665
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a5d430e56b0509428665
Content-Type: text/plain; charset=UTF-8

I am happy to announce the availability of Spark 1.1.1! This is a
maintenance release with many bug fixes, most of which are concentrated in
the core. This list includes various fixes to sort-based shuffle, memory
leak, and spilling issues. Contributions from this release came from 55
developers.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

[1] http://spark.apache.org/releases/spark-release-1-1-1.html
[2] http://spark.apache.org/downloads.html

Please e-mail me directly for any typo's in the release notes or name
listing.

Thanks for everyone who contributed, and congratulations!
-Andrew

--089e0160a5d430e56b0509428665--

From dev-return-10627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 22:08:17 2014
Return-Path: <dev-return-10627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA43B10622
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 22:08:17 +0000 (UTC)
Received: (qmail 56053 invoked by uid 500); 2 Dec 2014 22:08:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55973 invoked by uid 500); 2 Dec 2014 22:08:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55961 invoked by uid 99); 2 Dec 2014 22:08:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:08:16 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 98.138.121.113 as permitted sender)
Received: from [98.138.121.113] (HELO nm48-vm1.bullet.mail.ne1.yahoo.com) (98.138.121.113)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:07:47 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=fVzxupg553qv/1wvAf2Pg3Zv4vkH4mwT6kWsHbtm4uLqHhP1dvTLJ2QPknjemIrtd7VKVsDJ8Iui1lCoGypBXFuZfUS2vcPvVheOCfJxc+OZBg83IeDgvCxSdjFALQFN+aUrCy3GQEZ59fyifBFQTL69fnOPBRhbMteTZ73S1yDubfKKvXPFh2Jha/05U6uvI7N+DSrBvHTrvu/V3JNcLWVNRoyL3IP2RHlZCTYYrUjHCmhrbpsBLltYxH9WVf/OVgYqUoQOJpVQnf1z3AnV9RxOht9EJtAzCs5IUz0SRXsM1kTPsucSQs4kuBNpLr5N0YbfYeIdXDMzaB1KVc9cCw==;
Received: from [127.0.0.1] by nm48.bullet.mail.ne1.yahoo.com with NNFMP; 02 Dec 2014 22:07:45 -0000
Received: from [98.138.100.118] by nm48.bullet.mail.ne1.yahoo.com with NNFMP; 02 Dec 2014 22:04:54 -0000
Received: from [66.196.81.174] by tm109.bullet.mail.ne1.yahoo.com with NNFMP; 02 Dec 2014 22:04:54 -0000
Received: from [98.139.212.251] by tm20.bullet.mail.bf1.yahoo.com with NNFMP; 02 Dec 2014 22:04:54 -0000
Received: from [127.0.0.1] by omp1060.mail.bf1.yahoo.com with NNFMP; 02 Dec 2014 22:04:54 -0000
X-Yahoo-Newman-Property: ymail-4
X-Yahoo-Newman-Id: 539651.25879.bm@omp1060.mail.bf1.yahoo.com
X-YMail-OSG: VdGED1EVM1kiZ8Sb3kwyTEaBDcGsvocqh_XQUa17LWT4ekvQIhWZQsyDvr5U5FH
 CSsJuuGkWquTBmP.s5GOnNHXtm0w32YBRRcy6mqDwQCPuzk1LFauTKJjgTBQMwKvoc00t1PRBKWm
 ZYwy3HBB1h_.Tv9MzYygcu1uHFJDO_rScMgG3.sNzA8Os5Lt.x8qbEhfbAK0UzPApgINuv57DxMz
 eh9SJjv190vps959tyEVy_cvOcUPpWeTqPptSt.T8JF5avZKXSsxR0x.VqoBid3A_hsqbV9WIbkR
 jFK_qDMqa8lvSVDLW5vIZg9SGdwBxu_OILkzPMHog6dvzgWzR.VBmKP_7JFavoREzkzrvMQlFVr8
 Y2z4FJvWVmPI5l7TuEa2GfHUV5UR0kwGE3IL5m2DpfZBmGeizbOPFUov9S.SjjIOivaPxCoESMOF
 0y7rmgonPnMNhKonJ2YHyec5FnBEIxQLDIAofJKcmW5Iki23eqFr39BR.EAZVzewfqE8I41kAjSR
 q_VWCldGfQJMhGXa_lmkVMHqmDMDeE19Mwl01Hn7gkZikMkim1F8kvVij4fxkhRxAzDHuUpTPvj3
 3aOwDyo4D1sY-
Received: by 76.13.27.54; Tue, 02 Dec 2014 22:04:54 +0000 
Date: Tue, 2 Dec 2014 22:04:53 +0000 (UTC)
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
To: Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1817763605.4066444.1417557893287.JavaMail.yahoo@jws10617.mail.bf1.yahoo.com>
In-Reply-To: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_4066443_1696088822.1417557893283"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_4066443_1696088822.1417557893283
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

+1 tested on yarn.
Tom 

     On Friday, November 28, 2014 11:18 PM, Patrick Wendell <pwendell@gmail.com> wrote:
   

 Please vote on releasing the following candidate as Apache Spark version 1.2.0!

The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1048/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.2.0!

The vote is open until Tuesday, December 02, at 05:15 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.1.X, minor
regressions, or bugs related to new features will not block this
release.

== What default changes should I be aware of? ==
1. The default value of "spark.shuffle.blockTransferService" has been
changed to "netty"
--> Old behavior can be restored by switching to "nio"

2. The default value of "spark.shuffle.manager" has been changed to "sort".
--> Old behavior can be restored by setting "spark.shuffle.manager" to "hash".

== Other notes ==
Because this vote is occurring over a weekend, I will likely extend
the vote if this RC survives until the end of the vote period.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org



    
------=_Part_4066443_1696088822.1417557893283--

From dev-return-10628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 22:41:19 2014
Return-Path: <dev-return-10628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 598FE107EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 22:41:19 +0000 (UTC)
Received: (qmail 77309 invoked by uid 500); 2 Dec 2014 22:41:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77223 invoked by uid 500); 2 Dec 2014 22:41:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77205 invoked by uid 99); 2 Dec 2014 22:41:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:41:17 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.213.49 as permitted sender)
Received: from [209.85.213.49] (HELO mail-yh0-f49.google.com) (209.85.213.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:41:14 +0000
Received: by mail-yh0-f49.google.com with SMTP id f10so6557745yha.36
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 14:40:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=pMTLazMfHEuy7ijBDvh/vIhNWvH5iT1fy/U+kY4ftDI=;
        b=pfX2jaz9vEutoDyNkNglYjfJVrayvThsR4pbwmJECfuavgXXcZWDU6XFWD1lrkA7fL
         CwS/Jcc8utNzbEY3zy9iQdbSioqn7K7gRs4B8IVSQz8N/Su9euw8xtxCwcnrZF47AxD0
         /M88mHzxFFE8neh27KadPNRZVWwQBUNLplj+50DopadUSx4lD4E4yZ9tahS/ModwWFM6
         FAe7j/x0zVmcxvyTEa3YhNV6FtIRGw+GuJQpMAQ8QHjsDCjszTGBUFneeioOp8Bk6Km6
         uiU4cOv6Fxyg2Yl68/GSvgWcPI/DEvD8jvgYz1//D3OIi+whxcEqt45rjK56gJgsMy5E
         2DKw==
X-Received: by 10.236.222.74 with SMTP id s70mr2509829yhp.88.1417560008236;
 Tue, 02 Dec 2014 14:40:08 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Tue, 02 Dec 2014 22:40:07 +0000
Message-ID: <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Mark Hamstra <mark@clearstorydata.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01681e44bd23f305094368bb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01681e44bd23f305094368bb
Content-Type: text/plain; charset=UTF-8

Following on Mark's Maven examples, here is another related issue I'm
having:

I'd like to compile just the `core` module after a `mvn clean`, without
building an assembly JAR first. Is this possible?

Attempting to do it myself, the steps I performed were:

- `mvn compile -pl core`: fails because `core` depends on `network/common`
and `network/shuffle`, neither of which is installed in my local maven
cache (and which don't exist in central Maven repositories, I guess? I
thought Spark is publishing snapshot releases?)

- `network/shuffle` also depends on `network/common`, so I'll `mvn install`
the latter first: `mvn install -DskipTests -pl network/common`. That
succeeds, and I see a newly built 1.3.0-SNAPSHOT jar in my local maven
repository.

- However, `mvn install -DskipTests -pl network/shuffle` subsequently
fails, seemingly due to not finding network/core. Here's
<https://gist.github.com/ryan-williams/1711189e7d0af558738d> a sample full
output from running `mvn install -X -U -DskipTests -pl network/shuffle`
from such a state (the -U was to get around a previous failure based on
having cached a failed lookup of network-common-1.3.0-SNAPSHOT).

- Thinking maven might be special-casing "-SNAPSHOT" versions, I tried
replacing "1.3.0-SNAPSHOT" with "1.3.0.1" globally and repeating these
steps, but the error seems to be the same
<https://gist.github.com/ryan-williams/37fcdd14dd92fa562dbe>.

Any ideas?

Thanks,

-Ryan

On Sun Nov 30 2014 at 6:37:28 PM Mark Hamstra <mark@clearstorydata.com>
wrote:

> >
> > - Start the SBT interactive console with sbt/sbt
> > - Build your assembly by running the "assembly" target in the assembly
> > project: assembly/assembly
> > - Run all the tests in one module: core/test
> > - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
> (this
> > also supports tab completion)
>
>
> The equivalent using Maven:
>
> - Start zinc
> - Build your assembly using the mvn "package" or "install" target
> ("install" is actually the equivalent of SBT's "publishLocal") -- this step
> is the first step in
> http://spark.apache.org/docs/latest/building-with-maven.
> html#spark-tests-in-maven
> - Run all the tests in one module: mvn -pl core test
> - Run a specific suite: mvn -pl core
> -DwildcardSuites=org.apache.spark.rdd.RDDSuite test (the -pl option isn't
> strictly necessary if you don't mind waiting for Maven to scan through all
> the other sub-projects only to do nothing; and, of course, it needs to be
> something other than "core" if the test you want to run is in another
> sub-project.)
>
> You also typically want to carry along in each subsequent step any relevant
> command line options you added in the "package"/"install" step.
>
> On Sun, Nov 30, 2014 at 3:06 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
> > Hi Ryan,
> >
> > As a tip (and maybe this isn't documented well), I normally use SBT for
> > development to avoid the slow build process, and use its interactive
> > console to run only specific tests. The nice advantage is that SBT can
> keep
> > the Scala compiler loaded and JITed across builds, making it faster to
> > iterate. To use it, you can do the following:
> >
> > - Start the SBT interactive console with sbt/sbt
> > - Build your assembly by running the "assembly" target in the assembly
> > project: assembly/assembly
> > - Run all the tests in one module: core/test
> > - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
> (this
> > also supports tab completion)
> >
> > Running all the tests does take a while, and I usually just rely on
> > Jenkins for that once I've run the tests for the things I believed my
> patch
> > could break. But this is because some of them are integration tests (e.g.
> > DistributedSuite, which creates multi-process mini-clusters). Many of the
> > individual suites run fast without requiring this, however, so you can
> pick
> > the ones you want. Perhaps we should find a way to tag them so people
> can
> > do a "quick-test" that skips the integration ones.
> >
> > The assembly builds are annoying but they only take about a minute for me
> > on a MacBook Pro with SBT warmed up. The assembly is actually only
> required
> > for some of the "integration" tests (which launch new processes), but I'd
> > recommend doing it all the time anyway since it would be very confusing
> to
> > run those with an old assembly. The Scala compiler crash issue can also
> be
> > a problem, but I don't see it very often with SBT. If it happens, I exit
> > SBT and do sbt clean.
> >
> > Anyway, this is useful feedback and I think we should try to improve some
> > of these suites, but hopefully you can also try the faster SBT process.
> At
> > the end of the day, if we want integration tests, the whole test process
> > will take an hour, but most of the developers I know leave that to
> Jenkins
> > and only run individual tests locally before submitting a patch.
> >
> > Matei
> >
> >
> > > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
> > ryan.blake.williams@gmail.com> wrote:
> > >
> > > In the course of trying to make contributions to Spark, I have had a
> lot
> > of
> > > trouble running Spark's tests successfully. The main pain points I've
> > > experienced are:
> > >
> > >    1) frequent, spurious test failures
> > >    2) high latency of running tests
> > >    3) difficulty running specific tests in an iterative fashion
> > >
> > > Here is an example series of failures that I encountered this weekend
> > > (along with footnote links to the console output from each and
> > > approximately how long each took):
> > >
> > > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
> > > before.
> > > - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
> > > - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]:
> BroadcastSuite
> > > passed, but scala compiler crashed on the "catalyst" project.
> > > - `mvn clean`: some attempts to run earlier commands (that previously
> > > didn't crash the compiler) all result in the same compiler crash.
> > Previous
> > > discussion on this list implies this can only be solved by a `mvn
> clean`
> > > [4].
> > > - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
> > > BroadcastSuite can't run because assembly is not built.
> > > - `./dev/run-tests` again [6]: pyspark tests fail, some messages about
> > > version mismatches and python 2.6. The machine this ran on has python
> > 2.7,
> > > so I don't know what that's about.
> > > - `./dev/run-tests` again [7]: "too many open files" errors in several
> > > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this
> is
> > > not enough, but only some of the time? I increased it to 8192 and tried
> > > again.
> > > - `./dev/run-tests` again [8]: same pyspark errors as before. This
> seems
> > to
> > > be the issue from SPARK-3867 [9], which was supposedly fixed on October
> > 14;
> > > not sure how I'm seeing it now. In any case, switched to Python 2.6 and
> > > installed unittest2, and python/run-tests seems to be unblocked.
> > > - `./dev/run-tests` again [10]: finally passes!
> > >
> > > This was on a spark checkout at ceb6281 (ToT Friday), with a few
> trivial
> > > changes added on (that I wanted to test before sending out a PR), on a
> > > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
> > >
> > > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
> > commands
> > > from the same repo state:
> > >
> > > - `./dev/run-tests` [12]: YarnClusterSuite failure.
> > > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
> seen
> > > this one before on this machine and am guessing it actually occurs
> every
> > > time.
> > > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
> more
> > > time from ceb6281, and saw the same failure.
> > >
> > > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
> > narrow
> > > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my
> > mac,
> > > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
> > used),
> > > and it passed [16], so the failure seems specific to my linux
> > machine/arch.
> > >
> > > At this point I believe that my changes don't break any tests (the
> > > YarnClusterSuite failure on my linux presumably not being... "real"),
> > and I
> > > am ready to send out a PR. Whew!
> > >
> > > However, reflecting on the 5 or 6 distinct failure-modes represented
> > above:
> > >
> > > - One of them (too many files open), is something I can (and did,
> > > hopefully) fix once and for all. It cost me an ~hour this time
> > (approximate
> > > time of running ./dev/run-tests) and a few hours other times when I
> > didn't
> > > fully understand/fix it. It doesn't happen deterministically (why?),
> but
> > > does happen somewhat frequently to people, having been discussed on the
> > > user list multiple times [17] and on SO [18]. Maybe some note in the
> > > documentation advising people to check their ulimit makes sense?
> > > - One of them (unittest2 must be installed for python 2.6) was
> supposedly
> > > fixed upstream of the commits I tested here; I don't know why I'm still
> > > running into it. This cost me a few hours of running `./dev/run-tests`
> > > multiple times to see if it was transient, plus some time researching
> and
> > > working around it.
> > > - The original BroadcastSuite failure cost me a few hours and went away
> > > before I'd even run `mvn clean`.
> > > - A new incarnation of the sbt-compiler-crash phenomenon cost me a few
> > > hours of running `./dev/run-tests` in different ways before deciding
> > that,
> > > as usual, there was no way around it and that I'd need to run `mvn
> clean`
> > > and start running tests from scratch.
> > > - The YarnClusterSuite failures on my linux box have cost me hours of
> > > trying to figure out whether they're my fault. I've seen them many
> times
> > > over the past weeks/months, plus or minus other failures that have come
> > and
> > > gone, and was especially befuddled by them when I was seeing a disjoint
> > set
> > > of reproducible failures on my mac [19] (the triaging of which involved
> > > dozens of runs of `./dev/run-tests`).
> > >
> > > While I'm interested in digging into each of these issues, I also want
> to
> > > discuss the frequency with which I've run into issues like these. This
> is
> > > unfortunately not the first time in recent months that I've spent days
> > > playing spurious-test-failure whack-a-mole with a 60-90min
> dev/run-tests
> > > iteration time, which is no fun! So I am wondering/thinking:
> > >
> > > - Do other people experience this level of flakiness from spark tests?
> > > - Do other people bother running dev/run-tests locally, or just let
> > Jenkins
> > > do it during the CR process?
> > > - Needing to run a full assembly post-clean just to continue running
> one
> > > specific test case feels especially wasteful, and the failure output
> when
> > > naively attempting to run a specific test without having built an
> > assembly
> > > jar is not always clear about what the issue is or how to fix it; even
> > the
> > > fact that certain tests require "building the world" is not something I
> > > would have expected, and has cost me hours of confusion.
> > >    - Should a person running spark tests assume that they must build an
> > > assembly JAR before running anything?
> > >    - Are there some proper "unit" tests that are actually
> self-contained
> > /
> > > able to be run without building an assembly jar?
> > >    - Can we better document/demarcate which tests have which
> > dependencies?
> > >    - Is there something finer-grained than building an assembly JAR
> that
> > > is sufficient in some cases?
> > >        - If so, can we document that?
> > >        - If not, can we move to a world of finer-grained dependencies
> for
> > > some of these?
> > > - Leaving all of these spurious failures aside, the process of
> assembling
> > > and testing a new JAR is not a quick one (40 and 60 mins for me
> > typically,
> > > respectively). I would guess that there are dozens (hundreds?) of
> people
> > > who build a Spark assembly from various ToTs on any given day, and who
> > all
> > > wait on the exact same compilation / assembly steps to occur. Expanding
> > on
> > > the recent work to publish nightly snapshots [20], can we do a better
> job
> > > caching/sharing compilation artifacts at a more granular level
> (pre-built
> > > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
> more
> > > granular maven modules, plus the previous two?), or otherwise save some
> > of
> > > the considerable amount of redundant compilation work that I had to do
> > over
> > > the course of my odyssey this weekend?
> > >
> > > Ramping up on most projects involves some amount of supplementing the
> > > documentation with trial and error to figure out what to run, which
> > > "errors" are real errors and which can be ignored, etc., but navigating
> > > that minefield on Spark has proved especially challenging and
> > > time-consuming for me. Some of that comes directly from scala's
> > relatively
> > > slow compilation times and immature build-tooling ecosystem, but that
> is
> > > the world we live in and it would be nice if Spark took the alleviation
> > of
> > > the resulting pain more seriously, as one of the more interesting and
> > > well-known large scala projects around right now. The official
> > > documentation around how to build different subsets of the codebase is
> > > somewhat sparse [21], and there have been many mixed [22] accounts [23]
> > on
> > > this mailing list about preferred ways to build on mvn vs. sbt (none of
> > > which has made it into official documentation, as far as I've seen).
> > > Expecting new contributors to piece together all of this received
> > > folk-wisdom about how to build/test in a sane way by trawling mailing
> > list
> > > archives seems suboptimal.
> > >
> > > Thanks for reading, looking forward to hearing your ideas!
> > >
> > > -Ryan
> > >
> > > P.S. Is "best practice" for emailing this list to not incorporate any
> > HTML
> > > in the body? It seems like all of the archives I've seen strip it out,
> > but
> > > other people have used it and gmail displays it.
> > >
> > >
> > > [1]
> > >
> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
> > > (57 mins)
> > > [2]
> > >
> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> > > (6 mins)
> > > [3]
> > >
> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
> 20pass%20test,%20fail%20subsequent%20compile
> > > (4 mins)
> > > [4]
> > >
> > https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&
> cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-
> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
> DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-
> iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=
> zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
> > > [5]
> > >
> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
> 20clean,%20need%20dependencies%20built
> > > [6]
> > >
> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
> 20post%20clean
> > > (50 mins)
> > > [7]
> > >
> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
> > > (1hr)
> > > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
> > > [9] https://issues.apache.org/jira/browse/SPARK-3867
> > > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> > > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> > > [12]
> > >
> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
> file-gistfile1-txt-L853
> > > (~90 mins)
> > > [13]
> > >
> > https://gist.github.com/ryan-williams/718f6324af358819b496#
> file-gistfile1-txt-L852
> > > (91 mins)
> > > [14]
> > >
> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
> file-gistfile1-txt-L854
> > > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> > > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> > > [17]
> > >
> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
> > > [18]
> > >
> > http://stackoverflow.com/questions/25707629/why-does-
> spark-job-fail-with-too-many-open-files
> > > [19] https://issues.apache.org/jira/browse/SPARK-4002
> > > [20] https://issues.apache.org/jira/browse/SPARK-4542
> > > [21]
> > >
> > https://spark.apache.org/docs/latest/building-with-maven.
> html#spark-tests-in-maven
> > > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
> > > [23]
> > >
> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--089e01681e44bd23f305094368bb--

From dev-return-10629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 22:46:16 2014
Return-Path: <dev-return-10629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5459A10844
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 22:46:16 +0000 (UTC)
Received: (qmail 9200 invoked by uid 500); 2 Dec 2014 22:46:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9123 invoked by uid 500); 2 Dec 2014 22:46:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9081 invoked by uid 99); 2 Dec 2014 22:46:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:46:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:45:48 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so9922271qge.8
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 14:45:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=xI5UiGg7Q+beV5BW22QkYc0z8k7fTnpEWhGeFhWMBLs=;
        b=cef2+c29p3tTXpOilWzjw/FhczB1q3Q3yyG7hTSPpA6jVXBtdLPllYpuQ8gLJyPSCP
         S1FO5E3/1x2+su580c5h0HUcmv6CGzZ+4VDFzWzI91LMYERzuoultpg+qyDakmh/C594
         9TU418uQoRgmF745up9JmxClaj8ZmJW/tzAlfX1xzyQZJDESnJO078m+lagquaYsVo0y
         tNE2zxVtDx7wwTWPtBRgFNlKCCRWtkQC8RJ+QuK1rHIu3bQToHM4DzxsoP76y/kO/n8+
         uvWBAdJNEgwPOd69/IN4T1t/Yj6Jk9t1kVSQWcVG/M8que43PBOmYJhH2j9cUg02LQOv
         qmHw==
X-Gm-Message-State: ALoCoQlknPT5sulyZcEKPzFX7x1ES9sIdrt5jlggAyfzhJybazcYxyb+XmpqH31SgXZEkOjgCKF+
MIME-Version: 1.0
X-Received: by 10.229.44.7 with SMTP id y7mr2613958qce.26.1417560347392; Tue,
 02 Dec 2014 14:45:47 -0800 (PST)
Received: by 10.229.15.202 with HTTP; Tue, 2 Dec 2014 14:45:47 -0800 (PST)
In-Reply-To: <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
	<CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
Date: Tue, 2 Dec 2014 14:45:47 -0800
Message-ID: <CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Mark Hamstra <mark@clearstorydata.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, Dec 2, 2014 at 2:40 PM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
> Following on Mark's Maven examples, here is another related issue I'm
> having:
>
> I'd like to compile just the `core` module after a `mvn clean`, without
> building an assembly JAR first. Is this possible?

Out of curiosity, may I ask why? What's the problem with running "mvn
install -DskipTests" first (or "package" instead of "install",
although I generally do the latter)?

You can probably do what you want if you manually build / install all
the needed dependencies first; you found two, but it seems you're also
missing the "spark-parent" project (which is the top-level pom). That
sounds like a lot of trouble though, for not any gains that I can
see... after the first build you should be able to do what you want
easily.

-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 22:46:37 2014
Return-Path: <dev-return-10630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C2FB10849
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 22:46:37 +0000 (UTC)
Received: (qmail 11468 invoked by uid 500); 2 Dec 2014 22:46:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11392 invoked by uid 500); 2 Dec 2014 22:46:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11379 invoked by uid 99); 2 Dec 2014 22:46:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:46:36 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 22:46:10 +0000
Received: by mail-ob0-f173.google.com with SMTP id uy5so10575047obc.18
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 14:46:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=peQ299lZ6he1EHdEd0zDtvRHV+59hpE9WXlTGjfJjrQ=;
        b=mXA7CVZwX3QjntDjf0EYy7WMO8nJaS3o6ECAMe4/jhyW80mOclELLwFC40VQRIJi0I
         W2iAjoLmwAXPUlNZ27Wx/rfOsANYMeWWbWi63LuEpKm2IKH414ukBfCG+QyGCrN3ZrPp
         moXif5uQYG10OlvMbIS+ZbPgKzL37QvqL6AQlem9woEs2OJ0AkRIpQhaAYtr0yNJw68S
         mqQj5/yhk+Kz0Azr+xpmC+AF1RR3mBvhcyZAjH5ZOxliOAGdE8ibAQzKZcLU71qmUzXn
         mfEpK/eINUeJQAhqAvw5CKoWyyeJwgbSj4CrsUm5dmcNPpF7XQijYMQuGvcyREY1o4OS
         lVPg==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr1078158oie.78.1417560369372;
 Tue, 02 Dec 2014 14:46:09 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Tue, 2 Dec 2014 14:46:09 -0800 (PST)
In-Reply-To: <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
	<CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
Date: Tue, 2 Dec 2014 14:46:09 -0800
Message-ID: <CABPQxss+CHdgBK31fgFOpZzi3fHOtrk-L7k0=fZNZRx38PrtwQ@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Patrick Wendell <pwendell@gmail.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Mark Hamstra <mark@clearstorydata.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Ryan,

What if you run a single "mvn install" to install all libraries
locally - then can you "mvn compile -pl core"? I think this may be the
only way to make it work.

- Patrick

On Tue, Dec 2, 2014 at 2:40 PM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
> Following on Mark's Maven examples, here is another related issue I'm
> having:
>
> I'd like to compile just the `core` module after a `mvn clean`, without
> building an assembly JAR first. Is this possible?
>
> Attempting to do it myself, the steps I performed were:
>
> - `mvn compile -pl core`: fails because `core` depends on `network/common`
> and `network/shuffle`, neither of which is installed in my local maven
> cache (and which don't exist in central Maven repositories, I guess? I
> thought Spark is publishing snapshot releases?)
>
> - `network/shuffle` also depends on `network/common`, so I'll `mvn install`
> the latter first: `mvn install -DskipTests -pl network/common`. That
> succeeds, and I see a newly built 1.3.0-SNAPSHOT jar in my local maven
> repository.
>
> - However, `mvn install -DskipTests -pl network/shuffle` subsequently
> fails, seemingly due to not finding network/core. Here's
> <https://gist.github.com/ryan-williams/1711189e7d0af558738d> a sample full
> output from running `mvn install -X -U -DskipTests -pl network/shuffle`
> from such a state (the -U was to get around a previous failure based on
> having cached a failed lookup of network-common-1.3.0-SNAPSHOT).
>
> - Thinking maven might be special-casing "-SNAPSHOT" versions, I tried
> replacing "1.3.0-SNAPSHOT" with "1.3.0.1" globally and repeating these
> steps, but the error seems to be the same
> <https://gist.github.com/ryan-williams/37fcdd14dd92fa562dbe>.
>
> Any ideas?
>
> Thanks,
>
> -Ryan
>
> On Sun Nov 30 2014 at 6:37:28 PM Mark Hamstra <mark@clearstorydata.com>
> wrote:
>
>> >
>> > - Start the SBT interactive console with sbt/sbt
>> > - Build your assembly by running the "assembly" target in the assembly
>> > project: assembly/assembly
>> > - Run all the tests in one module: core/test
>> > - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>> (this
>> > also supports tab completion)
>>
>>
>> The equivalent using Maven:
>>
>> - Start zinc
>> - Build your assembly using the mvn "package" or "install" target
>> ("install" is actually the equivalent of SBT's "publishLocal") -- this step
>> is the first step in
>> http://spark.apache.org/docs/latest/building-with-maven.
>> html#spark-tests-in-maven
>> - Run all the tests in one module: mvn -pl core test
>> - Run a specific suite: mvn -pl core
>> -DwildcardSuites=org.apache.spark.rdd.RDDSuite test (the -pl option isn't
>> strictly necessary if you don't mind waiting for Maven to scan through all
>> the other sub-projects only to do nothing; and, of course, it needs to be
>> something other than "core" if the test you want to run is in another
>> sub-project.)
>>
>> You also typically want to carry along in each subsequent step any relevant
>> command line options you added in the "package"/"install" step.
>>
>> On Sun, Nov 30, 2014 at 3:06 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>
>> > Hi Ryan,
>> >
>> > As a tip (and maybe this isn't documented well), I normally use SBT for
>> > development to avoid the slow build process, and use its interactive
>> > console to run only specific tests. The nice advantage is that SBT can
>> keep
>> > the Scala compiler loaded and JITed across builds, making it faster to
>> > iterate. To use it, you can do the following:
>> >
>> > - Start the SBT interactive console with sbt/sbt
>> > - Build your assembly by running the "assembly" target in the assembly
>> > project: assembly/assembly
>> > - Run all the tests in one module: core/test
>> > - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite
>> (this
>> > also supports tab completion)
>> >
>> > Running all the tests does take a while, and I usually just rely on
>> > Jenkins for that once I've run the tests for the things I believed my
>> patch
>> > could break. But this is because some of them are integration tests (e.g.
>> > DistributedSuite, which creates multi-process mini-clusters). Many of the
>> > individual suites run fast without requiring this, however, so you can
>> pick
>> > the ones you want. Perhaps we should find a way to tag them so people
>> can
>> > do a "quick-test" that skips the integration ones.
>> >
>> > The assembly builds are annoying but they only take about a minute for me
>> > on a MacBook Pro with SBT warmed up. The assembly is actually only
>> required
>> > for some of the "integration" tests (which launch new processes), but I'd
>> > recommend doing it all the time anyway since it would be very confusing
>> to
>> > run those with an old assembly. The Scala compiler crash issue can also
>> be
>> > a problem, but I don't see it very often with SBT. If it happens, I exit
>> > SBT and do sbt clean.
>> >
>> > Anyway, this is useful feedback and I think we should try to improve some
>> > of these suites, but hopefully you can also try the faster SBT process.
>> At
>> > the end of the day, if we want integration tests, the whole test process
>> > will take an hour, but most of the developers I know leave that to
>> Jenkins
>> > and only run individual tests locally before submitting a patch.
>> >
>> > Matei
>> >
>> >
>> > > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
>> > ryan.blake.williams@gmail.com> wrote:
>> > >
>> > > In the course of trying to make contributions to Spark, I have had a
>> lot
>> > of
>> > > trouble running Spark's tests successfully. The main pain points I've
>> > > experienced are:
>> > >
>> > >    1) frequent, spurious test failures
>> > >    2) high latency of running tests
>> > >    3) difficulty running specific tests in an iterative fashion
>> > >
>> > > Here is an example series of failures that I encountered this weekend
>> > > (along with footnote links to the console output from each and
>> > > approximately how long each took):
>> > >
>> > > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
>> > > before.
>> > > - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
>> > > - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]:
>> BroadcastSuite
>> > > passed, but scala compiler crashed on the "catalyst" project.
>> > > - `mvn clean`: some attempts to run earlier commands (that previously
>> > > didn't crash the compiler) all result in the same compiler crash.
>> > Previous
>> > > discussion on this list implies this can only be solved by a `mvn
>> clean`
>> > > [4].
>> > > - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
>> > > BroadcastSuite can't run because assembly is not built.
>> > > - `./dev/run-tests` again [6]: pyspark tests fail, some messages about
>> > > version mismatches and python 2.6. The machine this ran on has python
>> > 2.7,
>> > > so I don't know what that's about.
>> > > - `./dev/run-tests` again [7]: "too many open files" errors in several
>> > > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this
>> is
>> > > not enough, but only some of the time? I increased it to 8192 and tried
>> > > again.
>> > > - `./dev/run-tests` again [8]: same pyspark errors as before. This
>> seems
>> > to
>> > > be the issue from SPARK-3867 [9], which was supposedly fixed on October
>> > 14;
>> > > not sure how I'm seeing it now. In any case, switched to Python 2.6 and
>> > > installed unittest2, and python/run-tests seems to be unblocked.
>> > > - `./dev/run-tests` again [10]: finally passes!
>> > >
>> > > This was on a spark checkout at ceb6281 (ToT Friday), with a few
>> trivial
>> > > changes added on (that I wanted to test before sending out a PR), on a
>> > > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
>> > >
>> > > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
>> > commands
>> > > from the same repo state:
>> > >
>> > > - `./dev/run-tests` [12]: YarnClusterSuite failure.
>> > > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've
>> seen
>> > > this one before on this machine and am guessing it actually occurs
>> every
>> > > time.
>> > > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one
>> more
>> > > time from ceb6281, and saw the same failure.
>> > >
>> > > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
>> > narrow
>> > > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my
>> > mac,
>> > > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
>> > used),
>> > > and it passed [16], so the failure seems specific to my linux
>> > machine/arch.
>> > >
>> > > At this point I believe that my changes don't break any tests (the
>> > > YarnClusterSuite failure on my linux presumably not being... "real"),
>> > and I
>> > > am ready to send out a PR. Whew!
>> > >
>> > > However, reflecting on the 5 or 6 distinct failure-modes represented
>> > above:
>> > >
>> > > - One of them (too many files open), is something I can (and did,
>> > > hopefully) fix once and for all. It cost me an ~hour this time
>> > (approximate
>> > > time of running ./dev/run-tests) and a few hours other times when I
>> > didn't
>> > > fully understand/fix it. It doesn't happen deterministically (why?),
>> but
>> > > does happen somewhat frequently to people, having been discussed on the
>> > > user list multiple times [17] and on SO [18]. Maybe some note in the
>> > > documentation advising people to check their ulimit makes sense?
>> > > - One of them (unittest2 must be installed for python 2.6) was
>> supposedly
>> > > fixed upstream of the commits I tested here; I don't know why I'm still
>> > > running into it. This cost me a few hours of running `./dev/run-tests`
>> > > multiple times to see if it was transient, plus some time researching
>> and
>> > > working around it.
>> > > - The original BroadcastSuite failure cost me a few hours and went away
>> > > before I'd even run `mvn clean`.
>> > > - A new incarnation of the sbt-compiler-crash phenomenon cost me a few
>> > > hours of running `./dev/run-tests` in different ways before deciding
>> > that,
>> > > as usual, there was no way around it and that I'd need to run `mvn
>> clean`
>> > > and start running tests from scratch.
>> > > - The YarnClusterSuite failures on my linux box have cost me hours of
>> > > trying to figure out whether they're my fault. I've seen them many
>> times
>> > > over the past weeks/months, plus or minus other failures that have come
>> > and
>> > > gone, and was especially befuddled by them when I was seeing a disjoint
>> > set
>> > > of reproducible failures on my mac [19] (the triaging of which involved
>> > > dozens of runs of `./dev/run-tests`).
>> > >
>> > > While I'm interested in digging into each of these issues, I also want
>> to
>> > > discuss the frequency with which I've run into issues like these. This
>> is
>> > > unfortunately not the first time in recent months that I've spent days
>> > > playing spurious-test-failure whack-a-mole with a 60-90min
>> dev/run-tests
>> > > iteration time, which is no fun! So I am wondering/thinking:
>> > >
>> > > - Do other people experience this level of flakiness from spark tests?
>> > > - Do other people bother running dev/run-tests locally, or just let
>> > Jenkins
>> > > do it during the CR process?
>> > > - Needing to run a full assembly post-clean just to continue running
>> one
>> > > specific test case feels especially wasteful, and the failure output
>> when
>> > > naively attempting to run a specific test without having built an
>> > assembly
>> > > jar is not always clear about what the issue is or how to fix it; even
>> > the
>> > > fact that certain tests require "building the world" is not something I
>> > > would have expected, and has cost me hours of confusion.
>> > >    - Should a person running spark tests assume that they must build an
>> > > assembly JAR before running anything?
>> > >    - Are there some proper "unit" tests that are actually
>> self-contained
>> > /
>> > > able to be run without building an assembly jar?
>> > >    - Can we better document/demarcate which tests have which
>> > dependencies?
>> > >    - Is there something finer-grained than building an assembly JAR
>> that
>> > > is sufficient in some cases?
>> > >        - If so, can we document that?
>> > >        - If not, can we move to a world of finer-grained dependencies
>> for
>> > > some of these?
>> > > - Leaving all of these spurious failures aside, the process of
>> assembling
>> > > and testing a new JAR is not a quick one (40 and 60 mins for me
>> > typically,
>> > > respectively). I would guess that there are dozens (hundreds?) of
>> people
>> > > who build a Spark assembly from various ToTs on any given day, and who
>> > all
>> > > wait on the exact same compilation / assembly steps to occur. Expanding
>> > on
>> > > the recent work to publish nightly snapshots [20], can we do a better
>> job
>> > > caching/sharing compilation artifacts at a more granular level
>> (pre-built
>> > > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA?
>> more
>> > > granular maven modules, plus the previous two?), or otherwise save some
>> > of
>> > > the considerable amount of redundant compilation work that I had to do
>> > over
>> > > the course of my odyssey this weekend?
>> > >
>> > > Ramping up on most projects involves some amount of supplementing the
>> > > documentation with trial and error to figure out what to run, which
>> > > "errors" are real errors and which can be ignored, etc., but navigating
>> > > that minefield on Spark has proved especially challenging and
>> > > time-consuming for me. Some of that comes directly from scala's
>> > relatively
>> > > slow compilation times and immature build-tooling ecosystem, but that
>> is
>> > > the world we live in and it would be nice if Spark took the alleviation
>> > of
>> > > the resulting pain more seriously, as one of the more interesting and
>> > > well-known large scala projects around right now. The official
>> > > documentation around how to build different subsets of the codebase is
>> > > somewhat sparse [21], and there have been many mixed [22] accounts [23]
>> > on
>> > > this mailing list about preferred ways to build on mvn vs. sbt (none of
>> > > which has made it into official documentation, as far as I've seen).
>> > > Expecting new contributors to piece together all of this received
>> > > folk-wisdom about how to build/test in a sane way by trawling mailing
>> > list
>> > > archives seems suboptimal.
>> > >
>> > > Thanks for reading, looking forward to hearing your ideas!
>> > >
>> > > -Ryan
>> > >
>> > > P.S. Is "best practice" for emailing this list to not incorporate any
>> > HTML
>> > > in the body? It seems like all of the archives I've seen strip it out,
>> > but
>> > > other people have used it and gmail displays it.
>> > >
>> > >
>> > > [1]
>> > >
>> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>> raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
>> > > (57 mins)
>> > > [2]
>> > >
>> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>> raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
>> > > (6 mins)
>> > > [3]
>> > >
>> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>> raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%
>> 20pass%20test,%20fail%20subsequent%20compile
>> > > (4 mins)
>> > > [4]
>> > >
>> > https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&
>> cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-
>> list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-
>> DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-
>> iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=
>> zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
>> > > [5]
>> > >
>> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>> raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%
>> 20clean,%20need%20dependencies%20built
>> > > [6]
>> > >
>> > https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/
>> raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%
>> 20post%20clean
>> > > (50 mins)
>> > > [7]
>> > >
>> > https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#
>> file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
>> > > (1hr)
>> > > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
>> > > [9] https://issues.apache.org/jira/browse/SPARK-3867
>> > > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
>> > > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
>> > > [12]
>> > >
>> > https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#
>> file-gistfile1-txt-L853
>> > > (~90 mins)
>> > > [13]
>> > >
>> > https://gist.github.com/ryan-williams/718f6324af358819b496#
>> file-gistfile1-txt-L852
>> > > (91 mins)
>> > > [14]
>> > >
>> > https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#
>> file-gistfile1-txt-L854
>> > > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
>> > > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
>> > > [17]
>> > >
>> > http://apache-spark-user-list.1001560.n3.nabble.com/quot-
>> Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
>> > > [18]
>> > >
>> > http://stackoverflow.com/questions/25707629/why-does-
>> spark-job-fail-with-too-many-open-files
>> > > [19] https://issues.apache.org/jira/browse/SPARK-4002
>> > > [20] https://issues.apache.org/jira/browse/SPARK-4542
>> > > [21]
>> > >
>> > https://spark.apache.org/docs/latest/building-with-maven.
>> html#spark-tests-in-maven
>> > > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
>> > > [23]
>> > >
>> > http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%
>> 3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
>> >
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 23:42:23 2014
Return-Path: <dev-return-10631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 468E110A16
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 23:42:23 +0000 (UTC)
Received: (qmail 46662 invoked by uid 500); 2 Dec 2014 23:42:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46589 invoked by uid 500); 2 Dec 2014 23:42:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46577 invoked by uid 99); 2 Dec 2014 23:42:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 23:42:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.181 as permitted sender)
Received: from [209.85.160.181] (HELO mail-yk0-f181.google.com) (209.85.160.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 23:42:17 +0000
Received: by mail-yk0-f181.google.com with SMTP id 142so6351959ykq.12
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 15:39:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=oa2a1YSVMa0GlnzBgz2tqCQaArIldUFRlQkuAt26qYE=;
        b=Oiq4fQVinzXWUVqjuWRatJ9qfMz4bjzWnwFGIGqyx9nHM14YNidY9efZXMZ4nZP233
         wSlzrviOxjK8xnc1ePPob6A7orXgZc01Uu8i64DelZBVXd8jv49S4whg71K8L11O7G1F
         rv5IX+04zrcokNBF0IGwFEsWLU9wkcz1767begLkU5xqtEF9ktqUwQgaNwogVUDdLzWK
         JfqwelgTYPhhAoC+mX2FE/1xBYD5cDpa8am0tj73nFnFDN5ldQ+3wG1At/wt6GyYAMeK
         SWJps6b4u82096ylZIFH6SwwEfAe8XUheTCW0o04j66iN0HqUexSIP+YAHaXIo0Xnd19
         iHoA==
X-Received: by 10.236.222.74 with SMTP id s70mr2743627yhp.88.1417563581894;
 Tue, 02 Dec 2014 15:39:41 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
 <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com> <CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Tue, 02 Dec 2014 23:39:40 +0000
Message-ID: <CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Mark Hamstra <mark@clearstorydata.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01681e44bed9020509443dd5
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01681e44bed9020509443dd5
Content-Type: text/plain; charset=UTF-8

Marcelo: by my count, there are 19 maven modules in the codebase. I am
typically only concerned with "core" (and therefore its two dependencies as
well, `network/{shuffle,common}`).

The `mvn package` workflow (and its sbt equivalent) that most people
apparently use involves (for me) compiling+packaging 16 other modules that
I don't care about; I pay this cost whenever I rebase off of master or
encounter the sbt-compiler-crash bug, among other possible scenarios.

Compiling one module (after building/installing its dependencies) seems
like the sort of thing that should be possible, and I don't see why my
previously-documented attempt is failing.

re: Marcelo's comment about "missing the 'spark-parent' project", I saw
that error message too and tried to ascertain what it could mean. Why would
`network/shuffle` need something from the parent project? AFAICT
`network/common` has the same references to the parent project as
`network/shuffle` (namely just a <parent> block in its POM), and yet I can
`mvn install -pl` the former but not the latter. Why would this be? One
difference is that `network/shuffle` has a dependency on another module,
while `network/common` does not.

Does Maven not let you build modules that depend on *any* other modules
without building *all* modules, or is there a way to do this that we've not
found yet?

Patrick: per my response to Marcelo above, I am trying to avoid having to
compile and package a bunch of stuff I am not using, which both `mvn
package` and `mvn install` on the parent project do.





On Tue Dec 02 2014 at 3:45:48 PM Marcelo Vanzin <vanzin@cloudera.com> wrote:

> On Tue, Dec 2, 2014 at 2:40 PM, Ryan Williams
> <ryan.blake.williams@gmail.com> wrote:
> > Following on Mark's Maven examples, here is another related issue I'm
> > having:
> >
> > I'd like to compile just the `core` module after a `mvn clean`, without
> > building an assembly JAR first. Is this possible?
>
> Out of curiosity, may I ask why? What's the problem with running "mvn
> install -DskipTests" first (or "package" instead of "install",
> although I generally do the latter)?
>
> You can probably do what you want if you manually build / install all
> the needed dependencies first; you found two, but it seems you're also
> missing the "spark-parent" project (which is the top-level pom). That
> sounds like a lot of trouble though, for not any gains that I can
> see... after the first build you should be able to do what you want
> easily.
>
> --
> Marcelo
>

--089e01681e44bed9020509443dd5--

From dev-return-10632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  2 23:46:47 2014
Return-Path: <dev-return-10632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A452010A49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Dec 2014 23:46:47 +0000 (UTC)
Received: (qmail 58477 invoked by uid 500); 2 Dec 2014 23:46:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58397 invoked by uid 500); 2 Dec 2014 23:46:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58386 invoked by uid 99); 2 Dec 2014 23:46:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 23:46:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Dec 2014 23:46:20 +0000
Received: by mail-qc0-f172.google.com with SMTP id m20so10374556qcx.31
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 15:46:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=zmkSJVjMtfuuONI/nG68ESjHtlt6Zr80OzQe2eNFwss=;
        b=GM1ArHSVk9TRzoLCpnhj3wxKyrcU5Hk4EHrx3ZB3+fXaPuztSLGRof6OAoGHR+i7R6
         sODZSwF6GyEYn0SG71yQ6u366y5ceTByN4gphw5zjdbqKSAK6qwvcmvqhHAPP7ZHsFco
         LFItw17J0JYv0dL40pgH2RM6QPVnbPYHOZl7g8CnnSBo1MD0E60ykckVn704RQnMpCAD
         P8RuQ/sewFrCYbnGhqf6d0uq291h4qp53es+t0fdv9Ya9gcB691RBtnGY1YJZ4hSJMks
         SCThww80qldXD+qJrcTnzF07dG3yRWqzvSryx9ukGjkBCcLVrMjHK85H0Itm8buAPXeE
         cFuw==
X-Gm-Message-State: ALoCoQnR/Apd5QKsjBpeNK/EBb1g+V0zdMtKzhJqamTyd7gk0eHduYJeSdB5mOpPB8hCVEzaEVr/
MIME-Version: 1.0
X-Received: by 10.140.25.179 with SMTP id 48mr3042982qgt.24.1417563979218;
 Tue, 02 Dec 2014 15:46:19 -0800 (PST)
Received: by 10.229.15.202 with HTTP; Tue, 2 Dec 2014 15:46:19 -0800 (PST)
In-Reply-To: <CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
	<CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
	<CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
	<CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com>
Date: Tue, 2 Dec 2014 15:46:19 -0800
Message-ID: <CAAOnQ7vggDLGyH4oVpZw9UzgcAs8JmwVymOe2dTc0HxijQ0rrw@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, Dec 2, 2014 at 3:39 PM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
> Marcelo: by my count, there are 19 maven modules in the codebase. I am
> typically only concerned with "core" (and therefore its two dependencies as
> well, `network/{shuffle,common}`).

But you only need to compile the others once. Once you've established
the baseline, you can just compile / test "core" to your heart's
desire. Core tests won't even run until you build the assembly anyway,
since some of them require the assembly to be present.

Also, even if you work in core - I'd say especially if you work in
core - you should still, at some point, compile and test everything
else that depends on it.

So, do this ONCE:

  mvn install -DskipTests

Then do this as many times as you want:

  mvn -pl spark-core_2.10 something

That doesn't seem too bad to me. (Be aware of the "assembly" comment
above, since testing spark-core means you may have to rebuild the
assembly from time to time, if your changes affect those tests.)

> re: Marcelo's comment about "missing the 'spark-parent' project", I saw that
> error message too and tried to ascertain what it could mean. Why would
> `network/shuffle` need something from the parent project?

The "spark-parent" project is the main pom that defines dependencies
and their version, along with lots of build plugins and
configurations. It's needed by all modules to compile correctly.

-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 00:40:51 2014
Return-Path: <dev-return-10633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D531410CCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 00:40:51 +0000 (UTC)
Received: (qmail 52369 invoked by uid 500); 3 Dec 2014 00:40:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52295 invoked by uid 500); 3 Dec 2014 00:40:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52280 invoked by uid 99); 3 Dec 2014 00:40:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 00:40:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.213.45 as permitted sender)
Received: from [209.85.213.45] (HELO mail-yh0-f45.google.com) (209.85.213.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 00:40:43 +0000
Received: by mail-yh0-f45.google.com with SMTP id f10so6623791yha.32
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 16:40:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=0oj/Y8injqLNkmgxrLwHXAeIVBGLvV/EAZnabwpe7Rs=;
        b=VgT6mT83kkKfFFezc2wpJ9hHYNLlu0lgJ6UeRWr6B0cM7vZl9NFAbiqWFQsbsixZsh
         vyTqcGPQxIyU8fO0A3NA8tLpdutAJB96/fgvBPoCVQMoKC2VdDRlxza1fOpNJw2vWAJr
         fEwIiWwbV3D2vtZgLFdKjOYezsDTJLv8H/bn7mzz5dRbY9zTsIcoErwmwXq/811e3hRz
         +VjlgqfDMOFd14PAJxGDzgXVrGqKHQB1NdxjTNANNIJHX4S5JxOgCF9l2WxVyN012A28
         LVY+Oe9fXXCEG2G8iLMYsvKo9b2LmlURbLIEDdmztkX2RruMVuIxpnK2nZlAMfWKcqcf
         rugg==
X-Received: by 10.170.98.135 with SMTP id p129mr3097109yka.101.1417567223106;
 Tue, 02 Dec 2014 16:40:23 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
 <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
 <CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
 <CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com> <CAAOnQ7vggDLGyH4oVpZw9UzgcAs8JmwVymOe2dTc0HxijQ0rrw@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Wed, 03 Dec 2014 00:40:20 +0000
Message-ID: <CANeJXFP7x2WSVVqGJKSAYRjGOXowGNKn2bVxHCh7QLsL8oh4jg@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a7e5ec7569305094516ce
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a7e5ec7569305094516ce
Content-Type: text/plain; charset=UTF-8

On Tue Dec 02 2014 at 4:46:20 PM Marcelo Vanzin <vanzin@cloudera.com> wrote:

> On Tue, Dec 2, 2014 at 3:39 PM, Ryan Williams
> <ryan.blake.williams@gmail.com> wrote:
> > Marcelo: by my count, there are 19 maven modules in the codebase. I am
> > typically only concerned with "core" (and therefore its two dependencies
> as
> > well, `network/{shuffle,common}`).
>
> But you only need to compile the others once.


once... every time I rebase off master, or am obliged to `mvn clean` by
some other build-correctness bug, as I said before. In my experience this
works out to a few times per week.


> Once you've established
> the baseline, you can just compile / test "core" to your heart's
> desire.


I understand that this is a workflow that does what I want as a side effect
of doing 3-5x more work (depending whether you count [number of modules
built] or [lines of scala/java compiled]), none of the extra work being
useful to me (more on that below).


> Core tests won't even run until you build the assembly anyway,
> since some of them require the assembly to be present.


The tests you refer to are exactly the ones that I'd like to let Jenkins
run from here on out, per advice I was given elsewhere in this thread and
due to the myriad unpleasantries I've encountered in trying to run them
myself.


>
> Also, even if you work in core - I'd say especially if you work in
> core - you should still, at some point, compile and test everything
> else that depends on it.
>

Last response applies.


>
> So, do this ONCE:
>

again, s/ONCE/several times a week/, in my experience.


>
>   mvn install -DskipTests
>
> Then do this as many times as you want:
>
>   mvn -pl spark-core_2.10 something
>
> That doesn't seem too bad to me.

(Be aware of the "assembly" comment
> above, since testing spark-core means you may have to rebuild the
> assembly from time to time, if your changes affect those tests.)
>
> > re: Marcelo's comment about "missing the 'spark-parent' project", I saw
> that
> > error message too and tried to ascertain what it could mean. Why would
> > `network/shuffle` need something from the parent project?
>
> The "spark-parent" project is the main pom that defines dependencies
> and their version, along with lots of build plugins and
> configurations. It's needed by all modules to compile correctly.
>

- I understand the parent POM has that information.

- I don't understand why Maven would feel that it is unable to compile the
`network/shuffle` module without having first compiled, packaged, and
installed 17 modules (19 minus `network/shuffle` and its dependency
`network/common`) that are not transitive dependencies of `network/shuffle`.

- I am trying to understand whether my failure to get Maven to compile
`network/shuffle` stems from my not knowing the correct incantation to feed
to Maven or from Maven's having a different (and seemingly worse) model for
how it handles module dependencies than I expected.



>
> --
> Marcelo
>

--001a113a7e5ec7569305094516ce--

From dev-return-10634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 00:51:11 2014
Return-Path: <dev-return-10634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0D49D10D0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 00:51:11 +0000 (UTC)
Received: (qmail 69069 invoked by uid 500); 3 Dec 2014 00:51:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68989 invoked by uid 500); 3 Dec 2014 00:51:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68977 invoked by uid 99); 3 Dec 2014 00:51:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 00:51:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 00:50:43 +0000
Received: by mail-qa0-f44.google.com with SMTP id i13so9744521qae.31
        for <dev@spark.apache.org>; Tue, 02 Dec 2014 16:49:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=RBqU1CZD6thlldCdKHKDa2GKzTn4YKE9WmhiTAMiqCg=;
        b=KcNVTS/u1mkUuVF24sggdzIuwzdPvBtQmjq6zU/vw27ngQdnyHdyKZGo4JjvK2VocD
         mTJTCABOJS6Q15m+SRidYCncsDVHNnvWnoOZ+u6ZYZ2rRrTpWCG8dPsFIlIHiUKMXKOF
         Ro/pWBsZ+83XSPZV2Gb3lNNl3IV3rXRLpzCrXXuC9AHKNLFvi/g8ywhHOItqTxz2KjCQ
         MZtbtq0FzFVQDRGm+Fz/0DUwug7GvKy2yszFLz++UVFk4+l6j3+eJ9/FPaK6Ll4giaMb
         R3AnBD2sTz4JztPk7z7uV6BsT0VavIIywKJws3xftJK7vPx07TI7T7DBrjabIXVycYmL
         y4JQ==
X-Gm-Message-State: ALoCoQkgmX+XBOfoBdcYlxoMiA4y+uVX37+q3aFpFDnB/gKdmkZQtWyigB0AQE8Wy97nOy8Fq5rf
MIME-Version: 1.0
X-Received: by 10.140.25.179 with SMTP id 48mr3447251qgt.24.1417567797072;
 Tue, 02 Dec 2014 16:49:57 -0800 (PST)
Received: by 10.229.15.202 with HTTP; Tue, 2 Dec 2014 16:49:56 -0800 (PST)
In-Reply-To: <CANeJXFP7x2WSVVqGJKSAYRjGOXowGNKn2bVxHCh7QLsL8oh4jg@mail.gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
	<CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
	<CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
	<CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
	<CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com>
	<CAAOnQ7vggDLGyH4oVpZw9UzgcAs8JmwVymOe2dTc0HxijQ0rrw@mail.gmail.com>
	<CANeJXFP7x2WSVVqGJKSAYRjGOXowGNKn2bVxHCh7QLsL8oh4jg@mail.gmail.com>
Date: Tue, 2 Dec 2014 16:49:56 -0800
Message-ID: <CAAOnQ7undG8AHGGSFRBu=B3BQB7DahZVViNOTLc=Q_BNjb_W4Q@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, Dec 2, 2014 at 4:40 PM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
>> But you only need to compile the others once.
>
> once... every time I rebase off master, or am obliged to `mvn clean` by some
> other build-correctness bug, as I said before. In my experience this works
> out to a few times per week.

No, you only need to do it something upstream from core changed (i.e.,
spark-parent, network/common or network/shuffle) in an incompatible
way. Otherwise, you can rebase and just recompile / retest core,
without having to install everything else. I do this kind of thing all
the time. If you have to do "mvn clean" often you're probably doing
something wrong somewhere else.

I understand where you're coming from, but the way you're thinking is
just not how maven works. I too find annoying that maven requires lots
of things to be "installed" before you can use them, when they're all
part of the same project. But well, that's the way things are.

-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 01:38:19 2014
Return-Path: <dev-return-10635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75F0910E78
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 01:38:19 +0000 (UTC)
Received: (qmail 56069 invoked by uid 500); 3 Dec 2014 01:38:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55992 invoked by uid 500); 3 Dec 2014 01:38:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55981 invoked by uid 99); 3 Dec 2014 01:38:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 01:38:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.171] (HELO mail-lb0-f171.google.com) (209.85.217.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 01:38:13 +0000
Received: by mail-lb0-f171.google.com with SMTP id n15so11610867lbi.30
        for <dev@spark.incubator.apache.org>; Tue, 02 Dec 2014 17:37:51 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=gCjVb+bI5U/jvxbRHSiokMiot3FrkS/t+vIyfzYwito=;
        b=WnmaruuPagkEAjA1U8u0R7swrU/u6Mrc84ydGuE67lWSiy8zosgY4h4Nuolzmcys8R
         i3vjHsFPBN38s8PmOvmSClYi29HIABQGOwX7z5jakGOghlxKaOt4ro5FTEcwTjSFmpAF
         QSWwbBrwLIIcFNNBQUhYHq4uXcqaAmPvAQhHNbIdRvZj/ckx8peHTJHSeyE47qAAfGnH
         tu31MM9FdltUanISQ1W57gNiX3u4WUT1J0v9InfcaoFwF2hHQ6Dbz6//Fz1Rm6w8395D
         RZZfJy3sDMnfcb34HJDwsj3gaGDwH0bD6JDQPAD8yyr6iK6wIO8mBuXL+cEjoY9vboSz
         B7GQ==
X-Gm-Message-State: ALoCoQmlQJQQYppPlvjhVetiXdmV5UpAzzJtyYx8LuQZBZdhGSONldN/ZRLpfkBfrCHcMn5//Uva
X-Received: by 10.152.238.1 with SMTP id vg1mr1864809lac.83.1417570671796;
 Tue, 02 Dec 2014 17:37:51 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Tue, 2 Dec 2014 17:37:31 -0800 (PST)
In-Reply-To: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
References: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 2 Dec 2014 17:37:31 -0800
Message-ID: <CAAswR-5Qu2kj=EFoz4uT+PdDGK2Tsi4N1WarVS=XDami8iWcgA@mail.gmail.com>
Subject: Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe
To: Yana Kadiyska <yana.kadiyska@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1134930e563873050945e43d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134930e563873050945e43d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

 In Hive 13 (which is the default for Spark 1.2), parquet is included and
thus we no longer include the Hive parquet bundle. You can now use the
included
ParquetSerDe: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe

If you want to compile Spark 1.2 with Hive 12 instead you can pass
-Phive-0.12.0 and  parquet.hive.serde.ParquetHiveSerDe will be included as
before.

Michael

On Tue, Dec 2, 2014 at 9:31 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
wrote:

> Apologies if people get this more than once -- I sent mail to dev@spark
> last night and don't see it in the archives. Trying the incubator list
> now...wanted to make sure it doesn't get lost in case it's a bug...
>
> ---------- Forwarded message ----------
> From: Yana Kadiyska <yana.kadiyska@gmail.com>
> Date: Mon, Dec 1, 2014 at 8:10 PM
> Subject: [Thrift,1.2 RC] what happened to
> parquet.hive.serde.ParquetHiveSerDe
> To: dev@spark.apache.org
>
>
> Hi all, apologies if this is not a question for the dev list -- figured
> User list might not be appropriate since I'm having trouble with the RC
> tag.
>
> I just tried deploying the RC and running ThriftServer. I see the followi=
ng
> error:
>
> 14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionException
> as:anonymous (auth:SIMPLE)
> cause:org.apache.hive.service.cli.HiveSQLException:
> java.lang.RuntimeException:
> MetaException(message:java.lang.ClassNotFoundException Class
> parquet.hive.serde.ParquetHiveSerDe not found)
> 14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
> org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException:
> MetaException(message:java.lang.ClassNotFoundException Class
> parquet.hive.serde.ParquetHiveSerDe not found)
> at
>
> org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run=
(Shim13.scala:192)
> at
>
> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInter=
nal(HiveSessionImpl.java:231)
> at
>
> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(Hive=
SessionImpl.java:212)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> at
>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at
>
> org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionPr=
oxy.java:79)
> at
>
> org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessi=
onProxy.java:37)
> at
>
> org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionPro=
xy.java:64)
> at java.security.AccessController.doPrivileged(Native Method)
> at javax.security.auth.Subject.doAs(Subject.java:415)
> =E2=80=8B
>
>
> I looked at a working installation that I have(build master a few weeks
> ago) and this class used to be included in spark-assembly:
>
> ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
> Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
> matches
>
> but with the RC build it's not there?
>
> I tried both the prebuilt CDH drop and later manually built the tag with
> the following command:
>
>  ./make-distribution.sh --tgz -Phive -Dhadoop.version=3D2.0.0-mr1-cdh4.2.=
0
> -Phive-thriftserver
> $JAVA_HOME/bin/jar -tvf spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.jar
> |grep parquet.hive.serde.ParquetHiveSerDe
>
> comes back empty...
>

--001a1134930e563873050945e43d--

From dev-return-10636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 03:24:01 2014
Return-Path: <dev-return-10636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8421110136
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 03:24:01 +0000 (UTC)
Received: (qmail 16926 invoked by uid 500); 3 Dec 2014 03:24:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16844 invoked by uid 500); 3 Dec 2014 03:24:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16833 invoked by uid 99); 3 Dec 2014 03:23:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 03:23:59 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of m_qiu@msn.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 03:23:33 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id B59ADC049A7
	for <dev@spark.incubator.apache.org>; Tue,  2 Dec 2014 19:22:32 -0800 (PST)
Date: Tue, 2 Dec 2014 20:22:32 -0700 (MST)
From: flyson <m_qiu@msn.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417576952411-9619.post@n3.nabble.com>
Subject: object xxx is not a member of package com
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello everyone,

Could anybody tell me how to import and call the 3rd party java classes from
inside spark?
Here's my case:
I have a jar file (the directory layout is com.xxx.yyy.zzz) which contains
some java classes, and I need to call some of them in spark code.
I used the statement "import com.xxx.yyy.zzz._" on top of the impacted spark
file and set the location of the jar file in the CLASSPATH environment, and
use ".sbt/sbt assembly" to build the project. As a result, I got an error
saying "object xxx is not a member of package com".

I thought that could be related to the library dependencies, but couldn't
figure it out. Any suggestion/solution from you would be appreciated!

By the way in the scala console, if the :cp is used to point to the jar
file, I can import the classes from the jar file.

Thanks! 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/object-xxx-is-not-a-member-of-package-com-tp9619.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 04:30:51 2014
Return-Path: <dev-return-10637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 989ED104EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 04:30:51 +0000 (UTC)
Received: (qmail 42283 invoked by uid 500); 3 Dec 2014 04:30:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42202 invoked by uid 500); 3 Dec 2014 04:30:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42101 invoked by uid 99); 3 Dec 2014 04:30:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 04:30:50 +0000
X-ASF-Spam-Status: No, hits=-2.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of daoyuan.wang@intel.com designates 134.134.136.65 as permitted sender)
Received: from [134.134.136.65] (HELO mga03.intel.com) (134.134.136.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 04:30:45 +0000
Received: from orsmga001.jf.intel.com ([10.7.209.18])
  by orsmga103.jf.intel.com with ESMTP; 02 Dec 2014 20:26:02 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,505,1413270000"; 
   d="scan'208";a="617711317"
Received: from pgsmsx107.gar.corp.intel.com ([10.221.44.105])
  by orsmga001.jf.intel.com with ESMTP; 02 Dec 2014 20:29:24 -0800
Received: from shsmsx151.ccr.corp.intel.com (10.239.6.50) by
 PGSMSX107.gar.corp.intel.com (10.221.44.105) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Wed, 3 Dec 2014 12:29:23 +0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.110]) by
 SHSMSX151.ccr.corp.intel.com ([169.254.3.86]) with mapi id 14.03.0195.001;
 Wed, 3 Dec 2014 12:29:22 +0800
From: "Wang, Daoyuan" <daoyuan.wang@intel.com>
To: flyson <m_qiu@msn.com>, "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>
Subject: RE: object xxx is not a member of package com
Thread-Topic: object xxx is not a member of package com
Thread-Index: AQHQDqiewqwdrOJrxEqfSc4CeDhE6Zx9RYog
Date: Wed, 3 Dec 2014 04:29:21 +0000
Message-ID: <CF8AD92F21C4704CAD056AADDD3CCC09012E633C@SHSMSX101.ccr.corp.intel.com>
References: <1417576952411-9619.post@n3.nabble.com>
In-Reply-To: <1417576952411-9619.post@n3.nabble.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

I think you can place the jar in lib/ in SPARK_HOME, and then compile witho=
ut any change to your class path. This could be a temporary way to include =
your jar. You can also put them in your pom.xml.

Thanks,
Daoyuan

-----Original Message-----
From: flyson [mailto:m_qiu@msn.com]=20
Sent: Wednesday, December 03, 2014 11:23 AM
To: dev@spark.incubator.apache.org
Subject: object xxx is not a member of package com

Hello everyone,

Could anybody tell me how to import and call the 3rd party java classes fro=
m inside spark?
Here's my case:
I have a jar file (the directory layout is com.xxx.yyy.zzz) which contains =
some java classes, and I need to call some of them in spark code.
I used the statement "import com.xxx.yyy.zzz._" on top of the impacted spar=
k file and set the location of the jar file in the CLASSPATH environment, a=
nd use ".sbt/sbt assembly" to build the project. As a result, I got an erro=
r saying "object xxx is not a member of package com".

I thought that could be related to the library dependencies, but couldn't f=
igure it out. Any suggestion/solution from you would be appreciated!

By the way in the scala console, if the :cp is used to point to the jar fil=
e, I can import the classes from the jar file.

Thanks!=20



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/object-xxx-is-not-a-member-of-package-com-tp9619.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional com=
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 15:10:31 2014
Return-Path: <dev-return-10638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 96FC210A36
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 15:10:31 +0000 (UTC)
Received: (qmail 78871 invoked by uid 500); 3 Dec 2014 15:10:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78799 invoked by uid 500); 3 Dec 2014 15:10:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78786 invoked by uid 99); 3 Dec 2014 15:10:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 15:10:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yana.kadiyska@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 15:10:26 +0000
Received: by mail-wi0-f182.google.com with SMTP id h11so24747749wiw.3
        for <dev@spark.incubator.apache.org>; Wed, 03 Dec 2014 07:09:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:in-reply-to:references:date:message-id
         :subject:from:to:cc:content-type;
        bh=4BcDrEkiPsKLpGoqXvyoWEbggzLNC+/6McHG0rOS7cc=;
        b=tLzuRgMVHJ86dlCyx/Bw5ZMUyLEiv4EGYCn8IIRmCQw5DBA4f5vt+ocyqMz8HYlX/M
         SfpU6wZQKrH8CVLApILVcPXoC2pl/HHoO6ANDobT/NgrFPgwFn/xzMzM9wca/pTHSldn
         euRkNgRjgeZsVFwYHgPsZ42j3l43xrJakDzuKqm0hfVwXZ3P1uv8LN/9LRVyM540zY5x
         FdQ7oTrEPf8TsYcru+k6JoQGYg5SYMS3eVqLiVucFKf0GGlz42ICwNKNi+Z5xcSciMg0
         VOXslJB38GigT4K4ASYMABUZYcjZgxUHVR6ES+refYISSlZmOxHWhsS2icejLjDi4kx8
         A2DA==
MIME-Version: 1.0
X-Received: by 10.194.20.98 with SMTP id m2mr8243678wje.52.1417619359907; Wed,
 03 Dec 2014 07:09:19 -0800 (PST)
Received: by 10.216.212.199 with HTTP; Wed, 3 Dec 2014 07:09:19 -0800 (PST)
Reply-To: yana.kadiyska@gmail.com
In-Reply-To: <CAAswR-5Qu2kj=EFoz4uT+PdDGK2Tsi4N1WarVS=XDami8iWcgA@mail.gmail.com>
References: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
	<CAAswR-5Qu2kj=EFoz4uT+PdDGK2Tsi4N1WarVS=XDami8iWcgA@mail.gmail.com>
Date: Wed, 3 Dec 2014 10:09:19 -0500
Message-ID: <CAJ4HpHEgppwd0J+pJouG5PMDdXrkXHSq6weYYdoHOU04veLs+A@mail.gmail.com>
Subject: Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe
From: Yana Kadiyska <yana.kadiyska@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d45805fc4580509513a70
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d45805fc4580509513a70
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks Michael, you are correct.

I also opened https://issues.apache.org/jira/browse/SPARK-4702 -- if
someone can comment on why this might be happening that would be great.
This would be a blocker to me using 1.2 and it used to work so I'm a bit
puzzled. I was hoping that it's again a result of the default profile
switch but it didn't seem to be the case

(ps. please advise if this is more user-list appropriate. I'm posting to
dev as it's an RC)

On Tue, Dec 2, 2014 at 8:37 PM, Michael Armbrust <michael@databricks.com>
wrote:

> In Hive 13 (which is the default for Spark 1.2), parquet is included and
> thus we no longer include the Hive parquet bundle. You can now use the
> included
> ParquetSerDe: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
>
> If you want to compile Spark 1.2 with Hive 12 instead you can pass
> -Phive-0.12.0 and  parquet.hive.serde.ParquetHiveSerDe will be included a=
s
> before.
>
> Michael
>
> On Tue, Dec 2, 2014 at 9:31 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
> wrote:
>
>> Apologies if people get this more than once -- I sent mail to dev@spark
>> last night and don't see it in the archives. Trying the incubator list
>> now...wanted to make sure it doesn't get lost in case it's a bug...
>>
>> ---------- Forwarded message ----------
>> From: Yana Kadiyska <yana.kadiyska@gmail.com>
>> Date: Mon, Dec 1, 2014 at 8:10 PM
>> Subject: [Thrift,1.2 RC] what happened to
>> parquet.hive.serde.ParquetHiveSerDe
>> To: dev@spark.apache.org
>>
>>
>> Hi all, apologies if this is not a question for the dev list -- figured
>> User list might not be appropriate since I'm having trouble with the RC
>> tag.
>>
>> I just tried deploying the RC and running ThriftServer. I see the
>> following
>> error:
>>
>> 14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionException
>> as:anonymous (auth:SIMPLE)
>> cause:org.apache.hive.service.cli.HiveSQLException:
>> java.lang.RuntimeException:
>> MetaException(message:java.lang.ClassNotFoundException Class
>> parquet.hive.serde.ParquetHiveSerDe not found)
>> 14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
>> org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException=
:
>> MetaException(message:java.lang.ClassNotFoundException Class
>> parquet.hive.serde.ParquetHiveSerDe not found)
>> at
>>
>> org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.ru=
n(Shim13.scala:192)
>> at
>>
>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInte=
rnal(HiveSessionImpl.java:231)
>> at
>>
>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(Hiv=
eSessionImpl.java:212)
>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> at
>>
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav=
a:57)
>> at
>>
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor=
Impl.java:43)
>> at java.lang.reflect.Method.invoke(Method.java:606)
>> at
>>
>> org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionP=
roxy.java:79)
>> at
>>
>> org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSess=
ionProxy.java:37)
>> at
>>
>> org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionPr=
oxy.java:64)
>> at java.security.AccessController.doPrivileged(Native Method)
>> at javax.security.auth.Subject.doAs(Subject.java:415)
>> =E2=80=8B
>>
>>
>> I looked at a working installation that I have(build master a few weeks
>> ago) and this class used to be included in spark-assembly:
>>
>> ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
>> Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
>> matches
>>
>> but with the RC build it's not there?
>>
>> I tried both the prebuilt CDH drop and later manually built the tag with
>> the following command:
>>
>>  ./make-distribution.sh --tgz -Phive -Dhadoop.version=3D2.0.0-mr1-cdh4.2=
.0
>> -Phive-thriftserver
>> $JAVA_HOME/bin/jar -tvf spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.ja=
r
>> |grep parquet.hive.serde.ParquetHiveSerDe
>>
>> comes back empty...
>>
>
>

--047d7b5d45805fc4580509513a70--

From dev-return-10639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec  3 15:27:05 2014
Return-Path: <dev-return-10639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 95EC910AB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  3 Dec 2014 15:27:05 +0000 (UTC)
Received: (qmail 6338 invoked by uid 500); 3 Dec 2014 15:27:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6263 invoked by uid 500); 3 Dec 2014 15:27:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6251 invoked by uid 99); 3 Dec 2014 15:27:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 15:27:03 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Brennon.York@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail02.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 15:26:38 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=2908; q=dns/txt; s=SM2048Apr2013K;
  t=1417620418; x=1417706818;
  h=from:to:date:subject:message-id:references:in-reply-to:
   mime-version:content-transfer-encoding;
  bh=ZBRY7azQ0JHSy5vPw55U8+K/E1fqHQxV+0X+UAEXniI=;
  b=tWRcpakj1+F9P4/hwZEgVoirPlM64mT0bosDgJhkBhETbUGLC9vQDcYt
   +iCoRN0WwAUgNP+LqVakwLu9YurRdwdBIfMdBteOKfyvjnu+781LLxyNv
   OqvPg5B8Sff6JMfl6TINhqq2ggFWQol6GSOqWSrG7mca4yr6FA+73uqg8
   oKtpsS8WwYOADTZHv7gmBBknNrZEGsq+6o9XPVR8zfyEUMyaR+mUIxLeg
   oF9XKPr4c6Qu7UdRL3UPwni1Q0ZJ60dC9JKDPWCBIVrVTfa0cH5mWXoVt
   E1UURwVlVRvqyhqLnQuSmeCfol/LV1bP3hmtNuOpzQtvj+wtwJOwge9s3
   A==;
X-IronPort-AV: E=McAfee;i="5600,1067,7640"; a="184591763"
X-IronPort-AV: E=Sophos;i="5.07,508,1413259200"; 
   d="scan'208";a="184591763"
Received: from kdcpexcasht02.cof.ds.capitalone.com ([10.37.194.12])
  by komail02.kdc.capitalone.com with ESMTP; 03 Dec 2014 10:24:37 -0500
Received: from KDCPEXCMB02.cof.ds.capitalone.com ([169.254.1.11]) by
 KDCPEXCASHT02.cof.ds.capitalone.com ([fe80::f13f:d6f2:e567:c056%16]) with
 mapi; Wed, 3 Dec 2014 10:24:36 -0500
From: "York, Brennon" <Brennon.York@capitalone.com>
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>, flyson <m_qiu@msn.com>,
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Date: Wed, 3 Dec 2014 10:24:35 -0500
Subject: Re: object xxx is not a member of package com
Thread-Topic: object xxx is not a member of package com
Thread-Index: AdAPDT8POx6tIHUpRVu9c77SwHhpyA==
Message-ID: <D0A49506.742C%brennon.york@capitalone.com>
References: <1417576952411-9619.post@n3.nabble.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012E633C@SHSMSX101.ccr.corp.intel.com>
In-Reply-To: <CF8AD92F21C4704CAD056AADDD3CCC09012E633C@SHSMSX101.ccr.corp.intel.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.6.141106
acceptlanguage: en-US
Content-Type: text/plain; charset="iso-8859-1"
MIME-Version: 1.0
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

For reference here is the relevant `sbt` documentation that states
Daoyuan=B9s solution as well as a few other options to try.

http://www.scala-sbt.org/0.13/tutorial/Library-Dependencies.html


On 12/2/14, 11:29 PM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:

>I think you can place the jar in lib/ in SPARK_HOME, and then compile
>without any change to your class path. This could be a temporary way to
>include your jar. You can also put them in your pom.xml.
>
>Thanks,
>Daoyuan
>
>-----Original Message-----
>From: flyson [mailto:m_qiu@msn.com]
>Sent: Wednesday, December 03, 2014 11:23 AM
>To: dev@spark.incubator.apache.org
>Subject: object xxx is not a member of package com
>
>Hello everyone,
>
>Could anybody tell me how to import and call the 3rd party java classes
>from inside spark?
>Here's my case:
>I have a jar file (the directory layout is com.xxx.yyy.zzz) which
>contains some java classes, and I need to call some of them in spark code.
>I used the statement "import com.xxx.yyy.zzz._" on top of the impacted
>spark file and set the location of the jar file in the CLASSPATH
>environment, and use ".sbt/sbt assembly" to build the project. As a
>result, I got an error saying "object xxx is not a member of package com".
>
>I thought that could be related to the library dependencies, but couldn't
>figure it out. Any suggestion/solution from you would be appreciated!
>
>By the way in the scala console, if the :cp is used to point to the jar
>file, I can import the classes from the jar file.
>
>Thanks! =

>
>
>
>--
>View this message in context:
>http://apache-spark-developers-list.1001551.n3.nabble.com/object-xxx-is-no
>t-a-member-of-package-com-tp9619.html
>Sent from the Apache Spark Developers List mailing list archive at
>Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional
>commands, e-mail: dev-help@spark.apache.org
>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:27:45 2014
Return-Path: <dev-return-10640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2A3DC0B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:27:45 +0000 (UTC)
Received: (qmail 24488 invoked by uid 500); 4 Dec 2014 12:27:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24419 invoked by uid 500); 4 Dec 2014 12:27:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81980 invoked by uid 99); 3 Dec 2014 21:46:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 21:46:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.181 as permitted sender)
Received: from [209.85.213.181] (HELO mail-ig0-f181.google.com) (209.85.213.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 21:45:37 +0000
Received: by mail-ig0-f181.google.com with SMTP id l13so14491730iga.14
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 13:44:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=1ugFWkE9Gnyv/d1fIkOs8Z/7kLfxIm5g1WDEvZrkl2Y=;
        b=rrDJhTn4C7Bqf+ytg4HHNem9iqsox+CAFRdCSu0lcitRkMa4/w128LrdqlFtn2MU01
         EWVqUBCFwOUGZ5jboycsMV2W6znC/DQ3D7K8GoTw0lambisZLEXyaZ4w1kPERFgH6p7f
         vdcbOA97n8OhSq5afyI0QHBY2l4UyIwTjOqf5rECd2KAfIgrXgwWrCAHtiMm4O3IUkYS
         kdTeC4mIrNU6DvF3iVXggTl9ZSj+u1Wn3J5f6ILk+sS4LIMlVZc2DwD1OAicSOQwVrOi
         Z/qLH8xVRhGdujQ8qRb5nM4dnK00WNPBgh4t2hkp0rcPFmxEVD0O11G0PYWBaaWydTrM
         M2EA==
X-Received: by 10.50.41.103 with SMTP id e7mr20388743igl.20.1417643066331;
 Wed, 03 Dec 2014 13:44:26 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 03 Dec 2014 21:44:25 +0000
Message-ID: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
Subject: zinc invocation examples
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0116051c635147050956bf7a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0116051c635147050956bf7a
Content-Type: text/plain; charset=UTF-8

https://github.com/apache/spark/blob/master/docs/building-spark.md#speeding-up-compilation-with-zinc

Could someone summarize how they invoke zinc as part of a regular
build-test-etc. cycle?

I'll add it in to the aforelinked page if appropriate.

Nick

--089e0116051c635147050956bf7a--

From dev-return-10641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:29:16 2014
Return-Path: <dev-return-10641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06868C156
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:29:16 +0000 (UTC)
Received: (qmail 49632 invoked by uid 500); 4 Dec 2014 12:29:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49563 invoked by uid 500); 4 Dec 2014 12:29:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57837 invoked by uid 99); 3 Dec 2014 19:07:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 19:07:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 19:07:30 +0000
Received: by mail-la0-f52.google.com with SMTP id hs14so8497383lab.11
        for <dev@spark.incubator.apache.org>; Wed, 03 Dec 2014 11:05:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=L26Dt07KLQgb7hxA6MPSWBP7bQwAvbz4mA33D6g1tzc=;
        b=kKRfdmO7vDvii8xuX4N9G8WZ2cV/HV+dJDs/sXRyHz07mA3aLmcHgFKXkQ7UOQSwjB
         vHxW3K8tWkucoF+E8HE4St45sGxqIO1XKm14OdGkzYK60713CdVXpQda9c1qHh6orP0T
         7rLUvjPF6Ubv6SSCcj1IBHTHktQXmklGA9kCsWwqB2q176e9+1enMOBQCLxOwwGCdfLW
         OjUDtEdkgOtV8Bw1iZNzilCSpEqKvADENS/hv5b8Vd+Zl5LxwWjwQbDt/+270vMfY9mV
         PreKudmtXrndTTQfx9fot99SuvWALWZFcAYq8IOic1gog/Pu7xTgXxdm2H6eFtXjtsSu
         Dibw==
X-Gm-Message-State: ALoCoQlFP0UtgooDDjGYR1Hupg2lRiRa8bnflZ+g5wtfUusWEyrqvDh9D+rFMwalLsO2nUZXS7Id
X-Received: by 10.152.28.131 with SMTP id b3mr5975674lah.12.1417633538291;
 Wed, 03 Dec 2014 11:05:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Wed, 3 Dec 2014 11:05:18 -0800 (PST)
In-Reply-To: <CAJ4HpHEgppwd0J+pJouG5PMDdXrkXHSq6weYYdoHOU04veLs+A@mail.gmail.com>
References: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
 <CAAswR-5Qu2kj=EFoz4uT+PdDGK2Tsi4N1WarVS=XDami8iWcgA@mail.gmail.com> <CAJ4HpHEgppwd0J+pJouG5PMDdXrkXHSq6weYYdoHOU04veLs+A@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 3 Dec 2014 11:05:18 -0800
Message-ID: <CAAswR-4nM0JteU8zbwZT-V69JJtWEhmbSw1M2Xw2sSxmU59oig@mail.gmail.com>
Subject: Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe
To: Yana Kadiyska <yana.kadiyska@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b7cc78df120509548760
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b7cc78df120509548760
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for reporting. As a workaround you should be able to SET
spark.sql.hive.convertMetastoreParquet=3Dfalse, but I'm going to try to fix
this before the next RC.

On Wed, Dec 3, 2014 at 7:09 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
wrote:

> Thanks Michael, you are correct.
>
> I also opened https://issues.apache.org/jira/browse/SPARK-4702 -- if
> someone can comment on why this might be happening that would be great.
> This would be a blocker to me using 1.2 and it used to work so I'm a bit
> puzzled. I was hoping that it's again a result of the default profile
> switch but it didn't seem to be the case
>
> (ps. please advise if this is more user-list appropriate. I'm posting to
> dev as it's an RC)
>
> On Tue, Dec 2, 2014 at 8:37 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> In Hive 13 (which is the default for Spark 1.2), parquet is included and
>> thus we no longer include the Hive parquet bundle. You can now use the
>> included
>> ParquetSerDe: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerD=
e
>>
>> If you want to compile Spark 1.2 with Hive 12 instead you can pass
>> -Phive-0.12.0 and  parquet.hive.serde.ParquetHiveSerDe will be included =
as
>> before.
>>
>> Michael
>>
>> On Tue, Dec 2, 2014 at 9:31 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
>> wrote:
>>
>>> Apologies if people get this more than once -- I sent mail to dev@spark
>>> last night and don't see it in the archives. Trying the incubator list
>>> now...wanted to make sure it doesn't get lost in case it's a bug...
>>>
>>> ---------- Forwarded message ----------
>>> From: Yana Kadiyska <yana.kadiyska@gmail.com>
>>> Date: Mon, Dec 1, 2014 at 8:10 PM
>>> Subject: [Thrift,1.2 RC] what happened to
>>> parquet.hive.serde.ParquetHiveSerDe
>>> To: dev@spark.apache.org
>>>
>>>
>>> Hi all, apologies if this is not a question for the dev list -- figured
>>> User list might not be appropriate since I'm having trouble with the RC
>>> tag.
>>>
>>> I just tried deploying the RC and running ThriftServer. I see the
>>> following
>>> error:
>>>
>>> 14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionExceptio=
n
>>> as:anonymous (auth:SIMPLE)
>>> cause:org.apache.hive.service.cli.HiveSQLException:
>>> java.lang.RuntimeException:
>>> MetaException(message:java.lang.ClassNotFoundException Class
>>> parquet.hive.serde.ParquetHiveSerDe not found)
>>> 14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
>>> org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeExceptio=
n:
>>> MetaException(message:java.lang.ClassNotFoundException Class
>>> parquet.hive.serde.ParquetHiveSerDe not found)
>>> at
>>>
>>> org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.r=
un(Shim13.scala:192)
>>> at
>>>
>>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInt=
ernal(HiveSessionImpl.java:231)
>>> at
>>>
>>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(Hi=
veSessionImpl.java:212)
>>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>> at
>>>
>>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.ja=
va:57)
>>> at
>>>
>>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccesso=
rImpl.java:43)
>>> at java.lang.reflect.Method.invoke(Method.java:606)
>>> at
>>>
>>> org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSession=
Proxy.java:79)
>>> at
>>>
>>> org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSes=
sionProxy.java:37)
>>> at
>>>
>>> org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionP=
roxy.java:64)
>>> at java.security.AccessController.doPrivileged(Native Method)
>>> at javax.security.auth.Subject.doAs(Subject.java:415)
>>> =E2=80=8B
>>>
>>>
>>> I looked at a working installation that I have(build master a few weeks
>>> ago) and this class used to be included in spark-assembly:
>>>
>>> ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
>>> Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
>>> matches
>>>
>>> but with the RC build it's not there?
>>>
>>> I tried both the prebuilt CDH drop and later manually built the tag wit=
h
>>> the following command:
>>>
>>>  ./make-distribution.sh --tgz -Phive -Dhadoop.version=3D2.0.0-mr1-cdh4.=
2.0
>>> -Phive-thriftserver
>>> $JAVA_HOME/bin/jar -tvf spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.j=
ar
>>> |grep parquet.hive.serde.ParquetHiveSerDe
>>>
>>> comes back empty...
>>>
>>
>>
>

--089e0160b7cc78df120509548760--

From dev-return-10642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:35:18 2014
Return-Path: <dev-return-10642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CFD72C259
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:35:18 +0000 (UTC)
Received: (qmail 18931 invoked by uid 500); 4 Dec 2014 12:35:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18773 invoked by uid 500); 4 Dec 2014 12:35:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92880 invoked by uid 99); 4 Dec 2014 00:20:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 00:20:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 00:19:49 +0000
Received: by mail-ig0-f173.google.com with SMTP id r2so17825946igi.12
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 16:19:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=A3paWG+t3V75B+BJT8aX4SuZ/8ibt4GazGP1l3mNJ0k=;
        b=UKRPRmmC+RVr9L9m4lDYPeoaltTj12ulZlDm2jXtPTHClViOCNONrY0lvbLh/W9d4r
         O6rt2zgOXcTPD0jFfOz9EVT8HrqZugsDzRu+wNtW6a6wlqKFvWFgiGSZoQMijf1URiTn
         qK0OHhCtHgRAioWP+H0wOr6at1BM/MofjrrBAoiYvXdHEa4zWSVocBwynKkN38C5zL1M
         ldjqMu6Aryj5IsKSB095s407bsfNNK3FmpEFh/YcHtcRdJLU0l26hQjG1dQGvFlEk+rm
         cIvNmKdjJ0cm6uFwswAM7nMTIjiVVgo5VAJa18AeS8UXsKzh8xin/tIjYGt/OTTsGhG2
         zPjg==
MIME-Version: 1.0
X-Received: by 10.43.66.9 with SMTP id xo9mr9352575icb.67.1417652387862; Wed,
 03 Dec 2014 16:19:47 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Wed, 3 Dec 2014 16:19:47 -0800 (PST)
In-Reply-To: <CAOTBr2=5S=cxYfi7OE4g_8nyOWsdFabaSCsJ2fkWsKHY9Nn=6Q@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<CAOTBr2=5S=cxYfi7OE4g_8nyOWsdFabaSCsJ2fkWsKHY9Nn=6Q@mail.gmail.com>
Date: Thu, 4 Dec 2014 08:19:47 +0800
Message-ID: <CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Krishna, could you send me some code snippets for the issues you saw
in naive Bayes and k-means? -Xiangrui

On Sun, Nov 30, 2014 at 6:49 AM, Krishna Sankar <ksankar42@gmail.com> wrote:
> +1
> 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
> -Dhadoop.version=2.4.0 -DskipTests clean package 16:46 min (slightly slower
> connection)
> 2. Tested pyspark, mlib - running as well as compare esults with 1.1.x
> 2.1. statistics OK
> 2.2. Linear/Ridge/Laso Regression OK
>        Slight difference in the print method (vs. 1.1.x) of the model
> object - with a label & more details. This is good.
> 2.3. Decision Tree, Naive Bayes OK
>        Changes in print(model) - now print (model.ToDebugString()) - OK
>        Some changes in NaiveBayes. Different from my 1.1.x code - had to
> flatten list structures, zip required same number in partitions
>        After code changes ran fine.
> 2.4. KMeans OK
>        zip occasionally fails with error "localhost):
> org.apache.spark.SparkException: Can only zip RDDs with same number of
> elements in each partition"
> Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
> Made it work by doing a different transformation ie reusing an original
> rdd.
> 2.5. rdd operations OK
>        State of the Union Texts - MapReduce, Filter,sortByKey (word count)
> 2.6. recommendation OK
> 2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
> dataset which never worked. Works fine in 1.2.0 !
> 3. Scala Mlib - subset of examples as in #2 above, with Scala
> 3.1. statistics OK
> 3.2. Linear Regression OK
> 3.3. Decision Tree OK
> 3.4. KMeans OK
> Cheers
> <k/>
> P.S: Plan to add RF and .ml mechanics to this bank
>
> On Fri, Nov 28, 2014 at 9:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.2.0!
>>
>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1048/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.2.0!
>>
>> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>> regressions, or bugs related to new features will not block this
>> release.
>>
>> == What default changes should I be aware of? ==
>> 1. The default value of "spark.shuffle.blockTransferService" has been
>> changed to "netty"
>> --> Old behavior can be restored by switching to "nio"
>>
>> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
>> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>> "hash".
>>
>> == Other notes ==
>> Because this vote is occurring over a weekend, I will likely extend
>> the vote if this RC survives until the end of the vote period.
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:36:41 2014
Return-Path: <dev-return-10643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7242DC282
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:36:41 +0000 (UTC)
Received: (qmail 28664 invoked by uid 500); 4 Dec 2014 12:36:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28593 invoked by uid 500); 4 Dec 2014 12:36:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90604 invoked by uid 99); 3 Dec 2014 17:49:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 17:49:58 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of m_qiu@msn.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 17:49:32 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 06A04C78C43
	for <dev@spark.incubator.apache.org>; Wed,  3 Dec 2014 09:49:02 -0800 (PST)
Date: Wed, 3 Dec 2014 10:49:00 -0700 (MST)
From: flyson <m_qiu@msn.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417628940694-9624.post@n3.nabble.com>
In-Reply-To: <CF8AD92F21C4704CAD056AADDD3CCC09012E633C@SHSMSX101.ccr.corp.intel.com>
References: <1417576952411-9619.post@n3.nabble.com> <CF8AD92F21C4704CAD056AADDD3CCC09012E633C@SHSMSX101.ccr.corp.intel.com>
Subject: RE: object xxx is not a member of package com
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Daoyuan,

Actually I had already tried the way as you mentioned, but it didn't work
for my case. I still got the same compilation errors.

Anyone can tell me how to resolve the library dependency on the 3rd party
jar in sbt?

Thanks!
Min



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/object-xxx-is-not-a-member-of-package-com-tp9619p9624.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:45:37 2014
Return-Path: <dev-return-10644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C889AC3A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:45:37 +0000 (UTC)
Received: (qmail 37051 invoked by uid 500); 4 Dec 2014 12:45:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36926 invoked by uid 500); 4 Dec 2014 12:45:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18329 invoked by uid 99); 4 Dec 2014 01:38:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 01:38:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of linguin.m.s@gmail.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 01:38:51 +0000
Received: by mail-qa0-f44.google.com with SMTP id i13so11421894qae.3
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 17:38:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2kLYpzLdPjmIgnYEbJQCw01fCxuHxEjVHqY6opsiZ64=;
        b=OBoTl8vZV9sA9Q5PSUnFpdnUJOEX2qp2cVtv8Bahwh7njYD8ytzdDh0aOdeiqv+4o5
         mfMFtld4TV0ah5676PGtk9irJjdLpG3mLmJvjsBiy/LfDq2rJCQ1m+nnZxJPkmpYw7ww
         dc+st9Dam+krOpA9CenaI0x4D2UP1nu2t88J2c0nbP9sh3mxvP9AUBxARR/WmoI1ZHod
         BFlpt/nCkdSoMuuHPurEFL4+mGVdwLHhuUHWwfbxNGuuQxPuEnVeYUbpRUnZi2UQ03oH
         a4r7xDNMF2z7BPpBrJvr2yreGkhgyGJibd21Ch+J551O6NMeHMoHVLUkQ1B5ZW6rnRyi
         Psnw==
MIME-Version: 1.0
X-Received: by 10.140.44.99 with SMTP id f90mr12599582qga.32.1417657110446;
 Wed, 03 Dec 2014 17:38:30 -0800 (PST)
Received: by 10.229.56.71 with HTTP; Wed, 3 Dec 2014 17:38:30 -0800 (PST)
In-Reply-To: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
Date: Thu, 4 Dec 2014 10:38:30 +0900
Message-ID: <CACVFzXDoZi2oS5hKqghWq9TTq=HUvuzBrP0ZhM46uh2=snfJ_Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Takeshi Yamamuro <linguin.m.s@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a9a687b477005095a041a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9a687b477005095a041a
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

Checked on CentOS 6.5, compiled from the source.
Ran various examples in stand-alone master and three slaves, and
browsed the web UI.

On Sat, Nov 29, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.2.0!
>
> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1048/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.0!
>
> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
>
> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> "hash".
>
> == Other notes ==
> Because this vote is occurring over a weekend, I will likely extend
> the vote if this RC survives until the end of the vote period.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113a9a687b477005095a041a--

From dev-return-10645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:47:28 2014
Return-Path: <dev-return-10645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 729ACC3C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:47:28 +0000 (UTC)
Received: (qmail 53139 invoked by uid 500); 4 Dec 2014 12:47:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53054 invoked by uid 500); 4 Dec 2014 12:47:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26144 invoked by uid 99); 3 Dec 2014 18:00:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 18:00:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 18:00:17 +0000
Received: by mail-wi0-f179.google.com with SMTP id ex7so25361959wid.12
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 10:00:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=xVF0sYWAhGxJARXThSrzz7YcKiUovo5lMwK937R2/V8=;
        b=xZtdMBSK4naKGEvXNzojgk8YmIt86pzmqhQDnFaVwcDv723PaiIeJMy3OVbMBfSAxf
         d4UUfU6/fJiqWmJg1vFOePAEOmrw2R199jqj+M0rEyJ3RhMs0nZYqdtZohw+c3RQOihw
         MIkWnE89WQIGWQbn0P/2LMnuPX1uMubREWOG3l6jG463QbWVriLC1jBh8Rckf4vxayXo
         IaTUlQFGk4QZgpsW+M9/n+ZkBu4XN3OvFsFHuS8D6wxi4mxhxiwLOn5Kc24ygNDCZD2+
         ZMqdixjCumK8P9OzhisIQmttjEgU9RiSnoDah5BoluhqdUeBYbOI9xVEgIrUrfPv+0CO
         tq9g==
MIME-Version: 1.0
X-Received: by 10.180.103.40 with SMTP id ft8mr57521005wib.68.1417629616633;
 Wed, 03 Dec 2014 10:00:16 -0800 (PST)
Received: by 10.194.37.225 with HTTP; Wed, 3 Dec 2014 10:00:16 -0800 (PST)
Date: Wed, 3 Dec 2014 13:00:16 -0500
Message-ID: <CADtDQQ+pB4JOqfn6wMg0HFnepV5k+BkGVtcL7HDbrF9F+EC5Lw@mail.gmail.com>
Subject: RDDs for "dimensional" (time series, spatial) data
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d044282acb8fcfb0509539dc6
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044282acb8fcfb0509539dc6
Content-Type: text/plain; charset=UTF-8

Hi all,

I created a JIRA to discuss adding RDDs for "dimensional" (not sure what
else to call it) data like time series and spatial data.  Spark could be a
better time series and/or spatial "database" than existing approaches out
there.

https://issues.apache.org/jira/browse/SPARK-4727

I saw that MLlib supports some operations for time series in 1.2.0-rc1, but
I think that specialized RDDs could optimize the partitioning and
algorithms better than a regular RDD.  Or, for example, spatial data could
be partitioned into a grid.

Any feedback would be great!

Thanks,
RJ Nowling

-- 
em rnowling@gmail.com

--f46d044282acb8fcfb0509539dc6--

From dev-return-10646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:47:59 2014
Return-Path: <dev-return-10646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CEBF8C3CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:47:59 +0000 (UTC)
Received: (qmail 57965 invoked by uid 500); 4 Dec 2014 12:47:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57889 invoked by uid 500); 4 Dec 2014 12:47:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57877 invoked by uid 99); 4 Dec 2014 12:47:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 12:47:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 12:47:53 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so22600397wgh.39
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 04:47:32 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=0FiymRoKOOf5iKjuF7puTjUljSNETw75SpgRtbNAHIg=;
        b=J20QYXXAV1Kmv1KYV6aEZ22bfvZP6a7hXYCP8wuBSwnAZuRq9uz27Ak0AQdZfdRWHJ
         8onbAYK0LbEIwhVmGPm7dkN3aPB63AXP7Y2uElfQFmh/lqGMIBApMWcKtzVoHPktMERs
         c1FnYJbAwiXAshhbiHLj3inGI/XFdpqkYVvdzYcNS/kGgHLgT4MEUNIgGSvXGVb5xIvv
         XS+yJp7BuZW3KpxphUxPRtQ+ci5BT5hxMw18uJM9m/an2duBfSlpcfJCem7cMjcbJg/D
         lJ+8O7N2xISnI8I1ZQVT22QEIHwau2IEEhpMXdP9F9e7A3jIcuyYT6npgxEUp9u0FZNA
         cvpg==
X-Gm-Message-State: ALoCoQm31m6XGiyi5wSid3u9UyGVMgfZCbUAZgM0AWI7mXMyV44SgrkK9xN7v3OUHMKgVSJZOMe9
X-Received: by 10.194.190.19 with SMTP id gm19mr15507613wjc.51.1417697252637;
 Thu, 04 Dec 2014 04:47:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Thu, 4 Dec 2014 04:47:12 -0800 (PST)
In-Reply-To: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 4 Dec 2014 06:47:12 -0600
Message-ID: <CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com>
Subject: Re: zinc invocation examples
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You just run it once with "zinc -start" and leave it running as a
background process on your build machine. You don't have to do
anything for each build.

On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> https://github.com/apache/spark/blob/master/docs/building-spark.md#speeding-up-compilation-with-zinc
>
> Could someone summarize how they invoke zinc as part of a regular
> build-test-etc. cycle?
>
> I'll add it in to the aforelinked page if appropriate.
>
> Nick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:48:37 2014
Return-Path: <dev-return-10647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC23AC3D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:48:37 +0000 (UTC)
Received: (qmail 60786 invoked by uid 500); 4 Dec 2014 12:48:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60712 invoked by uid 500); 4 Dec 2014 12:48:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85992 invoked by uid 99); 3 Dec 2014 19:26:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 19:26:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 19:26:52 +0000
Received: by mail-lb0-f169.google.com with SMTP id p9so12897705lbv.0
        for <dev@spark.incubator.apache.org>; Wed, 03 Dec 2014 11:25:46 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ioV1SF3XkE413dYD8v9A6eP+Hu94GY0gvRbVOwwOQFw=;
        b=Wnj3Y5s182Ve7XtEplIBC+Go+ularKDlPVt9oFSGVNkXuu98WAwlOq3JCUlQMXqu8O
         ieA+bN3ob458dNV9zjZwaRBC5eO5HqBaS5Ark41PInlHp+QjkZTo6o6kEoKaIn8xmcla
         CaBAFJkolUbdHibI+VRmg7jjZd4tCaV85L/PwZRUhG8MhV/nVebQSPmT09Ij5DyUeXa6
         J6DJDFJsJczXIAB0pGWJQpdKtI8VrGyRYF4Nnt1L8RUEM8a0C96V+BHLdOkUVBbmjd/w
         lCjL3OjppkUD8Ml6KhGWFHe8FBVtYFPjJHp29RoS8L7ZjtsLlLAchw+JkDwO6URWctDZ
         O9kg==
X-Gm-Message-State: ALoCoQm5gufUzwZ60AP6cCVRNdbq4tHZbaHjOoneLK7RvaGrTGCR1+yI1Kbbz+MZe/s8pVpZS8Ak
X-Received: by 10.112.201.226 with SMTP id kd2mr5755574lbc.98.1417634746080;
 Wed, 03 Dec 2014 11:25:46 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Wed, 3 Dec 2014 11:25:25 -0800 (PST)
In-Reply-To: <CAAswR-4nM0JteU8zbwZT-V69JJtWEhmbSw1M2Xw2sSxmU59oig@mail.gmail.com>
References: <CAJ4HpHEND==wxS9TcpeBKC9dFbc-odVidFNMYYxnqu+Ng89rdw@mail.gmail.com>
 <CAAswR-5Qu2kj=EFoz4uT+PdDGK2Tsi4N1WarVS=XDami8iWcgA@mail.gmail.com>
 <CAJ4HpHEgppwd0J+pJouG5PMDdXrkXHSq6weYYdoHOU04veLs+A@mail.gmail.com> <CAAswR-4nM0JteU8zbwZT-V69JJtWEhmbSw1M2Xw2sSxmU59oig@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 3 Dec 2014 11:25:25 -0800
Message-ID: <CAAswR-4d2LiScJCg1kytbsb4VQdr4VJ8uZjnYYnE0JFy5NBZbQ@mail.gmail.com>
Subject: Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe
To: Yana Kadiyska <yana.kadiyska@gmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37eb4763558050954cf28
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37eb4763558050954cf28
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here's a fix: https://github.com/apache/spark/pull/3586

On Wed, Dec 3, 2014 at 11:05 AM, Michael Armbrust <michael@databricks.com>
wrote:

> Thanks for reporting. As a workaround you should be able to SET
> spark.sql.hive.convertMetastoreParquet=3Dfalse, but I'm going to try to f=
ix
> this before the next RC.
>
> On Wed, Dec 3, 2014 at 7:09 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
> wrote:
>
>> Thanks Michael, you are correct.
>>
>> I also opened https://issues.apache.org/jira/browse/SPARK-4702 -- if
>> someone can comment on why this might be happening that would be great.
>> This would be a blocker to me using 1.2 and it used to work so I'm a bit
>> puzzled. I was hoping that it's again a result of the default profile
>> switch but it didn't seem to be the case
>>
>> (ps. please advise if this is more user-list appropriate. I'm posting to
>> dev as it's an RC)
>>
>> On Tue, Dec 2, 2014 at 8:37 PM, Michael Armbrust <michael@databricks.com=
>
>> wrote:
>>
>>> In Hive 13 (which is the default for Spark 1.2), parquet is included an=
d
>>> thus we no longer include the Hive parquet bundle. You can now use the
>>> included
>>> ParquetSerDe: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSer=
De
>>>
>>> If you want to compile Spark 1.2 with Hive 12 instead you can pass
>>> -Phive-0.12.0 and  parquet.hive.serde.ParquetHiveSerDe will be included=
 as
>>> before.
>>>
>>> Michael
>>>
>>> On Tue, Dec 2, 2014 at 9:31 AM, Yana Kadiyska <yana.kadiyska@gmail.com>
>>> wrote:
>>>
>>>> Apologies if people get this more than once -- I sent mail to dev@spar=
k
>>>> last night and don't see it in the archives. Trying the incubator list
>>>> now...wanted to make sure it doesn't get lost in case it's a bug...
>>>>
>>>> ---------- Forwarded message ----------
>>>> From: Yana Kadiyska <yana.kadiyska@gmail.com>
>>>> Date: Mon, Dec 1, 2014 at 8:10 PM
>>>> Subject: [Thrift,1.2 RC] what happened to
>>>> parquet.hive.serde.ParquetHiveSerDe
>>>> To: dev@spark.apache.org
>>>>
>>>>
>>>> Hi all, apologies if this is not a question for the dev list -- figure=
d
>>>> User list might not be appropriate since I'm having trouble with the R=
C
>>>> tag.
>>>>
>>>> I just tried deploying the RC and running ThriftServer. I see the
>>>> following
>>>> error:
>>>>
>>>> 14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionExcepti=
on
>>>> as:anonymous (auth:SIMPLE)
>>>> cause:org.apache.hive.service.cli.HiveSQLException:
>>>> java.lang.RuntimeException:
>>>> MetaException(message:java.lang.ClassNotFoundException Class
>>>> parquet.hive.serde.ParquetHiveSerDe not found)
>>>> 14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
>>>> org.apache.hive.service.cli.HiveSQLException:
>>>> java.lang.RuntimeException:
>>>> MetaException(message:java.lang.ClassNotFoundException Class
>>>> parquet.hive.serde.ParquetHiveSerDe not found)
>>>> at
>>>>
>>>> org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.=
run(Shim13.scala:192)
>>>> at
>>>>
>>>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementIn=
ternal(HiveSessionImpl.java:231)
>>>> at
>>>>
>>>> org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(H=
iveSessionImpl.java:212)
>>>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>>> at
>>>>
>>>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.j=
ava:57)
>>>> at
>>>>
>>>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccess=
orImpl.java:43)
>>>> at java.lang.reflect.Method.invoke(Method.java:606)
>>>> at
>>>>
>>>> org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessio=
nProxy.java:79)
>>>> at
>>>>
>>>> org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSe=
ssionProxy.java:37)
>>>> at
>>>>
>>>> org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSession=
Proxy.java:64)
>>>> at java.security.AccessController.doPrivileged(Native Method)
>>>> at javax.security.auth.Subject.doAs(Subject.java:415)
>>>> =E2=80=8B
>>>>
>>>>
>>>> I looked at a working installation that I have(build master a few week=
s
>>>> ago) and this class used to be included in spark-assembly:
>>>>
>>>> ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
>>>> Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
>>>> matches
>>>>
>>>> but with the RC build it's not there?
>>>>
>>>> I tried both the prebuilt CDH drop and later manually built the tag wi=
th
>>>> the following command:
>>>>
>>>>  ./make-distribution.sh --tgz -Phive -Dhadoop.version=3D2.0.0-mr1-cdh4=
.2.0
>>>> -Phive-thriftserver
>>>> $JAVA_HOME/bin/jar -tvf
>>>> spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.jar
>>>> |grep parquet.hive.serde.ParquetHiveSerDe
>>>>
>>>> comes back empty...
>>>>
>>>
>>>
>>
>

--001a11c37eb4763558050954cf28--

From dev-return-10648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 12:52:53 2014
Return-Path: <dev-return-10648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0FDC9C458
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 12:52:53 +0000 (UTC)
Received: (qmail 5012 invoked by uid 500); 4 Dec 2014 12:52:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4824 invoked by uid 500); 4 Dec 2014 12:52:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7006 invoked by uid 99); 4 Dec 2014 02:51:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 02:51:11 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lochanac@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 02:51:06 +0000
Received: by mail-pa0-f52.google.com with SMTP id eu11so16906895pac.25
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 18:50:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type;
        bh=kOEx/NpwoZQCl5vZSgusvthFnSkTMuuxeDlH1R/XfBs=;
        b=AuBViX1bhcuHTBZYOqmciT79WXPO41733eiRsEhkuRdkQWV7az7vLm3AHRiYN65P4B
         oFb/qCeX6CWELuKBjkNWR4vwcxKYgdG/LTNjC1ba+sY9mRHNdyIKcd9T6iGab+TS6X6z
         Xs5H7DnAqUOGd8aQuD0OqYQZtANyh90GLUZ6clgTwtT1E+GqYvWp6rxNQLdxhgyWCMg3
         vG3k1ljh8PSPH7loo2RtGuHhQwUbkz60Cg0JUKGkxLqeVA7eMU/rvJ2Pf9uocsftswaT
         kr2hFwYc7mW5al+ya6wMWlHSBA4fp03TGjGH+YJ8MQ8Ilu7TnUGGe3dTh5TZ+fW0dBBn
         vwvA==
X-Received: by 10.70.135.226 with SMTP id pv2mr14407780pdb.11.1417661446372;
        Wed, 03 Dec 2014 18:50:46 -0800 (PST)
Received: from Lochanas-MacBook-Pro.local ([203.94.95.4])
        by mx.google.com with ESMTPSA id th7sm24498980pac.47.2014.12.03.18.50.44
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 03 Dec 2014 18:50:45 -0800 (PST)
Message-ID: <547FCC04.9090300@gmail.com>
Date: Thu, 04 Dec 2014 08:20:44 +0530
From: Lochana Menikarachchi <lochanac@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "Dinesh J. Weerakkody" <dineshjweerakkody@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: packaging spark run time with osgi service
References: <547C49EA.9040804@gmail.com> <CAGC4hZtkqqVV49RSJ0hCMeE4r8=ri2c1A24-q57_RFU1dxpMhA@mail.gmail.com>
In-Reply-To: <CAGC4hZtkqqVV49RSJ0hCMeE4r8=ri2c1A24-q57_RFU1dxpMhA@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------060400060809070008070003"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060400060809070008070003
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

I think the problem has to do with akka not picking up the 
reference.conf file in the assembly.jar

We managed to make akka pick the conf file by temporary switching the 
class loaders.

Thread.currentThread().setContextClassLoader(JavaSparkContext.class.getClassLoader());

The model gets build but execution fails during some later stage with a snappy error..

14/12/04 08:07:44 ERROR Executor: Exception in task 0.0 in stage 105.0 (TID 104)
java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
	at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
	at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)
	at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)
	at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)

According to akka documentation a conf file can be parsed with -Dconfig.file= but, we couldn't get it to work..

Any ideas how to do this?

Lochana




On 12/2/14 8:17 AM, Dinesh J. Weerakkody wrote:
> Hi Lochana,
>
> can you please go through this mail thread [1]. I haven't tried but 
> can be useful.
>
> [1] 
> http://apache-spark-user-list.1001560.n3.nabble.com/Packaging-a-spark-job-using-maven-td5615.html 
>
>
> On Mon, Dec 1, 2014 at 4:28 PM, Lochana Menikarachchi 
> <lochanac@gmail.com <mailto:lochanac@gmail.com>> wrote:
>
>     I have spark core and mllib as dependencies for a spark based osgi
>     service. When I call the model building method through a unit test
>     (without osgi) it works OK. When I call it through the osgi
>     service, nothing happens. I tried adding spark assembly jar. Now
>     it throws following error..
>
>     An error occurred while building supervised machine learning
>     model: No configuration setting found for key 'akka.version'
>     com.typesafe.config.ConfigException$Missing: No configuration
>     setting found for key 'akka.version'
>         at
>     com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:136)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:142)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:150)
>         at
>     com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:155)
>         at
>     com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:197)
>
>     What is the correct way to include spark runtime dependencies to
>     osgi service.. Thanks.
>
>     Lochana
>
>     ---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>
>
>
> -- 
> Thanks & Best Regards,
>
> *Dinesh J. Weerakkody*
> /www.dineshjweerakkody.com <http://www.dineshjweerakkody.com>/


--------------060400060809070008070003--

From dev-return-10649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:06:10 2014
Return-Path: <dev-return-10649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D42AAC6D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:06:10 +0000 (UTC)
Received: (qmail 64662 invoked by uid 500); 4 Dec 2014 13:06:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64586 invoked by uid 500); 4 Dec 2014 13:06:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85393 invoked by uid 99); 4 Dec 2014 05:04:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 05:04:29 +0000
X-ASF-Spam-Status: No, hits=1.2 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_16,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.142 as permitted sender)
Received: from [202.81.31.142] (HELO e23smtp09.au.ibm.com) (202.81.31.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 05:04:21 +0000
Received: from /spool/local
	by e23smtp09.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Thu, 4 Dec 2014 15:02:58 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp09.au.ibm.com (202.81.31.206) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Thu, 4 Dec 2014 15:02:56 +1000
Received: from d23relay08.au.ibm.com (d23relay08.au.ibm.com [9.185.71.33])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id 292EC2BB0040
	for <dev@spark.apache.org>; Thu,  4 Dec 2014 16:02:56 +1100 (EST)
Received: from d23av01.au.ibm.com (d23av01.au.ibm.com [9.190.234.96])
	by d23relay08.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sB452tv641222340
	for <dev@spark.apache.org>; Thu, 4 Dec 2014 16:02:56 +1100
Received: from d23av01.au.ibm.com (localhost [127.0.0.1])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sB452tsE016049
	for <dev@spark.apache.org>; Thu, 4 Dec 2014 16:02:55 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sB452l5Z015818
	for <dev@spark.apache.org>; Thu, 4 Dec 2014 16:02:55 +1100
To: dev@spark.apache.org
MIME-Version: 1.0
Subject: Ooyala Spark JobServer
X-KeepSent: 4E0755FF:D4C7B247-48257DA4:001B9269;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OF4E0755FF.D4C7B247-ON48257DA4.001B9269-48257DA4.001BB47A@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Thu, 4 Dec 2014 13:02:43 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/04/2014 13:02:55,
	Serialize complete at 12/04/2014 13:02:55
Content-Type: multipart/related; boundary="=_related 001BB47148257DA4_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14120405-0033-0000-0000-000000A9D451
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 001BB47148257DA4_=
Content-Type: multipart/alternative; boundary="=_alternative 001BB47448257DA4_="


--=_alternative 001BB47448257DA4_=
Content-Type: text/plain; charset="US-ASCII"

Hi, I am wondering the status of the Ooyala Spark Jobserver, any plan to 
get it into the spark release?
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 
--=_alternative 001BB47448257DA4_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64

PGZvbnQgc2l6ZT0yIGZhY2U9InNhbnMtc2VyaWYiPkhpLCBJIGFtIHdvbmRlcmluZyB0aGUgc3Rh
dHVzIG9mIHRoZSBPb3lhbGENClNwYXJrIEpvYnNlcnZlciwgYW55IHBsYW4gdG8gZ2V0IGl0IGlu
dG8gdGhlIHNwYXJrIHJlbGVhc2U/PGJyPg0KPC9mb250Pjxmb250IHNpemU9MSBmYWNlPSJBcmlh
bCI+IDwvZm9udD4NCjxwPjxmb250IHNpemU9MSBmYWNlPSJBcmlhbCI+QmVzdCBSZWdhcmRzPC9m
b250Pg0KPHA+PGZvbnQgc2l6ZT0xIGZhY2U9IkFyaWFsIj4mbmJzcDs8L2ZvbnQ+DQo8YnI+PGZv
bnQgc2l6ZT0zIGNvbG9yPSM4ZjhmOGYgZmFjZT0iQXJpYWwiPjxiPkp1biBGZW5nIExpdTwvYj48
L2ZvbnQ+PGZvbnQgc2l6ZT0xIGZhY2U9IkFyaWFsIj48YnI+DQpJQk0gQ2hpbmEgU3lzdGVtcyAm
YW1wOyBUZWNobm9sb2d5IExhYm9yYXRvcnkgaW4gQmVpamluZzwvZm9udD4NCjxwPg0KPHRhYmxl
Pg0KPHRyPg0KPHRkIGNvbHNwYW49Mz4NCjxkaXYgYWxpZ249Y2VudGVyPg0KPGhyIG5vc2hhZGU+
PC9kaXY+DQo8dHI+DQo8dGQgcm93c3Bhbj0yPjxpbWcgc3JjPWNpZDpfMl8xNzFEQzdENDE3MURD
NDAwMDAxQkI0NjY0ODI1N0RBNCBhbHQ9IjJEIGJhcmNvZGUgLSBlbmNvZGVkIHdpdGggY29udGFj
dCBpbmZvcm1hdGlvbiI+DQo8dGQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM0MTgxYzAgZmFjZT0iy87M
5SI+PGI+UGhvbmU6IDwvYj48L2ZvbnQ+PGZvbnQgc2l6ZT0xIGNvbG9yPSM1ZjVmNWYgZmFjZT0i
y87M5SI+ODYtMTAtODI0NTI2ODMNCjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzQxODFjMD48
Yj48YnI+DQpFLW1haWw6PC9iPjwvZm9udD48Zm9udCBzaXplPTEgY29sb3I9IzVmNWY1Zj4gPC9m
b250PjxhIGhyZWY9bWFpbHRvOmxpdWp1bmZAY24uaWJtLmNvbSB0YXJnZXQ9X2JsYW5rPjxmb250
IHNpemU9MSBjb2xvcj0jNWY1ZjVmIGZhY2U9IsvOzOUiPjx1PmxpdWp1bmZAY24uaWJtLmNvbTwv
dT48L2ZvbnQ+PC9hPg0KPHRkIHJvd3NwYW49Mj4NCjxkaXYgYWxpZ249cmlnaHQ+PGltZyBzcmM9
Y2lkOl8xXzE3MUREMTgwMTcxRENEQUMwMDFCQjQ2NjQ4MjU3REE0IHdpZHRoPTMyIGhlaWdodD0z
MiBhbHQ9SUJNPjxmb250IHNpemU9MSBjb2xvcj0jNWY1ZjVmPjxicj4NCjwvZm9udD48Zm9udCBz
aXplPTEgY29sb3I9IzVmNWY1ZiBmYWNlPSLLzszlIj48YnI+DQpCTEQgMjgsWkdDIFNvZnR3YXJl
IFBhcmsgPGJyPg0KTm8uOCBSZC5Eb25nIEJlaSBXYW5nIFdlc3QsIERpc3QuSGFpZGlhbiBCZWlq
aW5nIDEwMDE5MyA8YnI+DQpDaGluYSA8L2ZvbnQ+PC9kaXY+DQo8dHI+DQo8dGQ+PGZvbnQgc2l6
ZT0xIGNvbG9yPSM1ZjVmNWY+Jm5ic3A7PC9mb250PjwvdGFibGU+DQo8YnI+DQo8cD48Zm9udCBz
aXplPTM+Jm5ic3A7PC9mb250Pg0K
--=_alternative 001BB47448257DA4_=--
--=_related 001BB47148257DA4_=--


From dev-return-10650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:06:21 2014
Return-Path: <dev-return-10650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58925C6D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:06:21 +0000 (UTC)
Received: (qmail 66938 invoked by uid 500); 4 Dec 2014 13:06:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66869 invoked by uid 500); 4 Dec 2014 13:06:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85273 invoked by uid 99); 3 Dec 2014 16:51:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 16:51:19 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of invkrh@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 16:51:14 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id C84EDC77049
	for <dev@spark.incubator.apache.org>; Wed,  3 Dec 2014 08:50:24 -0800 (PST)
Date: Wed, 3 Dec 2014 09:50:23 -0700 (MST)
From: invkrh <invkrh@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417625423553-9623.post@n3.nabble.com>
Subject: scala.MatchError on SparkSQL when creating ArrayType of StructType
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I am using SparkSQL on 1.1.0 branch.

The following code leads to a scala.MatchError 
at
org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)

val scm = StructType(*inputRDD*.schema.fields.init :+
      StructField("list",
        ArrayType(
          StructType(
            Seq(StructField("*date*", StringType, nullable = *false*),
              StructField("*nbPurchase*", IntegerType, nullable =
*false*)))),
        nullable = false))

// *purchaseRDD* is RDD[sql.ROW] whose schema is corresponding to scm. It is
transformed from *inputRDD*
val schemaRDD = hiveContext.applySchema(purchaseRDD, scm)
schemaRDD.registerTempTable("t_purchase")

Here's the stackTrace:
scala.MatchError: ArrayType(StructType(List(StructField(date,StringType,
*true* ), StructField(n_reachat,IntegerType, *true* ))),true) (of class
org.apache.spark.sql.catalyst.types.ArrayType)
	at
org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
	at org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
	at
org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:84)
	at
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:66)
	at
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:149)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

The strange thing is that *nullable* of *date* and *nbPurchase* field are
set to true while it were false in the code. If I set both to *true*, it
works. But, in fact, they should not be nullable.

Here's what I find at Cast.scala:247 on 1.1.0 branch

  private[this] lazy val cast: Any => Any = dataType match {
    case StringType => castToString
    case BinaryType => castToBinary
    case DecimalType => castToDecimal
    case TimestampType => castToTimestamp
    case BooleanType => castToBoolean
    case ByteType => castToByte
    case ShortType => castToShort
    case IntegerType => castToInt
    case FloatType => castToFloat
    case LongType => castToLong
    case DoubleType => castToDouble
  }

Any idea? Thank you.

Hao



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/scala-MatchError-on-SparkSQL-when-creating-ArrayType-of-StructType-tp9623.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:07:10 2014
Return-Path: <dev-return-10651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A44FC705
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:07:10 +0000 (UTC)
Received: (qmail 73013 invoked by uid 500); 4 Dec 2014 13:07:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72941 invoked by uid 500); 4 Dec 2014 13:07:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47595 invoked by uid 99); 4 Dec 2014 02:00:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 02:00:28 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 02:00:23 +0000
Received: by mail-pd0-f177.google.com with SMTP id ft15so16510517pdb.22
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 17:59:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=F8QmE3wgHeeXd269xfmO50koaq4azx2hAGaCkLiR7Nw=;
        b=daD2+nvzIiHtU22q1LZ7/QqPGqXsp8Qpu12vfdn6FBzlzzq8IzCm0iDZpbkkVxG3Y5
         aJKh4ikQsZ0zkn9afeQNqkOTNGhfETVnTswFNizOtzbfCNW4mnJMso6xqdC4u0RgXIfk
         3lRHqig5JIFuJLNIp/lOaEKBM1DA0t9AwDER6tAHr2K8OElaIGV0tJMcywXVbmrofvyn
         wyLV3Bb6L1pFr/pAjfR0sQ+LtgRmEfSTrWJRfsVENcIoWd6sOGfUEwbI1PZoWyRglAhr
         JHij8owNDGltuS/dgtwHXySH1++y/JBe7uJTS/gUaCSqCN8fX30im/6JI+a3XZVCvju0
         x8Gg==
MIME-Version: 1.0
X-Received: by 10.70.94.197 with SMTP id de5mr13642463pdb.161.1417658357654;
 Wed, 03 Dec 2014 17:59:17 -0800 (PST)
Received: by 10.70.37.230 with HTTP; Wed, 3 Dec 2014 17:59:17 -0800 (PST)
In-Reply-To: <CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<CAOTBr2=5S=cxYfi7OE4g_8nyOWsdFabaSCsJ2fkWsKHY9Nn=6Q@mail.gmail.com>
	<CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
Date: Wed, 3 Dec 2014 17:59:17 -0800
Message-ID: <CAOTBr2=SzXGv8fkfVXBo2umGKoJyOO40c7f7pU5N9Z-pH7YrRg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Krishna Sankar <ksankar42@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ddb2d22a9e05095a4e44
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ddb2d22a9e05095a4e44
Content-Type: text/plain; charset=UTF-8

Will do. Am on the road - will annotate an iPython notebook with what works
& what didn't work ...
Cheers
<k/>

On Wed, Dec 3, 2014 at 4:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Krishna, could you send me some code snippets for the issues you saw
> in naive Bayes and k-means? -Xiangrui
>
> On Sun, Nov 30, 2014 at 6:49 AM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
> > +1
> > 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
> > -Dhadoop.version=2.4.0 -DskipTests clean package 16:46 min (slightly
> slower
> > connection)
> > 2. Tested pyspark, mlib - running as well as compare esults with 1.1.x
> > 2.1. statistics OK
> > 2.2. Linear/Ridge/Laso Regression OK
> >        Slight difference in the print method (vs. 1.1.x) of the model
> > object - with a label & more details. This is good.
> > 2.3. Decision Tree, Naive Bayes OK
> >        Changes in print(model) - now print (model.ToDebugString()) - OK
> >        Some changes in NaiveBayes. Different from my 1.1.x code - had to
> > flatten list structures, zip required same number in partitions
> >        After code changes ran fine.
> > 2.4. KMeans OK
> >        zip occasionally fails with error "localhost):
> > org.apache.spark.SparkException: Can only zip RDDs with same number of
> > elements in each partition"
> > Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
> > Made it work by doing a different transformation ie reusing an original
> > rdd.
> > 2.5. rdd operations OK
> >        State of the Union Texts - MapReduce, Filter,sortByKey (word
> count)
> > 2.6. recommendation OK
> > 2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
> > dataset which never worked. Works fine in 1.2.0 !
> > 3. Scala Mlib - subset of examples as in #2 above, with Scala
> > 3.1. statistics OK
> > 3.2. Linear Regression OK
> > 3.3. Decision Tree OK
> > 3.4. KMeans OK
> > Cheers
> > <k/>
> > P.S: Plan to add RF and .ml mechanics to this bank
> >
> > On Fri, Nov 28, 2014 at 9:16 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark version
> >> 1.2.0!
> >>
> >> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1048/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.2.0!
> >>
> >> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> >> if a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.1.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening very late into the QA period compared with
> >> previous votes, so -1 votes should only occur for significant
> >> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> >> regressions, or bugs related to new features will not block this
> >> release.
> >>
> >> == What default changes should I be aware of? ==
> >> 1. The default value of "spark.shuffle.blockTransferService" has been
> >> changed to "netty"
> >> --> Old behavior can be restored by switching to "nio"
> >>
> >> 2. The default value of "spark.shuffle.manager" has been changed to
> "sort".
> >> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> >> "hash".
> >>
> >> == Other notes ==
> >> Because this vote is occurring over a weekend, I will likely extend
> >> the vote if this RC survives until the end of the vote period.
> >>
> >> - Patrick
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>

--001a11c2ddb2d22a9e05095a4e44--

From dev-return-10652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:11:38 2014
Return-Path: <dev-return-10652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEB98C780
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:11:38 +0000 (UTC)
Received: (qmail 20022 invoked by uid 500); 4 Dec 2014 13:11:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19956 invoked by uid 500); 4 Dec 2014 13:11:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59612 invoked by uid 99); 4 Dec 2014 06:04:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 06:04:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lochanac@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 06:04:29 +0000
Received: by mail-pd0-f169.google.com with SMTP id z10so1876473pdj.28
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 22:04:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject
         :content-type;
        bh=Xc0+5NI/1C8ZcPpc+/B4Cvh5+MmkoBe+B5sDpc+3xuE=;
        b=b4Z9KE2adnvl3bRdbyacb24ugPipEXVfAuls+vJACZ3E+3eo4KyPBW8Rfv0U0sp6xN
         n87ZvvgTzlfwb9UusLAIh45Goeqy/VvnlH1nCr1UPDRNn9JQZufXol6Qlhp2UsIzwX2Y
         2XqWdQMsVkXyUMpl9BqiCgiT6qVlSW2rmMxBqenhPVJZ+1RFW5RPbqROjV3EKyNH10H/
         PFq82oF50pVZEsiOVeWja88fc6egubynsDMXVDUVY7FI4Q76bFj0GjMKfGuuCR4vYBaH
         7T72t7xE64Y8ZXikr6/3kEEOzEGY05wJT1gsKknQ+ve7RdfRBrEx7ge7VfcaVdTHzLh/
         h1eg==
X-Received: by 10.68.131.163 with SMTP id on3mr22271390pbb.169.1417673068313;
        Wed, 03 Dec 2014 22:04:28 -0800 (PST)
Received: from Lochanas-MacBook-Pro.local ([203.94.95.4])
        by mx.google.com with ESMTPSA id d6sm24838956pdn.40.2014.12.03.22.04.26
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 03 Dec 2014 22:04:27 -0800 (PST)
Message-ID: <547FF96F.2030403@gmail.com>
Date: Thu, 04 Dec 2014 11:34:31 +0530
From: Lochana Menikarachchi <lochanac@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: spark osgi class loading issue
Content-Type: multipart/alternative;
 boundary="------------070008050800080001090905"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------070008050800080001090905
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

We are trying to call spark through an osgi service (with osgifyed 
version of assembly.jar). Spark does not work (due to the way spark 
reads akka reference.conf) unless we switch the class loader as follows.

Thread.currentThread().setContextClassLoader(JavaSparkContext.class.getClassLoader());

The problem is there is no way to switch between class loaders and get the information generated by spark operations.
Is there a way to run spark through an osgi service. I think if we can parse the reference.conf some other way this might work. Can somebody shed some light on this..

Thanks.

Lochana




--------------070008050800080001090905--

From dev-return-10653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:17:44 2014
Return-Path: <dev-return-10653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5A86C82C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:17:44 +0000 (UTC)
Received: (qmail 85510 invoked by uid 500); 4 Dec 2014 13:17:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85441 invoked by uid 500); 4 Dec 2014 13:17:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53180 invoked by uid 99); 3 Dec 2014 23:51:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 23:51:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.180 as permitted sender)
Received: from [209.85.160.180] (HELO mail-yk0-f180.google.com) (209.85.160.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 23:50:38 +0000
Received: by mail-yk0-f180.google.com with SMTP id 9so7369898ykp.25
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 15:49:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=aHkqa+dzasd7zqM4a+WNrihSSFRZ05tEkc2wMHNV9cM=;
        b=Pl6msTGKwtgczwK4XjCFofGhdcAlvOgwoF5oyyR2yB7pj/LLgU6UGE6SRMcUpAv41E
         jg5YwY37OsCxJ0Wpbm2EmTF6WnSmfd6X0Yg2PFIV3jk/auyfCG+Gk+v2z05wvuk9x0a3
         F5isG7W6kdm0AfsLMX6QkdUpWYroanBcUBhSGJJFLA1kFgQCCR/ThyTnsi2t0GJPKbqh
         NmTpks1odguU7mrgZ2ceMUuftmmKKS5p29ZZC6PrRjrS3iH1nvTLTlXMn0Rbu10eEUq6
         d+Vq2NgO8Gu0o9NK75jx1fghrXZRZEQC5gR0CV/qWOKlidhKRcC5VLQSDErPl1QrOEVT
         IPyw==
X-Received: by 10.236.202.173 with SMTP id d33mr9712495yho.184.1417650592272;
 Wed, 03 Dec 2014 15:49:52 -0800 (PST)
MIME-Version: 1.0
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
 <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com> <CAAsvFP=V9FL=KvXNUeWVfFz8q04oj1exdwSaSNiab5Vc9hFkUg@mail.gmail.com>
 <CANeJXFMRj2JdyrAQsBvyK3s3t+R1_QBeDSzqQ-OKUC2WhdfSvQ@mail.gmail.com>
 <CAAOnQ7tAEWS5dEX4bG6ik6j+820er3RCokwP3+A9caFyExDrdg@mail.gmail.com>
 <CANeJXFORRC0rOSq5KJuWURCVNeMPE6JT5krE9E+kHP4ZQh5QgA@mail.gmail.com>
 <CAAOnQ7vggDLGyH4oVpZw9UzgcAs8JmwVymOe2dTc0HxijQ0rrw@mail.gmail.com>
 <CANeJXFP7x2WSVVqGJKSAYRjGOXowGNKn2bVxHCh7QLsL8oh4jg@mail.gmail.com> <CAAOnQ7undG8AHGGSFRBu=B3BQB7DahZVViNOTLc=Q_BNjb_W4Q@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Wed, 03 Dec 2014 23:49:50 +0000
Message-ID: <CANeJXFO4xJ8-mDj+FDJ=vB8Uu54ZKd3nTTaUz__57fLim=bB3g@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01537e0cf7db780509587f10
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01537e0cf7db780509587f10
Content-Type: text/plain; charset=UTF-8

Thanks Marcelo, "this is just how Maven works (unfortunately)" answers my
question.

Another related question: I tried to use `mvn scala:cc` and discovered that
it only seems to work scan src/main and src/test directories (according to its
docs <http://scala-tools.org/mvnsites/maven-scala-plugin/usage_cc.html>),
and so can only be run from within submodules, not from the root directory.

I'll add a note about this to building-spark.html unless there is a way to
do it for all modules / from the root directory that I've missed. Let me
know!




On Tue Dec 02 2014 at 5:49:58 PM Marcelo Vanzin <vanzin@cloudera.com> wrote:

> On Tue, Dec 2, 2014 at 4:40 PM, Ryan Williams
> <ryan.blake.williams@gmail.com> wrote:
> >> But you only need to compile the others once.
> >
> > once... every time I rebase off master, or am obliged to `mvn clean` by
> some
> > other build-correctness bug, as I said before. In my experience this
> works
> > out to a few times per week.
>
> No, you only need to do it something upstream from core changed (i.e.,
> spark-parent, network/common or network/shuffle) in an incompatible
> way. Otherwise, you can rebase and just recompile / retest core,
> without having to install everything else. I do this kind of thing all
> the time. If you have to do "mvn clean" often you're probably doing
> something wrong somewhere else.
>
> I understand where you're coming from, but the way you're thinking is
> just not how maven works. I too find annoying that maven requires lots
> of things to be "installed" before you can use them, when they're all
> part of the same project. But well, that's the way things are.
>
> --
> Marcelo
>

--089e01537e0cf7db780509587f10--

From dev-return-10654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 13:35:29 2014
Return-Path: <dev-return-10654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 074B3CA1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 13:35:29 +0000 (UTC)
Received: (qmail 32881 invoked by uid 500); 4 Dec 2014 13:35:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32809 invoked by uid 500); 4 Dec 2014 13:35:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67914 invoked by uid 99); 3 Dec 2014 21:35:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 21:35:27 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 03 Dec 2014 21:35:00 +0000
Received: by mail-lb0-f170.google.com with SMTP id w7so13154248lbi.15
        for <dev@spark.apache.org>; Wed, 03 Dec 2014 13:34:14 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=3Q8x+1Mvhpdr/hrRyKteR6hSzoTDKfQao+AEw0hToZI=;
        b=YSpOhAnEFd6fxmZOX77OiLHHlPWfhYu/9jPKV9D5QsaeCgJ1MxZBCFM1fFV05FrMDS
         aNXirD3+X4OjwKa5Y2iu2XFmw+xujtza0LN+U7Hau1tsmgwHSwQv4aq5/Rla8rmrfGAs
         /nJT70lhoSx1pOa37zmr5DdC1I0s2ntSFfniSE/OBJYM/ODrdZntTerzYppNChPJONSc
         nUevYiTxMuCAlB7lkyERiGGraeb3HIcEBzd04AK50En6pYVy9bskVdME0+KbvRx/gXSS
         xnCZVaZKjn5ishlTdLjhUKFJkRL7Kg4FVe3bs1p/LhBMlXRE01GmLva9ylbbvFx6jUJF
         UB0Q==
X-Gm-Message-State: ALoCoQmGFYH/11dXaGudJjRKo0P0yT5RR89vCdBf12pqSnxB1nEWfQP68jsbF9oGGfogzkpxITY6
MIME-Version: 1.0
X-Received: by 10.112.162.101 with SMTP id xz5mr6074141lbb.49.1417642454444;
 Wed, 03 Dec 2014 13:34:14 -0800 (PST)
Received: by 10.112.112.66 with HTTP; Wed, 3 Dec 2014 13:34:14 -0800 (PST)
In-Reply-To: <CABPQxsuXEPu-oXHVcN9azSH=1B5eeACoB6dLDjXdnEH8A2Tw5g@mail.gmail.com>
References: <CAKJXNjHdX0Rqpgn7XJ_6GF2Chz4yE=oh_RY-UqbsWDeTuky6=Q@mail.gmail.com>
	<CAJiQeYLEYppjbvUsF_ww1Ezeye=qU1C6nPa_yDueiQzXE_haOw@mail.gmail.com>
	<CABPQxsuXEPu-oXHVcN9azSH=1B5eeACoB6dLDjXdnEH8A2Tw5g@mail.gmail.com>
Date: Wed, 3 Dec 2014 13:34:14 -0800
Message-ID: <CAMJOb8muYOh8zQKtz-27AUOeRBF5DSmD=FA7UPy41kP1fyBd2A@mail.gmail.com>
Subject: Re: keeping PR titles / descriptions up to date
From: Andrew Or <andrew@databricks.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Mridul Muralidharan <mridul@gmail.com>, Kay Ousterhout <kayousterhout@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01182b1aea89b10509569a8d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182b1aea89b10509569a8d
Content-Type: text/plain; charset=UTF-8

I realize we're not voting, but +1 to this proposal since commit messages
can't be changed whereas JIRA issues can always be updated after the fact.

2014-12-02 13:05 GMT-08:00 Patrick Wendell <pwendell@gmail.com>:

> Also a note on this for committers - it's possible to re-word the
> title during merging, by just running "git commit -a --amend" before
> you push the PR.
>
> - Patrick
>
> On Tue, Dec 2, 2014 at 12:50 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > I second that !
> > Would also be great if the JIRA was updated accordingly too.
> >
> > Regards,
> > Mridul
> >
> >
> > On Wed, Dec 3, 2014 at 1:53 AM, Kay Ousterhout <kayousterhout@gmail.com>
> wrote:
> >> Hi all,
> >>
> >> I've noticed a bunch of times lately where a pull request changes to be
> >> pretty different from the original pull request, and the title /
> >> description never get updated.  Because the pull request title and
> >> description are used as the commit message, the incorrect description
> lives
> >> on forever, making it harder to understand the reason behind a
> particular
> >> commit without going back and reading the entire conversation on the
> pull
> >> request.  If folks could try to keep these up to date (and committers,
> try
> >> to remember to verify that the title and description are correct before
> >> making merging pull requests), that would be awesome.
> >>
> >> -Kay
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e01182b1aea89b10509569a8d--

From dev-return-10655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 14:32:09 2014
Return-Path: <dev-return-10655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DEA9CCF9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 14:32:09 +0000 (UTC)
Received: (qmail 26013 invoked by uid 500); 4 Dec 2014 14:32:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25931 invoked by uid 500); 4 Dec 2014 14:32:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25909 invoked by uid 99); 4 Dec 2014 14:32:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 14:32:07 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ludwine.probst@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 14:32:02 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 1E678C9AFB5
	for <dev@spark.incubator.apache.org>; Thu,  4 Dec 2014 06:31:43 -0800 (PST)
Date: Thu, 4 Dec 2014 07:31:41 -0700 (MST)
From: nivdul <ludwine.probst@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417703501757-9638.post@n3.nabble.com>
Subject: Dependent on multiple versions of servlet-api jars lead to throw an
 SecurityException when Spark built for hadoop 2.5.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi !

I have the same issue that this  one
<https://issues.apache.org/jira/browse/SPARK-1693>   but using the version
2.5 of Hadoop.

The fix bug is  here
<https://github.com/witgo/spark/commit/dc63905908cb7c84c741bb5fdc4ad7d4abdcb0b2>  
for Hadoop 2.4 and 2.3.

For now I just changed my version of Hadoop but I would like to use the 2.5
one.

Any suggestion ?

Ludwine





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Dependent-on-multiple-versions-of-servlet-api-jars-lead-to-throw-an-SecurityException-when-Spark-bui0-tp9638.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 15:12:31 2014
Return-Path: <dev-return-10656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC75CCE69
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 15:12:31 +0000 (UTC)
Received: (qmail 40885 invoked by uid 500); 4 Dec 2014 15:12:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40814 invoked by uid 500); 4 Dec 2014 15:12:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40777 invoked by uid 99); 4 Dec 2014 15:12:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 15:12:29 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS,UNPARSEABLE_RELAY
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jinkui.sjk@alibaba-inc.com designates 42.120.133.50 as permitted sender)
Received: from [42.120.133.50] (HELO out4133-50.mail.aliyun.com) (42.120.133.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 15:12:01 +0000
DKIM-Signature:v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=alibaba-inc.com; s=default;
	t=1417705915; h=From:Content-Type:Subject:Message-Id:Date:To:Mime-Version;
	bh=3y+UA8sWwu+rnBI9hIlDAVUDlRkXAkPi7+mtiYOe7C8=;
	b=AvcQdbc4zTEO7ymio3/4ZjKx4MKqBH8/csXvNC0XkRxEwG8wW2WF8qdChRgpR0H4U8ksp0YNSSGadK9RVM4DUmkK+suQig2QvgimavJa3Y8hAB6U5kE6/fSV3Rzs1N/njCp/FUpK+l1JxE3KS/pREtD6cns0tmWR7xvDHzcay6M=
X-Alimail-AntiSpam:AC=PASS;BC=-1|-1;BR=01201311R171e4;FP=0|-1|-1|-1|0|-1|-1|-1;HT=r41g08156;MF=jinkui.sjk@alibaba-inc.com;PH=DS;RN=1;RT=1;SR=0;
Received: from 10.65.144.196(mailfrom:jinkui.sjk@alibaba-inc.com ip:42.120.74.148)
          by smtp.aliyun-inc.com(127.0.0.1);
          Thu, 04 Dec 2014 23:11:42 +0800
From: "jinkui.sjk" <jinkui.sjk@alibaba-inc.com>
Content-Type: multipart/signed; boundary="Apple-Mail=_0F60B3F5-9E6C-4E99-BAB4-BC4A222F7979"; protocol="application/pkcs7-signature"; micalg=sha1
Subject: a question of  Graph build api 
Message-Id: <3C34860E-9308-4039-8E48-84C7314F26CD@alibaba-inc.com>
Date: Thu, 4 Dec 2014 23:11:41 +0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_0F60B3F5-9E6C-4E99-BAB4-BC4A222F7979
Content-Type: multipart/alternative;
	boundary="Apple-Mail=_BB9B3D8C-6319-4A45-A9D5-A290E198DCC7"


--Apple-Mail=_BB9B3D8C-6319-4A45-A9D5-A290E198DCC7
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

hi, all

where build a graph from edge tuples with api  Graph.fromEdgeTuples,=20
the edges object type is RDD[Edge], inside of  EdgeRDD.fromEdge,  =
EdgePartitionBuilder.add func=E2=80=99s param is better to be Edge =
object.
no need to create a new Edge object again.



  def fromEdgeTuples[VD: ClassTag](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] =3D None,
      edgeStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY,
      vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY): =
Graph[VD, Int] =3D
  {
    val edges =3D rawEdges.map(p =3D> Edge(p._1, p._2, 1))
    val graph =3D GraphImpl(edges, defaultValue, edgeStorageLevel, =
vertexStorageLevel)
    uniqueEdges match {
      case Some(p) =3D> graph.partitionBy(p).groupEdges((a, b) =3D> a + =
b)
      case None =3D> graph
    }
  }




  object GraphImpl {

  /** Create a graph from edges, setting referenced vertices to =
`defaultVertexAttr`. */
  def apply[VD: ClassTag, ED: ClassTag](
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD,
      edgeStorageLevel: StorageLevel,
      vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] =3D {
    fromEdgeRDD(EdgeRDD.fromEdges(edges), defaultVertexAttr, =
edgeStorageLevel, vertexStorageLevel)
  }



  object EdgeRDD {
  /**
   * Creates an EdgeRDD from a set of edges.
   *
   * @tparam ED the edge attribute type
   * @tparam VD the type of the vertex attributes that may be joined =
with the returned EdgeRDD
   */
  def fromEdges[ED: ClassTag, VD: ClassTag](edges: RDD[Edge[ED]]): =
EdgeRDD[ED, VD] =3D {
    val edgePartitions =3D edges.mapPartitionsWithIndex { (pid, iter) =3D>=

      val builder =3D new EdgePartitionBuilder[ED, VD]
      iter.foreach { e =3D>
        builder.add(e.srcId, e.dstId, e.attr)
      }
      Iterator((pid, builder.toEdgePartition))
    }
    EdgeRDD.fromEdgePartitions(edgePartitions)
  }



--Apple-Mail=_BB9B3D8C-6319-4A45-A9D5-A290E198DCC7
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html =
charset=3Dutf-8"></head><body style=3D"word-wrap: break-word; =
-webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" =
class=3D""><div class=3D"">hi, all</div><div class=3D""><br =
class=3D""></div><div class=3D"">where build a graph from edge tuples =
with api &nbsp;Graph.fromEdgeTuples,&nbsp;</div><div class=3D"">the =
edges object type is RDD[Edge], inside of &nbsp;EdgeRDD.fromEdge, =
&nbsp;EdgePartitionBuilder.add func=E2=80=99s param is better to be Edge =
object.</div><div class=3D"">no need to create a new Edge object =
again.</div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; def fromEdgeTuples[VD: ClassTag](</div><div =
class=3D""><div class=3D"">&nbsp; &nbsp; &nbsp; rawEdges: RDD[(VertexId, =
VertexId)],</div><div class=3D"">&nbsp; &nbsp; &nbsp; defaultValue: =
VD,</div><div class=3D"">&nbsp; &nbsp; &nbsp; uniqueEdges: =
Option[PartitionStrategy] =3D None,</div><div class=3D"">&nbsp; &nbsp; =
&nbsp; edgeStorageLevel: StorageLevel =3D =
StorageLevel.MEMORY_ONLY,</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY): =
Graph[VD, Int] =3D</div><div class=3D"">&nbsp; {</div><div class=3D""><b =
class=3D"">&nbsp; &nbsp; val edges =3D rawEdges.map(p =3D&gt; Edge(p._1, =
p._2, 1))</b></div><div class=3D"">&nbsp; &nbsp; val graph =3D =
GraphImpl(edges, defaultValue, edgeStorageLevel, =
vertexStorageLevel)</div><div class=3D"">&nbsp; &nbsp; uniqueEdges match =
{</div><div class=3D"">&nbsp; &nbsp; &nbsp; case Some(p) =3D&gt; =
graph.partitionBy(p).groupEdges((a, b) =3D&gt; a + b)</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; case None =3D&gt; graph</div><div =
class=3D"">&nbsp; &nbsp; }</div><div class=3D"">&nbsp; }</div><div =
class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; object GraphImpl {</div><div class=3D""><br =
class=3D""></div><div class=3D"">&nbsp; /** Create a graph from edges, =
setting referenced vertices to `defaultVertexAttr`. */</div><div =
class=3D"">&nbsp; def apply[VD: ClassTag, ED: ClassTag](</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; edges: RDD[Edge[ED]],</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; defaultVertexAttr: VD,</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; edgeStorageLevel: =
StorageLevel,</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] =3D {</div><div =
class=3D"">&nbsp; &nbsp; fromEdgeRDD(<b =
class=3D"">EdgeRDD.fromEdges(edges)</b>, defaultVertexAttr, =
edgeStorageLevel, vertexStorageLevel)</div><div class=3D"">&nbsp; =
}</div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; object EdgeRDD {</div><div class=3D"">&nbsp; =
/**</div><div class=3D"">&nbsp; &nbsp;* Creates an EdgeRDD from a set of =
edges.</div><div class=3D"">&nbsp; &nbsp;*</div><div class=3D"">&nbsp; =
&nbsp;* @tparam ED the edge attribute type</div><div class=3D"">&nbsp; =
&nbsp;* @tparam VD the type of the vertex attributes that may be joined =
with the returned EdgeRDD</div><div class=3D"">&nbsp; &nbsp;*/</div><div =
class=3D"">&nbsp; def fromEdges[ED: ClassTag, VD: ClassTag](edges: =
RDD[Edge[ED]]): EdgeRDD[ED, VD] =3D {</div><div class=3D"">&nbsp; &nbsp; =
val edgePartitions =3D edges.mapPartitionsWithIndex { (pid, iter) =
=3D&gt;</div><div class=3D"">&nbsp; &nbsp; &nbsp; val builder =3D new =
EdgePartitionBuilder[ED, VD]</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
iter.foreach { e =3D&gt;</div><div class=3D""><b class=3D"">&nbsp; =
&nbsp; &nbsp; &nbsp; builder.add(e.srcId, e.dstId, e.attr)</b></div><div =
class=3D"">&nbsp; &nbsp; &nbsp; }</div><div class=3D"">&nbsp; &nbsp; =
&nbsp; Iterator((pid, builder.toEdgePartition))</div><div =
class=3D"">&nbsp; &nbsp; }</div><div class=3D"">&nbsp; &nbsp; =
EdgeRDD.fromEdgePartitions(edgePartitions)</div><div class=3D"">&nbsp; =
}</div></div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div></body></html>=

--Apple-Mail=_BB9B3D8C-6319-4A45-A9D5-A290E198DCC7--

--Apple-Mail=_0F60B3F5-9E6C-4E99-BAB4-BC4A222F7979
Content-Disposition: attachment;
	filename=smime.p7s
Content-Type: application/pkcs7-signature;
	name=smime.p7s
Content-Transfer-Encoding: base64

MIAGCSqGSIb3DQEHAqCAMIACAQExCzAJBgUrDgMCGgUAMIAGCSqGSIb3DQEHAQAAoIIG7DCCAzow
ggKjoAMCAQICAQMwDQYJKoZIhvcNAQEFBQAwgZAxJjAkBgkqhkiG9w0BCQEWF2FsaWxhbmdAYWxp
YmFiYS1pbmMuY29tMRMwEQYDVQQDDApBbGlsYW5nIENBMQwwCgYDVQQLDANJbmMxEDAOBgNVBAoM
B0FsaWJhYmExETAPBgNVBAcMCEhhbmdaaG91MREwDwYDVQQIDAhaaGVKaWFuZzELMAkGA1UEBhMC
Q04wHhcNMTQwMzIxMTgyNjQ4WhcNMzQwMzE2MTgyNjQ4WjA/MR0wGwYDVQQDDBRBbGlsYW5nIENs
YXNzIDMgUm9vdDEMMAoGA1UECwwDSW5jMRAwDgYDVQQKDAdBbGliYWJhMIGfMA0GCSqGSIb3DQEB
AQUAA4GNADCBiQKBgQDkSZUxhsryrulw7ShMNYBuF61+lkYT1/ydmAXyMyMXhSGaKJo6onqdui8W
3X8TZ44E5ZMEMM3rTb0o3+t6N2G9qstbpfm/wnxlNsdFKBZAQpsTp7YKDaVGo1iMir6NE5VXtlZc
ZlfdXLncAGxGIDITf6+7nvWWU2eQCAdeGH4+iwIDAQABo4HzMIHwMB0GA1UdDgQWBBS8GDnn09jS
1FKSW9ID4Szifw9bNTCBvQYDVR0jBIG1MIGygBRsQqcG9DDm61l1oc2mKWiF5GLUraGBlqSBkzCB
kDEmMCQGCSqGSIb3DQEJARYXYWxpbGFuZ0BhbGliYWJhLWluYy5jb20xEzARBgNVBAMMCkFsaWxh
bmcgQ0ExDDAKBgNVBAsMA0luYzEQMA4GA1UECgwHQWxpYmFiYTERMA8GA1UEBwwISGFuZ1pob3Ux
ETAPBgNVBAgMCFpoZUppYW5nMQswCQYDVQQGEwJDToIBATAPBgNVHRMBAf8EBTADAQH/MA0GCSqG
SIb3DQEBBQUAA4GBAAfkwgwem9gawUb7wU4oiGw2aCw6QbKgfSBv+Dj6RiA4CJztW0IF6g6cRM85
Uv6+AoGyC14x871UJBHNt75ZBgSyvZpXu2n3BLQmTe3gQNXsipOvCSlM2V98U/KVKRgUgzb6dNDd
jMzv9yEIGuar9zbk5GRDDSQR+qTM+a8i2umFMIIDqjCCAxOgAwIBAgIGAUjEHmeLMA0GCSqGSIb3
DQEBBQUAMD8xHTAbBgNVBAMMFEFsaWxhbmcgQ2xhc3MgMyBSb290MQwwCgYDVQQLDANJbmMxEDAO
BgNVBAoMB0FsaWJhYmEwHhcNMTQwOTI5MDExMzIwWhcNMTkwOTI4MDExMzIwWjCB2jEpMCcGCSqG
SIb3DQEJARYaamlua3VpLnNqa0BhbGliYWJhLWluYy5jb20xDjAMBgNVBAMMBTY3NDExMUowSAYD
VQQMDEE2NzQxMXxDNmQ1YzI3MDFlNjA2MDYyNTkwODY5OGU2NzlmMTRkMTJ8NDhkNzA1YjU3N2U5
XzMyMDAxZWY4ODAwMDEMMAoGA1UECwwDSW5jMRAwDgYDVQQKDAdBbGliYWJhMREwDwYDVQQHDAhI
YW5nWmhvdTERMA8GA1UECAwIWmhlSmlhbmcxCzAJBgNVBAYTAkNOMIGfMA0GCSqGSIb3DQEBAQUA
A4GNADCBiQKBgQCp+oc+kuIkaFyo0VABXQAKjchegSQ/DRA8rWdwQySeYT3F2KmNUNGarnZj115A
KKpocR/8Gwgcdhe7ngdFEv6noqaR0pjw1d7JP6zV0fHee2BP8Ecp6Z1a571lQdfGI36CDhv+4DO6
3R3DUAjLcQmOq9ighaKfBK+2GZkcBAWHowIDAQABo4IBEzCCAQ8wJQYGKQEBAg0eBBsEGTQ4ZDcw
NWI1NzdlOV8zMjAwMWVmODgwMDAwDwYGKQEBAgQeBAUEA29zeDAtBgYpAQECDB4EIwQhQzZkNWMy
NzAxZTYwNjA2MjU5MDg2OThlNjc5ZjE0ZDEyMBEGBikBAQILHgQHBAU2NzQxMTBFBgNVHSUEPjA8
BggrBgEFBQcDAQYIKwYBBQUHAwIGCCsGAQUFBwMEBggrBgEFBQcDDQYIKwYBBQUHAw4GCCsGAQUF
BwMRMAwGA1UdDwQFAwMH/4AwHQYDVR0OBBYEFOaDGsNJIpJZkp0/hFVnzXtPTNiMMB8GA1UdIwQY
MBaAFLwYOefT2NLUUpJb0gPhLOJ/D1s1MA0GCSqGSIb3DQEBBQUAA4GBAN1RT267BNdCVTAzdnyK
F9knaLO5LHcUQF0lQQ4QVVmjYFD7GM229RM/dqTtL/SWg1SyjB+N8wPtgUXWGxyyAyyYUlh8D1SA
SkY+l8Di2Z3zdD4XzwShUn3Tsp8/MwioOTJwok509mM+WuXg5t1N8MKyeh+53EdM6xjAjkcckKcM
MYICBjCCAgICAQEwSTA/MR0wGwYDVQQDDBRBbGlsYW5nIENsYXNzIDMgUm9vdDEMMAoGA1UECwwD
SW5jMRAwDgYDVQQKDAdBbGliYWJhAgYBSMQeZ4swCQYFKw4DAhoFAKCCARMwGAYJKoZIhvcNAQkD
MQsGCSqGSIb3DQEHATAcBgkqhkiG9w0BCQUxDxcNMTQxMjA0MTUxMTQyWjAjBgkqhkiG9w0BCQQx
FgQUdx/UhEtdoi/VSq/xGgASE/wqma4wWAYJKwYBBAGCNxAEMUswSTA/MR0wGwYDVQQDDBRBbGls
YW5nIENsYXNzIDMgUm9vdDEMMAoGA1UECwwDSW5jMRAwDgYDVQQKDAdBbGliYWJhAgYBSMQeZ4sw
WgYLKoZIhvcNAQkQAgsxS6BJMD8xHTAbBgNVBAMMFEFsaWxhbmcgQ2xhc3MgMyBSb290MQwwCgYD
VQQLDANJbmMxEDAOBgNVBAoMB0FsaWJhYmECBgFIxB5nizANBgkqhkiG9w0BAQEFAASBgHBqM2gy
ow33010bYOhyABuBGjXPE7ER5ikCMACCzAuPSJDx6qBApKFSTm3v0gTuadTvYTCQvHPInYG/dV3U
tP5DPIW5ioTwvkUxyjwKDImiGDkKEk4SV0c8bM9EymXn1tPxLtsEFGHcaGZcOye+nIO0Z1O5DEXv
yNyCP5spvR3zAAAAAAAA
--Apple-Mail=_0F60B3F5-9E6C-4E99-BAB4-BC4A222F7979--

From dev-return-10657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 15:18:21 2014
Return-Path: <dev-return-10657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7DD5CEA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 15:18:21 +0000 (UTC)
Received: (qmail 56728 invoked by uid 500); 4 Dec 2014 15:18:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56648 invoked by uid 500); 4 Dec 2014 15:18:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56621 invoked by uid 99); 4 Dec 2014 15:18:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 15:18:18 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS,UNPARSEABLE_RELAY
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jinkui.sjk@alibaba-inc.com designates 42.120.133.82 as permitted sender)
Received: from [42.120.133.82] (HELO out4133-82.mail.aliyun.com) (42.120.133.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 15:17:52 +0000
DKIM-Signature:v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=alibaba-inc.com; s=default;
	t=1417705970; h=From:Content-Type:Subject:Message-Id:Date:To:Mime-Version;
	bh=HfIKKSuo3lPWo+NHA03OT12p3hs65zRdDGVigrUF9+Q=;
	b=druhANjwb5vGi07+IH9YfRfTmh9IklxQJXW5/zwh02NZIrx7b9hETGFbTOyXJur3qmDo/p+6MTdGh4he7O79NCZgq+cdelilssoVtYNRKNs4zO4342jReWlhkuiaVkmRMgkHeeaBbRGV5lGdpkboHvWgBJj+8CN2l6D9mOQnrUo=
X-Alimail-AntiSpam:AC=PASS;BC=-1|-1;BR=01201311R951e4;FP=0|-1|-1|-1|0|-1|-1|-1;HT=r46d02005;MF=jinkui.sjk@alibaba-inc.com;PH=DS;RN=1;RT=1;SR=0;
Received: from 10.65.144.196(mailfrom:jinkui.sjk@alibaba-inc.com ip:42.120.74.148)
          by smtp.aliyun-inc.com(127.0.0.1);
          Thu, 04 Dec 2014 23:12:38 +0800
From: jinkui.sjk <jinkui.sjk@alibaba-inc.com>
Content-Type: multipart/signed; boundary="Apple-Mail=_2FAF45F8-FD2A-45C0-AB40-68870E5092E4"; protocol="application/pkcs7-signature"; micalg=sha1
Subject: a question of  Graph build api 
Message-Id: <E5BFC5AC-7208-48CE-AAC7-F963EF2AF791@alibaba-inc.com>
Date: Thu, 4 Dec 2014 23:12:37 +0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_2FAF45F8-FD2A-45C0-AB40-68870E5092E4
Content-Type: multipart/alternative;
	boundary="Apple-Mail=_F51C0590-4F62-49BF-B44D-28A865162090"


--Apple-Mail=_F51C0590-4F62-49BF-B44D-28A865162090
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

hi, all

where build a graph from edge tuples with api  Graph.fromEdgeTuples,=20
the edges object type is RDD[Edge], inside of  EdgeRDD.fromEdge,  =
EdgePartitionBuilder.add func=E2=80=99s param is better to be Edge =
object.
no need to create a new Edge object again.



  def fromEdgeTuples[VD: ClassTag](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] =3D None,
      edgeStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY,
      vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY): =
Graph[VD, Int] =3D
  {
    val edges =3D rawEdges.map(p =3D> Edge(p._1, p._2, 1))
    val graph =3D GraphImpl(edges, defaultValue, edgeStorageLevel, =
vertexStorageLevel)
    uniqueEdges match {
      case Some(p) =3D> graph.partitionBy(p).groupEdges((a, b) =3D> a + =
b)
      case None =3D> graph
    }
  }




  object GraphImpl {

  /** Create a graph from edges, setting referenced vertices to =
`defaultVertexAttr`. */
  def apply[VD: ClassTag, ED: ClassTag](
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD,
      edgeStorageLevel: StorageLevel,
      vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] =3D {
    fromEdgeRDD(EdgeRDD.fromEdges(edges), defaultVertexAttr, =
edgeStorageLevel, vertexStorageLevel)
  }



  object EdgeRDD {
  /**
   * Creates an EdgeRDD from a set of edges.
   *
   * @tparam ED the edge attribute type
   * @tparam VD the type of the vertex attributes that may be joined =
with the returned EdgeRDD
   */
  def fromEdges[ED: ClassTag, VD: ClassTag](edges: RDD[Edge[ED]]): =
EdgeRDD[ED, VD] =3D {
    val edgePartitions =3D edges.mapPartitionsWithIndex { (pid, iter) =3D>=

      val builder =3D new EdgePartitionBuilder[ED, VD]
      iter.foreach { e =3D>
        builder.add(e.srcId, e.dstId, e.attr)
      }
      Iterator((pid, builder.toEdgePartition))
    }
    EdgeRDD.fromEdgePartitions(edgePartitions)
  }



--Apple-Mail=_F51C0590-4F62-49BF-B44D-28A865162090
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html =
charset=3Dutf-8"></head><body style=3D"word-wrap: break-word; =
-webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" =
class=3D""><div class=3D"">hi, all</div><div class=3D""><br =
class=3D""></div><div class=3D"">where build a graph from edge tuples =
with api &nbsp;Graph.fromEdgeTuples,&nbsp;</div><div class=3D"">the =
edges object type is RDD[Edge], inside of &nbsp;EdgeRDD.fromEdge, =
&nbsp;EdgePartitionBuilder.add func=E2=80=99s param is better to be Edge =
object.</div><div class=3D"">no need to create a new Edge object =
again.</div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; def fromEdgeTuples[VD: ClassTag](</div><div =
class=3D""><div class=3D"">&nbsp; &nbsp; &nbsp; rawEdges: RDD[(VertexId, =
VertexId)],</div><div class=3D"">&nbsp; &nbsp; &nbsp; defaultValue: =
VD,</div><div class=3D"">&nbsp; &nbsp; &nbsp; uniqueEdges: =
Option[PartitionStrategy] =3D None,</div><div class=3D"">&nbsp; &nbsp; =
&nbsp; edgeStorageLevel: StorageLevel =3D =
StorageLevel.MEMORY_ONLY,</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY): =
Graph[VD, Int] =3D</div><div class=3D"">&nbsp; {</div><div class=3D""><b =
class=3D"">&nbsp; &nbsp; val edges =3D rawEdges.map(p =3D&gt; Edge(p._1, =
p._2, 1))</b></div><div class=3D"">&nbsp; &nbsp; val graph =3D =
GraphImpl(edges, defaultValue, edgeStorageLevel, =
vertexStorageLevel)</div><div class=3D"">&nbsp; &nbsp; uniqueEdges match =
{</div><div class=3D"">&nbsp; &nbsp; &nbsp; case Some(p) =3D&gt; =
graph.partitionBy(p).groupEdges((a, b) =3D&gt; a + b)</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; case None =3D&gt; graph</div><div =
class=3D"">&nbsp; &nbsp; }</div><div class=3D"">&nbsp; }</div><div =
class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; object GraphImpl {</div><div class=3D""><br =
class=3D""></div><div class=3D"">&nbsp; /** Create a graph from edges, =
setting referenced vertices to `defaultVertexAttr`. */</div><div =
class=3D"">&nbsp; def apply[VD: ClassTag, ED: ClassTag](</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; edges: RDD[Edge[ED]],</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; defaultVertexAttr: VD,</div><div =
class=3D"">&nbsp; &nbsp; &nbsp; edgeStorageLevel: =
StorageLevel,</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] =3D {</div><div =
class=3D"">&nbsp; &nbsp; fromEdgeRDD(<b =
class=3D"">EdgeRDD.fromEdges(edges)</b>, defaultVertexAttr, =
edgeStorageLevel, vertexStorageLevel)</div><div class=3D"">&nbsp; =
}</div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div><div class=3D""><br class=3D""></div><div =
class=3D"">&nbsp; object EdgeRDD {</div><div class=3D"">&nbsp; =
/**</div><div class=3D"">&nbsp; &nbsp;* Creates an EdgeRDD from a set of =
edges.</div><div class=3D"">&nbsp; &nbsp;*</div><div class=3D"">&nbsp; =
&nbsp;* @tparam ED the edge attribute type</div><div class=3D"">&nbsp; =
&nbsp;* @tparam VD the type of the vertex attributes that may be joined =
with the returned EdgeRDD</div><div class=3D"">&nbsp; &nbsp;*/</div><div =
class=3D"">&nbsp; def fromEdges[ED: ClassTag, VD: ClassTag](edges: =
RDD[Edge[ED]]): EdgeRDD[ED, VD] =3D {</div><div class=3D"">&nbsp; &nbsp; =
val edgePartitions =3D edges.mapPartitionsWithIndex { (pid, iter) =
=3D&gt;</div><div class=3D"">&nbsp; &nbsp; &nbsp; val builder =3D new =
EdgePartitionBuilder[ED, VD]</div><div class=3D"">&nbsp; &nbsp; &nbsp; =
iter.foreach { e =3D&gt;</div><div class=3D""><b class=3D"">&nbsp; =
&nbsp; &nbsp; &nbsp; builder.add(e.srcId, e.dstId, e.attr)</b></div><div =
class=3D"">&nbsp; &nbsp; &nbsp; }</div><div class=3D"">&nbsp; &nbsp; =
&nbsp; Iterator((pid, builder.toEdgePartition))</div><div =
class=3D"">&nbsp; &nbsp; }</div><div class=3D"">&nbsp; &nbsp; =
EdgeRDD.fromEdgePartitions(edgePartitions)</div><div class=3D"">&nbsp; =
}</div></div><div class=3D""><br class=3D""></div><div class=3D""><br =
class=3D""></div></body></html>=

--Apple-Mail=_F51C0590-4F62-49BF-B44D-28A865162090--

--Apple-Mail=_2FAF45F8-FD2A-45C0-AB40-68870E5092E4
Content-Disposition: attachment;
	filename=smime.p7s
Content-Type: application/pkcs7-signature;
	name=smime.p7s
Content-Transfer-Encoding: base64
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--Apple-Mail=_2FAF45F8-FD2A-45C0-AB40-68870E5092E4--

From dev-return-10658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 17:51:49 2014
Return-Path: <dev-return-10658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 239971049B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 17:51:49 +0000 (UTC)
Received: (qmail 79977 invoked by uid 500); 4 Dec 2014 17:51:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79905 invoked by uid 500); 4 Dec 2014 17:51:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79892 invoked by uid 99); 4 Dec 2014 17:51:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 17:51:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of imranrashi@gmail.com designates 209.85.216.173 as permitted sender)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 17:51:18 +0000
Received: by mail-qc0-f173.google.com with SMTP id i17so13146537qcy.32
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 09:49:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=LYEU1xkRusNF7UzDByMAApMVOYHkBfgAtVo2yEwfMVA=;
        b=qq8RY/3CLjv2Zn4GhaIuP6SGaEgtDwD7pyGZ/+QDkaHlNMdn3/AViJ3/Z78/UobNSI
         WecAv9e78QDPf7SzE7Lcz62LdZT9ZLs0Tp7CSJc8nXX/Onivw/XcmP0gG2l1R5WEAelL
         izA9UHPKVKvFNHgT0082KZ81ZuCOoAwrrJRSh8ThN2sJlXWKcPom7Tg6KJ/4rZWWtWh9
         siA3FGOeIQ85abHgWArpP0n72KG+OhX4/FxOeGqbBh7F203Cb1EIrpeuDGq0X1/xxvw1
         sEDsBNhgACcp1tdC+ds/lSdeiyIccBKaUy4QsckI++y4/84gfiNSZS19Oulzv/gmtENW
         Wv5g==
MIME-Version: 1.0
X-Received: by 10.224.3.137 with SMTP id 9mr19054056qan.64.1417715386675; Thu,
 04 Dec 2014 09:49:46 -0800 (PST)
Sender: imranrashi@gmail.com
Received: by 10.96.195.71 with HTTP; Thu, 4 Dec 2014 09:49:46 -0800 (PST)
Received: by 10.96.195.71 with HTTP; Thu, 4 Dec 2014 09:49:46 -0800 (PST)
In-Reply-To: <C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
References: <CANeJXFPNU5kweJ2ftsG_bZbqqJ-v=TG5FkdvuENiFdWUJAYabQ@mail.gmail.com>
	<C09AD697-30E7-471E-8F14-AF5814EA216D@gmail.com>
Date: Thu, 4 Dec 2014 09:49:46 -0800
X-Google-Sender-Auth: QWOzgWnNO8ZcjHIvZZyBXP-XslQ
Message-ID: <CAN_Cyt_wRKgZfTACLX0NBsB3_OvWf41=oJ=U6-js4jFdgnG9XA@mail.gmail.com>
Subject: Re: Spurious test failures, testing best practices
From: Imran Rashid <imran@therashids.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Ryan Williams <ryan.blake.williams@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2480a03f82e0509679659
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2480a03f82e0509679659
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I agree we should separate out the integration tests so it's easy for dev
to just run the other fast tests locally.  I opened a jira for it

https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4746
On Nov 30, 2014 3:08 PM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:

> Hi Ryan,
>
> As a tip (and maybe this isn't documented well), I normally use SBT for
> development to avoid the slow build process, and use its interactive
> console to run only specific tests. The nice advantage is that SBT can ke=
ep
> the Scala compiler loaded and JITed across builds, making it faster to
> iterate. To use it, you can do the following:
>
> - Start the SBT interactive console with sbt/sbt
> - Build your assembly by running the "assembly" target in the assembly
> project: assembly/assembly
> - Run all the tests in one module: core/test
> - Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite (thi=
s
> also supports tab completion)
>
> Running all the tests does take a while, and I usually just rely on
> Jenkins for that once I've run the tests for the things I believed my pat=
ch
> could break. But this is because some of them are integration tests (e.g.
> DistributedSuite, which creates multi-process mini-clusters). Many of the
> individual suites run fast without requiring this, however, so you can pi=
ck
> the ones you want. Perhaps we should find a way to tag them so people  ca=
n
> do a "quick-test" that skips the integration ones.
>
> The assembly builds are annoying but they only take about a minute for me
> on a MacBook Pro with SBT warmed up. The assembly is actually only requir=
ed
> for some of the "integration" tests (which launch new processes), but I'd
> recommend doing it all the time anyway since it would be very confusing t=
o
> run those with an old assembly. The Scala compiler crash issue can also b=
e
> a problem, but I don't see it very often with SBT. If it happens, I exit
> SBT and do sbt clean.
>
> Anyway, this is useful feedback and I think we should try to improve some
> of these suites, but hopefully you can also try the faster SBT process. A=
t
> the end of the day, if we want integration tests, the whole test process
> will take an hour, but most of the developers I know leave that to Jenkin=
s
> and only run individual tests locally before submitting a patch.
>
> Matei
>
>
> > On Nov 30, 2014, at 2:39 PM, Ryan Williams <
> ryan.blake.williams@gmail.com> wrote:
> >
> > In the course of trying to make contributions to Spark, I have had a lo=
t
> of
> > trouble running Spark's tests successfully. The main pain points I've
> > experienced are:
> >
> >    1) frequent, spurious test failures
> >    2) high latency of running tests
> >    3) difficulty running specific tests in an iterative fashion
> >
> > Here is an example series of failures that I encountered this weekend
> > (along with footnote links to the console output from each and
> > approximately how long each took):
> >
> > - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
> > before.
> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [2]: same failure.
> > - `mvn '-Dsuites=3D*BroadcastSuite* Unpersisting' test` [3]: BroadcastS=
uite
> > passed, but scala compiler crashed on the "catalyst" project.
> > - `mvn clean`: some attempts to run earlier commands (that previously
> > didn't crash the compiler) all result in the same compiler crash.
> Previous
> > discussion on this list implies this can only be solved by a `mvn clean=
`
> > [4].
> > - `mvn '-Dsuites=3D*BroadcastSuite*' test` [5]: immediately post-clean,
> > BroadcastSuite can't run because assembly is not built.
> > - `./dev/run-tests` again [6]: pyspark tests fail, some messages about
> > version mismatches and python 2.6. The machine this ran on has python
> 2.7,
> > so I don't know what that's about.
> > - `./dev/run-tests` again [7]: "too many open files" errors in several
> > tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this =
is
> > not enough, but only some of the time? I increased it to 8192 and tried
> > again.
> > - `./dev/run-tests` again [8]: same pyspark errors as before. This seem=
s
> to
> > be the issue from SPARK-3867 [9], which was supposedly fixed on October
> 14;
> > not sure how I'm seeing it now. In any case, switched to Python 2.6 and
> > installed unittest2, and python/run-tests seems to be unblocked.
> > - `./dev/run-tests` again [10]: finally passes!
> >
> > This was on a spark checkout at ceb6281 (ToT Friday), with a few trivia=
l
> > changes added on (that I wanted to test before sending out a PR), on a
> > macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
> >
> > Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar
> commands
> > from the same repo state:
> >
> > - `./dev/run-tests` [12]: YarnClusterSuite failure.
> > - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've se=
en
> > this one before on this machine and am guessing it actually occurs ever=
y
> > time.
> > - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one mo=
re
> > time from ceb6281, and saw the same failure.
> >
> > This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to
> narrow
> > down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my
> mac,
> > from ceb6281, with java 1.7 (instead of 1.8, which the previous runs
> used),
> > and it passed [16], so the failure seems specific to my linux
> machine/arch.
> >
> > At this point I believe that my changes don't break any tests (the
> > YarnClusterSuite failure on my linux presumably not being... "real"),
> and I
> > am ready to send out a PR. Whew!
> >
> > However, reflecting on the 5 or 6 distinct failure-modes represented
> above:
> >
> > - One of them (too many files open), is something I can (and did,
> > hopefully) fix once and for all. It cost me an ~hour this time
> (approximate
> > time of running ./dev/run-tests) and a few hours other times when I
> didn't
> > fully understand/fix it. It doesn't happen deterministically (why?), bu=
t
> > does happen somewhat frequently to people, having been discussed on the
> > user list multiple times [17] and on SO [18]. Maybe some note in the
> > documentation advising people to check their ulimit makes sense?
> > - One of them (unittest2 must be installed for python 2.6) was supposed=
ly
> > fixed upstream of the commits I tested here; I don't know why I'm still
> > running into it. This cost me a few hours of running `./dev/run-tests`
> > multiple times to see if it was transient, plus some time researching a=
nd
> > working around it.
> > - The original BroadcastSuite failure cost me a few hours and went away
> > before I'd even run `mvn clean`.
> > - A new incarnation of the sbt-compiler-crash phenomenon cost me a few
> > hours of running `./dev/run-tests` in different ways before deciding
> that,
> > as usual, there was no way around it and that I'd need to run `mvn clea=
n`
> > and start running tests from scratch.
> > - The YarnClusterSuite failures on my linux box have cost me hours of
> > trying to figure out whether they're my fault. I've seen them many time=
s
> > over the past weeks/months, plus or minus other failures that have come
> and
> > gone, and was especially befuddled by them when I was seeing a disjoint
> set
> > of reproducible failures on my mac [19] (the triaging of which involved
> > dozens of runs of `./dev/run-tests`).
> >
> > While I'm interested in digging into each of these issues, I also want =
to
> > discuss the frequency with which I've run into issues like these. This =
is
> > unfortunately not the first time in recent months that I've spent days
> > playing spurious-test-failure whack-a-mole with a 60-90min dev/run-test=
s
> > iteration time, which is no fun! So I am wondering/thinking:
> >
> > - Do other people experience this level of flakiness from spark tests?
> > - Do other people bother running dev/run-tests locally, or just let
> Jenkins
> > do it during the CR process?
> > - Needing to run a full assembly post-clean just to continue running on=
e
> > specific test case feels especially wasteful, and the failure output wh=
en
> > naively attempting to run a specific test without having built an
> assembly
> > jar is not always clear about what the issue is or how to fix it; even
> the
> > fact that certain tests require "building the world" is not something I
> > would have expected, and has cost me hours of confusion.
> >    - Should a person running spark tests assume that they must build an
> > assembly JAR before running anything?
> >    - Are there some proper "unit" tests that are actually self-containe=
d
> /
> > able to be run without building an assembly jar?
> >    - Can we better document/demarcate which tests have which
> dependencies?
> >    - Is there something finer-grained than building an assembly JAR tha=
t
> > is sufficient in some cases?
> >        - If so, can we document that?
> >        - If not, can we move to a world of finer-grained dependencies f=
or
> > some of these?
> > - Leaving all of these spurious failures aside, the process of assembli=
ng
> > and testing a new JAR is not a quick one (40 and 60 mins for me
> typically,
> > respectively). I would guess that there are dozens (hundreds?) of peopl=
e
> > who build a Spark assembly from various ToTs on any given day, and who
> all
> > wait on the exact same compilation / assembly steps to occur. Expanding
> on
> > the recent work to publish nightly snapshots [20], can we do a better j=
ob
> > caching/sharing compilation artifacts at a more granular level (pre-bui=
lt
> > assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA? mo=
re
> > granular maven modules, plus the previous two?), or otherwise save some
> of
> > the considerable amount of redundant compilation work that I had to do
> over
> > the course of my odyssey this weekend?
> >
> > Ramping up on most projects involves some amount of supplementing the
> > documentation with trial and error to figure out what to run, which
> > "errors" are real errors and which can be ignored, etc., but navigating
> > that minefield on Spark has proved especially challenging and
> > time-consuming for me. Some of that comes directly from scala's
> relatively
> > slow compilation times and immature build-tooling ecosystem, but that i=
s
> > the world we live in and it would be nice if Spark took the alleviation
> of
> > the resulting pain more seriously, as one of the more interesting and
> > well-known large scala projects around right now. The official
> > documentation around how to build different subsets of the codebase is
> > somewhat sparse [21], and there have been many mixed [22] accounts [23]
> on
> > this mailing list about preferred ways to build on mvn vs. sbt (none of
> > which has made it into official documentation, as far as I've seen).
> > Expecting new contributors to piece together all of this received
> > folk-wisdom about how to build/test in a sane way by trawling mailing
> list
> > archives seems suboptimal.
> >
> > Thanks for reading, looking forward to hearing your ideas!
> >
> > -Ryan
> >
> > P.S. Is "best practice" for emailing this list to not incorporate any
> HTML
> > in the body? It seems like all of the archives I've seen strip it out,
> but
> > other people have used it and gmail displays it.
> >
> >
> > [1]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
> > (57 mins)
> > [2]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> > (6 mins)
> > [3]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,=
%20fail%20subsequent%20compile
> > (4 mins)
> > [4]
> >
> https://www.google.com/url?sa=3Dt&rct=3Dj&q=3D&esrc=3Ds&source=3Dweb&cd=
=3D2&ved=3D0CCUQFjAB&url=3Dhttp%3A%2F%2Fapache-spark-user-list.1001560.n3.n=
abble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.h=
tml&ei=3DaRF6VJrpNKr-iAKDgYGYBQ&usg=3DAFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&si=
g2=3DzDeSqOgs02AXJXj78w5I9g&bvm=3Dbv.80642063,d.cGE&cad=3Drja
> > [5]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20nee=
d%20dependencies%20built
> > [6]
> >
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw=
/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
> > (50 mins)
> > [7]
> >
> https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#file-dev-run-t=
ests-failure-too-many-files-open-then-hang-L5260
> > (1hr)
> > [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
> > [9] https://issues.apache.org/jira/browse/SPARK-3867
> > [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> > [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> > [12]
> >
> https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#file-gistfile1=
-txt-L853
> > (~90 mins)
> > [13]
> >
> https://gist.github.com/ryan-williams/718f6324af358819b496#file-gistfile1=
-txt-L852
> > (91 mins)
> > [14]
> >
> https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#file-gistfile1=
-txt-L854
> > [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> > [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> > [17]
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/quot-Too-many-open-fi=
les-quot-exception-on-reduceByKey-td2462.html
> > [18]
> >
> http://stackoverflow.com/questions/25707629/why-does-spark-job-fail-with-=
too-many-open-files
> > [19] https://issues.apache.org/jira/browse/SPARK-4002
> > [20] https://issues.apache.org/jira/browse/SPARK-4542
> > [21]
> >
> https://spark.apache.org/docs/latest/building-with-maven.html#spark-tests=
-in-maven
> > [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
> > [23]
> >
> http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%3CCAOhmDz=
eUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c2480a03f82e0509679659--

From dev-return-10659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 18:27:10 2014
Return-Path: <dev-return-10659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CEEE10677
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 18:27:10 +0000 (UTC)
Received: (qmail 89467 invoked by uid 500); 4 Dec 2014 18:27:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89400 invoked by uid 500); 4 Dec 2014 18:27:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88485 invoked by uid 99); 4 Dec 2014 18:27:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 18:27:07 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 18:27:03 +0000
Received: by mail-ob0-f179.google.com with SMTP id va2so3260700obc.10
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 10:26:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4BeXkydIEDzmntktINbK4Qr0z99ha0Yb6YuFqz9vKTg=;
        b=ZGHhx19+1UHOA31mCL8zHzxtm8tIxQlTy3Q4y5COraCiExXLPjx0zlmYVDXYEuJ+1d
         ZOfZgoZA1Aux4OzCCf5WewlkZ7m9ZrDJeYFlIg9LJmUM78JQkoZWI3B+0RDz+tNqp4Jr
         oNMsfnKoMQwYkZf9UqA9l41ahclHs/Rn4wYMxsdZwOSCksQ8X54/i8Q2n6NkjiuuGGu5
         27E+9wZYf85DJv8rYKzZoh44CQWWrucxGX+9NGl0/uGbm5ln5yMUBj4L2PsaVFAlnFQE
         LpiMbf5BFmim+pvpDTIvQouKwq91zzgFnTPRYyMUpY/siClhCAUd5Lr3XixVYK4RQ3cs
         J8bg==
MIME-Version: 1.0
X-Received: by 10.182.79.41 with SMTP id g9mr7665489obx.14.1417717602829; Thu,
 04 Dec 2014 10:26:42 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Thu, 4 Dec 2014 10:26:42 -0800 (PST)
In-Reply-To: <OF4E0755FF.D4C7B247-ON48257DA4.001B9269-48257DA4.001BB47A@cn.ibm.com>
References: <OF4E0755FF.D4C7B247-ON48257DA4.001B9269-48257DA4.001BB47A@cn.ibm.com>
Date: Thu, 4 Dec 2014 10:26:42 -0800
Message-ID: <CABPQxsvbo6OJpxx6dMsinrOG0746TcHkC1W_zL+8etr=1HHFrQ@mail.gmail.com>
Subject: Re: Ooyala Spark JobServer
From: Patrick Wendell <pwendell@gmail.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2e468a1bcf300509681a40
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e468a1bcf300509681a40
Content-Type: text/plain; charset=ISO-8859-1

Hey Jun,

The Ooyala server is being maintained by it's original author (Evan Chan)
here:

https://github.com/spark-jobserver/spark-jobserver

This is likely to stay as a standalone project for now, since it builds
directly on Spark's public API's.

- Patrick

On Wed, Dec 3, 2014 at 9:02 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Hi, I am wondering the status of the Ooyala Spark Jobserver, any plan to
> get it into the spark release?
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>

--047d7b2e468a1bcf300509681a40--

From dev-return-10660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec  4 23:25:32 2014
Return-Path: <dev-return-10660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF2191054F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  4 Dec 2014 23:25:31 +0000 (UTC)
Received: (qmail 51518 invoked by uid 500); 4 Dec 2014 23:25:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51450 invoked by uid 500); 4 Dec 2014 23:25:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51437 invoked by uid 99); 4 Dec 2014 23:25:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 23:25:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 04 Dec 2014 23:25:04 +0000
Received: by mail-ie0-f175.google.com with SMTP id x19so11873935ier.34
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 15:22:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=FBKf20M77ALdQkJfQdkC21yGL0BkJK0LFEHO4680Ov8=;
        b=EH3IQ4xPEgB6QnVSHbcxinLwU+/6nf4ngWHxTsZ5wM7jr58sWSqWr/ioD0kyxRhd3M
         klbJT1Bcppj+v7ruwG5wYSBkQUZn9ROxp82idWbtIuJykRWyWYwTcMxDJtnOQdlvMmGQ
         w4TZESj+nr6BIsI9Gltw3F0e91n8Gh+k64YW67/ZyenhWi+jOpBRK5mi9bKgOmNwMIUN
         8UwjrGKXzLTob9OpV8uY5X9u7ccmvam7l38QR5zsgC+Gj4y1+4GcXrZ/v/ogIorgBLim
         eEepjFzV8XsOwCFV6aZnXMHCgL6GwBhFhK8yws2kv4Rf8kW71Nx7XdgKhmOImuo69b6L
         D0TA==
X-Received: by 10.107.25.129 with SMTP id 123mr11917738ioz.90.1417735368666;
 Thu, 04 Dec 2014 15:22:48 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
 <CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 04 Dec 2014 23:22:48 +0000
Message-ID: <CAOhmDzddsPU6sXtFxwkxtvWj+H0va=3QQHspPqWUp-OvM4v74g@mail.gmail.com>
Subject: Re: zinc invocation examples
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fe4ca08f6b705096c3d8b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fe4ca08f6b705096c3d8b
Content-Type: text/plain; charset=UTF-8

Oh, derp. I just assumed from looking at all the options that there was
something to it. Thanks Sean.

On Thu Dec 04 2014 at 7:47:33 AM Sean Owen <sowen@cloudera.com> wrote:

> You just run it once with "zinc -start" and leave it running as a
> background process on your build machine. You don't have to do
> anything for each build.
>
> On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > https://github.com/apache/spark/blob/master/docs/
> building-spark.md#speeding-up-compilation-with-zinc
> >
> > Could someone summarize how they invoke zinc as part of a regular
> > build-test-etc. cycle?
> >
> > I'll add it in to the aforelinked page if appropriate.
> >
> > Nick
>

--001a113fe4ca08f6b705096c3d8b--

From dev-return-10661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 00:05:55 2014
Return-Path: <dev-return-10661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D72110759
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 00:05:55 +0000 (UTC)
Received: (qmail 45677 invoked by uid 500); 5 Dec 2014 00:05:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45605 invoked by uid 500); 5 Dec 2014 00:05:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45576 invoked by uid 99); 5 Dec 2014 00:05:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 00:05:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 00:05:27 +0000
Received: by mail-ie0-f169.google.com with SMTP id y20so16936356ier.14
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 16:05:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=Wn2U2r1Uk3Tl9jMBtXJRxPjISmUQ846hk66Zt4Xu8jA=;
        b=irWrPocDn8bCxmS0lXJsXgQjv3t2pSAGwVA1mXskCknrCzO0UQkZ5jCO7sY/rJNOr7
         zqoV1kbs+osrx51CT4cjjRZu1kTAI9aVyNQPTK14HAB5cKd1XkiWo7urXS2E9TabYY1f
         3Mq2OdjROPJ0alP+Dcw74ZVNOMdPDIPh/a84M9Qd1jWKQpuKT2q68svJ2XB1WHNzT7c8
         od8/6S0W1j69KUnHYoMg7wIIz1suKdKTntTe/z57DKTbYpvaN0HVeFJwCI1ko57mXpLQ
         HbbYTqICKYQBJhKzgQ74U391XaH5FJ9lbq6xShEBcBDEbQHjf0yZQZN1flszZPzLH2ow
         qS3g==
X-Received: by 10.50.29.3 with SMTP id f3mr810707igh.23.1417737926697; Thu, 04
 Dec 2014 16:05:26 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
 <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
 <CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com> <CAOhmDzdLANy8adD4y02isgh5UVDCr4tee_Wz1ecO=gTY4Mr+sw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 05 Dec 2014 00:05:26 +0000
Message-ID: <CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Reynold Xin <rxin@databricks.com>
Cc: Ted Yu <yuzhihong@gmail.com>, Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd74b5481685905096cd5b9
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74b5481685905096cd5b9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

fwiw, when we did this work in HBase, we categorized the tests. Then some
tests can share a single jvm, while some others need to be isolated in
their own jvm. Nevertheless surefire can still run them in parallel by
starting/stopping several jvm.

I think we need to do this as well. Perhaps the test naming hierarchy can
be used to group non-parallelizable tests in the same JVM.

For example, here are some Hive tests from our project:

org.apache.spark.sql.hive.StatisticsSuite
org.apache.spark.sql.hive.execution.HiveQuerySuite
org.apache.spark.sql.QueryTest
org.apache.spark.sql.parquet.HiveParquetSuite

If we group tests by the first 5 parts of their name (e.g.
org.apache.spark.sql.hive), then we=E2=80=99d have the first 2 tests run in=
 the
same JVM, and the next 2 tests each run in their own JVM.

I=E2=80=99m new to this stuff so I=E2=80=99m not sure if I=E2=80=99m going =
about this in the right
way, but you can see my attempt with this approach on GitHub
<https://github.com/nchammas/spark/blob/ab127b798dbfa9399833d546e627f9651b0=
60918/project/SparkBuild.scala#L388-L397>,
as well as the related discussion on JIRA
<https://issues.apache.org/jira/browse/SPARK-3431>.

If anyone has more feedback on this, I=E2=80=99d love to hear it (either on=
 this
thread or in the JIRA issue).

Nick
=E2=80=8B

On Sun Sep 07 2014 at 8:28:51 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On Fri, Aug 8, 2014 at 1:12 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Nick,
>>
>> Would you like to file a ticket to track this?
>>
>
> SPARK-3431 <https://issues.apache.org/jira/browse/SPARK-3431>:
> Parallelize execution of tests
> > Sub-task: SPARK-3432 <https://issues.apache.org/jira/browse/SPARK-3432>=
:
> Fix logging of unit test execution time
>
> Nick
>

--047d7bd74b5481685905096cd5b9--

From dev-return-10662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 00:33:49 2014
Return-Path: <dev-return-10662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39B5010890
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 00:33:49 +0000 (UTC)
Received: (qmail 8800 invoked by uid 500); 5 Dec 2014 00:33:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8723 invoked by uid 500); 5 Dec 2014 00:33:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8512 invoked by uid 99); 5 Dec 2014 00:33:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 00:33:47 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.169 as permitted sender)
Received: from [209.85.160.169] (HELO mail-yk0-f169.google.com) (209.85.160.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 00:33:43 +0000
Received: by mail-yk0-f169.google.com with SMTP id 79so8544410ykr.28
        for <dev@spark.apache.org>; Thu, 04 Dec 2014 16:33:22 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tGAk5If15/7LPd3KHDUb5UJa11BEGQs5kZY/sRCSW04=;
        b=m18Q+Wvz3DjDJc7wTmqVaYzZWkYINXplarvVakvx49raXI0tS2fVL+5VfLw/22EcCe
         rDi8Jqmp+wPjO1mFBQxPOd4ekbAImoPg6q4udlpQZKuzeMMgOmYuxmCrh/NPj38c2lH2
         204wt/w6AnBJRt/XmFe3aFHZ0m0es+PPqKNUW52ZCOCMP2KU9JDv0HzOcEtkV01K/trb
         ylvtV7WdCIBe+h51+ZSM9SZxSpYpBsEzId3PcsaUpV90fWOIO8sqHBKUktgGC+kvxkIj
         rD3C0NytDxgwykjpRVVuY3M5UzW2b6f5CsVqssExqPVIp5gEVst7BBpxh9m9qEwMdNNy
         nEGw==
MIME-Version: 1.0
X-Received: by 10.236.223.105 with SMTP id u99mr16732700yhp.150.1417739602808;
 Thu, 04 Dec 2014 16:33:22 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Thu, 4 Dec 2014 16:33:22 -0800 (PST)
In-Reply-To: <CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
	<CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
	<CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
	<CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com>
	<CAOhmDzdLANy8adD4y02isgh5UVDCr4tee_Wz1ecO=gTY4Mr+sw@mail.gmail.com>
	<CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
Date: Thu, 4 Dec 2014 16:33:22 -0800
Message-ID: <CALte62w1RbC4zKDoWnz6esrjk=ZpsjY6dPcVmOcTxMW3Jb6VxA@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Ted Yu <yuzhihong@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160bfaa68d4a905096d3921
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bfaa68d4a905096d3921
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Have you seen this thread http://search-hadoop.com/m/JW1q5xxSAa2 ?

Test categorization in HBase is done through maven-surefire-plugin

Cheers

On Thu, Dec 4, 2014 at 4:05 PM, Nicholas Chammas <nicholas.chammas@gmail.co=
m
> wrote:

> fwiw, when we did this work in HBase, we categorized the tests. Then some
> tests can share a single jvm, while some others need to be isolated in
> their own jvm. Nevertheless surefire can still run them in parallel by
> starting/stopping several jvm.
>
> I think we need to do this as well. Perhaps the test naming hierarchy can
> be used to group non-parallelizable tests in the same JVM.
>
> For example, here are some Hive tests from our project:
>
> org.apache.spark.sql.hive.StatisticsSuite
> org.apache.spark.sql.hive.execution.HiveQuerySuite
> org.apache.spark.sql.QueryTest
> org.apache.spark.sql.parquet.HiveParquetSuite
>
> If we group tests by the first 5 parts of their name (e.g.
> org.apache.spark.sql.hive), then we=E2=80=99d have the first 2 tests run =
in the
> same JVM, and the next 2 tests each run in their own JVM.
>
> I=E2=80=99m new to this stuff so I=E2=80=99m not sure if I=E2=80=99m goin=
g about this in the right
> way, but you can see my attempt with this approach on GitHub
> <https://github.com/nchammas/spark/blob/ab127b798dbfa9399833d546e627f9651=
b060918/project/SparkBuild.scala#L388-L397>,
> as well as the related discussion on JIRA
> <https://issues.apache.org/jira/browse/SPARK-3431>.
>
> If anyone has more feedback on this, I=E2=80=99d love to hear it (either =
on this
> thread or in the JIRA issue).
>
> Nick
> =E2=80=8B
>
> On Sun Sep 07 2014 at 8:28:51 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> On Fri, Aug 8, 2014 at 1:12 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> Nick,
>>>
>>> Would you like to file a ticket to track this?
>>>
>>
>> SPARK-3431 <https://issues.apache.org/jira/browse/SPARK-3431>:
>> Parallelize execution of tests
>> > Sub-task: SPARK-3432 <https://issues.apache.org/jira/browse/SPARK-3432=
>:
>> Fix logging of unit test execution time
>>
>> Nick
>>
>

--089e0160bfaa68d4a905096d3921--

From dev-return-10663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 03:38:40 2014
Return-Path: <dev-return-10663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C85310FA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 03:38:40 +0000 (UTC)
Received: (qmail 12808 invoked by uid 500); 5 Dec 2014 03:38:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12509 invoked by uid 500); 5 Dec 2014 03:38:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11862 invoked by uid 99); 5 Dec 2014 03:38:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 03:38:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.181 as permitted sender)
Received: from [209.85.217.181] (HELO mail-lb0-f181.google.com) (209.85.217.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 03:38:11 +0000
Received: by mail-lb0-f181.google.com with SMTP id l4so5804181lbv.12
        for <multiple recipients>; Thu, 04 Dec 2014 19:38:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=qZ3twQaj3pTB298nhqbgIOs97hYVxa1Y5YeO5KtC8+k=;
        b=mO0RB8t8FwqqIeGORDQ0CXdkA3ZRCKgsc15Cel4MRWqIVLF8zCPTzlY1pmr/NA7qVq
         gDulBj5pL7TDjl7jS0vXaXS5eH304GxVijSMwiXnBVmb9+476Bio6haUxchbniA+qXHX
         DBMop9pyfVrzJFu8E5HRyI1vyLT79FaNaKa1dW1o483F3V/BrVPsfPdi0pI5Dw4gMwEY
         aBIV08zHXFLGXxq1kjJPc/eETHzFnPLzdcQeKFug+uafIQHxLpEFyhZDzbF9HN2A/l1p
         U3wlsBbX21pIvGa7n0B1gU+wZdSRHipwfHOdnIqTlITLxmMZVs6515DTk8tJiOeYAn4h
         87yw==
X-Received: by 10.112.172.97 with SMTP id bb1mr774513lbc.38.1417750690347;
 Thu, 04 Dec 2014 19:38:10 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 19:37:50 -0800 (PST)
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 11:37:50 +0800
Message-ID: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
Subject: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33e9047411305096fce57
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33e9047411305096fce57
Content-Type: text/plain; charset=UTF-8

I got the following error during Spark startup (Yarn-client mode):

14/12/04 19:33:58 INFO Client: Uploading resource
file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
->
hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
java.lang.IllegalArgumentException: Wrong FS:
hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
        at
org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
        at
org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
        at
org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
        at
org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
        at
org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
        at
org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
        at
org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
        at
org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
        at
org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
        at
org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
        at
org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
        at
org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
        at
org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
        at
org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
        at
org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
        at $iwC$$iwC.<init>(<console>:9)
        at $iwC.<init>(<console>:18)
        at <init>(<console>:20)
        at .<init>(<console>:24)

I'm using latest Spark built from master HEAD yesterday. Is this a bug?

-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c33e9047411305096fce57--

From dev-return-10664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 03:56:31 2014
Return-Path: <dev-return-10664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CCE310055
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 03:56:31 +0000 (UTC)
Received: (qmail 45154 invoked by uid 500); 5 Dec 2014 03:56:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44946 invoked by uid 500); 5 Dec 2014 03:56:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43626 invoked by uid 99); 5 Dec 2014 03:56:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 03:56:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 03:55:56 +0000
Received: by mail-lb0-f172.google.com with SMTP id u10so15268970lbd.17
        for <multiple recipients>; Thu, 04 Dec 2014 19:55:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=bTeBnsJ4fHa3rX3EEqwb1kes/D3fJUiI82njNDY69bQ=;
        b=OrcE1AYJoovSKy7mq+Rb0Ih48oUmFlQWNALfN3KUmYsW30ObhJhvNYJx/JFpK1L1gu
         zMaQ0hlg/q9BdwiyHsaWqzJI4Frll5RDveCR1ciCLqD+ZzYSztnCP0+M3xzA7Uzkiltx
         OU0uqfqfMNAQ+ZyRduKf6mjV+p0CwkRZb+uZX3uhQFCPYV8XSii2DmbgG1jDftXCSlOd
         fUVEgD124jnkp7C+j7nqBTgSf2yZA41oZr3li1r0Xm60Ibg2kaqZn7BgIjM8NGmGdQux
         NchqEWR5dyYL+l4HAEn0tjilHQpeON1KFbleJ4Ayc/s9+Jfr37D+9j6JCJttLgeTERUY
         Nheg==
X-Received: by 10.152.87.42 with SMTP id u10mr859026laz.0.1417751710581; Thu,
 04 Dec 2014 19:55:10 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 19:54:50 -0800 (PST)
In-Reply-To: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 11:54:50 +0800
Message-ID: <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3443816cecd0509700b3d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3443816cecd0509700b3d
Content-Type: text/plain; charset=UTF-8

Looks like somehow Spark failed to find the core-site.xml in /et/hadoop/conf

I've already set the following env variables:

export YARN_CONF_DIR=/etc/hadoop/conf
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HBASE_CONF_DIR=/etc/hbase/conf

Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?

Jianshi

On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> I got the following error during Spark startup (Yarn-client mode):
>
> 14/12/04 19:33:58 INFO Client: Uploading resource
> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
> ->
> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
> java.lang.IllegalArgumentException: Wrong FS:
> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
> expected: file:///
>         at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>         at
> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>         at
> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>         at
> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>         at
> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>         at
> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>         at
> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>         at
> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>         at
> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>         at scala.Option.foreach(Option.scala:236)
>         at
> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>         at
> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>         at
> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>         at
> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>         at
> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>         at
> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>         at
> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>         at
> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>         at $iwC$$iwC.<init>(<console>:9)
>         at $iwC.<init>(<console>:18)
>         at <init>(<console>:20)
>         at .<init>(<console>:24)
>
> I'm using latest Spark built from master HEAD yesterday. Is this a bug?
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c3443816cecd0509700b3d--

From dev-return-10665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 04:03:47 2014
Return-Path: <dev-return-10665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 85B6C100D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 04:03:47 +0000 (UTC)
Received: (qmail 65000 invoked by uid 500); 5 Dec 2014 04:03:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64884 invoked by uid 500); 5 Dec 2014 04:03:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63399 invoked by uid 99); 5 Dec 2014 04:03:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 04:03:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.215.41 as permitted sender)
Received: from [209.85.215.41] (HELO mail-la0-f41.google.com) (209.85.215.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 04:03:18 +0000
Received: by mail-la0-f41.google.com with SMTP id hv19so24574lab.14
        for <multiple recipients>; Thu, 04 Dec 2014 20:02:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=qW371P/l+mw8xw3XqBfQm6wuf8+qw5XMX0M3gdQIlQw=;
        b=B02lbSa3xtopUBt6ur1mbEuq0Ual5IJ4qAbWifZFq9mIFQ6CNo1A0SCxL//WEXfoaW
         8lBJ2+h8Y1o8OG1Wjt2ZbMOYy7mOy+Wd7F2MhdarxViAn6es+flcoO/XIX+AlDhufBuz
         ly4NDEGa1n0E5tS4LlOZdubJnWfH54RoehHgHUCjQKqwz5xjRapGku2gcDyetdEOmIw4
         Z4yxqyO0PbtJGv09gIAi3PWXGboJs1QTOYrIonRGmOjYnJUtL5biP8BlFsehMd6UCUFk
         erw6oBrGa6Fv7p8ybLKtq7ZXWuzvV3v7aNy1iEbDZsfrFnZj/VwOcTVlkBqF4P6JyHz0
         2IrA==
X-Received: by 10.112.189.10 with SMTP id ge10mr773754lbc.23.1417752152212;
 Thu, 04 Dec 2014 20:02:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 20:02:11 -0800 (PST)
In-Reply-To: <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
 <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 12:02:11 +0800
Message-ID: <CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37016698ae705097025e6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37016698ae705097025e6
Content-Type: text/plain; charset=UTF-8

Actually my HADOOP_CLASSPATH has already been set to include
/etc/hadoop/conf/*

export
HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
classpath)

Jianshi

On Fri, Dec 5, 2014 at 11:54 AM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Looks like somehow Spark failed to find the core-site.xml in
> /et/hadoop/conf
>
> I've already set the following env variables:
>
> export YARN_CONF_DIR=/etc/hadoop/conf
> export HADOOP_CONF_DIR=/etc/hadoop/conf
> export HBASE_CONF_DIR=/etc/hbase/conf
>
> Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?
>
> Jianshi
>
> On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> I got the following error during Spark startup (Yarn-client mode):
>>
>> 14/12/04 19:33:58 INFO Client: Uploading resource
>> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
>> ->
>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
>> java.lang.IllegalArgumentException: Wrong FS:
>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
>> expected: file:///
>>         at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>>         at
>> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>>         at
>> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>>         at
>> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>>         at
>> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>>         at
>> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>>         at
>> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>>         at
>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>>         at
>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>>         at scala.Option.foreach(Option.scala:236)
>>         at
>> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>>         at
>> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>>         at
>> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>>         at
>> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>>         at
>> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>>         at
>> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>>         at
>> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>>         at
>> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>>         at $iwC$$iwC.<init>(<console>:9)
>>         at $iwC.<init>(<console>:18)
>>         at <init>(<console>:20)
>>         at .<init>(<console>:24)
>>
>> I'm using latest Spark built from master HEAD yesterday. Is this a bug?
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c37016698ae705097025e6--

From dev-return-10666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 04:49:02 2014
Return-Path: <dev-return-10666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 858AF10201
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 04:49:02 +0000 (UTC)
Received: (qmail 57845 invoked by uid 500); 5 Dec 2014 04:48:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57767 invoked by uid 500); 5 Dec 2014 04:48:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56466 invoked by uid 99); 5 Dec 2014 04:48:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 04:48:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.176 as permitted sender)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 04:48:27 +0000
Received: by mail-lb0-f176.google.com with SMTP id p9so14988212lbv.21
        for <multiple recipients>; Thu, 04 Dec 2014 20:46:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ACdQdnuYecZWSu41XK4vzWbPay7I30Mx+2yeMQ7fprk=;
        b=uwy2++SY1an4yw/xuIiCot+q8F1LDwjLmp5UIGN79QLhFWZkFRq5g9D3tCPnGEexW2
         h03yTjCfJWFbHFT400Ea3SJlw+3PoqhEgWLwCjhbGNjuswVauPBVChhiVT/tXtLoy5NX
         ng6Sn/xJCiNFhowRWC6IEh9FRNkhOEav2MoNUyl8tkxLCCd+iNfTY2S/7F+ZIUFTXTUr
         gTr+OxttxCRrNZiUtHxIrEqLa1+bT+5RWVFJCD7yuQ0xafWWaVN9y/fKQvLHSrUBj+cL
         R3cOipK5Ni3xr3F3/B6VXeySb1B/7Sw2y+rRXSzmDFFBHB0t/GR+cZfSHmV7uddx7C7W
         z3zA==
X-Received: by 10.153.5.33 with SMTP id cj1mr908836lad.65.1417754771379; Thu,
 04 Dec 2014 20:46:11 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 20:45:51 -0800 (PST)
In-Reply-To: <CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
 <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com> <CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 12:45:51 +0800
Message-ID: <CACA1tWLZcoHmhJHe-RmDkA8RY4Qe6XbUWMFho_+_puG88AmhDQ@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134962886df1d050970c104
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134962886df1d050970c104
Content-Type: text/plain; charset=UTF-8

Looks like the datanucleus*.jar shouldn't appear in the hdfs path in
Yarn-client mode.

Maybe this patch broke yarn-client.

https://github.com/apache/spark/commit/a975dc32799bb8a14f9e1c76defaaa7cfbaf8b53

Jianshi

On Fri, Dec 5, 2014 at 12:02 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Actually my HADOOP_CLASSPATH has already been set to include
> /etc/hadoop/conf/*
>
> export
> HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
> classpath)
>
> Jianshi
>
> On Fri, Dec 5, 2014 at 11:54 AM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Looks like somehow Spark failed to find the core-site.xml in
>> /et/hadoop/conf
>>
>> I've already set the following env variables:
>>
>> export YARN_CONF_DIR=/etc/hadoop/conf
>> export HADOOP_CONF_DIR=/etc/hadoop/conf
>> export HBASE_CONF_DIR=/etc/hbase/conf
>>
>> Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?
>>
>> Jianshi
>>
>> On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> I got the following error during Spark startup (Yarn-client mode):
>>>
>>> 14/12/04 19:33:58 INFO Client: Uploading resource
>>> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
>>> ->
>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
>>> java.lang.IllegalArgumentException: Wrong FS:
>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
>>> expected: file:///
>>>         at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>>>         at
>>> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>>>         at
>>> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>>>         at
>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>>>         at
>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>>>         at
>>> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>>>         at
>>> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>>>         at
>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>>>         at
>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>>>         at scala.Option.foreach(Option.scala:236)
>>>         at
>>> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>>>         at
>>> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>>>         at
>>> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>>>         at
>>> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>>>         at
>>> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>>>         at
>>> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>>>         at
>>> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>>>         at
>>> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>>>         at $iwC$$iwC.<init>(<console>:9)
>>>         at $iwC.<init>(<console>:18)
>>>         at <init>(<console>:20)
>>>         at .<init>(<console>:24)
>>>
>>> I'm using latest Spark built from master HEAD yesterday. Is this a bug?
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a1134962886df1d050970c104--

From dev-return-10667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 05:32:40 2014
Return-Path: <dev-return-10667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7FA010317
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 05:32:40 +0000 (UTC)
Received: (qmail 25260 invoked by uid 500); 5 Dec 2014 05:32:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25155 invoked by uid 500); 5 Dec 2014 05:32:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23425 invoked by uid 99); 5 Dec 2014 05:32:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 05:32:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jianshi.huang@gmail.com designates 209.85.215.41 as permitted sender)
Received: from [209.85.215.41] (HELO mail-la0-f41.google.com) (209.85.215.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 05:32:34 +0000
Received: by mail-la0-f41.google.com with SMTP id hv19so91499lab.28
        for <multiple recipients>; Thu, 04 Dec 2014 21:32:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=oNQr5sePdKmEbDM1DSm5xKfPdl1zxp/V8q0rUIqKwDw=;
        b=TlplKfnQNg9LaFNEi/Rxkpc3X+U39xh9B+uCSaSHbjDxOIN75rnGHcF4uJMB4HbE7v
         +eQlsTC5yRTIHUTmI+JPHg+yluGA5obLnnvxzBKKjNmbmDm+lFoDs/D01UowKj2Ev5aL
         uBDvEklrqhtPbnYGBHjMxYovQlXSopU6RkkzD5IhV+j1kUwdd+5B/E2wZJIleEZJrSeb
         6LoO54nQRMDR6+C0t3fNndbBv6McyoOKUcYozB9Hni2++uODMKGZoNFsg0puOKrH1MCN
         ghE7kctSwtOK/JXA+QRigNs3rTdXurC9DhGJCtrPyFkOK56WNLVbnOhvJaBHHVFDHSkc
         yZ+A==
X-Received: by 10.152.121.1 with SMTP id lg1mr1097506lab.28.1417757533212;
 Thu, 04 Dec 2014 21:32:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 21:31:52 -0800 (PST)
In-Reply-To: <CACA1tWLZcoHmhJHe-RmDkA8RY4Qe6XbUWMFho_+_puG88AmhDQ@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
 <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
 <CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com> <CACA1tWLZcoHmhJHe-RmDkA8RY4Qe6XbUWMFho_+_puG88AmhDQ@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 13:31:52 +0800
Message-ID: <CACA1tWJ5KynUe5Ndkd6rmQKJfpiZ2FQdVLzvVrwsRt7_hOCNiw@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115ef102515870509716630
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115ef102515870509716630
Content-Type: text/plain; charset=UTF-8

Correction:

According to Liancheng, this hotfix might be the root cause:


https://github.com/apache/spark/commit/38cb2c3a36a5c9ead4494cbc3dde008c2f0698ce

Jianshi

On Fri, Dec 5, 2014 at 12:45 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Looks like the datanucleus*.jar shouldn't appear in the hdfs path in
> Yarn-client mode.
>
> Maybe this patch broke yarn-client.
>
>
> https://github.com/apache/spark/commit/a975dc32799bb8a14f9e1c76defaaa7cfbaf8b53
>
> Jianshi
>
> On Fri, Dec 5, 2014 at 12:02 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Actually my HADOOP_CLASSPATH has already been set to include
>> /etc/hadoop/conf/*
>>
>> export
>> HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
>> classpath)
>>
>> Jianshi
>>
>> On Fri, Dec 5, 2014 at 11:54 AM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Looks like somehow Spark failed to find the core-site.xml in
>>> /et/hadoop/conf
>>>
>>> I've already set the following env variables:
>>>
>>> export YARN_CONF_DIR=/etc/hadoop/conf
>>> export HADOOP_CONF_DIR=/etc/hadoop/conf
>>> export HBASE_CONF_DIR=/etc/hbase/conf
>>>
>>> Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?
>>>
>>> Jianshi
>>>
>>> On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> I got the following error during Spark startup (Yarn-client mode):
>>>>
>>>> 14/12/04 19:33:58 INFO Client: Uploading resource
>>>> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
>>>> ->
>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
>>>> java.lang.IllegalArgumentException: Wrong FS:
>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
>>>> expected: file:///
>>>>         at
>>>> org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>>>>         at
>>>> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>>>>         at
>>>> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>>>>         at
>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>>>>         at
>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>>>>         at
>>>> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>>>>         at
>>>> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>>>>         at
>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>>>>         at
>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>>>>         at scala.Option.foreach(Option.scala:236)
>>>>         at
>>>> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>>>>         at
>>>> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>>>>         at
>>>> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>>>>         at
>>>> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>>>>         at
>>>> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>>>>         at
>>>> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>>>>         at
>>>> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>>>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>>>>         at
>>>> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>>>>         at $iwC$$iwC.<init>(<console>:9)
>>>>         at $iwC.<init>(<console>:18)
>>>>         at <init>(<console>:20)
>>>>         at .<init>(<console>:24)
>>>>
>>>> I'm using latest Spark built from master HEAD yesterday. Is this a bug?
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--089e0115ef102515870509716630--

From dev-return-10668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 05:53:25 2014
Return-Path: <dev-return-10668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DCFD10428
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 05:53:25 +0000 (UTC)
Received: (qmail 77603 invoked by uid 500); 5 Dec 2014 05:53:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77503 invoked by uid 500); 5 Dec 2014 05:53:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75724 invoked by uid 99); 5 Dec 2014 05:53:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 05:53:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jianshi.huang@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 05:53:17 +0000
Received: by mail-vc0-f181.google.com with SMTP id le20so6126vcb.26
        for <multiple recipients>; Thu, 04 Dec 2014 21:52:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=wg0tLSJiAIN3E/fURh8qqjFPfNw65nMSA4zKuF3La/0=;
        b=wMzLaoKqSO0HPKNrDvA4D3fmSqu5U2Uh8gVvsS2i0re4x8CeglbWeNqZN9Rqxtd+pq
         YZ7egePC/Ar7No5ICYyAMBBDyhfMtaUDQbnajZ36JVK1Vp8+z1ImAmiRMOz21tNzYxDr
         eisFtozb08vF9gq9Mfh3WZk/XNLc+WR/dyTW79m/c0I0nbNeWeJmXAAXYmjn4w4ZHjtI
         lUwFxpKiWuVg4TcSsOWiKcOf4zhM5Nq5iOu93DvV1qbQJKuDcJIqCNkcrU6pnHXpSxqc
         rz4FC0kVgb/QuCnlQUGk+BjgCiJy0DARpa66GrJstVEfqZpn5AmmU2m9PQ/EcEMzPASg
         sdyw==
X-Received: by 10.52.231.165 with SMTP id th5mr6416256vdc.48.1417758731905;
 Thu, 04 Dec 2014 21:52:11 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.59.193 with HTTP; Thu, 4 Dec 2014 21:51:51 -0800 (PST)
In-Reply-To: <CACA1tWJ5KynUe5Ndkd6rmQKJfpiZ2FQdVLzvVrwsRt7_hOCNiw@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
 <CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
 <CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com>
 <CACA1tWLZcoHmhJHe-RmDkA8RY4Qe6XbUWMFho_+_puG88AmhDQ@mail.gmail.com> <CACA1tWJ5KynUe5Ndkd6rmQKJfpiZ2FQdVLzvVrwsRt7_hOCNiw@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 13:51:51 +0800
Message-ID: <CACA1tWLE6yNYnwxpd82m7d3D3W292YeyaK8Zt4kshb_r1fVsKg@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0111d6bc97e652050971ad65
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0111d6bc97e652050971ad65
Content-Type: text/plain; charset=UTF-8

I created a ticket for this:

  https://issues.apache.org/jira/browse/SPARK-4757


Jianshi

On Fri, Dec 5, 2014 at 1:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Correction:
>
> According to Liancheng, this hotfix might be the root cause:
>
>
> https://github.com/apache/spark/commit/38cb2c3a36a5c9ead4494cbc3dde008c2f0698ce
>
> Jianshi
>
> On Fri, Dec 5, 2014 at 12:45 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Looks like the datanucleus*.jar shouldn't appear in the hdfs path in
>> Yarn-client mode.
>>
>> Maybe this patch broke yarn-client.
>>
>>
>> https://github.com/apache/spark/commit/a975dc32799bb8a14f9e1c76defaaa7cfbaf8b53
>>
>> Jianshi
>>
>> On Fri, Dec 5, 2014 at 12:02 PM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Actually my HADOOP_CLASSPATH has already been set to include
>>> /etc/hadoop/conf/*
>>>
>>> export
>>> HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
>>> classpath)
>>>
>>> Jianshi
>>>
>>> On Fri, Dec 5, 2014 at 11:54 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Looks like somehow Spark failed to find the core-site.xml in
>>>> /et/hadoop/conf
>>>>
>>>> I've already set the following env variables:
>>>>
>>>> export YARN_CONF_DIR=/etc/hadoop/conf
>>>> export HADOOP_CONF_DIR=/etc/hadoop/conf
>>>> export HBASE_CONF_DIR=/etc/hbase/conf
>>>>
>>>> Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?
>>>>
>>>> Jianshi
>>>>
>>>> On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang <jianshi.huang@gmail.com
>>>> > wrote:
>>>>
>>>>> I got the following error during Spark startup (Yarn-client mode):
>>>>>
>>>>> 14/12/04 19:33:58 INFO Client: Uploading resource
>>>>> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
>>>>> ->
>>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
>>>>> java.lang.IllegalArgumentException: Wrong FS:
>>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
>>>>> expected: file:///
>>>>>         at
>>>>> org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>>>>>         at
>>>>> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>>>>>         at
>>>>> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>>>>>         at
>>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>>>>>         at
>>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>>>>>         at
>>>>> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>>>>>         at scala.Option.foreach(Option.scala:236)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>>>>>         at
>>>>> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>>>>>         at
>>>>> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>>>>>         at
>>>>> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>>>>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>>>>>         at
>>>>> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>>>>>         at $iwC$$iwC.<init>(<console>:9)
>>>>>         at $iwC.<init>(<console>:18)
>>>>>         at <init>(<console>:20)
>>>>>         at .<init>(<console>:24)
>>>>>
>>>>> I'm using latest Spark built from master HEAD yesterday. Is this a bug?
>>>>>
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--089e0111d6bc97e652050971ad65--

From dev-return-10669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 06:14:50 2014
Return-Path: <dev-return-10669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B482C10504
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 06:14:50 +0000 (UTC)
Received: (qmail 16334 invoked by uid 500); 5 Dec 2014 06:14:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16232 invoked by uid 500); 5 Dec 2014 06:14:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14493 invoked by uid 99); 5 Dec 2014 06:14:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 06:14:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 06:14:42 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so29167oib.16
        for <multiple recipients>; Thu, 04 Dec 2014 22:12:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=IepSm6pVLxYAb8bUXH44BYMeG4gR6Z12wZfNOt492YQ=;
        b=csGSvHQdHlbB/2ReLplC3wUXhbcixUVhG3x3LFkwIBFzHQWQ5U8yth4rj2xnAjIOlU
         8m60qv4NBGU1I7bzc+/36FEvmvDxGfL5Kh3vr8GxlFMdCUm74tk+LhClpfAaiYHdHfeC
         6L8CkLWRYr1g00MK4wPGC2gvLOleaEsZKb7bH2zcAWRW4sED2Hmjkq0TjrLAEQsNAeT4
         y00bjG/imymY0IZLf/GHrGe1UFK1D4k2dDFLRu6pJC3jZlSELxC+p3c5MkAPPZ4f6LsR
         ajcuX7DkRrMmCvCVVY9S8szrbtJFpHWEFl8eCWDNTiZKugUZpwfDpt2tfARToxNAfLFb
         CaCg==
MIME-Version: 1.0
X-Received: by 10.202.13.132 with SMTP id 126mr8912264oin.75.1417759971744;
 Thu, 04 Dec 2014 22:12:51 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Thu, 4 Dec 2014 22:12:51 -0800 (PST)
In-Reply-To: <CACA1tWLE6yNYnwxpd82m7d3D3W292YeyaK8Zt4kshb_r1fVsKg@mail.gmail.com>
References: <CACA1tWK0jWFU4ZCGFZW0+gGCn9XVnG2U8RDFE9yonDeEPbQ9wA@mail.gmail.com>
	<CACA1tWKhzTH22kSX3OqGLG8ZF1FfGDQ7c1scLWnZJtwYGgKHOg@mail.gmail.com>
	<CACA1tWJ88=WenF-e4wYW4rFm5y--wySRd=rQKTDbCicThGQ9eQ@mail.gmail.com>
	<CACA1tWLZcoHmhJHe-RmDkA8RY4Qe6XbUWMFho_+_puG88AmhDQ@mail.gmail.com>
	<CACA1tWJ5KynUe5Ndkd6rmQKJfpiZ2FQdVLzvVrwsRt7_hOCNiw@mail.gmail.com>
	<CACA1tWLE6yNYnwxpd82m7d3D3W292YeyaK8Zt4kshb_r1fVsKg@mail.gmail.com>
Date: Thu, 4 Dec 2014 22:12:51 -0800
Message-ID: <CABPQxsuWjirkMgJX5Lbo_pNqmpaAq_HMTD6RoMSYvSV++aLkQg@mail.gmail.com>
Subject: Re: Exception adding resource files in latest Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Jianshi Huang <jianshi.huang@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for flagging this. I reverted the relevant YARN fix in Spark
1.2 release. We can try to debug this in master.

On Thu, Dec 4, 2014 at 9:51 PM, Jianshi Huang <jianshi.huang@gmail.com> wrote:
> I created a ticket for this:
>
>   https://issues.apache.org/jira/browse/SPARK-4757
>
>
> Jianshi
>
> On Fri, Dec 5, 2014 at 1:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>>
>> Correction:
>>
>> According to Liancheng, this hotfix might be the root cause:
>>
>>
>> https://github.com/apache/spark/commit/38cb2c3a36a5c9ead4494cbc3dde008c2f0698ce
>>
>> Jianshi
>>
>> On Fri, Dec 5, 2014 at 12:45 PM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>>
>>> Looks like the datanucleus*.jar shouldn't appear in the hdfs path in
>>> Yarn-client mode.
>>>
>>> Maybe this patch broke yarn-client.
>>>
>>>
>>> https://github.com/apache/spark/commit/a975dc32799bb8a14f9e1c76defaaa7cfbaf8b53
>>>
>>> Jianshi
>>>
>>> On Fri, Dec 5, 2014 at 12:02 PM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>>
>>>> Actually my HADOOP_CLASSPATH has already been set to include
>>>> /etc/hadoop/conf/*
>>>>
>>>> export
>>>> HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
>>>> classpath)
>>>>
>>>> Jianshi
>>>>
>>>> On Fri, Dec 5, 2014 at 11:54 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>>> wrote:
>>>>>
>>>>> Looks like somehow Spark failed to find the core-site.xml in
>>>>> /et/hadoop/conf
>>>>>
>>>>> I've already set the following env variables:
>>>>>
>>>>> export YARN_CONF_DIR=/etc/hadoop/conf
>>>>> export HADOOP_CONF_DIR=/etc/hadoop/conf
>>>>> export HBASE_CONF_DIR=/etc/hbase/conf
>>>>>
>>>>> Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?
>>>>>
>>>>> Jianshi
>>>>>
>>>>> On Fri, Dec 5, 2014 at 11:37 AM, Jianshi Huang
>>>>> <jianshi.huang@gmail.com> wrote:
>>>>>>
>>>>>> I got the following error during Spark startup (Yarn-client mode):
>>>>>>
>>>>>> 14/12/04 19:33:58 INFO Client: Uploading resource
>>>>>> file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
>>>>>> ->
>>>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
>>>>>> java.lang.IllegalArgumentException: Wrong FS:
>>>>>> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
>>>>>> expected: file:///
>>>>>>         at
>>>>>> org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>>>>>>         at
>>>>>> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
>>>>>>         at
>>>>>> org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
>>>>>>         at
>>>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
>>>>>>         at
>>>>>> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
>>>>>>         at
>>>>>> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
>>>>>>         at scala.Option.foreach(Option.scala:236)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
>>>>>>         at
>>>>>> org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
>>>>>>         at
>>>>>> org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
>>>>>>         at
>>>>>> org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
>>>>>>         at
>>>>>> org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
>>>>>>         at
>>>>>> org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
>>>>>>         at $iwC$$iwC.<init>(<console>:9)
>>>>>>         at $iwC.<init>(<console>:18)
>>>>>>         at <init>(<console>:20)
>>>>>>         at .<init>(<console>:24)
>>>>>>
>>>>>> I'm using latest Spark built from master HEAD yesterday. Is this a
>>>>>> bug?
>>>>>>
>>>>>> --
>>>>>> Jianshi Huang
>>>>>>
>>>>>> LinkedIn: jianshi
>>>>>> Twitter: @jshuang
>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 07:31:02 2014
Return-Path: <dev-return-10670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4E1D1079A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 07:31:02 +0000 (UTC)
Received: (qmail 72603 invoked by uid 500); 5 Dec 2014 07:30:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72469 invoked by uid 500); 5 Dec 2014 07:30:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71053 invoked by uid 99); 5 Dec 2014 07:30:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:30:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.171 as permitted sender)
Received: from [209.85.217.171] (HELO mail-lb0-f171.google.com) (209.85.217.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:30:50 +0000
Received: by mail-lb0-f171.google.com with SMTP id n15so96914lbi.30
        for <multiple recipients>; Thu, 04 Dec 2014 23:30:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=6M/VT1lhqa6FM6H2MvoNu/oUOJYp9dAkPwjXGCXpsnQ=;
        b=ji8wqn/a4nK7ZotSZ3rIXJad3pEDY3Yl7hKly1HHfsAGjxv/WYB00XCM9XIey8NmZf
         3cCcSjfjxejtk7q1drvveiP4E79b/sF9lefVx0ucwrxreMKaYpM8cM+4STJA8VlFWAzU
         HIz0YLLql56mgx80PqBSrKJstqJ1nPHFlqIKlaSrGn4lL/bXHmUavQgzSKqJPLyCRha3
         WrODNvij6qLbzFuX/7JoBG/7cFh2druTFlynDm8y70FuStKnN24QHLotHU6yxTOYBO9Q
         0kYcj3E5f314pkUDHoXb+4tatY/90SoIxusgRUc3JtJi/HLlZJBBw09FjK/iGr/SH7P7
         wSbQ==
X-Received: by 10.112.172.97 with SMTP id bb1mr1485763lbc.38.1417764628823;
 Thu, 04 Dec 2014 23:30:28 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 23:30:08 -0800 (PST)
In-Reply-To: <80833ADD533E324CA05C160E41B63661027AB5EC@shsmsx102.ccr.corp.intel.com>
References: <CACA1tWLAAqFC4mtS4U8gyedap-4y7nGAnmbj0QEMw_AXx_NRNw@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AAE40@shsmsx102.ccr.corp.intel.com>
 <CACA1tWLVN4w2i3meNZCCUWec9o4BZuEKxPES-JNLwc6GKm0XEQ@mail.gmail.com> <80833ADD533E324CA05C160E41B63661027AB5EC@shsmsx102.ccr.corp.intel.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 15:30:08 +0800
Message-ID: <CACA1tW+kv=j-qTz1k8=K9MHvcHY2BJRPq_Z9-6AzMFyscCgH9w@mail.gmail.com>
Subject: Re: Auto BroadcastJoin optimization failed in latest Spark
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33e9013856f0509730d3e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33e9013856f0509730d3e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sorry for the late of follow-up.

I used Hao's DESC EXTENDED command and found some clue:

new (broadcast broken Spark build):
parameters:{numFiles=3D0, EXTERNAL=3DTRUE, transient_lastDdlTime=3D14177638=
92,
COLUMN_STATS_ACCURATE=3Dfalse, totalSize=3D0, numRows=3D-1, rawDataSize=3D-=
1}

old (broadcast working Spark build):
parameters:{EXTERNAL=3DTRUE, transient_lastDdlTime=3D1417763591,
totalSize=3D56166}

Looks like the table size computation failed in the latest version.

I've run the analyze command:

  ANALYZE TABLE $table COMPUTE STATISTICS noscan

And the tables are created from Parquet files:

e.g.
CREATE EXTERNAL TABLE table1 (
  code int,
  desc string
)
STORED AS PARQUET
LOCATION '/user/jianshuang/data/dim_tables/table1.parquet'


Anyone knows what went wrong?


Thanks,
Jianshi



On Fri, Nov 28, 2014 at 1:24 PM, Cheng, Hao <hao.cheng@intel.com> wrote:

>  Hi Jianshi,
>
> I couldn=E2=80=99t reproduce that with latest MASTER, and I can always ge=
t the
> BroadcastHashJoin for managed tables (in .csv file) in my testing, are
> there any external tables in your case?
>
>
>
> In general probably couple of things you can try first (with HiveContext)=
:
>
> 1)      ANALYZE TABLE xxx COMPUTE STATISTICS NOSCAN; (apply that to all
> of the tables);
>
> 2)      SET spark.sql.autoBroadcastJoinThreshold=3Dxxx; (Set the threshol=
d
> as a greater value, it is 1024*1024*10 by default, just make sure the
> maximum dimension tables size (in bytes) is less than this)
>
> 3)      Always put the main table(the biggest table) in the left-most
> among the inner joins;
>
>
>
> DESC EXTENDED tablename; -- this will print the detail information for th=
e
> statistic table size (the field =E2=80=9CtotalSize=E2=80=9D)
>
> EXPLAIN EXTENDED query; -- this will print the detail physical plan.
>
>
>
> Let me know if you still have problem.
>
>
>
> Hao
>
>
>
> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
> *Sent:* Thursday, November 27, 2014 10:24 PM
> *To:* Cheng, Hao
> *Cc:* user
> *Subject:* Re: Auto BroadcastJoin optimization failed in latest Spark
>
>
>
> Hi Hao,
>
>
>
> I'm using inner join as Broadcast join didn't work for left joins (thanks
> for the links for the latest improvements).
>
>
>
> And I'm using HiveConext and it worked in a previous build (10/12) when
> joining 15 dimension tables.
>
>
>
> Jianshi
>
>
>
> On Thu, Nov 27, 2014 at 8:35 AM, Cheng, Hao <hao.cheng@intel.com> wrote:
>
>  Are all of your join keys the same? and I guess the join type are all
> =E2=80=9CLeft=E2=80=9D join, https://github.com/apache/spark/pull/3362 pr=
obably is what
> you need.
>
>
>
> And, SparkSQL doesn=E2=80=99t support the multiway-join (and multiway-bro=
adcast
> join) currently,  https://github.com/apache/spark/pull/3270 should be
> another optimization for this.
>
>
>
>
>
> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
> *Sent:* Wednesday, November 26, 2014 4:36 PM
> *To:* user
> *Subject:* Auto BroadcastJoin optimization failed in latest Spark
>
>
>
> Hi,
>
>
>
> I've confirmed that the latest Spark with either Hive 0.12 or 0.13.1 fail=
s
> optimizing auto broadcast join in my query. I have a query that joins a
> huge fact table with 15 tiny dimension tables.
>
>
>
> I'm currently using an older version of Spark which was built on Oct. 12.
>
>
>
> Anyone else has met similar situation?
>
>
>
> --
>
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>
>
>
>
>
> --
>
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



--=20
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c33e9013856f0509730d3e--

From dev-return-10671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 07:32:45 2014
Return-Path: <dev-return-10671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC709107C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 07:32:45 +0000 (UTC)
Received: (qmail 78154 invoked by uid 500); 5 Dec 2014 07:32:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78074 invoked by uid 500); 5 Dec 2014 07:32:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76931 invoked by uid 99); 5 Dec 2014 07:32:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:32:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:32:17 +0000
Received: by mail-lb0-f172.google.com with SMTP id u10so102596lbd.17
        for <multiple recipients>; Thu, 04 Dec 2014 23:32:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=OC9h1PspR+EMa+yMvX2IRKdoDcnnPNC3ZLuKR+y8Ddc=;
        b=R/ODH2zLZvlgXx7O85FLWHt1pw721rx1CSrJt+W9z7Ud+5YvrCI2dQO5Lma4Ud2xX2
         w3W0kNDyGja/NALjY86dE27fg6XTsHByV9VINR+LI/EApRKxjpC+ausTfnAgiCDNJ2WS
         f8pal3Fq6xezMsHvFzsgS5jI6rLNi/kZaOco+ZEJMkfFIiDuxZNie+i5e0Cv8M5xe+JQ
         M5a5p0RGrtxNgxPLVYSA5HI8HoO2QtaBDzPtZol6xrxpAcHwowFoJUJjZuS0tIkTxc9P
         rHZsrnXYwdixnF0/wu469spSLHtmghf5m1PMHia7Jwebac5culny2Vf6LHnWRQb1JxAB
         mqtQ==
X-Received: by 10.152.26.131 with SMTP id l3mr1448524lag.49.1417764736921;
 Thu, 04 Dec 2014 23:32:16 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 23:31:56 -0800 (PST)
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 15:31:56 +0800
Message-ID: <CACA1tWLH0VZr_aJVtVp0__EaCLeUbW0XugUxXS8chmfpBJRfMA@mail.gmail.com>
Subject: drop table if exists throws exception
To: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160ba3a84f6900509731326
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160ba3a84f6900509731326
Content-Type: text/plain; charset=UTF-8

Hi,

I got exception saying Hive: NoSuchObjectException(message:<table> table
not found)

when running "DROP TABLE IF EXISTS <table>"

Looks like a new regression in Hive module.

Anyone can confirm this?

Thanks,
-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--089e0160ba3a84f6900509731326--

From dev-return-10672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 07:34:13 2014
Return-Path: <dev-return-10672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD3D4107D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 07:34:13 +0000 (UTC)
Received: (qmail 86693 invoked by uid 500); 5 Dec 2014 07:34:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86604 invoked by uid 500); 5 Dec 2014 07:34:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85090 invoked by uid 99); 5 Dec 2014 07:34:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:34:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:33:38 +0000
Received: by mail-lb0-f170.google.com with SMTP id w7so106230lbi.15
        for <multiple recipients>; Thu, 04 Dec 2014 23:33:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=bHCxezhZw0rdigIMCrsITc72aamHJhqkziymqg8YIvQ=;
        b=d8f3kbmXOY/g2RtXdNGF6c/RjEaX7bcbCTnvgDuCzsU6sumdMoecFZrHOjlCFFgQ90
         arG4lE8yN66OVKWJ/RJTRtGjlgExYnuWBsFLwbluS4WRmRWFw6sKNrzsicbowbWtfp/7
         LX338Qlm9lTL1RrDzcCa0RkCSlDK3ynScvXp+xtlIBI7VcZ3Cm0q5dBd75jj0sqbpuiy
         GEyJxF0cFUTQlYoHqRt/75U28/t/OAHrleK2Q3M8RPJCt78eQgInyYwDnXs5dVGmPmTj
         U0w20ctrb5O+cHxTUkI1QS0/1IVCu8sZcsozuthPMvWWtc0eNmJvqJJZM+TUZ7jPzpCw
         Fb4Q==
X-Received: by 10.152.10.40 with SMTP id f8mr1540039lab.52.1417764817729; Thu,
 04 Dec 2014 23:33:37 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 23:33:17 -0800 (PST)
In-Reply-To: <CACA1tW+kv=j-qTz1k8=K9MHvcHY2BJRPq_Z9-6AzMFyscCgH9w@mail.gmail.com>
References: <CACA1tWLAAqFC4mtS4U8gyedap-4y7nGAnmbj0QEMw_AXx_NRNw@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AAE40@shsmsx102.ccr.corp.intel.com>
 <CACA1tWLVN4w2i3meNZCCUWec9o4BZuEKxPES-JNLwc6GKm0XEQ@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AB5EC@shsmsx102.ccr.corp.intel.com> <CACA1tW+kv=j-qTz1k8=K9MHvcHY2BJRPq_Z9-6AzMFyscCgH9w@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 15:33:17 +0800
Message-ID: <CACA1tWLw17A_VtVEg64u1q1fOVBdLSaLiwDfOZFcX09f+_QnSQ@mail.gmail.com>
Subject: Re: Auto BroadcastJoin optimization failed in latest Spark
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ed6455fdea05097318b8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ed6455fdea05097318b8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

If I run ANALYZE without NOSCAN, then Hive can successfully get the size:

parameters:{numFiles=3D0, EXTERNAL=3DTRUE, transient_lastDdlTime=3D14177645=
89,
COLUMN_STATS_ACCURATE=3Dtrue, totalSize=3D0, numRows=3D1156, rawDataSize=3D=
76296}

Is Hive's PARQUET support broken?

Jianshi


On Fri, Dec 5, 2014 at 3:30 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Sorry for the late of follow-up.
>
> I used Hao's DESC EXTENDED command and found some clue:
>
> new (broadcast broken Spark build):
> parameters:{numFiles=3D0, EXTERNAL=3DTRUE, transient_lastDdlTime=3D141776=
3892,
> COLUMN_STATS_ACCURATE=3Dfalse, totalSize=3D0, numRows=3D-1, rawDataSize=
=3D-1}
>
> old (broadcast working Spark build):
> parameters:{EXTERNAL=3DTRUE, transient_lastDdlTime=3D1417763591,
> totalSize=3D56166}
>
> Looks like the table size computation failed in the latest version.
>
> I've run the analyze command:
>
>   ANALYZE TABLE $table COMPUTE STATISTICS noscan
>
> And the tables are created from Parquet files:
>
> e.g.
> CREATE EXTERNAL TABLE table1 (
>   code int,
>   desc string
> )
> STORED AS PARQUET
> LOCATION '/user/jianshuang/data/dim_tables/table1.parquet'
>
>
> Anyone knows what went wrong?
>
>
> Thanks,
> Jianshi
>
>
>
> On Fri, Nov 28, 2014 at 1:24 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
>
>>  Hi Jianshi,
>>
>> I couldn=E2=80=99t reproduce that with latest MASTER, and I can always g=
et the
>> BroadcastHashJoin for managed tables (in .csv file) in my testing, are
>> there any external tables in your case?
>>
>>
>>
>> In general probably couple of things you can try first (with HiveContext=
):
>>
>> 1)      ANALYZE TABLE xxx COMPUTE STATISTICS NOSCAN; (apply that to all
>> of the tables);
>>
>> 2)      SET spark.sql.autoBroadcastJoinThreshold=3Dxxx; (Set the thresho=
ld
>> as a greater value, it is 1024*1024*10 by default, just make sure the
>> maximum dimension tables size (in bytes) is less than this)
>>
>> 3)      Always put the main table(the biggest table) in the left-most
>> among the inner joins;
>>
>>
>>
>> DESC EXTENDED tablename; -- this will print the detail information for
>> the statistic table size (the field =E2=80=9CtotalSize=E2=80=9D)
>>
>> EXPLAIN EXTENDED query; -- this will print the detail physical plan.
>>
>>
>>
>> Let me know if you still have problem.
>>
>>
>>
>> Hao
>>
>>
>>
>> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
>> *Sent:* Thursday, November 27, 2014 10:24 PM
>> *To:* Cheng, Hao
>> *Cc:* user
>> *Subject:* Re: Auto BroadcastJoin optimization failed in latest Spark
>>
>>
>>
>> Hi Hao,
>>
>>
>>
>> I'm using inner join as Broadcast join didn't work for left joins (thank=
s
>> for the links for the latest improvements).
>>
>>
>>
>> And I'm using HiveConext and it worked in a previous build (10/12) when
>> joining 15 dimension tables.
>>
>>
>>
>> Jianshi
>>
>>
>>
>> On Thu, Nov 27, 2014 at 8:35 AM, Cheng, Hao <hao.cheng@intel.com> wrote:
>>
>>  Are all of your join keys the same? and I guess the join type are all
>> =E2=80=9CLeft=E2=80=9D join, https://github.com/apache/spark/pull/3362 p=
robably is what
>> you need.
>>
>>
>>
>> And, SparkSQL doesn=E2=80=99t support the multiway-join (and multiway-br=
oadcast
>> join) currently,  https://github.com/apache/spark/pull/3270 should be
>> another optimization for this.
>>
>>
>>
>>
>>
>> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
>> *Sent:* Wednesday, November 26, 2014 4:36 PM
>> *To:* user
>> *Subject:* Auto BroadcastJoin optimization failed in latest Spark
>>
>>
>>
>> Hi,
>>
>>
>>
>> I've confirmed that the latest Spark with either Hive 0.12 or 0.13.1
>> fails optimizing auto broadcast join in my query. I have a query that jo=
ins
>> a huge fact table with 15 tiny dimension tables.
>>
>>
>>
>> I'm currently using an older version of Spark which was built on Oct. 12=
.
>>
>>
>>
>> Anyone else has met similar situation?
>>
>>
>>
>> --
>>
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>>
>>
>>
>>
>> --
>>
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



--=20
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a1132ed6455fdea05097318b8--

From dev-return-10673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 07:52:10 2014
Return-Path: <dev-return-10673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F5FC10858
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 07:52:10 +0000 (UTC)
Received: (qmail 24736 invoked by uid 500); 5 Dec 2014 07:52:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24648 invoked by uid 500); 5 Dec 2014 07:52:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22685 invoked by uid 99); 5 Dec 2014 07:52:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:52:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.174 as permitted sender)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 07:51:56 +0000
Received: by mail-lb0-f174.google.com with SMTP id w7so119373lbi.19
        for <multiple recipients>; Thu, 04 Dec 2014 23:50:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=FZ/FX6WQfDQSmIfX4ZQMtHCRH4jj3rtvkPyu8VKQTKw=;
        b=oowEMCPoaq6Y0d4Ysv7thUmCPH95kdeNe4vQun8Asr9Ke0S/yc+YdYocbQwbErYBsS
         Z1RqZjZNLH5Jv9I/AObeNQacv4xusrxNwvQn8f3V8kpwT0vLA0NMXdZSALD3+2+Q1/36
         vhFeF3cKKtZAwDhdOBM107GXIVl6ERHEoWIQZbvFdoX/EMkzuOzi//o/Gs2I/VMuPYEg
         9SxUlDVB1TfBiKqs8i4FPzOZkryJF42Y6YaC39z8eM7kSdBdqOkI/J+qlfNyS8vBlMcx
         UXiz+jNcyo4F99yhd/qnFwSsz83z/jV7Zz6JD+4O3AsTgDirrgEHmjC+emI7tPxHNprk
         FvbQ==
X-Received: by 10.152.87.42 with SMTP id u10mr1609838laz.0.1417765850202; Thu,
 04 Dec 2014 23:50:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Thu, 4 Dec 2014 23:50:30 -0800 (PST)
In-Reply-To: <CACA1tWLw17A_VtVEg64u1q1fOVBdLSaLiwDfOZFcX09f+_QnSQ@mail.gmail.com>
References: <CACA1tWLAAqFC4mtS4U8gyedap-4y7nGAnmbj0QEMw_AXx_NRNw@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AAE40@shsmsx102.ccr.corp.intel.com>
 <CACA1tWLVN4w2i3meNZCCUWec9o4BZuEKxPES-JNLwc6GKm0XEQ@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AB5EC@shsmsx102.ccr.corp.intel.com>
 <CACA1tW+kv=j-qTz1k8=K9MHvcHY2BJRPq_Z9-6AzMFyscCgH9w@mail.gmail.com> <CACA1tWLw17A_VtVEg64u1q1fOVBdLSaLiwDfOZFcX09f+_QnSQ@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Fri, 5 Dec 2014 15:50:30 +0800
Message-ID: <CACA1tW+e=mvy=rG7TbVheBEbRpon0RVsjv4k9T4xcSf6cukw-A@mail.gmail.com>
Subject: Re: Auto BroadcastJoin optimization failed in latest Spark
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c34438e048b40509735591
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c34438e048b40509735591
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

With Liancheng's suggestion, I've tried setting

 spark.sql.hive.convertMetastoreParquet  false

but still analyze noscan return -1 in rawDataSize

Jianshi


On Fri, Dec 5, 2014 at 3:33 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> If I run ANALYZE without NOSCAN, then Hive can successfully get the size:
>
> parameters:{numFiles=3D0, EXTERNAL=3DTRUE, transient_lastDdlTime=3D141776=
4589,
> COLUMN_STATS_ACCURATE=3Dtrue, totalSize=3D0, numRows=3D1156, rawDataSize=
=3D76296}
>
> Is Hive's PARQUET support broken?
>
> Jianshi
>
>
> On Fri, Dec 5, 2014 at 3:30 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Sorry for the late of follow-up.
>>
>> I used Hao's DESC EXTENDED command and found some clue:
>>
>> new (broadcast broken Spark build):
>> parameters:{numFiles=3D0, EXTERNAL=3DTRUE, transient_lastDdlTime=3D14177=
63892,
>> COLUMN_STATS_ACCURATE=3Dfalse, totalSize=3D0, numRows=3D-1, rawDataSize=
=3D-1}
>>
>> old (broadcast working Spark build):
>> parameters:{EXTERNAL=3DTRUE, transient_lastDdlTime=3D1417763591,
>> totalSize=3D56166}
>>
>> Looks like the table size computation failed in the latest version.
>>
>> I've run the analyze command:
>>
>>   ANALYZE TABLE $table COMPUTE STATISTICS noscan
>>
>> And the tables are created from Parquet files:
>>
>> e.g.
>> CREATE EXTERNAL TABLE table1 (
>>   code int,
>>   desc string
>> )
>> STORED AS PARQUET
>> LOCATION '/user/jianshuang/data/dim_tables/table1.parquet'
>>
>>
>> Anyone knows what went wrong?
>>
>>
>> Thanks,
>> Jianshi
>>
>>
>>
>> On Fri, Nov 28, 2014 at 1:24 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
>>
>>>  Hi Jianshi,
>>>
>>> I couldn=E2=80=99t reproduce that with latest MASTER, and I can always =
get the
>>> BroadcastHashJoin for managed tables (in .csv file) in my testing, are
>>> there any external tables in your case?
>>>
>>>
>>>
>>> In general probably couple of things you can try first (with
>>> HiveContext):
>>>
>>> 1)      ANALYZE TABLE xxx COMPUTE STATISTICS NOSCAN; (apply that to all
>>> of the tables);
>>>
>>> 2)      SET spark.sql.autoBroadcastJoinThreshold=3Dxxx; (Set the
>>> threshold as a greater value, it is 1024*1024*10 by default, just make =
sure
>>> the maximum dimension tables size (in bytes) is less than this)
>>>
>>> 3)      Always put the main table(the biggest table) in the left-most
>>> among the inner joins;
>>>
>>>
>>>
>>> DESC EXTENDED tablename; -- this will print the detail information for
>>> the statistic table size (the field =E2=80=9CtotalSize=E2=80=9D)
>>>
>>> EXPLAIN EXTENDED query; -- this will print the detail physical plan.
>>>
>>>
>>>
>>> Let me know if you still have problem.
>>>
>>>
>>>
>>> Hao
>>>
>>>
>>>
>>> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
>>> *Sent:* Thursday, November 27, 2014 10:24 PM
>>> *To:* Cheng, Hao
>>> *Cc:* user
>>> *Subject:* Re: Auto BroadcastJoin optimization failed in latest Spark
>>>
>>>
>>>
>>> Hi Hao,
>>>
>>>
>>>
>>> I'm using inner join as Broadcast join didn't work for left joins
>>> (thanks for the links for the latest improvements).
>>>
>>>
>>>
>>> And I'm using HiveConext and it worked in a previous build (10/12) when
>>> joining 15 dimension tables.
>>>
>>>
>>>
>>> Jianshi
>>>
>>>
>>>
>>> On Thu, Nov 27, 2014 at 8:35 AM, Cheng, Hao <hao.cheng@intel.com> wrote=
:
>>>
>>>  Are all of your join keys the same? and I guess the join type are all
>>> =E2=80=9CLeft=E2=80=9D join, https://github.com/apache/spark/pull/3362 =
probably is what
>>> you need.
>>>
>>>
>>>
>>> And, SparkSQL doesn=E2=80=99t support the multiway-join (and multiway-b=
roadcast
>>> join) currently,  https://github.com/apache/spark/pull/3270 should be
>>> another optimization for this.
>>>
>>>
>>>
>>>
>>>
>>> *From:* Jianshi Huang [mailto:jianshi.huang@gmail.com]
>>> *Sent:* Wednesday, November 26, 2014 4:36 PM
>>> *To:* user
>>> *Subject:* Auto BroadcastJoin optimization failed in latest Spark
>>>
>>>
>>>
>>> Hi,
>>>
>>>
>>>
>>> I've confirmed that the latest Spark with either Hive 0.12 or 0.13.1
>>> fails optimizing auto broadcast join in my query. I have a query that j=
oins
>>> a huge fact table with 15 tiny dimension tables.
>>>
>>>
>>>
>>> I'm currently using an older version of Spark which was built on Oct. 1=
2.
>>>
>>>
>>>
>>> Anyone else has met similar situation?
>>>
>>>
>>>
>>> --
>>>
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>>
>>>
>>>
>>>
>>> --
>>>
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



--=20
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c34438e048b40509735591--

From dev-return-10674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 19:06:29 2014
Return-Path: <dev-return-10674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ECB36108BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 19:06:28 +0000 (UTC)
Received: (qmail 5887 invoked by uid 500); 5 Dec 2014 19:06:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5808 invoked by uid 500); 5 Dec 2014 19:06:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5795 invoked by uid 99); 5 Dec 2014 19:06:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:06:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nobigdealstyle@gmail.com designates 209.85.160.174 as permitted sender)
Received: from [209.85.160.174] (HELO mail-yk0-f174.google.com) (209.85.160.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:06:22 +0000
Received: by mail-yk0-f174.google.com with SMTP id 10so583416ykt.33
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 11:05:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=HI08z8lIoYJ42GHtm62dj501L4hbrGlzDDNzockr0B8=;
        b=d88q/eZfaxQ/GioclCFtXU2VHAIISSlMejYF2xWZeK3mgiQq4yQZpv6tGKzxY3XsFQ
         BjHcVcZA8arpexfsqSfGFG6s2x0+c3dYxJ0io4m1kIw0dW14MDwXKd34001hX4P5SU0C
         /Nzm6R4MGy0frxFq/lGFuutufXgGq9xalPn6Q0eZYhQAmspQmnUPIEcQMrSLr8c058Fm
         iwmjUAsjR0jDDvxsbzl6hpdfEQTokfFTo3irnPVBX4n/TnqEa8bgGLyPAMzOiKZIQ4ZM
         0w2GwOmcSq6ySYl4SGQjMKYcOJ59L7Vr7wXFNVHHH3zN+C07NnYFXrt9uZigPUctMbRf
         TLOg==
X-Received: by 10.236.28.20 with SMTP id f20mr20420046yha.197.1417806317016;
 Fri, 05 Dec 2014 11:05:17 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
 <CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com> <CAOhmDzddsPU6sXtFxwkxtvWj+H0va=3QQHspPqWUp-OvM4v74g@mail.gmail.com>
From: Ryan Williams <ryan.blake.williams@gmail.com>
Date: Fri, 05 Dec 2014 19:05:15 +0000
Message-ID: <CANeJXFPMCQ54p0L7DNQpeOwmVsgGL=TfZLACE9o=kgA+qko8Dg@mail.gmail.com>
Subject: Re: zinc invocation examples
To: Nicholas Chammas <nicholas.chammas@gmail.com>, Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149bdeae2dccb05097cc1f8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149bdeae2dccb05097cc1f8
Content-Type: text/plain; charset=UTF-8

fwiw I've been using `zinc -scala-home $SCALA_HOME -nailed -start` which:

- starts a nailgun server as well,
- uses my installed scala 2.{10,11}, as opposed to zinc's default 2.9.2
<https://github.com/typesafehub/zinc#scala>: "If no options are passed to
locate a version of Scala then Scala 2.9.2 is used by default (which is
bundled with zinc)."

The latter seems like it might be especially important.


On Thu Dec 04 2014 at 4:25:32 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Oh, derp. I just assumed from looking at all the options that there was
> something to it. Thanks Sean.
>
> On Thu Dec 04 2014 at 7:47:33 AM Sean Owen <sowen@cloudera.com> wrote:
>
> > You just run it once with "zinc -start" and leave it running as a
> > background process on your build machine. You don't have to do
> > anything for each build.
> >
> > On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
> > <nicholas.chammas@gmail.com> wrote:
> > > https://github.com/apache/spark/blob/master/docs/
> > building-spark.md#speeding-up-compilation-with-zinc
> > >
> > > Could someone summarize how they invoke zinc as part of a regular
> > > build-test-etc. cycle?
> > >
> > > I'll add it in to the aforelinked page if appropriate.
> > >
> > > Nick
> >
>

--089e0149bdeae2dccb05097cc1f8--

From dev-return-10675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 19:07:27 2014
Return-Path: <dev-return-10675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98AE8108C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 19:07:27 +0000 (UTC)
Received: (qmail 10836 invoked by uid 500); 5 Dec 2014 19:07:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10760 invoked by uid 500); 5 Dec 2014 19:07:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10744 invoked by uid 99); 5 Dec 2014 19:07:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:07:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:07:20 +0000
Received: by mail-lb0-f169.google.com with SMTP id p9so1062278lbv.28
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 11:05:28 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=HWjAeZSy7PKMc/6iLOqibifoD9vfXQMpGxa5Y6HJdRs=;
        b=VDCSmYEqyH3H3k0wQu4tzXP94zYR8azwT16vtPKzJ9/j15YH4MmbXu/pi7Y6X35SmO
         O3B9yseOHFoaJK4DxTAzW9yA8Lj5L1kn3xJqMxeWpalzH09UJccwgHOuWTELKHmhnbsu
         lI4O289/OKGZsEMj2bHtKHR4EIQx8d2PRRLBaRnQd5Le1NwzLeugMHl/9yfZh+dviw0+
         xalSQtSE+89I11Uycrmz7vH+OIWHe3zNSGuqx6w09A8o0R0vDdVR+cws24U3IaGBBvvc
         BEoXtX/evmeeeR4gNw8iGzUC9oZE8NOdz4g6Hndhv3NIQZAo9k5Qbp2ZwcMAjJd7CxGw
         B7Mw==
X-Gm-Message-State: ALoCoQk+iTNc6Z3+BLgx1+HTMpm6sswe9w/Zc631nnFaxDY7Zqt6eE2sj4V6Bgv+kb/verT4EFYW
MIME-Version: 1.0
X-Received: by 10.112.150.136 with SMTP id ui8mr4396263lbb.60.1417806328661;
 Fri, 05 Dec 2014 11:05:28 -0800 (PST)
Received: by 10.112.112.66 with HTTP; Fri, 5 Dec 2014 11:05:28 -0800 (PST)
In-Reply-To: <CALte62w1RbC4zKDoWnz6esrjk=ZpsjY6dPcVmOcTxMW3Jb6VxA@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
	<CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
	<CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
	<CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com>
	<CAOhmDzdLANy8adD4y02isgh5UVDCr4tee_Wz1ecO=gTY4Mr+sw@mail.gmail.com>
	<CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
	<CALte62w1RbC4zKDoWnz6esrjk=ZpsjY6dPcVmOcTxMW3Jb6VxA@mail.gmail.com>
Date: Fri, 5 Dec 2014 11:05:28 -0800
Message-ID: <CAMJOb8knFj6iEPpT6ietTKhKQMAxPbpcaE_aGv=41nCgKH9uKw@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Andrew Or <andrew@databricks.com>
To: Ted Yu <yuzhihong@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a8c80949ef005097cc266
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8c80949ef005097cc266
Content-Type: text/plain; charset=UTF-8

@Patrick and Josh actually we went even further than that. We simply
disable the UI for most tests and these used to be the single largest
source of port conflict.

--047d7b3a8c80949ef005097cc266--

From dev-return-10676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 19:12:12 2014
Return-Path: <dev-return-10676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B9C510903
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 19:12:12 +0000 (UTC)
Received: (qmail 33925 invoked by uid 500); 5 Dec 2014 19:12:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33854 invoked by uid 500); 5 Dec 2014 19:12:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33842 invoked by uid 99); 5 Dec 2014 19:12:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:12:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:11:45 +0000
Received: by mail-oi0-f53.google.com with SMTP id x69so883937oia.40
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 11:10:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ceIr5OPR9qJYxl2GfUEVgh4S1gWsh8tRu788lvHx0XE=;
        b=yLNqYhH8ZN1mUl/wEtdKPg+ANASIeEx/Ed9drscEzqoe7P5PoPf/7wDpr4tMHBtPx/
         x5eWi7Est8AcGeBK2s0HpY/61FlQ9Jlakd01S5UbDQVfOh8FKzETI6IDdkBraj1+rW7D
         hLHY0bAlc38s5had5HQZFnVgdKCimiqW/mITzdMr9rOUpqu2PDAuxirX/5pevA2A3fxp
         wZ7bqnF5QLruK3HqdpCjS6Pobclj4zLpn0aPcSngGk9q9rJI3c+xCCmtnJeKbsZX09M2
         3/sY7CljUofI7yoG9ci+WeV+P++MyxbNVaan5i2VzP9XpoYVzONLAjRLFQwU699HXDUO
         bhZA==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr10783197oie.78.1417806658927;
 Fri, 05 Dec 2014 11:10:58 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Fri, 5 Dec 2014 11:10:58 -0800 (PST)
In-Reply-To: <CANeJXFPMCQ54p0L7DNQpeOwmVsgGL=TfZLACE9o=kgA+qko8Dg@mail.gmail.com>
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
	<CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com>
	<CAOhmDzddsPU6sXtFxwkxtvWj+H0va=3QQHspPqWUp-OvM4v74g@mail.gmail.com>
	<CANeJXFPMCQ54p0L7DNQpeOwmVsgGL=TfZLACE9o=kgA+qko8Dg@mail.gmail.com>
Date: Fri, 5 Dec 2014 11:10:58 -0800
Message-ID: <CABPQxsuq_-CyknzRNhe6DDeZ_6ddArzV9Z4pLgaodZPL6Uz3pw@mail.gmail.com>
Subject: Re: zinc invocation examples
From: Patrick Wendell <pwendell@gmail.com>
To: Ryan Williams <ryan.blake.williams@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

One thing I created a JIRA for a while back was to have a similar
script to "sbt/sbt" that transparently downloads Zinc, Scala, and
Maven in a subdirectory of Spark and sets it up correctly. I.e.
"build/mvn".

Outside of brew for MacOS there aren't good Zinc packages, and it's a
pain to figure out how to set it up.

https://issues.apache.org/jira/browse/SPARK-4501

Prashant Sharma looked at this for a bit but I don't think he's
working on it actively any more, so if someone wanted to do this, I'd
be extremely grateful.

- Patrick

On Fri, Dec 5, 2014 at 11:05 AM, Ryan Williams
<ryan.blake.williams@gmail.com> wrote:
> fwiw I've been using `zinc -scala-home $SCALA_HOME -nailed -start` which:
>
> - starts a nailgun server as well,
> - uses my installed scala 2.{10,11}, as opposed to zinc's default 2.9.2
> <https://github.com/typesafehub/zinc#scala>: "If no options are passed to
> locate a version of Scala then Scala 2.9.2 is used by default (which is
> bundled with zinc)."
>
> The latter seems like it might be especially important.
>
>
> On Thu Dec 04 2014 at 4:25:32 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> Oh, derp. I just assumed from looking at all the options that there was
>> something to it. Thanks Sean.
>>
>> On Thu Dec 04 2014 at 7:47:33 AM Sean Owen <sowen@cloudera.com> wrote:
>>
>> > You just run it once with "zinc -start" and leave it running as a
>> > background process on your build machine. You don't have to do
>> > anything for each build.
>> >
>> > On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
>> > <nicholas.chammas@gmail.com> wrote:
>> > > https://github.com/apache/spark/blob/master/docs/
>> > building-spark.md#speeding-up-compilation-with-zinc
>> > >
>> > > Could someone summarize how they invoke zinc as part of a regular
>> > > build-test-etc. cycle?
>> > >
>> > > I'll add it in to the aforelinked page if appropriate.
>> > >
>> > > Nick
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 19:43:40 2014
Return-Path: <dev-return-10677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA64C10A6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 19:43:40 +0000 (UTC)
Received: (qmail 60804 invoked by uid 500); 5 Dec 2014 19:43:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60736 invoked by uid 500); 5 Dec 2014 19:43:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60725 invoked by uid 99); 5 Dec 2014 19:43:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:43:38 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:43:13 +0000
Received: by mail-la0-f46.google.com with SMTP id q1so1236574lam.5
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 11:43:11 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=w99DC72ulZ+V+sI3r/gQAcYBLQ3q9FrHTmrdcGALqac=;
        b=Of7mlTOI8owswtvmdBOIlPLMK65h0Tkh+k/4mSome8Y/OIp7t5BgNBA9zaZpufzEZe
         nNm8jt83P2wv/ahKGIldaRkwMJKVDzxnqW91owwKYfcAUiE+UVaG9N5dKCO1jY28LyMc
         pLE7nFlzqoAxOuIT/SytJKrfGIvr+U1B+SlRw+tltMPqCHIJQDeYrR7Gz8h/VX3aBXYh
         /R9f9An19P27ClC4gn2zSFrB3gLq45I1rDyhwrrw029C84uvciTMqo/8H3+dAZhd86L8
         WqkhXoo0VkVb2DVpmigZUGuPVS5MGMthOF9ZzN/N6bs9Y0cT2D5lZly0Feu9OfoLMvXX
         dFTw==
X-Gm-Message-State: ALoCoQn/SkXqifC1EqroB60Okb7G6g/FxAKfvaWitM4DTHXM/csNdwiYKyhQUXe2MfBjeWyCP1Ic
X-Received: by 10.152.28.131 with SMTP id b3mr4714936lah.12.1417808591739;
 Fri, 05 Dec 2014 11:43:11 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Fri, 5 Dec 2014 11:42:51 -0800 (PST)
In-Reply-To: <CACA1tWLH0VZr_aJVtVp0__EaCLeUbW0XugUxXS8chmfpBJRfMA@mail.gmail.com>
References: <CACA1tWLH0VZr_aJVtVp0__EaCLeUbW0XugUxXS8chmfpBJRfMA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 5 Dec 2014 11:42:51 -0800
Message-ID: <CAAswR-5aG=bPGX1wQeeL2F+eAdGNdwrQ42npbBo_JNTyPOZ4BQ@mail.gmail.com>
Subject: Re: drop table if exists throws exception
To: Jianshi Huang <jianshi.huang@gmail.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b7cc786f8505097d4992
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b7cc786f8505097d4992
Content-Type: text/plain; charset=UTF-8

The command run fine for me on master.  Note that Hive does print an
exception in the logs, but that exception does not propogate to user code.

On Thu, Dec 4, 2014 at 11:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Hi,
>
> I got exception saying Hive: NoSuchObjectException(message:<table> table
> not found)
>
> when running "DROP TABLE IF EXISTS <table>"
>
> Looks like a new regression in Hive module.
>
> Anyone can confirm this?
>
> Thanks,
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>

--089e0160b7cc786f8505097d4992--

From dev-return-10678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 19:47:47 2014
Return-Path: <dev-return-10678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48D3510A81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 19:47:47 +0000 (UTC)
Received: (qmail 74946 invoked by uid 500); 5 Dec 2014 19:47:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74859 invoked by uid 500); 5 Dec 2014 19:47:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74847 invoked by uid 99); 5 Dec 2014 19:47:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:47:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 19:47:20 +0000
Received: by mail-wg0-f42.google.com with SMTP id z12so1774362wgg.29
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 11:45:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=kB0COMLYk4LX1GytKwkAY002u9D+/7lZH60a7/O6Tbg=;
        b=SEnANVf+MwZGh70TVHEYw49MzZrRO5UO4Q3XzSL1r4deRzBaWik482VgHVLxTKKJta
         uSKLSC5QyCfvKg3TtiMlVGN5ReizaNNaS5wZ3vZ85tuk8VLmJSJE/eso7aOSNnar0az0
         mtKZQN6S65aR426PCwaB8aQmz6omavCDEc6FwftDwtEzFwoYMK9KJ0yN4WbjcfltUs45
         moyUGlh+l/wF4gEuUG0eGWe4pWUe1WzoQFh+nBux1DHAiW+Oqkpj3IfNqkuoi08pui3/
         b2nlbl2syhH+x2Fmku3brZfTOFT9fnKIsLdtTuI6Mz3gn12OcwjyuzqeEfeWx9OQZolh
         oE3A==
X-Gm-Message-State: ALoCoQmsMiscwe7kwj9F4JZpyJB4OD6m/OD4GFzv+L3VUMV53jIr4iAweHOnLEkBnFqG+5QAjNKD
MIME-Version: 1.0
X-Received: by 10.194.108.162 with SMTP id hl2mr27262510wjb.102.1417808704266;
 Fri, 05 Dec 2014 11:45:04 -0800 (PST)
Received: by 10.216.68.137 with HTTP; Fri, 5 Dec 2014 11:45:04 -0800 (PST)
In-Reply-To: <CAAswR-5aG=bPGX1wQeeL2F+eAdGNdwrQ42npbBo_JNTyPOZ4BQ@mail.gmail.com>
References: <CACA1tWLH0VZr_aJVtVp0__EaCLeUbW0XugUxXS8chmfpBJRfMA@mail.gmail.com>
	<CAAswR-5aG=bPGX1wQeeL2F+eAdGNdwrQ42npbBo_JNTyPOZ4BQ@mail.gmail.com>
Date: Fri, 5 Dec 2014 11:45:04 -0800
Message-ID: <CAAsvFPmP2wC0CL5KcKfSMya8TBrdv0z7xJVCy0Jvz+D-ePPHow@mail.gmail.com>
Subject: Re: drop table if exists throws exception
From: Mark Hamstra <mark@clearstorydata.com>
To: Michael Armbrust <michael@databricks.com>
Cc: Jianshi Huang <jianshi.huang@gmail.com>, user <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0102f2122d7ee305097d5022
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0102f2122d7ee305097d5022
Content-Type: text/plain; charset=UTF-8

And that is no different from how Hive has worked for a long time.

On Fri, Dec 5, 2014 at 11:42 AM, Michael Armbrust <michael@databricks.com>
wrote:

> The command run fine for me on master.  Note that Hive does print an
> exception in the logs, but that exception does not propogate to user code.
>
> On Thu, Dec 4, 2014 at 11:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
> > Hi,
> >
> > I got exception saying Hive: NoSuchObjectException(message:<table> table
> > not found)
> >
> > when running "DROP TABLE IF EXISTS <table>"
> >
> > Looks like a new regression in Hive module.
> >
> > Anyone can confirm this?
> >
> > Thanks,
> > --
> > Jianshi Huang
> >
> > LinkedIn: jianshi
> > Twitter: @jshuang
> > Github & Blog: http://huangjs.github.com/
> >
>

--089e0102f2122d7ee305097d5022--

From dev-return-10679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 20:15:33 2014
Return-Path: <dev-return-10679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74ACF10BD4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 20:15:33 +0000 (UTC)
Received: (qmail 53878 invoked by uid 500); 5 Dec 2014 20:15:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53797 invoked by uid 500); 5 Dec 2014 20:15:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 12090 invoked by uid 99); 5 Dec 2014 20:04:48 -0000
X-ASF-Spam-Status: No, hits=3.7 required=10.0
	tests=FORGED_HOTMAIL_RCVD2,FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of kendb15@hotmail.com does not designate 162.253.133.43 as permitted sender)
Date: Fri, 5 Dec 2014 13:03:19 -0700 (MST)
From: kb <kendb15@hotmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1417809799843-9662.post@n3.nabble.com>
Subject: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am having trouble getting "create table as select" or saveAsTable from a
hiveContext to work with temp tables in spark 1.2.  No issues in 1.1.0 or
1.1.1

Simple modification to test case in the hive SQLQuerySuite.scala:

test("double nested data") {
    sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
Nil).registerTempTable("nested")
    checkAnswer(
      sql("SELECT f1.f2.f3 FROM nested"),
      1)
    checkAnswer(sql("CREATE TABLE test_ctas_1234 AS SELECT * from nested"),
Seq.empty[Row])
    checkAnswer(
      sql("SELECT * FROM test_ctas_1234"),
      sql("SELECT * FROM nested").collect().toSeq)
  }


output:

11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not found
'nested'
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
	at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply$mcV$sp(SQLQuerySuite.scala:122)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
	at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at
org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at
org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at
org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.scalatest.FunSuite.run(FunSuite.scala:1555)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at
org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at
org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at
org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at
org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at
org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
	at
org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/CREATE-TABLE-AS-SELECT-does-not-work-with-temp-tables-in-1-2-0-tp9662.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 20:52:04 2014
Return-Path: <dev-return-10680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB16F10D42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 20:52:04 +0000 (UTC)
Received: (qmail 76488 invoked by uid 500); 5 Dec 2014 20:52:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76414 invoked by uid 500); 5 Dec 2014 20:52:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75965 invoked by uid 99); 5 Dec 2014 20:52:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:52:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.26 as permitted sender)
Received: from [77.75.72.26] (HELO mxh1.seznam.cz) (77.75.72.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:51:56 +0000
Received: from email.seznam.cz
	by email-smtpc8b.go.seznam.cz (email-smtpc8b.go.seznam.cz [192.168.92.47])
	id 386ceb87bf750d413bf4eec5;
	Fri, 05 Dec 2014 21:51:34 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1417812694; bh=m0Sw6PnEJ5RhK8qhOFblRUGnaZi27NRkq9U8bHwwYiQ=;
	h=Received:From:To:Subject:Date:Message-Id:Mime-Version:X-Mailer:
	 Content-Type;
	b=PtqnwiyGB2J1JRfUx40qNke0s215HPby3oz7RjQuMzuiyezh2mpoLJkNKgLUASgLI
	 rFyWwOGBfOkUbUhKifMc4U4Ml0dOK5UVazTC/zjBVIBVNQUp0myDTkVdILgNJn/vlE
	 NQKFMEi0xsOm46xUZsR182YaFHw8xx4Yu21RpBsQ=
Received: from nat.2-52-prg.avast.com (nat.2-52-prg.avast.com [91.213.143.252])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Fri, 05 Dec 2014 21:51:32 +0100 (CET)
From: <spark.dubovsky.jakub@seznam.cz>
To: <dev@spark.apache.org>
Subject: Protobuf version in mvn vs sbt
Date: Fri, 05 Dec 2014 21:51:32 +0100 (CET)
Message-Id: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_765c300a2ab380064db74ede=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_765c300a2ab380064db74ede=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi devs,=0A=
=0A=
=C2=A0 I play with your amazing Spark here in Prague for some time. I have=
 =0A=
stumbled on a thing which I like to ask about. I create assembly jars from=
 =0A=
source and then use it to run simple jobs on our 2.3.0-cdh5.1.3 cluster =
=0A=
using yarn. Example of my usage [1]. Formerly I had started to use sbt for=
 =0A=
creating assemblies like this [2] which runs just fine. Then reading those=
 =0A=
maven-prefered stories here on dev list I found make-distribution.sh scrip=
t =0A=
in root of codebase and wanted to give it a try. I used it to create =
=0A=
assembly by both [3] and [4].=0A=
=0A=
=C2=A0 But I am not able to use assemblies created by make-distribution be=
cause =0A=
it refuses to be submited to cluster. Here is what happens:=0A=
- run [3] or [4]=0A=
- recompile app agains new assembly=0A=
- submit job using new assembly by [1] like command=0A=
- submit fails with important parts of stack trace being [5]=0A=
=0A=
=C2=A0 My guess is that it is due to improper version of protobuf included=
 in =0A=
assembly jar. My questions are:=0A=
- Can you confirm this hypothesis?=0A=
- What is the difference between sbt and mvn way of creating assembly? I =
=0A=
mean sbt works and mvn not...=0A=
- What additional option I need to pass to make-distribution to make it =
=0A=
work?=0A=
=0A=
=C2=A0 Any help/explanation here would be appreciated=0A=
=0A=
=C2=A0 Jakub=0A=
----------------------=0A=
[1] ./bin/spark-submit --num-executors 200 --master yarn-cluster --conf =
=0A=
spark.yarn.jar=3Dassembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSHOT-=
=0A=
hadoop2.3.0-cdh5.1.3.jar --class org.apache.spark.mllib.=0A=
CreateGuidDomainDictionary root-0.1.jar ${args}=0A=
=0A=
[2] ./sbt/sbt -Dhadoop.version=3D2.3.0-cdh5.1.3 -Pyarn -Phive assembly/=
=0A=
assembly=0A=
=0A=
[3] ./make-distribution.sh -Dhadoop.version=3D2.3.0-cdh5.1.3 -Pyarn -Phive=
 -=0A=
DskipTests=0A=
=0A=
[4] ./make-distribution.sh -Dyarn.version=3D2.3.0 -Dhadoop.version=3D2.3.0=
-cdh=0A=
5.1.3 -Pyarn -Phive -DskipTests=0A=
=0A=
[5]Exception in thread "main" org.apache.hadoop.yarn.exceptions.=0A=
YarnRuntimeException: java.lang.reflect.InvocationTargetException=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at org.apache.hadoop.yarn.facto=
ries.impl.pb.RpcClientFactoryPBImpl.=0A=
getClient(RpcClientFactoryPBImpl.java:79)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at org.apache.hadoop.yarn.ipc.H=
adoopYarnProtoRPC.getProxy=0A=
(HadoopYarnProtoRPC.java:48)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at org.apache.hadoop.yarn.clien=
t.RMProxy$1.run(RMProxy.java:134)=0A=
...=0A=
Caused by: java.lang.reflect.InvocationTargetException=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at sun.reflect.NativeConstructo=
rAccessorImpl.newInstance0(Native =0A=
Method)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at sun.reflect.NativeConstructo=
rAccessorImpl.newInstance=0A=
(NativeConstructorAccessorImpl.java:39)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at sun.reflect.DelegatingConstr=
uctorAccessorImpl.newInstance=0A=
(DelegatingConstructorAccessorImpl.java:27)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at java.lang.reflect.Constructo=
r.newInstance(Constructor.java:513)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at org.apache.hadoop.yarn.facto=
ries.impl.pb.RpcClientFactoryPBImpl.=0A=
getClient(RpcClientFactoryPBImpl.java:76)=0A=
... 27 more=0A=
Caused by: java.lang.VerifyError: class org.apache.hadoop.yarn.proto.=
=0A=
YarnServiceProtos$SubmitApplicationRequestProto overrides final method =
=0A=
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at java.lang.ClassLoader.define=
Class1(Native Method)=0A=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 at java.lang.ClassLoader.define=
ClassCond(ClassLoader.java:631)=0A=
=0A=
--=_765c300a2ab380064db74ede=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-10681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 20:52:09 2014
Return-Path: <dev-return-10681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8980510D43
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 20:52:09 +0000 (UTC)
Received: (qmail 78042 invoked by uid 500); 5 Dec 2014 20:52:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77968 invoked by uid 500); 5 Dec 2014 20:52:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77957 invoked by uid 99); 5 Dec 2014 20:52:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:52:07 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:52:03 +0000
Received: by mail-la0-f52.google.com with SMTP id hs14so1310390lab.39
        for <dev@spark.incubator.apache.org>; Fri, 05 Dec 2014 12:51:42 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1yRV+1qRzPQRltAko77e1fyLjICPVshLqmn0JAWmXss=;
        b=RVxoQ/Nl2fjoLkVG4bX32wiI4Dt26ZgIVTJz0M7/7R0z6dmd1XqUlLXTVCKqN2kAg3
         DOA3GkNWclgTLlxokDE/kRhT0MD06UHG8A1w20s3ih/ub0gbXNWU97x6YMw0RsPO9qAK
         cb7vrCOewnvbuDzAaADYnmr3+g6ecz27AAHF0MQN/2CYJJdqb1m/+RVA/0T569fm/u0W
         wBVU3D+/y30KFqdaUfuHtaHwAXaNFIxvPCpoDPQKwr1iHFL/BBm/qfs9FS3E4ctzDRhZ
         8zSkM9NfWY5kjcY2ohrBd4C5GoONk1zSpaiHOEVeS9TDzXtznuVEhz8scxxaxap7THas
         Zgjg==
X-Gm-Message-State: ALoCoQlKc+ZCOXPr1Yy8FIu8m0WcVLCLEDtRZdB6UmEZE/uWYF6f0Q9QlSkT1/uwq+T9x29OIyVl
X-Received: by 10.112.133.138 with SMTP id pc10mr4824298lbb.48.1417812701981;
 Fri, 05 Dec 2014 12:51:41 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Fri, 5 Dec 2014 12:51:21 -0800 (PST)
In-Reply-To: <1417809799843-9662.post@n3.nabble.com>
References: <1417809799843-9662.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 5 Dec 2014 12:51:21 -0800
Message-ID: <CAAswR-7-j1ZRd5qkfpEHQoEm7_PHXWrX6sHYjQPh2NBur6ZM=g@mail.gmail.com>
Subject: Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
To: kb <kendb15@hotmail.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, Cheng Hao <chhao01@gmail.com>
Content-Type: multipart/alternative; boundary=047d7b34398e75c6a705097e3e6b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b34398e75c6a705097e3e6b
Content-Type: text/plain; charset=UTF-8

Thanks for reporting.  This looks like a regression related to:
https://github.com/apache/spark/pull/2570

I've filed it here: https://issues.apache.org/jira/browse/SPARK-4769

On Fri, Dec 5, 2014 at 12:03 PM, kb <kendb15@hotmail.com> wrote:

> I am having trouble getting "create table as select" or saveAsTable from a
> hiveContext to work with temp tables in spark 1.2.  No issues in 1.1.0 or
> 1.1.1
>
> Simple modification to test case in the hive SQLQuerySuite.scala:
>
> test("double nested data") {
>     sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
> Nil).registerTempTable("nested")
>     checkAnswer(
>       sql("SELECT f1.f2.f3 FROM nested"),
>       1)
>     checkAnswer(sql("CREATE TABLE test_ctas_1234 AS SELECT * from nested"),
> Seq.empty[Row])
>     checkAnswer(
>       sql("SELECT * FROM test_ctas_1234"),
>       sql("SELECT * FROM nested").collect().toSeq)
>   }
>
>
> output:
>
> 11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
> org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not
> found
> 'nested'
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
>         at
>
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
>         at
>
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
>         at
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
>         at
> org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
>         at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
>         at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply$mcV$sp(SQLQuerySuite.scala:122)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
>         at
>
> org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
>         at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>         at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>         at org.scalatest.Transformer.apply(Transformer.scala:22)
>         at org.scalatest.Transformer.apply(Transformer.scala:20)
>         at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
>         at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
>         at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
>         at
>
> org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
>         at
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
>         at
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
>         at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
>         at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
>         at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
>         at
>
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
>         at
>
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
>         at
>
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
>         at
>
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
>         at scala.collection.immutable.List.foreach(List.scala:318)
>         at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
>         at
> org.scalatest.SuperEngine.org
> $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
>         at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
>         at
> org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
>         at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
>         at org.scalatest.Suite$class.run(Suite.scala:1424)
>         at
> org.scalatest.FunSuite.org
> $scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
>         at
> org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
>         at
> org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
>         at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
>         at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
>         at org.scalatest.FunSuite.run(FunSuite.scala:1555)
>         at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
>         at
>
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
>         at
>
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
>         at scala.collection.immutable.List.foreach(List.scala:318)
>         at
> org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
>         at
>
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
>         at
>
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
>         at
>
> org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
>         at
>
> org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
>         at org.scalatest.tools.Runner$.run(Runner.scala:883)
>         at org.scalatest.tools.Runner.run(Runner.scala)
>         at
>
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
>         at
>
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>         at
>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>         at java.lang.reflect.Method.invoke(Method.java:606)
>         at
> com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/CREATE-TABLE-AS-SELECT-does-not-work-with-temp-tables-in-1-2-0-tp9662.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b34398e75c6a705097e3e6b--

From dev-return-10682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 20:55:53 2014
Return-Path: <dev-return-10682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB81E10D61
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 20:55:53 +0000 (UTC)
Received: (qmail 88439 invoked by uid 500); 5 Dec 2014 20:55:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88362 invoked by uid 500); 5 Dec 2014 20:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88347 invoked by uid 99); 5 Dec 2014 20:55:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:55:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 20:55:26 +0000
Received: by mail-qc0-f181.google.com with SMTP id m20so1087221qcx.40
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 12:54:40 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=azER/llpSXrODrGW9JViEFCEpcrJ1xZvJqwoupOBkKg=;
        b=gEF8MgUtEe2J861TBYCacAHiwWWB8vAVSIL6lfeFeEBETMIS8eOZRdJEzdSWCayAs3
         IprnQLAFSGisI6X8M2/Ihd/8XPcHT5hqyRMLUrKqDhKt54pmtVuqud3pO4Ea5S+roDGR
         Vt9DnHjfp3Bgs9277/9e5G1zuW3WTyBQ1HouPuRI/f5rsj7oZ0pBEyEx/p2ygVQWvjaC
         9jq6DG5qIapCTJ45zXCLn2HcmBnqhz3bXcEgRwPyMeZnm/R1Z5SNB6SbDytf+I/vzYxw
         Nw9TIpH7uMeq8UWHYTbA6TLJUnGPjWMGFmL/aM188t65LRf78Gb8LSC7uJ+f/yjuJWnU
         ylmQ==
X-Gm-Message-State: ALoCoQl6scgWyZGDqiCLNlWVNrvC+x30gWSO+W0o15KEBx4cdbFWRmgAdWpt2VAB2msFGNBumtBk
MIME-Version: 1.0
X-Received: by 10.224.88.3 with SMTP id y3mr29337277qal.75.1417812880386; Fri,
 05 Dec 2014 12:54:40 -0800 (PST)
Received: by 10.229.15.202 with HTTP; Fri, 5 Dec 2014 12:54:40 -0800 (PST)
In-Reply-To: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
References: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
Date: Fri, 5 Dec 2014 12:54:40 -0800
Message-ID: <CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
Subject: Re: Protobuf version in mvn vs sbt
From: Marcelo Vanzin <vanzin@cloudera.com>
To: spark.dubovsky.jakub@seznam.cz
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

When building against Hadoop 2.x, you need to enable the appropriate
profile, aside from just specifying the version. e.g. "-Phadoop-2.3"
for Hadoop 2.3.

On Fri, Dec 5, 2014 at 12:51 PM,  <spark.dubovsky.jakub@seznam.cz> wrote:
> Hi devs,
>
>   I play with your amazing Spark here in Prague for some time. I have
> stumbled on a thing which I like to ask about. I create assembly jars from
> source and then use it to run simple jobs on our 2.3.0-cdh5.1.3 cluster
> using yarn. Example of my usage [1]. Formerly I had started to use sbt for
> creating assemblies like this [2] which runs just fine. Then reading those
> maven-prefered stories here on dev list I found make-distribution.sh script
> in root of codebase and wanted to give it a try. I used it to create
> assembly by both [3] and [4].
>
>   But I am not able to use assemblies created by make-distribution because
> it refuses to be submited to cluster. Here is what happens:
> - run [3] or [4]
> - recompile app agains new assembly
> - submit job using new assembly by [1] like command
> - submit fails with important parts of stack trace being [5]
>
>   My guess is that it is due to improper version of protobuf included in
> assembly jar. My questions are:
> - Can you confirm this hypothesis?
> - What is the difference between sbt and mvn way of creating assembly? I
> mean sbt works and mvn not...
> - What additional option I need to pass to make-distribution to make it
> work?
>
>   Any help/explanation here would be appreciated
>
>   Jakub
> ----------------------
> [1] ./bin/spark-submit --num-executors 200 --master yarn-cluster --conf
> spark.yarn.jar=assembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSHOT-
> hadoop2.3.0-cdh5.1.3.jar --class org.apache.spark.mllib.
> CreateGuidDomainDictionary root-0.1.jar ${args}
>
> [2] ./sbt/sbt -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive assembly/
> assembly
>
> [3] ./make-distribution.sh -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive -
> DskipTests
>
> [4] ./make-distribution.sh -Dyarn.version=2.3.0 -Dhadoop.version=2.3.0-cdh
> 5.1.3 -Pyarn -Phive -DskipTests
>
> [5]Exception in thread "main" org.apache.hadoop.yarn.exceptions.
> YarnRuntimeException: java.lang.reflect.InvocationTargetException
>         at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
> getClient(RpcClientFactoryPBImpl.java:79)
>         at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getProxy
> (HadoopYarnProtoRPC.java:48)
>         at org.apache.hadoop.yarn.client.RMProxy$1.run(RMProxy.java:134)
> ...
> Caused by: java.lang.reflect.InvocationTargetException
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> Method)
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance
> (NativeConstructorAccessorImpl.java:39)
>         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance
> (DelegatingConstructorAccessorImpl.java:27)
>         at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
>         at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
> getClient(RpcClientFactoryPBImpl.java:76)
> ... 27 more
> Caused by: java.lang.VerifyError: class org.apache.hadoop.yarn.proto.
> YarnServiceProtos$SubmitApplicationRequestProto overrides final method
> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
>         at java.lang.ClassLoader.defineClass1(Native Method)
>         at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 21:31:09 2014
Return-Path: <dev-return-10683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E16A10F27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 21:31:09 +0000 (UTC)
Received: (qmail 97170 invoked by uid 500); 5 Dec 2014 21:31:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97095 invoked by uid 500); 5 Dec 2014 21:31:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96496 invoked by uid 99); 5 Dec 2014 21:31:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:31:07 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:30:42 +0000
Received: by mail-qa0-f54.google.com with SMTP id i13so1021822qae.41
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 13:29:10 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=I8y9e8mC3GhxdPBITk5oKo+nqrIHiYu6lRsbO+3I6+k=;
        b=Sq15Iny77zFJW8YkmqpXxzHHyERHZkngEShtzHwQ3sU42akzFTnZ0GQIVio6NgRoWT
         hkM8LFm0bs630lSFBP5p4wRYKKBx3/3msJzFGKRP/pHZHGEIFD2C0FIIolanZu6nEvzP
         vLWHt9wTYZwYSKSxRDOTNim8n2n67IB3FO1BqSX2WpkcTtipmsAqRMWBt6Ls1QIGt/IC
         7QiHNp/oa3HE8TiSfBM+nnf2XXzuKY1M67yBDBvZZtD8NFV+kTXY4RsN5CjKKT2FaOoN
         CmRwY6zYqvxFia0jcar0GGDb/isIUNdWfIBhUVmgd3CK79rddoONvIK8CF4/BCJHw/Ju
         c9mQ==
X-Gm-Message-State: ALoCoQmfpUqm80D4zUyffMZxD4GqPwvbRcVWnzh/e1ZfLAC2ljw2r9epbmN9S4j5kjgoPtzTneJd
MIME-Version: 1.0
X-Received: by 10.140.106.35 with SMTP id d32mr29176141qgf.48.1417814950599;
 Fri, 05 Dec 2014 13:29:10 -0800 (PST)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.com
Received: by 10.229.15.72 with HTTP; Fri, 5 Dec 2014 13:29:10 -0800 (PST)
In-Reply-To: <CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
References: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
	<CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
Date: Fri, 5 Dec 2014 13:29:10 -0800
X-Google-Sender-Auth: BL9olBKBmu28c-NYEuT02WfASSk
Message-ID: <CAEYYnxba7SaWwgMjtc_kmiC-vdurHSyBQ4w6eMqLTtDjSYwsjA@mail.gmail.com>
Subject: Re: Protobuf version in mvn vs sbt
From: DB Tsai <dbtsai@dbtsai.com>
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: spark.dubovsky.jakub@seznam.cz, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try

./make-distribution.sh -Pyarn -Phive -Phadoop-2.3
-Dhadoop.version=2.3.0-cdh5.1.3 -DskipTests

See the detail of how to change the profile at
https://spark.apache.org/docs/latest/building-with-maven.html

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Fri, Dec 5, 2014 at 12:54 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> When building against Hadoop 2.x, you need to enable the appropriate
> profile, aside from just specifying the version. e.g. "-Phadoop-2.3"
> for Hadoop 2.3.
>
> On Fri, Dec 5, 2014 at 12:51 PM,  <spark.dubovsky.jakub@seznam.cz> wrote:
>> Hi devs,
>>
>>   I play with your amazing Spark here in Prague for some time. I have
>> stumbled on a thing which I like to ask about. I create assembly jars from
>> source and then use it to run simple jobs on our 2.3.0-cdh5.1.3 cluster
>> using yarn. Example of my usage [1]. Formerly I had started to use sbt for
>> creating assemblies like this [2] which runs just fine. Then reading those
>> maven-prefered stories here on dev list I found make-distribution.sh script
>> in root of codebase and wanted to give it a try. I used it to create
>> assembly by both [3] and [4].
>>
>>   But I am not able to use assemblies created by make-distribution because
>> it refuses to be submited to cluster. Here is what happens:
>> - run [3] or [4]
>> - recompile app agains new assembly
>> - submit job using new assembly by [1] like command
>> - submit fails with important parts of stack trace being [5]
>>
>>   My guess is that it is due to improper version of protobuf included in
>> assembly jar. My questions are:
>> - Can you confirm this hypothesis?
>> - What is the difference between sbt and mvn way of creating assembly? I
>> mean sbt works and mvn not...
>> - What additional option I need to pass to make-distribution to make it
>> work?
>>
>>   Any help/explanation here would be appreciated
>>
>>   Jakub
>> ----------------------
>> [1] ./bin/spark-submit --num-executors 200 --master yarn-cluster --conf
>> spark.yarn.jar=assembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSHOT-
>> hadoop2.3.0-cdh5.1.3.jar --class org.apache.spark.mllib.
>> CreateGuidDomainDictionary root-0.1.jar ${args}
>>
>> [2] ./sbt/sbt -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive assembly/
>> assembly
>>
>> [3] ./make-distribution.sh -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive -
>> DskipTests
>>
>> [4] ./make-distribution.sh -Dyarn.version=2.3.0 -Dhadoop.version=2.3.0-cdh
>> 5.1.3 -Pyarn -Phive -DskipTests
>>
>> [5]Exception in thread "main" org.apache.hadoop.yarn.exceptions.
>> YarnRuntimeException: java.lang.reflect.InvocationTargetException
>>         at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
>> getClient(RpcClientFactoryPBImpl.java:79)
>>         at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getProxy
>> (HadoopYarnProtoRPC.java:48)
>>         at org.apache.hadoop.yarn.client.RMProxy$1.run(RMProxy.java:134)
>> ...
>> Caused by: java.lang.reflect.InvocationTargetException
>>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> Method)
>>         at sun.reflect.NativeConstructorAccessorImpl.newInstance
>> (NativeConstructorAccessorImpl.java:39)
>>         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance
>> (DelegatingConstructorAccessorImpl.java:27)
>>         at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
>>         at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
>> getClient(RpcClientFactoryPBImpl.java:76)
>> ... 27 more
>> Caused by: java.lang.VerifyError: class org.apache.hadoop.yarn.proto.
>> YarnServiceProtos$SubmitApplicationRequestProto overrides final method
>> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
>>         at java.lang.ClassLoader.defineClass1(Native Method)
>>         at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
>>
>
>
>
> --
> Marcelo
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 21:38:48 2014
Return-Path: <dev-return-10684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F09FF10F78
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 21:38:47 +0000 (UTC)
Received: (qmail 28845 invoked by uid 500); 5 Dec 2014 21:38:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28767 invoked by uid 500); 5 Dec 2014 21:38:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28745 invoked by uid 99); 5 Dec 2014 21:38:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:38:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:38:21 +0000
Received: by mail-wi0-f178.google.com with SMTP id em10so2708299wid.11
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 13:38:21 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=KuR20MRvciviCghv7PyNgvGIuG6LtS2lz4W41kHv3y4=;
        b=Nno1eoNEdX657cEjujjdi+zgCGHKEdPQEMw9ChAiY9cT1vjJX821r2RN095/+Se74a
         bmx63+w86aASJjkfhGKe5c5q8z+SodTD0G4/XMZtkZw8uT9pnOkmoeRDdvJYabuTRoVO
         U0kkT2qpFU4FcAWpzueWHoiDeaxiumnlcI4WVVFWMlfZ9RS4cUOjNZzN8quzBWwBK/cz
         b0x1sUnWZL5F7q7+UP+immDVrf/iTJrOw8rnyMIGt/1Jw/3VE9iFnAoSkcexiT30Ohyu
         /ckB/Nf0osh/frn0Mv9fWciONk6wWrZVhyhNW3b7TZxtIEJxwuPe1iTz5W5GkfQF79Gt
         VMjQ==
X-Gm-Message-State: ALoCoQlMz1NUKHOUM3JczfLwwm2EP0mQGt9s1PWwSbpuqlYUh7UhmZdqimk7INzfwEQ+tyH9RIOz
X-Received: by 10.194.191.227 with SMTP id hb3mr28086976wjc.79.1417815501006;
 Fri, 05 Dec 2014 13:38:21 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 5 Dec 2014 13:38:00 -0800 (PST)
In-Reply-To: <CAEYYnxba7SaWwgMjtc_kmiC-vdurHSyBQ4w6eMqLTtDjSYwsjA@mail.gmail.com>
References: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz> <CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
 <CAEYYnxba7SaWwgMjtc_kmiC-vdurHSyBQ4w6eMqLTtDjSYwsjA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 5 Dec 2014 15:38:00 -0600
Message-ID: <CAMAsSd+aeCMfeJsSz6h60nQHowLhk6vPTv_zj0OuNwKDL_KpEg@mail.gmail.com>
Subject: Re: Protobuf version in mvn vs sbt
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Marcelo Vanzin <vanzin@cloudera.com>, spark.dubovsky.jakub@seznam.cz, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

(Nit: CDH *5.1.x*, including 5.1.3, is derived from Hadoop 2.3.x. 5.3
is based on 2.5.x)

On Fri, Dec 5, 2014 at 3:29 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 21:50:27 2014
Return-Path: <dev-return-10685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D16EC10FFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 21:50:27 +0000 (UTC)
Received: (qmail 53310 invoked by uid 500); 5 Dec 2014 21:50:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53235 invoked by uid 500); 5 Dec 2014 21:50:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53224 invoked by uid 99); 5 Dec 2014 21:50:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:50:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:50:01 +0000
Received: by mail-qa0-f54.google.com with SMTP id i13so1043220qae.41
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 13:48:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=11WFncknaC0jkO6Hzz8PhghHv3l7rLQDFXhmmv2Pi8M=;
        b=ek6LfqtHIqIg0cbQI1YV7zRTHL3yGdRfgmQxZ/siS9D2v2N8Xnw3uaEUuefZYavxOh
         NgVKsEjVhR9UYumhKuyc5gp/IlhAdPsuD6vS01PixjDy+iqqyT7wfh4iT3OgfTvJT02L
         50+J1bEoLHJ2bM6ZjCfaD4FGb4yKOoKZITmaCPLdH3xJCIk145+EOMK/3+QchPdhKarv
         8wcWZLqtG6X5aDCthJdrAUNaw6EizbS+NVIw9+NSNEffD/UIjWIW+EYGg74rNvgTvEcM
         DhMOleiwyKfgfocEfkubTA3rChfQgX2+2h2b3u63KpVbE2cJIa3Q18vwwkXuoZTaLr0B
         Eytg==
X-Gm-Message-State: ALoCoQlZlD4Y/+9ewomL615Ihw3lGiPKMayUo+Y+dUVDFi3paJqiJBypqhNjZc/ZJy2WwB4O+KO3
MIME-Version: 1.0
X-Received: by 10.140.93.49 with SMTP id c46mr28891291qge.58.1417816109977;
 Fri, 05 Dec 2014 13:48:29 -0800 (PST)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.com
Received: by 10.229.15.72 with HTTP; Fri, 5 Dec 2014 13:48:29 -0800 (PST)
In-Reply-To: <CAMAsSd+aeCMfeJsSz6h60nQHowLhk6vPTv_zj0OuNwKDL_KpEg@mail.gmail.com>
References: <Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
	<CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
	<CAEYYnxba7SaWwgMjtc_kmiC-vdurHSyBQ4w6eMqLTtDjSYwsjA@mail.gmail.com>
	<CAMAsSd+aeCMfeJsSz6h60nQHowLhk6vPTv_zj0OuNwKDL_KpEg@mail.gmail.com>
Date: Fri, 5 Dec 2014 13:48:29 -0800
X-Google-Sender-Auth: CF1IFi1eoQ4wMK9aLB82Kv1Rv64
Message-ID: <CAEYYnxZLGtU5sdm8eMtaxZkGBxkxVJRSTeAsDW9_dZwbaYBtxA@mail.gmail.com>
Subject: Re: Protobuf version in mvn vs sbt
From: DB Tsai <dbtsai@dbtsai.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Marcelo Vanzin <vanzin@cloudera.com>, spark.dubovsky.jakub@seznam.cz, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a4d2297a19a05097f09ab
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a4d2297a19a05097f09ab
Content-Type: text/plain; charset=UTF-8

oh, I meant to say cdh5.1.3 used by Jakub's company is based on 2.3. You
can see it from the first part of the Cloudera's version number - "2.3.0-cdh
5.1.3".


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

On Fri, Dec 5, 2014 at 1:38 PM, Sean Owen <sowen@cloudera.com> wrote:

> (Nit: CDH *5.1.x*, including 5.1.3, is derived from Hadoop 2.3.x. 5.3
> is based on 2.5.x)
>
> On Fri, Dec 5, 2014 at 3:29 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> > As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try
>

--001a113a4d2297a19a05097f09ab--

From dev-return-10686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 21:52:36 2014
Return-Path: <dev-return-10686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4ADEBC039
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 21:52:36 +0000 (UTC)
Received: (qmail 58538 invoked by uid 500); 5 Dec 2014 21:52:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58469 invoked by uid 500); 5 Dec 2014 21:52:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58445 invoked by uid 99); 5 Dec 2014 21:52:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:52:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of judynash@exchange.microsoft.com designates 157.55.158.30 as permitted sender)
Received: from [157.55.158.30] (HELO na01-sn2-obe.outbound.o365filtering.com) (157.55.158.30)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 21:52:26 +0000
Received: from BY2SR01CA103.namsdf01.sdf.exchangelabs.com (10.255.93.148) by
 BL2SR01MB605.namsdf01.sdf.exchangelabs.com (10.255.109.167) with Microsoft
 SMTP Server (TLS) id 15.1.49.2; Fri, 5 Dec 2014 21:52:03 +0000
Received: from SN2FFOFD002.ffo.gbl (2a01:111:f400:7c04::26) by
 BY2SR01CA103.outlook.office365.com (2a01:111:e400:2c01::20) with Microsoft
 SMTP Server (TLS) id 15.1.49.2 via Frontend Transport; Fri, 5 Dec 2014
 21:52:03 +0000
Received: from hybrid.exchange.microsoft.com (131.107.159.99) by
 SN2FFOFD002.mail.o365filtering.com (10.111.201.21) with Microsoft SMTP Server
 (TLS) id 15.1.36.5 via Frontend Transport; Fri, 5 Dec 2014 21:52:03 +0000
Received: from DFM-TK5MBX15-05.exchange.corp.microsoft.com (157.54.109.44) by
 DFM-TK5EDG15-01.exchange.corp.microsoft.com (157.54.27.96) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Fri, 5 Dec 2014 21:51:57 +0000
Received: from DFM-DB3MBX15-06.exchange.corp.microsoft.com (10.221.24.76) by
 DFM-TK5MBX15-05.exchange.corp.microsoft.com (157.54.109.44) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Fri, 5 Dec 2014 13:51:56 -0800
Received: from DFM-DB3MBX15-08.exchange.corp.microsoft.com (10.221.24.69) by
 DFM-DB3MBX15-06.exchange.corp.microsoft.com (10.221.24.76) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Fri, 5 Dec 2014 13:51:54 -0800
Received: from DFM-DB3MBX15-08.exchange.corp.microsoft.com ([169.254.12.242])
 by DFM-DB3MBX15-08.exchange.corp.microsoft.com ([169.254.12.242]) with mapi
 id 15.00.1044.021; Fri, 5 Dec 2014 13:51:54 -0800
From: Judy Nash <judynash@exchange.microsoft.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: build in IntelliJ IDEA
Thread-Topic: build in IntelliJ IDEA
Thread-Index: AdAQ1KT6iWdYfqaZQGWM3WexXhvDfg==
Date: Fri, 5 Dec 2014 21:51:54 +0000
Message-ID: <e3af038200f640b8a745e88c63d544b7@DFM-DB3MBX15-08.exchange.corp.microsoft.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [157.59.235.233]
Content-Type: multipart/alternative;
	boundary="_000_e3af038200f640b8a745e88c63d544b7DFMDB3MBX1508exchangeco_"
MIME-Version: 1.0
X-EOPAttributedMessage: 0
Received-SPF: SoftFail (protection.outlook.com: domain of transitioning
 exchange.microsoft.com discourages use of 131.107.159.99 as permitted sender)
Authentication-Results: spf=softfail (sender IP is 131.107.159.99)
 smtp.mailfrom=judynash@exchange.microsoft.com; 
X-Forefront-Antispam-Report:
	CIP:131.107.159.99;IPV:NLI;EFV:NLI;SFV:NSPM;SFS:(10019020)(164054003)(53754006)(189002)(199003)(57704003)(97736003)(2501002)(512954002)(84326002)(92726002)(107046002)(92566001)(229853001)(120916001)(19625215002)(77156002)(450100001)(62966003)(99396003)(16236675004)(68736005)(102836002)(15975445007)(19300405004)(84676001)(2656002)(87936001)(31966008)(2351001)(19580395003)(107886001)(6806004)(106466001)(4396001)(33646002)(46102003)(21056001)(108616004)(110136001)(19617315012)(105596002)(50986999)(54356999)(20776003)(66066001)(71186001)(64706001)(24736002);DIR:OUT;SFP:1102;SCL:1;SRVR:BL2SR01MB605;H:hybrid.exchange.microsoft.com;FPR:;SPF:SoftFail;PTR:InfoDomainNonexistent;MX:1;A:1;LANG:en;
X-Microsoft-Antispam: UriScan:;
X-Microsoft-Antispam: BCL:0;PCL:0;RULEID:;SRVR:BL2SR01MB605;
X-DmarcStatus: Failed
X-Exchange-Antispam-Report-Test: UriScan:;
X-Exchange-Antispam-Report-CFA-Test:
	BCL:0;PCL:0;RULEID:(4002003);SRVR:BL2SR01MB605;
X-Forefront-PRVS: 04163EF38A
X-Exchange-Antispam-Report-CFA-Test: BCL:0;PCL:0;RULEID:;SRVR:BL2SR01MB605;
X-OriginatorOrg: exchange.microsoft.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 05 Dec 2014 21:52:03.3410
 (UTC)
X-MS-Exchange-CrossTenant-Id: f686d426-8d16-42db-81b7-ab578e110ccd
X-MS-Exchange-CrossTenant-FromEntityHeader: HybridOnPrem
X-MS-Exchange-Transport-CrossTenantHeadersStamped: BL2SR01MB605
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_e3af038200f640b8a745e88c63d544b7DFMDB3MBX1508exchangeco_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi everyone,

Have a newbie question on using IntelliJ to build and debug.

I followed this wiki to setup IntelliJ:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#Us=
efulDeveloperTools-BuildingSparkinIntelliJIDEA

Afterward I tried to build via Toolbar (Build > Rebuild Project).
The action fails with the error message:
Cannot start compiler: the SDK is not specified.

What SDK do I need to specify to get the build working?

Thanks,
Judy

--_000_e3af038200f640b8a745e88c63d544b7DFMDB3MBX1508exchangeco_--

From dev-return-10687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 22:04:05 2014
Return-Path: <dev-return-10687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A89F3C0DB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 22:04:05 +0000 (UTC)
Received: (qmail 3919 invoked by uid 500); 5 Dec 2014 22:04:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3839 invoked by uid 500); 5 Dec 2014 22:04:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3798 invoked by uid 99); 5 Dec 2014 22:04:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 22:04:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 22:03:35 +0000
Received: by mail-ob0-f173.google.com with SMTP id uy5so1251720obc.32
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 14:02:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8NVAQrbTHx9HjMH+4YSoko5mxzAEc2k43RKxb1IKQxo=;
        b=xHv3OMdXHmLWGgB+BCIkKQLt8RB17qGFyii/xbUMk17iukyqJzIaHX6qpNPniGLe+g
         QLNuaoy4YWB9vbkqWvF7sLbUn1FM/6RsuCjTYhnGAeTCkDPYATxsZAk3qNBZz9z431in
         ZqpKmyOlnv253Zuyeti4fJ1vWvJk6e8+IGHk7iXK80+JD+x6oCNEfMumsQuD6PWCeKyl
         7zs9SlcJa+mVAEW++Xrm24AjuIXM9WUH0fZPdBEKKOuUjeJYiQ1UP2kOhbEBFoz9x0lq
         ykJY34I8fAT3U4sDpQ/NTyBHgv3SSwg87x6hVfjyog/j0eylDFm6C/zCjcMWPgH16InG
         fdSA==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr112076oie.78.1417816969683;
 Fri, 05 Dec 2014 14:02:49 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Fri, 5 Dec 2014 14:02:49 -0800 (PST)
In-Reply-To: <CACVFzXDoZi2oS5hKqghWq9TTq=HUvuzBrP0ZhM46uh2=snfJ_Q@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<CACVFzXDoZi2oS5hKqghWq9TTq=HUvuzBrP0ZhM46uh2=snfJ_Q@mail.gmail.com>
Date: Fri, 5 Dec 2014 14:02:49 -0800
Message-ID: <CABPQxst=L=pLUyAmtB7gr0q0LhvtASmt05BWH1xn=o2bN7c+cA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Takeshi Yamamuro <linguin.m.s@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Thanks all for the continued testing!

The issue I mentioned earlier SPARK-4498 was fixed earlier this week
(hat tip to Mark Hamstra who contributed to fix).

In the interim a few smaller blocker-level issues with Spark SQL were
found and fixed (SPARK-4753, SPARK-4552, SPARK-4761).

There is currently an outstanding issue (SPARK-4740[1]) in Spark core
that needs to be fixed.

I want to thank in particular Shopify and Intel China who have
identified and helped test blocker issues with the release. This type
of workload testing around releases is really helpful for us.

Once things stabilize I will cut RC2. I think we're pretty close with this one.

- Patrick

On Wed, Dec 3, 2014 at 5:38 PM, Takeshi Yamamuro <linguin.m.s@gmail.com> wrote:
> +1 (non-binding)
>
> Checked on CentOS 6.5, compiled from the source.
> Ran various examples in stand-alone master and three slaves, and
> browsed the web UI.
>
> On Sat, Nov 29, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.2.0!
>>
>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1048/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.2.0!
>>
>> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.1.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening very late into the QA period compared with
>> previous votes, so -1 votes should only occur for significant
>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>> regressions, or bugs related to new features will not block this
>> release.
>>
>> == What default changes should I be aware of? ==
>> 1. The default value of "spark.shuffle.blockTransferService" has been
>> changed to "netty"
>> --> Old behavior can be restored by switching to "nio"
>>
>> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
>> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>> "hash".
>>
>> == Other notes ==
>> Because this vote is occurring over a weekend, I will likely extend
>> the vote if this RC survives until the end of the vote period.
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec  5 23:21:47 2014
Return-Path: <dev-return-10688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E8A6BC537
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  5 Dec 2014 23:21:46 +0000 (UTC)
Received: (qmail 81685 invoked by uid 500); 5 Dec 2014 23:21:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81610 invoked by uid 500); 5 Dec 2014 23:21:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81597 invoked by uid 99); 5 Dec 2014 23:21:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 23:21:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.174 as permitted sender)
Received: from [209.85.192.174] (HELO mail-pd0-f174.google.com) (209.85.192.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 05 Dec 2014 23:21:18 +0000
Received: by mail-pd0-f174.google.com with SMTP id fp1so1246955pdb.5
        for <dev@spark.apache.org>; Fri, 05 Dec 2014 15:21:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=sXhdBahOu2sy7RdtpZIWdPMonY7O0I+E23w3eQ4KaxI=;
        b=E7SllUHOGyWRICd+rr3YK3Gee2zfQ9pn/6KYliR9ILPEXMgz/fCFNalsmKhckjbbxp
         jbMqnVoJ2Hk/TNPzh7ynUBLf0TltsKGbVeGXrFLjxo+DrGDdBIL1egeZr1NyPi8+w2tk
         h5x1sDAqmYNdaJH47kLP+fkeeJbtElvKEuTeAvAwmO2Wo9QY1hju5XMbp/Zmp06yWO7T
         LKZiatIPosj04O4pNTrkrG+Pg5AYSO3GjUEvjikferOYz0OtSEJJLZEMec2Es9/wyr+v
         NGwCiNyoBEmuopixtO7NqBNP3rB/ies3zAsVI8zV4i0+Sw+GdSG5AeCsN9yiwyEF1/Qp
         m5lA==
X-Received: by 10.70.37.4 with SMTP id u4mr19349964pdj.3.1417821677597;
        Fri, 05 Dec 2014 15:21:17 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id hk9sm19855122pdb.47.2014.12.05.15.21.16
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 05 Dec 2014 15:21:16 -0800 (PST)
Date: Fri, 5 Dec 2014 15:21:14 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Judy Nash <judynash@exchange.microsoft.com>, 
 "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.54823deb.643c9869.3214@joshs-mbp>
In-Reply-To: <e3af038200f640b8a745e88c63d544b7@DFM-DB3MBX15-08.exchange.corp.microsoft.com>
References: <e3af038200f640b8a745e88c63d544b7@DFM-DB3MBX15-08.exchange.corp.microsoft.com>
Subject: Re: build in IntelliJ IDEA
X-Mailer: Airmail (280)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54823deb_66334873_3214"
X-Virus-Checked: Checked by ClamAV on apache.org

--54823deb_66334873_3214
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

If you go to =E2=80=9C=46ile -> Project Structure=E2=80=9D and click on =E2=
=80=9CProject=E2=80=9D under the =E2=80=9CProject settings=E2=80=9D headi=
ng, do you see an entry for =E2=80=9CProject SDK=3F=E2=80=9D =C2=A0If not=
, you should click =E2=80=9CNew=E2=80=A6=E2=80=9D and configure a JDK; by=
 default, I think IntelliJ should figure out a correct path to your syste=
m JDK, so you should just be able to hit =E2=80=9COk=E2=80=9D then rebuil=
d your project. =C2=A0 =46or reference, here=E2=80=99s a screenshot showi=
ng what my version of that window looks like:=C2=A0http://i.imgur.com/hRf=
QjIi.png


On December 5, 2014 at 1:52:35 PM, Judy Nash (judynash=40exchange.microso=
ft.com) wrote:
Hi everyone, =20

Have a newbie question on using IntelliJ to build and debug. =20

I followed this wiki to setup IntelliJ: =20
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools=23=
UsefulDeveloperTools-BuildingSparkinIntelliJIDEA =20

Afterward I tried to build via Toolbar (Build > Rebuild Project). =20
The action fails with the error message: =20
Cannot start compiler: the SDK is not specified. =20

What SDK do I need to specify to get the build working=3F =20

Thanks, =20
Judy =20

--54823deb_66334873_3214--


From dev-return-10689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec  6 00:32:34 2014
Return-Path: <dev-return-10689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A36AC7B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  6 Dec 2014 00:32:34 +0000 (UTC)
Received: (qmail 22688 invoked by uid 500); 6 Dec 2014 00:32:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22611 invoked by uid 500); 6 Dec 2014 00:32:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21331 invoked by uid 99); 6 Dec 2014 00:32:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 00:32:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 00:32:05 +0000
Received: by mail-la0-f44.google.com with SMTP id gd6so1058861lab.17
        for <multiple recipients>; Fri, 05 Dec 2014 16:29:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=UpI8Bm0Eonj32dBucYWk0wlAFb8iTirYYOP8UDTPxKQ=;
        b=QbqzjZjVEXMt9Lb8/mi/4CySDUfW3Z0dMN+pzPu8YNdUgUT+xpIZd0OG9fr5lTJwDL
         tn+9GqJql1Z2nNaIqLUFVLpiETouCSkepwW2DUK0L68IIFys45cRP9i/y0ZlBo3H/SyH
         4FOdXj6j/ZXzY8VP/WHQZTdRG42LjLJPBBYDhLVPNPZjdVImYBs0cjvgBpxPZ8RGZJ3z
         zw6fjtLNPtevhhXc+elM1+fLPyNIwbS0lz5jSemeu5ROO2jOspprHXRrg1sxBOL3gHQT
         w3xMUH1+ha8IePhhipZkyswXCsAczJKIyFHo1CJc9IQxb9+dfkY03pUIBbXPPqw6mu+o
         EsUA==
X-Received: by 10.112.234.201 with SMTP id ug9mr5299649lbc.79.1417825789403;
 Fri, 05 Dec 2014 16:29:49 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Fri, 5 Dec 2014 16:29:29 -0800 (PST)
In-Reply-To: <CAAsvFPmP2wC0CL5KcKfSMya8TBrdv0z7xJVCy0Jvz+D-ePPHow@mail.gmail.com>
References: <CACA1tWLH0VZr_aJVtVp0__EaCLeUbW0XugUxXS8chmfpBJRfMA@mail.gmail.com>
 <CAAswR-5aG=bPGX1wQeeL2F+eAdGNdwrQ42npbBo_JNTyPOZ4BQ@mail.gmail.com> <CAAsvFPmP2wC0CL5KcKfSMya8TBrdv0z7xJVCy0Jvz+D-ePPHow@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Sat, 6 Dec 2014 08:29:29 +0800
Message-ID: <CACA1tW+LxRfopZPU8kPVwqYTO05MLFV-ZUv70SsqfexXcYRAMQ@mail.gmail.com>
Subject: Re: drop table if exists throws exception
To: Mark Hamstra <mark@clearstorydata.com>
Cc: Michael Armbrust <michael@databricks.com>, user <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3c83287ea820509814ae3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3c83287ea820509814ae3
Content-Type: text/plain; charset=UTF-8

I see. The resulting SchemaRDD is returned so like Michael said, the
exception does not propogate to user code.

However printing out the following log is confusing :)

scala> sql("drop table if exists abc")
14/12/05 16:27:02 INFO ParseDriver: Parsing command: drop table if exists
abc
14/12/05 16:27:02 INFO ParseDriver: Parse Completed
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=Driver.run
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=TimeToSubmit
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Concurrency mode is disabled, not creating a
lock manager
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=compile
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=parse
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO ParseDriver: Parsing command: DROP TABLE IF EXISTS
abc
14/12/05 16:27:02 INFO ParseDriver: Parse Completed
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=parse
start=1417825622650 end=1417825622650 duration=0
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=semanticAnalyze
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO HiveMetaStore: 0: get_table : db=default tbl=abc
14/12/05 16:27:02 INFO audit: ugi=jianshuang    ip=unknown-ip-addr
 cmd=get_table : db=default tbl=abc
14/12/05 16:27:02 INFO Driver: Semantic Analysis Completed
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=semanticAnalyze
start=1417825622650 end=1417825622653 duration=3
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Returning Hive schema:
Schema(fieldSchemas:null, properties:null)
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=compile
start=1417825622650 end=1417825622654 duration=4
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=Driver.execute
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Starting command: DROP TABLE IF EXISTS abc
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=TimeToSubmit
start=1417825622650 end=1417825622654 duration=4
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=runTasks
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO HiveMetaStore: 0: get_table : db=default tbl=abc
14/12/05 16:27:02 INFO audit: ugi=jianshuang    ip=unknown-ip-addr
 cmd=get_table : db=default tbl=abc
14/12/05 16:27:02 ERROR Hive: NoSuchObjectException(message:default.abc
table not found)
        at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1560)
        at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)

On Sat, Dec 6, 2014 at 3:45 AM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> And that is no different from how Hive has worked for a long time.
>
> On Fri, Dec 5, 2014 at 11:42 AM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> The command run fine for me on master.  Note that Hive does print an
>> exception in the logs, but that exception does not propogate to user code.
>>
>> On Thu, Dec 4, 2014 at 11:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>> > Hi,
>> >
>> > I got exception saying Hive: NoSuchObjectException(message:<table> table
>> > not found)
>> >
>> > when running "DROP TABLE IF EXISTS <table>"
>> >
>> > Looks like a new regression in Hive module.
>> >
>> > Anyone can confirm this?
>> >
>> > Thanks,
>> > --
>> > Jianshi Huang
>> >
>> > LinkedIn: jianshi
>> > Twitter: @jshuang
>> > Github & Blog: http://huangjs.github.com/
>> >
>>
>
>


-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c3c83287ea820509814ae3--

From dev-return-10690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec  6 10:41:13 2014
Return-Path: <dev-return-10690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45B0610537
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  6 Dec 2014 10:41:12 +0000 (UTC)
Received: (qmail 57130 invoked by uid 500); 6 Dec 2014 10:41:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57060 invoked by uid 500); 6 Dec 2014 10:41:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57048 invoked by uid 99); 6 Dec 2014 10:41:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 10:41:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of linguin.m.s@gmail.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 10:40:44 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so1503666qaq.24
        for <dev@spark.apache.org>; Sat, 06 Dec 2014 02:40:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Q2AdQPZhdpncTSYUeTOy5qoSPMlE6YzrmpjiNPT959s=;
        b=kLKRwUUspTDLCmS/5nESAgVziMJzrE68gmis4+aXCbk8FJ/Lahp7kuCeTXSxYOpY82
         95990002XaOB2rET4/bTaqDva8Uu2nLc8ftwbxLqIdNcwIW4ctPPSG/jOcTZgIHHBjUF
         pJ0eJFbpwySR+0qHGBznjzjV5Ay+QWE/Azd3cswcrF4GYD3cyIcjccjeuGDwGjtOuc1r
         OKV+2jGq3a/rgMuHlI9qsdTLMxIYrKx1v7L6nfW+oftQtrh6S2JhxyCJeg1qKiVA4Rz8
         +WMnaaM+ODjflRw0GbksdosAU64Afqr0A7C9Uyd3ejJeLctD4wuo+D8hu1yj9VxJFxv0
         Ksmw==
MIME-Version: 1.0
X-Received: by 10.224.4.8 with SMTP id 8mr34384185qap.77.1417862442579; Sat,
 06 Dec 2014 02:40:42 -0800 (PST)
Received: by 10.229.56.71 with HTTP; Sat, 6 Dec 2014 02:40:42 -0800 (PST)
In-Reply-To: <E5BFC5AC-7208-48CE-AAC7-F963EF2AF791@alibaba-inc.com>
References: <E5BFC5AC-7208-48CE-AAC7-F963EF2AF791@alibaba-inc.com>
Date: Sat, 6 Dec 2014 19:40:42 +0900
Message-ID: <CACVFzXDtba8QGCaw5ydg99RU0p4sGK1WxWQPCut-TJzZw3M8gw@mail.gmail.com>
Subject: Re: a question of Graph build api
From: Takeshi Yamamuro <linguin.m.s@gmail.com>
To: "jinkui.sjk" <jinkui.sjk@alibaba-inc.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2be423afbc5050989d352
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2be423afbc5050989d352
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

Yes, I think so.
However, EdgePartitionBuilder might have edge lists as other types instead
of Edge in a future.
This is because the change can make fast graph construction.
See also SPARK-1987.
https://issues.apache.org/jira/browse/SPARK-1987

Thanks,
takeshi


On Fri, Dec 5, 2014 at 12:12 AM, jinkui.sjk <jinkui.sjk@alibaba-inc.com>
wrote:

> hi, all
>
> where build a graph from edge tuples with api  Graph.fromEdgeTuples,
> the edges object type is RDD[Edge], inside of  EdgeRDD.fromEdge,
>  EdgePartitionBuilder.add func=E2=80=99s param is better to be Edge objec=
t.
> no need to create a new Edge object again.
>
>
>
>   def fromEdgeTuples[VD: ClassTag](
>       rawEdges: RDD[(VertexId, VertexId)],
>       defaultValue: VD,
>       uniqueEdges: Option[PartitionStrategy] =3D None,
>       edgeStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY,
>       vertexStorageLevel: StorageLevel =3D StorageLevel.MEMORY_ONLY):
> Graph[VD, Int] =3D
>   {
> *    val edges =3D rawEdges.map(p =3D> Edge(p._1, p._2, 1))*
>     val graph =3D GraphImpl(edges, defaultValue, edgeStorageLevel,
> vertexStorageLevel)
>     uniqueEdges match {
>       case Some(p) =3D> graph.partitionBy(p).groupEdges((a, b) =3D> a + b=
)
>       case None =3D> graph
>     }
>   }
>
>
>
>
>   object GraphImpl {
>
>   /** Create a graph from edges, setting referenced vertices to
> `defaultVertexAttr`. */
>   def apply[VD: ClassTag, ED: ClassTag](
>       edges: RDD[Edge[ED]],
>       defaultVertexAttr: VD,
>       edgeStorageLevel: StorageLevel,
>       vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] =3D {
>     fromEdgeRDD(*EdgeRDD.fromEdges(edges)*, defaultVertexAttr,
> edgeStorageLevel, vertexStorageLevel)
>   }
>
>
>
>   object EdgeRDD {
>   /**
>    * Creates an EdgeRDD from a set of edges.
>    *
>    * @tparam ED the edge attribute type
>    * @tparam VD the type of the vertex attributes that may be joined with
> the returned EdgeRDD
>    */
>   def fromEdges[ED: ClassTag, VD: ClassTag](edges: RDD[Edge[ED]]):
> EdgeRDD[ED, VD] =3D {
>     val edgePartitions =3D edges.mapPartitionsWithIndex { (pid, iter) =3D=
>
>       val builder =3D new EdgePartitionBuilder[ED, VD]
>       iter.foreach { e =3D>
> *        builder.add(e.srcId, e.dstId, e.attr)*
>       }
>       Iterator((pid, builder.toEdgePartition))
>     }
>     EdgeRDD.fromEdgePartitions(edgePartitions)
>   }
>
>
>

--001a11c2be423afbc5050989d352--

From dev-return-10691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec  6 13:47:49 2014
Return-Path: <dev-return-10691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11F2E10966
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  6 Dec 2014 13:47:48 +0000 (UTC)
Received: (qmail 13625 invoked by uid 500); 6 Dec 2014 13:47:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13552 invoked by uid 500); 6 Dec 2014 13:47:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13531 invoked by uid 99); 6 Dec 2014 13:47:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 13:47:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.43 as permitted sender)
Received: from [77.75.72.43] (HELO smtp1.seznam.cz) (77.75.72.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 13:47:19 +0000
Received: from email.seznam.cz
	by email-smtpc4b.go.seznam.cz (email-smtpc4b.go.seznam.cz [192.168.92.39])
	id 2249b581a550534721d1b0c3;
	Sat, 06 Dec 2014 14:46:16 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1417873576; bh=/K3IK2UJcAaJcbHAMpgpgFdvbvbVMY/d7gseINIvbVI=;
	h=Received:From:To:Cc:Subject:Date:Message-Id:References:
	 Mime-Version:X-Mailer:Content-Type;
	b=POiC/X/qnTp8gE/8HFgrgzNryjTFqBofKziFHDe2JXxL8lE2/KfbZy1bpfb0RzjJv
	 x1LKAbKYEn61FJQkI93F7gp/VbZjGLTj4quEgnxwnKIRqo49UqlczBL2MU7ReY74qG
	 DyUubOm3aU6Iwpzr4Kfj4ZRqa8x56Hp/onjKiNTY=
Received: from nat.2-52-prg.avast.com (nat.2-52-prg.avast.com [91.213.143.252])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Sat, 06 Dec 2014 14:46:16 +0100 (CET)
From: <spark.dubovsky.jakub@seznam.cz>
To: <dbtsai@dbtsai.com>
Cc: =?utf-8?q?dev=40spark=2Eapache=2Eorg?= <dev@spark.apache.org>,
	"Marcelo Vanzin" <vanzin@cloudera.com>
Subject: Re: Protobuf version in mvn vs sbt
Date: Sat, 06 Dec 2014 14:46:15 +0100 (CET)
Message-Id: <OG1.3c0AJ.1EZk459A0gY.1KWmYd@seznam.cz>
References: <CAAOnQ7v2tyr6BGZzSSKGVUOQU+Au2tnYXNg56e1GPwwioV3jng@mail.gmail.com>
	<Lhp.3c08L.spDiK0bKPE.1KWXhK@seznam.cz>
	<CAEYYnxba7SaWwgMjtc_kmiC-vdurHSyBQ4w6eMqLTtDjSYwsjA@mail.gmail.com>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_13b80ea90f5fa91e2419e542=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_13b80ea90f5fa91e2419e542=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi,=0A=
=0A=
=C2=A0 I have created assembly with additional hadoop-2.3 profile and subm=
it is =0A=
smooth now.=0A=
=0A=
=C2=A0 Thank you for quick reply!=0A=
=0A=
=C2=A0 Now I can move to another maybe dev related problem. Posted to user=
 =0A=
mailinglist under "Including data nucleus tools".=0A=
=0A=
=C2=A0 Jakub=0A=
=0A=
=0A=
---------- P=C5=AFvodn=C3=AD zpr=C3=A1va ----------=0A=
Od: DB Tsai <dbtsai@dbtsai.com>=0A=
Komu: Marcelo Vanzin <vanzin@cloudera.com>=0A=
Datum: 5. 12. 2014 22:31:13=0A=
P=C5=99edm=C4=9Bt: Re: Protobuf version in mvn vs sbt=0A=
=0A=
"As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try=0A=
=0A=
./make-distribution.sh -Pyarn -Phive -Phadoop-2.3=0A=
-Dhadoop.version=3D2.3.0-cdh5.1.3 -DskipTests=0A=
=0A=
See the detail of how to change the profile at=0A=
https://spark.apache.org/docs/latest/building-with-maven.html=0A=
=0A=
Sincerely,=0A=
=0A=
DB Tsai=0A=
-------------------------------------------------------=0A=
My Blog: https://www.dbtsai.com=0A=
LinkedIn: https://www.linkedin.com/in/dbtsai=0A=
=0A=
=0A=
On Fri, Dec 5, 2014 at 12:54 PM, Marcelo Vanzin <vanzin@cloudera.com> wrot=
e:=0A=
> When building against Hadoop 2.x, you need to enable the appropriate=
=0A=
> profile, aside from just specifying the version. e.g. "-Phadoop-2.3"=
=0A=
> for Hadoop 2.3.=0A=
>=0A=
> On Fri, Dec 5, 2014 at 12:51 PM, <spark.dubovsky.jakub@seznam.cz> wrote:=
=0A=
>> Hi devs,=0A=
>>=0A=
>> I play with your amazing Spark here in Prague for some time. I have=
=0A=
>> stumbled on a thing which I like to ask about. I create assembly jars =
=0A=
from=0A=
>> source and then use it to run simple jobs on our 2.3.0-cdh5.1.3 cluster=
=0A=
>> using yarn. Example of my usage [1]. Formerly I had started to use sbt =
=0A=
for=0A=
>> creating assemblies like this [2] which runs just fine. Then reading =
=0A=
those=0A=
>> maven-prefered stories here on dev list I found make-distribution.sh =
=0A=
script=0A=
>> in root of codebase and wanted to give it a try. I used it to create=
=0A=
>> assembly by both [3] and [4].=0A=
>>=0A=
>> But I am not able to use assemblies created by make-distribution becaus=
e=0A=
>> it refuses to be submited to cluster. Here is what happens:=0A=
>> - run [3] or [4]=0A=
>> - recompile app agains new assembly=0A=
>> - submit job using new assembly by [1] like command=0A=
>> - submit fails with important parts of stack trace being [5]=0A=
>>=0A=
>> My guess is that it is due to improper version of protobuf included in=
=0A=
>> assembly jar. My questions are:=0A=
>> - Can you confirm this hypothesis?=0A=
>> - What is the difference between sbt and mvn way of creating assembly? =
I=0A=
>> mean sbt works and mvn not...=0A=
>> - What additional option I need to pass to make-distribution to make it=
=0A=
>> work?=0A=
>>=0A=
>> Any help/explanation here would be appreciated=0A=
>>=0A=
>> Jakub=0A=
>> ----------------------=0A=
>> [1] ./bin/spark-submit --num-executors 200 --master yarn-cluster --conf=
=0A=
>> spark.yarn.jar=3Dassembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSH=
OT-=0A=
>> hadoop2.3.0-cdh5.1.3.jar --class org.apache.spark.mllib.=0A=
>> CreateGuidDomainDictionary root-0.1.jar ${args}=0A=
>>=0A=
>> [2] ./sbt/sbt -Dhadoop.version=3D2.3.0-cdh5.1.3 -Pyarn -Phive assembly/=
=0A=
>> assembly=0A=
>>=0A=
>> [3] ./make-distribution.sh -Dhadoop.version=3D2.3.0-cdh5.1.3 -Pyarn -Ph=
ive =0A=
-=0A=
>> DskipTests=0A=
>>=0A=
>> [4] ./make-distribution.sh -Dyarn.version=3D2.3.0 -Dhadoop.version=3D2.=
3.0-=0A=
cdh=0A=
>> 5.1.3 -Pyarn -Phive -DskipTests=0A=
>>=0A=
>> [5]Exception in thread "main" org.apache.hadoop.yarn.exceptions.=0A=
>> YarnRuntimeException: java.lang.reflect.InvocationTargetException=0A=
>> at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.=
=0A=
>> getClient(RpcClientFactoryPBImpl.java:79)=0A=
>> at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getProxy=0A=
>> (HadoopYarnProtoRPC.java:48)=0A=
>> at org.apache.hadoop.yarn.client.RMProxy$1.run(RMProxy.java:134)=0A=
>> ...=0A=
>> Caused by: java.lang.reflect.InvocationTargetException=0A=
>> at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native=0A=
>> Method)=0A=
>> at sun.reflect.NativeConstructorAccessorImpl.newInstance=0A=
>> (NativeConstructorAccessorImpl.java:39)=0A=
>> at sun.reflect.DelegatingConstructorAccessorImpl.newInstance=0A=
>> (DelegatingConstructorAccessorImpl.java:27)=0A=
>> at java.lang.reflect.Constructor.newInstance(Constructor.java:513)=
=0A=
>> at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.=
=0A=
>> getClient(RpcClientFactoryPBImpl.java:76)=0A=
>> ... 27 more=0A=
>> Caused by: java.lang.VerifyError: class org.apache.hadoop.yarn.proto.=
=0A=
>> YarnServiceProtos$SubmitApplicationRequestProto overrides final method=
=0A=
>> getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;=0A=
>> at java.lang.ClassLoader.defineClass1(Native Method)=0A=
>> at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)=0A=
>>=0A=
>=0A=
>=0A=
>=0A=
> --=0A=
> Marcelo=0A=
>=0A=
> ---------------------------------------------------------------------=
=0A=
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org=0A=
> For additional commands, e-mail: dev-help@spark.apache.org=0A=
>=0A=
=0A=
---------------------------------------------------------------------=
=0A=
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org=0A=
For additional commands, e-mail: dev-help@spark.apache.org"=
--=_13b80ea90f5fa91e2419e542=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-10692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec  6 18:54:59 2014
Return-Path: <dev-return-10692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCECB100C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  6 Dec 2014 18:54:58 +0000 (UTC)
Received: (qmail 11360 invoked by uid 500); 6 Dec 2014 18:54:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11284 invoked by uid 500); 6 Dec 2014 18:54:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11272 invoked by uid 99); 6 Dec 2014 18:54:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 18:54:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 18:54:31 +0000
Received: by mail-ig0-f175.google.com with SMTP id h15so906852igd.14
        for <dev@spark.apache.org>; Sat, 06 Dec 2014 10:54:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=zs3azCfnCqETCuYlcuy+KXASpH/+W2uPotmeHfPkDsc=;
        b=rJp0tRQRhSgpLPL1mRnEykHvyKU39RfBaoG1T03BUnCOVLDxyFHak/BCIuolSaESvQ
         sSN8DrVl9qHQX1JgiZg5N4QUMDg6Ju1trycSvWQac7WEDwEgCVbQ2TZdOpKL9H+A7KbA
         ziiCBRE4LHZZZ8OpF1TCmY/CtxCWzcG+eUUOWFD9tkeCK8Oj4LXo6MKV2jd5FlxXVhF6
         3EsgJYowOgMkrIoM/sPc2lewy8uiNyuKuTu5MHfA3kAmPWNrOI8eEf5Y/vbX1RJk159R
         eLup46F07HodtdgYh4OL1rL4JWgTcsbai/DwPKA0Y0CpBLCw/ubYLvSE2IlvfPTt8oIJ
         EpFg==
X-Received: by 10.50.29.3 with SMTP id f3mr8041473igh.23.1417892070204; Sat,
 06 Dec 2014 10:54:30 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
 <CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
 <CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
 <CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
 <CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com>
 <CAOhmDzdLANy8adD4y02isgh5UVDCr4tee_Wz1ecO=gTY4Mr+sw@mail.gmail.com>
 <CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
 <CALte62w1RbC4zKDoWnz6esrjk=ZpsjY6dPcVmOcTxMW3Jb6VxA@mail.gmail.com> <CAMJOb8knFj6iEPpT6ietTKhKQMAxPbpcaE_aGv=41nCgKH9uKw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 06 Dec 2014 18:54:29 +0000
Message-ID: <CAOhmDzdzanx-4CO45B7_80WPc0LhQxAm5zQuD_KaowXJUZPd9A@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
To: Andrew Or <andrew@databricks.com>, Ted Yu <yuzhihong@gmail.com>
Cc: Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd74b542caa6c050990b932
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74b542caa6c050990b932
Content-Type: text/plain; charset=UTF-8

Ted,

I posted some updates
<https://issues.apache.org/jira/browse/SPARK-3431?focusedCommentId=14236540&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14236540>
on
JIRA on my progress (or lack thereof) getting SBT to parallelize test
suites properly. I'm currently stuck with SBT / ScalaTest, so I may move on
to trying Maven.

Andrew,

Once we have a basic grasp of how to parallelize some of the tests, the
next step will probably be to use containers (i.e. Docker) to allow more
parallelization, especially for those tests that, for example, contend for
ports.

Nick

On Fri Dec 05 2014 at 2:05:29 PM Andrew Or <andrew@databricks.com> wrote:

> @Patrick and Josh actually we went even further than that. We simply
> disable the UI for most tests and these used to be the single largest
> source of port conflict.
>

--047d7bd74b542caa6c050990b932--

From dev-return-10693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec  6 18:57:38 2014
Return-Path: <dev-return-10693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A495100CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  6 Dec 2014 18:57:38 +0000 (UTC)
Received: (qmail 15392 invoked by uid 500); 6 Dec 2014 18:57:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15313 invoked by uid 500); 6 Dec 2014 18:57:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15300 invoked by uid 99); 6 Dec 2014 18:57:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 18:57:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 06 Dec 2014 18:57:10 +0000
Received: by mail-yh0-f41.google.com with SMTP id a41so1269572yho.28
        for <dev@spark.apache.org>; Sat, 06 Dec 2014 10:57:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lh/AIJYvI9FTlw9KNI80JiXXfXnE12UvZqpoba6i3ZU=;
        b=vck2T3Lv4b1r/5UuZzHBgQysCCk0qHYzFVtzUw6uBRujYz+NtvovdgU8s6S+G4zLJI
         bHe3eOhE8zZqXDORw8bPtOUv72PTDDZlW0cEoeMELyHJ+h5/z64WyGrZxGLUwC7590yE
         9L0ZhtkxQIS6e78HoUJgOeInMGnNUMEbzmyypw8sbKK9aucMv8Z9R299W96MPdtWTxJ9
         Z/7UiGk86HecNarY4SSCXfC2XcDWTNeGYwBtpntrO6zir40+sJ1xQej4RDN2GIkgTSh4
         VbzPqpYWhZczAkQB9S6cxDOggvbcPQLI5lZECcdrgfhbEIs1IThrYxfuRvkk17714eri
         UHFQ==
MIME-Version: 1.0
X-Received: by 10.236.61.8 with SMTP id v8mr16110430yhc.44.1417892229408; Sat,
 06 Dec 2014 10:57:09 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Sat, 6 Dec 2014 10:57:09 -0800 (PST)
In-Reply-To: <CAOhmDzdzanx-4CO45B7_80WPc0LhQxAm5zQuD_KaowXJUZPd9A@mail.gmail.com>
References: <CAOhmDzfyxrC=-xicZEzJK5Sgc6YWjy-qC8ef3GRhgA4cxnVZPA@mail.gmail.com>
	<CAMAsSd+S4ivrj7fjt_SAA0QOF1H1uCs=inrpf73tt4PfYAkPRA@mail.gmail.com>
	<CALte62yeOkn8OzsmUowO8Ou5RyPcdShozGZwnGVvp8mhHyLi3w@mail.gmail.com>
	<CAPh_B=bh+JHqABK2gQVpt155Ec1zzO1qSTH1Bo76tZhQ5vhMCQ@mail.gmail.com>
	<CAPh_B=bDCGAJXPP_CgiU0NJS1+KmhmX31But57WDqUeJ=buF+Q@mail.gmail.com>
	<CAOhmDzdLANy8adD4y02isgh5UVDCr4tee_Wz1ecO=gTY4Mr+sw@mail.gmail.com>
	<CAOhmDzfB1AU47NHRoD-m35Eo+ZKXc=wO92yPrBvUuS0MkFMK=g@mail.gmail.com>
	<CALte62w1RbC4zKDoWnz6esrjk=ZpsjY6dPcVmOcTxMW3Jb6VxA@mail.gmail.com>
	<CAMJOb8knFj6iEPpT6ietTKhKQMAxPbpcaE_aGv=41nCgKH9uKw@mail.gmail.com>
	<CAOhmDzdzanx-4CO45B7_80WPc0LhQxAm5zQuD_KaowXJUZPd9A@mail.gmail.com>
Date: Sat, 6 Dec 2014 10:57:09 -0800
Message-ID: <CALte62x5dYrpzo1XCooqwcv_fmT=X_9EL=jtQP7u0yFYmNdnig@mail.gmail.com>
Subject: Re: Unit tests in < 5 minutes
From: Ted Yu <yuzhihong@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Andrew Or <andrew@databricks.com>, Reynold Xin <rxin@databricks.com>, 
	Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160a6d4a9ef5f050990c28c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a6d4a9ef5f050990c28c
Content-Type: text/plain; charset=UTF-8

bq. I may move on to trying Maven.

Maven is my favorite :-)

On Sat, Dec 6, 2014 at 10:54 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Ted,
>
> I posted some updates
> <https://issues.apache.org/jira/browse/SPARK-3431?focusedCommentId=14236540&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14236540> on
> JIRA on my progress (or lack thereof) getting SBT to parallelize test
> suites properly. I'm currently stuck with SBT / ScalaTest, so I may move on
> to trying Maven.
>
> Andrew,
>
> Once we have a basic grasp of how to parallelize some of the tests, the
> next step will probably be to use containers (i.e. Docker) to allow more
> parallelization, especially for those tests that, for example, contend for
> ports.
>
> Nick
>
> On Fri Dec 05 2014 at 2:05:29 PM Andrew Or <andrew@databricks.com> wrote:
>
>> @Patrick and Josh actually we went even further than that. We simply
>> disable the UI for most tests and these used to be the single largest
>> source of port conflict.
>>
>

--089e0160a6d4a9ef5f050990c28c--

From dev-return-10694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec  7 04:28:39 2014
Return-Path: <dev-return-10694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A039410D30
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  7 Dec 2014 04:28:39 +0000 (UTC)
Received: (qmail 90094 invoked by uid 500); 7 Dec 2014 04:28:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90000 invoked by uid 500); 7 Dec 2014 04:28:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89983 invoked by uid 99); 7 Dec 2014 04:28:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 07 Dec 2014 04:28:37 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hao.cheng@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 07 Dec 2014 04:28:33 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga102.fm.intel.com with ESMTP; 06 Dec 2014 20:28:12 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,532,1413270000"; 
   d="scan'208";a="643653782"
Received: from pgsmsx108.gar.corp.intel.com ([10.221.44.103])
  by fmsmga002.fm.intel.com with ESMTP; 06 Dec 2014 20:28:10 -0800
Received: from pgsmsx102.gar.corp.intel.com (10.221.44.80) by
 PGSMSX108.gar.corp.intel.com (10.221.44.103) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Sun, 7 Dec 2014 12:28:09 +0800
Received: from shsmsx101.ccr.corp.intel.com (10.239.4.153) by
 PGSMSX102.gar.corp.intel.com (10.221.44.80) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Sun, 7 Dec 2014 12:28:09 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.216]) by
 SHSMSX101.ccr.corp.intel.com ([169.254.1.110]) with mapi id 14.03.0195.001;
 Sun, 7 Dec 2014 12:28:07 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Michael Armbrust <michael@databricks.com>, kb <kendb15@hotmail.com>
CC: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, "Cheng
 Hao" <chhao01@gmail.com>
Subject: RE: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
Thread-Topic: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
Thread-Index: AQHQEMg7X9Xj1mbF7kW1lynp5WTuypyA8sWAgAKXwlA=
Date: Sun, 7 Dec 2014 04:28:07 +0000
Message-ID: <80833ADD533E324CA05C160E41B63661027AFC49@shsmsx102.ccr.corp.intel.com>
References: <1417809799843-9662.post@n3.nabble.com>
 <CAAswR-7-j1ZRd5qkfpEHQoEm7_PHXWrX6sHYjQPh2NBur6ZM=g@mail.gmail.com>
In-Reply-To: <CAAswR-7-j1ZRd5qkfpEHQoEm7_PHXWrX6sHYjQPh2NBur6ZM=g@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec  7 04:29:38 2014
Return-Path: <dev-return-10695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2CB810D34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  7 Dec 2014 04:29:38 +0000 (UTC)
Received: (qmail 93074 invoked by uid 500); 7 Dec 2014 04:29:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92990 invoked by uid 500); 7 Dec 2014 04:29:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91816 invoked by uid 99); 7 Dec 2014 04:29:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 07 Dec 2014 04:29:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 07 Dec 2014 04:29:09 +0000
Received: by mail-lb0-f180.google.com with SMTP id l4so2365276lbv.11
        for <multiple recipients>; Sat, 06 Dec 2014 20:29:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=PbJedDbf4OzMoOEXNKv3NLaYNhpsdyEuy5ineVNqHg8=;
        b=lS6YzKciD6lWOgQUN+gZv5Ub4bDBB0ADOXhgkLhLdDBCjQIdiWBrZAIMeRyxkO9IR1
         Te47O3KVnOlwOgwFwwEvLI8QpnWYkIlyaU/3Wbi9kND9TPfxD47INcKyQR18h+dXo86y
         aWhCxj3uDbhhXT6Mvah8DXanH19zlV7brw/752Lltnz5wzuO1oBKmmXmWhWiU2zynJLM
         D8fSzs2TOM+p1UqHhupuoQIslCWnIl+ahknug3Wq+1GJqv4liIK6/sSD3OlgX57zfx2+
         LAmGko0hTn/epy9Hx0+Qou4kasYhhwPU6NgGsTyEe7aRZbenDRwOnlR4hn7+xZNEtzuf
         h0+Q==
X-Received: by 10.112.172.97 with SMTP id bb1mr10037917lbc.38.1417926548783;
 Sat, 06 Dec 2014 20:29:08 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Sat, 6 Dec 2014 20:28:48 -0800 (PST)
In-Reply-To: <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com>
References: <CACA1tWJFdVw8+Mh20HZ8Dedjsq-L-fP9_jiFO=GCm7L=ryYCjg@mail.gmail.com>
 <CACA1tW+=HAmTtu0GgmZSgRj0bYY9v4uLBMrmFtroD_Snivh=KQ@mail.gmail.com>
 <CACA1tW+f4m+=8Pc2sXJTR9S6Jxxx-an1Dhqp=vhgJ4AWVXFmtw@mail.gmail.com> <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Sun, 7 Dec 2014 12:28:48 +0800
Message-ID: <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com>
Subject: Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)
To: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33e904208f3050998c0cf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33e904208f3050998c0cf
Content-Type: text/plain; charset=UTF-8

Ok, found another possible bug in Hive.

My current solution is to use ALTER TABLE CHANGE to rename the column names.

The problem is after renaming the column names, the value of the columns
became all NULL.

Before renaming:
scala> sql("select `sorted::cre_ts` from pmt limit 1").collect
res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])

Execute renaming:
scala> sql("alter table pmt change `sorted::cre_ts` cre_ts string")
res13: org.apache.spark.sql.SchemaRDD =
SchemaRDD[972] at RDD at SchemaRDD.scala:108
== Query Plan ==
<Native command: executed by Hive>

After renaming:
scala> sql("select cre_ts from pmt limit 1").collect
res16: Array[org.apache.spark.sql.Row] = Array([null])

I created a JIRA for it:

  https://issues.apache.org/jira/browse/SPARK-4781


Jianshi

On Sun, Dec 7, 2014 at 1:06 AM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Hmm... another issue I found doing this approach is that ANALYZE TABLE ...
> COMPUTE STATISTICS will fail to attach the metadata to the table, and later
> broadcast join and such will fail...
>
> Any idea how to fix this issue?
>
> Jianshi
>
> On Sat, Dec 6, 2014 at 9:10 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Very interesting, the line doing drop table will throws an exception.
>> After removing it all works.
>>
>> Jianshi
>>
>> On Sat, Dec 6, 2014 at 9:11 AM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Here's the solution I got after talking with Liancheng:
>>>
>>> 1) using backquote `..` to wrap up all illegal characters
>>>
>>>     val rdd = parquetFile(file)
>>>     val schema = rdd.schema.fields.map(f => s"`${f.name}`
>>> ${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
>>>
>>>     val ddl_13 = s"""
>>>       |CREATE EXTERNAL TABLE $name (
>>>       |  $schema
>>>       |)
>>>       |STORED AS PARQUET
>>>       |LOCATION '$file'
>>>       """.stripMargin
>>>
>>>     sql(ddl_13)
>>>
>>> 2) create a new Schema and do applySchema to generate a new SchemaRDD,
>>> had to drop and register table
>>>
>>>     val t = table(name)
>>>     val newSchema = StructType(t.schema.fields.map(s => s.copy(name =
>>> s.name.replaceAll(".*?::", ""))))
>>>     sql(s"drop table $name")
>>>     applySchema(t, newSchema).registerTempTable(name)
>>>
>>> I'm testing it for now.
>>>
>>> Thanks for the help!
>>>
>>>
>>> Jianshi
>>>
>>> On Sat, Dec 6, 2014 at 8:41 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Hi,
>>>>
>>>> I had to use Pig for some preprocessing and to generate Parquet files
>>>> for Spark to consume.
>>>>
>>>> However, due to Pig's limitation, the generated schema contains Pig's
>>>> identifier
>>>>
>>>> e.g.
>>>> sorted::id, sorted::cre_ts, ...
>>>>
>>>> I tried to put the schema inside CREATE EXTERNAL TABLE, e.g.
>>>>
>>>>   create external table pmt (
>>>>     sorted::id bigint
>>>>   )
>>>>   stored as parquet
>>>>   location '...'
>>>>
>>>> Obviously it didn't work, I also tried removing the identifier
>>>> sorted::, but the resulting rows contain only nulls.
>>>>
>>>> Any idea how to create a table in HiveContext from these Parquet files?
>>>>
>>>> Thanks,
>>>> Jianshi
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a11c33e904208f3050998c0cf--

From dev-return-10696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 05:08:32 2014
Return-Path: <dev-return-10696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B2D91034F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 05:08:32 +0000 (UTC)
Received: (qmail 24048 invoked by uid 500); 8 Dec 2014 05:08:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23978 invoked by uid 500); 8 Dec 2014 05:08:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23967 invoked by uid 99); 8 Dec 2014 05:08:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 05:08:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of judynash@exchange.microsoft.com designates 64.4.22.88 as permitted sender)
Received: from [64.4.22.88] (HELO na01-by1-obe.outbound.o365filtering.com) (64.4.22.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 05:08:02 +0000
Received: from BY2SR01CA102.namsdf01.sdf.exchangelabs.com (10.255.93.147) by
 BLUSR01MB603.namsdf01.sdf.exchangelabs.com (10.255.124.168) with Microsoft
 SMTP Server (TLS) id 15.1.49.2; Mon, 8 Dec 2014 05:07:16 +0000
Received: from BY1FFOFD003.ffo.gbl (2a01:111:f400:7c00::89) by
 BY2SR01CA102.outlook.office365.com (2a01:111:e400:2c01::19) with Microsoft
 SMTP Server (TLS) id 15.1.49.2 via Frontend Transport; Mon, 8 Dec 2014
 05:07:16 +0000
Received: from hybrid.exchange.microsoft.com (131.107.147.100) by
 BY1FFOFD003.mail.o365filtering.com (10.1.16.90) with Microsoft SMTP Server
 (TLS) id 15.1.36.5 via Frontend Transport; Mon, 8 Dec 2014 05:07:15 +0000
Received: from DFM-TK5MBX15-05.exchange.corp.microsoft.com (157.54.109.44) by
 DFM-TK5EDG15-02.exchange.corp.microsoft.com (157.54.27.97) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Mon, 8 Dec 2014 05:07:15 +0000
Received: from DFM-DB3MBX15-06.exchange.corp.microsoft.com (10.221.24.76) by
 DFM-TK5MBX15-05.exchange.corp.microsoft.com (157.54.109.44) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Sun, 7 Dec 2014 21:07:14 -0800
Received: from DFM-DB3MBX15-08.exchange.corp.microsoft.com (10.221.24.69) by
 DFM-DB3MBX15-06.exchange.corp.microsoft.com (10.221.24.76) with Microsoft
 SMTP Server (TLS) id 15.0.1044.22; Sun, 7 Dec 2014 21:07:13 -0800
Received: from DFM-DB3MBX15-08.exchange.corp.microsoft.com ([169.254.12.242])
 by DFM-DB3MBX15-08.exchange.corp.microsoft.com ([169.254.12.242]) with mapi
 id 15.00.1044.021; Sun, 7 Dec 2014 21:07:13 -0800
From: Judy Nash <judynash@exchange.microsoft.com>
To: Josh Rosen <rosenville@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: build in IntelliJ IDEA
Thread-Topic: build in IntelliJ IDEA
Thread-Index: AdAQ1KT6iWdYfqaZQGWM3WexXhvDfgAUJK8AAF/lHYA=
Date: Mon, 8 Dec 2014 05:07:10 +0000
Message-ID: <2594f5fd906b4eefb9d30ec7b147d69c@DFM-DB3MBX15-08.exchange.corp.microsoft.com>
References: <e3af038200f640b8a745e88c63d544b7@DFM-DB3MBX15-08.exchange.corp.microsoft.com>
 <etPan.54823deb.643c9869.3214@joshs-mbp>
In-Reply-To: <etPan.54823deb.643c9869.3214@joshs-mbp>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [157.59.235.233]
Content-Type: multipart/alternative;
	boundary="_000_2594f5fd906b4eefb9d30ec7b147d69cDFMDB3MBX1508exchangeco_"
MIME-Version: 1.0
X-EOPAttributedMessage: 0
Received-SPF: SoftFail (protection.outlook.com: domain of transitioning
 exchange.microsoft.com discourages use of 131.107.147.100 as permitted
 sender)
Authentication-Results: spf=softfail (sender IP is 131.107.147.100)
 smtp.mailfrom=judynash@exchange.microsoft.com; spark.apache.org; dkim=none
 (message not signed) header.d=none;spark.apache.org; dmarc=fail action=none
 header.from=exchange.microsoft.com;
X-Forefront-Antispam-Report:
	CIP:131.107.147.100;IPV:NLI;EFV:NLI;SFV:NSPM;SFS:(10019020)(53754006)(164054003)(189002)(199003)(57704003)(377454003)(24454002)(107886001)(107046002)(512874002)(54356999)(76176999)(19300405004)(105596002)(64706001)(66066001)(20776003)(4396001)(21056001)(16236675004)(19625215002)(92566001)(84676001)(6806004)(19580395003)(46102003)(84326002)(71186001)(19580405001)(2656002)(33646002)(87936001)(15975445007)(92726002)(2501002)(97736003)(108616004)(19617315012)(99396003)(62966003)(106466001)(120916001)(102836002)(50986999)(31966008)(77156002)(68736005)(15519875005)(24736002);DIR:OUT;SFP:1102;SCL:1;SRVR:BLUSR01MB603;H:hybrid.exchange.microsoft.com;FPR:;SPF:SoftFail;PTR:InfoDomainNonexistent;MX:1;A:1;LANG:en;
X-Microsoft-Antispam: UriScan:;
X-Microsoft-Antispam: BCL:0;PCL:0;RULEID:;SRVR:BLUSR01MB603;
X-DmarcStatus: Failed
X-Exchange-Antispam-Report-Test: UriScan:;
X-Exchange-Antispam-Report-CFA-Test:
	BCL:0;PCL:0;RULEID:(4002003);SRVR:BLUSR01MB603;
X-Forefront-PRVS: 041963B986
X-Exchange-Antispam-Report-CFA-Test: BCL:0;PCL:0;RULEID:;SRVR:BLUSR01MB603;
X-OriginatorOrg: exchange.microsoft.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 08 Dec 2014 05:07:15.8859
 (UTC)
X-MS-Exchange-CrossTenant-Id: f686d426-8d16-42db-81b7-ab578e110ccd
X-MS-Exchange-CrossTenant-FromEntityHeader: HybridOnPrem
X-MS-Exchange-Transport-CrossTenantHeadersStamped: BLUSR01MB603
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_2594f5fd906b4eefb9d30ec7b147d69cDFMDB3MBX1508exchangeco_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

VGhhbmtzIEpvc2guIFRoYXQgd2FzIHRoZSBpc3N1ZS4NCg0KRnJvbTogSm9zaCBSb3NlbiBbbWFp
bHRvOnJvc2VudmlsbGVAZ21haWwuY29tXQ0KU2VudDogRnJpZGF5LCBEZWNlbWJlciA1LCAyMDE0
IDM6MjEgUE0NClRvOiBKdWR5IE5hc2g7IGRldkBzcGFyay5hcGFjaGUub3JnDQpTdWJqZWN0OiBS
ZTogYnVpbGQgaW4gSW50ZWxsaUogSURFQQ0KDQpJZiB5b3UgZ28gdG8g4oCcRmlsZSAtPiBQcm9q
ZWN0IFN0cnVjdHVyZeKAnSBhbmQgY2xpY2sgb24g4oCcUHJvamVjdOKAnSB1bmRlciB0aGUg4oCc
UHJvamVjdCBzZXR0aW5nc+KAnSBoZWFkaW5nLCBkbyB5b3Ugc2VlIGFuIGVudHJ5IGZvciDigJxQ
cm9qZWN0IFNESz/igJ0gIElmIG5vdCwgeW91IHNob3VsZCBjbGljayDigJxOZXfigKbigJ0gYW5k
IGNvbmZpZ3VyZSBhIEpESzsgYnkgZGVmYXVsdCwgSSB0aGluayBJbnRlbGxpSiBzaG91bGQgZmln
dXJlIG91dCBhIGNvcnJlY3QgcGF0aCB0byB5b3VyIHN5c3RlbSBKREssIHNvIHlvdSBzaG91bGQg
anVzdCBiZSBhYmxlIHRvIGhpdCDigJxPa+KAnSB0aGVuIHJlYnVpbGQgeW91ciBwcm9qZWN0LiAg
IEZvciByZWZlcmVuY2UsIGhlcmXigJlzIGEgc2NyZWVuc2hvdCBzaG93aW5nIHdoYXQgbXkgdmVy
c2lvbiBvZiB0aGF0IHdpbmRvdyBsb29rcyBsaWtlOiBodHRwOi8vaS5pbWd1ci5jb20vaFJmUWpJ
aS5wbmcNCg0KDQpPbiBEZWNlbWJlciA1LCAyMDE0IGF0IDE6NTI6MzUgUE0sIEp1ZHkgTmFzaCAo
anVkeW5hc2hAZXhjaGFuZ2UubWljcm9zb2Z0LmNvbTxtYWlsdG86anVkeW5hc2hAZXhjaGFuZ2Uu
bWljcm9zb2Z0LmNvbT4pIHdyb3RlOg0KSGkgZXZlcnlvbmUsDQoNCkhhdmUgYSBuZXdiaWUgcXVl
c3Rpb24gb24gdXNpbmcgSW50ZWxsaUogdG8gYnVpbGQgYW5kIGRlYnVnLg0KDQpJIGZvbGxvd2Vk
IHRoaXMgd2lraSB0byBzZXR1cCBJbnRlbGxpSjoNCmh0dHBzOi8vY3dpa2kuYXBhY2hlLm9yZy9j
b25mbHVlbmNlL2Rpc3BsYXkvU1BBUksvVXNlZnVsK0RldmVsb3BlcitUb29scyNVc2VmdWxEZXZl
bG9wZXJUb29scy1CdWlsZGluZ1NwYXJraW5JbnRlbGxpSklERUENCg0KQWZ0ZXJ3YXJkIEkgdHJp
ZWQgdG8gYnVpbGQgdmlhIFRvb2xiYXIgKEJ1aWxkID4gUmVidWlsZCBQcm9qZWN0KS4NClRoZSBh
Y3Rpb24gZmFpbHMgd2l0aCB0aGUgZXJyb3IgbWVzc2FnZToNCkNhbm5vdCBzdGFydCBjb21waWxl
cjogdGhlIFNESyBpcyBub3Qgc3BlY2lmaWVkLg0KDQpXaGF0IFNESyBkbyBJIG5lZWQgdG8gc3Bl
Y2lmeSB0byBnZXQgdGhlIGJ1aWxkIHdvcmtpbmc/DQoNClRoYW5rcywNCkp1ZHkNCg==

--_000_2594f5fd906b4eefb9d30ec7b147d69cDFMDB3MBX1508exchangeco_--

From dev-return-10697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 15:40:48 2014
Return-Path: <dev-return-10697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E523108BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 15:40:48 +0000 (UTC)
Received: (qmail 26939 invoked by uid 500); 8 Dec 2014 15:40:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26852 invoked by uid 500); 8 Dec 2014 15:40:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25622 invoked by uid 99); 8 Dec 2014 15:40:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 15:40:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 15:40:38 +0000
Received: by mail-wg0-f46.google.com with SMTP id x13so3994695wgg.33
        for <multiple recipients>; Mon, 08 Dec 2014 07:39:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=aJozK/WMNgLLk/V3S0x+OS8QezMVx49CxY92vnsEl3k=;
        b=0pJ45oYsAk3AzUpnRIFttjL1hRN5DAu54ZvHjpPqOZ1izehvGDWt1KqeRhRowM0VD7
         5KzhRhjDfw3Kjy/lfUbXONb9B0m/0mRIY7c4uBEm3/Le4W8iJccKTCx87rieUd3hGH1H
         jayfgOyagMfVYc+ptXxYyk33omFhmLFbz9TWihrJsIRZ86XPOt4JXMCX6s38RdtrY61q
         qrVEtEiHCFlY7OecB9vM4HMAYy1C7AwQhka0mJtywgh6etUsxhh2ScKBHqIjJpYHHRCy
         BZRKaeqNkrAflPHvkPd0dGQbgSwAfpuQJBhHtRQElYkfqSIwrsyu5GbfyqBKHmwz7kQ6
         +VjQ==
X-Received: by 10.194.90.244 with SMTP id bz20mr46713766wjb.125.1418053169535;
 Mon, 08 Dec 2014 07:39:29 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.189.8 with HTTP; Mon, 8 Dec 2014 07:38:59 -0800 (PST)
From: Gerard Maas <gerard.maas@gmail.com>
Date: Mon, 8 Dec 2014 16:38:59 +0100
Message-ID: <CAMc-71n+z6+9Jqm6v4OWq2ii691OVb2vgH1upirdOSOEK6skOQ@mail.gmail.com>
Subject: Understanding reported times on the Spark UI [+ Streaming]
To: spark users <user@spark.apache.org>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd090a7161430509b63b14
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd090a7161430509b63b14
Content-Type: text/plain; charset=UTF-8

Hi,

I'm confused about the Stage times reported on the Spark-UI (Spark 1.1.0)
for an Spark-Streaming job.  I'm hoping somebody can shine some light on it:

Let's do this with an example:

On the /stages page, stage # 232 is reported to have lasted 18 seconds:
232runJob at RDDFunctions.scala:23
<http://localhost:24040/stages/stage?id=232&attempt=0>+details

2014/12/08 15:06:2518 s
12/12
When I click on it for details, I see: [1]

Total time across all tasks = 42s

Aggregated metrics by executor:
Executor1 19s
Executor2 24s

Summing all tasks is actually: 40,009s

What is the time reported on the overview page? (18s?)

What is relation between the reported time on the overview and the detail
page?

My Spark Streaming job is reported to be taking 3m24s, and (I think)
there's only 1 stage in my job. How does the timing per stage relate to the
Spark Streaming reported in the 'streaming' page ? (e.g. 'last batch') ?

Is there a way to relate a streaming batch to the stages executed  to
complete that batch?
The numbers as they are at the moment don't seem to add up.

Thanks,

Gerard.


[1] https://drive.google.com/file/d/0BznIWnuWhoLlMkZubzY2dTdOWDQ

--047d7bfd090a7161430509b63b14--

From dev-return-10698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 19:36:42 2014
Return-Path: <dev-return-10698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9643F9548
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 19:36:42 +0000 (UTC)
Received: (qmail 38105 invoked by uid 500); 8 Dec 2014 19:36:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38016 invoked by uid 500); 8 Dec 2014 19:36:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36770 invoked by uid 99); 8 Dec 2014 19:36:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 19:36:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 19:36:34 +0000
Received: by mail-la0-f46.google.com with SMTP id q1so4637227lam.33
        for <dev@spark.apache.org>; Mon, 08 Dec 2014 11:35:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ywQ5URPtYgEAXe2kim4dsFeaP6hxwYdQ42CwzaWdclY=;
        b=H3CJxSGB6jsozIM5A7dxWq22E+AP2yf93bT26MeFctldXZLQVnNiVFAAgkwO/MZOXd
         86NCeELPT0DOWUSeqYc6eF/3TD7ltYDXqZlu/AUjGaqCwiZhiDeFcvTm/3/wdurtDmkI
         +RPUgFbruaJ+j7maWc9tJNsCRY8MO+t8e3QdRsTf4u/sU4L4o7S/goBey1RkF6vfjFKF
         q00bPFHLLvB169T++4XItvNhx5IdAAdAnfzfZCuHGL7k63+KHkmpPffeADutGeKtZdxO
         vEbyV3AfU1gKhZQSpq5Mkxelwb1DjFzDk6rkkbGYhRF28yZ0kGnaGJOeqV+2hzxhExru
         ZmDw==
X-Gm-Message-State: ALoCoQm6f1JkRC8UxW07NrpPoQrOt1yniJctYF73i52SMvB4k4AyQzE+3Q04rfk/GmAZRcF/QDVE
X-Received: by 10.152.29.97 with SMTP id j1mr18057017lah.3.1418067307882; Mon,
 08 Dec 2014 11:35:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 8 Dec 2014 11:34:47 -0800 (PST)
In-Reply-To: <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com>
References: <CACA1tWJFdVw8+Mh20HZ8Dedjsq-L-fP9_jiFO=GCm7L=ryYCjg@mail.gmail.com>
 <CACA1tW+=HAmTtu0GgmZSgRj0bYY9v4uLBMrmFtroD_Snivh=KQ@mail.gmail.com>
 <CACA1tW+f4m+=8Pc2sXJTR9S6Jxxx-an1Dhqp=vhgJ4AWVXFmtw@mail.gmail.com>
 <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com> <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 8 Dec 2014 11:34:47 -0800
Message-ID: <CAAswR-7=doK5JkUoxvPfTT0g0UmQ6hUMWHUZhxuA9WdhUobWNA@mail.gmail.com>
Subject: Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)
To: Jianshi Huang <jianshi.huang@gmail.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160bdaa2782d10509b986b4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bdaa2782d10509b986b4
Content-Type: text/plain; charset=UTF-8

This is by hive's design.  From the Hive documentation:

The column change command will only modify Hive's metadata, and will not
> modify data. Users should make sure the actual data layout of the
> table/partition conforms with the metadata definition.



On Sat, Dec 6, 2014 at 8:28 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Ok, found another possible bug in Hive.
>
> My current solution is to use ALTER TABLE CHANGE to rename the column
> names.
>
> The problem is after renaming the column names, the value of the columns
> became all NULL.
>
> Before renaming:
> scala> sql("select `sorted::cre_ts` from pmt limit 1").collect
> res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])
>
> Execute renaming:
> scala> sql("alter table pmt change `sorted::cre_ts` cre_ts string")
> res13: org.apache.spark.sql.SchemaRDD =
> SchemaRDD[972] at RDD at SchemaRDD.scala:108
> == Query Plan ==
> <Native command: executed by Hive>
>
> After renaming:
> scala> sql("select cre_ts from pmt limit 1").collect
> res16: Array[org.apache.spark.sql.Row] = Array([null])
>
> I created a JIRA for it:
>
>   https://issues.apache.org/jira/browse/SPARK-4781
>
>
> Jianshi
>
> On Sun, Dec 7, 2014 at 1:06 AM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Hmm... another issue I found doing this approach is that ANALYZE TABLE
>> ... COMPUTE STATISTICS will fail to attach the metadata to the table, and
>> later broadcast join and such will fail...
>>
>> Any idea how to fix this issue?
>>
>> Jianshi
>>
>> On Sat, Dec 6, 2014 at 9:10 PM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Very interesting, the line doing drop table will throws an exception.
>>> After removing it all works.
>>>
>>> Jianshi
>>>
>>> On Sat, Dec 6, 2014 at 9:11 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Here's the solution I got after talking with Liancheng:
>>>>
>>>> 1) using backquote `..` to wrap up all illegal characters
>>>>
>>>>     val rdd = parquetFile(file)
>>>>     val schema = rdd.schema.fields.map(f => s"`${f.name}`
>>>> ${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
>>>>
>>>>     val ddl_13 = s"""
>>>>       |CREATE EXTERNAL TABLE $name (
>>>>       |  $schema
>>>>       |)
>>>>       |STORED AS PARQUET
>>>>       |LOCATION '$file'
>>>>       """.stripMargin
>>>>
>>>>     sql(ddl_13)
>>>>
>>>> 2) create a new Schema and do applySchema to generate a new SchemaRDD,
>>>> had to drop and register table
>>>>
>>>>     val t = table(name)
>>>>     val newSchema = StructType(t.schema.fields.map(s => s.copy(name =
>>>> s.name.replaceAll(".*?::", ""))))
>>>>     sql(s"drop table $name")
>>>>     applySchema(t, newSchema).registerTempTable(name)
>>>>
>>>> I'm testing it for now.
>>>>
>>>> Thanks for the help!
>>>>
>>>>
>>>> Jianshi
>>>>
>>>> On Sat, Dec 6, 2014 at 8:41 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>>> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> I had to use Pig for some preprocessing and to generate Parquet files
>>>>> for Spark to consume.
>>>>>
>>>>> However, due to Pig's limitation, the generated schema contains Pig's
>>>>> identifier
>>>>>
>>>>> e.g.
>>>>> sorted::id, sorted::cre_ts, ...
>>>>>
>>>>> I tried to put the schema inside CREATE EXTERNAL TABLE, e.g.
>>>>>
>>>>>   create external table pmt (
>>>>>     sorted::id bigint
>>>>>   )
>>>>>   stored as parquet
>>>>>   location '...'
>>>>>
>>>>> Obviously it didn't work, I also tried removing the identifier
>>>>> sorted::, but the resulting rows contain only nulls.
>>>>>
>>>>> Any idea how to create a table in HiveContext from these Parquet files?
>>>>>
>>>>> Thanks,
>>>>> Jianshi
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>

--089e0160bdaa2782d10509b986b4--

From dev-return-10699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 19:58:37 2014
Return-Path: <dev-return-10699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C029B9647
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 19:58:37 +0000 (UTC)
Received: (qmail 16062 invoked by uid 500); 8 Dec 2014 19:58:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15988 invoked by uid 500); 8 Dec 2014 19:58:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15965 invoked by uid 99); 8 Dec 2014 19:58:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 19:58:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 19:58:10 +0000
Received: by mail-ig0-f171.google.com with SMTP id z20so3311706igj.16
        for <dev@spark.apache.org>; Mon, 08 Dec 2014 11:58:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=/7GBptnPJd1XYEem8QMe84LNTdKZx5bxmd+7gt8OaDQ=;
        b=Dtu0myl8TFlBf9+e1OlFPR2P4JZRUj2C+71KVUdffYhuNHpeJX9FlNb3zO8moqo7qP
         yDxPFcdZ5AU/a6e/ASwTZNabQHPhbE5WuCM8/Va9Avb1wbi+TbAfSyisVuaqFFTrJoYj
         qPNupV5EReGV0Nvk8/sKWtkh/VQrIy3xFUGiiOUWbi4XFIOmzr3WLhBGo1flE35XmyIE
         bgamNkwUWXfuE2WtZPdx1YPMg3wg2cAAp3oMBMUH7wtqgB8/SrBlBRwFzqWX0JNr1LFL
         PWin/rrMU8T04ZNd0Qk9FbFG+GuIhdpKaFPphGfGYtcE1pct8ezzgMFwEGfW3UivZpA9
         ckew==
X-Received: by 10.50.29.3 with SMTP id f3mr16388198igh.23.1418068689626; Mon,
 08 Dec 2014 11:58:09 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzeAO2QojA5pFGJ_4FiSjswG2KmVhhwZM=cczZ0GYFEZKA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 08 Dec 2014 19:58:08 +0000
Message-ID: <CAOhmDzfRrwe41rGXKMv2RGjKqW-YW-2ZYr9BHvXRqnq+7rsWeg@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Patrick Wendell <pwendell@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd74b54832cb80509b9d852
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74b54832cb80509b9d852
Content-Type: text/plain; charset=UTF-8

I recently came across this blog post, which reminded me of this thread.

How to Discourage Open Source Contributions
<http://danluu.com/discourage-oss/>

We are currently at 320+ open PRs, many of which haven't been updated in
over a month. We have quite a few PRs that haven't been touched in 3-5
months.

*If you have the time and interest, please hop on over to the Spark PR
Dashboard <https://spark-prs.appspot.com/>, sort the PRs by
least-recently-updated, and update them where you can.*

I share the blog author's opinion that letting PRs go stale discourages
contributions, especially from first-time contributors, and especially more
so when the PR author is waiting on feedback from a committer or
contributor.

I've been thinking about simple ways to make it easier for all of us to
chip in on controlling stale PRs in an incremental way. For starters, would
it help if an automated email went out to the dev list once a week that a)
reported the number of stale PRs, and b) directly linked to the 5 least
recently updated PRs?

Nick

On Sat Aug 30 2014 at 3:41:39 AM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> it's actually precedurally difficult for us to close pull requests
>
>
> Just an FYI: Seems like the GitHub-sanctioned work-around to having
> issues-only permissions is to have a second, issues-only repository
> <https://help.github.com/articles/issues-only-access-permissions>. Not a
> very attractive work-around...
>
> Nick
>

--047d7bd74b54832cb80509b9d852--

From dev-return-10700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 20:10:31 2014
Return-Path: <dev-return-10700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E4F1970C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 20:10:31 +0000 (UTC)
Received: (qmail 58659 invoked by uid 500); 8 Dec 2014 20:10:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58585 invoked by uid 500); 8 Dec 2014 20:10:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58569 invoked by uid 99); 8 Dec 2014 20:10:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:10:28 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail01.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:10:02 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=2890; q=dns/txt; s=SM2048Apr2013K;
  t=1418069422; x=1418155822;
  h=from:to:cc:date:subject:message-id:references:
   in-reply-to:mime-version:content-transfer-encoding;
  bh=D624td4oiiicTvtC6dl8VbgZAe9ejX6NsOyu/3i3lSk=;
  b=dbsxYQCFCiZms6ofchhS65tAmkkc+86srTpdIX5GZP/z8qGbPWEj+cxe
   eGqnAq69rV+n3J5EMeEc++SPEuvXCacLePx7OE9G+OucQVNRDt6k9Yv4z
   DoRKxU1vwxw03Gvv3KbVzxCEzFRnDk0c5cBBfPT21JW8vRLIOUUnkMA2/
   8nhziWVxO4DELDpvOc6QqC3vUZn7+44o3aPplS42kKqhN0xv/fYFQqvHB
   TqVaEaJJuIRomnoaa15HDIRxkXO447HyoXM8XMGXW6pOWrAK6UDysjXzq
   KFpqHyFsR1nu6fcfgVEXg8LIEXSPTUKIJq5N57ph8+SmzSGD7qVnQm1oA
   A==;
X-IronPort-AV: E=McAfee;i="5600,1067,7646"; a="183568899"
X-IronPort-AV: E=Sophos;i="5.07,540,1413259200"; 
   d="scan'208";a="183568899"
X-OSD_Exception: TRUE
Received: from kdcpexcasht04.cof.ds.capitalone.com ([10.37.194.14])
  by komail01.kdc.capitalone.com with ESMTP; 08 Dec 2014 15:08:01 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 KDCPEXCASHT04.cof.ds.capitalone.com ([10.37.194.14]) with mapi; Mon, 8 Dec
 2014 15:08:00 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>, Patrick Wendell
	<pwendell@gmail.com>
CC: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Date: Mon, 8 Dec 2014 15:07:59 -0500
Subject: Re: Handling stale PRs
Thread-Topic: Handling stale PRs
Thread-Index: AdATIqoFkG3XdxRFSSWaEI3+C1S4OQ==
Message-ID: <D0AB445F.6BD0%ilya.ganelin@capitalone.com>
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local>
 <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzeAO2QojA5pFGJ_4FiSjswG2KmVhhwZM=cczZ0GYFEZKA@mail.gmail.com>
 <CAOhmDzfRrwe41rGXKMv2RGjKqW-YW-2ZYr9BHvXRqnq+7rsWeg@mail.gmail.com>
In-Reply-To: <CAOhmDzfRrwe41rGXKMv2RGjKqW-YW-2ZYr9BHvXRqnq+7rsWeg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.3.140616
acceptlanguage: en-US
Content-Type: text/plain; charset="iso-8859-1"
MIME-Version: 1.0
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you for pointing this out, Nick. I know that for myself and my
colleague who are starting to contribute to Spark, it=B9s definitely
discouraging to have fixes sitting in the pipeline. Could you recommend
any other ways that we can facilitate getting these PRs accepted? Clean,
well-tested code is an obvious one but I=B9d like to know if there are some
non-obvious things we (as contributors) could do to make the committers=B9
lives easier? Thanks!

-Ilya =


On 12/8/14, 11:58 AM, "Nicholas Chammas" <nicholas.chammas@gmail.com>
wrote:

>I recently came across this blog post, which reminded me of this thread.
>
>How to Discourage Open Source Contributions
><http://danluu.com/discourage-oss/>
>
>We are currently at 320+ open PRs, many of which haven't been updated in
>over a month. We have quite a few PRs that haven't been touched in 3-5
>months.
>
>*If you have the time and interest, please hop on over to the Spark PR
>Dashboard <https://spark-prs.appspot.com/>, sort the PRs by
>least-recently-updated, and update them where you can.*
>
>I share the blog author's opinion that letting PRs go stale discourages
>contributions, especially from first-time contributors, and especially
>more
>so when the PR author is waiting on feedback from a committer or
>contributor.
>
>I've been thinking about simple ways to make it easier for all of us to
>chip in on controlling stale PRs in an incremental way. For starters,
>would
>it help if an automated email went out to the dev list once a week that a)
>reported the number of stale PRs, and b) directly linked to the 5 least
>recently updated PRs?
>
>Nick
>
>On Sat Aug 30 2014 at 3:41:39 AM Nicholas Chammas <
>nicholas.chammas@gmail.com> wrote:
>
>> On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> it's actually precedurally difficult for us to close pull requests
>>
>>
>> Just an FYI: Seems like the GitHub-sanctioned work-around to having
>> issues-only permissions is to have a second, issues-only repository
>> <https://help.github.com/articles/issues-only-access-permissions>. Not a
>> very attractive work-around...
>>
>> Nick
>>

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 20:27:33 2014
Return-Path: <dev-return-10701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A8F3E97E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 20:27:33 +0000 (UTC)
Received: (qmail 24318 invoked by uid 500); 8 Dec 2014 20:27:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24253 invoked by uid 500); 8 Dec 2014 20:27:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24241 invoked by uid 99); 8 Dec 2014 20:27:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:27:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:27:27 +0000
Received: by mail-ie0-f178.google.com with SMTP id tp5so5265089ieb.9
        for <dev@spark.apache.org>; Mon, 08 Dec 2014 12:27:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=wBFOjR8yiftvI8W3v6ALekBcEc2hbuCMtjAD4B67Lgk=;
        b=Ltsbh5xNsy28U664G/iKebxuj/PazBQYh83+tpsvhC2+IcR5/aEPMvTo52aN5HPa/K
         ht4Z96B6JULFwcMZl0eqcrm2iaN7045ciifdA/Bq0gsX4fXwbaTIvDOt448VSCfEBHx9
         QHeA9aA39lgisAbMJdzZ/NPiF0Sl2GV01RtIhr30/EoxXC5yObb5WJW1/n5OU0sOKDBP
         u68La82agSYj1mwWbGPnaLyQQ7KfGks+nrtFs9DEOBAFCYkzJLsduEkypr9pswD+Hacs
         I0JXN7aViAddKKzXgpr9yguqRA3UR93Q9WJf4mIMubw1JtI6IvCCjF/IGFq4F+auuhdr
         3zQw==
X-Received: by 10.42.231.139 with SMTP id jq11mr22618358icb.84.1418070427412;
 Mon, 08 Dec 2014 12:27:07 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzeAO2QojA5pFGJ_4FiSjswG2KmVhhwZM=cczZ0GYFEZKA@mail.gmail.com>
 <CAOhmDzfRrwe41rGXKMv2RGjKqW-YW-2ZYr9BHvXRqnq+7rsWeg@mail.gmail.com> <D0AB445F.6BD0%ilya.ganelin@capitalone.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 08 Dec 2014 20:27:07 +0000
Message-ID: <CAOhmDzfQWVXmVjfFB1jALRW53+wRVUUZb_gAf51EmvEdzmrQ-g@mail.gmail.com>
Subject: Re: Handling stale PRs
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>, Patrick Wendell <pwendell@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6705a317d91f0509ba404b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6705a317d91f0509ba404b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Things that help:

   - Be persistent. People are busy, so just ping them if there=E2=80=99s b=
een no
   response for a couple of weeks. Hopefully, as the project continues to
   develop, this will become less necessary.
   - Only ping reviewers after test results are back from Jenkins. Make
   sure all the tests are clear before reaching out, unless you need help
   understanding why a test is failing.
   - Whenever possible, keep PRs small, small, small.
   - Get buy-in on the dev list before working on something, especially
   larger features, to make sure you are making something that people
   understand and that is in accordance with Spark=E2=80=99s design.

I=E2=80=99m just speaking as a random contributor here, so don=E2=80=99t ta=
ke this advice
as gospel.

Nick
=E2=80=8B

On Mon Dec 08 2014 at 3:08:02 PM Ganelin, Ilya <Ilya.Ganelin@capitalone.com=
>
wrote:

> Thank you for pointing this out, Nick. I know that for myself and my
> colleague who are starting to contribute to Spark, it=C2=B9s definitely
> discouraging to have fixes sitting in the pipeline. Could you recommend
> any other ways that we can facilitate getting these PRs accepted? Clean,
> well-tested code is an obvious one but I=C2=B9d like to know if there are=
 some
> non-obvious things we (as contributors) could do to make the committers=
=C2=B9
> lives easier? Thanks!
>
> -Ilya
>
> On 12/8/14, 11:58 AM, "Nicholas Chammas" <nicholas.chammas@gmail.com>
> wrote:
>
> >I recently came across this blog post, which reminded me of this thread.
> >
> >How to Discourage Open Source Contributions
> ><http://danluu.com/discourage-oss/>
> >
> >We are currently at 320+ open PRs, many of which haven't been updated in
> >over a month. We have quite a few PRs that haven't been touched in 3-5
> >months.
> >
> >*If you have the time and interest, please hop on over to the Spark PR
> >Dashboard <https://spark-prs.appspot.com/>, sort the PRs by
> >least-recently-updated, and update them where you can.*
> >
> >I share the blog author's opinion that letting PRs go stale discourages
> >contributions, especially from first-time contributors, and especially
> >more
> >so when the PR author is waiting on feedback from a committer or
> >contributor.
> >
> >I've been thinking about simple ways to make it easier for all of us to
> >chip in on controlling stale PRs in an incremental way. For starters,
> >would
> >it help if an automated email went out to the dev list once a week that =
a)
> >reported the number of stale PRs, and b) directly linked to the 5 least
> >recently updated PRs?
> >
> >Nick
> >
> >On Sat Aug 30 2014 at 3:41:39 AM Nicholas Chammas <
> >nicholas.chammas@gmail.com> wrote:
> >
> >> On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >>> it's actually precedurally difficult for us to close pull requests
> >>
> >>
> >> Just an FYI: Seems like the GitHub-sanctioned work-around to having
> >> issues-only permissions is to have a second, issues-only repository
> >> <https://help.github.com/articles/issues-only-access-permissions>. Not
> a
> >> very attractive work-around...
> >>
> >> Nick
> >>
>
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or
> proprietary to Capital One and/or its affiliates. The information
> transmitted herewith is intended only for use by the individual or entity
> to which it is addressed.  If the reader of this message is not the
> intended recipient, you are hereby notified that any review,
> retransmission, dissemination, distribution, copying or other use of, or
> taking of any action in reliance upon this information is strictly
> prohibited. If you have received this communication in error, please
> contact the sender and delete the material from your computer.
>
>

--047d7b6705a317d91f0509ba404b--

From dev-return-10702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec  8 20:59:41 2014
Return-Path: <dev-return-10702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EE90993A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  8 Dec 2014 20:59:41 +0000 (UTC)
Received: (qmail 5065 invoked by uid 500); 8 Dec 2014 20:59:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4991 invoked by uid 500); 8 Dec 2014 20:59:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4978 invoked by uid 99); 8 Dec 2014 20:59:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:59:39 +0000
X-ASF-Spam-Status: No, hits=4.4 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of huaiyin.thu@gmail.com designates 209.85.215.47 as permitted sender)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 08 Dec 2014 20:59:35 +0000
Received: by mail-la0-f47.google.com with SMTP id hz20so4723053lab.34
        for <dev@spark.incubator.apache.org>; Mon, 08 Dec 2014 12:59:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Vceu/o6dq4KJouyFkgkHMN9+onev57FVjrzY0JpQKog=;
        b=bKCPwC/zyi0s4eb0x6D1p0+CH3Khg0S8eEdWI0LPyfiBaWO58dkCLQcRSBpXOaxl6x
         j3oNTjasP1v8HkxFH4A7Ah8PFOU9MgWMXB7X7NGB3cTsZAeP8dmIs9PpXXe4TFSu29F1
         nGdWoRKIil81Bs72Q2/KxO3An04hh+DL+4Uca4MgqWoB35Cr7Wa3W983VjFQhAh7w5Hw
         jHwtzW/9Hm60i6dnhQwS/NY9NAeMqYXIi8zLR/H2rXeeKtonRST7wuTz8XyVDxddreUc
         1HMwj31MTEzmINW1ggvGMQbltxe26r6CoVqWMQd5OlXyjRPBIE9mTmU4aefN0ezQIdlK
         uSZg==
X-Received: by 10.112.146.37 with SMTP id sz5mr18305377lbb.27.1418072353550;
 Mon, 08 Dec 2014 12:59:13 -0800 (PST)
MIME-Version: 1.0
Received: by 10.112.56.37 with HTTP; Mon, 8 Dec 2014 12:58:53 -0800 (PST)
In-Reply-To: <1417625423553-9623.post@n3.nabble.com>
References: <1417625423553-9623.post@n3.nabble.com>
From: Yin Huai <huaiyin.thu@gmail.com>
Date: Mon, 8 Dec 2014 15:58:53 -0500
Message-ID: <CAOSFW127ka21vmuOZSqX4Nd95zH0O4o5JneKYb=w-mngbEpj7g@mail.gmail.com>
Subject: Re: scala.MatchError on SparkSQL when creating ArrayType of StructType
To: invkrh <invkrh@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a8b1ee636b00509bab2d3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8b1ee636b00509bab2d3
Content-Type: text/plain; charset=UTF-8

Seems you hit https://issues.apache.org/jira/browse/SPARK-4245. It was
fixed in 1.2.

Thanks,

Yin

On Wed, Dec 3, 2014 at 11:50 AM, invkrh <invkrh@gmail.com> wrote:

> Hi,
>
> I am using SparkSQL on 1.1.0 branch.
>
> The following code leads to a scala.MatchError
> at
>
> org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)
>
> val scm = StructType(*inputRDD*.schema.fields.init :+
>       StructField("list",
>         ArrayType(
>           StructType(
>             Seq(StructField("*date*", StringType, nullable = *false*),
>               StructField("*nbPurchase*", IntegerType, nullable =
> *false*)))),
>         nullable = false))
>
> // *purchaseRDD* is RDD[sql.ROW] whose schema is corresponding to scm. It
> is
> transformed from *inputRDD*
> val schemaRDD = hiveContext.applySchema(purchaseRDD, scm)
> schemaRDD.registerTempTable("t_purchase")
>
> Here's the stackTrace:
> scala.MatchError: ArrayType(StructType(List(StructField(date,StringType,
> *true* ), StructField(n_reachat,IntegerType, *true* ))),true) (of class
> org.apache.spark.sql.catalyst.types.ArrayType)
>         at
>
> org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)
>         at
> org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
>         at
> org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
>         at
>
> org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:84)
>         at
>
> org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:66)
>         at
>
> org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:50)
>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>         at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>         at
> org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org
> $apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:149)
>         at
>
> org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
>         at
>
> org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>         at org.apache.spark.scheduler.Task.run(Task.scala:54)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
>         at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:744)
>
> The strange thing is that *nullable* of *date* and *nbPurchase* field are
> set to true while it were false in the code. If I set both to *true*, it
> works. But, in fact, they should not be nullable.
>
> Here's what I find at Cast.scala:247 on 1.1.0 branch
>
>   private[this] lazy val cast: Any => Any = dataType match {
>     case StringType => castToString
>     case BinaryType => castToBinary
>     case DecimalType => castToDecimal
>     case TimestampType => castToTimestamp
>     case BooleanType => castToBoolean
>     case ByteType => castToByte
>     case ShortType => castToShort
>     case IntegerType => castToInt
>     case FloatType => castToFloat
>     case LongType => castToLong
>     case DoubleType => castToDouble
>   }
>
> Any idea? Thank you.
>
> Hao
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/scala-MatchError-on-SparkSQL-when-creating-ArrayType-of-StructType-tp9623.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b3a8b1ee636b00509bab2d3--

From dev-return-10703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 01:46:04 2014
Return-Path: <dev-return-10703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED004C75D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 01:46:03 +0000 (UTC)
Received: (qmail 5946 invoked by uid 500); 9 Dec 2014 01:46:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5868 invoked by uid 500); 9 Dec 2014 01:46:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5857 invoked by uid 99); 9 Dec 2014 01:46:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 01:46:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.182] (HELO mail-lb0-f182.google.com) (209.85.217.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 01:45:36 +0000
Received: by mail-lb0-f182.google.com with SMTP id f15so5192616lbj.13
        for <dev@spark.incubator.apache.org>; Mon, 08 Dec 2014 17:45:14 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=vsZySyhzGVUOmJwVclOEkeTuxisPAQTN3WRHnsfpHb8=;
        b=mPyCxtCmyx9byKUriwhrOLlfjBjc6c20c1e39Cx9KJNhHxx6+VWwPO8ZrURIFbqfUl
         pCjby6gvAcHJLPxHIBnoEH4hUFO1uYgRlw4vq9GfR6FuaLUIPnbCf+L8cf0I0NPGw4ps
         7JjQJrOVdKf/v0zwITY2xRJ0yy6sJukLjdyE6oZMRZS2IoY2jAPburbOmcurSDyAiccr
         DN4HZ1RC+vBzCxv2SOHZGOGMJNuowM89SNDb8WAioHyIpLa4B4fwqKamx6Me08KRgE8Z
         Gold14cIDKQKJ2hMxlPqj+i+R0tCw6p56QVj1zgzOHkGcg97aedcLUDlNSa//g2QF289
         jWKQ==
X-Gm-Message-State: ALoCoQnxXAxmrL1xNDsfTIpGwNQQKwnw4ETbvBec/aPIl1Or7pYd5tZFh1dK52HVCmdsqNHJg+5C
X-Received: by 10.152.18.166 with SMTP id x6mr19045339lad.18.1418089514671;
 Mon, 08 Dec 2014 17:45:14 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 8 Dec 2014 17:44:53 -0800 (PST)
In-Reply-To: <80833ADD533E324CA05C160E41B63661027AFC49@shsmsx102.ccr.corp.intel.com>
References: <1417809799843-9662.post@n3.nabble.com> <CAAswR-7-j1ZRd5qkfpEHQoEm7_PHXWrX6sHYjQPh2NBur6ZM=g@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027AFC49@shsmsx102.ccr.corp.intel.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 8 Dec 2014 17:44:53 -0800
Message-ID: <CAAswR-4yM_MEq6=sUPjuO0uRiDpLoXzf74SUaVOq-72SCH0GXA@mail.gmail.com>
Subject: Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: kb <kendb15@hotmail.com>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, Cheng Hao <chhao01@gmail.com>
Content-Type: multipart/alternative; boundary=089e01494210c83b890509beb19d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01494210c83b890509beb19d
Content-Type: text/plain; charset=UTF-8

This is merged now and should be fixed in the next 1.2 RC.

On Sat, Dec 6, 2014 at 8:28 PM, Cheng, Hao <hao.cheng@intel.com> wrote:

> I've created(reused) the PR https://github.com/apache/spark/pull/3336,
> hopefully we can fix this regression.
>
> Thanks for the reporting.
>
> Cheng Hao
>
> -----Original Message-----
> From: Michael Armbrust [mailto:michael@databricks.com]
> Sent: Saturday, December 6, 2014 4:51 AM
> To: kb
> Cc: dev@spark.incubator.apache.org; Cheng Hao
> Subject: Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0
>
> Thanks for reporting.  This looks like a regression related to:
> https://github.com/apache/spark/pull/2570
>
> I've filed it here: https://issues.apache.org/jira/browse/SPARK-4769
>
> On Fri, Dec 5, 2014 at 12:03 PM, kb <kendb15@hotmail.com> wrote:
>
> > I am having trouble getting "create table as select" or saveAsTable
> > from a hiveContext to work with temp tables in spark 1.2.  No issues
> > in 1.1.0 or
> > 1.1.1
> >
> > Simple modification to test case in the hive SQLQuerySuite.scala:
> >
> > test("double nested data") {
> >     sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
> > Nil).registerTempTable("nested")
> >     checkAnswer(
> >       sql("SELECT f1.f2.f3 FROM nested"),
> >       1)
> >     checkAnswer(sql("CREATE TABLE test_ctas_1234 AS SELECT * from
> > nested"),
> > Seq.empty[Row])
> >     checkAnswer(
> >       sql("SELECT * FROM test_ctas_1234"),
> >       sql("SELECT * FROM nested").collect().toSeq)
> >   }
> >
> >
> > output:
> >
> > 11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
> > org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not
> > found 'nested'
> >         at
> >
> >
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
> >         at
> >
> >
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
> >         at
> >
> >
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
> >         at
> >
> >
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
> >         at
> >
> >
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
> >         at
> >
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
> >         at
> > org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
> >         at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
> >         at
> org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply$mcV$sp(SQLQuerySuite.scala:122)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
> >         at
> >
> >
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
> >         at
> >
> >
> org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
> >         at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
> >         at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
> >         at org.scalatest.Transformer.apply(Transformer.scala:22)
> >         at org.scalatest.Transformer.apply(Transformer.scala:20)
> >         at
> org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
> >         at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
> >         at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
> >         at
> >
> >
> org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
> >         at
> >
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
> >         at
> >
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
> >         at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
> >         at
> org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
> >         at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
> >         at
> >
> >
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
> >         at
> >
> >
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
> >         at
> >
> >
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
> >         at
> >
> >
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
> >         at scala.collection.immutable.List.foreach(List.scala:318)
> >         at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
> >         at
> > org.scalatest.SuperEngine.org
> > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
> >         at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
> >         at
> > org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
> >         at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
> >         at org.scalatest.Suite$class.run(Suite.scala:1424)
> >         at
> > org.scalatest.FunSuite.org
> > $scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
> >         at
> > org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
> >         at
> > org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
> >         at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
> >         at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
> >         at org.scalatest.FunSuite.run(FunSuite.scala:1555)
> >         at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
> >         at
> >
> >
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
> >         at
> >
> >
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
> >         at scala.collection.immutable.List.foreach(List.scala:318)
> >         at
> > org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
> >         at
> >
> >
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
> >         at
> >
> >
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
> >         at
> >
> >
> org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
> >         at
> >
> >
> org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
> >         at org.scalatest.tools.Runner$.run(Runner.scala:883)
> >         at org.scalatest.tools.Runner.run(Runner.scala)
> >         at
> >
> >
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
> >         at
> >
> >
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
> >         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >         at
> >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> >         at
> >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> >         at java.lang.reflect.Method.invoke(Method.java:606)
> >         at
> > com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> > http://apache-spark-developers-list.1001551.n3.nabble.com/CREATE-TABLE
> > -AS-SELECT-does-not-work-with-temp-tables-in-1-2-0-tp9662.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
> > additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--089e01494210c83b890509beb19d--

From dev-return-10704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 05:33:44 2014
Return-Path: <dev-return-10704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8C8ABCDB9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 05:33:44 +0000 (UTC)
Received: (qmail 99754 invoked by uid 500); 9 Dec 2014 05:33:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99649 invoked by uid 500); 9 Dec 2014 05:33:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98117 invoked by uid 99); 9 Dec 2014 05:33:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 05:33:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jianshi.huang@gmail.com designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 05:33:36 +0000
Received: by mail-la0-f53.google.com with SMTP id gm9so5112384lab.26
        for <multiple recipients>; Mon, 08 Dec 2014 21:31:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=oDzbJEaNhyXktBi+JaIBQDWCmfCd5zDtiotq9jIXkd8=;
        b=Ox+J3nM/4Jd0wM1tl0eM11h7dE9vEYpOdWuCtGKM/Dcpm5UbnyJrwQ5Ls5QAJ8oYbK
         sLkLUNMUUNJQ9XN9EOU5Ar7KECZmv9MlZTTDopvFiJgM1xOSbtWugkMR7BMsBoawJkfe
         c4/622JfF5hTyFhLj8UFjceUS0x1fr7VYhRy0yGntfnBTjp1Nt5nIKeZUX47WcrROBf2
         7OJfUkg/p22JXhWophsz3Nb4xunKGhggoqrn3cD+Q/+kDO09nCF13/5GwjXf923etxcJ
         /jnRxuHycaJWeo7hr/ij0mvEY87z10OJjMuSOPK31LFfQY56U1N0EB0ZVatxq5fbC+fF
         XZbw==
X-Received: by 10.152.10.40 with SMTP id f8mr20335168lab.52.1418103104865;
 Mon, 08 Dec 2014 21:31:44 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.14.207 with HTTP; Mon, 8 Dec 2014 21:31:24 -0800 (PST)
In-Reply-To: <CAAswR-7=doK5JkUoxvPfTT0g0UmQ6hUMWHUZhxuA9WdhUobWNA@mail.gmail.com>
References: <CACA1tWJFdVw8+Mh20HZ8Dedjsq-L-fP9_jiFO=GCm7L=ryYCjg@mail.gmail.com>
 <CACA1tW+=HAmTtu0GgmZSgRj0bYY9v4uLBMrmFtroD_Snivh=KQ@mail.gmail.com>
 <CACA1tW+f4m+=8Pc2sXJTR9S6Jxxx-an1Dhqp=vhgJ4AWVXFmtw@mail.gmail.com>
 <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com>
 <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com> <CAAswR-7=doK5JkUoxvPfTT0g0UmQ6hUMWHUZhxuA9WdhUobWNA@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Tue, 9 Dec 2014 13:31:24 +0800
Message-ID: <CACA1tWLGnZ_RjtnZOdz+ZLDWMQiO7oA53o8J7G3FWz8EOSnbuA@mail.gmail.com>
Subject: Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)
To: Michael Armbrust <michael@databricks.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ed64d20acf0509c1db76
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ed64d20acf0509c1db76
Content-Type: text/plain; charset=UTF-8

Ah... I see. Thanks for pointing it out.

Then it means we cannot mount external table using customized column names.
hmm...

Then the only option left is to use a subquery to add a bunch of column
alias. I'll try it later.

Thanks,
Jianshi

On Tue, Dec 9, 2014 at 3:34 AM, Michael Armbrust <michael@databricks.com>
wrote:

> This is by hive's design.  From the Hive documentation:
>
> The column change command will only modify Hive's metadata, and will not
>> modify data. Users should make sure the actual data layout of the
>> table/partition conforms with the metadata definition.
>
>
>
> On Sat, Dec 6, 2014 at 8:28 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Ok, found another possible bug in Hive.
>>
>> My current solution is to use ALTER TABLE CHANGE to rename the column
>> names.
>>
>> The problem is after renaming the column names, the value of the columns
>> became all NULL.
>>
>> Before renaming:
>> scala> sql("select `sorted::cre_ts` from pmt limit 1").collect
>> res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])
>>
>> Execute renaming:
>> scala> sql("alter table pmt change `sorted::cre_ts` cre_ts string")
>> res13: org.apache.spark.sql.SchemaRDD =
>> SchemaRDD[972] at RDD at SchemaRDD.scala:108
>> == Query Plan ==
>> <Native command: executed by Hive>
>>
>> After renaming:
>> scala> sql("select cre_ts from pmt limit 1").collect
>> res16: Array[org.apache.spark.sql.Row] = Array([null])
>>
>> I created a JIRA for it:
>>
>>   https://issues.apache.org/jira/browse/SPARK-4781
>>
>>
>> Jianshi
>>
>> On Sun, Dec 7, 2014 at 1:06 AM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Hmm... another issue I found doing this approach is that ANALYZE TABLE
>>> ... COMPUTE STATISTICS will fail to attach the metadata to the table, and
>>> later broadcast join and such will fail...
>>>
>>> Any idea how to fix this issue?
>>>
>>> Jianshi
>>>
>>> On Sat, Dec 6, 2014 at 9:10 PM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Very interesting, the line doing drop table will throws an exception.
>>>> After removing it all works.
>>>>
>>>> Jianshi
>>>>
>>>> On Sat, Dec 6, 2014 at 9:11 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>>> wrote:
>>>>
>>>>> Here's the solution I got after talking with Liancheng:
>>>>>
>>>>> 1) using backquote `..` to wrap up all illegal characters
>>>>>
>>>>>     val rdd = parquetFile(file)
>>>>>     val schema = rdd.schema.fields.map(f => s"`${f.name}`
>>>>> ${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
>>>>>
>>>>>     val ddl_13 = s"""
>>>>>       |CREATE EXTERNAL TABLE $name (
>>>>>       |  $schema
>>>>>       |)
>>>>>       |STORED AS PARQUET
>>>>>       |LOCATION '$file'
>>>>>       """.stripMargin
>>>>>
>>>>>     sql(ddl_13)
>>>>>
>>>>> 2) create a new Schema and do applySchema to generate a new SchemaRDD,
>>>>> had to drop and register table
>>>>>
>>>>>     val t = table(name)
>>>>>     val newSchema = StructType(t.schema.fields.map(s => s.copy(name =
>>>>> s.name.replaceAll(".*?::", ""))))
>>>>>     sql(s"drop table $name")
>>>>>     applySchema(t, newSchema).registerTempTable(name)
>>>>>
>>>>> I'm testing it for now.
>>>>>
>>>>> Thanks for the help!
>>>>>
>>>>>
>>>>> Jianshi
>>>>>
>>>>> On Sat, Dec 6, 2014 at 8:41 AM, Jianshi Huang <jianshi.huang@gmail.com
>>>>> > wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I had to use Pig for some preprocessing and to generate Parquet files
>>>>>> for Spark to consume.
>>>>>>
>>>>>> However, due to Pig's limitation, the generated schema contains Pig's
>>>>>> identifier
>>>>>>
>>>>>> e.g.
>>>>>> sorted::id, sorted::cre_ts, ...
>>>>>>
>>>>>> I tried to put the schema inside CREATE EXTERNAL TABLE, e.g.
>>>>>>
>>>>>>   create external table pmt (
>>>>>>     sorted::id bigint
>>>>>>   )
>>>>>>   stored as parquet
>>>>>>   location '...'
>>>>>>
>>>>>> Obviously it didn't work, I also tried removing the identifier
>>>>>> sorted::, but the resulting rows contain only nulls.
>>>>>>
>>>>>> Any idea how to create a table in HiveContext from these Parquet
>>>>>> files?
>>>>>>
>>>>>> Thanks,
>>>>>> Jianshi
>>>>>> --
>>>>>> Jianshi Huang
>>>>>>
>>>>>> LinkedIn: jianshi
>>>>>> Twitter: @jshuang
>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>


-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--001a1132ed64d20acf0509c1db76--

From dev-return-10705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 18:51:20 2014
Return-Path: <dev-return-10705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87E81102A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 18:51:20 +0000 (UTC)
Received: (qmail 89093 invoked by uid 500); 9 Dec 2014 18:51:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89022 invoked by uid 500); 9 Dec 2014 18:51:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89011 invoked by uid 99); 9 Dec 2014 18:51:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 18:51:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 18:51:12 +0000
Received: by mail-la0-f43.google.com with SMTP id s18so1076386lam.30
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 10:49:21 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=lc0LMD0oQiIjDbYXKCBKfLORrKOkLQnO0XT5q4vMfN8=;
        b=UJKJ1MsWkQ0Up7IC3EbkThtfWIzE2e28hO1VnlPtNy+GbGt5eDc98HdGEMfyhKdc/g
         JKADcxca979FGQhsfsH9iPeNNOG+369yQga2aWSh0dqofbzk+Gb5N47kX8hMOtMZYGKA
         S4midD7quD7e2wcQ91QgFgvrkU494GAHn1FHqwFY8xZfwfxCox3HG8iTEVdCUX3BaNUQ
         AxHKoByIE3mieaJJKQLZTjQlNMfSQmbwCX7IWJZbwhH2f/apPTLfuLvKfqE4jMvbpNGn
         x06DFycdoIj8T39xwRF4oninQjhvPEgqPrCQY3fJRWR5hb8vLcK0w0wINc7jqwZbm5Bd
         zFbg==
X-Gm-Message-State: ALoCoQlZXpbuArpjoArRjHgDMm/TYxRh+bq+WGfbt+gKwBsWsc5ppuFUY8LdG/7aTLS2PV5r/JsC
X-Received: by 10.112.55.7 with SMTP id n7mr17322739lbp.49.1418150960957; Tue,
 09 Dec 2014 10:49:20 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Tue, 9 Dec 2014 10:49:00 -0800 (PST)
From: shane knapp <sknapp@berkeley.edu>
Date: Tue, 9 Dec 2014 10:49:00 -0800
Message-ID: <CACdU-dR+6LLgzMUSmN4FArHT--Wr3wbc_+HD1r=L6WgWvy1cSw@mail.gmail.com>
Subject: adding new jenkins worker nodes to eventually replace existing ones
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3d11e441f710509cd0062
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3d11e441f710509cd0062
Content-Type: text/plain; charset=UTF-8

i just turned up a new jenkins slave (amp-jenkins-worker-01) to ensure it
builds properly.  these machines have half the ram, same number of
processors and more disk, which will hopefully help us achieve more than
the ~15-20% system utilization we're getting on the current
amp-jenkins-slave-{01..05} nodes.

instead of 5 super beefy slaves w/16 workers each, we're planning on 8 less
beefy slaves w/12 workers each.  this should definitely cut down on the
build queue, and not impact build times in a negative way at all.

i'll keep a close eye on amp-jenkins-worker-01 before i start releasing the
other seven in to the wild.

there should be a minimal user impact, but if i happen to miss something,
please don't hesitate to let me know!

thanks,

shane

--001a11c3d11e441f710509cd0062--

From dev-return-10706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 19:38:49 2014
Return-Path: <dev-return-10706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B0F5109C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 19:38:49 +0000 (UTC)
Received: (qmail 53961 invoked by uid 500); 9 Dec 2014 19:38:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53880 invoked by uid 500); 9 Dec 2014 19:38:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53042 invoked by uid 99); 9 Dec 2014 19:38:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 19:38:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 19:38:17 +0000
Received: by mail-lb0-f174.google.com with SMTP id 10so1127261lbg.33
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 11:37:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=uRhlD8q7D9sonH+ioXHy3UtNDWsM0mp7hR9No9Yd3gI=;
        b=J41FmY4050g5rIVa4AA/o2x+8SvyszSOQfRUfX+YfHAx6StyGnfKq78MMrsuzt1VHU
         XvU19MCcoDXQps49dHP2DK3TPiflIGJUuJvVh5YSBVV/6ineM4j/QC9caUh3v6pXLYOu
         uh1sk5SM8G9Dc7U4Qe9kdfN4BuFaSBxEhaz2rVeZXBk9C/x+zwTDUykran2HAV2iSfJZ
         YR8QZPzLVKWrMfWMMklom3NGGJ/StLvm0KKGVfSKj0aiScqG72z9gFHAg2hZU/YzCbKs
         Fs+o8A63pjFsVmPA5aJT9DFciMMRjijjNcH00UH+sP4hZSC7x7K5vygNNEgaJ/GK7Ykg
         ZqTQ==
X-Gm-Message-State: ALoCoQmb8nbdy9xS68k8OKOOInWBoc3sbTz4g1vWmUjbB8jzZ30iTZCWlB6Q9x/xl13ASlUDzCkw
X-Received: by 10.112.132.2 with SMTP id oq2mr70568lbb.11.1418153875819; Tue,
 09 Dec 2014 11:37:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Tue, 9 Dec 2014 11:37:35 -0800 (PST)
In-Reply-To: <CACA1tWLGnZ_RjtnZOdz+ZLDWMQiO7oA53o8J7G3FWz8EOSnbuA@mail.gmail.com>
References: <CACA1tWJFdVw8+Mh20HZ8Dedjsq-L-fP9_jiFO=GCm7L=ryYCjg@mail.gmail.com>
 <CACA1tW+=HAmTtu0GgmZSgRj0bYY9v4uLBMrmFtroD_Snivh=KQ@mail.gmail.com>
 <CACA1tW+f4m+=8Pc2sXJTR9S6Jxxx-an1Dhqp=vhgJ4AWVXFmtw@mail.gmail.com>
 <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com>
 <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com>
 <CAAswR-7=doK5JkUoxvPfTT0g0UmQ6hUMWHUZhxuA9WdhUobWNA@mail.gmail.com> <CACA1tWLGnZ_RjtnZOdz+ZLDWMQiO7oA53o8J7G3FWz8EOSnbuA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 9 Dec 2014 11:37:35 -0800
Message-ID: <CAAswR-4YtZHTRDVjEhKERPvjfPmmHup+JoeYLFFnTFYPnKP9mw@mail.gmail.com>
Subject: Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)
To: Jianshi Huang <jianshi.huang@gmail.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a823a0162e30509cdae14
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a823a0162e30509cdae14
Content-Type: text/plain; charset=UTF-8

You might also try out the recently added support for views.

On Mon, Dec 8, 2014 at 9:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
wrote:

> Ah... I see. Thanks for pointing it out.
>
> Then it means we cannot mount external table using customized column
> names. hmm...
>
> Then the only option left is to use a subquery to add a bunch of column
> alias. I'll try it later.
>
> Thanks,
> Jianshi
>
> On Tue, Dec 9, 2014 at 3:34 AM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> This is by hive's design.  From the Hive documentation:
>>
>> The column change command will only modify Hive's metadata, and will not
>>> modify data. Users should make sure the actual data layout of the
>>> table/partition conforms with the metadata definition.
>>
>>
>>
>> On Sat, Dec 6, 2014 at 8:28 PM, Jianshi Huang <jianshi.huang@gmail.com>
>> wrote:
>>
>>> Ok, found another possible bug in Hive.
>>>
>>> My current solution is to use ALTER TABLE CHANGE to rename the column
>>> names.
>>>
>>> The problem is after renaming the column names, the value of the columns
>>> became all NULL.
>>>
>>> Before renaming:
>>> scala> sql("select `sorted::cre_ts` from pmt limit 1").collect
>>> res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])
>>>
>>> Execute renaming:
>>> scala> sql("alter table pmt change `sorted::cre_ts` cre_ts string")
>>> res13: org.apache.spark.sql.SchemaRDD =
>>> SchemaRDD[972] at RDD at SchemaRDD.scala:108
>>> == Query Plan ==
>>> <Native command: executed by Hive>
>>>
>>> After renaming:
>>> scala> sql("select cre_ts from pmt limit 1").collect
>>> res16: Array[org.apache.spark.sql.Row] = Array([null])
>>>
>>> I created a JIRA for it:
>>>
>>>   https://issues.apache.org/jira/browse/SPARK-4781
>>>
>>>
>>> Jianshi
>>>
>>> On Sun, Dec 7, 2014 at 1:06 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Hmm... another issue I found doing this approach is that ANALYZE TABLE
>>>> ... COMPUTE STATISTICS will fail to attach the metadata to the table, and
>>>> later broadcast join and such will fail...
>>>>
>>>> Any idea how to fix this issue?
>>>>
>>>> Jianshi
>>>>
>>>> On Sat, Dec 6, 2014 at 9:10 PM, Jianshi Huang <jianshi.huang@gmail.com>
>>>> wrote:
>>>>
>>>>> Very interesting, the line doing drop table will throws an exception.
>>>>> After removing it all works.
>>>>>
>>>>> Jianshi
>>>>>
>>>>> On Sat, Dec 6, 2014 at 9:11 AM, Jianshi Huang <jianshi.huang@gmail.com
>>>>> > wrote:
>>>>>
>>>>>> Here's the solution I got after talking with Liancheng:
>>>>>>
>>>>>> 1) using backquote `..` to wrap up all illegal characters
>>>>>>
>>>>>>     val rdd = parquetFile(file)
>>>>>>     val schema = rdd.schema.fields.map(f => s"`${f.name}`
>>>>>> ${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
>>>>>>
>>>>>>     val ddl_13 = s"""
>>>>>>       |CREATE EXTERNAL TABLE $name (
>>>>>>       |  $schema
>>>>>>       |)
>>>>>>       |STORED AS PARQUET
>>>>>>       |LOCATION '$file'
>>>>>>       """.stripMargin
>>>>>>
>>>>>>     sql(ddl_13)
>>>>>>
>>>>>> 2) create a new Schema and do applySchema to generate a new
>>>>>> SchemaRDD, had to drop and register table
>>>>>>
>>>>>>     val t = table(name)
>>>>>>     val newSchema = StructType(t.schema.fields.map(s => s.copy(name =
>>>>>> s.name.replaceAll(".*?::", ""))))
>>>>>>     sql(s"drop table $name")
>>>>>>     applySchema(t, newSchema).registerTempTable(name)
>>>>>>
>>>>>> I'm testing it for now.
>>>>>>
>>>>>> Thanks for the help!
>>>>>>
>>>>>>
>>>>>> Jianshi
>>>>>>
>>>>>> On Sat, Dec 6, 2014 at 8:41 AM, Jianshi Huang <
>>>>>> jianshi.huang@gmail.com> wrote:
>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> I had to use Pig for some preprocessing and to generate Parquet
>>>>>>> files for Spark to consume.
>>>>>>>
>>>>>>> However, due to Pig's limitation, the generated schema contains
>>>>>>> Pig's identifier
>>>>>>>
>>>>>>> e.g.
>>>>>>> sorted::id, sorted::cre_ts, ...
>>>>>>>
>>>>>>> I tried to put the schema inside CREATE EXTERNAL TABLE, e.g.
>>>>>>>
>>>>>>>   create external table pmt (
>>>>>>>     sorted::id bigint
>>>>>>>   )
>>>>>>>   stored as parquet
>>>>>>>   location '...'
>>>>>>>
>>>>>>> Obviously it didn't work, I also tried removing the identifier
>>>>>>> sorted::, but the resulting rows contain only nulls.
>>>>>>>
>>>>>>> Any idea how to create a table in HiveContext from these Parquet
>>>>>>> files?
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Jianshi
>>>>>>> --
>>>>>>> Jianshi Huang
>>>>>>>
>>>>>>> LinkedIn: jianshi
>>>>>>> Twitter: @jshuang
>>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Jianshi Huang
>>>>>>
>>>>>> LinkedIn: jianshi
>>>>>> Twitter: @jshuang
>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>>
>>> --
>>> Jianshi Huang
>>>
>>> LinkedIn: jianshi
>>> Twitter: @jshuang
>>> Github & Blog: http://huangjs.github.com/
>>>
>>
>>
>
>
> --
> Jianshi Huang
>
> LinkedIn: jianshi
> Twitter: @jshuang
> Github & Blog: http://huangjs.github.com/
>

--047d7b3a823a0162e30509cdae14--

From dev-return-10707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 20:07:22 2014
Return-Path: <dev-return-10707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7483310D5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 20:07:22 +0000 (UTC)
Received: (qmail 60382 invoked by uid 500); 9 Dec 2014 20:07:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60316 invoked by uid 500); 9 Dec 2014 20:07:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60299 invoked by uid 99); 9 Dec 2014 20:07:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 20:07:20 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Brennon.York@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail01.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 20:06:55 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=3280; q=dns/txt; s=SM2048Apr2013K;
  t=1418155634; x=1418242034;
  h=from:to:cc:date:subject:message-id:references:
   in-reply-to:mime-version:content-transfer-encoding;
  bh=iuomr1GOkcE7cTbIX+selvHvajfqxNcICYFuYxb+54k=;
  b=X0ed3wZu0hagcqbN8AX4ExHBaM+LGwgNQG/fH+nrTm5udVHGoyUuiGU3
   ZZpnYit4LcblDJ1I536r/NXw7wGVajJyDniLBqUID+ygNCOUMHk2wObL+
   SMaiGoPyiH+Y1uionx6u1Gyo1qLGsdwPfikuHNt9GfH9g61qJctCeSIR0
   W7LA6HNkptwcIHNjj843KbCiBbGVV6B0eppRDjJajXXXchTwYywNJliNM
   w5Pq1MOo1iaDPsjbmsu9vAXWZGYdsC/OEaeZu/mDhfuvOyuEHqVY91L9D
   Eq6vt9l6HRIFNRBkOKdiqVxL7RgHlyjvNjy1hSuHsHq4WrdwGSwWaBDpu
   w==;
X-IronPort-AV: E=McAfee;i="5600,1067,7646"; a="183839321"
X-IronPort-AV: E=Sophos;i="5.07,547,1413259200"; 
   d="scan'208";a="183839321"
X-OSD_Exception: TRUE
Received: from kdcpexcasht01.cof.ds.capitalone.com ([10.37.194.11])
  by komail01.kdc.capitalone.com with ESMTP; 09 Dec 2014 15:05:53 -0500
Received: from KDCPEXCMB02.cof.ds.capitalone.com ([169.254.1.11]) by
 KDCPEXCASHT01.cof.ds.capitalone.com ([10.37.194.11]) with mapi; Tue, 9 Dec
 2014 15:05:52 -0500
From: "York, Brennon" <Brennon.York@capitalone.com>
To: Patrick Wendell <pwendell@gmail.com>, Ryan Williams
	<ryan.blake.williams@gmail.com>
CC: Nicholas Chammas <nicholas.chammas@gmail.com>, Sean Owen
	<sowen@cloudera.com>, dev <dev@spark.apache.org>
Date: Tue, 9 Dec 2014 15:05:58 -0500
Subject: Re: zinc invocation examples
Thread-Topic: zinc invocation examples
Thread-Index: AdAT64i4pF/CJnjKRIuA3LxKSZtMNQ==
Message-ID: <D0AC9593.7E33%brennon.york@capitalone.com>
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
 <CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com>
 <CAOhmDzddsPU6sXtFxwkxtvWj+H0va=3QQHspPqWUp-OvM4v74g@mail.gmail.com>
 <CANeJXFPMCQ54p0L7DNQpeOwmVsgGL=TfZLACE9o=kgA+qko8Dg@mail.gmail.com>
 <CABPQxsuq_-CyknzRNhe6DDeZ_6ddArzV9Z4pLgaodZPL6Uz3pw@mail.gmail.com>
In-Reply-To: <CABPQxsuq_-CyknzRNhe6DDeZ_6ddArzV9Z4pLgaodZPL6Uz3pw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.6.141106
acceptlanguage: en-US
Content-Type: text/plain; charset="iso-8859-1"
MIME-Version: 1.0
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Patrick, I=B9ve nearly completed a basic build out for the SPARK-4501 issue
(at https://github.com/brennonyork/spark/tree/SPARK-4501) and would be
great to get your initial read on it. Per this thread I need to add in the
-scala-home call to zinc, but its close to ready for a PR.

On 12/5/14, 2:10 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:

>One thing I created a JIRA for a while back was to have a similar
>script to "sbt/sbt" that transparently downloads Zinc, Scala, and
>Maven in a subdirectory of Spark and sets it up correctly. I.e.
>"build/mvn".
>
>Outside of brew for MacOS there aren't good Zinc packages, and it's a
>pain to figure out how to set it up.
>
>https://issues.apache.org/jira/browse/SPARK-4501
>
>Prashant Sharma looked at this for a bit but I don't think he's
>working on it actively any more, so if someone wanted to do this, I'd
>be extremely grateful.
>
>- Patrick
>
>On Fri, Dec 5, 2014 at 11:05 AM, Ryan Williams
><ryan.blake.williams@gmail.com> wrote:
>> fwiw I've been using `zinc -scala-home $SCALA_HOME -nailed -start`
>>which:
>>
>> - starts a nailgun server as well,
>> - uses my installed scala 2.{10,11}, as opposed to zinc's default 2.9.2
>> <https://github.com/typesafehub/zinc#scala>: "If no options are passed
>>to
>> locate a version of Scala then Scala 2.9.2 is used by default (which is
>> bundled with zinc)."
>>
>> The latter seems like it might be especially important.
>>
>>
>> On Thu Dec 04 2014 at 4:25:32 PM Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> Oh, derp. I just assumed from looking at all the options that there was
>>> something to it. Thanks Sean.
>>>
>>> On Thu Dec 04 2014 at 7:47:33 AM Sean Owen <sowen@cloudera.com> wrote:
>>>
>>> > You just run it once with "zinc -start" and leave it running as a
>>> > background process on your build machine. You don't have to do
>>> > anything for each build.
>>> >
>>> > On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
>>> > <nicholas.chammas@gmail.com> wrote:
>>> > > https://github.com/apache/spark/blob/master/docs/
>>> > building-spark.md#speeding-up-compilation-with-zinc
>>> > >
>>> > > Could someone summarize how they invoke zinc as part of a regular
>>> > > build-test-etc. cycle?
>>> > >
>>> > > I'll add it in to the aforelinked page if appropriate.
>>> > >
>>> > > Nick
>>> >
>>>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 20:40:22 2014
Return-Path: <dev-return-10708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38A239179
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 20:40:22 +0000 (UTC)
Received: (qmail 59595 invoked by uid 500); 9 Dec 2014 20:40:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59523 invoked by uid 500); 9 Dec 2014 20:40:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59509 invoked by uid 99); 9 Dec 2014 20:40:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 20:40:19 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Ilya.Ganelin@capitalone.com designates 204.63.55.164 as permitted sender)
Received: from [204.63.55.164] (HELO pomail03.capitalone.com) (204.63.55.164)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 20:40:15 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=5268; q=dns/txt; s=SM2048Apr2013P;
  t=1418157615; x=1418244015;
  h=from:to:date:subject:message-id:mime-version;
  bh=0ZtTyluD56WhOVA9eXixRqMlWwxd2taAciqG410rIYM=;
  b=DjNO3AwS9MHDgYQPt6rQsLmJes2wnqdaBx+684w49OzkdLlMpLO7oqPH
   jMZlSOxzM7JfHSfaJiOro8DLhZTmgH0m6Ujv3C5yJ9b9PkrcRa+FybsUd
   cmeQvIqrAFvDM/OmyI2GfRZcjbFODBwc4CN+vF4J+5ZI9JkOwF8lwdgFG
   JYM3yfvWmqBQU9m2MwrVr4NvfJK/IjgaCMGNKNHqLXYe3KZ9gocVsDPBE
   Z5NofS7WAcY4gB082Osespj0gQKkiKCzt+olJQk+nJd/2tPT+00PBxjw3
   wvrTaaWdQJ9g2J8ilCpiWEqZKokpg2vQaPiyqX7ylUUAvVxmqg60Cd1Gs
   g==;
X-IronPort-AV: E=McAfee;i="5600,1067,7646"; a="62642632"
X-IronPort-AV: E=Sophos;i="5.07,547,1413259200"; 
   d="scan'208,217";a="62642632"
X-HTML-Disclaimer: True
Received: from kdcpexcasht01.cof.ds.capitalone.com ([10.37.194.11])
  by pomail03.kdc.capitalone.com with ESMTP; 09 Dec 2014 15:38:53 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 KDCPEXCASHT01.cof.ds.capitalone.com ([10.37.194.11]) with mapi; Tue, 9 Dec
 2014 15:38:53 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: dev <dev@spark.apache.org>
Date: Tue, 9 Dec 2014 15:38:52 -0500
Subject: Adding RDD function to segment an RDD (like substring)
Thread-Topic: Adding RDD function to segment an RDD (like substring)
Thread-Index: AdAT8CTe/ifAt/OSSr6HdUptML9NQQ==
Message-ID: <D0AC9D8A.6C80%ilya.ganelin@capitalone.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.3.140616
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_D0AC9D8A6C80ilyaganelincapitalonecom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D0AC9D8A6C80ilyaganelincapitalonecom_
MIME-Version: 1.0
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all =96 a utility that I=92ve found useful several times now when workin=
g with RDDs is to be able to reason about segments of the RDD.

For example, if I have two large RDDs and I want to combine them in a way t=
hat would be intractable in terms of memory or disk storage (e.g. A cartesi=
an) but a piece-wise approach is tractable, there are not many methods that=
 enable this.

Existing relevant methods are :
takeSample =96 Which allows me to take a number of random values from an RD=
D but does not let me process the entire RDD
randomSplit =96 Which segments the RDD into an array of smaller RDDs =96 wi=
th this, however, I=92ve seen numerous memory issues during execution on la=
rger datasets or when splitting into many RDDs
forEach or collect =96 Which require the RDD to fit into memory =96 which i=
s not possible for larger datasets

What I am proposing is the addition of a simple method to operate on an RDD=
 which would operate equivalently to a substring operation in String:

For an RDD[T]:

def sample(startIdx : Int, endIdx : Int) : RDD[T] =3D {
val zipped =3D this.zipWithIndex()
val sampled =3D filter(idx =3D> (idx >=3D startIdx) && (idx < endIdx))
sampled.map(_._1)
}

I would appreciate any feedback =96 thank you!


________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.

--_000_D0AC9D8A6C80ilyaganelincapitalonecom_--


From dev-return-10709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 21:17:21 2014
Return-Path: <dev-return-10709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7216A9861
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 21:17:21 +0000 (UTC)
Received: (qmail 88213 invoked by uid 500); 9 Dec 2014 21:17:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88139 invoked by uid 500); 9 Dec 2014 21:17:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88128 invoked by uid 99); 9 Dec 2014 21:17:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 21:17:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 21:16:53 +0000
Received: by mail-wg0-f50.google.com with SMTP id a1so2007644wgh.9
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 13:16:52 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=2g2CL8VzBc0BAXs/EnjzjLpKTSQpdOGSkM847hXM4hA=;
        b=ZMYy44f8EjbhJEmOhw+SzUcdnQq1lQTRPilLKRwxTeT3EWI/O8XC9+50j3dXoMMfUl
         qd8T5R63/UQ8cazjjjj0dVhsIf01hxxKH7+4uYn/5cS8a5j+J10hwBAlw+S7hU1dh0JC
         QsAW60pdaAM8dsl/bUVDpaX7FPFO5A4ZzXaNFOYqtYFkfTOFYuFbLnUh5f9YYg/MIqpx
         CJLhi9n/nuIeCvY6XnGDErDrvWqoEbzyPFr2QVzwZPuc8ZJy+sxhgvLpo1UTHgEEXXhd
         XlFNNSfafAq+BfFFh1MM3tbFSG33bfnueedZB2WN8cEyWEj/0nRsIrEvWZCMafd3TiEf
         839w==
X-Gm-Message-State: ALoCoQm1lohB/YSQyH4puH02RTkyEjB0XdVbqNIm1hHco3FeeiO7rrWFGa/yYtLJIbjsh1xSr1Lc
MIME-Version: 1.0
X-Received: by 10.194.80.38 with SMTP id o6mr668183wjx.126.1418159812021; Tue,
 09 Dec 2014 13:16:52 -0800 (PST)
Received: by 10.216.68.137 with HTTP; Tue, 9 Dec 2014 13:16:51 -0800 (PST)
In-Reply-To: <D0AC9D8A.6C80%ilya.ganelin@capitalone.com>
References: <D0AC9D8A.6C80%ilya.ganelin@capitalone.com>
Date: Tue, 9 Dec 2014 13:16:51 -0800
Message-ID: <CAAsvFP=RgChSZ_1ZfLJQx6Jq-35sk-OoFq4asXpXLaxGHbhRVw@mail.gmail.com>
Subject: Re: Adding RDD function to segment an RDD (like substring)
From: Mark Hamstra <mark@clearstorydata.com>
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb04db0d4abdf0509cf0f28
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04db0d4abdf0509cf0f28
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

`zipWithIndex` is both compute intensive and breaks Spark's
"transformations are lazy" model, so it is probably not appropriate to add
this to the public RDD API.  If `zipWithIndex` weren't already what I
consider to be broken, I'd be much friendlier to building something more on
top of it, but I really don't like `zipWithIndex` or this `sample` idea of
yours as they stand.  Aside from those concerns, the name `sample` doesn't
really work since it is too easy to assume that it means some kind of
random sampling.

None of that is to say that you can't make effective use of `zipWithIndex`
and your `sample` in your own code; but I'm not a fan of extending the
Spark public API in this way.

On Tue, Dec 9, 2014 at 12:38 PM, Ganelin, Ilya <Ilya.Ganelin@capitalone.com=
>
wrote:

> Hi all =E2=80=93 a utility that I=E2=80=99ve found useful several times n=
ow when working
> with RDDs is to be able to reason about segments of the RDD.
>
> For example, if I have two large RDDs and I want to combine them in a way
> that would be intractable in terms of memory or disk storage (e.g. A
> cartesian) but a piece-wise approach is tractable, there are not many
> methods that enable this.
>
> Existing relevant methods are :
> takeSample =E2=80=93 Which allows me to take a number of random values fr=
om an RDD
> but does not let me process the entire RDD
> randomSplit =E2=80=93 Which segments the RDD into an array of smaller RDD=
s =E2=80=93 with
> this, however, I=E2=80=99ve seen numerous memory issues during execution =
on larger
> datasets or when splitting into many RDDs
> forEach or collect =E2=80=93 Which require the RDD to fit into memory =E2=
=80=93 which is
> not possible for larger datasets
>
> What I am proposing is the addition of a simple method to operate on an
> RDD which would operate equivalently to a substring operation in String:
>
> For an RDD[T]:
>
> def sample(startIdx : Int, endIdx : Int) : RDD[T] =3D {
> val zipped =3D this.zipWithIndex()
> val sampled =3D filter(idx =3D> (idx >=3D startIdx) && (idx < endIdx))
> sampled.map(_._1)
> }
>
> I would appreciate any feedback =E2=80=93 thank you!
>
>
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or
> proprietary to Capital One and/or its affiliates. The information
> transmitted herewith is intended only for use by the individual or entity
> to which it is addressed.  If the reader of this message is not the
> intended recipient, you are hereby notified that any review,
> retransmission, dissemination, distribution, copying or other use of, or
> taking of any action in reliance upon this information is strictly
> prohibited. If you have received this communication in error, please
> contact the sender and delete the material from your computer.
>

--047d7bb04db0d4abdf0509cf0f28--

From dev-return-10710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 21:54:41 2014
Return-Path: <dev-return-10710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E87EF9C02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 21:54:40 +0000 (UTC)
Received: (qmail 25020 invoked by uid 500); 9 Dec 2014 21:54:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24944 invoked by uid 500); 9 Dec 2014 21:54:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24933 invoked by uid 99); 9 Dec 2014 21:54:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 21:54:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 21:54:13 +0000
Received: by mail-wi0-f176.google.com with SMTP id ex7so9340267wid.15
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 13:53:28 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=UvXVtqjWF/lef2NqJo/NWLyDhhG7oPbp/5uCzZ2OjPU=;
        b=Cv5dGIC17FFgrA2jhwZ2J0V4on2TAgOK55szFS+PBV9rg60kb4sppZ7FTTXeNJCHMB
         6nuNlGiehcRF917cM5P7XtmuBB3amE+lRJow4wfcc6g6w8zrIt6kXD9BM9beQ0z+XoFw
         UJ1hUiprCJNSmzUX3EQh3MICDbyFoTWMpgopAuWGSG7LZRISzbxxXKswokn3hyhY2Wdb
         cVGb7HPZZTUN1ytaoTBM2haQzQU0C/SjEYDZTiQPIKokXE4tD9IuGLpEDvX+T28Yh2zD
         XH1magUTYHp8TKjac1pU95LR0LGakuKuZr3lTztDhWrRtkTL7KnDn9ueWr4F0EY7pOPQ
         9EHQ==
X-Gm-Message-State: ALoCoQmP+au73U19BAY8zFaOOICE+4GiSPt+iAyir65d1whl02mASKnD4XAqArow0iJ1mFLTr7iw
X-Received: by 10.194.190.19 with SMTP id gm19mr793729wjc.51.1418162008288;
 Tue, 09 Dec 2014 13:53:28 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Tue, 9 Dec 2014 13:53:08 -0800 (PST)
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 9 Dec 2014 21:53:08 +0000
Message-ID: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
Subject: Is this a little bug in BlockTransferMessage ?
To: aaron@databricks.com, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

https://github.com/apache/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockTransferMessage.java#L70

public byte[] toByteArray() {
  ByteBuf buf = Unpooled.buffer(encodedLength());
  buf.writeByte(type().id);
  encode(buf);
  assert buf.writableBytes() == 0 : "Writable bytes remain: " +
buf.writableBytes();
  return buf.array();
}

Running the Java tests at last might have turned up a little bug here,
but wanted to check. This makes a buffer to hold enough bytes to
encode the message. But it writes 1 byte, plus the message. This makes
the buffer expand, and then does have nonzero capacity afterwards, so
the assert fails.

So just needs a "+ 1" in the size?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 22:34:22 2014
Return-Path: <dev-return-10711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 95716C050
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 22:34:22 +0000 (UTC)
Received: (qmail 73622 invoked by uid 500); 9 Dec 2014 22:34:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73544 invoked by uid 500); 9 Dec 2014 22:34:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73532 invoked by uid 99); 9 Dec 2014 22:34:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 22:34:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 22:34:15 +0000
Received: by mail-wi0-f177.google.com with SMTP id l15so3337426wiw.10
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 14:33:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Nfx34pCVEGWMPdufJOYoaESlAKKhQzVH1x5qZrlW6qc=;
        b=kZed/8XTG9+ozWsbpP15i29P11OQSSzYg9qQOfsRspQ9mOFyu1yeS6Pn4cowVVhmT7
         dprkjhc3iAmdweMMaLDTw4HRPLweNY0tZpD0nlADNzrUcbcfqjeLmVabeOTgWMPih3yw
         ZqbaMsCXDuumxcH6Osj/4kLdEcc3oM8uaAVJn9/6YuUo4h1PeMuoKu+OnwkUZvtx3MQB
         Rcqi0Lkrs50eUFEVOzObqcv6csAxW/rVRgLEMbgW9cs4SMy4s250UuZKjbBoqcGRNNWh
         GkhiXhTrTV14awKbpO4UbyYc78VZkyHK7ZlN697/dV5pM4MC9ht7mLI1C5Rz6KKrs/uE
         N92Q==
X-Gm-Message-State: ALoCoQn6Io6Xa6xKpvPMd50n7Q/n6AqLdqkgHsGOnz7iidihwTVn3tIQ79xbWPEbW02VQnpJIYsM
X-Received: by 10.194.8.34 with SMTP id o2mr903098wja.129.1418164434731; Tue,
 09 Dec 2014 14:33:54 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Tue, 9 Dec 2014 14:33:34 -0800 (PST)
In-Reply-To: <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
 <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 9 Dec 2014 22:33:34 +0000
Message-ID: <CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
To: Aaron Davidson <aaron@databricks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yep, will do. The test does catch it -- it's just not being executed.
I think I have a reasonable start on re-enabling surefire + Java tests
for SPARK-4159.

On Tue, Dec 9, 2014 at 10:30 PM, Aaron Davidson <aaron@databricks.com> wrote:
> Oops, that does look like a bug. Strange that the BlockTransferMessageSuite
> did not catch this. "+1" sounds like the right solution, would you be able
> to submit a PR?
>
> On Tue, Dec 9, 2014 at 1:53 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>>
>> https://github.com/apache/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockTransferMessage.java#L70
>>
>> public byte[] toByteArray() {
>>   ByteBuf buf = Unpooled.buffer(encodedLength());
>>   buf.writeByte(type().id);
>>   encode(buf);
>>   assert buf.writableBytes() == 0 : "Writable bytes remain: " +
>> buf.writableBytes();
>>   return buf.array();
>> }
>>
>> Running the Java tests at last might have turned up a little bug here,
>> but wanted to check. This makes a buffer to hold enough bytes to
>> encode the message. But it writes 1 byte, plus the message. This makes
>> the buffer expand, and then does have nonzero capacity afterwards, so
>> the assert fails.
>>
>> So just needs a "+ 1" in the size?
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 23:05:01 2014
Return-Path: <dev-return-10712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59B8AC436
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 23:05:01 +0000 (UTC)
Received: (qmail 79637 invoked by uid 500); 9 Dec 2014 23:04:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79563 invoked by uid 500); 9 Dec 2014 23:04:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79552 invoked by uid 99); 9 Dec 2014 23:04:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:04:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.176 as permitted sender)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:04:53 +0000
Received: by mail-lb0-f176.google.com with SMTP id p9so1406923lbv.35
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 15:04:32 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=GJYh0MA6N8tFtuQicpjrftIE7O4PkZOErLtiLxj8oH8=;
        b=maOALroZF18NUJ9FO6bNjM8o84jnfT6eY68XMNcqiMus8vSPUKHNW8kT94cr32k4an
         MAAnTxx4w6NaT+SYl/2Ys5WBzZqQeNTVQwlLB2A/ymWDcl/tz+s8Yt45rwG0VkKusiUz
         Ea3Pwweu7ifnj46IDznxCtdL8kzqfsAfAa8ZU4uHKE/Ee4Rqq6qKwUXsLwuzaZnhy1EZ
         1GYDfzvTzBZo3dMpPovXtzMhpZSHZzxuvJ0aubhIc+MrS5HbamB3M2IFWFcZWi505tyV
         JIbjQHTvQeFvVpMF+tbq8FsWG8EXaFNko4ePKlcgrfn/iSQyIBLfUNRRa7kUwS/WVMrr
         VR9Q==
X-Gm-Message-State: ALoCoQmsWwZ3B2xW0Br6tIwUXCaXUWV2zoh0QvylukOhM5UT9cgl8VRyMg6+qPThjEDpsE+CrtTk
X-Received: by 10.152.45.41 with SMTP id j9mr826115lam.59.1418166272038; Tue,
 09 Dec 2014 15:04:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Tue, 9 Dec 2014 15:04:11 -0800 (PST)
In-Reply-To: <CACdU-dR+6LLgzMUSmN4FArHT--Wr3wbc_+HD1r=L6WgWvy1cSw@mail.gmail.com>
References: <CACdU-dR+6LLgzMUSmN4FArHT--Wr3wbc_+HD1r=L6WgWvy1cSw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Tue, 9 Dec 2014 15:04:11 -0800
Message-ID: <CACdU-dS9GsUVwT1HQz=cPrHQfBGbUBk9MyfRcNpFq36HAgcoSw@mail.gmail.com>
Subject: Re: adding new jenkins worker nodes to eventually replace existing ones
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c29bcae0adb60509d0908c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29bcae0adb60509d0908c
Content-Type: text/plain; charset=UTF-8

forgot to install git on this node.  /headdesk

i retirggered the failed spark prb jobs.

On Tue, Dec 9, 2014 at 10:49 AM, shane knapp <sknapp@berkeley.edu> wrote:

> i just turned up a new jenkins slave (amp-jenkins-worker-01) to ensure it
> builds properly.  these machines have half the ram, same number of
> processors and more disk, which will hopefully help us achieve more than
> the ~15-20% system utilization we're getting on the current
> amp-jenkins-slave-{01..05} nodes.
>
> instead of 5 super beefy slaves w/16 workers each, we're planning on 8
> less beefy slaves w/12 workers each.  this should definitely cut down on
> the build queue, and not impact build times in a negative way at all.
>
> i'll keep a close eye on amp-jenkins-worker-01 before i start releasing
> the other seven in to the wild.
>
> there should be a minimal user impact, but if i happen to miss something,
> please don't hesitate to let me know!
>
> thanks,
>
> shane
>

--001a11c29bcae0adb60509d0908c--

From dev-return-10713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 23:31:13 2014
Return-Path: <dev-return-10713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFB89C6E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 23:31:13 +0000 (UTC)
Received: (qmail 50766 invoked by uid 500); 9 Dec 2014 23:31:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50683 invoked by uid 500); 9 Dec 2014 23:31:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50671 invoked by uid 99); 9 Dec 2014 23:31:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:31:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:31:08 +0000
Received: by mail-ig0-f174.google.com with SMTP id hn15so5496036igb.13
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 15:30:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=6NaogjeqZFp2BTklqpDG67pNwGh7RslprM+rLi5G+OA=;
        b=ZWJ65LwDySUc3Hy9IM6KLFCPNWS/syWsjUAelrRBckE+b4Vh53vwKWMeXyR399cqDO
         5fUef3xcHBBzsqLVAAufn6vbz+go++6aLfW7oVjmY5EyFA8cwvj944heNZAvyIJHNn59
         c2bKxdMbn74sIV+d/Aw/zdmT7tSlAAZSz9FeMleA7FXBNM/GcZmVoALGNOs4wf7R8PRz
         LdNMf+Axq+4N+ucUFqTA65xx9zq447tIpA2ahI/HAHVbHf/ki0Ez4r78aXTbynkrD6wj
         xkm27TeHbl1uZnRpfaUYXOnKx/wilwR5Se7gbZ8beol8mhbhkqWUx/bQ5cDrXsa97zaY
         mbZQ==
X-Received: by 10.107.136.92 with SMTP id k89mr1050885iod.43.1418167847665;
 Tue, 09 Dec 2014 15:30:47 -0800 (PST)
MIME-Version: 1.0
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
 <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com> <CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 09 Dec 2014 23:30:46 +0000
Message-ID: <CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
To: Sean Owen <sowen@cloudera.com>, Aaron Davidson <aaron@databricks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ecad4cac6db0509d0ee99
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ecad4cac6db0509d0ee99
Content-Type: text/plain; charset=UTF-8

So all this time the tests that Jenkins has been running via Jenkins and
SBT + ScalaTest... those haven't been running any of the Java unit tests?

SPARK-4159 <https://issues.apache.org/jira/browse/SPARK-4159> only mentions
Maven as a problem, but I'm wondering how these tests got through Jenkins
OK.

On Tue Dec 09 2014 at 5:34:22 PM Sean Owen <sowen@cloudera.com> wrote:

> Yep, will do. The test does catch it -- it's just not being executed.
> I think I have a reasonable start on re-enabling surefire + Java tests
> for SPARK-4159.
>
> On Tue, Dec 9, 2014 at 10:30 PM, Aaron Davidson <aaron@databricks.com>
> wrote:
> > Oops, that does look like a bug. Strange that the
> BlockTransferMessageSuite
> > did not catch this. "+1" sounds like the right solution, would you be
> able
> > to submit a PR?
> >
> > On Tue, Dec 9, 2014 at 1:53 PM, Sean Owen <sowen@cloudera.com> wrote:
> >>
> >>
> >> https://github.com/apache/spark/blob/master/network/
> shuffle/src/main/java/org/apache/spark/network/shuffle/
> protocol/BlockTransferMessage.java#L70
> >>
> >> public byte[] toByteArray() {
> >>   ByteBuf buf = Unpooled.buffer(encodedLength());
> >>   buf.writeByte(type().id);
> >>   encode(buf);
> >>   assert buf.writableBytes() == 0 : "Writable bytes remain: " +
> >> buf.writableBytes();
> >>   return buf.array();
> >> }
> >>
> >> Running the Java tests at last might have turned up a little bug here,
> >> but wanted to check. This makes a buffer to hold enough bytes to
> >> encode the message. But it writes 1 byte, plus the message. This makes
> >> the buffer expand, and then does have nonzero capacity afterwards, so
> >> the assert fails.
> >>
> >> So just needs a "+ 1" in the size?
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113ecad4cac6db0509d0ee99--

From dev-return-10714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 23:34:50 2014
Return-Path: <dev-return-10714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E85FAC746
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 23:34:50 +0000 (UTC)
Received: (qmail 62002 invoked by uid 500); 9 Dec 2014 23:34:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61922 invoked by uid 500); 9 Dec 2014 23:34:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61910 invoked by uid 99); 9 Dec 2014 23:34:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:34:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:34:23 +0000
Received: by mail-wg0-f47.google.com with SMTP id n12so2221012wgh.34
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 15:32:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=hgiJAQtaNnSvte73uhqNm+NRMoWOWG/PjQRRSWhAI8E=;
        b=LGJzYS/+/8YE5w0DLgdv2VajWRsD2hEBXjUe3vW4YjfWBcicRab1gFz05d5cMXrx5o
         opOVlzKgKYMO5AHcNn+iwDLSEpUimU9DSfLdWhJH/ACKGwzljwCYzDG1n7d9qXNT+a3j
         VlXVd/ZnGmCFFMf1COXhJ3IPGUpjr5yFtlZF4Tol9+PTtlW8uy3bH2RylB/0VMMFZ9kx
         fTUGz+Eu0xl15WVRkc2oI5jKz9UAWgHx0x7LS8oEe1iXshkqBT3KRwci5G5S+hvg7qEa
         jJvCXUPP2IMQomC/+UoCgHYH305lJFKoS5hO+v+K4jvo63LTBKw8xLY/3lrFBPy5PuAy
         bj7A==
X-Gm-Message-State: ALoCoQmtuY9/WEZWuePbcWmQAmbnbt7z1j/9nf9V4OHlbrLhF0m9kdPrLwrIewrhjy2tXnFAeLGQ
X-Received: by 10.181.28.3 with SMTP id jk3mr8401730wid.16.1418167973107; Tue,
 09 Dec 2014 15:32:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Tue, 9 Dec 2014 15:32:32 -0800 (PST)
In-Reply-To: <CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com>
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
 <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
 <CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com> <CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 9 Dec 2014 23:32:32 +0000
Message-ID: <CAMAsSd++qvxa_8zmjC3RojThcAmbNa-oc2bqkL0jJNR31v4hBA@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Aaron Davidson <aaron@databricks.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm not so sure about SBT, but I'm looking at the output now and do
not see things like JavaAPISuite being run. I see them compiled. That
I'm not as sure how to fix. I think I have a solution for Maven on
SPARK-4159.

On Tue, Dec 9, 2014 at 11:30 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> So all this time the tests that Jenkins has been running via Jenkins and SBT
> + ScalaTest... those haven't been running any of the Java unit tests?
>
> SPARK-4159 only mentions Maven as a problem, but I'm wondering how these
> tests got through Jenkins OK.
>
> On Tue Dec 09 2014 at 5:34:22 PM Sean Owen <sowen@cloudera.com> wrote:
>>
>> Yep, will do. The test does catch it -- it's just not being executed.
>> I think I have a reasonable start on re-enabling surefire + Java tests
>> for SPARK-4159.
>>
>> On Tue, Dec 9, 2014 at 10:30 PM, Aaron Davidson <aaron@databricks.com>
>> wrote:
>> > Oops, that does look like a bug. Strange that the
>> > BlockTransferMessageSuite
>> > did not catch this. "+1" sounds like the right solution, would you be
>> > able
>> > to submit a PR?
>> >
>> > On Tue, Dec 9, 2014 at 1:53 PM, Sean Owen <sowen@cloudera.com> wrote:
>> >>
>> >>
>> >>
>> >> https://github.com/apache/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockTransferMessage.java#L70
>> >>
>> >> public byte[] toByteArray() {
>> >>   ByteBuf buf = Unpooled.buffer(encodedLength());
>> >>   buf.writeByte(type().id);
>> >>   encode(buf);
>> >>   assert buf.writableBytes() == 0 : "Writable bytes remain: " +
>> >> buf.writableBytes();
>> >>   return buf.array();
>> >> }
>> >>
>> >> Running the Java tests at last might have turned up a little bug here,
>> >> but wanted to check. This makes a buffer to hold enough bytes to
>> >> encode the message. But it writes 1 byte, plus the message. This makes
>> >> the buffer expand, and then does have nonzero capacity afterwards, so
>> >> the assert fails.
>> >>
>> >> So just needs a "+ 1" in the size?
>> >
>> >
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec  9 23:38:06 2014
Return-Path: <dev-return-10715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F5A1C782
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  9 Dec 2014 23:38:06 +0000 (UTC)
Received: (qmail 76168 invoked by uid 500); 9 Dec 2014 23:38:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76113 invoked by uid 500); 9 Dec 2014 23:38:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76101 invoked by uid 99); 9 Dec 2014 23:38:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:38:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.181 as permitted sender)
Received: from [209.85.213.181] (HELO mail-ig0-f181.google.com) (209.85.213.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 09 Dec 2014 23:38:00 +0000
Received: by mail-ig0-f181.google.com with SMTP id l13so2058157iga.14
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 15:35:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=9RIG9q7/2vsrQkfc/zoZ8q9GR4eGSao9znaT6zBX8VM=;
        b=aWW6pbEV59cjmlOKnDmq+qLSKc2gDBBCG8ZR8BxCYgN+DCdXkDSUL6zsKc1uMx9UAt
         oR4/p4vEQDcDcHwHsTSeN+UzrlvBG7Fm6CztUrwybeXmhlZHea4tVkdcLJBQ6Kxf8/yl
         LTUtRCJ0UsB2YIDPgVRkv9z+Rr1kmGLeysqjDupmlGq9ZqJdLTiLYw+grZjiwrVkQCsa
         OXuloddHf+vIES63MkwKXzUK2TxecVYIN3fe6OObcKdV3BqMpDkKgv6YULvDZLcPqLHv
         VSObqfaHbvpQzxofDXhrtIbEKnq6IcjQQU+Sa9fyPkTkx0501xudWLcnkWEJHhdsLY1r
         2cXQ==
X-Received: by 10.50.29.3 with SMTP id f3mr22745370igh.23.1418168154534; Tue,
 09 Dec 2014 15:35:54 -0800 (PST)
MIME-Version: 1.0
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
 <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
 <CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com>
 <CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com> <CAMAsSd++qvxa_8zmjC3RojThcAmbNa-oc2bqkL0jJNR31v4hBA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 09 Dec 2014 23:35:54 +0000
Message-ID: <CAOhmDzf34_zSCAd2eq7Do5wOY7hUX-yUMWi7y0g58dcVym411Q@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
To: Sean Owen <sowen@cloudera.com>
Cc: Aaron Davidson <aaron@databricks.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd74b5415385a0509d10127
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74b5415385a0509d10127
Content-Type: text/plain; charset=UTF-8

OK. That's concerning. Hopefully that's the only bug we'll dig up once we
run all the Java tests but who knows.

Patrick,

Shouldn't this be a release blocking bug for 1.2 (mostly just because it
has already been covered by a unit test)? Well, that, as well as any other
bugs that come up as we run these Java tests.

Nick

On Tue Dec 09 2014 at 6:32:53 PM Sean Owen <sowen@cloudera.com> wrote:

> I'm not so sure about SBT, but I'm looking at the output now and do
> not see things like JavaAPISuite being run. I see them compiled. That
> I'm not as sure how to fix. I think I have a solution for Maven on
> SPARK-4159.
>
> On Tue, Dec 9, 2014 at 11:30 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > So all this time the tests that Jenkins has been running via Jenkins and
> SBT
> > + ScalaTest... those haven't been running any of the Java unit tests?
> >
> > SPARK-4159 only mentions Maven as a problem, but I'm wondering how these
> > tests got through Jenkins OK.
> >
> > On Tue Dec 09 2014 at 5:34:22 PM Sean Owen <sowen@cloudera.com> wrote:
> >>
> >> Yep, will do. The test does catch it -- it's just not being executed.
> >> I think I have a reasonable start on re-enabling surefire + Java tests
> >> for SPARK-4159.
> >>
> >> On Tue, Dec 9, 2014 at 10:30 PM, Aaron Davidson <aaron@databricks.com>
> >> wrote:
> >> > Oops, that does look like a bug. Strange that the
> >> > BlockTransferMessageSuite
> >> > did not catch this. "+1" sounds like the right solution, would you be
> >> > able
> >> > to submit a PR?
> >> >
> >> > On Tue, Dec 9, 2014 at 1:53 PM, Sean Owen <sowen@cloudera.com> wrote:
> >> >>
> >> >>
> >> >>
> >> >> https://github.com/apache/spark/blob/master/network/
> shuffle/src/main/java/org/apache/spark/network/shuffle/
> protocol/BlockTransferMessage.java#L70
> >> >>
> >> >> public byte[] toByteArray() {
> >> >>   ByteBuf buf = Unpooled.buffer(encodedLength());
> >> >>   buf.writeByte(type().id);
> >> >>   encode(buf);
> >> >>   assert buf.writableBytes() == 0 : "Writable bytes remain: " +
> >> >> buf.writableBytes();
> >> >>   return buf.array();
> >> >> }
> >> >>
> >> >> Running the Java tests at last might have turned up a little bug
> here,
> >> >> but wanted to check. This makes a buffer to hold enough bytes to
> >> >> encode the message. But it writes 1 byte, plus the message. This
> makes
> >> >> the buffer expand, and then does have nonzero capacity afterwards, so
> >> >> the assert fails.
> >> >>
> >> >> So just needs a "+ 1" in the size?
> >> >
> >> >
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >
>

--047d7bd74b5415385a0509d10127--

From dev-return-10716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 06:09:44 2014
Return-Path: <dev-return-10716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48DA79836
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 06:09:44 +0000 (UTC)
Received: (qmail 36782 invoked by uid 500); 10 Dec 2014 06:09:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36701 invoked by uid 500); 10 Dec 2014 06:09:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36689 invoked by uid 99); 10 Dec 2014 06:09:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 06:09:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 06:09:16 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so1735111obc.6
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 22:07:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Tb8lBIY8WHEAdP4LJxVtpZOXYbSRn9DJ7jjfy79Sk6Y=;
        b=fZnj5S3XcQpujSO2uL2l4oxrtqKVTN1wztLmyZI6mpG/vkzDvgXSGwyun8VD41XAtW
         YS6wu+dAMZGQfCxYvbv+DpB2MC90KzruE7Iumt82QS+rBpCdxDnLgkQTKnXltvyw5wn8
         wAl4ZxaRO2RKGsHkKHmXGg4STP7+XkhjZ5Jc1AMKe2Ers670hZ45gSvUG3NgcurHSq8Z
         CEwPQoh1E3xTucHT2Lk1L83NysU8rBMwKmD0FJvlJYRqlP248kTFv7dq5e6jbAhE38bz
         hDTOsYE0HbNie542plqfmGniGtG/8Dtd0b/8reUNokAfLpUMi0LJ/MxVNk7Rehd3USBb
         +i3w==
MIME-Version: 1.0
X-Received: by 10.202.218.2 with SMTP id r2mr1338491oig.82.1418191665214; Tue,
 09 Dec 2014 22:07:45 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Tue, 9 Dec 2014 22:07:45 -0800 (PST)
In-Reply-To: <CAOhmDzf34_zSCAd2eq7Do5wOY7hUX-yUMWi7y0g58dcVym411Q@mail.gmail.com>
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
	<CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
	<CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com>
	<CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com>
	<CAMAsSd++qvxa_8zmjC3RojThcAmbNa-oc2bqkL0jJNR31v4hBA@mail.gmail.com>
	<CAOhmDzf34_zSCAd2eq7Do5wOY7hUX-yUMWi7y0g58dcVym411Q@mail.gmail.com>
Date: Tue, 9 Dec 2014 22:07:45 -0800
Message-ID: <CABPQxstKTZHvsnL-CjBLQt1ahP8EFfuhPk4Pvpjeckc++URGhg@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Aaron Davidson <aaron@databricks.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nick,

Thanks for bringing this up. I believe these Java tests are running in
the sbt build right now, the issue is that this particular bug was
flagged by the triggering of a runtime Java "assert" (not a normal
Junit test assertion) and those are not enabled in our sbt tests. It
would be good to fix it so that assertions run when we do the sbt
tests, for some reason I think the sbt tests disable them by default.

I think the original issue is fixed now (that Sean found and
reported). It would be good to get assertions running in our tests,
but I'm not sure I'd block the release on it. The normal JUnit
assertions are running correctly.

- Patrick

On Tue, Dec 9, 2014 at 3:35 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> OK. That's concerning. Hopefully that's the only bug we'll dig up once we
> run all the Java tests but who knows.
>
> Patrick,
>
> Shouldn't this be a release blocking bug for 1.2 (mostly just because it
> has already been covered by a unit test)? Well, that, as well as any other
> bugs that come up as we run these Java tests.
>
> Nick
>
> On Tue Dec 09 2014 at 6:32:53 PM Sean Owen <sowen@cloudera.com> wrote:
>
>> I'm not so sure about SBT, but I'm looking at the output now and do
>> not see things like JavaAPISuite being run. I see them compiled. That
>> I'm not as sure how to fix. I think I have a solution for Maven on
>> SPARK-4159.
>>
>> On Tue, Dec 9, 2014 at 11:30 PM, Nicholas Chammas
>> <nicholas.chammas@gmail.com> wrote:
>> > So all this time the tests that Jenkins has been running via Jenkins and
>> SBT
>> > + ScalaTest... those haven't been running any of the Java unit tests?
>> >
>> > SPARK-4159 only mentions Maven as a problem, but I'm wondering how these
>> > tests got through Jenkins OK.
>> >
>> > On Tue Dec 09 2014 at 5:34:22 PM Sean Owen <sowen@cloudera.com> wrote:
>> >>
>> >> Yep, will do. The test does catch it -- it's just not being executed.
>> >> I think I have a reasonable start on re-enabling surefire + Java tests
>> >> for SPARK-4159.
>> >>
>> >> On Tue, Dec 9, 2014 at 10:30 PM, Aaron Davidson <aaron@databricks.com>
>> >> wrote:
>> >> > Oops, that does look like a bug. Strange that the
>> >> > BlockTransferMessageSuite
>> >> > did not catch this. "+1" sounds like the right solution, would you be
>> >> > able
>> >> > to submit a PR?
>> >> >
>> >> > On Tue, Dec 9, 2014 at 1:53 PM, Sean Owen <sowen@cloudera.com> wrote:
>> >> >>
>> >> >>
>> >> >>
>> >> >> https://github.com/apache/spark/blob/master/network/
>> shuffle/src/main/java/org/apache/spark/network/shuffle/
>> protocol/BlockTransferMessage.java#L70
>> >> >>
>> >> >> public byte[] toByteArray() {
>> >> >>   ByteBuf buf = Unpooled.buffer(encodedLength());
>> >> >>   buf.writeByte(type().id);
>> >> >>   encode(buf);
>> >> >>   assert buf.writableBytes() == 0 : "Writable bytes remain: " +
>> >> >> buf.writableBytes();
>> >> >>   return buf.array();
>> >> >> }
>> >> >>
>> >> >> Running the Java tests at last might have turned up a little bug
>> here,
>> >> >> but wanted to check. This makes a buffer to hold enough bytes to
>> >> >> encode the message. But it writes 1 byte, plus the message. This
>> makes
>> >> >> the buffer expand, and then does have nonzero capacity afterwards, so
>> >> >> the assert fails.
>> >> >>
>> >> >> So just needs a "+ 1" in the size?
>> >> >
>> >> >
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 07:24:55 2014
Return-Path: <dev-return-10717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 515119B91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 07:24:55 +0000 (UTC)
Received: (qmail 56528 invoked by uid 500); 10 Dec 2014 07:24:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56466 invoked by uid 500); 10 Dec 2014 07:24:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56455 invoked by uid 99); 10 Dec 2014 07:24:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 07:24:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 07:24:47 +0000
Received: by mail-qc0-f169.google.com with SMTP id w7so1731090qcr.0
        for <dev@spark.apache.org>; Tue, 09 Dec 2014 23:22:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=44nZbRbkiRugCO8boJo5NytLUt9I2hUPRFzQfSWdoGE=;
        b=BecJMLr9OxIGu9vDrT6UCvb/5w641i4dcEQkK5gAIE6ySD4rLqnkJQl8Sgam85wiAo
         dA9RKMjUnqrmtmfwIR2dm0dRWO5CvSY7rVrSPkNbFb0umQr63FeTBJ8f7+lI+4+WSD5+
         xf7jgdQ7aK0lyyaMIKXi2tkAMKLMlwO/fhVEDFpojbj1ql8SD1R7Yl3DxXjXTE8KW3PQ
         iENYhUnJsvL91PwFwam9bvePI3d4w15WOcJjJW6i1d0t+PW0h39PYa9mCR3o7nmwFNj1
         0N/iMeYo0E8TZo5U49+xYWdag1kvE+bEZfs+z5ehwxmTlUNNc76qUxym78EbLdaR1viI
         1zuw==
X-Gm-Message-State: ALoCoQnUjTZ11lXuNaxQc7eCRa/Ix4XwDQgOwX9U8fzZqLEydj+9BXVYDriNKn21YnQNknVSbs9W
X-Received: by 10.140.102.248 with SMTP id w111mr5217706qge.33.1418196176959;
 Tue, 09 Dec 2014 23:22:56 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.188.135 with HTTP; Tue, 9 Dec 2014 23:22:36 -0800 (PST)
In-Reply-To: <CABPQxstKTZHvsnL-CjBLQt1ahP8EFfuhPk4Pvpjeckc++URGhg@mail.gmail.com>
References: <CAMAsSd+NaMNZaz=e4jD1kAgAHWeUU8+6XhD8uj2ZCP8oH9Jg+Q@mail.gmail.com>
 <CAGnzRo=xzNK6edNq-oNiOYSYwd-geHZDAQJW6NCdqCp=8U+01w@mail.gmail.com>
 <CAMAsSdLAcWrkmo-fzNLNJJTmn96NxhUiBSDVcGNK+mp2Eb0uzg@mail.gmail.com>
 <CAOhmDzfWfyfJnr6re0527sA8-3_AT7pjHuo32_+-H7u9c9dr=A@mail.gmail.com>
 <CAMAsSd++qvxa_8zmjC3RojThcAmbNa-oc2bqkL0jJNR31v4hBA@mail.gmail.com>
 <CAOhmDzf34_zSCAd2eq7Do5wOY7hUX-yUMWi7y0g58dcVym411Q@mail.gmail.com> <CABPQxstKTZHvsnL-CjBLQt1ahP8EFfuhPk4Pvpjeckc++URGhg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 10 Dec 2014 07:22:36 +0000
Message-ID: <CAMAsSd+b2R-b+y6G5zL8eL1c=CJq0ELOm2nxwdif1sZbeToAGg@mail.gmail.com>
Subject: Re: Is this a little bug in BlockTransferMessage ?
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Aaron Davidson <aaron@databricks.com>, 
	dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Oops, yes I see Java tests run with SBT now. You're right, it must be
because of the assertion. I can try to add '-ea' to the SBT build as a
closely-related change for SPARK-4159.

FWIW this error is the only one I saw once the Maven tests ran the Java tests.

On Wed, Dec 10, 2014 at 6:07 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Nick,
>
> Thanks for bringing this up. I believe these Java tests are running in
> the sbt build right now, the issue is that this particular bug was
> flagged by the triggering of a runtime Java "assert" (not a normal
> Junit test assertion) and those are not enabled in our sbt tests. It
> would be good to fix it so that assertions run when we do the sbt
> tests, for some reason I think the sbt tests disable them by default.
>
> I think the original issue is fixed now (that Sean found and
> reported). It would be good to get assertions running in our tests,
> but I'm not sure I'd block the release on it. The normal JUnit
> assertions are running correctly.
>
> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 08:28:11 2014
Return-Path: <dev-return-10718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 598DF9F0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 08:28:11 +0000 (UTC)
Received: (qmail 76560 invoked by uid 500); 10 Dec 2014 08:28:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76477 invoked by uid 500); 10 Dec 2014 08:28:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76461 invoked by uid 99); 10 Dec 2014 08:28:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 08:28:09 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_20,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.140 as permitted sender)
Received: from [202.81.31.140] (HELO e23smtp07.au.ibm.com) (202.81.31.140)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 08:28:01 +0000
Received: from /spool/local
	by e23smtp07.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Wed, 10 Dec 2014 18:26:38 +1000
Received: from d23dlp01.au.ibm.com (202.81.31.203)
	by e23smtp07.au.ibm.com (202.81.31.204) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Wed, 10 Dec 2014 18:26:36 +1000
Received: from d23relay09.au.ibm.com (d23relay09.au.ibm.com [9.185.63.181])
	by d23dlp01.au.ibm.com (Postfix) with ESMTP id 3B5502CE8072
	for <dev@spark.apache.org>; Wed, 10 Dec 2014 19:26:36 +1100 (EST)
Received: from d23av03.au.ibm.com (d23av03.au.ibm.com [9.190.234.97])
	by d23relay09.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBA8QRw918219144
	for <dev@spark.apache.org>; Wed, 10 Dec 2014 19:26:36 +1100
Received: from d23av03.au.ibm.com (localhost [127.0.0.1])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBA8Q30F030066
	for <dev@spark.apache.org>; Wed, 10 Dec 2014 19:26:03 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBA8Q2fD029567
	for <dev@spark.apache.org>; Wed, 10 Dec 2014 19:26:03 +1100
To: dev@spark.apache.org
MIME-Version: 1.0
Subject: HA support for Spark
X-KeepSent: 5A4A2C9F:554DDE0D-48257DAA:002DD89C;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Wed, 10 Dec 2014 16:25:37 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/10/2014 16:26:03,
	Serialize complete at 12/10/2014 16:26:03
Content-Type: multipart/related; boundary="=_related 002E48C148257DAA_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121008-0025-0000-0000-000000B2C787
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 002E48C148257DAA_=
Content-Type: multipart/alternative; boundary="=_alternative 002E48C148257DAA_="


--=_alternative 002E48C148257DAA_=
Content-Type: text/plain; charset="US-ASCII"

Do we have any high availability support in Spark driver level? For 
example, if we want spark drive can move to another node continue 
execution when failure happen. I can see the RDD checkpoint can help to 
serialization the status of RDD. I can image to load the check point from 
another node when error happen, but seems like will lost track all tasks 
status or even executor information that maintain in spark context. I am 
not sure if there is any existing stuff I can leverage to do that. thanks 
for any suggests
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 
--=_alternative 002E48C148257DAA_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 002E48C148257DAA_=--
--=_related 002E48C148257DAA_=--


From dev-return-10719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 08:32:05 2014
Return-Path: <dev-return-10719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58EA59F3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 08:32:05 +0000 (UTC)
Received: (qmail 86606 invoked by uid 500); 10 Dec 2014 08:32:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86535 invoked by uid 500); 10 Dec 2014 08:32:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86520 invoked by uid 99); 10 Dec 2014 08:32:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 08:32:03 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_28,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 08:31:57 +0000
Received: by mail-qa0-f43.google.com with SMTP id bm13so1674092qab.30
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 00:31:16 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=+MkL1Oo+1Qwxh9OrnGZb62/HzhWvBl3qsVHqncveXzQ=;
        b=CESToVebqT+JYxa7iprxDRNa9Ms5rFhCKjIQkOm9lpngn7A+L8gpJvLwVJBa0jXb8c
         gb3Ca0zkwcrmxaGNh5djRuqzZagcomopwDQZJNFnQxTfWHrqaSksNIPDemw9vJ/4Tmar
         4ZFyXIc62PLtvmSQdHaSZBCVm9xzVWHL8vnbgs4NgyNCYmS8my2DW0Q16a2NBx0Cj31m
         yorLnZo/gkf95/TqdPSH8JsLAUHJWsRSkpGdOyK1Q3F/3kmbeiPv07GxHTFw2I9RXzZ1
         x8DmdCHwE6CJ0opH2ruxejtEbaj9CLuUrc0xLkWjjzm3pO38yk8AnWBCO3bIn9GcQBpn
         L36g==
X-Gm-Message-State: ALoCoQnnKieyVO5+HdjvzKT38HP8UuTa4dVyGa9+Kcmxz74EvPnD9oKl5O9eTXNWa3P+JLnte9oX
X-Received: by 10.224.74.207 with SMTP id v15mr5795401qaj.53.1418200276141;
 Wed, 10 Dec 2014 00:31:16 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Wed, 10 Dec 2014 00:30:55 -0800 (PST)
In-Reply-To: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 10 Dec 2014 00:30:55 -0800
Message-ID: <CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
Subject: Re: HA support for Spark
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c9a0ae3a4d0509d87be1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c9a0ae3a4d0509d87be1
Content-Type: text/plain; charset=UTF-8

This would be plausible for specific purposes such as Spark streaming or
Spark SQL, but I don't think it is doable for general Spark driver since it
is just a normal JVM process with arbitrary program state.

On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Do we have any high availability support in Spark driver level? For
> example, if we want spark drive can move to another node continue execution
> when failure happen. I can see the RDD checkpoint can help to serialization
> the status of RDD. I can image to load the check point from another node
> when error happen, but seems like will lost track all tasks status or even
> executor information that maintain in spark context. I am not sure if there
> is any existing stuff I can leverage to do that. thanks for any suggests
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>

--089e0149c9a0ae3a4d0509d87be1--

From dev-return-10720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 10:43:36 2014
Return-Path: <dev-return-10720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F789C6E5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 10:43:36 +0000 (UTC)
Received: (qmail 83577 invoked by uid 500); 10 Dec 2014 10:43:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83511 invoked by uid 500); 10 Dec 2014 10:43:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80745 invoked by uid 99); 10 Dec 2014 10:43:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 10:43:28 +0000
X-ASF-Spam-Status: No, hits=3.5 required=5.0
	tests=HTML_MESSAGE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of guillaume.pitel@exensa.com designates 91.121.232.90 as permitted sender)
Received: from [91.121.232.90] (HELO mail.exensa.com) (91.121.232.90)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 10:43:24 +0000
Received: from localhost (localhost [127.0.0.1])
	by mail.exensa.com (Postfix) with ESMTP id 95F98640197;
	Wed, 10 Dec 2014 10:42:33 +0000 (UTC)
Received: from mail.exensa.com ([127.0.0.1])
	by localhost (mail.exensa.com [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id MQP+gVsEbVTv; Wed, 10 Dec 2014 10:42:33 +0000 (UTC)
Received: from [10.1.42.4] (LPuteaux-656-1-229-158.w80-12.abo.wanadoo.fr [80.12.90.158])
	(using TLSv1 with cipher ECDHE-RSA-AES128-SHA (128/128 bits))
	(No client certificate requested)
	by mail.exensa.com (Postfix) with ESMTPSA id 6AA9A640196;
	Wed, 10 Dec 2014 10:42:33 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=exensa.com;
	s=default; t=1418208153;
	bh=mf0GKdCmMMFW8BKEUW3plwarmE8KOSjtSN0dSSU4sd8=;
	h=Date:From:To:Subject:References:In-Reply-To;
	b=TbukPWE47cGUEVblKcaVbiPX0aOJci8xPvjHbdKmYvD4EYCAMpdGKTfaZuCKAuusz
	 rsaH34I0MkqF9KlQOy+NQUPbrANUUjWcX/bb5CyhirZFB7KLboPJ/eGvcIz+Okd
Message-ID: <5488239C.30108@exensa.com>
Date: Wed, 10 Dec 2014 11:42:36 +0100
From: Guillaume Pitel <guillaume.pitel@exensa.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: user@spark.apache.org, dev@spark.apache.org
Subject: Maven profile in MLLib netlib-lgpl not working (1.1.1)
References: <1416847893169-19662.post@n3.nabble.com>	<CAJgQjQ9t9K1YUgVpAVhxbnt7h0ScABiSNhzA22CiWo_1KcB_YQ@mail.gmail.com>	<1416863329911-19681.post@n3.nabble.com>	<CAKx7Bf8Apgmp9K7VVRNAymdQ1RVwABfYJuJFOBReH9iz=E8GdA@mail.gmail.com>	<1416882664883-19705.post@n3.nabble.com>	<CABjXkq5cLDCUQTSMWJ9K06qJOatGSmbtbtkMGYnA0BSxdpGQaw@mail.gmail.com>	<1417452584987-20110.post@n3.nabble.com> <CAKOOPDu4gv=ZO8xNS+boDSXJf=RZy09s9AdK8kbiXHqoWM4rAw@mail.gmail.com>
In-Reply-To: <CAKOOPDu4gv=ZO8xNS+boDSXJf=RZy09s9AdK8kbiXHqoWM4rAw@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------050408090103000205090100"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------050408090103000205090100
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi

Issue created https://issues.apache.org/jira/browse/SPARK-4816

Probably a maven-related question for profiles in child modules

I couldn't find a clean solution, just a workaround : modify pom.xml in 
mllib module to force activation of netlib-lgpl module.

Hope a maven expert will help.

Guillaume
> +1 with 1.3-SNAPSHOT.
>
> On Mon, Dec 1, 2014 at 5:49 PM, agg212 <alexander_galakatos@brown.edu 
> <mailto:alexander_galakatos@brown.edu>> wrote:
>
>     Thanks for your reply, but I'm still running into issues
>     installing/configuring the native libraries for MLlib.  Here are
>     the steps
>     I've taken, please let me know if anything is incorrect.
>
>     - Download Spark source
>     - unzip and compile using `mvn -Pnetlib-lgpl -DskipTests clean
>     package `
>     - Run `sbt/sbt publish-local`
>
>     The last step fails with the following error (full stack trace is
>     attached
>     here:  error.txt
>     <http://apache-spark-user-list.1001560.n3.nabble.com/file/n20110/error.txt>
>     ):
>     [error] (sql/compile:compile) java.lang.AssertionError: assertion
>     failed:
>     List(object package$DebugNode, object package$DebugNode)
>
>     Do I still have to install OPENBLAS/anything else if I build Spark
>     from the
>     source using the -Pnetlib-lgpl flag?  Also, do I change the Spark
>     version
>     (from 1.1.0 to 1.2.0-SNAPSHOT) in the .sbt file for my app?
>
>     Thanks!
>
>
>
>     --
>     View this message in context:
>     http://apache-spark-user-list.1001560.n3.nabble.com/Mllib-native-netlib-java-OpenBLAS-tp19662p20110.html
>     Sent from the Apache Spark User List mailing list archive at
>     Nabble.com.
>
>     ---------------------------------------------------------------------
>     To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>     <mailto:user-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: user-help@spark.apache.org
>     <mailto:user-help@spark.apache.org>
>
>


-- 
eXenSa

	
*Guillaume PITEL, Prsident*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Prier - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705


--------------050408090103000205090100
Content-Type: multipart/related;
 boundary="------------060106020105040102020207"


--------------060106020105040102020207
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: 8bit

<html>
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    Hi<br>
    <br>
    Issue created <a class="moz-txt-link-freetext" href="https://issues.apache.org/jira/browse/SPARK-4816">https://issues.apache.org/jira/browse/SPARK-4816</a><br>
    <br>
    Probably a maven-related question for profiles in child modules<br>
    <br>
    I couldn't find a clean solution, just a workaround : modify pom.xml
    in mllib module to force activation of netlib-lgpl module.<br>
    <br>
    Hope a maven expert will help.<br>
    <br>
    Guillaume<br>
    <blockquote
cite="mid:CAKOOPDu4gv=ZO8xNS+boDSXJf=RZy09s9AdK8kbiXHqoWM4rAw@mail.gmail.com"
      type="cite">
      <div dir="ltr">+1 with 1.3-SNAPSHOT.</div>
      <div class="gmail_extra"><br>
        <div class="gmail_quote">On Mon, Dec 1, 2014 at 5:49 PM, agg212
          <span dir="ltr">&lt;<a moz-do-not-send="true"
              href="mailto:alexander_galakatos@brown.edu"
              target="_blank">alexander_galakatos@brown.edu</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">Thanks for
            your reply, but I'm still running into issues<br>
            installing/configuring the native libraries for MLlib. Here
            are the steps<br>
            I've taken, please let me know if anything is incorrect.<br>
            <br>
            - Download Spark source<br>
            - unzip and compile using `mvn -Pnetlib-lgpl -DskipTests
            clean package `<br>
            - Run `sbt/sbt publish-local`<br>
            <br>
            The last step fails with the following error (full stack
            trace is attached<br>
            here: error.txt<br>
            &lt;<a moz-do-not-send="true"
href="http://apache-spark-user-list.1001560.n3.nabble.com/file/n20110/error.txt"
              target="_blank">http://apache-spark-user-list.1001560.n3.nabble.com/file/n20110/error.txt</a>&gt;<br>
            ):<br>
            [error] (sql/compile:compile) java.lang.AssertionError:
            assertion failed:<br>
            List(object package$DebugNode, object package$DebugNode)<br>
            <br>
            Do I still have to install OPENBLAS/anything else if I build
            Spark from the<br>
            source using the -Pnetlib-lgpl flag? Also, do I change the
            Spark version<br>
            (from 1.1.0 to 1.2.0-SNAPSHOT) in the .sbt file for my app?<br>
            <br>
            Thanks!<br>
            <br>
            <br>
            <br>
            --<br>
            View this message in context: <a moz-do-not-send="true"
href="http://apache-spark-user-list.1001560.n3.nabble.com/Mllib-native-netlib-java-OpenBLAS-tp19662p20110.html"
              target="_blank">http://apache-spark-user-list.1001560.n3.nabble.com/Mllib-native-netlib-java-OpenBLAS-tp19662p20110.html</a><br>
            <div class="HOEnZb">
              <div class="h5">Sent from the Apache Spark User List
                mailing list archive at Nabble.com.<br>
                <br>
---------------------------------------------------------------------<br>
                To unsubscribe, e-mail: <a moz-do-not-send="true"
                  href="mailto:user-unsubscribe@spark.apache.org">user-unsubscribe@spark.apache.org</a><br>
                For additional commands, e-mail: <a
                  moz-do-not-send="true"
                  href="mailto:user-help@spark.apache.org">user-help@spark.apache.org</a><br>
                <br>
              </div>
            </div>
          </blockquote>
        </div>
        <br>
      </div>
    </blockquote>
    <br>
    <br>
    <div class="moz-signature">-- <br>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <span style="font-size: 8.5pt; color: rgb(51, 51, 51);
        font-family: Helvetica;">
        <div style="width: 529px;" align="left">
          <table style="width: 524px;" cellpadding="0" cellspacing="0"
            border="0">
            <tbody>
              <tr>
                <td style="border-top: medium none rgb(236, 233, 216);
                  border-right: medium none rgb(236, 233, 216);
                  border-bottom: medium none rgb(236, 233, 216);
                  padding-right: 0cm; width: 219px; height: 33.75pt;
                  background-color: transparent; vertical-align:
                  middle;">
                  <center style="width: 210px;"><img style="border: 0px
                      solid ; width: 160px; height: 64px;" alt="eXenSa"
                      src="cid:part6.02010009.08080100@exensa.com"></center>
                </td>
                <td style="padding-left:10px; border-top: medium none
                  rgb(236, 233, 216); border-right: medium none rgb(236,
                  233, 216); border-bottom: medium none rgb(236, 233,
                  216); padding-right: 0cm; width: 358px; height:
                  33.75pt; background-color: transparent;
                  vertical-align: top;">
                  <div style="text-align: left; width: 299px;"
                    align="left"> <span style="font-size: 7.5pt; color:
                      rgb(75, 80, 85); font-family: Helvetica;"> <b>Guillaume
                        PITEL, Prsident</b> <br>
                      +33(0)626 222 431<br>
                    </span> <br>
                    <span style="font-size: 7.5pt; color: #505050;
                      font-family: Helvetica;"><a
                        href="http://www.exensa.com/" target="_blank">eXenSa
                        S.A.S.</a> </span><br>
                    <span style="font-size: 7.5pt; color: rgb(75, 80,
                      85);"> <font face="Helvetica">41, rue Prier -
                        92120 Montrouge - FRANCE <br>
                        Tel +33(0)184 163 677 / Fax +33(0)972 283 705 </font>
                    </span> </div>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </span>
    </div>
  </body>
</html>

--------------060106020105040102020207
Content-Type: image/png;
 name="exensa_logo_mail.png"
Content-Transfer-Encoding: base64
Content-ID: <part6.02010009.08080100@exensa.com>
Content-Disposition: inline;
 filename="exensa_logo_mail.png"
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==
--------------060106020105040102020207--

--------------050408090103000205090100--

From dev-return-10721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 13:31:44 2014
Return-Path: <dev-return-10721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F089CF4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 13:31:44 +0000 (UTC)
Received: (qmail 17245 invoked by uid 500); 10 Dec 2014 13:31:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17172 invoked by uid 500); 10 Dec 2014 13:31:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17160 invoked by uid 99); 10 Dec 2014 13:31:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 13:31:42 +0000
X-ASF-Spam-Status: No, hits=-2.5 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.141 as permitted sender)
Received: from [202.81.31.141] (HELO e23smtp08.au.ibm.com) (202.81.31.141)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 13:31:14 +0000
Received: from /spool/local
	by e23smtp08.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Wed, 10 Dec 2014 23:30:40 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp08.au.ibm.com (202.81.31.205) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Wed, 10 Dec 2014 23:30:38 +1000
Received: from d23relay06.au.ibm.com (d23relay06.au.ibm.com [9.185.63.219])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id C2CF92BB005A
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:30:35 +1100 (EST)
Received: from d23av05.au.ibm.com (d23av05.au.ibm.com [9.190.234.119])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBADUUqt25624728
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:30:30 +1100
Received: from d23av05.au.ibm.com (localhost [127.0.0.1])
	by d23av05.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBADUZ6B031492
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:30:35 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av05.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBADURMO031412
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:30:35 +1100
In-Reply-To: <CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com> <CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Re: HA support for Spark
X-KeepSent: 82EF1068:807D462B-48257DAA:00498429;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Wed, 10 Dec 2014 21:30:24 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/10/2014 21:30:34,
	Serialize complete at 12/10/2014 21:30:34
Content-Type: multipart/related; boundary="=_related 004A305548257DAA_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121013-0029-0000-0000-000000C99CBD
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 004A305548257DAA_=
Content-Type: multipart/alternative; boundary="=_alternative 004A305548257DAA_="


--=_alternative 004A305548257DAA_=
Content-Type: text/plain; charset="US-ASCII"

Well, it should not be mission impossible thinking there are so many HA 
solution existing today. I would interest to know if there is any specific 
difficult.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Reynold Xin <rxin@databricks.com> 
2014/12/10 16:30

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
"dev@spark.apache.org" <dev@spark.apache.org>
Subject
Re: HA support for Spark






This would be plausible for specific purposes such as Spark streaming or
Spark SQL, but I don't think it is doable for general Spark driver since 
it
is just a normal JVM process with arbitrary program state.

On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Do we have any high availability support in Spark driver level? For
> example, if we want spark drive can move to another node continue 
execution
> when failure happen. I can see the RDD checkpoint can help to 
serialization
> the status of RDD. I can image to load the check point from another node
> when error happen, but seems like will lost track all tasks status or 
even
> executor information that maintain in spark context. I am not sure if 
there
> is any existing stuff I can leverage to do that. thanks for any suggests
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: 
*86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>


--=_alternative 004A305548257DAA_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 004A305548257DAA_=--
--=_related 004A305548257DAA_=--


From dev-return-10722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 13:47:58 2014
Return-Path: <dev-return-10722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3B5D910008
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 13:47:58 +0000 (UTC)
Received: (qmail 46955 invoked by uid 500); 10 Dec 2014 13:47:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46879 invoked by uid 500); 10 Dec 2014 13:47:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46868 invoked by uid 99); 10 Dec 2014 13:47:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 13:47:56 +0000
X-ASF-Spam-Status: No, hits=-1.5 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_16,HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.141 as permitted sender)
Received: from [202.81.31.141] (HELO e23smtp08.au.ibm.com) (202.81.31.141)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 13:47:48 +0000
Received: from /spool/local
	by e23smtp08.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Wed, 10 Dec 2014 23:47:26 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp08.au.ibm.com (202.81.31.205) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Wed, 10 Dec 2014 23:47:23 +1000
Received: from d23relay08.au.ibm.com (d23relay08.au.ibm.com [9.185.71.33])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id E11332BB0023
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:47:22 +1100 (EST)
Received: from d23av04.au.ibm.com (d23av04.au.ibm.com [9.190.235.139])
	by d23relay08.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBADlM2K33685698
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:47:22 +1100
Received: from d23av04.au.ibm.com (localhost [127.0.0.1])
	by d23av04.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBADlMXW002324
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:47:22 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av04.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBADlEwQ002234
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 00:47:22 +1100
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Tachyon in Spark
X-KeepSent: 70D8AF83:8078E988-48257DAA:004B96A6;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Wed, 10 Dec 2014 21:47:12 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/10/2014 21:47:22,
	Serialize complete at 12/10/2014 21:47:22
Content-Type: multipart/related; boundary="=_related 004BBA0748257DAA_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121013-0029-0000-0000-000000C9A131
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 004BBA0748257DAA_=
Content-Type: multipart/alternative; boundary="=_alternative 004BBA0748257DAA_="


--=_alternative 004BBA0748257DAA_=
Content-Type: text/plain; charset="US-ASCII"

Dose Spark today really leverage Tachyon linage to process data? It seems 
like the application should call createDependency function in TachyonFS to 
create a new linage node. But I did not find any place call that in Spark 
code. Did I missed anything?
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 
--=_alternative 004BBA0748257DAA_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 004BBA0748257DAA_=--
--=_related 004BBA0748257DAA_=--


From dev-return-10723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 17:36:23 2014
Return-Path: <dev-return-10723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B1D910E1D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 17:36:23 +0000 (UTC)
Received: (qmail 57770 invoked by uid 500); 10 Dec 2014 17:36:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57700 invoked by uid 500); 10 Dec 2014 17:36:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57684 invoked by uid 99); 10 Dec 2014 17:36:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:36:21 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:36:17 +0000
Received: by mail-qa0-f49.google.com with SMTP id s7so2295023qap.22
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 09:34:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=qg64XvLFgiiRxxpPfMMZzOxLTyINb0h/3x4otUDlOBE=;
        b=BRyplBCFvifrlJPjH5hTMbm36S4SE6cciO4Eptsebbi2UbqtntE3elYCMOnMNOUvmC
         6wM+kvy3VACSdE3sf6HcUHZBRO5wdAAmwgYpmS1MfyR25FmxyquWxrpqy6DUIAXSqW0d
         4BRXymoSvucXWlCVv2Zpa/sCUQ2t3O8N83ogkxlOcWLQ8ByISqNeBo2dVzUv0jm8G5nH
         th5mQD4k9WO4Pg6LJ6rZQhNIJGnISdZ7dyVYttOopLrKb0tCttE1BRjqaENp2v1+A8WA
         7VaJkLMk9NF0L62JJ14WB0zf1KBD40On2HujvBuda0w/SEn5bq/gS1Ja6IIEAljyy687
         cB9A==
X-Gm-Message-State: ALoCoQkeLPG/gAz0D54qdlJUjgSyjlRkzQz2nQjdw/x50HKmgNU9lcDYJt8O/1BPJ65J7hEGU1/R
MIME-Version: 1.0
X-Received: by 10.224.137.131 with SMTP id w3mr10461249qat.95.1418232866710;
 Wed, 10 Dec 2014 09:34:26 -0800 (PST)
Received: by 10.140.102.113 with HTTP; Wed, 10 Dec 2014 09:34:26 -0800 (PST)
In-Reply-To: <OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>
	<CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
	<OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com>
Date: Wed, 10 Dec 2014 09:34:26 -0800
Message-ID: <CACBYxKKYSqsWUvkvPspMdK5KumU+mDCzyMijxOf5VsoftXz7xQ@mail.gmail.com>
Subject: Re: HA support for Spark
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2d9783af40b0509e0128b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d9783af40b0509e0128b
Content-Type: text/plain; charset=UTF-8

I think that if we were able to maintain the full set of created RDDs as
well as some scheduler and block manager state, it would be enough for most
apps to recover.

On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Well, it should not be mission impossible thinking there are so many HA
> solution existing today. I would interest to know if there is any specific
> difficult.
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
>
> 2014/12/10 16:30
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> "dev@spark.apache.org" <dev@spark.apache.org>
> Subject
> Re: HA support for Spark
>
>
>
>
> This would be plausible for specific purposes such as Spark streaming or
> Spark SQL, but I don't think it is doable for general Spark driver since it
> is just a normal JVM process with arbitrary program state.
>
> On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
> > Do we have any high availability support in Spark driver level? For
> > example, if we want spark drive can move to another node continue
> execution
> > when failure happen. I can see the RDD checkpoint can help to
> serialization
> > the status of RDD. I can image to load the check point from another node
> > when error happen, but seems like will lost track all tasks status or
> even
> > executor information that maintain in spark context. I am not sure if
> there
> > is any existing stuff I can leverage to do that. thanks for any suggests
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  [image: 2D barcode - encoded with contact information] *Phone:
> *86-10-82452683
> >
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
>
>

--001a11c2d9783af40b0509e0128b--

From dev-return-10724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 17:44:47 2014
Return-Path: <dev-return-10724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91FE110E5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 17:44:47 +0000 (UTC)
Received: (qmail 82440 invoked by uid 500); 10 Dec 2014 17:44:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82362 invoked by uid 500); 10 Dec 2014 17:44:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82351 invoked by uid 99); 10 Dec 2014 17:44:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:44:45 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alee526@hotmail.com designates 65.55.111.95 as permitted sender)
Received: from [65.55.111.95] (HELO BLU004-OMC2S20.hotmail.com) (65.55.111.95)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:44:39 +0000
Received: from BLU184-W30 ([65.55.111.73]) by BLU004-OMC2S20.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Wed, 10 Dec 2014 09:44:18 -0800
X-TMN: [p0cYgffKhRhFGo7zNqlEJC5S63fODlI2]
X-Originating-Email: [alee526@hotmail.com]
Message-ID: <BLU184-W301F9045B30DAF51D19204F3620@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_060d87ba-68c4-45bf-b323-4e5537946a9c_"
From: Andrew Lee <alee526@hotmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Build Spark 1.2.0-rc1 encounter exceptions when running HiveContext
 - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
Date: Wed, 10 Dec 2014 09:44:18 -0800
Importance: Normal
MIME-Version: 1.0
X-OriginalArrivalTime: 10 Dec 2014 17:44:18.0694 (UTC) FILETIME=[EC626A60:01D014A0]
X-Virus-Checked: Checked by ClamAV on apache.org

--_060d87ba-68c4-45bf-b323-4e5537946a9c_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Hi All=2C
I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.sh t=
o include auxiliaries JARs and datanucleus*.jars from Hive=2C however=2C wh=
en I run HiveContext=2C it gives me the following error:
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.or=
g.objenesis.strategy.InstantiatorStrategy
I have checked the JARs with (jar tf)=2C looks like this is already include=
d (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which=
 is configured in the System classpath already. I couldn't figure out what =
is going on with the shading on the esotericsoftware JARs here. Any help is=
 appreciated.
How to reproduce the problem?Run the following 3 statements in spark-shell =
( This is how I launched my spark-shell. cd /opt/spark=3B ./bin/spark-shell=
 --master yarn --deploy-mode client --queue research --driver-memory 1024M)
import org.apache.spark.SparkContextval hiveContext =3D new org.apache.spar=
k.sql.hive.HiveContext(sc)hiveContext.hql("CREATE TABLE IF NOT EXISTS spark=
_hive_test_table (key INT=2C value STRING)")

A reference of my environment.Apache Hadoop 2.4.1Apache Hive 0.13.1Apache S=
park branch-1.2 (installed under /opt/spark/=2C and config under /etc/spark=
/)Maven build command:=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=3D2.4.1=
 -Dyarn.version=3D2.4.1 -Dhive.version=3D0.13.1 -DskipTests install
Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=

My spark-env.sh have the following contents when I executed spark-shell:=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
HADOOP_HOME=3D/opt/hadoop/=0A=
HIVE_HOME=3D/opt/hive/=0A=
HADOOP_CONF_DIR=3D/etc/hadoop/=0A=
YARN_CONF_DIR=3D/etc/hadoop/=0A=
HIVE_CONF_DIR=3D/etc/hive/=0A=
HADOOP_SNAPPY_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -n=
ame "snappy-java-*.jar")=0A=
HADOOP_LZO_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -name=
 "hadoop-lzo-*.jar")=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
SPARK_YARN_DIST_FILES=3D/user/spark/libs/spark-assembly-1.2.0-hadoop2.4.1.j=
ar=0A=
export JAVA_LIBRARY_PATH=3D$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native=0A=
export LD_LIBRARY_PATH=3D$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native=0A=
export SPARK_LIBRARY_PATH=3D$SPARK_LIBRARY_PATH:$HADOOP_HOME/lib/native=0A=
export SPARK_CLASSPATH=3D$SPARK_CLASSPATH:$HADOOP_SNAPPY_JAR:$HADOOP_LZO_JA=
R:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/lib/=
datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
Here's what I see from my stack trace.
warning: there were 1 deprecation warning(s)=3B re-run with -deprecation fo=
r details=0A=
Hive history file=3D/home/hive/log/alti-test-01/hive_job_log_b5db9539-4736-=
44b3-a601-04fa77cb6730_1220828461.txt=0A=
java.lang.NoClassDefFoundError: com/esotericsoftware/shaded/org/objenesis/s=
trategy/InstantiatorStrategy=0A=
	at org.apache.hadoop.hive.ql.exec.Utilities.<clinit>(Utilities.java:925)=
=0A=
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAnaly=
zer.java:9718)=0A=
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAnaly=
zer.java:9712)=0A=
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)=0A=
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)=0A=
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)=0A=
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)=0A=
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)=0A=
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)=0A=
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)=0A=
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)=
=0A=
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzyc=
ompute(NativeCommand.scala:35)=0A=
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(Nati=
veCommand.scala:35)=0A=
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)=
=0A=
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand=
.scala:30)=0A=
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLCont=
ext.scala:425)=0A=
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:4=
25)=0A=
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)=
=0A=
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)=0A=
	at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:102)=0A=
	at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:106)=0A=
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:16)=0A=
	at $iwC$$iwC$$iwC.<init>(<console>:21)=0A=
	at $iwC$$iwC.<init>(<console>:23)=0A=
	at $iwC.<init>(<console>:25)=0A=
	at <init>(<console>:27)=0A=
	at .<init>(<console>:31)=0A=
	at .<clinit>(<console>)=0A=
	at .<init>(<console>:7)=0A=
	at .<clinit>(<console>)=0A=
	at $print(<console>)=0A=
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)=0A=
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.ja=
va:57)=0A=
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccesso=
rImpl.java:43)=0A=
	at java.lang.reflect.Method.invoke(Method.java:606)=0A=
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:85=
2)=0A=
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:11=
25)=0A=
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)=
=0A=
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)=0A=
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)=0A=
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828=
)=0A=
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala=
:873)=0A=
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)=0A=
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:628)=0A=
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:636)=0A=
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)=0A=
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkI=
Loop.scala:968)=0A=
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.sc=
ala:916)=0A=
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.sc=
ala:916)=0A=
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLo=
ader.scala:135)=0A=
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)=0A=
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)=0A=
	at org.apache.spark.repl.Main$.main(Main.scala:31)=0A=
	at org.apache.spark.repl.Main.main(Main.scala)=0A=
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)=0A=
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.ja=
va:57)=0A=
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccesso=
rImpl.java:43)=0A=
	at java.lang.reflect.Method.invoke(Method.java:606)=0A=
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:353)=0A=
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)=0A=
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)=0A=
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.or=
g.objenesis.strategy.InstantiatorStrategy=0A=
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)=0A=
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)=0A=
	at java.security.AccessController.doPrivileged(Native Method)=0A=
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)=0A=
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)=0A=
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)=0A=
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)=0A=
	... 61 more=0A=

 		 	   		  =

--_060d87ba-68c4-45bf-b323-4e5537946a9c_--

From dev-return-10725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 17:48:56 2014
Return-Path: <dev-return-10725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33D7E10E7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 17:48:56 +0000 (UTC)
Received: (qmail 88998 invoked by uid 500); 10 Dec 2014 17:48:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88928 invoked by uid 500); 10 Dec 2014 17:48:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88914 invoked by uid 99); 10 Dec 2014 17:48:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:48:53 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alee526@hotmail.com designates 65.55.111.93 as permitted sender)
Received: from [65.55.111.93] (HELO BLU004-OMC2S18.hotmail.com) (65.55.111.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 17:48:47 +0000
Received: from BLU184-W76 ([65.55.111.72]) by BLU004-OMC2S18.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Wed, 10 Dec 2014 09:48:26 -0800
X-TMN: [ro5mogwCMnsGJ2LtD7PCtDgYu92eP3oO]
X-Originating-Email: [alee526@hotmail.com]
Message-ID: <BLU184-W76EEA650AC39A61793316EF3620@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_047ff8f7-62a5-4a8e-a1e8-55b5b820e199_"
From: Andrew Lee <alee526@hotmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
Date: Wed, 10 Dec 2014 09:48:26 -0800
Importance: Normal
In-Reply-To: <BLU184-W301F9045B30DAF51D19204F3620@phx.gbl>
References: <BLU184-W301F9045B30DAF51D19204F3620@phx.gbl>
MIME-Version: 1.0
X-OriginalArrivalTime: 10 Dec 2014 17:48:26.0724 (UTC) FILETIME=[8038CA40:01D014A1]
X-Virus-Checked: Checked by ClamAV on apache.org

--_047ff8f7-62a5-4a8e-a1e8-55b5b820e199_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Apologize for the format=2C somehow it got messed up and linefeed were remo=
ved. Here's a reformatted version.
Hi All=2C
I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.sh t=
o include auxiliaries JARs and datanucleus*.jars from Hive=2C however=2C wh=
en I run HiveContext=2C it gives me the following error:

Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.or=
g.objenesis.strategy.InstantiatorStrategy

I have checked the JARs with (jar tf)=2C looks like this is already include=
d (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which=
 is configured in the System classpath already. I couldn't figure out what =
is going on with the shading on the esotericsoftware JARs here.  Any help i=
s appreciated.


How to reproduce the problem?
Run the following 3 statements in spark-shell ( This is how I launched my s=
park-shell. cd /opt/spark=3B ./bin/spark-shell --master yarn --deploy-mode =
client --queue research --driver-memory 1024M)

import org.apache.spark.SparkContext
val hiveContext =3D new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.hql("CREATE TABLE IF NOT EXISTS spark_hive_test_table (key INT=
=2C value STRING)")



A reference of my environment.
Apache Hadoop 2.4.1
Apache Hive 0.13.1
Apache Spark branch-1.2 (installed under /opt/spark/=2C and config under /e=
tc/spark/)
Maven build command:

mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=3D2.4.1=
 -Dyarn.version=3D2.4.1 -Dhive.version=3D0.13.1 -DskipTests install

Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82

My spark-env.sh have the following contents when I executed spark-shell:
> HADOOP_HOME=3D/opt/hadoop/
> HIVE_HOME=3D/opt/hive/
> HADOOP_CONF_DIR=3D/etc/hadoop/
> YARN_CONF_DIR=3D/etc/hadoop/
> HIVE_CONF_DIR=3D/etc/hive/
> HADOOP_SNAPPY_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f =
-name "snappy-java-*.jar")
> HADOOP_LZO_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -na=
me "hadoop-lzo-*.jar")
> SPARK_YARN_DIST_FILES=3D/user/spark/libs/spark-assembly-1.2.0-hadoop2.4.1=
.jar
> export JAVA_LIBRARY_PATH=3D$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native
> export LD_LIBRARY_PATH=3D$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native
> export SPARK_LIBRARY_PATH=3D$SPARK_LIBRARY_PATH:$HADOOP_HOME/lib/native
> export SPARK_CLASSPATH=3D$SPARK_CLASSPATH:$HADOOP_SNAPPY_JAR:$HADOOP_LZO_=
JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/li=
b/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar


> Here's what I see from my stack trace.
> warning: there were 1 deprecation warning(s)=3B re-run with -deprecation =
for details
> Hive history file=3D/home/hive/log/alti-test-01/hive_job_log_b5db9539-473=
6-44b3-a601-04fa77cb6730_1220828461.txt
> java.lang.NoClassDefFoundError: com/esotericsoftware/shaded/org/objenesis=
/strategy/InstantiatorStrategy
> 	at org.apache.hadoop.hive.ql.exec.Utilities.<clinit>(Utilities.java:925)
> 	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAna=
lyzer.java:9718)
> 	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAna=
lyzer.java:9712)
> 	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
> 	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
> 	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
> 	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
> 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
> 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
> 	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
> 	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:27=
6)
> 	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lz=
ycompute(NativeCommand.scala:35)
> 	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(Na=
tiveCommand.scala:35)
> 	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:4=
6)
> 	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeComma=
nd.scala:30)
> 	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLCo=
ntext.scala:425)
> 	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala=
:425)
> 	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:5=
8)
> 	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
> 	at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:102)
> 	at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:106)
> 	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:16)
> 	at $iwC$$iwC$$iwC.<init>(<console>:21)
> 	at $iwC$$iwC.<init>(<console>:23)
> 	at $iwC.<init>(<console>:25)
> 	at <init>(<console>:27)
> 	at .<init>(<console>:31)
> 	at .<clinit>(<console>)
> 	at .<init>(<console>:7)
> 	at .<clinit>(<console>)
> 	at $print(<console>)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.=
java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces=
sorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:=
852)
> 	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:=
1125)
> 	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674=
)
> 	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
> 	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
> 	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:8=
28)
> 	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.sca=
la:873)
> 	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
> 	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:628)
> 	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:636)
> 	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)
> 	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(Spar=
kILoop.scala:968)
> 	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.=
scala:916)
> 	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.=
scala:916)
> 	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClass=
Loader.scala:135)
> 	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
> 	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
> 	at org.apache.spark.repl.Main$.main(Main.scala:31)
> 	at org.apache.spark.repl.Main.main(Main.scala)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.=
java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces=
sorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:353)
> 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
> 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.=
org.objenesis.strategy.InstantiatorStrategy
> 	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> 	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> 	at java.security.AccessController.doPrivileged(Native Method)
> 	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> 	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
> 	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> 	... 61 more
>=20
>  		 	   		 =20
 		 	   		  =

--_047ff8f7-62a5-4a8e-a1e8-55b5b820e199_--

From dev-return-10726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 19:29:11 2014
Return-Path: <dev-return-10726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC721961B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 19:29:11 +0000 (UTC)
Received: (qmail 54943 invoked by uid 500); 10 Dec 2014 19:29:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54876 invoked by uid 500); 10 Dec 2014 19:29:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54865 invoked by uid 99); 10 Dec 2014 19:29:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 19:29:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.49 as permitted sender)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 19:29:06 +0000
Received: by mail-la0-f49.google.com with SMTP id hs14so2908363lab.8
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 11:28:44 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=u8GR9H/XUfIyhye7Q49tkUKxn5j877UDzSAGA7n8cLI=;
        b=fEH40Y+Oak7Tv14+nLjEC5c8LAo2rxBOfAV0zoaM+3OC2vnYr+x/gPIwWzs3j9UZHf
         zhabWD/RNOzVmdI2thj8MrTLP1UpnsdMKoax/3ryaKV4brJqgEHDggrw88XHoS3xc1Sa
         Gd3prGGGT1DguPfMJguWyAUlD+gcTrgq7owRU8P1oNlzc9udqrtdc84ll/82Ynq+tS7z
         QF/B8MHx10yKgFOHh6AuF2jZvJQG81imGUukUsoxfMUCcvaxedkToaSdKnRjhH++rLj2
         b+ZTQnv84SV3ijv68oAl5+OGIobmZ8YZck0CfN2GecZqJQggwEeb0jvC7UMQ+JlAVQbb
         qQ3w==
X-Gm-Message-State: ALoCoQkxsGi99rhSV5rsDSaR0cBxwlVQ2AvRp7Glshmk+niBE46ALKL6hB5Tim6Vp3zC73Q99K4G
X-Received: by 10.112.42.198 with SMTP id q6mr5684008lbl.69.1418239724539;
 Wed, 10 Dec 2014 11:28:44 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Wed, 10 Dec 2014 11:28:23 -0800 (PST)
In-Reply-To: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 10 Dec 2014 11:28:23 -0800
Message-ID: <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11347786fcde2d0509e1aa7f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11347786fcde2d0509e1aa7f
Content-Type: text/plain; charset=UTF-8

reminder -- this is happening friday morning @ 730am!

On Mon, Dec 1, 2014 at 5:10 PM, shane knapp <sknapp@berkeley.edu> wrote:

> i'll send out a reminder next week, but i wanted to give a heads up:  i'll
> be bringing down the entire jenkins infrastructure for reboots and system
> updates.
>
> please let me know if there are any conflicts with this, thanks!
>
> shane
>

--001a11347786fcde2d0509e1aa7f--

From dev-return-10727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 19:45:32 2014
Return-Path: <dev-return-10727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B6C89738
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 19:45:32 +0000 (UTC)
Received: (qmail 3239 invoked by uid 500); 10 Dec 2014 19:45:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3162 invoked by uid 500); 10 Dec 2014 19:45:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2625 invoked by uid 99); 10 Dec 2014 19:45:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 19:45:24 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 19:45:19 +0000
Received: by mail-lb0-f177.google.com with SMTP id b6so2926647lbj.36
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 11:44:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=AV3qb3qRWjDlGvRrxxLFkrYCO7Wvgs9QGoSwZXaGwbQ=;
        b=l29IHeP8X92kIFPjpWds3bqRlTyngnV8UKKoFWe8A4YFRNVkQ86aUgBdQBYvCS+bvT
         uBolfRlBYMofNU6gsmfAbsDCqJSYfpCrvMVQppI8B40QaC4bN5twiYm6eYk8RoSyD26s
         SiispHyzSqMDMRodKwu9j3qobARfAnea2qTA359ryKF4Oj35/lCNSc/2sNvYTCmARQXp
         eQHsnRxTHlXHY49zH20NyIc9Bp6mLTBXcjHc8BDteZBrG2Bvvm2XQboQUiCqVH7UJNkv
         kJTmeMXaBqhQGrz6otcRMokhKkHEO3sS8lH1oBspAQ1cZluFHY8Dz/t7FcrQ4e+pIkVR
         XryQ==
MIME-Version: 1.0
X-Received: by 10.112.147.199 with SMTP id tm7mr5894777lbb.92.1418240698360;
 Wed, 10 Dec 2014 11:44:58 -0800 (PST)
Received: by 10.25.212.144 with HTTP; Wed, 10 Dec 2014 11:44:58 -0800 (PST)
Date: Wed, 10 Dec 2014 11:44:58 -0800
Message-ID: <CA+B-+fyHmpvyByMHV9=BWFxGeLfgLEprOmX3AKKVQeTM5L74LA@mail.gmail.com>
Subject: Row Similarity
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a893c082cf50509e1e524
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a893c082cf50509e1e524
Content-Type: text/plain; charset=UTF-8

Hi,

It seems there are multiple places where we would like to compute row
similarity (accurate or approximate similarities)

Basically through RowMatrix columnSimilarities we can compute column
similarities of a tall skinny matrix

Similarly we should have an API in RowMatrix called rowSimilarities where
we can compute similar rows in a map-reduce fashion. It will be useful for
following use-cases:

1. Generate topK users for each user from matrix factorization model
2. Generate topK products for each product from matrix factorization model
3. Generate kernel matrix for use in spectral clustering
4. Generate kernel matrix for use in kernel regression/classification

I am not sure if there are already good implementation for map-reduce row
similarity that we can use (ideas like fastfood and kitchen sink felt more
like for classification use-case but for recommendation also user
similarities show up which is unsupervised)...

Is there a JIRA tracking it ? If not I can open one and we can discuss
further on it.

Thanks.
Deb

--047d7b3a893c082cf50509e1e524--

From dev-return-10728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 20:23:36 2014
Return-Path: <dev-return-10728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 57E9A9964
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 20:23:36 +0000 (UTC)
Received: (qmail 27185 invoked by uid 500); 10 Dec 2014 20:23:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27095 invoked by uid 500); 10 Dec 2014 20:23:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27083 invoked by uid 99); 10 Dec 2014 20:23:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 20:23:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 20:23:29 +0000
Received: by mail-ob0-f170.google.com with SMTP id wp18so2903800obc.1
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 12:23:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=XFZkCt6Nae08F7GblCUc4SUnzWr1AeYbnFwzqODHgtk=;
        b=dQMMp6hMHu2duvFB1JfS9eEQWKS65+8lCZbMA9aPbvDMHuH6Fb2j71NOCRxtpBD4wg
         8F4YUL2yzcR8p5GZlXxZXuEEM8nEwAI0yyKSGGTYSFCdSUD07qLZDUEZNNNeb3a/o6bd
         AGysYGuTrhbvkLQ/XKdkV4wSPESQFXEi/Y5KWEXglPPzbhieDUzj4ZJPo62fXfAvKxX0
         ZvD4bUIkZHB/bJw91GG4e+48Jx3NE7AMXul+Cu77XfRq2Cgeh9svjz33MBRVzNmLjSTw
         jSA7gDOsiASskG4KbeaIi0p5Wc2yvUHxJNcSnbJ62ZsAZ2MpmAxfi1PUZE+8DTplHvCI
         8qqw==
MIME-Version: 1.0
X-Received: by 10.202.168.204 with SMTP id r195mr3815179oie.72.1418242988980;
 Wed, 10 Dec 2014 12:23:08 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Wed, 10 Dec 2014 12:23:08 -0800 (PST)
In-Reply-To: <BLU184-W76EEA650AC39A61793316EF3620@phx.gbl>
References: <BLU184-W301F9045B30DAF51D19204F3620@phx.gbl>
	<BLU184-W76EEA650AC39A61793316EF3620@phx.gbl>
Date: Wed, 10 Dec 2014 12:23:08 -0800
Message-ID: <CABPQxsuRZqFtd4DPM1emG_aD0C27DChWQsJA6wYpF6n3RXLdyQ@mail.gmail.com>
Subject: Re: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
From: Patrick Wendell <pwendell@gmail.com>
To: Andrew Lee <alee526@hotmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Andrew,

It looks like somehow you are including jars from the upstream Apache
Hive 0.13 project on your classpath. For Spark 1.2 Hive 0.13 support,
we had to modify Hive to use a different version of Kryo that was
compatible with Spark's Kryo version.

https://github.com/pwendell/hive/commit/5b582f242946312e353cfce92fc3f3fa472=
aedf3

I would look through the actual classpath and make sure you aren't
including your own hive-exec jar somehow.

- Patrick

On Wed, Dec 10, 2014 at 9:48 AM, Andrew Lee <alee526@hotmail.com> wrote:
> Apologize for the format, somehow it got messed up and linefeed were remo=
ved. Here's a reformatted version.
> Hi All,
> I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.sh=
 to include auxiliaries JARs and datanucleus*.jars from Hive, however, when=
 I run HiveContext, it gives me the following error:
>
> Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.=
org.objenesis.strategy.InstantiatorStrategy
>
> I have checked the JARs with (jar tf), looks like this is already include=
d (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which=
 is configured in the System classpath already. I couldn't figure out what =
is going on with the shading on the esotericsoftware JARs here.  Any help i=
s appreciated.
>
>
> How to reproduce the problem?
> Run the following 3 statements in spark-shell ( This is how I launched my=
 spark-shell. cd /opt/spark; ./bin/spark-shell --master yarn --deploy-mode =
client --queue research --driver-memory 1024M)
>
> import org.apache.spark.SparkContext
> val hiveContext =3D new org.apache.spark.sql.hive.HiveContext(sc)
> hiveContext.hql("CREATE TABLE IF NOT EXISTS spark_hive_test_table (key IN=
T, value STRING)")
>
>
>
> A reference of my environment.
> Apache Hadoop 2.4.1
> Apache Hive 0.13.1
> Apache Spark branch-1.2 (installed under /opt/spark/, and config under /e=
tc/spark/)
> Maven build command:
>
> mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=3D2.4=
.1 -Dyarn.version=3D2.4.1 -Dhive.version=3D0.13.1 -DskipTests install
>
> Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82
>
> My spark-env.sh have the following contents when I executed spark-shell:
>> HADOOP_HOME=3D/opt/hadoop/
>> HIVE_HOME=3D/opt/hive/
>> HADOOP_CONF_DIR=3D/etc/hadoop/
>> YARN_CONF_DIR=3D/etc/hadoop/
>> HIVE_CONF_DIR=3D/etc/hive/
>> HADOOP_SNAPPY_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f=
 -name "snappy-java-*.jar")
>> HADOOP_LZO_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -n=
ame "hadoop-lzo-*.jar")
>> SPARK_YARN_DIST_FILES=3D/user/spark/libs/spark-assembly-1.2.0-hadoop2.4.=
1.jar
>> export JAVA_LIBRARY_PATH=3D$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native
>> export LD_LIBRARY_PATH=3D$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native
>> export SPARK_LIBRARY_PATH=3D$SPARK_LIBRARY_PATH:$HADOOP_HOME/lib/native
>> export SPARK_CLASSPATH=3D$SPARK_CLASSPATH:$HADOOP_SNAPPY_JAR:$HADOOP_LZO=
_JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/l=
ib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
>
>
>> Here's what I see from my stack trace.
>> warning: there were 1 deprecation warning(s); re-run with -deprecation f=
or details
>> Hive history file=3D/home/hive/log/alti-test-01/hive_job_log_b5db9539-47=
36-44b3-a601-04fa77cb6730_1220828461.txt
>> java.lang.NoClassDefFoundError: com/esotericsoftware/shaded/org/objenesi=
s/strategy/InstantiatorStrategy
>>       at org.apache.hadoop.hive.ql.exec.Utilities.<clinit>(Utilities.jav=
a:925)
>>       at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(Seman=
ticAnalyzer.java:9718)
>>       at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(Seman=
ticAnalyzer.java:9712)
>>       at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
>>       at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
>>       at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:97=
5)
>>       at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
>>       at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
>>       at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
>>       at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala=
:305)
>>       at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.sc=
ala:276)
>>       at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectRes=
ult$lzycompute(NativeCommand.scala:35)
>>       at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectRes=
ult(NativeCommand.scala:35)
>>       at org.apache.spark.sql.execution.Command$class.execute(commands.s=
cala:46)
>>       at org.apache.spark.sql.hive.execution.NativeCommand.execute(Nativ=
eCommand.scala:30)
>>       at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute=
(SQLContext.scala:425)
>>       at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext=
.scala:425)
>>       at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.s=
cala:58)
>>       at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
>>       at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:=
102)
>>       at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:106=
)
>>       at $iwC$$iwC$$iwC$$iwC.<init>(<console>:16)
>>       at $iwC$$iwC$$iwC.<init>(<console>:21)
>>       at $iwC$$iwC.<init>(<console>:23)
>>       at $iwC.<init>(<console>:25)
>>       at <init>(<console>:27)
>>       at .<init>(<console>:31)
>>       at .<clinit>(<console>)
>>       at .<init>(<console>:7)
>>       at .<clinit>(<console>)
>>       at $print(<console>)
>>       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccesso=
rImpl.java:57)
>>       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho=
dAccessorImpl.java:43)
>>       at java.lang.reflect.Method.invoke(Method.java:606)
>>       at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.=
scala:852)
>>       at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.=
scala:1125)
>>       at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.sca=
la:674)
>>       at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705=
)
>>       at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669=
)
>>       at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.s=
cala:828)
>>       at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILo=
op.scala:873)
>>       at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
>>       at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala=
:628)
>>       at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:6=
36)
>>       at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)
>>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$s=
p(SparkILoop.scala:968)
>>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(Spark=
ILoop.scala:916)
>>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(Spark=
ILoop.scala:916)
>>       at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(Scal=
aClassLoader.scala:135)
>>       at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
>>       at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
>>       at org.apache.spark.repl.Main$.main(Main.scala:31)
>>       at org.apache.spark.repl.Main.main(Main.scala)
>>       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccesso=
rImpl.java:57)
>>       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho=
dAccessorImpl.java:43)
>>       at java.lang.reflect.Method.invoke(Method.java:606)
>>       at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:3=
53)
>>       at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
>>       at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
>> Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded=
.org.objenesis.strategy.InstantiatorStrategy
>>       at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>>       at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>>       at java.security.AccessController.doPrivileged(Native Method)
>>       at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>>       at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>>       at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>>       at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>>       ... 61 more
>>
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 21:08:27 2014
Return-Path: <dev-return-10729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 764BD9BD2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 21:08:27 +0000 (UTC)
Received: (qmail 77123 invoked by uid 500); 10 Dec 2014 21:08:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77056 invoked by uid 500); 10 Dec 2014 21:08:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77036 invoked by uid 99); 10 Dec 2014 21:08:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 21:08:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 21:08:19 +0000
Received: by mail-oi0-f49.google.com with SMTP id i138so2643117oig.36
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 13:07:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:cc:content-type;
        bh=BRY5RMKPGlrBmzM9RumFcfEKN8iZb4lPCFyV5RihpO0=;
        b=I6X1e0VENlTRsTx40ORv+xKS2PCr88bUaAPZv7mbpXXeCFVtmjmQ73MqgF845UMnWK
         I10ut4iayZpVYDaXUR8V2oHNO9LRd55hXvJ8UZ62lx+IfU0arh1FPtoRLX10Y/2OuS/g
         5zNUBR4kVpDp0PeZyzhEvuDFzvuEy9dw4NdDfc8go5FE0ZTH5AlZQhrHNVwPk4+tdDZ9
         LHZI12RtMG7WiYFnwV6nvwUwhMbQ12ihdyax0jX0I6b244pd1L4m1hvjrPlE/KbykWLw
         BeuWBqKNM+gWet+S+7ePYBkc8Xr7dPN2QUfjlvktJuwsbqjjNWhif+2VcI8XFtm71ajQ
         T3DQ==
MIME-Version: 1.0
X-Received: by 10.182.79.41 with SMTP id g9mr4061367obx.14.1418245634463; Wed,
 10 Dec 2014 13:07:14 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Wed, 10 Dec 2014 13:07:14 -0800 (PST)
Date: Wed, 10 Dec 2014 13:07:14 -0800
Message-ID: <CABPQxssUd92YXULqV6z1SK50CzXcP_LL3JzbtqJtCfEO74pYWw@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: Takeshi Yamamuro <linguin.m.s@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is closed in favor of RC2.

On Fri, Dec 5, 2014 at 2:02 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey All,
>
> Thanks all for the continued testing!
>
> The issue I mentioned earlier SPARK-4498 was fixed earlier this week
> (hat tip to Mark Hamstra who contributed to fix).
>
> In the interim a few smaller blocker-level issues with Spark SQL were
> found and fixed (SPARK-4753, SPARK-4552, SPARK-4761).
>
> There is currently an outstanding issue (SPARK-4740[1]) in Spark core
> that needs to be fixed.
>
> I want to thank in particular Shopify and Intel China who have
> identified and helped test blocker issues with the release. This type
> of workload testing around releases is really helpful for us.
>
> Once things stabilize I will cut RC2. I think we're pretty close with this one.
>
> - Patrick
>
> On Wed, Dec 3, 2014 at 5:38 PM, Takeshi Yamamuro <linguin.m.s@gmail.com> wrote:
>> +1 (non-binding)
>>
>> Checked on CentOS 6.5, compiled from the source.
>> Ran various examples in stand-alone master and three slaves, and
>> browsed the web UI.
>>
>> On Sat, Nov 29, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>>> Please vote on releasing the following candidate as Apache Spark version
>>> 1.2.0!
>>>
>>> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
>>>
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>>>
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1048/
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>>>
>>> Please vote on releasing this package as Apache Spark 1.2.0!
>>>
>>> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
>>> if a majority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 1.1.0
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> == What justifies a -1 vote for this release? ==
>>> This vote is happening very late into the QA period compared with
>>> previous votes, so -1 votes should only occur for significant
>>> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>>> regressions, or bugs related to new features will not block this
>>> release.
>>>
>>> == What default changes should I be aware of? ==
>>> 1. The default value of "spark.shuffle.blockTransferService" has been
>>> changed to "netty"
>>> --> Old behavior can be restored by switching to "nio"
>>>
>>> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
>>> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>>> "hash".
>>>
>>> == Other notes ==
>>> Because this vote is occurring over a weekend, I will likely extend
>>> the vote if this RC survives until the end of the vote period.
>>>
>>> - Patrick
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 21:09:56 2014
Return-Path: <dev-return-10730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 80B289BE1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 21:09:56 +0000 (UTC)
Received: (qmail 83222 invoked by uid 500); 10 Dec 2014 21:09:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83147 invoked by uid 500); 10 Dec 2014 21:09:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83132 invoked by uid 99); 10 Dec 2014 21:09:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 21:09:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 21:09:49 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so2698882oib.16
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 13:08:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=QR8EBnMzZXhSrnWYG9UqlgTKRrHm40XA+itM/UwsfOg=;
        b=AbjxmD4/gzE2LbfH1rwoJxlrcq9kqCbBWIBwLfNExgdd4CTU9FZ6ywzznD7ANkHAD2
         WxMiqujdcfuRwVUmiEpciV2amm7dNk+LMPLGSU5O7AsCnsWHMcEAE2LViHSIgtCLpWs3
         F63YbWiJISUOqi2wYtUa73JsKOD0NskVZbQ1SEaF/9Cwg6n3+U/yxL1bod9/rDN5ypcJ
         p8/nSfKKEUY9Eh2JwVZNK3NAJUvANSlgcfhItlLLXFeiLpc7ByrW/NV6nIzIjADnDkvl
         dekaSMp24QpkWWvTD15+eU9fYqOiYpbp7ZN1rpFK8h/o6WZw1yXFI8wXHr6oDIqCVKaq
         dM1A==
MIME-Version: 1.0
X-Received: by 10.60.58.65 with SMTP id o1mr4158474oeq.30.1418245724115; Wed,
 10 Dec 2014 13:08:44 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Wed, 10 Dec 2014 13:08:44 -0800 (PST)
Date: Wed, 10 Dec 2014 13:08:44 -0800
Message-ID: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.2.0!

The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1055/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.2.0!

The vote is open until Saturday, December 13, at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening relatively late into the QA period, so
-1 votes should only occur for significant regressions from
1.0.2. Bugs already present in 1.1.X, minor
regressions, or bugs related to new features will not block this
release.

== What default changes should I be aware of? ==
1. The default value of "spark.shuffle.blockTransferService" has been
changed to "netty"
--> Old behavior can be restored by switching to "nio"

2. The default value of "spark.shuffle.manager" has been changed to "sort".
--> Old behavior can be restored by setting "spark.shuffle.manager" to "hash".

== How does this differ from RC1 ==
This has fixes for a handful of issues identified - some of the
notable fixes are:

[Core]
SPARK-4498: Standalone Master can fail to recognize completed/failed
applications

[SQL]
SPARK-4552: Query for empty parquet table in spark sql hive get
IllegalArgumentException
SPARK-4753: Parquet2 does not prune based on OR filters on partition columns
SPARK-4761: With JDBC server, set Kryo as default serializer and
disable reference tracking
SPARK-4785: When called with arguments referring column fields, PMOD throws NPE

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 22:32:26 2014
Return-Path: <dev-return-10731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3540100E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 22:32:25 +0000 (UTC)
Received: (qmail 99786 invoked by uid 500); 10 Dec 2014 22:32:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99711 invoked by uid 500); 10 Dec 2014 22:32:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99700 invoked by uid 99); 10 Dec 2014 22:32:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 22:32:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 22:31:56 +0000
Received: by mail-ig0-f177.google.com with SMTP id z20so3794365igj.10
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 14:30:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wmJQGhbhQ2YloAeqR+Ovq+MbHf2T6tN6VE/8ogdfXSw=;
        b=ei8CUKZrZ278nrSDnWHnmJxV98rR94fKHZ5I4CPnrn+f4lgQb7wZyR3Grmsl1gHJn5
         CGXNgr5wNfLcY67E7ArQFMMkvR8oFQyOc7RpPL2GeLLzyOaWWm3qa1+rreU7UMS72F3L
         61sYhfQM3+tNE1NCr8AUgCQsbvoL3MgSFB7sNRWnjWC2L9nLMcghphHvYulPnwQWLQ7+
         9QDqxyAfDBOsWX8Xcoy7IDepetgfObbt6sTHUYg3vesNkCqIHNRadzVt8WvySs2eMM4v
         jAXezqDhQUw0guZiCs/29zGnOjvFR92gkHvrtTbLeLEp9stsaciNgRo1sPThHpqMxemc
         Nq0g==
X-Gm-Message-State: ALoCoQkrx8VS1JSvRBCFhiBWrn5QVaKIk/GNX/xyp6vYaSR2YnjADoFzJyederMljBawJepW1G+B
MIME-Version: 1.0
X-Received: by 10.107.15.226 with SMTP id 95mr6820866iop.12.1418250650065;
 Wed, 10 Dec 2014 14:30:50 -0800 (PST)
Received: by 10.107.37.131 with HTTP; Wed, 10 Dec 2014 14:30:50 -0800 (PST)
In-Reply-To: <CA+B-+fyHmpvyByMHV9=BWFxGeLfgLEprOmX3AKKVQeTM5L74LA@mail.gmail.com>
References: <CA+B-+fyHmpvyByMHV9=BWFxGeLfgLEprOmX3AKKVQeTM5L74LA@mail.gmail.com>
Date: Wed, 10 Dec 2014 17:30:50 -0500
Message-ID: <CAHuE29b7NBSkr1PKBjXGRgddfKCLmQAzX3TP5qf+90jib6ezEw@mail.gmail.com>
Subject: Re: Row Similarity
From: Reza Zadeh <reza@databricks.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fe428333a7d0509e436f7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fe428333a7d0509e436f7
Content-Type: text/plain; charset=UTF-8

It's not so cheap to compute row similarities when there are many rows, as
it amounts to computing the outer product of a matrix A (i.e. computing
AA^T, which is expensive).

There is a JIRA to track handling (1) and (2) more efficiently than
computing all pairs: https://issues.apache.org/jira/browse/SPARK-3066



On Wed, Dec 10, 2014 at 2:44 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> It seems there are multiple places where we would like to compute row
> similarity (accurate or approximate similarities)
>
> Basically through RowMatrix columnSimilarities we can compute column
> similarities of a tall skinny matrix
>
> Similarly we should have an API in RowMatrix called rowSimilarities where
> we can compute similar rows in a map-reduce fashion. It will be useful for
> following use-cases:
>
> 1. Generate topK users for each user from matrix factorization model
> 2. Generate topK products for each product from matrix factorization model
> 3. Generate kernel matrix for use in spectral clustering
> 4. Generate kernel matrix for use in kernel regression/classification
>
> I am not sure if there are already good implementation for map-reduce row
> similarity that we can use (ideas like fastfood and kitchen sink felt more
> like for classification use-case but for recommendation also user
> similarities show up which is unsupervised)...
>
> Is there a JIRA tracking it ? If not I can open one and we can discuss
> further on it.
>
> Thanks.
> Deb
>

--001a113fe428333a7d0509e436f7--

From dev-return-10732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 10 23:06:13 2014
Return-Path: <dev-return-10732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE53210252
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 10 Dec 2014 23:06:12 +0000 (UTC)
Received: (qmail 80528 invoked by uid 500); 10 Dec 2014 23:06:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80454 invoked by uid 500); 10 Dec 2014 23:06:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80431 invoked by uid 99); 10 Dec 2014 23:06:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 23:06:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 10 Dec 2014 23:06:04 +0000
Received: by mail-pa0-f52.google.com with SMTP id eu11so3740394pac.11
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 15:05:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=by4L13vqsmv7C/8JpWD7AMabXQacAAt2Uz7WTs8lOe0=;
        b=Ms51OG93/ijkra6jWr6eAqCNCIjWEhhelx04m3roQbPDuJl8nBg99xeD4/zxjuRoC0
         UQoFuYIScdZJAPOQ0QWVYMXWlgmaUKZmotIwS9DsHcfpWK94YO5ypBxf0fZQfmEXmVOC
         zpTbOdAckIypKs/YrhtRrWBOboTXnQMZA6DQ2NjfM0hh16HyGCuEBCXuiYrEKMJKDUfO
         lAGLvv6sxmt86svs+mhnLMMVX2DEvGYkzgwoTCMYzQY/wveeZ6glTJBC4egfnJJSvlsO
         ALtlS4AjRTzE8584GWVb4vUD8KgmIUiX4En/auQyWlH4ntBHw33qq/P8H5FGp8IsxX3G
         sP1Q==
X-Received: by 10.66.231.141 with SMTP id tg13mr11690532pac.122.1418252744302;
        Wed, 10 Dec 2014 15:05:44 -0800 (PST)
Received: from [192.168.1.100] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id vz3sm306395pab.2.2014.12.10.15.05.42
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 10 Dec 2014 15:05:43 -0800 (PST)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
Date: Wed, 10 Dec 2014 15:05:41 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <A7E5DDE3-FC90-484C-8A50-0AB1A5455124@gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested on Mac OS X.

Matei

> On Dec 10, 2014, at 1:08 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>=20
> Please vote on releasing the following candidate as Apache Spark =
version 1.2.0!
>=20
> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Da428c=
446e23e628b746e0626cc02b7b3cadf588e
>=20
> The release files, including signatures, digests, etc. can be found =
at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>=20
> The staging repository for this release can be found at:
> =
https://repository.apache.org/content/repositories/orgapachespark-1055/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.2.0!
>=20
> The vote is open until Saturday, December 13, at 21:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.2.0
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D What justifies a -1 vote for this release? =3D=3D
> This vote is happening relatively late into the QA period, so
> -1 votes should only occur for significant regressions from
> 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
>=20
> =3D=3D What default changes should I be aware of? =3D=3D
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
>=20
> 2. The default value of "spark.shuffle.manager" has been changed to =
"sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to =
"hash".
>=20
> =3D=3D How does this differ from RC1 =3D=3D
> This has fixes for a handful of issues identified - some of the
> notable fixes are:
>=20
> [Core]
> SPARK-4498: Standalone Master can fail to recognize completed/failed
> applications
>=20
> [SQL]
> SPARK-4552: Query for empty parquet table in spark sql hive get
> IllegalArgumentException
> SPARK-4753: Parquet2 does not prune based on OR filters on partition =
columns
> SPARK-4761: With JDBC server, set Kryo as default serializer and
> disable reference tracking
> SPARK-4785: When called with arguments referring column fields, PMOD =
throws NPE
>=20
> - Patrick
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 01:25:21 2014
Return-Path: <dev-return-10733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 789E610713
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 01:25:21 +0000 (UTC)
Received: (qmail 95467 invoked by uid 500); 11 Dec 2014 01:25:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95385 invoked by uid 500); 11 Dec 2014 01:25:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95368 invoked by uid 99); 11 Dec 2014 01:25:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 01:25:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 01:24:52 +0000
Received: by mail-ig0-f177.google.com with SMTP id z20so4002475igj.10
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 17:24:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=7aNLyDRCn/0gKKHpmf0qIGogKkVXIs+xiT8c434sg6k=;
        b=NuMotO6hmMVgn/MAMT4VQjE7yqo53GO45fgTUOcCVG0UXMrUOH8Lnwx0iUshMe3omL
         nrNOB0AxL/lOkQWmMXvAVD4zZTX1WTMfNGfXvHtgf3R0DMpr/4hcD/eWY0zFt/1ACEc3
         xgcdj3tcJ7m+xmY03xgqieFWTK+5X2J8p/ARw9If8ZqQcNgRMLGOxCpsFSOUbZi8BTAh
         JZpXQM/Bj9BZbu3ArSv2BrTwXgfbacRzqvnKY8R1eUgeY3UglssZ4Z/Y5CY0UhXMm/6f
         u6GjHY3c4T2HdmBvGdsTjZsKZ92SzYbILNaLtYRSoIYnufehL5+IUNzK+nxWeFem+lnw
         2x4g==
X-Received: by 10.51.17.107 with SMTP id gd11mr11363816igd.45.1418261091184;
 Wed, 10 Dec 2014 17:24:51 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzfX+1Csr92UC_gzrcFh2CwUDJj+D-cUMexdrepTW6G9dQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 11 Dec 2014 01:24:50 +0000
Message-ID: <CAOhmDze0UcOF0C=X5tkABStEG8aqR+pkHx0ZhgdELK81dtb8OA@mail.gmail.com>
Subject: Re: Is Apache JIRA down?
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136000e89f7830509e6a460
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136000e89f7830509e6a460
Content-Type: text/plain; charset=UTF-8

Nevermind, seems to be back up now.

On Wed Dec 10 2014 at 7:46:30 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> For example: https://issues.apache.org/jira/browse/SPARK-3431
>
> Where do we report/track issues with JIRA itself being down?
>
> Nick
>

--001a1136000e89f7830509e6a460--

From dev-return-10734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 01:26:00 2014
Return-Path: <dev-return-10734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 988361072C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 01:26:00 +0000 (UTC)
Received: (qmail 173 invoked by uid 500); 11 Dec 2014 01:25:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99964 invoked by uid 500); 11 Dec 2014 01:25:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99173 invoked by uid 99); 11 Dec 2014 01:25:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 01:25:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 00:48:02 +0000
Received: by mail-ie0-f180.google.com with SMTP id rp18so3875344iec.39
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 16:46:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=UhyY7OjXJyqzybJru9nSzWd9yLmW6zgyAdTBfRqaCy0=;
        b=p6q21kcwBFITIF9HPU6wNQ+Rw4PgAY8MzzHxTOCLGJSHIAI2OVBUoE63PPNEprjRNa
         AoYMU2U6+6ZjN7NSMLh+6AZPWH7gyhMfGyTstHjPF5XhwtwhISfMKEfrPXvjK9RfxhJU
         kY6RwDL4+DM/Qz86VKwOrXwnXslHp857uT/9joPbmsnwvEQBfNEA267BIOksvD6LF+R0
         zwWb0ourFBGkwPuGuqrsx/xL6CowCxNe5RbyzfB0g/H6mzNc/WFQyOl61NYa7gce5Qwh
         Q9IDqf2h8ehwnR9I4oXTac+bnmI0ZQ99jRPu7OreNso8TUX0RPXJ9ZZGlgEeil1TGZi6
         adpg==
X-Received: by 10.42.26.147 with SMTP id f19mr8457251icc.84.1418258790703;
 Wed, 10 Dec 2014 16:46:30 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 11 Dec 2014 00:46:30 +0000
Message-ID: <CAOhmDzfX+1Csr92UC_gzrcFh2CwUDJj+D-cUMexdrepTW6G9dQ@mail.gmail.com>
Subject: Is Apache JIRA down?
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303f64f66b696c0509e61b65
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303f64f66b696c0509e61b65
Content-Type: text/plain; charset=UTF-8

For example: https://issues.apache.org/jira/browse/SPARK-3431

Where do we report/track issues with JIRA itself being down?

Nick

--20cf303f64f66b696c0509e61b65--

From dev-return-10735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 01:31:53 2014
Return-Path: <dev-return-10735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E708107B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 01:31:53 +0000 (UTC)
Received: (qmail 19513 invoked by uid 500); 11 Dec 2014 01:31:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19437 invoked by uid 500); 11 Dec 2014 01:31:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19425 invoked by uid 99); 11 Dec 2014 01:31:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 01:31:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 01:31:24 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so3024787oib.2
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 17:29:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8tm417LEaKVFgnuM7XJADWFWriYUDSjU7beqBj4wBdw=;
        b=Oypc55mLZ+1cJgnJOAEBjOgYwEV4CRgmNRZW/s6JPV5IvFfEzE5FGfPC5vN849mxPj
         UuVKPYkLLXh18G2rjp2Ekd2r9fJ8tN6MeVGnKOZIcUt5aBa1oOYVg34kke6Wrblh4t1f
         uuGC+f1kttjoN3utvBg1pmsJz1v8YwzJ/QDjq4mpXH64Jfi+WdoD7VIBOqQCkkDAy3Y8
         mn+csnlaqWIAuXdZyq+XeMII/Nu1bT4wdyDIK3TlOIoCqhuVXpAmNBBu7cxkd+3MG6kE
         lI5DPrePdJ9uiD/sih3FWMhT2pyv6LByIMioyLB5j6ksVkiSLVCUyEjRi5sz6KiUE8Db
         zHag==
MIME-Version: 1.0
X-Received: by 10.60.78.167 with SMTP id c7mr4761318oex.18.1418261393344; Wed,
 10 Dec 2014 17:29:53 -0800 (PST)
Received: by 10.202.56.84 with HTTP; Wed, 10 Dec 2014 17:29:53 -0800 (PST)
In-Reply-To: <CAOhmDze0UcOF0C=X5tkABStEG8aqR+pkHx0ZhgdELK81dtb8OA@mail.gmail.com>
References: <CAOhmDzfX+1Csr92UC_gzrcFh2CwUDJj+D-cUMexdrepTW6G9dQ@mail.gmail.com>
	<CAOhmDze0UcOF0C=X5tkABStEG8aqR+pkHx0ZhgdELK81dtb8OA@mail.gmail.com>
Date: Wed, 10 Dec 2014 17:29:53 -0800
Message-ID: <CABPQxssvO+7mfVmgh9J2eJAZ=jCBYp2MGM5S76uPiDL9VucxRw@mail.gmail.com>
Subject: Re: Is Apache JIRA down?
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I believe many apache services are/were down due to an outage.

On Wed, Dec 10, 2014 at 5:24 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Nevermind, seems to be back up now.
>
> On Wed Dec 10 2014 at 7:46:30 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> For example: https://issues.apache.org/jira/browse/SPARK-3431
>>
>> Where do we report/track issues with JIRA itself being down?
>>
>> Nick
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 02:01:30 2014
Return-Path: <dev-return-10736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA6F2109B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 02:01:30 +0000 (UTC)
Received: (qmail 8484 invoked by uid 500); 11 Dec 2014 02:01:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8409 invoked by uid 500); 11 Dec 2014 02:01:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8393 invoked by uid 99); 11 Dec 2014 02:01:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:01:28 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:01:03 +0000
Received: by mail-lb0-f177.google.com with SMTP id b6so3271225lbj.22
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 18:01:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rDsaMgOJX54/34KDe4RzcjFwPbw4nfZBgGxu4v1bYd0=;
        b=SdhFWwrBWxo7kJmEBUCR3loHRRb6BkqajpP39GBp1CCO1LbxArVIaV43JJn1PRL6NC
         hJhVuPTobqGze4mkCyg9u+poH/x/mMvkwAr5DxDYAsbk2O0LpAToF2nQ6svzpPJdwjcy
         yZ2iyvniu8TQgT4+ojXZOVvgd/V+Znb75BWxnWooSGw/7QbVt5UfF7aPzE40rDxOHxje
         QxGXPwQCqHKzwDUWmX+UkseoIrRgigLOUHp3dI5trV6O7SvsDSLU7QMjxDYLS6ps03y/
         GhwDlG4YKfybvCskAY84lP1sm/Vq/1gx2h8ARRI/AbrbEn9LVH9KSqwDp1dS5jvxg1Ga
         P2Ug==
MIME-Version: 1.0
X-Received: by 10.153.6.33 with SMTP id cr1mr7111436lad.63.1418263262233; Wed,
 10 Dec 2014 18:01:02 -0800 (PST)
Received: by 10.25.212.144 with HTTP; Wed, 10 Dec 2014 18:01:02 -0800 (PST)
In-Reply-To: <CAHuE29b7NBSkr1PKBjXGRgddfKCLmQAzX3TP5qf+90jib6ezEw@mail.gmail.com>
References: <CA+B-+fyHmpvyByMHV9=BWFxGeLfgLEprOmX3AKKVQeTM5L74LA@mail.gmail.com>
	<CAHuE29b7NBSkr1PKBjXGRgddfKCLmQAzX3TP5qf+90jib6ezEw@mail.gmail.com>
Date: Wed, 10 Dec 2014 18:01:02 -0800
Message-ID: <CA+B-+fyKmuu0H1yOrsANj9VYtMEC7_99Z7E8KWCVSb=myaHVng@mail.gmail.com>
Subject: Re: Row Similarity
From: Debasish Das <debasish.das83@gmail.com>
To: Reza Zadeh <reza@databricks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113498e2f18aeb0509e725f9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113498e2f18aeb0509e725f9
Content-Type: text/plain; charset=UTF-8

I added code to compute topK products for each user and topK user for each
product in SPARK-3066..

That is different than row similarity calculation as we need both user and
product factors to calculate the topK recommendations..

For (1) and (2) we are trying to answer similarUsers to given a user and
similarProducts to a given product....

similarProducts to a given product is straightforward to compute through
columnSimilarities/dimsum when products are skinny...

similarUser to a given user will need a map-reduce implementation of row
similarity since the matrix is tall...

I don't see a JIRA for that yet...Are there any good reference for map
reduce implementation of row similarity ?

On Wed, Dec 10, 2014 at 2:30 PM, Reza Zadeh <reza@databricks.com> wrote:

> It's not so cheap to compute row similarities when there are many rows, as
> it amounts to computing the outer product of a matrix A (i.e. computing
> AA^T, which is expensive).
>
> There is a JIRA to track handling (1) and (2) more efficiently than
> computing all pairs: https://issues.apache.org/jira/browse/SPARK-3066
>
>
>
> On Wed, Dec 10, 2014 at 2:44 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Hi,
>>
>> It seems there are multiple places where we would like to compute row
>> similarity (accurate or approximate similarities)
>>
>> Basically through RowMatrix columnSimilarities we can compute column
>> similarities of a tall skinny matrix
>>
>> Similarly we should have an API in RowMatrix called rowSimilarities where
>> we can compute similar rows in a map-reduce fashion. It will be useful for
>> following use-cases:
>>
>> 1. Generate topK users for each user from matrix factorization model
>> 2. Generate topK products for each product from matrix factorization model
>> 3. Generate kernel matrix for use in spectral clustering
>> 4. Generate kernel matrix for use in kernel regression/classification
>>
>> I am not sure if there are already good implementation for map-reduce row
>> similarity that we can use (ideas like fastfood and kitchen sink felt more
>> like for classification use-case but for recommendation also user
>> similarities show up which is unsupervised)...
>>
>> Is there a JIRA tracking it ? If not I can open one and we can discuss
>> further on it.
>>
>> Thanks.
>> Deb
>>
>
>

--001a113498e2f18aeb0509e725f9--

From dev-return-10737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 02:18:51 2014
Return-Path: <dev-return-10737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2C0E10A5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 02:18:51 +0000 (UTC)
Received: (qmail 42497 invoked by uid 500); 11 Dec 2014 02:18:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42436 invoked by uid 500); 11 Dec 2014 02:18:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42424 invoked by uid 99); 11 Dec 2014 02:18:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:18:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:18:45 +0000
Received: by mail-ie0-f181.google.com with SMTP id tp5so3948624ieb.40
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 18:15:48 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=6ONYmFmHvF2PzUU64Be3lucdVIfKoLF/4cKS7qDf30E=;
        b=QhvSfdsEsRD2n1tCV75mtHSUAC6qxm/HO5bqJ4fxMTzr1B7aNIxqUy+Mga3btoSyxy
         PybqvtIZ79y46iTEUcqbfy8ay8lEAy71hD+e33379zo7AHSelAlEKIeOCPWRNbMZLr63
         77jHJosRzt/fLC0CatTEfiimtdjUD/CVyFocyLQ8wEdMTft454la9WiNtQaoTDbMc36g
         cK0AhoOvuypGOkWfRRAD6BPTkBSgEXtsk00mmwEacKFwa8whYmGURlTkAk8p27uRDqSS
         2slrdk2/VoCe/ld+BzZCHCu3nw5zJY/bhUUkeVAuMUY5VOplnKoBKr8JZmj5PBD6Eysc
         VkmA==
X-Gm-Message-State: ALoCoQlQg6ce6UE2NKJAhaFXhKiiVkAe3NwOmH/yWSAK6V9rAHN5B+PxXHvEqZg82KGLoX/M4kfZ
MIME-Version: 1.0
X-Received: by 10.50.114.233 with SMTP id jj9mr29470736igb.18.1418264148566;
 Wed, 10 Dec 2014 18:15:48 -0800 (PST)
Received: by 10.107.37.131 with HTTP; Wed, 10 Dec 2014 18:15:48 -0800 (PST)
In-Reply-To: <CA+B-+fyKmuu0H1yOrsANj9VYtMEC7_99Z7E8KWCVSb=myaHVng@mail.gmail.com>
References: <CA+B-+fyHmpvyByMHV9=BWFxGeLfgLEprOmX3AKKVQeTM5L74LA@mail.gmail.com>
	<CAHuE29b7NBSkr1PKBjXGRgddfKCLmQAzX3TP5qf+90jib6ezEw@mail.gmail.com>
	<CA+B-+fyKmuu0H1yOrsANj9VYtMEC7_99Z7E8KWCVSb=myaHVng@mail.gmail.com>
Date: Wed, 10 Dec 2014 21:15:48 -0500
Message-ID: <CAHuE29YNoebs7t9=kV07GaNgum=jXY2K_BUP83EXM_1zcbBPmQ@mail.gmail.com>
Subject: Re: Row Similarity
From: Reza Zadeh <reza@databricks.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011833eac69e440509e75a0d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011833eac69e440509e75a0d
Content-Type: text/plain; charset=UTF-8

Here we go: https://issues.apache.org/jira/browse/SPARK-4823

On Wed, Dec 10, 2014 at 9:01 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> I added code to compute topK products for each user and topK user for each
> product in SPARK-3066..
>
> That is different than row similarity calculation as we need both user and
> product factors to calculate the topK recommendations..
>
> For (1) and (2) we are trying to answer similarUsers to given a user and
> similarProducts to a given product....
>
> similarProducts to a given product is straightforward to compute through
> columnSimilarities/dimsum when products are skinny...
>
> similarUser to a given user will need a map-reduce implementation of row
> similarity since the matrix is tall...
>
> I don't see a JIRA for that yet...Are there any good reference for map
> reduce implementation of row similarity ?
>
> On Wed, Dec 10, 2014 at 2:30 PM, Reza Zadeh <reza@databricks.com> wrote:
>
>> It's not so cheap to compute row similarities when there are many rows,
>> as it amounts to computing the outer product of a matrix A (i.e. computing
>> AA^T, which is expensive).
>>
>> There is a JIRA to track handling (1) and (2) more efficiently than
>> computing all pairs: https://issues.apache.org/jira/browse/SPARK-3066
>>
>>
>>
>> On Wed, Dec 10, 2014 at 2:44 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> It seems there are multiple places where we would like to compute row
>>> similarity (accurate or approximate similarities)
>>>
>>> Basically through RowMatrix columnSimilarities we can compute column
>>> similarities of a tall skinny matrix
>>>
>>> Similarly we should have an API in RowMatrix called rowSimilarities where
>>> we can compute similar rows in a map-reduce fashion. It will be useful
>>> for
>>> following use-cases:
>>>
>>> 1. Generate topK users for each user from matrix factorization model
>>> 2. Generate topK products for each product from matrix factorization
>>> model
>>> 3. Generate kernel matrix for use in spectral clustering
>>> 4. Generate kernel matrix for use in kernel regression/classification
>>>
>>> I am not sure if there are already good implementation for map-reduce row
>>> similarity that we can use (ideas like fastfood and kitchen sink felt
>>> more
>>> like for classification use-case but for recommendation also user
>>> similarities show up which is unsupervised)...
>>>
>>> Is there a JIRA tracking it ? If not I can open one and we can discuss
>>> further on it.
>>>
>>> Thanks.
>>> Deb
>>>
>>
>>
>

--089e011833eac69e440509e75a0d--

From dev-return-10738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 02:19:51 2014
Return-Path: <dev-return-10738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 40B3E10A67
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 02:19:51 +0000 (UTC)
Received: (qmail 46413 invoked by uid 500); 11 Dec 2014 02:19:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46334 invoked by uid 500); 11 Dec 2014 02:19:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46314 invoked by uid 99); 11 Dec 2014 02:19:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:19:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:19:44 +0000
Received: by mail-oi0-f47.google.com with SMTP id v63so3075194oia.34
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 18:19:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=WcdpvkypMBSkXHmpoj9p2Asz/Fz65FitjLQzAHxcMnY=;
        b=wmmLEmppD/55pzMjuRsvsZTUnoriGGNITEgnLYgaQ2tED32Pn0wQTYJY7uz427d2Vr
         WtrH0Y7pu0aFlHxAhrz3fjfZAhvm6ueS4qziBW+4sc+XXr17l3uKjrO2LuX6/iV3cX3v
         9VGf3nEd5sYtrluUAKVU3wd56PmnTZz4HKhbDExPra7lCXZ1K5We6LGtwfEUV4/70sFC
         oLLrevxf1laU2VFzfWdeH+in3SDusn+qELZMlH5+qG8LBBGNCOx1TOk9gKMhedrGpb37
         JULnhJlEoDXoljy3Ya2LpQd6IDdVKteUn4A7oA7k1dX9RBuMZpc1Yd5AqaGy6SFENvmG
         R0/A==
X-Received: by 10.202.2.79 with SMTP id 76mr4468270oic.106.1418264363500; Wed,
 10 Dec 2014 18:19:23 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.71.67 with HTTP; Wed, 10 Dec 2014 18:19:03 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Wed, 10 Dec 2014 18:19:03 -0800
Message-ID: <CAJc_syLHn8y11bRDX5H-6DvpQCjpZWKgsWGe8hJGu+shTs=vNw@mail.gmail.com>
Subject: SparkSQL not honoring schema
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1137bac8958c4b0509e76766
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137bac8958c4b0509e76766
Content-Type: text/plain; charset=UTF-8

Hello,

I defined a SchemaRDD by applying a hand-crafted StructType to an RDD. Some
of the Rows in the RDD are malformed--that is, they do not conform to the
schema defined by the StructType. When running a select statement on this
SchemaRDD I would expect SparkSQL to either reject the malformed rows or
fail. Instead, it returns whatever data it finds, even if malformed. Is
this the desired behavior? Is there no method in SparkSQL to check for
validity with respect to the schema?

Thanks.

Alex

--001a1137bac8958c4b0509e76766--

From dev-return-10739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 02:30:40 2014
Return-Path: <dev-return-10739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F90410ADC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 02:30:40 +0000 (UTC)
Received: (qmail 69844 invoked by uid 500); 11 Dec 2014 02:30:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69758 invoked by uid 500); 11 Dec 2014 02:30:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69747 invoked by uid 99); 11 Dec 2014 02:30:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:30:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:30:11 +0000
Received: by mail-la0-f48.google.com with SMTP id gf13so3429183lab.7
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 18:28:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Zpxc4IE44loz15yi+VzwP77prvVZ/slp5Z2Bw+BQU0w=;
        b=PbOWoB9uis2LQurhQEram9+vySlaWHPC1CSQG40VEbK1glzhRSTUlF4eRyTuqQIVuB
         1CrvN3zLsbAxfIWmSrt9SrzvGNDwxzZEJfxUagAbJmESB07til+zFXx3wd6KG+V4B3Zd
         sl2as9ASLjxxr05v3fUQM/UWicG3tILqcuc1xwwKr5A6GFHTLWIB01LtZYTmwGHkYsvV
         shaCPGX5oKs49JKRPc/98rcucFZvYwYrErqMiyfhXc56uE9ua0Dz1LPM/pP7z9rjIlT/
         TA77qFe4fAUWxnxkF79qQqchZtlSgi1ogO3rEIRDsVxa7GKkVBoWJw3OuHdo7HZLiujP
         az8g==
X-Gm-Message-State: ALoCoQldMhV2Wmd7UmWAaVGO835lH+uurinrGpgfzhg5z/Ihc3lBWN1HqnnIYbOJqk9pHOIFulIn
X-Received: by 10.112.128.197 with SMTP id nq5mr7282392lbb.0.1418264899841;
 Wed, 10 Dec 2014 18:28:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Wed, 10 Dec 2014 18:27:58 -0800 (PST)
In-Reply-To: <CAJc_syLHn8y11bRDX5H-6DvpQCjpZWKgsWGe8hJGu+shTs=vNw@mail.gmail.com>
References: <CAJc_syLHn8y11bRDX5H-6DvpQCjpZWKgsWGe8hJGu+shTs=vNw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 10 Dec 2014 18:27:58 -0800
Message-ID: <CAAswR-6w6ZaHehtXYAuBqLcGH=ZLa0K9vMb8SKR=mOpbX0cekA@mail.gmail.com>
Subject: Re: SparkSQL not honoring schema
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a85a68d83060509e787bf
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a85a68d83060509e787bf
Content-Type: text/plain; charset=UTF-8

As the scala doc for applySchema says, "It is important to make sure that
the structure of every [[Row]] of the provided RDD matches the provided
schema. Otherwise, there will be runtime exceptions."  We don't check as
doing runtime reflection on all of the data would be very expensive.  You
will only get errors if you try to manipulate the data, but otherwise it
will pass it though.

I have written some debugging code (developer API, not guaranteed to be
stable) though that you can use.

import org.apache.spark.sql.execution.debug._
schemaRDD.typeCheck()

On Wed, Dec 10, 2014 at 6:19 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Hello,
>
> I defined a SchemaRDD by applying a hand-crafted StructType to an RDD. Some
> of the Rows in the RDD are malformed--that is, they do not conform to the
> schema defined by the StructType. When running a select statement on this
> SchemaRDD I would expect SparkSQL to either reject the malformed rows or
> fail. Instead, it returns whatever data it finds, even if malformed. Is
> this the desired behavior? Is there no method in SparkSQL to check for
> validity with respect to the schema?
>
> Thanks.
>
> Alex
>

--047d7b3a85a68d83060509e787bf--

From dev-return-10740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 02:45:56 2014
Return-Path: <dev-return-10740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C703E10B33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 02:45:56 +0000 (UTC)
Received: (qmail 84479 invoked by uid 500); 11 Dec 2014 02:45:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84402 invoked by uid 500); 11 Dec 2014 02:45:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84390 invoked by uid 99); 11 Dec 2014 02:45:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:45:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 02:45:28 +0000
Received: by mail-oi0-f46.google.com with SMTP id h136so3066264oig.33
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 18:45:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=pQ9broURZ3p2B1M0bntsZCxXoxNrGZW+tgeGbyix8YA=;
        b=BCLpQ8SPGpUs1i/NXtX0yfLaRompGU9frn/+YytuhbydtbHE0/8TBOXMwm9M4yLIrv
         d91rK7pHEPFrdjI/r2zOWBqDelAgnqxpDstMeN+/BQwMFQ7qk9A6wFYtBaCIu6VtbWRz
         ZruatTmS8rVWTz+Supn2Be+MRHe0f6RxcsyOn29AjuVswkN+BypO/kssLSfDq35I0Bld
         fOifvcsX4XCjbnnMllxan1UgJrZ/rI/WxBLY0eUdklcdec2pcWPvE7b2fKVELCkvQ3vX
         eDjBwZwEo1AT8oYIOVOmgD6878H3Buq88QZMCPwVveDQpDeoc7OQA/upTb7smWTPN8wr
         bmyw==
X-Received: by 10.202.72.85 with SMTP id v82mr4693450oia.14.1418265927715;
 Wed, 10 Dec 2014 18:45:27 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.71.67 with HTTP; Wed, 10 Dec 2014 18:45:07 -0800 (PST)
In-Reply-To: <CAAswR-6w6ZaHehtXYAuBqLcGH=ZLa0K9vMb8SKR=mOpbX0cekA@mail.gmail.com>
References: <CAJc_syLHn8y11bRDX5H-6DvpQCjpZWKgsWGe8hJGu+shTs=vNw@mail.gmail.com>
 <CAAswR-6w6ZaHehtXYAuBqLcGH=ZLa0K9vMb8SKR=mOpbX0cekA@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Wed, 10 Dec 2014 18:45:07 -0800
Message-ID: <CAJc_syJd4YOc6w_rxBABEnfNpEwQgk82M+RCSijhqMhseqPh=A@mail.gmail.com>
Subject: Re: SparkSQL not honoring schema
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113db21ed1b9980509e7c423
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113db21ed1b9980509e7c423
Content-Type: text/plain; charset=UTF-8

Hey Michael,

Thanks for the clarification. I was actually assuming the query would fail.
Ok, so this means I will have to do the validation in an RDD transformation
feeding into the SchemaRDD.

On Wed, Dec 10, 2014 at 6:27 PM, Michael Armbrust <michael@databricks.com>
wrote:

> As the scala doc for applySchema says, "It is important to make sure that
> the structure of every [[Row]] of the provided RDD matches the provided
> schema. Otherwise, there will be runtime exceptions."  We don't check as
> doing runtime reflection on all of the data would be very expensive.  You
> will only get errors if you try to manipulate the data, but otherwise it
> will pass it though.
>
> I have written some debugging code (developer API, not guaranteed to be
> stable) though that you can use.
>
> import org.apache.spark.sql.execution.debug._
> schemaRDD.typeCheck()
>
> On Wed, Dec 10, 2014 at 6:19 PM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Hello,
>>
>> I defined a SchemaRDD by applying a hand-crafted StructType to an RDD.
>> Some
>> of the Rows in the RDD are malformed--that is, they do not conform to the
>> schema defined by the StructType. When running a select statement on this
>> SchemaRDD I would expect SparkSQL to either reject the malformed rows or
>> fail. Instead, it returns whatever data it finds, even if malformed. Is
>> this the desired behavior? Is there no method in SparkSQL to check for
>> validity with respect to the schema?
>>
>> Thanks.
>>
>> Alex
>>
>
>

--001a113db21ed1b9980509e7c423--

From dev-return-10741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 03:37:09 2014
Return-Path: <dev-return-10741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCC3010D09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 03:37:09 +0000 (UTC)
Received: (qmail 57786 invoked by uid 500); 11 Dec 2014 03:37:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57701 invoked by uid 500); 11 Dec 2014 03:37:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57690 invoked by uid 99); 11 Dec 2014 03:37:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 03:37:07 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS,TVD_FW_GRAPHIC_NAME_MID
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.142 as permitted sender)
Received: from [202.81.31.142] (HELO e23smtp09.au.ibm.com) (202.81.31.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 03:36:40 +0000
Received: from /spool/local
	by e23smtp09.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Thu, 11 Dec 2014 13:35:07 +1000
Received: from d23dlp03.au.ibm.com (202.81.31.214)
	by e23smtp09.au.ibm.com (202.81.31.206) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Thu, 11 Dec 2014 13:35:04 +1000
Received: from d23relay06.au.ibm.com (d23relay06.au.ibm.com [9.185.63.219])
	by d23dlp03.au.ibm.com (Postfix) with ESMTP id 42E1E3578057
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 14:35:04 +1100 (EST)
Received: from d23av03.au.ibm.com (d23av03.au.ibm.com [9.190.234.97])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBB3YuA520512922
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 14:35:04 +1100
Received: from d23av03.au.ibm.com (localhost [127.0.0.1])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBB3YWJa001851
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 14:34:32 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBB3YUGv000984
	for <dev@spark.apache.org>; Thu, 11 Dec 2014 14:34:31 +1100
In-Reply-To: <CACBYxKKYSqsWUvkvPspMdK5KumU+mDCzyMijxOf5VsoftXz7xQ@mail.gmail.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>	<CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
	<OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com> <CACBYxKKYSqsWUvkvPspMdK5KumU+mDCzyMijxOf5VsoftXz7xQ@mail.gmail.com>
Subject: Re: HA support for Spark
X-KeepSent: 1B7F9E07:326E1FF3-48257DAB:00137AD8;
 type=4; name=$KeepSent
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
        Reynold Xin <rxin@databricks.com>
X-Mailer: Lotus Notes Release 8.5.3FP3 Septem 15, 2011
Message-ID: <OF1B7F9E07.326E1FF3-ON48257DAB.00137AD8-48257DAB.00139872@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Thu, 11 Dec 2014 11:34:06 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/11/2014 11:34:31
MIME-Version: 1.0
Content-type: multipart/related; 
	Boundary="0__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48"
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121103-0033-0000-0000-000000B7703D
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48
Content-type: multipart/alternative; 
	Boundary="1__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48"

--1__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48
Content-type: text/plain; charset=US-ASCII
Content-transfer-encoding: quoted-printable


Right, perhaps also need preserve some DAG information? I am wondering =
if
there is any work around this.



                                                                       =
    
             Sandy Ryza                                                =
    
             <sandy.ryza@cloud                                         =
    
             era.com>                                                  =
 To 
                                       Jun Feng Liu/China/IBM@IBMCN,   =
    
             2014-12-11 01:34                                          =
 cc 
                                       Reynold Xin <rxin@databricks.com=
>,  
                                       "dev@spark.apache.org"          =
    
                                       <dev@spark.apache.org>          =
    
                                                                   Subj=
ect 
                                       Re: HA support for Spark        =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    




I think that if we were able to maintain the full set of created RDDs a=
s
well as some scheduler and block manager state, it would be enough for =
most
apps to recover.

On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrot=
e:

> Well, it should not be mission impossible thinking there are so many =
HA
> solution existing today. I would interest to know if there is any
specific
> difficult.
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone:
*86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
>
> 2014/12/10 16:30
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> "dev@spark.apache.org" <dev@spark.apache.org>
> Subject
> Re: HA support for Spark
>
>
>
>
> This would be plausible for specific purposes such as Spark streaming=
 or
> Spark SQL, but I don't think it is doable for general Spark driver si=
nce
it
> is just a normal JVM process with arbitrary program state.
>
> On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com>
wrote:
>
> > Do we have any high availability support in Spark driver level? For=

> > example, if we want spark drive can move to another node continue
> execution
> > when failure happen. I can see the RDD checkpoint can help to
> serialization
> > the status of RDD. I can image to load the check point from another=

node
> > when error happen, but seems like will lost track all tasks status =
or
> even
> > executor information that maintain in spark context. I am not sure =
if
> there
> > is any existing stuff I can leverage to do that. thanks for any
suggests
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  [image: 2D barcode - encoded with contact information] *Phone:
> *86-10-82452683
> >
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
>
>
=

--1__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48
Content-type: text/html; charset=US-ASCII
Content-Disposition: inline
Content-transfer-encoding: quoted-printable

<html><body>
<p><font size=3D"2" face=3D"sans-serif">Right, perhaps also need preser=
ve some DAG information? I am wondering if there is any work around thi=
s.</font><br>
<br>
<br>
<img width=3D"16" height=3D"16" src=3D"cid:1__=3DC7BBF738DF80FC488f9e8a=
93df938@cn.ibm.com" border=3D"0" alt=3D"Inactive hide details for Sandy=
 Ryza ---2014-12-11 01:36:35---Sandy Ryza &lt;sandy.ryza@cloudera.com&g=
t;"><font size=3D"2" color=3D"#424282" face=3D"sans-serif">Sandy Ryza -=
--2014-12-11 01:36:35---Sandy Ryza &lt;sandy.ryza@cloudera.com&gt;</fon=
t><br>
<br>

<table width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0">=

<tr valign=3D"top"><td style=3D"background-image:url(cid:2__=3DC7BBF738=
DF80FC488f9e8a93df938@cn.ibm.com); background-repeat: no-repeat; " widt=
h=3D"40%">
<ul style=3D"padding-left: 72pt"><font size=3D"1" face=3D"sans-serif"><=
b>Sandy Ryza &lt;sandy.ryza@cloudera.com&gt;</b></font><font size=3D"1"=
 face=3D"sans-serif">&nbsp;</font>
<p><font size=3D"1" face=3D"sans-serif">2014-12-11 01:34</font></ul>
</td><td width=3D"60%">
<table width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0">=

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">To</font></di=
v></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""><br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Ju=
n Feng Liu/China/IBM@IBMCN, </font></ul>
</td></tr>

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">cc</font></di=
v></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""><br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Re=
ynold Xin &lt;rxin@databricks.com&gt;, &quot;dev@spark.apache.org&quot;=
 &lt;dev@spark.apache.org&gt;</font></ul>
</td></tr>

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">Subject</font=
></div></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid=
:3__=3DC7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D"">=
<br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Re=
: HA support for Spark</font></ul>
</td></tr>
</table>

<table border=3D"0" cellspacing=3D"0" cellpadding=3D"0">
<tr valign=3D"top"><td width=3D"58"><img width=3D"1" height=3D"1" src=3D=
"cid:3__=3DC7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""></td><td width=3D"336"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF738DF80FC488f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""></td></=
tr>
</table>
</td></tr>
</table>
<br>
<tt><font size=3D"2">I think that if we were able to maintain the full =
set of created RDDs as<br>
well as some scheduler and block manager state, it would be enough for =
most<br>
apps to recover.<br>
<br>
On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu &lt;liujunf@cn.ibm.com&gt=
; wrote:<br>
<br>
&gt; Well, it should not be mission impossible thinking there are so ma=
ny HA<br>
&gt; solution existing today. I would interest to know if there is any =
specific<br>
&gt; difficult.<br>
&gt;<br>
&gt; Best Regards<br>
&gt;<br>
&gt;<br>
&gt; *Jun Feng Liu*<br>
&gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt;<br>
&gt; &nbsp; ------------------------------<br>
&gt; &nbsp;[image: 2D barcode - encoded with contact information] *Phon=
e: *86-10-82452683<br>
&gt;<br>
&gt; * E-mail:* *liujunf@cn.ibm.com* &lt;liujunf@cn.ibm.com&gt;<br>
&gt; [image: IBM]<br>
&gt;<br>
&gt; BLD 28,ZGC Software Park<br>
&gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; China<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; &nbsp;*Reynold Xin &lt;rxin@databricks.com &lt;rxin@databricks.com=
&gt;&gt;*<br>
&gt;<br>
&gt; 2014/12/10 16:30<br>
&gt; &nbsp; To<br>
&gt; Jun Feng Liu/China/IBM@IBMCN,<br>
&gt; cc<br>
&gt; &quot;dev@spark.apache.org&quot; &lt;dev@spark.apache.org&gt;<br>
&gt; Subject<br>
&gt; Re: HA support for Spark<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; This would be plausible for specific purposes such as Spark stream=
ing or<br>
&gt; Spark SQL, but I don't think it is doable for general Spark driver=
 since it<br>
&gt; is just a normal JVM process with arbitrary program state.<br>
&gt;<br>
&gt; On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu &lt;liujunf@cn.ibm.=
com&gt; wrote:<br>
&gt;<br>
&gt; &gt; Do we have any high availability support in Spark driver leve=
l? For<br>
&gt; &gt; example, if we want spark drive can move to another node cont=
inue<br>
&gt; execution<br>
&gt; &gt; when failure happen. I can see the RDD checkpoint can help to=
<br>
&gt; serialization<br>
&gt; &gt; the status of RDD. I can image to load the check point from a=
nother node<br>
&gt; &gt; when error happen, but seems like will lost track all tasks s=
tatus or<br>
&gt; even<br>
&gt; &gt; executor information that maintain in spark context. I am not=
 sure if<br>
&gt; there<br>
&gt; &gt; is any existing stuff I can leverage to do that. thanks for a=
ny suggests<br>
&gt; &gt;<br>
&gt; &gt; Best Regards<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; *Jun Feng Liu*<br>
&gt; &gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; ------------------------------<br>
&gt; &gt; &nbsp;[image: 2D barcode - encoded with contact information] =
*Phone:<br>
&gt; *86-10-82452683<br>
&gt; &gt;<br>
&gt; &gt; * E-mail:* *liujunf@cn.ibm.com* &lt;liujunf@cn.ibm.com&gt;<br=
>
&gt; &gt; [image: IBM]<br>
&gt; &gt;<br>
&gt; &gt; BLD 28,ZGC Software Park<br>
&gt; &gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; &gt; China<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt;<br>
&gt;<br>
</font></tt><br>
</body></html>=


--1__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48--


--0__=C7BBF738DF80FC488f9e8a93df938690918cC7BBF738DF80FC48--


From dev-return-10742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 05:55:46 2014
Return-Path: <dev-return-10742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C4619166
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 05:55:46 +0000 (UTC)
Received: (qmail 36164 invoked by uid 500); 11 Dec 2014 05:55:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36094 invoked by uid 500); 11 Dec 2014 05:55:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36069 invoked by uid 99); 11 Dec 2014 05:55:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 05:55:43 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 05:55:35 +0000
Received: by mail-pa0-f47.google.com with SMTP id kq14so4352059pab.6
        for <dev@spark.apache.org>; Wed, 10 Dec 2014 21:55:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=kwpssyxnYJ/3P3runsXPfS0lgHQC9VmReUf5vAhPAAY=;
        b=Nzi9ZX2b4ye+55RYsMiIQx5sLsk0I1jvjuPcYkNVuEtPAplgqpEi3vKxEhUbNILb4k
         kk7D2h7OgDg6AlX94b2JHhQWueXaw8KNsWfUaODGsry+LLh4w4DOodUp250xpjmL8xSK
         Hxsqy7PZ0opLC4NI9ecNcyry3qZBdD368bGctspsfCXHi3wFvL6QvBimE8njCYeqb955
         iaGKaQt8h2JzQNaoKO5jpOCUHP0Y2n46HXT2D1YvtHEgc3gBuQ5/cMDWHUXS3f7oZ/I9
         nuMH8EAiV/5iPeQR9ldL7NhX6t86qDpde8Qr+ndIm6p+FBmwuXIiDy2kIPA3ukF4NLOm
         9aDg==
MIME-Version: 1.0
X-Received: by 10.70.2.65 with SMTP id 1mr13895623pds.13.1418277315048; Wed,
 10 Dec 2014 21:55:15 -0800 (PST)
Received: by 10.70.37.230 with HTTP; Wed, 10 Dec 2014 21:55:14 -0800 (PST)
In-Reply-To: <CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<CAOTBr2=5S=cxYfi7OE4g_8nyOWsdFabaSCsJ2fkWsKHY9Nn=6Q@mail.gmail.com>
	<CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
Date: Wed, 10 Dec 2014 21:55:14 -0800
Message-ID: <CAOTBr2=0AdSTkMLSyjnD4Wi5qwHRvngHyP2Ec6+kHgFCj=wJGQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Krishna Sankar <ksankar42@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/mixed; boundary=089e01294e548eab700509ea6b99
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01294e548eab700509ea6b99
Content-Type: multipart/alternative; boundary=089e01294e548eab660509ea6b97

--089e01294e548eab660509ea6b97
Content-Type: text/plain; charset=UTF-8

   - K-Means iPython notebook & data attached.
   - It is the zip that gives the error ; while one of the RDDs is from the
   prediction, most probably there is no problem with the K-Means.
   - Lines 34,35 & 36 essentially are the same. But only 36 works with
   1.2.0.
   - Interestingly, lines 34,35 & 36 work with 1.1.1 (Checked just now)
      - The plot thickens!
      - In 1.1.1, freq_cluster_map.take(5) prints normally for 34 & 35, but
      in exponential form for 36. So there is some difference even in 1.1.1.
      - #34,#35 [(array([28143, 0, 174, 1, 0, 0, 7000]), 1),

       (array([19244,     0,   215,     2,     0,     0,  6968]), 1),
       (array([41354,     0,  4123,     4,     0,     0,  7034]), 1),
       (array([14776,     0,   500,     1,     0,     0,  6952]), 1),
       (array([97752,     0, 43300,    26,  2077,     4,  6935]), 0)]

      -

      #36 [(array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,

                 1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 7.00000000e+03]), 1),
       (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,
                 2.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 6.96800000e+03]), 1),
       (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,
                 4.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 7.03400000e+03]), 1),
       (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,
                 1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 6.95200000e+03]), 1),
       (array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,
                 2.60000000e+01,   2.07700000e+03,   4.00000000e+00,
                 6.93500000e+03]), 0)]

      - I had overwritten the naive bayes example. Will chase the older
   versions down

Cheers
<k/>

On Wed, Dec 3, 2014 at 4:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Krishna, could you send me some code snippets for the issues you saw
> in naive Bayes and k-means? -Xiangrui
>
> On Sun, Nov 30, 2014 at 6:49 AM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
> > +1
> > 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
> > -Dhadoop.version=2.4.0 -DskipTests clean package 16:46 min (slightly
> slower
> > connection)
> > 2. Tested pyspark, mlib - running as well as compare esults with 1.1.x
> > 2.1. statistics OK
> > 2.2. Linear/Ridge/Laso Regression OK
> >        Slight difference in the print method (vs. 1.1.x) of the model
> > object - with a label & more details. This is good.
> > 2.3. Decision Tree, Naive Bayes OK
> >        Changes in print(model) - now print (model.ToDebugString()) - OK
> >        Some changes in NaiveBayes. Different from my 1.1.x code - had to
> > flatten list structures, zip required same number in partitions
> >        After code changes ran fine.
> > 2.4. KMeans OK
> >        zip occasionally fails with error "localhost):
> > org.apache.spark.SparkException: Can only zip RDDs with same number of
> > elements in each partition"
> > Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
> > Made it work by doing a different transformation ie reusing an original
> > rdd.
> > 2.5. rdd operations OK
> >        State of the Union Texts - MapReduce, Filter,sortByKey (word
> count)
> > 2.6. recommendation OK
> > 2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
> > dataset which never worked. Works fine in 1.2.0 !
> > 3. Scala Mlib - subset of examples as in #2 above, with Scala
> > 3.1. statistics OK
> > 3.2. Linear Regression OK
> > 3.3. Decision Tree OK
> > 3.4. KMeans OK
> > Cheers
> > <k/>
> > P.S: Plan to add RF and .ml mechanics to this bank
> >
> > On Fri, Nov 28, 2014 at 9:16 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark version
> >> 1.2.0!
> >>
> >> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1048/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.2.0!
> >>
> >> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> >> if a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.1.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening very late into the QA period compared with
> >> previous votes, so -1 votes should only occur for significant
> >> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> >> regressions, or bugs related to new features will not block this
> >> release.
> >>
> >> == What default changes should I be aware of? ==
> >> 1. The default value of "spark.shuffle.blockTransferService" has been
> >> changed to "netty"
> >> --> Old behavior can be restored by switching to "nio"
> >>
> >> 2. The default value of "spark.shuffle.manager" has been changed to
> "sort".
> >> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> >> "hash".
> >>
> >> == Other notes ==
> >> Because this vote is occurring over a weekend, I will likely extend
> >> the vote if this RC survives until the end of the vote period.
> >>
> >> - Patrick
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>

--089e01294e548eab660509ea6b97
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><ul><li>K-Means iPython notebook &amp; data attached.</li>=
<li>It is the zip that gives the error ; while one of the RDDs is from the =
prediction, most probably there is no problem with the K-Means.<br></li><li=
>Lines 34,35 &amp; 36 essentially are the same. But only 36 works with 1.2.=
0.=C2=A0</li><li>Interestingly, lines 34,35 &amp; 36 work with 1.1.1 (Check=
ed just now)</li><ul><li>The plot thickens!=C2=A0</li><li>In 1.1.1, freq_cl=
uster_map.take(5) prints normally for 34 &amp; 35, but in exponential form =
for 36. So there is some difference even in 1.1.1.</li><li>#34,#35=C2=A0<sp=
an style=3D"color:rgb(0,0,0);font-size:13.63636302948px;line-height:17.0000=
610351563px;white-space:pre-wrap">[(array([28143,     0,   174,     1,     =
0,     0,  7000]), 1),</span><pre style=3D"padding:0px;font-size:13.6363630=
2948px;color:rgb(0,0,0);border-radius:0px;margin-top:0px;margin-bottom:0px;=
line-height:17.0000610351563px;word-break:break-all;word-wrap:break-word;wh=
ite-space:pre-wrap;border:0px;vertical-align:baseline"> (array([19244,     =
0,   215,     2,     0,     0,  6968]), 1),
 (array([41354,     0,  4123,     4,     0,     0,  7034]), 1),
 (array([14776,     0,   500,     1,     0,     0,  6952]), 1),
 (array([97752,     0, 43300,    26,  2077,     4,  6935]), 0)]</pre></li><=
li><pre style=3D"padding:0px;font-size:13.63636302948px;color:rgb(0,0,0);bo=
rder-radius:0px;margin-top:0px;margin-bottom:0px;line-height:17.00006103515=
63px;word-break:break-all;word-wrap:break-word;white-space:pre-wrap;border:=
0px;vertical-align:baseline">#36 <span style=3D"font-size:13.63636302948px;=
line-height:17.0000610351563px;font-family:arial,sans-serif">[(array([  2.8=
1430000e+04,   0.00000000e+00,   1.74000000e+02,</span></pre><pre style=3D"=
padding:0px;font-size:13.63636302948px;color:rgb(0,0,0);border-radius:0px;m=
argin-top:0px;margin-bottom:0px;line-height:17.0000610351563px;word-break:b=
reak-all;word-wrap:break-word;white-space:pre-wrap;border:0px;vertical-alig=
n:baseline">           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
           7.00000000e+03]), 1),
 (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,
           2.00000000e+00,   0.00000000e+00,   0.00000000e+00,
           6.96800000e+03]), 1),
 (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,
           4.00000000e+00,   0.00000000e+00,   0.00000000e+00,
           7.03400000e+03]), 1),
 (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,
           1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
           6.95200000e+03]), 1),
 (array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,
           2.60000000e+01,   2.07700000e+03,   4.00000000e+00,
           6.93500000e+03]), 0)]</pre></li></ul><li><font color=3D"#000000"=
 face=3D"monospace"><span style=3D"font-size:14px;line-height:17.0000610351=
563px;white-space:pre-wrap">I had overwritten the naive bayes example. Will=
 chase the older versions down</span></font></li></ul><font color=3D"#00000=
0" face=3D"monospace"><span style=3D"font-size:14px;line-height:17.00006103=
51563px;white-space:pre-wrap">Cheers</span></font><div><font color=3D"#0000=
00" face=3D"monospace"><span style=3D"font-size:14px;line-height:17.0000610=
351563px;white-space:pre-wrap">&lt;k/&gt;</span></font></div></div><div cla=
ss=3D"gmail_extra"><br><div class=3D"gmail_quote">On Wed, Dec 3, 2014 at 4:=
19 PM, Xiangrui Meng <span dir=3D"ltr">&lt;<a href=3D"mailto:mengxr@gmail.c=
om" target=3D"_blank">mengxr@gmail.com</a>&gt;</span> wrote:<br><blockquote=
 class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc soli=
d;padding-left:1ex">Krishna, could you send me some code snippets for the i=
ssues you saw<br>
in naive Bayes and k-means? -Xiangrui<br>
<div class=3D"HOEnZb"><div class=3D"h5"><br>
On Sun, Nov 30, 2014 at 6:49 AM, Krishna Sankar &lt;<a href=3D"mailto:ksank=
ar42@gmail.com">ksankar42@gmail.com</a>&gt; wrote:<br>
&gt; +1<br>
&gt; 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4<br>
&gt; -Dhadoop.version=3D2.4.0 -DskipTests clean package 16:46 min (slightly=
 slower<br>
&gt; connection)<br>
&gt; 2. Tested pyspark, mlib - running as well as compare esults with 1.1.x=
<br>
&gt; 2.1. statistics OK<br>
&gt; 2.2. Linear/Ridge/Laso Regression OK<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 Slight difference in the print method (vs. =
1.1.x) of the model<br>
&gt; object - with a label &amp; more details. This is good.<br>
&gt; 2.3. Decision Tree, Naive Bayes OK<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 Changes in print(model) - now print (model.=
ToDebugString()) - OK<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 Some changes in NaiveBayes. Different from =
my 1.1.x code - had to<br>
&gt; flatten list structures, zip required same number in partitions<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 After code changes ran fine.<br>
&gt; 2.4. KMeans OK<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 zip occasionally fails with error &quot;loc=
alhost):<br>
&gt; org.apache.spark.SparkException: Can only zip RDDs with same number of=
<br>
&gt; elements in each partition&quot;<br>
&gt; Has <a href=3D"https://issues.apache.org/jira/browse/SPARK-2251" targe=
t=3D"_blank">https://issues.apache.org/jira/browse/SPARK-2251</a> reappeare=
d ?<br>
&gt; Made it work by doing a different transformation ie reusing an origina=
l<br>
&gt; rdd.<br>
&gt; 2.5. rdd operations OK<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 State of the Union Texts - MapReduce, Filte=
r,sortByKey (word count)<br>
&gt; 2.6. recommendation OK<br>
&gt; 2.7. Good work ! In 1.x.x, had a map distinct over the movielens mediu=
m<br>
&gt; dataset which never worked. Works fine in 1.2.0 !<br>
&gt; 3. Scala Mlib - subset of examples as in #2 above, with Scala<br>
&gt; 3.1. statistics OK<br>
&gt; 3.2. Linear Regression OK<br>
&gt; 3.3. Decision Tree OK<br>
&gt; 3.4. KMeans OK<br>
&gt; Cheers<br>
&gt; &lt;k/&gt;<br>
&gt; P.S: Plan to add RF and .ml mechanics to this bank<br>
&gt;<br>
&gt; On Fri, Nov 28, 2014 at 9:16 PM, Patrick Wendell &lt;<a href=3D"mailto=
:pwendell@gmail.com">pwendell@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Please vote on releasing the following candidate as Apache Spark v=
ersion<br>
&gt;&gt; 1.2.0!<br>
&gt;&gt;<br>
&gt;&gt; The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):<br>
&gt;&gt;<br>
&gt;&gt; <a href=3D"https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=
=3Dcommit;h=3D1056e9ec13203d0c51564265e94d77a054498fdb" target=3D"_blank">h=
ttps://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D1056e9e=
c13203d0c51564265e94d77a054498fdb</a><br>
&gt;&gt;<br>
&gt;&gt; The release files, including signatures, digests, etc. can be foun=
d at:<br>
&gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.2.0-rc1/" ta=
rget=3D"_blank">http://people.apache.org/~pwendell/spark-1.2.0-rc1/</a><br>
&gt;&gt;<br>
&gt;&gt; Release artifacts are signed with the following key:<br>
&gt;&gt; <a href=3D"https://people.apache.org/keys/committer/pwendell.asc" =
target=3D"_blank">https://people.apache.org/keys/committer/pwendell.asc</a>=
<br>
&gt;&gt;<br>
&gt;&gt; The staging repository for this release can be found at:<br>
&gt;&gt; <a href=3D"https://repository.apache.org/content/repositories/orga=
pachespark-1048/" target=3D"_blank">https://repository.apache.org/content/r=
epositories/orgapachespark-1048/</a><br>
&gt;&gt;<br>
&gt;&gt; The documentation corresponding to this release can be found at:<b=
r>
&gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs=
/" target=3D"_blank">http://people.apache.org/~pwendell/spark-1.2.0-rc1-doc=
s/</a><br>
&gt;&gt;<br>
&gt;&gt; Please vote on releasing this package as Apache Spark 1.2.0!<br>
&gt;&gt;<br>
&gt;&gt; The vote is open until Tuesday, December 02, at 05:15 UTC and pass=
es<br>
&gt;&gt; if a majority of at least 3 +1 PMC votes are cast.<br>
&gt;&gt;<br>
&gt;&gt; [ ] +1 Release this package as Apache Spark 1.1.0<br>
&gt;&gt; [ ] -1 Do not release this package because ...<br>
&gt;&gt;<br>
&gt;&gt; To learn more about Apache Spark, please see<br>
&gt;&gt; <a href=3D"http://spark.apache.org/" target=3D"_blank">http://spar=
k.apache.org/</a><br>
&gt;&gt;<br>
&gt;&gt; =3D=3D What justifies a -1 vote for this release? =3D=3D<br>
&gt;&gt; This vote is happening very late into the QA period compared with<=
br>
&gt;&gt; previous votes, so -1 votes should only occur for significant<br>
&gt;&gt; regressions from 1.0.2. Bugs already present in 1.1.X, minor<br>
&gt;&gt; regressions, or bugs related to new features will not block this<b=
r>
&gt;&gt; release.<br>
&gt;&gt;<br>
&gt;&gt; =3D=3D What default changes should I be aware of? =3D=3D<br>
&gt;&gt; 1. The default value of &quot;spark.shuffle.blockTransferService&q=
uot; has been<br>
&gt;&gt; changed to &quot;netty&quot;<br>
&gt;&gt; --&gt; Old behavior can be restored by switching to &quot;nio&quot=
;<br>
&gt;&gt;<br>
&gt;&gt; 2. The default value of &quot;spark.shuffle.manager&quot; has been=
 changed to &quot;sort&quot;.<br>
&gt;&gt; --&gt; Old behavior can be restored by setting &quot;spark.shuffle=
.manager&quot; to<br>
&gt;&gt; &quot;hash&quot;.<br>
&gt;&gt;<br>
&gt;&gt; =3D=3D Other notes =3D=3D<br>
&gt;&gt; Because this vote is occurring over a weekend, I will likely exten=
d<br>
&gt;&gt; the vote if this RC survives until the end of the vote period.<br>
&gt;&gt;<br>
&gt;&gt; - Patrick<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------------------------------------------=
---<br>
&gt;&gt; To unsubscribe, e-mail: <a href=3D"mailto:dev-unsubscribe@spark.ap=
ache.org">dev-unsubscribe@spark.apache.org</a><br>
&gt;&gt; For additional commands, e-mail: <a href=3D"mailto:dev-help@spark.=
apache.org">dev-help@spark.apache.org</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
</div></div></blockquote></div><br></div>

--089e01294e548eab660509ea6b97--
--089e01294e548eab700509ea6b99
Content-Type: text/csv; charset=US-ASCII; name="AirlinesCluster-01.csv"
Content-Disposition: attachment; filename="AirlinesCluster-01.csv"
Content-Transfer-Encoding: base64
X-Attachment-Id: f_i3jp8wsg1
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--089e01294e548eab700509ea6b99
Content-Type: text/plain; charset=us-ascii


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
--089e01294e548eab700509ea6b99--

From dev-return-10743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 12:22:31 2014
Return-Path: <dev-return-10743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 07F209EAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 12:22:31 +0000 (UTC)
Received: (qmail 29443 invoked by uid 500); 11 Dec 2014 12:22:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29368 invoked by uid 500); 11 Dec 2014 12:22:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29353 invoked by uid 99); 11 Dec 2014 12:22:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 12:22:28 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 12:22:24 +0000
Received: by mail-ie0-f178.google.com with SMTP id tp5so4673486ieb.37
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 04:21:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=qEnvGb4verkmo5q+YLIns4k/Im4s2wwaEg4viW8AkxQ=;
        b=nGpTj3jC5iNWZjrV47zF/V4LK/zT9Ndfzs7za05CmI45lhFDOCWm8pbZTbsD+Ajrm9
         qvCRVRrPJ5xEfpnCATBJVFts5aBSpuh91lIKofayIPaeu3xLP9INKmWNQMBAAv8MHVgp
         SdJ5ls6Q/nK8MpNvjgOpeMORELeQ4ZVMrOnLwugTOgFgBoBIIao9te0J4D5IDibwX08G
         ikVa8qj6fSZwEdlJJ73aCCkSYXg9daaXfIdFlrft9Ncj03Z1NjDopomlyKqu+e/U48nn
         y+4WSK4D+oy+3bj6xfyVnfvy6pNu0zn+fnywudz6bkadjBxHtroAZzPnPC1cBkjm2Nlj
         O/hQ==
X-Received: by 10.50.79.232 with SMTP id m8mr31103597igx.11.1418300479163;
 Thu, 11 Dec 2014 04:21:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.31.75 with HTTP; Thu, 11 Dec 2014 04:20:49 -0800 (PST)
In-Reply-To: <OF1B7F9E07.326E1FF3-ON48257DAB.00137AD8-48257DAB.00139872@cn.ibm.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com>
 <CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com>
 <OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com>
 <CACBYxKKYSqsWUvkvPspMdK5KumU+mDCzyMijxOf5VsoftXz7xQ@mail.gmail.com> <OF1B7F9E07.326E1FF3-ON48257DAB.00137AD8-48257DAB.00139872@cn.ibm.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 11 Dec 2014 04:20:49 -0800
Message-ID: <CAMwrk0kN_3LUGVTfBf_Zh9rtMrzeoyoqNzFFiKJqn3tDbOvYKw@mail.gmail.com>
Subject: Re: HA support for Spark
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: Sandy Ryza <sandy.ryza@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Reynold Xin <rxin@databricks.com>
Content-Type: multipart/alternative; boundary=089e013a06063ed6000509efd069
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a06063ed6000509efd069
Content-Type: text/plain; charset=UTF-8

Spark Streaming essentially does this by saving the DAG of DStreams, which
can deterministically regenerate the DAG of RDDs upon recovery from
failure. Along with that the progress information (which batches have
finished, which batches are queued, etc.) is also saved, so that upon
recovery the system can restart from where it was before failure. This was
conceptually easy to do because the RDDs are very deterministically
generated in every batch. Extending this to a very general Spark program
with arbitrary RDD computations is definitely conceptually possible but not
that easy to do.

On Wed, Dec 10, 2014 at 7:34 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Right, perhaps also need preserve some DAG information? I am wondering if
> there is any work around this.
>
>
> [image: Inactive hide details for Sandy Ryza ---2014-12-11
> 01:36:35---Sandy Ryza <sandy.ryza@cloudera.com>]Sandy Ryza ---2014-12-11
> 01:36:35---Sandy Ryza <sandy.ryza@cloudera.com>
>
>
>    *Sandy Ryza <sandy.ryza@cloudera.com <sandy.ryza@cloudera.com>>*
>
>    2014-12-11 01:34
>
>
> To
>
>
>    Jun Feng Liu/China/IBM@IBMCN,
>
>
> cc
>
>
>    Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <
>    dev@spark.apache.org>
>
>
> Subject
>
>
>    Re: HA support for Spark
>
>
> I think that if we were able to maintain the full set of created RDDs as
> well as some scheduler and block manager state, it would be enough for most
> apps to recover.
>
> On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
> > Well, it should not be mission impossible thinking there are so many HA
> > solution existing today. I would interest to know if there is any
> specific
> > difficult.
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  [image: 2D barcode - encoded with contact information] *Phone:
> *86-10-82452683
> >
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
> >  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
> >
> > 2014/12/10 16:30
> >   To
> > Jun Feng Liu/China/IBM@IBMCN,
> > cc
> > "dev@spark.apache.org" <dev@spark.apache.org>
> > Subject
> > Re: HA support for Spark
> >
> >
> >
> >
> > This would be plausible for specific purposes such as Spark streaming or
> > Spark SQL, but I don't think it is doable for general Spark driver since
> it
> > is just a normal JVM process with arbitrary program state.
> >
> > On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com>
> wrote:
> >
> > > Do we have any high availability support in Spark driver level? For
> > > example, if we want spark drive can move to another node continue
> > execution
> > > when failure happen. I can see the RDD checkpoint can help to
> > serialization
> > > the status of RDD. I can image to load the check point from another
> node
> > > when error happen, but seems like will lost track all tasks status or
> > even
> > > executor information that maintain in spark context. I am not sure if
> > there
> > > is any existing stuff I can leverage to do that. thanks for any
> suggests
> > >
> > > Best Regards
> > >
> > >
> > > *Jun Feng Liu*
> > > IBM China Systems & Technology Laboratory in Beijing
> > >
> > >   ------------------------------
> > >  [image: 2D barcode - encoded with contact information] *Phone:
> > *86-10-82452683
> > >
> > > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > > [image: IBM]
> > >
> > > BLD 28,ZGC Software Park
> > > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > > China
> > >
> > >
> > >
> > >
> > >
> >
> >
>
>

--089e013a06063ed6000509efd069--

From dev-return-10744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 14:30:49 2014
Return-Path: <dev-return-10744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6256AC54D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 14:30:49 +0000 (UTC)
Received: (qmail 55712 invoked by uid 500); 11 Dec 2014 14:30:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55637 invoked by uid 500); 11 Dec 2014 14:30:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55626 invoked by uid 99); 11 Dec 2014 14:30:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 14:30:47 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 14:30:22 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 7E576D5B1A7
	for <dev@spark.incubator.apache.org>; Thu, 11 Dec 2014 06:29:30 -0800 (PST)
Date: Thu, 11 Dec 2014 07:29:30 -0700 (MST)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418308170239-9728.post@n3.nabble.com>
In-Reply-To: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (non-binding)

Built and tested on Windows 7:

cd apache-spark
git fetch
git checkout v1.2.0-rc2
sbt assembly
[warn]
...
[warn]
[success] Total time: 720 s, completed Dec 11, 2014 8:57:36 AM

dir assembly\target\scala-2.10\spark-assembly-1.2.0-hadoop1.0.4.jar
110,361,054 spark-assembly-1.2.0-hadoop1.0.4.jar

Ran some of my 1.2 code successfully.
Review some docs, looks good.
spark-shell.cmd works as expected.

Env details:
sbtconfig.txt:
-Xmx1024M
-XX:MaxPermSize=256m
-XX:ReservedCodeCacheSize=128m

sbt --version
sbt launcher version 0.13.1




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-0-RC2-tp9713p9728.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 15:02:32 2014
Return-Path: <dev-return-10745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC268C7AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 15:02:32 +0000 (UTC)
Received: (qmail 99574 invoked by uid 500); 11 Dec 2014 15:02:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99503 invoked by uid 500); 11 Dec 2014 15:02:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99492 invoked by uid 99); 11 Dec 2014 15:02:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 15:02:30 +0000
X-ASF-Spam-Status: No, hits=0.2 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.140 as permitted sender)
Received: from [202.81.31.140] (HELO e23smtp07.au.ibm.com) (202.81.31.140)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 15:02:24 +0000
Received: from /spool/local
	by e23smtp07.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Fri, 12 Dec 2014 01:01:01 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp07.au.ibm.com (202.81.31.204) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 12 Dec 2014 01:00:59 +1000
Received: from d23relay07.au.ibm.com (d23relay07.au.ibm.com [9.190.26.37])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id D7D722BB0065
	for <dev@spark.apache.org>; Fri, 12 Dec 2014 02:00:58 +1100 (EST)
Received: from d23av03.au.ibm.com (d23av03.au.ibm.com [9.190.234.97])
	by d23relay07.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBBF0oG825886736
	for <dev@spark.apache.org>; Fri, 12 Dec 2014 02:00:58 +1100
Received: from d23av03.au.ibm.com (localhost [127.0.0.1])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBBF0Qkg014586
	for <dev@spark.apache.org>; Fri, 12 Dec 2014 02:00:26 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBBF0P03013775;
	Fri, 12 Dec 2014 02:00:25 +1100
In-Reply-To: <CAMwrk0kN_3LUGVTfBf_Zh9rtMrzeoyoqNzFFiKJqn3tDbOvYKw@mail.gmail.com>
References: <OF5A4A2C9F.554DDE0D-ON48257DAA.002DD89C-48257DAA.002E48C4@cn.ibm.com> <CAPh_B=YRYAXaPc2d6VJaVX-6813d45F4__dEUyzj2+jTyAXKWA@mail.gmail.com> <OF82EF1068.807D462B-ON48257DAA.00498429-48257DAA.004A3056@cn.ibm.com> <CACBYxKKYSqsWUvkvPspMdK5KumU+mDCzyMijxOf5VsoftXz7xQ@mail.gmail.com> <OF1B7F9E07.326E1FF3-ON48257DAB.00137AD8-48257DAB.00139872@cn.ibm.com> <CAMwrk0kN_3LUGVTfBf_Zh9rtMrzeoyoqNzFFiKJqn3tDbOvYKw@mail.gmail.com>
To: Tathagata Das <tathagata.das1565@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
        Reynold Xin <rxin@databricks.com>,
        Sandy Ryza <sandy.ryza@cloudera.com>
MIME-Version: 1.0
Subject: Re: HA support for Spark
X-KeepSent: F84B51D8:3DF95FDB-48257DAB:005242A9;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OFF84B51D8.3DF95FDB-ON48257DAB.005242A9-48257DAB.00526396@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Thu, 11 Dec 2014 22:59:59 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/11/2014 23:00:25,
	Serialize complete at 12/11/2014 23:00:25
Content-Type: multipart/related; boundary="=_related 0052638E48257DAB_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121115-0025-0000-0000-000000B5B56E
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 0052638E48257DAB_=
Content-Type: multipart/alternative; boundary="=_alternative 0052639348257DAB_="


--=_alternative 0052639348257DAB_=
Content-Type: text/plain; charset="US-ASCII"

Interesting, you saying StreamContext checkpoint can regenerate DAG stuff? 

 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Tathagata Das <tathagata.das1565@gmail.com> 
2014/12/11 20:20

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
Sandy Ryza <sandy.ryza@cloudera.com>, "dev@spark.apache.org" 
<dev@spark.apache.org>, Reynold Xin <rxin@databricks.com>
Subject
Re: HA support for Spark






Spark Streaming essentially does this by saving the DAG of DStreams, which 
can deterministically regenerate the DAG of RDDs upon recovery from 
failure. Along with that the progress information (which batches have 
finished, which batches are queued, etc.) is also saved, so that upon 
recovery the system can restart from where it was before failure. This was 
conceptually easy to do because the RDDs are very deterministically 
generated in every batch. Extending this to a very general Spark program 
with arbitrary RDD computations is definitely conceptually possible but 
not that easy to do.

On Wed, Dec 10, 2014 at 7:34 PM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
Right, perhaps also need preserve some DAG information? I am wondering if 
there is any work around this.


Sandy Ryza ---2014-12-11 01:36:35---Sandy Ryza <sandy.ryza@cloudera.com>


Sandy Ryza <sandy.ryza@cloudera.com>  
2014-12-11 01:34



To

Jun Feng Liu/China/IBM@IBMCN, 

cc

Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <
dev@spark.apache.org>

Subject

Re: HA support for Spark





I think that if we were able to maintain the full set of created RDDs as
well as some scheduler and block manager state, it would be enough for 
most
apps to recover.

On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Well, it should not be mission impossible thinking there are so many HA
> solution existing today. I would interest to know if there is any 
specific
> difficult.
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: 
*86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
>
> 2014/12/10 16:30
>   To
> Jun Feng Liu/China/IBM@IBMCN,
> cc
> "dev@spark.apache.org" <dev@spark.apache.org>
> Subject
> Re: HA support for Spark
>
>
>
>
> This would be plausible for specific purposes such as Spark streaming or
> Spark SQL, but I don't think it is doable for general Spark driver since 
it
> is just a normal JVM process with arbitrary program state.
>
> On Wed, Dec 10, 2014 at 12:25 AM, Jun Feng Liu <liujunf@cn.ibm.com> 
wrote:
>
> > Do we have any high availability support in Spark driver level? For
> > example, if we want spark drive can move to another node continue
> execution
> > when failure happen. I can see the RDD checkpoint can help to
> serialization
> > the status of RDD. I can image to load the check point from another 
node
> > when error happen, but seems like will lost track all tasks status or
> even
> > executor information that maintain in spark context. I am not sure if
> there
> > is any existing stuff I can leverage to do that. thanks for any 
suggests
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  [image: 2D barcode - encoded with contact information] *Phone:
> *86-10-82452683
> >
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
>
>



--=_alternative 0052639348257DAB_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 0052639348257DAB_=--
--=_related 0052638E48257DAB_=--


From dev-return-10746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 16:59:20 2014
Return-Path: <dev-return-10746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13502CE18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 16:59:20 +0000 (UTC)
Received: (qmail 51027 invoked by uid 500); 11 Dec 2014 16:59:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50952 invoked by uid 500); 11 Dec 2014 16:59:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50940 invoked by uid 99); 11 Dec 2014 16:59:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 16:59:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 16:58:51 +0000
Received: by mail-wg0-f42.google.com with SMTP id z12so6983342wgg.1
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 08:58:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=JflsEWgWFZG1iTH1lbNm0XBIJTGvub4GK9xdlAHnfUQ=;
        b=Sz6+8AIN2L31wAGTrPziBDNpFivyEjyX7d/l2Bfj/OWc71ULY7CMJMihttpXPI/ayo
         V3+YewQPozX82VVzjbp0V+cs0Pe5OBW2JxfnSOg+eddyPSQxxNOi3UYkx39nFCUirGQG
         FuiLSjUlZYDi2WA+5RAph305+JRPTKm3kstUJuKsxHyhFAQUdea0XeqfyKRxq83OEvL4
         C8mUCpIoc1PqgWeCY1f8D+kfUAp+kz155pbI0hcdHJoDMDZc18l+pgPjKI1Gy4+E3G9W
         Mn9c+MmmqDrJ6SKbyG41XHA9p0V3wRmpPail4m/nk6/hcH8PJyyth1DaVSYndnNL5O4e
         ayiw==
X-Gm-Message-State: ALoCoQkJl+PQMrB+DZJyHZNjsIbNdhU2c3cjoEt4+MQ7dLO4+DCYKEV7JEPOvjftiLtcWlOzMoq7
X-Received: by 10.194.80.193 with SMTP id t1mr18705677wjx.8.1418317130770;
 Thu, 11 Dec 2014 08:58:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Thu, 11 Dec 2014 08:58:29 -0800 (PST)
In-Reply-To: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 11 Dec 2014 16:58:29 +0000
Message-ID: <CAMAsSdKG0CKC9qRbTNMHe9JjLP69wasLoJOJ1TBF5R63CTBnew@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Signatures and checksums are OK. License and notice still looks fine.
The plain-vanilla source release compiles with Maven 3.2.1 and passes
tests, on OS X 10.10 + Java 8.

On Wed, Dec 10, 2014 at 9:08 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.2.0!
>
> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1055/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.0!
>
> The vote is open until Saturday, December 13, at 21:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == What justifies a -1 vote for this release? ==
> This vote is happening relatively late into the QA period, so
> -1 votes should only occur for significant regressions from
> 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
>
> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to "hash".
>
> == How does this differ from RC1 ==
> This has fixes for a handful of issues identified - some of the
> notable fixes are:
>
> [Core]
> SPARK-4498: Standalone Master can fail to recognize completed/failed
> applications
>
> [SQL]
> SPARK-4552: Query for empty parquet table in spark sql hive get
> IllegalArgumentException
> SPARK-4753: Parquet2 does not prune based on OR filters on partition columns
> SPARK-4761: With JDBC server, set Kryo as default serializer and
> disable reference tracking
> SPARK-4785: When called with arguments referring column fields, PMOD throws NPE
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 17:40:11 2014
Return-Path: <dev-return-10747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45EA3CF95
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 17:40:11 +0000 (UTC)
Received: (qmail 37036 invoked by uid 500); 11 Dec 2014 17:40:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36958 invoked by uid 500); 11 Dec 2014 17:40:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36941 invoked by uid 99); 11 Dec 2014 17:40:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 17:40:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 17:40:03 +0000
Received: by mail-qa0-f45.google.com with SMTP id x12so3913658qac.18
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 09:38:37 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=AKgFdcBisAVurD80DBEe+VGNuCByHdegI5M3fSVmmFk=;
        b=YxqZpDKJBCu/io3KT1HI/aPnJxJCzSEdJKegiaT/e5QxqrlVyK9E/83Id8zvqAr3Qg
         drdICwOLZ75WbCYfhm1vKSWbJueQvw1EcPJh4uG5sXxRcvWUZ7X259Fex+0+ERTnKTVY
         eJh21nq/MvihDbgHYVtEajsVFffoYQPN3cG17mlHH8qVvgyMXvIMB5SNcSyTXr5Af6N6
         Kmn+RFIrQPDkrAz9Shbz70TYPzsdvpbVxOTGczs31DP27jnct3IOzSN+e0cUVB/BHRMp
         WJqgM3qtU2Wa6ovFeiBQ5tJB9AJ4ZXU6B7gK6rx2WUG7jvhmaIuc0pDbLBM/pOrFO+mh
         O/AA==
X-Gm-Message-State: ALoCoQmNpzoAbzst3bin1NFoskOyQCyj25PDLkJfeEm2k1c2QBBiiiMaH4YQNuyZpWOI/gTYWnxM
MIME-Version: 1.0
X-Received: by 10.224.74.207 with SMTP id v15mr21743755qaj.53.1418319517064;
 Thu, 11 Dec 2014 09:38:37 -0800 (PST)
Received: by 10.96.173.100 with HTTP; Thu, 11 Dec 2014 09:38:36 -0800 (PST)
In-Reply-To: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
Date: Thu, 11 Dec 2014 09:38:36 -0800
Message-ID: <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Reynold Xin <rxin@databricks.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c9a0fe47210509f43ef4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c9a0fe47210509f43ef4
Content-Type: text/plain; charset=UTF-8

+1

Tested on OS X.

On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.2.0!
>
> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1055/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.2.0!
>
> The vote is open until Saturday, December 13, at 21:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.2.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == What justifies a -1 vote for this release? ==
> This vote is happening relatively late into the QA period, so
> -1 votes should only occur for significant regressions from
> 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
>
> == What default changes should I be aware of? ==
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
>
> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to
> "hash".
>
> == How does this differ from RC1 ==
> This has fixes for a handful of issues identified - some of the
> notable fixes are:
>
> [Core]
> SPARK-4498: Standalone Master can fail to recognize completed/failed
> applications
>
> [SQL]
> SPARK-4552: Query for empty parquet table in spark sql hive get
> IllegalArgumentException
> SPARK-4753: Parquet2 does not prune based on OR filters on partition
> columns
> SPARK-4761: With JDBC server, set Kryo as default serializer and
> disable reference tracking
> SPARK-4785: When called with arguments referring column fields, PMOD
> throws NPE
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;>
>
>

--089e0149c9a0fe47210509f43ef4--

From dev-return-10748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 17:52:39 2014
Return-Path: <dev-return-10748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7DA6110016
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 17:52:39 +0000 (UTC)
Received: (qmail 78555 invoked by uid 500); 11 Dec 2014 17:52:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78483 invoked by uid 500); 11 Dec 2014 17:52:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78472 invoked by uid 99); 11 Dec 2014 17:52:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 17:52:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 17:52:33 +0000
Received: by mail-qa0-f42.google.com with SMTP id j7so3944929qaq.15
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 09:51:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=lpLSeU9R35uo53cC58IXqqN8G6H/aWcgOGlrYxO6Qvg=;
        b=YWs+xXwHlpvtlofCI7lQeDo6cIyOCx1PDEKaYGf95DKP43R+LkvewrUUmxmphYF+qu
         JM4VMn3YWqr9zVSM+bAsIWHs2GXjsB7rmU8NJKWPQQJXcL9O3BMWqPYTEoZGddFjr/XG
         hEpvlS7WIukGW/pl6hZQwW4328zapfIEHFyuCXj0JwFcDZB95AhLb8+4feLoCFRzP6A+
         e2zj+56NpSp1c+8iH2KUqh+8unC0TBzw+naZq3mkqTZsLkj08JY1zO99b7bT8D819Z0D
         2jZAAQsZHYvnF5gWTXrsQDa95c4znbD2eoUR1+xwtYFauH8w27SAg0jHplNqVGc0EcOG
         wbvA==
X-Gm-Message-State: ALoCoQldvUK3SpCW1etKV2Mef1egvzqeLYad0KqrJDLe1dWDrE1yeaaYUtp313SSo8MW6JAZe4bS
MIME-Version: 1.0
X-Received: by 10.140.38.170 with SMTP id t39mr21113697qgt.15.1418320287753;
 Thu, 11 Dec 2014 09:51:27 -0800 (PST)
Received: by 10.140.102.113 with HTTP; Thu, 11 Dec 2014 09:51:27 -0800 (PST)
In-Reply-To: <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
	<CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
Date: Thu, 11 Dec 2014 09:51:27 -0800
Message-ID: <CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c12196ee030e0509f46c10
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12196ee030e0509f46c10
Content-Type: text/plain; charset=UTF-8

+1 (non-binding).  Tested on Ubuntu against YARN.

On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin <rxin@databricks.com> wrote:

> +1
>
> Tested on OS X.
>
> On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.2.0!
> >
> > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1055/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.2.0!
> >
> > The vote is open until Saturday, December 13, at 21:00 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > == What justifies a -1 vote for this release? ==
> > This vote is happening relatively late into the QA period, so
> > -1 votes should only occur for significant regressions from
> > 1.0.2. Bugs already present in 1.1.X, minor
> > regressions, or bugs related to new features will not block this
> > release.
> >
> > == What default changes should I be aware of? ==
> > 1. The default value of "spark.shuffle.blockTransferService" has been
> > changed to "netty"
> > --> Old behavior can be restored by switching to "nio"
> >
> > 2. The default value of "spark.shuffle.manager" has been changed to
> "sort".
> > --> Old behavior can be restored by setting "spark.shuffle.manager" to
> > "hash".
> >
> > == How does this differ from RC1 ==
> > This has fixes for a handful of issues identified - some of the
> > notable fixes are:
> >
> > [Core]
> > SPARK-4498: Standalone Master can fail to recognize completed/failed
> > applications
> >
> > [SQL]
> > SPARK-4552: Query for empty parquet table in spark sql hive get
> > IllegalArgumentException
> > SPARK-4753: Parquet2 does not prune based on OR filters on partition
> > columns
> > SPARK-4761: With JDBC server, set Kryo as default serializer and
> > disable reference tracking
> > SPARK-4785: When called with arguments referring column fields, PMOD
> > throws NPE
> >
> > - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> > For additional commands, e-mail: dev-help@spark.apache.org
> <javascript:;>
> >
> >
>

--001a11c12196ee030e0509f46c10--

From dev-return-10749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 18:02:39 2014
Return-Path: <dev-return-10749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43FF710090
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 18:02:39 +0000 (UTC)
Received: (qmail 97016 invoked by uid 500); 11 Dec 2014 18:02:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96939 invoked by uid 500); 11 Dec 2014 18:02:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 89373 invoked by uid 99); 11 Dec 2014 12:42:06 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of paul.mwanjohi@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Thu, 11 Dec 2014 05:41:09 -0700 (MST)
From: kidynamit <paul.mwanjohi@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418301669572-9727.post@n3.nabble.com>
Subject: Evaluation Metrics for Spark's MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, 

I would like to contribute to Spark's Machine Learning library by adding
evaluation metrics that would be used to gauge the accuracy of a model given
a certain features' set. In particular, I seek to contribute the k-fold
validation metrics, f-beta metric among others on top of the current MLlib
framework available.

Please assist in steps I could take to contribute in this manner. 

Regards, 
kidynamit



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Evaluation-Metrics-for-Spark-s-MLlib-tp9727.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 22:38:15 2014
Return-Path: <dev-return-10750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 46AC8C314
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 22:38:15 +0000 (UTC)
Received: (qmail 95556 invoked by uid 500); 11 Dec 2014 22:38:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95478 invoked by uid 500); 11 Dec 2014 22:38:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94931 invoked by uid 99); 11 Dec 2014 22:38:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 22:38:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 22:37:46 +0000
Received: by mail-ie0-f174.google.com with SMTP id rl12so5822530iec.19
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 14:37:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=MxjsV2o/vTpAPrFbCGbARrx6vVRpgQA4jo7ag0N5cwc=;
        b=BpHkHlQ0Lhf6rTr07neIWs6ywvcVutH/D2hPG68a+PswYE09tJIBap+J5furTxW8xl
         6y1MlW+7dhlpKsyHW66cXv1tai8I41cHKplnCZSPjwJjADSmI8xLw8fA76w3+FD4OQvW
         2HxWEfDVspYadkwOgg/RKNxWg4tLbMdkWpWxUSAv6GiWO+nT0W/yS5Tdo77E0qPsYydn
         3L48MjYf0T8ftj7xQFfoxtHYfs3m5HhUrfHBh9g5e8FEe1PCqPQV0oBH36OKxVdloZvA
         3WV06b1E+3/otB2aqNoekWoOHcUYRI0wJ50O2liz0HPtSDweC8K9ItPMHpOid2pgk2/x
         mBjw==
X-Received: by 10.50.29.3 with SMTP id f3mr1498918igh.23.1418337465612; Thu,
 11 Dec 2014 14:37:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.232.37 with HTTP; Thu, 11 Dec 2014 14:37:25 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 11 Dec 2014 14:37:25 -0800
Message-ID: <CAJc_syLHwp=E2qF=YUbXkhSN7bB=H7gT9Fdr9168Tbh5O=bNpQ@mail.gmail.com>
Subject: Where are the docs for the SparkSQL DataTypes?
To: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd74b54cf3fb70509f86cb3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74b54cf3fb70509f86cb3
Content-Type: text/plain; charset=UTF-8

Michael & other Spark SQL junkies,

As I read through the Spark API docs, in particular those for the
org.apache.spark.sql package, I can't seem to find details about the Scala
classes representing the various SparkSQL DataTypes, for instance
DecimalType. I find DataType classes in org.apache.spark.sql.api.java, but
they don't seem to match the similarly named scala classes. For instance,
DecimalType is documented as having a nullary constructor, but if I try to
construct an instance of org.apache.spark.sql.DecimalType without any
parameters, the compiler complains about the lack of a precisionInfo field,
which I have discovered can be passed in as None. Where is all this stuff
documented?

Alex

--047d7bd74b54cf3fb70509f86cb3--

From dev-return-10751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 23:10:07 2014
Return-Path: <dev-return-10751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA893C484
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 23:10:07 +0000 (UTC)
Received: (qmail 59684 invoked by uid 500); 11 Dec 2014 23:10:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59612 invoked by uid 500); 11 Dec 2014 23:10:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59596 invoked by uid 99); 11 Dec 2014 23:10:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:10:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:10:00 +0000
Received: by mail-la0-f50.google.com with SMTP id pn19so5274601lab.9
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 15:08:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=TKTLs+iI4q38EJzG5ZpcvZV8TCaDcrmHG42KXDuGOBM=;
        b=djg+tTnt+QmTreNymoGSuHj6bjwVtrai2rREsdxerbmYX4Sc41Iuuou1+HU3rhz57N
         7oxjL0Ex8HmUHSfUEFvYANvUDrPKVZUXKWZ1xJV180erI57/bYBCEqBr4O90mWo1Zd++
         uuXJeOkvrF61tvBk0iqU/McLSlcHBKYgxZOHs1icOXF+Fix9enADZvVD8UmV1ZOw8APS
         nO8vViXwK5yv1fxkfCvjjrg4D9Fame0EgQZ58uW/38klWBQN3SQs4encdaX9J3un4KtB
         aVEzmR3NFS6Des9cxpqRzq8YncxsajC0zVAaJcmTJfIJflt3P/Wmr/ZXZy9aoZPtwI45
         c6GQ==
X-Gm-Message-State: ALoCoQlDTZ5fzQu0PLqgQDgZCzI81835gj6HNgm7HWHRnuVihZi+/mSGFv0LERxRGfKXPfN0yW62
X-Received: by 10.152.5.38 with SMTP id p6mr12341440lap.91.1418339334445; Thu,
 11 Dec 2014 15:08:54 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Thu, 11 Dec 2014 15:08:34 -0800 (PST)
In-Reply-To: <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
 <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 11 Dec 2014 15:08:34 -0800
Message-ID: <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01419ace336b830509f8dcff
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01419ace336b830509f8dcff
Content-Type: text/plain; charset=UTF-8

here's the plan...  reboots, of course, come last.  :)

pause build queue at 7am, kill off (and eventually retrigger) any
stragglers at 8am.  then begin maintenance:

all systems:
* yum update all servers (amp-jekins-master, amp-jenkins-slave-{01..05},
amp-jenkins-worker-{01..08})
* reboots

jenkins slaves:
* install python2.7 (along side 2.6, which would remain the default)
* install numpy 1.9.1 (currently on 1.4, breaking some spark branch builds)
* add new slaves to the master, remove old ones (keep them around just in
case)

there will be no jenkins system or plugin upgrades at this time.  things
there seems to be working just fine!

i'm expecting to be up and building by 9am at the latest.  i'll update this
thread w/any new time estimates.

word.

shane, your rained-in devops guy :)

On Wed, Dec 10, 2014 at 11:28 AM, shane knapp <sknapp@berkeley.edu> wrote:

> reminder -- this is happening friday morning @ 730am!
>
> On Mon, Dec 1, 2014 at 5:10 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> i'll send out a reminder next week, but i wanted to give a heads up:
>>  i'll be bringing down the entire jenkins infrastructure for reboots and
>> system updates.
>>
>> please let me know if there are any conflicts with this, thanks!
>>
>> shane
>>
>
>

--089e01419ace336b830509f8dcff--

From dev-return-10752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 23:25:24 2014
Return-Path: <dev-return-10752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D4F2C5A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 23:25:24 +0000 (UTC)
Received: (qmail 90989 invoked by uid 500); 11 Dec 2014 23:25:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90910 invoked by uid 500); 11 Dec 2014 23:25:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90899 invoked by uid 99); 11 Dec 2014 23:25:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:25:22 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:24:57 +0000
Received: by mail-oi0-f49.google.com with SMTP id i138so4433855oig.36
        for <dev@spark.incubator.apache.org>; Thu, 11 Dec 2014 15:23:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=FUxoIWT9dUBn2FFGSghsdOliORqaLsP6z6XTUbVXp34=;
        b=GK0w/VUej7UOPkIjuCDERQ2xuvC1WOiVsGU1H7nzwALDcadK+hZUPagpaR6UsYlGF2
         pyVgVToEUxMsrftuPlRI9+8WDF3+rIEgJ2NC5bG2djKxevpZC8xZJCR6f5Jp3MDDSi4q
         Pg1FBABxrfB+lA/xb2GyqLQjIywL1Y7TRiWPBP6ndWz7iEr7Y7KPsF/9PVwxq+TXwU1N
         eVD9IZ3K0gFXHfTellMRULiNfEy/LL7Tte9VIIxCwJ1fqmvI9BawEIZRU7BSmJSWoqm+
         C1oIzdMApvdt+87KeOwIZ+r7/eqSsHjajtJUaVM21T+x2Ojk5liJAl5v+fkt3owdqEV7
         lTzA==
X-Gm-Message-State: ALoCoQkVn4kn4A5njLNaPUTj+i0IunS81q85aTPZy30xPznPqvF0fzW/Kmp/9+RR/IxJrSom/xfD
MIME-Version: 1.0
X-Received: by 10.202.48.82 with SMTP id w79mr7792748oiw.30.1418340230521;
 Thu, 11 Dec 2014 15:23:50 -0800 (PST)
Received: by 10.60.14.196 with HTTP; Thu, 11 Dec 2014 15:23:50 -0800 (PST)
In-Reply-To: <1418301669572-9727.post@n3.nabble.com>
References: <1418301669572-9727.post@n3.nabble.com>
Date: Thu, 11 Dec 2014 15:23:50 -0800
Message-ID: <CAF7ADNpOYhHZTjuzzt_nEnwJnzckg8eq6DCJRtXNy1sQPwPKeQ@mail.gmail.com>
Subject: Re: Evaluation Metrics for Spark's MLlib
From: Joseph Bradley <joseph@databricks.com>
To: kidynamit <paul.mwanjohi@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a113cda369c825e0509f911f0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cda369c825e0509f911f0
Content-Type: text/plain; charset=UTF-8

Hi, I'd recommend starting by checking out the existing helper
functionality for these tasks.  There are helper methods to do K-fold
cross-validation in MLUtils:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala

The experimental spark.ml API in the Spark 1.2 release (in branch-1.2 and
master) has a CrossValidator class which does this more automatically:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tuning/CrossValidator.scala

There are also a few evaluation metrics implemented:
https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/evaluation

There definitely could be more metrics and/or better APIs to make it easier
to evaluate models on RDDs.  If you spot such cases, I'd recommend opening
up JIRAs for the new features or improvements to get some feedback before
sending PRs:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Hope this helps & looking forward to the contributions!
Joseph

On Thu, Dec 11, 2014 at 4:41 AM, kidynamit <paul.mwanjohi@gmail.com> wrote:

> Hi,
>
> I would like to contribute to Spark's Machine Learning library by adding
> evaluation metrics that would be used to gauge the accuracy of a model
> given
> a certain features' set. In particular, I seek to contribute the k-fold
> validation metrics, f-beta metric among others on top of the current MLlib
> framework available.
>
> Please assist in steps I could take to contribute in this manner.
>
> Regards,
> kidynamit
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Evaluation-Metrics-for-Spark-s-MLlib-tp9727.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113cda369c825e0509f911f0--

From dev-return-10753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 23:52:26 2014
Return-Path: <dev-return-10753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58FD2C6C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 23:52:26 +0000 (UTC)
Received: (qmail 59698 invoked by uid 500); 11 Dec 2014 23:52:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59629 invoked by uid 500); 11 Dec 2014 23:52:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59618 invoked by uid 99); 11 Dec 2014 23:52:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:52:24 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:51:55 +0000
Received: by mail-ig0-f172.google.com with SMTP id hl2so565852igb.17
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 15:51:34 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=lif2T/J7HROBpjTxq/CLMVkp49itctNKsF8dXdenciE=;
        b=d4wfmpVf+MC0VLVmKgmtWtrLY9dQFeOlUL7Z4cUoCBnzmKzd8zvvq6sI0H/3+n2WmY
         zh3ywuE0fVlf7myckPZFSG81A4uw1KmG6azdPbr85stm2zvpWiQdlpsFcTqQ8xxK5gUD
         OuMoLvpcp8Hsam22MXZBugkAjPEu81CU6jDpe3T6yDFOkR7fxb8vhWxgdz3IT5S3puWh
         gJr6wF7L7WjkNIiBoQXO8UEJchCIEM2gyqs8AWFrrkQdhVl52/rur5aHh1WOnBUX+3CW
         qLqEwKR8e7TZbIBjpfGrzCGWxYfSlKJo0aS9CMgOqVHdYhLO0gzoVWGUmQ5wDghHsFYF
         3v3g==
X-Gm-Message-State: ALoCoQknucXadgbgNnH/Dx2qJfLyoVCKukJzDWK3EUq/Js5iAKMmhowK2IcqJMZlCELI3sQZsYxC
X-Received: by 10.50.117.71 with SMTP id kc7mr1813975igb.35.1418341894524;
        Thu, 11 Dec 2014 15:51:34 -0800 (PST)
Received: from mail-ie0-f171.google.com (mail-ie0-f171.google.com. [209.85.223.171])
        by mx.google.com with ESMTPSA id lr5sm511835igb.16.2014.12.11.15.51.33
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 11 Dec 2014 15:51:33 -0800 (PST)
Received: by mail-ie0-f171.google.com with SMTP id rl12so5859201iec.2
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 15:51:32 -0800 (PST)
X-Received: by 10.107.136.92 with SMTP id k89mr12767914iod.43.1418341892918;
 Thu, 11 Dec 2014 15:51:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.223.115 with HTTP; Thu, 11 Dec 2014 15:51:12 -0800 (PST)
In-Reply-To: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 11 Dec 2014 15:51:12 -0800
Message-ID: <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com>
Subject: Re: Tachyon in Spark
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ecad4b2b75b0509f97473
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ecad4b2b75b0509f97473
Content-Type: text/plain; charset=UTF-8

I'm interested in understanding this as well.  One of the main ways Tachyon
is supposed to realize performance gains without sacrificing durability is
by storing the lineage of data rather than full copies of it (similar to
Spark).  But if Spark isn't sending lineage information into Tachyon, then
I'm not sure how this isn't a durability concern.

On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:

> Dose Spark today really leverage Tachyon linage to process data? It seems
> like the application should call createDependency function in TachyonFS
> to create a new linage node. But I did not find any place call that in
> Spark code. Did I missed anything?
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>

--001a113ecad4b2b75b0509f97473--

From dev-return-10754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 11 23:56:51 2014
Return-Path: <dev-return-10754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8ED7C6EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Dec 2014 23:56:51 +0000 (UTC)
Received: (qmail 67489 invoked by uid 500); 11 Dec 2014 23:56:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67424 invoked by uid 500); 11 Dec 2014 23:56:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67413 invoked by uid 99); 11 Dec 2014 23:56:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:56:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Dec 2014 23:56:23 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so2598684qgd.33
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 15:54:32 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6f9cOkDZK4e5pxjRE1Vl2uWlkDwoEi0P1qlTpgLBOkg=;
        b=hWnxIV4jDQB/K70pVhMZ808Urj/Kpd+ZhKihg4BktwBE9z6QySDsPD+LclKCr8f17Q
         Bmbtn9guzbAWzX/9jpUspKVqSiIUVoarXkV0bt7ve8GoapZfZyToFsjW8MbZ4LSvsScg
         MpFvKLFVJfzlDJkePMo87fyJlbJbPnZ8GnXfREXOlr7rQUecdT7+J5luCxLT9ksSQHX6
         p+tsPbVP7pAxmwdQJi0h1uzkH4nWlL7xlbJtMasOailR75UDc/cvM0kR02QGCPfFERN0
         /XrwQOwvXlKgO4H7o4uEVvJwq8XaPGfB8Y6omhO70wMnyiHQEl49lFSuQqxc6FqiYWIe
         qliA==
X-Gm-Message-State: ALoCoQlFojdlbU8LSy1jbMJdqfcGRuX+w5xIxxyfna4eBtUqiIMxrbG6Pha50YhKsuqkEq5+FDmy
X-Received: by 10.224.124.71 with SMTP id t7mr13635225qar.42.1418342072084;
 Thu, 11 Dec 2014 15:54:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Thu, 11 Dec 2014 15:54:11 -0800 (PST)
In-Reply-To: <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
 <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 11 Dec 2014 15:54:11 -0800
Message-ID: <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com>
Subject: Re: Tachyon in Spark
To: Andrew Ash <andrew@andrewash.com>
Cc: Jun Feng Liu <liujunf@cn.ibm.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2bc7e6080240509f97f3c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2bc7e6080240509f97f3c
Content-Type: text/plain; charset=UTF-8

I don't think the lineage thing is even turned on in Tachyon - it was
mostly a research prototype, so I don't think it'd make sense for us to use
that.


On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash <andrew@andrewash.com> wrote:

> I'm interested in understanding this as well.  One of the main ways Tachyon
> is supposed to realize performance gains without sacrificing durability is
> by storing the lineage of data rather than full copies of it (similar to
> Spark).  But if Spark isn't sending lineage information into Tachyon, then
> I'm not sure how this isn't a durability concern.
>
> On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
> > Dose Spark today really leverage Tachyon linage to process data? It seems
> > like the application should call createDependency function in TachyonFS
> > to create a new linage node. But I did not find any place call that in
> > Spark code. Did I missed anything?
> >
> > Best Regards
> >
> >
> > *Jun Feng Liu*
> > IBM China Systems & Technology Laboratory in Beijing
> >
> >   ------------------------------
> >  [image: 2D barcode - encoded with contact information] *Phone:
> *86-10-82452683
> >
> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> > [image: IBM]
> >
> > BLD 28,ZGC Software Park
> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> > China
> >
> >
> >
> >
> >
>

--001a11c2bc7e6080240509f97f3c--

From dev-return-10755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 00:07:46 2014
Return-Path: <dev-return-10755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AFECEC76D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 00:07:46 +0000 (UTC)
Received: (qmail 86441 invoked by uid 500); 12 Dec 2014 00:07:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86374 invoked by uid 500); 12 Dec 2014 00:07:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86358 invoked by uid 99); 12 Dec 2014 00:07:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 00:07:44 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tharsch@cray.com designates 64.18.1.39 as permitted sender)
Received: from [64.18.1.39] (HELO exprod6og117.obsmtp.com) (64.18.1.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 00:07:40 +0000
Received: from CFWEX01.americas.cray.com ([136.162.34.11]) (using TLSv1) by exprod6ob117.postini.com ([64.18.5.12]) with SMTP
	ID DSNKVIoxf/mkpWnaol433eobnJYBQfJBwMKE@postini.com; Thu, 11 Dec 2014 16:07:20 PST
Received: from CFWEX02.americas.cray.com (172.30.74.25) by
 CFWEX01.americas.cray.com (172.30.88.25) with Microsoft SMTP Server (TLS) id
 14.2.347.0; Thu, 11 Dec 2014 18:06:22 -0600
Received: from CFWEX01.americas.cray.com ([169.254.1.243]) by
 cfwex02.americas.cray.com ([169.254.2.115]) with mapi id 14.02.0387.000; Thu,
 11 Dec 2014 18:06:22 -0600
From: Tim Harsch <tharsch@cray.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: running the Terasort example
Thread-Topic: running the Terasort example
Thread-Index: AQHQFZ91EFW+gRhFhEGi9Qc+FDrHCg==
Date: Fri, 12 Dec 2014 00:06:20 +0000
Message-ID: <D0AF7170.4C6E%tharsch@cray.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.7.141117
x-originating-ip: [192.168.231.39]
Content-Type: text/plain; charset="iso-8859-1"
Content-ID: <6013BF8BBFBA1443837DAA2A9372C3B3@cray.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,
I just joined the list, so I don=B9t have a message history that would allo=
w
me to reply to this post:
http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-example-
td9284.html

I am interested in running the terasort example.  I cloned the repo
https://github.com/ehiggs/spark and did checkout of the terasort branch.
In the above referenced post Ewan gives the example

# Generate 1M 100 byte records:
  ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in


I don=B9t see a =B3run-example=B2 in that repo.  I=B9m sure I am missing so=
mething
basic, or less likely, maybe some changes weren=B9t pushed?

Thanks for any help,
Tim


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 00:37:06 2014
Return-Path: <dev-return-10756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 94DB2C84D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 00:37:06 +0000 (UTC)
Received: (qmail 48735 invoked by uid 500); 12 Dec 2014 00:37:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48663 invoked by uid 500); 12 Dec 2014 00:37:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48652 invoked by uid 99); 12 Dec 2014 00:37:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 00:37:04 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hao.cheng@intel.com designates 134.134.136.24 as permitted sender)
Received: from [134.134.136.24] (HELO mga09.intel.com) (134.134.136.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 00:37:00 +0000
Received: from orsmga003.jf.intel.com ([10.7.209.27])
  by orsmga102.jf.intel.com with ESMTP; 11 Dec 2014 16:34:14 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.04,691,1406617200"; 
   d="scan'208";a="497557213"
Received: from kmsmsx152.gar.corp.intel.com ([172.21.73.87])
  by orsmga003.jf.intel.com with ESMTP; 11 Dec 2014 16:31:44 -0800
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 KMSMSX152.gar.corp.intel.com (172.21.73.87) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Fri, 12 Dec 2014 08:35:36 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.216]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.182]) with mapi id 14.03.0195.001;
 Fri, 12 Dec 2014 08:35:34 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Alessandro Baretta <alexbaretta@gmail.com>, Michael Armbrust
	<michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Where are the docs for the SparkSQL DataTypes?
Thread-Topic: Where are the docs for the SparkSQL DataTypes?
Thread-Index: AQHQFZM0cZn1Q4BZr0ChuB/jL1pg2pyLGyjA
Date: Fri, 12 Dec 2014 00:35:33 +0000
Message-ID: <80833ADD533E324CA05C160E41B63661027B443F@shsmsx102.ccr.corp.intel.com>
References: <CAJc_syLHwp=E2qF=YUbXkhSN7bB=H7gT9Fdr9168Tbh5O=bNpQ@mail.gmail.com>
In-Reply-To: <CAJc_syLHwp=E2qF=YUbXkhSN7bB=H7gT9Fdr9168Tbh5O=bNpQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

UGFydCBvZiBpdCBjYW4gYmUgZm91bmQgYXQ6DQpodHRwczovL2dpdGh1Yi5jb20vYXBhY2hlL3Nw
YXJrL3B1bGwvMzQyOS9maWxlcyNkaWZmLWY4OGMzZTczMWZjYjE3YjEzMjNiNzc4ODA3YzM1YjM4
UjM0DQogDQpTb3JyeSBpdCdzIGEgVE8gQkUgcmV2aWV3ZWQgUFIsIGJ1dCBzdGlsbCBzaG91bGQg
YmUgaW5mb3JtYXRpdmUuDQoNCkNoZW5nIEhhbw0KDQotLS0tLU9yaWdpbmFsIE1lc3NhZ2UtLS0t
LQ0KRnJvbTogQWxlc3NhbmRybyBCYXJldHRhIFttYWlsdG86YWxleGJhcmV0dGFAZ21haWwuY29t
XSANClNlbnQ6IEZyaWRheSwgRGVjZW1iZXIgMTIsIDIwMTQgNjozNyBBTQ0KVG86IE1pY2hhZWwg
QXJtYnJ1c3Q7IGRldkBzcGFyay5hcGFjaGUub3JnDQpTdWJqZWN0OiBXaGVyZSBhcmUgdGhlIGRv
Y3MgZm9yIHRoZSBTcGFya1NRTCBEYXRhVHlwZXM/DQoNCk1pY2hhZWwgJiBvdGhlciBTcGFyayBT
UUwganVua2llcywNCg0KQXMgSSByZWFkIHRocm91Z2ggdGhlIFNwYXJrIEFQSSBkb2NzLCBpbiBw
YXJ0aWN1bGFyIHRob3NlIGZvciB0aGUgb3JnLmFwYWNoZS5zcGFyay5zcWwgcGFja2FnZSwgSSBj
YW4ndCBzZWVtIHRvIGZpbmQgZGV0YWlscyBhYm91dCB0aGUgU2NhbGEgY2xhc3NlcyByZXByZXNl
bnRpbmcgdGhlIHZhcmlvdXMgU3BhcmtTUUwgRGF0YVR5cGVzLCBmb3IgaW5zdGFuY2UgRGVjaW1h
bFR5cGUuIEkgZmluZCBEYXRhVHlwZSBjbGFzc2VzIGluIG9yZy5hcGFjaGUuc3Bhcmsuc3FsLmFw
aS5qYXZhLCBidXQgdGhleSBkb24ndCBzZWVtIHRvIG1hdGNoIHRoZSBzaW1pbGFybHkgbmFtZWQg
c2NhbGEgY2xhc3Nlcy4gRm9yIGluc3RhbmNlLCBEZWNpbWFsVHlwZSBpcyBkb2N1bWVudGVkIGFz
IGhhdmluZyBhIG51bGxhcnkgY29uc3RydWN0b3IsIGJ1dCBpZiBJIHRyeSB0byBjb25zdHJ1Y3Qg
YW4gaW5zdGFuY2Ugb2Ygb3JnLmFwYWNoZS5zcGFyay5zcWwuRGVjaW1hbFR5cGUgd2l0aG91dCBh
bnkgcGFyYW1ldGVycywgdGhlIGNvbXBpbGVyIGNvbXBsYWlucyBhYm91dCB0aGUgbGFjayBvZiBh
IHByZWNpc2lvbkluZm8gZmllbGQsIHdoaWNoIEkgaGF2ZSBkaXNjb3ZlcmVkIGNhbiBiZSBwYXNz
ZWQgaW4gYXMgTm9uZS4gV2hlcmUgaXMgYWxsIHRoaXMgc3R1ZmYgZG9jdW1lbnRlZD8NCg0KQWxl
eA0K
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 01:47:42 2014
Return-Path: <dev-return-10757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C14A1CBC8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 01:47:42 +0000 (UTC)
Received: (qmail 7391 invoked by uid 500); 12 Dec 2014 01:47:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7318 invoked by uid 500); 12 Dec 2014 01:47:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7302 invoked by uid 99); 12 Dec 2014 01:47:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 01:47:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tianyi.asiainfo@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 01:47:34 +0000
Received: by mail-pa0-f47.google.com with SMTP id kq14so6178955pab.6
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 17:47:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:organization:user-agent:mime-version:to:cc
         :subject:content-type;
        bh=MBLJrBSe1PdinzkUaQcT4y2bGohYx2qrBv13khiZiPU=;
        b=u39mUTgsB6T8gt3ExZeF4F5xQLstJSE2dG7wapEUs5cZwrgHOLCFu7Vga5/eQvlpuS
         Sork+kLz1EEasGjo01NlHWcYOrO2bT3y3xldp7xnuDcgqj/zcfy2N7VInvoORACahbzR
         FNZqVqHWNxT/LNo5hHrOgK3utq5dbqHV6tN7EtsHqBNwD4Ps9FloU4wJyKvL/Z67vPW5
         WybRRHb5PyPipEKSD6f6KwhNnGcp4ughUElrUDH0968Hm9iXnwNzaipZge5y01NPO2hR
         KAvSNFB+Ttknjv6GQENLp00MRBIMlFnUx8zjUyI7LRYKRU/r5TS4KDEMIVWcjo82hbd9
         t4Aw==
X-Received: by 10.70.65.105 with SMTP id w9mr22586875pds.58.1418348833904;
        Thu, 11 Dec 2014 17:47:13 -0800 (PST)
Received: from [192.168.99.13] ([199.101.117.17])
        by mx.google.com with ESMTPSA id xq16sm2487092pac.31.2014.12.11.17.47.12
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 11 Dec 2014 17:47:13 -0800 (PST)
Message-ID: <548A491E.2050306@gmail.com>
Date: Fri, 12 Dec 2014 09:47:10 +0800
From: Yi Tian <tianyi.asiainfo@gmail.com>
Organization: Asiainfo.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: dev@spark.apache.org
CC: pwendell@gmail.com
Subject: Is there any document to explain how to build the hive jars for spark?
Content-Type: multipart/alternative;
 boundary="------------060904000102080409030006"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060904000102080409030006
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi, all

We found some bugs in hive-0.12, but we could not wait for hive 
community fixing them.

We want to fix these bugs in our lab and build a new release which could 
be recognized by spark.

As we know, spark depends on a special release of hive, like:

|<dependency>
   <groupId>org.spark-project.hive</groupId>
   <artifactId>hive-metastore</artifactId>
   <version>${hive.version}</version>
</dependency>
|

The different between |org.spark-project.hive| and |org.apache.hive| was 
described by Patrick:

|There are two differences:

1. We publish hive with a shaded protobuf dependency to avoid
conflicts with some Hadoop versions.
2. We publish a proper hive-exec jar that only includes hive packages.
The upstream version of hive-exec bundles a bunch of other random
dependencies in it which makes it really hard for third-party projects
to use it.
|

Is there any document to guide us how to build the hive jars for spark?

Any help would be greatly appreciated.



--------------060904000102080409030006--

From dev-return-10758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 02:23:56 2014
Return-Path: <dev-return-10758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56334CD1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 02:23:56 +0000 (UTC)
Received: (qmail 77271 invoked by uid 500); 12 Dec 2014 02:23:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77192 invoked by uid 500); 12 Dec 2014 02:23:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77176 invoked by uid 99); 12 Dec 2014 02:23:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 02:23:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 02:23:28 +0000
Received: by mail-qa0-f43.google.com with SMTP id bm13so4517797qab.16
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 18:23:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Kap0mYMGNfuUlotcDiGQDKE4SCZtxXfOrfLKUW+yDzI=;
        b=HU2gyanugHj0Gj3UTcQywaYsDE+b3T/uvasnBPPSZjs9COHRf8a+HJhON6Vy7Fjhom
         k4wZS0h70aYeCoU7pdnKt4I9nf/WiqpPB0jtcvtc5L6KCEjNggwGOa2VcL3tc1DpzM4G
         6pfHevhfBbpDJGm2/A1uWGE7eSF5qY8jKuUTKyDs+lHH2/WSx0JWF9l2qsv5KGNDXo1D
         oFuyqjOP4afwQkUPAh/ebGE9ZF6ik8oyChZxL/56EEJpSW+jIL2A08k9NXatM1dcOOyR
         Sw6Rr1jptu/G1/yXzWOAEwkcP9hcxR7zJeAQWE1LBJk3WwCTZjKYygVMwbJzocJ01fCi
         Lj1g==
X-Gm-Message-State: ALoCoQm2QC5r7rpQc15d6n4boWmbXWCietINWe89V5t1Ca+gKf748V7rGizJ7cLPMM2bbO5zDD3i
X-Received: by 10.224.7.197 with SMTP id e5mr26102187qae.71.1418350987127;
 Thu, 11 Dec 2014 18:23:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Thu, 11 Dec 2014 18:22:46 -0800 (PST)
In-Reply-To: <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
 <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com> <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 11 Dec 2014 18:22:46 -0800
Message-ID: <CAPh_B=askooSZon7fxhiHyf3Q84=KqZ1TVYvZBnGVZ8hwnw8kg@mail.gmail.com>
Subject: Re: Tachyon in Spark
To: Andrew Ash <andrew@andrewash.com>
Cc: Jun Feng Liu <liujunf@cn.ibm.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c223f4c146200509fb92fc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c223f4c146200509fb92fc
Content-Type: text/plain; charset=UTF-8

Actually HY emailed me offline about this and this is supported in the
latest version of Tachyon. It is a hard problem to push this into storage;
need to think about how to handle isolation, resource allocation, etc.

https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java

On Thu, Dec 11, 2014 at 3:54 PM, Reynold Xin <rxin@databricks.com> wrote:

> I don't think the lineage thing is even turned on in Tachyon - it was
> mostly a research prototype, so I don't think it'd make sense for us to use
> that.
>
>
> On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> I'm interested in understanding this as well.  One of the main ways
>> Tachyon
>> is supposed to realize performance gains without sacrificing durability is
>> by storing the lineage of data rather than full copies of it (similar to
>> Spark).  But if Spark isn't sending lineage information into Tachyon, then
>> I'm not sure how this isn't a durability concern.
>>
>> On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>>
>> > Dose Spark today really leverage Tachyon linage to process data? It
>> seems
>> > like the application should call createDependency function in TachyonFS
>> > to create a new linage node. But I did not find any place call that in
>> > Spark code. Did I missed anything?
>> >
>> > Best Regards
>> >
>> >
>> > *Jun Feng Liu*
>> > IBM China Systems & Technology Laboratory in Beijing
>> >
>> >   ------------------------------
>> >  [image: 2D barcode - encoded with contact information] *Phone:
>> *86-10-82452683
>> >
>> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>> > [image: IBM]
>> >
>> > BLD 28,ZGC Software Park
>> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
>> > China
>> >
>> >
>> >
>> >
>> >
>>
>
>

--001a11c223f4c146200509fb92fc--

From dev-return-10759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 02:47:11 2014
Return-Path: <dev-return-10759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F25DFCDD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 02:47:10 +0000 (UTC)
Received: (qmail 9718 invoked by uid 500); 12 Dec 2014 02:47:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9621 invoked by uid 500); 12 Dec 2014 02:47:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9554 invoked by uid 99); 12 Dec 2014 02:47:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 02:47:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 02:47:05 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so4689852oib.16
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 18:45:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=fG5YDNQN1kXJqZ3AwmM+9NKvZCuL8birJUSwJbAV9BY=;
        b=Wx8REDfm6vbKhure+pGa2G7pC0llAtDyg2Tk+X0qlMF1q+SlJg8NiJrXgQkG1A+dNb
         7Mc5G3bRWo2Df8VVkVO9VqtAasmPKWSXWIOKpDiBR932SEeziyMce9wnQSq/iXUlo+/D
         MImq1BJ8ro1aCl8cAb3TIQl8K7TJifVaeaFuMkLn2RDK9Ic6WDDMvK09PSMxcM9q77e2
         ABFpfYo7F2tkpQwdhRRhC2CJ+PZBDydYq3Zs2aBejfV1sQ8GNK/u+/KGyJUet6rhc44E
         pBoRPzsI3M8aBOdYif0DpQrOIXWO1VFi3p7xDN/8fjL277g/PEM8FQT0ixnlEsra7qRE
         Jqxg==
X-Received: by 10.182.94.204 with SMTP id de12mr8749730obb.82.1418352359446;
 Thu, 11 Dec 2014 18:45:59 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.71.67 with HTTP; Thu, 11 Dec 2014 18:45:39 -0800 (PST)
In-Reply-To: <80833ADD533E324CA05C160E41B63661027B443F@shsmsx102.ccr.corp.intel.com>
References: <CAJc_syLHwp=E2qF=YUbXkhSN7bB=H7gT9Fdr9168Tbh5O=bNpQ@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027B443F@shsmsx102.ccr.corp.intel.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 11 Dec 2014 18:45:39 -0800
Message-ID: <CAJc_sy+W9mtcabDAt3XnnQHcZT3d30tTmn66V0btc+NDbABThw@mail.gmail.com>
Subject: Re: Where are the docs for the SparkSQL DataTypes?
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f83964f8d202d0509fbe409
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f83964f8d202d0509fbe409
Content-Type: text/plain; charset=UTF-8

Thanks. This is useful.

Alex

On Thu, Dec 11, 2014 at 4:35 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
>
> Part of it can be found at:
>
> https://github.com/apache/spark/pull/3429/files#diff-f88c3e731fcb17b1323b778807c35b38R34
>
> Sorry it's a TO BE reviewed PR, but still should be informative.
>
> Cheng Hao
>
> -----Original Message-----
> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
> Sent: Friday, December 12, 2014 6:37 AM
> To: Michael Armbrust; dev@spark.apache.org
> Subject: Where are the docs for the SparkSQL DataTypes?
>
> Michael & other Spark SQL junkies,
>
> As I read through the Spark API docs, in particular those for the
> org.apache.spark.sql package, I can't seem to find details about the Scala
> classes representing the various SparkSQL DataTypes, for instance
> DecimalType. I find DataType classes in org.apache.spark.sql.api.java, but
> they don't seem to match the similarly named scala classes. For instance,
> DecimalType is documented as having a nullary constructor, but if I try to
> construct an instance of org.apache.spark.sql.DecimalType without any
> parameters, the compiler complains about the lack of a precisionInfo field,
> which I have discovered can be passed in as None. Where is all this stuff
> documented?
>
> Alex
>

--e89a8f83964f8d202d0509fbe409--

From dev-return-10760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 07:03:22 2014
Return-Path: <dev-return-10760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 218B79578
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 07:03:22 +0000 (UTC)
Received: (qmail 81080 invoked by uid 500); 12 Dec 2014 07:03:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81001 invoked by uid 500); 12 Dec 2014 07:03:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80990 invoked by uid 99); 12 Dec 2014 07:03:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 07:03:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 07:03:16 +0000
Received: by mail-la0-f47.google.com with SMTP id hz20so5487410lab.6
        for <dev@spark.apache.org>; Thu, 11 Dec 2014 23:01:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=pwteSDNWqxTEDL/C26k/Tt7nBiXFiWosQa7pGUKpoeU=;
        b=Q3h0IT1TdBAGinTR2Lz3fhGN+hlISF8sHbEUwSVpj7CvJmFQH5nCxBHwA0NUekVZXq
         1PQPhH4VVbz6JrAwKkW+otki6MRJuMIlvFTAbj9b6JC4ZvGeeHqzIsUjrfs2qMzwQh+4
         HBFDQ9CYzqZI01120t1aaVq5xS4H8Nv2pkt5nN02MNxfjvZ7X6yLihGKkarO+7uORn5R
         qZzZO+TWMH88byel18MQ6C3+ZY4rI+U4mHKxD3PkX/Pkw86n7pCNTrVW+7CrJZal8mkQ
         7t4iGDg/h7TGSV453buQBpg/+k9X15oXLIRH2F78GPcySqWJF5SsTxA2HlPesu8H2RSL
         gb8A==
X-Gm-Message-State: ALoCoQk0orXKLBIPMiYkGhPV+F5dtPOCCIOD0IijChA/wCRob/NBr0fGf4pLgW3bQ90rYy3ADH5p
X-Received: by 10.112.129.195 with SMTP id ny3mr14196234lbb.10.1418367664109;
 Thu, 11 Dec 2014 23:01:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Thu, 11 Dec 2014 23:00:43 -0800 (PST)
In-Reply-To: <CAJc_sy+W9mtcabDAt3XnnQHcZT3d30tTmn66V0btc+NDbABThw@mail.gmail.com>
References: <CAJc_syLHwp=E2qF=YUbXkhSN7bB=H7gT9Fdr9168Tbh5O=bNpQ@mail.gmail.com>
 <80833ADD533E324CA05C160E41B63661027B443F@shsmsx102.ccr.corp.intel.com> <CAJc_sy+W9mtcabDAt3XnnQHcZT3d30tTmn66V0btc+NDbABThw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 11 Dec 2014 23:00:43 -0800
Message-ID: <CAAswR-6j-7dNvse4vO-g5y1wkB3TgB8PXjLHBHLWuuFz+3rqqg@mail.gmail.com>
Subject: Re: Where are the docs for the SparkSQL DataTypes?
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "Cheng, Hao" <hao.cheng@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3441c6c7cbb20509ff7492
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3441c6c7cbb20509ff7492
Content-Type: text/plain; charset=UTF-8

I'd suggest looking at the reference in the programming guide:
http://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-datatype-reference


On Thu, Dec 11, 2014 at 6:45 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Thanks. This is useful.
>
> Alex
>
> On Thu, Dec 11, 2014 at 4:35 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
>>
>> Part of it can be found at:
>>
>> https://github.com/apache/spark/pull/3429/files#diff-f88c3e731fcb17b1323b778807c35b38R34
>>
>> Sorry it's a TO BE reviewed PR, but still should be informative.
>>
>> Cheng Hao
>>
>> -----Original Message-----
>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>> Sent: Friday, December 12, 2014 6:37 AM
>> To: Michael Armbrust; dev@spark.apache.org
>> Subject: Where are the docs for the SparkSQL DataTypes?
>>
>> Michael & other Spark SQL junkies,
>>
>> As I read through the Spark API docs, in particular those for the
>> org.apache.spark.sql package, I can't seem to find details about the Scala
>> classes representing the various SparkSQL DataTypes, for instance
>> DecimalType. I find DataType classes in org.apache.spark.sql.api.java, but
>> they don't seem to match the similarly named scala classes. For instance,
>> DecimalType is documented as having a nullary constructor, but if I try to
>> construct an instance of org.apache.spark.sql.DecimalType without any
>> parameters, the compiler complains about the lack of a precisionInfo field,
>> which I have discovered can be passed in as None. Where is all this stuff
>> documented?
>>
>> Alex
>>
>

--047d7b3441c6c7cbb20509ff7492--

From dev-return-10761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 09:21:18 2014
Return-Path: <dev-return-10761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 825B0999E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 09:21:18 +0000 (UTC)
Received: (qmail 31074 invoked by uid 500); 12 Dec 2014 09:21:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31001 invoked by uid 500); 12 Dec 2014 09:21:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30989 invoked by uid 99); 12 Dec 2014 09:21:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 09:21:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mrohad@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 09:21:12 +0000
Received: by mail-wg0-f45.google.com with SMTP id b13so8546629wgh.32
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 01:19:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:cc:content-type;
        bh=kjvjmDgCpjb118MBvNWgnPOE3y48GAXmoua9hld32Ro=;
        b=ID16uEf+2fiXzDN91EC2naDOxNyBl67oUMsezXybTYXFVwvnJtOunYGwkyxR6SGZSt
         30YDemlJWrlkzh7pWFsWL4Snhm/hk73MEJeN8KrDJM4MV7s/FCEloF8RSEsD1ispTc67
         UFtnJ8egu0Dny6gfo8zr7Ql0AHDN8wCfuC5Ty5JxSvhthGUhQfDq9qh7UO90GP06LBdv
         kfiJ9Ou/oknaKon7XiQJBc0iIwch0OeNiWbhAucF9bTi6QvDpyGTDtpAYdj9CZQS+00H
         ZC9Di24pv7XQB+81WZaUv7PIu/OS0fMRiB4vFxxmks2EXvbapOGDgXMWMbSHOBnhnF6Y
         Tj7Q==
X-Received: by 10.180.109.45 with SMTP id hp13mr5781146wib.4.1418375961462;
 Fri, 12 Dec 2014 01:19:21 -0800 (PST)
MIME-Version: 1.0
Received: by 10.180.101.36 with HTTP; Fri, 12 Dec 2014 01:19:01 -0800 (PST)
From: Ohad Assulin <mrohad@gmail.com>
Date: Fri, 12 Dec 2014 11:19:01 +0200
Message-ID: <CAGJg7dZ97Ov1Q4Vg7rOX+0C2CUNO9psKnOsL=kAnc4820QfhyQ@mail.gmail.com>
Subject: JavaScript run-time contribution to Spark
To: dev@spark.apache.org
Cc: erez.yaary@hp.com
Content-Type: multipart/alternative; boundary=e89a8f3bae99574f3b050a0163d3
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3bae99574f3b050a0163d3
Content-Type: text/plain; charset=UTF-8

Hello there.
I am running the Internet Technologies Lab at <a href="
http://new.huji.ac.il/en">HUJI</a>.
A team of my students would like to contribute a JavaScript run-time (
node.js/v8 based) to Spark.

I wonder
(1) What do you think about the necessity of such a project?
(2) Where should we get started? We have only experience as Spark users.
What are the right docs? Who should we talk to? What architecture guidance
we better follow? etc.

Thanks!
Ohad Assulin

--e89a8f3bae99574f3b050a0163d3--

From dev-return-10762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 13:07:00 2014
Return-Path: <dev-return-10762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45DC4F11A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 13:07:00 +0000 (UTC)
Received: (qmail 61184 invoked by uid 500); 12 Dec 2014 13:06:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61110 invoked by uid 500); 12 Dec 2014 13:06:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61098 invoked by uid 99); 12 Dec 2014 13:06:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 13:06:57 +0000
X-ASF-Spam-Status: No, hits=0.2 required=10.0
	tests=HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.142 as permitted sender)
Received: from [202.81.31.142] (HELO e23smtp09.au.ibm.com) (202.81.31.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 13:06:30 +0000
Received: from /spool/local
	by e23smtp09.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Fri, 12 Dec 2014 23:06:26 +1000
Received: from d23dlp01.au.ibm.com (202.81.31.203)
	by e23smtp09.au.ibm.com (202.81.31.206) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 12 Dec 2014 23:06:23 +1000
Received: from d23relay10.au.ibm.com (d23relay10.au.ibm.com [9.190.26.77])
	by d23dlp01.au.ibm.com (Postfix) with ESMTP id 086512CE8067
	for <dev@spark.apache.org>; Sat, 13 Dec 2014 00:06:22 +1100 (EST)
Received: from d23av02.au.ibm.com (d23av02.au.ibm.com [9.190.235.138])
	by d23relay10.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBCD6Le531850500
	for <dev@spark.apache.org>; Sat, 13 Dec 2014 00:06:22 +1100
Received: from d23av02.au.ibm.com (localhost [127.0.0.1])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBCD6Le3022895
	for <dev@spark.apache.org>; Sat, 13 Dec 2014 00:06:21 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBCD6CRJ022681;
	Sat, 13 Dec 2014 00:06:20 +1100
In-Reply-To: <CAPh_B=askooSZon7fxhiHyf3Q84=KqZ1TVYvZBnGVZ8hwnw8kg@mail.gmail.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com> <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com> <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com> <CAPh_B=askooSZon7fxhiHyf3Q84=KqZ1TVYvZBnGVZ8hwnw8kg@mail.gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Andrew Ash <andrew@andrewash.com>,
        "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Re: Tachyon in Spark
X-KeepSent: F275D2D7:2DB740E6-48257DAC:004710E2;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3 September 15, 2011
Message-ID: <OFF275D2D7.2DB740E6-ON48257DAC.004710E2-48257DAC.0047F840@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Fri, 12 Dec 2014 21:06:11 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/12/2014 21:06:20,
	Serialize complete at 12/12/2014 21:06:20
Content-Type: multipart/related; boundary="=_related 0047F83E48257DAC_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121213-0033-0000-0000-000000BA9F52
X-Virus-Checked: Checked by ClamAV on apache.org

--=_related 0047F83E48257DAC_=
Content-Type: multipart/alternative; boundary="=_alternative 0047F83E48257DAC_="


--=_alternative 0047F83E48257DAC_=
Content-Type: text/plain; charset="US-ASCII"

I think the linage is the key feature of tachyon to reproduce the RDD when 
any error happen. Otherwise, there have to be some data replica among 
tachyon nodes to ensure the data redundancy for fault tolerant - I think 
tachyon is avoiding to go to this path. Dose it mean the off-heap solution 
is not ready yet if tachyon linage dose not work right now? 
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Reynold Xin <rxin@databricks.com> 
2014/12/12 10:22

To
Andrew Ash <andrew@andrewash.com>, 
cc
Jun Feng Liu/China/IBM@IBMCN, "dev@spark.apache.org" 
<dev@spark.apache.org>
Subject
Re: Tachyon in Spark






Actually HY emailed me offline about this and this is supported in the
latest version of Tachyon. It is a hard problem to push this into storage;
need to think about how to handle isolation, resource allocation, etc.

https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java


On Thu, Dec 11, 2014 at 3:54 PM, Reynold Xin <rxin@databricks.com> wrote:

> I don't think the lineage thing is even turned on in Tachyon - it was
> mostly a research prototype, so I don't think it'd make sense for us to 
use
> that.
>
>
> On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash <andrew@andrewash.com> 
wrote:
>
>> I'm interested in understanding this as well.  One of the main ways
>> Tachyon
>> is supposed to realize performance gains without sacrificing durability 
is
>> by storing the lineage of data rather than full copies of it (similar 
to
>> Spark).  But if Spark isn't sending lineage information into Tachyon, 
then
>> I'm not sure how this isn't a durability concern.
>>
>> On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com> 
wrote:
>>
>> > Dose Spark today really leverage Tachyon linage to process data? It
>> seems
>> > like the application should call createDependency function in 
TachyonFS
>> > to create a new linage node. But I did not find any place call that 
in
>> > Spark code. Did I missed anything?
>> >
>> > Best Regards
>> >
>> >
>> > *Jun Feng Liu*
>> > IBM China Systems & Technology Laboratory in Beijing
>> >
>> >   ------------------------------
>> >  [image: 2D barcode - encoded with contact information] *Phone:
>> *86-10-82452683
>> >
>> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
>> > [image: IBM]
>> >
>> > BLD 28,ZGC Software Park
>> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
>> > China
>> >
>> >
>> >
>> >
>> >
>>
>
>


--=_alternative 0047F83E48257DAC_=
Content-Type: text/html; charset="GB2312"
Content-Transfer-Encoding: base64
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--=_alternative 0047F83E48257DAC_=--
--=_related 0047F83E48257DAC_=--


From dev-return-10763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 15:27:33 2014
Return-Path: <dev-return-10763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2D99F64F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 15:27:33 +0000 (UTC)
Received: (qmail 49009 invoked by uid 500); 12 Dec 2014 15:27:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48936 invoked by uid 500); 12 Dec 2014 15:27:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48896 invoked by uid 99); 12 Dec 2014 15:27:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 15:27:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 15:27:27 +0000
Received: by mail-lb0-f180.google.com with SMTP id l4so5964640lbv.25
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 07:27:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=z2oEfc0DwtyhP1KFcHDVMrMkCo247vqYqUTiokHzHxI=;
        b=AAyJBJTGF6dk4z/mIYNTt9tlr+37LtycpKg+JVumoMsrsASdDDzzpawjcFyuptSqra
         AklTphG30sWoss2EoBd++7ksiZD8stsNek50OQHMOzVM5X8k7VCNK7KBfnJbJy7uK4hX
         utEMR0nr+sTohtRp3frei5ug4XRyPYs+AOAZBFtrVEyLLfSUWPsRq4URShvO0rmMEZES
         evhVSBQYUHJ0D3laF4wd4ZbgWbnRHgsaqGv7Z4VDTvEmC/j/X6DnA2O/KJW/az+afGtq
         a17XSWiYuAZ5gNk4bOIM1UVIBlNbccyNe+OVlImtmzqeO8DxMjfJsd1MsmCiGKywnN3o
         fvdA==
X-Gm-Message-State: ALoCoQn0zCuI2L7eAB0N2OJNPPPRTeEHWzOBSLP9kjiwa/DYVM5AeQrbNRcz86eVGcoQ6GfGIMzO
X-Received: by 10.112.173.39 with SMTP id bh7mr16285279lbc.53.1418398026379;
 Fri, 12 Dec 2014 07:27:06 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Fri, 12 Dec 2014 07:26:45 -0800 (PST)
In-Reply-To: <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
 <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com> <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 12 Dec 2014 07:26:45 -0800
Message-ID: <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c264148344d7050a06869b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c264148344d7050a06869b
Content-Type: text/plain; charset=UTF-8

reminder:  jenkins is going down NOW.

On Thu, Dec 11, 2014 at 3:08 PM, shane knapp <sknapp@berkeley.edu> wrote:

> here's the plan...  reboots, of course, come last.  :)
>
> pause build queue at 7am, kill off (and eventually retrigger) any
> stragglers at 8am.  then begin maintenance:
>
> all systems:
> * yum update all servers (amp-jekins-master, amp-jenkins-slave-{01..05},
> amp-jenkins-worker-{01..08})
> * reboots
>
> jenkins slaves:
> * install python2.7 (along side 2.6, which would remain the default)
> * install numpy 1.9.1 (currently on 1.4, breaking some spark branch builds)
> * add new slaves to the master, remove old ones (keep them around just in
> case)
>
> there will be no jenkins system or plugin upgrades at this time.  things
> there seems to be working just fine!
>
> i'm expecting to be up and building by 9am at the latest.  i'll update
> this thread w/any new time estimates.
>
> word.
>
> shane, your rained-in devops guy :)
>
> On Wed, Dec 10, 2014 at 11:28 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> reminder -- this is happening friday morning @ 730am!
>>
>> On Mon, Dec 1, 2014 at 5:10 PM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>>> i'll send out a reminder next week, but i wanted to give a heads up:
>>>  i'll be bringing down the entire jenkins infrastructure for reboots and
>>> system updates.
>>>
>>> please let me know if there are any conflicts with this, thanks!
>>>
>>> shane
>>>
>>
>>
>

--001a11c264148344d7050a06869b--

From dev-return-10764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 16:19:14 2014
Return-Path: <dev-return-10764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16F8AF80F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 16:19:14 +0000 (UTC)
Received: (qmail 90198 invoked by uid 500); 12 Dec 2014 16:19:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90101 invoked by uid 500); 12 Dec 2014 16:19:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90082 invoked by uid 99); 12 Dec 2014 16:19:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 16:19:12 +0000
X-ASF-Spam-Status: No, hits=3.4 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of haoyuan.li@gmail.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 16:18:47 +0000
Received: by mail-vc0-f173.google.com with SMTP id kv19so1992293vcb.4
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 08:18:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=LdXQQyyAest1VJ0KHPMVgj0UZgcTUlWfOwlkX6GhfUo=;
        b=a90RW9Xx+XSx2oNJ3XAybgnnULuv+FHwYaZYhEBtGiD996HuTxqklUYI3gJaFTgxcz
         RuyCJvcdRp9QO+c7fHLl4YPG/gvynOZEKyorMwdeF3BI8Lr/EWbW0fkP8DvHI3ZS/11l
         049Ec3aUBXkGpdNbNusfSfSVgtqz0nMLAKrNo7kcmvdQe/6zq1Ke68cfShuQv7zt0WrM
         TB7dBy/yzLBxmJ1nT13zOuUTeZz0c6/qrli2Rrv3O3rj5x43I4E/aq9ELNkG6WKEXt0H
         qEgpaulfpA5mbC5UXRMvP/9/bixMfdD99K+HF2i95+M/nlAZwwCTbA4tbJd5V8SE896Q
         4r/A==
X-Received: by 10.221.3.1 with SMTP id nw1mr10917784vcb.72.1418401080740; Fri,
 12 Dec 2014 08:18:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.246.230 with HTTP; Fri, 12 Dec 2014 08:17:40 -0800 (PST)
In-Reply-To: <OFF275D2D7.2DB740E6-ON48257DAC.004710E2-48257DAC.0047F840@cn.ibm.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com>
 <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com>
 <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com>
 <CAPh_B=askooSZon7fxhiHyf3Q84=KqZ1TVYvZBnGVZ8hwnw8kg@mail.gmail.com> <OFF275D2D7.2DB740E6-ON48257DAC.004710E2-48257DAC.0047F840@cn.ibm.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Fri, 12 Dec 2014 08:17:40 -0800
Message-ID: <CAG2iju1G+HGfzHBUFDi+ohckAPhEfh3ZU00xRLf=R3XD94X-6g@mail.gmail.com>
Subject: Re: Tachyon in Spark
To: Jun Feng Liu <liujunf@cn.ibm.com>
Cc: Reynold Xin <rxin@databricks.com>, Andrew Ash <andrew@andrewash.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a15d6911381050a073ca3
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a15d6911381050a073ca3
Content-Type: text/plain; charset=UTF-8

Junfeng, by off the heap solution, did you mean "rdd.persist(OFF_HEAP)"?
That feature is different from the lineage feature. You can use this
feature (rdd.persist(OFF_HEAP)) now for any Spark version later than 1.0.0
with Tachyon without a problem.

Regarding Reynold's last email, those are good points. Tachyon had provided
this a while ago. We are working on enhancing this feature and the
integration part with Spark.

Thanks,

Haoyuan

On Fri, Dec 12, 2014 at 5:06 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrote:
>
> I think the linage is the key feature of tachyon to reproduce the RDD when
> any error happen. Otherwise, there have to be some data replica among
> tachyon nodes to ensure the data redundancy for fault tolerant - I think
> tachyon is avoiding to go to this path. Dose it mean the off-heap solution
> is not ready yet if tachyon linage dose not work right now?
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone: *86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
>
> 2014/12/12 10:22
>   To
> Andrew Ash <andrew@andrewash.com>,
> cc
> Jun Feng Liu/China/IBM@IBMCN, "dev@spark.apache.org" <dev@spark.apache.org
> >
> Subject
> Re: Tachyon in Spark
>
>
>
>
> Actually HY emailed me offline about this and this is supported in the
> latest version of Tachyon. It is a hard problem to push this into storage;
> need to think about how to handle isolation, resource allocation, etc.
>
>
> https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java
>
> On Thu, Dec 11, 2014 at 3:54 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > I don't think the lineage thing is even turned on in Tachyon - it was
> > mostly a research prototype, so I don't think it'd make sense for us to
> use
> > that.
> >
> >
> > On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> >> I'm interested in understanding this as well.  One of the main ways
> >> Tachyon
> >> is supposed to realize performance gains without sacrificing durability
> is
> >> by storing the lineage of data rather than full copies of it (similar to
> >> Spark).  But if Spark isn't sending lineage information into Tachyon,
> then
> >> I'm not sure how this isn't a durability concern.
> >>
> >> On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com>
> wrote:
> >>
> >> > Dose Spark today really leverage Tachyon linage to process data? It
> >> seems
> >> > like the application should call createDependency function in
> TachyonFS
> >> > to create a new linage node. But I did not find any place call that in
> >> > Spark code. Did I missed anything?
> >> >
> >> > Best Regards
> >> >
> >> >
> >> > *Jun Feng Liu*
> >> > IBM China Systems & Technology Laboratory in Beijing
> >> >
> >> >   ------------------------------
> >> >  [image: 2D barcode - encoded with contact information] *Phone:
> >> *86-10-82452683
> >> >
> >> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> >> > [image: IBM]
> >> >
> >> > BLD 28,ZGC Software Park
> >> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> >> > China
> >> >
> >> >
> >> >
> >> >
> >> >
> >>
> >
> >
>
>

-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--089e013a15d6911381050a073ca3--

From dev-return-10765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 16:50:10 2014
Return-Path: <dev-return-10765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B160BF95B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 16:50:10 +0000 (UTC)
Received: (qmail 81883 invoked by uid 500); 12 Dec 2014 16:50:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81805 invoked by uid 500); 12 Dec 2014 16:50:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81794 invoked by uid 99); 12 Dec 2014 16:50:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 16:50:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.173 as permitted sender)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 16:49:43 +0000
Received: by mail-lb0-f173.google.com with SMTP id z12so6230085lbi.18
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 08:47:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=jq7/+9uvDF3UQZiJ8x8KwjBgiKh2LdjFhmJFnVfA3Qk=;
        b=M52qpKS1opmmPxAUR0ZeVltg4DqxcYEt53ZKbc1iRX1ekikgQMaLOnSK3RX8+gSuPg
         5zSea10IuNVkXX5fyOulXW68zNRyuJXvO1GXshBVPNSnYg4cMAStV9/DT0oOK25EqtJs
         2jw/G/mJfJpH7Z6Ard3qLk2rwusaiEI5qBiiFEc5qoMktEfw4yhcxIaZRh5wIQPACgZ6
         qDH6FakgyoLXqxHHGyLrUkfjfhj1HsVmtqpf2pZ9xDvVpzV3ZKoTWPAQnJlBNPhAAvVd
         GV56mTpcB6TRQpS4pLwAriS2daRAgxq64x3UyI2Xi/w1qM3QljMg77CxLs19xJb6B3tA
         K4iw==
X-Gm-Message-State: ALoCoQk90uGwvHg1BaOVH0f0geHc2bW9fMSkqrnpUXvsh89qb7HhOFJFNHkKJdygkRtYi1zuZ6WC
X-Received: by 10.112.136.69 with SMTP id py5mr3003913lbb.56.1418402846584;
 Fri, 12 Dec 2014 08:47:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Fri, 12 Dec 2014 08:47:06 -0800 (PST)
In-Reply-To: <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
 <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
 <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com> <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 12 Dec 2014 08:47:06 -0800
Message-ID: <CACdU-dQ4wy6V__Vum_eeywqM1MsDxfA3kSvbK=fwoXvivkAE6A@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01183b86d1c70b050a07a515
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01183b86d1c70b050a07a515
Content-Type: text/plain; charset=UTF-8

downtime is extended to 10am PST so that i can finish testing the numpy
upgrade...  besides that, everything looks good and the system updates and
reboots went off w/o a hitch.

shane

On Fri, Dec 12, 2014 at 7:26 AM, shane knapp <sknapp@berkeley.edu> wrote:

> reminder:  jenkins is going down NOW.
>
> On Thu, Dec 11, 2014 at 3:08 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> here's the plan...  reboots, of course, come last.  :)
>>
>> pause build queue at 7am, kill off (and eventually retrigger) any
>> stragglers at 8am.  then begin maintenance:
>>
>> all systems:
>> * yum update all servers (amp-jekins-master, amp-jenkins-slave-{01..05},
>> amp-jenkins-worker-{01..08})
>> * reboots
>>
>> jenkins slaves:
>> * install python2.7 (along side 2.6, which would remain the default)
>> * install numpy 1.9.1 (currently on 1.4, breaking some spark branch
>> builds)
>> * add new slaves to the master, remove old ones (keep them around just in
>> case)
>>
>> there will be no jenkins system or plugin upgrades at this time.  things
>> there seems to be working just fine!
>>
>> i'm expecting to be up and building by 9am at the latest.  i'll update
>> this thread w/any new time estimates.
>>
>> word.
>>
>> shane, your rained-in devops guy :)
>>
>> On Wed, Dec 10, 2014 at 11:28 AM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>>
>>> reminder -- this is happening friday morning @ 730am!
>>>
>>> On Mon, Dec 1, 2014 at 5:10 PM, shane knapp <sknapp@berkeley.edu> wrote:
>>>
>>>> i'll send out a reminder next week, but i wanted to give a heads up:
>>>>  i'll be bringing down the entire jenkins infrastructure for reboots and
>>>> system updates.
>>>>
>>>> please let me know if there are any conflicts with this, thanks!
>>>>
>>>> shane
>>>>
>>>
>>>
>>
>

--089e01183b86d1c70b050a07a515--

From dev-return-10766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 17:49:55 2014
Return-Path: <dev-return-10766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E6ECFD17
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 17:49:55 +0000 (UTC)
Received: (qmail 49541 invoked by uid 500); 12 Dec 2014 17:49:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49470 invoked by uid 500); 12 Dec 2014 17:49:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49451 invoked by uid 99); 12 Dec 2014 17:49:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 17:49:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.174 as permitted sender)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 17:49:27 +0000
Received: by mail-lb0-f174.google.com with SMTP id 10so6418277lbg.33
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 09:47:56 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=gZgV9mNln93dxqTTCGxva6FVFFYplEwt+Q5NaGMj7Iw=;
        b=FUsmHz0DxdcYDseQ+wIHIv51yr45KOVEEsd5rLkI6vsERWO5J0aiDc1rrtNVF/0Wwe
         vWz+RIHtPGoLVVk5S8iDd47M/QUATQcWoEuC+M1yRcj9JXcOQ+QMbmmSRL3bFFOo12AW
         cGMTNT6OsNsmcKObLaxVsWsawMs0OfeOx2xStLQqoRhgZ9PSA1l6FLMV8cZTwR/3Xt14
         DdX1e5jqqSkVQR3ahBG2GfB0MhZMWKeGzxqnS1i7rbG4kSUgzQuokGYMacQSCKUgfD4J
         pfy2DHgPnYmjJPMIzYyh4+D2KeGISt4giZD1SUDjvhpESXc6oJUcx3T2H0rvmEaOMw+X
         LSMQ==
X-Gm-Message-State: ALoCoQlfbn3UxGdUELgG7hmMlthpbHDlr3y5A06FSl98aK8s0J4/vva0cC+wEvdkemJDSmvjW9+O
X-Received: by 10.112.27.133 with SMTP id t5mr16704143lbg.45.1418406476225;
 Fri, 12 Dec 2014 09:47:56 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Fri, 12 Dec 2014 09:47:35 -0800 (PST)
In-Reply-To: <CACdU-dQ4wy6V__Vum_eeywqM1MsDxfA3kSvbK=fwoXvivkAE6A@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
 <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
 <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com>
 <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com> <CACdU-dQ4wy6V__Vum_eeywqM1MsDxfA3kSvbK=fwoXvivkAE6A@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 12 Dec 2014 09:47:35 -0800
Message-ID: <CACdU-dQETzji1n1EA5geJnpbXEvqHNxH-PGn2D-NXSOL63w66w@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133b04a29abd2050a087ea2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133b04a29abd2050a087ea2
Content-Type: text/plain; charset=UTF-8

ok, we're back up w/all new jenkins workers.  i'll be keeping an eye on
these pretty closely today for any build failures caused by the new
systems, and if things look bleak, i'll switch back to the original five.

thanks for your patience!

On Fri, Dec 12, 2014 at 8:47 AM, shane knapp <sknapp@berkeley.edu> wrote:

> downtime is extended to 10am PST so that i can finish testing the numpy
> upgrade...  besides that, everything looks good and the system updates and
> reboots went off w/o a hitch.
>
> shane
>
> On Fri, Dec 12, 2014 at 7:26 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> reminder:  jenkins is going down NOW.
>>
>> On Thu, Dec 11, 2014 at 3:08 PM, shane knapp <sknapp@berkeley.edu> wrote:
>>
>>> here's the plan...  reboots, of course, come last.  :)
>>>
>>> pause build queue at 7am, kill off (and eventually retrigger) any
>>> stragglers at 8am.  then begin maintenance:
>>>
>>> all systems:
>>> * yum update all servers (amp-jekins-master, amp-jenkins-slave-{01..05},
>>> amp-jenkins-worker-{01..08})
>>> * reboots
>>>
>>> jenkins slaves:
>>> * install python2.7 (along side 2.6, which would remain the default)
>>> * install numpy 1.9.1 (currently on 1.4, breaking some spark branch
>>> builds)
>>> * add new slaves to the master, remove old ones (keep them around just
>>> in case)
>>>
>>> there will be no jenkins system or plugin upgrades at this time.  things
>>> there seems to be working just fine!
>>>
>>> i'm expecting to be up and building by 9am at the latest.  i'll update
>>> this thread w/any new time estimates.
>>>
>>> word.
>>>
>>> shane, your rained-in devops guy :)
>>>
>>> On Wed, Dec 10, 2014 at 11:28 AM, shane knapp <sknapp@berkeley.edu>
>>> wrote:
>>>
>>>> reminder -- this is happening friday morning @ 730am!
>>>>
>>>> On Mon, Dec 1, 2014 at 5:10 PM, shane knapp <sknapp@berkeley.edu>
>>>> wrote:
>>>>
>>>>> i'll send out a reminder next week, but i wanted to give a heads up:
>>>>>  i'll be bringing down the entire jenkins infrastructure for reboots and
>>>>> system updates.
>>>>>
>>>>> please let me know if there are any conflicts with this, thanks!
>>>>>
>>>>> shane
>>>>>
>>>>
>>>>
>>>
>>
>

--001a1133b04a29abd2050a087ea2--

From dev-return-10767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 18:33:55 2014
Return-Path: <dev-return-10767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38C9B10009
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 18:33:55 +0000 (UTC)
Received: (qmail 12393 invoked by uid 500); 12 Dec 2014 18:33:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12317 invoked by uid 500); 12 Dec 2014 18:33:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12305 invoked by uid 99); 12 Dec 2014 18:33:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 18:33:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 18:33:49 +0000
Received: by mail-oi0-f54.google.com with SMTP id u20so5569033oif.41
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 10:31:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=bWTVSkHptzXn08lgoo/755Edr+t4PybQB08j65NzkVM=;
        b=YkCJ4rfQcNx0bneblkBxKXcQRZL/C7FwhLRKu0eR4XggLCy6oW05+FVP1kBT4QVW9j
         bKm07ZRIek9LPrh476MzQGeaXdwMka/1ofh02K2HDKFUjkhrKAq52PXXJAjfjx5CKdTr
         TIMsMLE5JWIPtBrbHoTj54ZfXsQNcGAJsAs2zzTPVWG98bJskGR80Ol7sltZXhZ5KaK5
         NbUgBL9PHZEiheEdyhUqMs0oQCh5raez1QlzfynsEWxETjSQBT20djDaQyY5F9rFRf/s
         R5dU0At3C9cNyVEdNkfujWC12EzmnKOQLSIV3bgiXfm7q3s97EqiBE1L923yppiWSdN2
         LvDA==
MIME-Version: 1.0
X-Received: by 10.202.217.137 with SMTP id q131mr10510290oig.100.1418409119072;
 Fri, 12 Dec 2014 10:31:59 -0800 (PST)
Received: by 10.202.178.137 with HTTP; Fri, 12 Dec 2014 10:31:58 -0800 (PST)
In-Reply-To: <D0AC9593.7E33%brennon.york@capitalone.com>
References: <CAOhmDzc+xAw-NKX4+E3sAO9omYMD9e3beQkL8_WtCmVUQUi4qg@mail.gmail.com>
	<CAMAsSd+CQXvPJRmxKKjotHiohOFU8Tfn6S3bnN77HsA+0xvboA@mail.gmail.com>
	<CAOhmDzddsPU6sXtFxwkxtvWj+H0va=3QQHspPqWUp-OvM4v74g@mail.gmail.com>
	<CANeJXFPMCQ54p0L7DNQpeOwmVsgGL=TfZLACE9o=kgA+qko8Dg@mail.gmail.com>
	<CABPQxsuq_-CyknzRNhe6DDeZ_6ddArzV9Z4pLgaodZPL6Uz3pw@mail.gmail.com>
	<D0AC9593.7E33%brennon.york@capitalone.com>
Date: Fri, 12 Dec 2014 10:31:58 -0800
Message-ID: <CABPQxst3VWq3hxwdf0yFGOdgoKVBOaw9HGpjPSYX_K_A6Pv5ag@mail.gmail.com>
Subject: Re: zinc invocation examples
From: Patrick Wendell <pwendell@gmail.com>
To: "York, Brennon" <Brennon.York@capitalone.com>
Cc: Ryan Williams <ryan.blake.williams@gmail.com>, 
	Nicholas Chammas <nicholas.chammas@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey York - I'm sending some feedback off-list, feel free to open a PR as we=
ll.


On Tue, Dec 9, 2014 at 12:05 PM, York, Brennon
<Brennon.York@capitalone.com> wrote:
> Patrick, I=B9ve nearly completed a basic build out for the SPARK-4501 iss=
ue
> (at https://github.com/brennonyork/spark/tree/SPARK-4501) and would be
> great to get your initial read on it. Per this thread I need to add in th=
e
> -scala-home call to zinc, but its close to ready for a PR.
>
> On 12/5/14, 2:10 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>>One thing I created a JIRA for a while back was to have a similar
>>script to "sbt/sbt" that transparently downloads Zinc, Scala, and
>>Maven in a subdirectory of Spark and sets it up correctly. I.e.
>>"build/mvn".
>>
>>Outside of brew for MacOS there aren't good Zinc packages, and it's a
>>pain to figure out how to set it up.
>>
>>https://issues.apache.org/jira/browse/SPARK-4501
>>
>>Prashant Sharma looked at this for a bit but I don't think he's
>>working on it actively any more, so if someone wanted to do this, I'd
>>be extremely grateful.
>>
>>- Patrick
>>
>>On Fri, Dec 5, 2014 at 11:05 AM, Ryan Williams
>><ryan.blake.williams@gmail.com> wrote:
>>> fwiw I've been using `zinc -scala-home $SCALA_HOME -nailed -start`
>>>which:
>>>
>>> - starts a nailgun server as well,
>>> - uses my installed scala 2.{10,11}, as opposed to zinc's default 2.9.2
>>> <https://github.com/typesafehub/zinc#scala>: "If no options are passed
>>>to
>>> locate a version of Scala then Scala 2.9.2 is used by default (which is
>>> bundled with zinc)."
>>>
>>> The latter seems like it might be especially important.
>>>
>>>
>>> On Thu Dec 04 2014 at 4:25:32 PM Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> Oh, derp. I just assumed from looking at all the options that there wa=
s
>>>> something to it. Thanks Sean.
>>>>
>>>> On Thu Dec 04 2014 at 7:47:33 AM Sean Owen <sowen@cloudera.com> wrote:
>>>>
>>>> > You just run it once with "zinc -start" and leave it running as a
>>>> > background process on your build machine. You don't have to do
>>>> > anything for each build.
>>>> >
>>>> > On Wed, Dec 3, 2014 at 3:44 PM, Nicholas Chammas
>>>> > <nicholas.chammas@gmail.com> wrote:
>>>> > > https://github.com/apache/spark/blob/master/docs/
>>>> > building-spark.md#speeding-up-compilation-with-zinc
>>>> > >
>>>> > > Could someone summarize how they invoke zinc as part of a regular
>>>> > > build-test-etc. cycle?
>>>> > >
>>>> > > I'll add it in to the aforelinked page if appropriate.
>>>> > >
>>>> > > Nick
>>>> >
>>>>
>>
>>---------------------------------------------------------------------
>>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>For additional commands, e-mail: dev-help@spark.apache.org
>>
>
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or proprieta=
ry to Capital One and/or its affiliates. The information transmitted herewi=
th is intended only for use by the individual or entity to which it is addr=
essed.  If the reader of this message is not the intended recipient, you ar=
e hereby notified that any review, retransmission, dissemination, distribut=
ion, copying or other use of, or taking of any action in reliance upon this=
 information is strictly prohibited. If you have received this communicatio=
n in error, please contact the sender and delete the material from your com=
puter.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 19:38:37 2014
Return-Path: <dev-return-10768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2213D103BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 19:38:37 +0000 (UTC)
Received: (qmail 2640 invoked by uid 500); 12 Dec 2014 19:38:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2455 invoked by uid 500); 12 Dec 2014 19:38:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2441 invoked by uid 99); 12 Dec 2014 19:38:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 19:38:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 19:38:31 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so6011181qga.28
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 11:37:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:date:message-id
         :subject:from:to:content-type;
        bh=r0gCjHDUzk5oksFjNpOCwGHklOZifutIPF+l2xa4Mlg=;
        b=hmwkoPVtXUW0BSKsb2R2qWmxPzqNsqlsnPX713U9a+Etuk4WawGUKztrbzDGWaltoz
         UzYP6Z/C0HDyiQ2ZCX6PpxQx2nN3MtoFLWoPi3l93+TjoJ55vi9z/VfXaQh6bRDCslRw
         x7UE9hqpF2VixVZU3OzVb/po5VRQS+lMkILAmX15Nxli3zKU4CiI6oaZgNTgW4DMIP12
         ls8By/L8uAcZ2GyQtCf7IzO+UsYDFLemdhfCunx3LLY7Yq1y8p9r4EgJEbuMDvfzyBTo
         gK5UvM0PbbZsIJ3M8xnDp/1cwvWIpS2S9ygG7ArX/9037426n3xgBBprpJmIYwbcly/8
         jJEw==
X-Gm-Message-State: ALoCoQkzZIBMzg6yO3sKRWFjiUmRM3d5tLZEkg+wX92mBIcUXJlBP8eh8mBZ0myg+CJ2J2uZoKKc
MIME-Version: 1.0
X-Received: by 10.140.30.163 with SMTP id d32mr32410260qgd.105.1418413025488;
 Fri, 12 Dec 2014 11:37:05 -0800 (PST)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.com
Received: by 10.229.15.72 with HTTP; Fri, 12 Dec 2014 11:37:05 -0800 (PST)
Date: Fri, 12 Dec 2014 11:37:05 -0800
X-Google-Sender-Auth: VcwwqLhsiAhAovcTa0sd3SzzUW4
Message-ID: <CAEYYnxbo_WQZAz+ES34Zi4hL2nbM9RcnZSFHD6ykNqiRx_e0vw@mail.gmail.com>
Subject: CrossValidator API in new spark.ml package
From: DB Tsai <dbtsai@dbtsai.com>
To: Xiangrui Meng <mengxr@gmail.com>, joseph@databricks.com, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Xiangrui,

It seems that it's stateless so will be hard to implement
regularization path. Any suggestion to extend it? Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 21:43:22 2014
Return-Path: <dev-return-10769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5538B10AB5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 21:43:22 +0000 (UTC)
Received: (qmail 14514 invoked by uid 500); 12 Dec 2014 21:43:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14446 invoked by uid 500); 12 Dec 2014 21:43:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14433 invoked by uid 99); 12 Dec 2014 21:43:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:43:19 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail01.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:43:15 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=3416; q=dns/txt; s=SM2048Apr2013K;
  t=1418420595; x=1418506995;
  h=from:to:date:subject:message-id:mime-version;
  bh=ud9wcWUARR6QpDt7USHmodBBjI9NXvjA7RKxvtAM9OQ=;
  b=YeWO5a2pbvCidHOmPsr4cpkyvqa6Di3Md9eKZnAvKd/r4VZmDRITToS1
   41MEta+dEHzjbTwiGYKCb8wB1sgfQEHTTmtKgfst7b2f5VhlF+NrhPJeG
   JS7L13Un5WPOvYFHUz9rLtiCIKmgpjdUrvVyOtbP1ooAA6YkpEZM3e8VX
   v+EM/2w1+QxbGbT7Iy1sbXSMm1/aerbUDAq1gm8cTa7uJW5ua8hFagGKq
   3+opCeKmTqfOWFxYAZfp8/VuBFc53/l6k1MBCl4mv7wgJI6zTVxGIyMTp
   zU9A8z52bmJgaP/pg2x2G0hxMVMW7WI5g7HUXP63D+ybYuAQwAqZ1p1Kb
   g==;
X-IronPort-AV: E=McAfee;i="5600,1067,7650"; a="184665430"
X-IronPort-AV: E=Sophos;i="5.07,566,1413259200"; 
   d="scan'208,217";a="184665430"
X-HTML-Disclaimer: True
Received: from kdcpexcasht05.cof.ds.capitalone.com ([10.37.194.49])
  by komail01.kdc.capitalone.com with ESMTP; 12 Dec 2014 16:41:53 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 KDCPEXCASHT05.cof.ds.capitalone.com ([10.37.194.49]) with mapi; Fri, 12 Dec
 2014 16:41:53 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: dev <dev@spark.apache.org>
Date: Fri, 12 Dec 2014 16:41:51 -0500
Subject: Newest ML-Lib on Spark 1.1
Thread-Topic: Newest ML-Lib on Spark 1.1
Thread-Index: AdAWVHFcjSfpOwRuRGmlxgnYq2Qqww==
Message-ID: <D0B0A11F.6F63%ilya.ganelin@capitalone.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.3.140616
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_D0B0A11F6F63ilyaganelincapitalonecom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D0B0A11F6F63ilyaganelincapitalonecom_
MIME-Version: 1.0
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all =96 we=92re running CDH 5.2 and would be interested in having the la=
test and greatest ML Lib version on our cluster (with YARN). Could anyone h=
elp me out in terms of figuring out what build profiles to use to get this =
to play well? Will I be able to update ML-Lib independently of updating the=
 rest of spark to 1.2 and beyond? I ran into numerous issues trying to buil=
d 1.2 against CDH=92s Hadoop deployment. Alternately, if anyone has managed=
 to get the trunk successfully built and tested against Cloudera=92s YARN a=
nd Hadoop for 5.2 I would love some help. Thanks!
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.

--_000_D0B0A11F6F63ilyaganelincapitalonecom_--


From dev-return-10770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 21:51:42 2014
Return-Path: <dev-return-10770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA66310AF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 21:51:42 +0000 (UTC)
Received: (qmail 40568 invoked by uid 500); 12 Dec 2014 21:51:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40494 invoked by uid 500); 12 Dec 2014 21:51:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40482 invoked by uid 99); 12 Dec 2014 21:51:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:51:40 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:51:36 +0000
Received: by mail-lb0-f172.google.com with SMTP id u10so6618789lbd.31
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 13:50:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mkfqFLd92Gby1B7Qmefgs7VwsfggOnrgu0NQcTNLw+c=;
        b=FvQccAY1XSbLtwRoI5rMTo/eLaA5C7oJnVGQ/5r7hx6vAvOFey4RM2Ajhd+i250od+
         afPKO8gUxHydwiqre9Ux1llPGRTMan8xkdQjWbaczFZHtgzt4LB2CdVqDUIRBiiLzQC5
         yijfupm021Zfn0aK+uqrjZNfFPAAKmT2W3np9r48D8R2fjSvjW7DS4bMAzrepoYeDCKX
         G+kcxy7zQ1OGuU1kotG4NrXfoTGs0sqbh0kNvjLwgwKZupbu7AnFEjUBowxptkqe/3yn
         WKl9qg1vex+zVsmY9jm2GKETMApy56xm8GNlowJB61LlxYdIbIlONGHzHd1SLFLUWvht
         eDkA==
MIME-Version: 1.0
X-Received: by 10.152.37.168 with SMTP id z8mr5098107laj.63.1418421030292;
 Fri, 12 Dec 2014 13:50:30 -0800 (PST)
Received: by 10.25.212.144 with HTTP; Fri, 12 Dec 2014 13:50:30 -0800 (PST)
In-Reply-To: <D0B0A11F.6F63%ilya.ganelin@capitalone.com>
References: <D0B0A11F.6F63%ilya.ganelin@capitalone.com>
Date: Fri, 12 Dec 2014 13:50:30 -0800
Message-ID: <CA+B-+fyyoOeb4H46Qw1JzY1z-D7twj+8Fi3Bma0erKkkXvuPCA@mail.gmail.com>
Subject: Re: Newest ML-Lib on Spark 1.1
From: Debasish Das <debasish.das83@gmail.com>
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e014941faa713c7050a0be1e0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e014941faa713c7050a0be1e0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

For CDH this works well for me...tested till 5.1...

./make-distribution -Dhadoop.version=3D2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn
-Phive -DskipTests

To build with hive thriftserver support for spark-sql

On Fri, Dec 12, 2014 at 1:41 PM, Ganelin, Ilya <Ilya.Ganelin@capitalone.com=
>
wrote:
>
> Hi all =E2=80=93 we=E2=80=99re running CDH 5.2 and would be interested in=
 having the
> latest and greatest ML Lib version on our cluster (with YARN). Could anyo=
ne
> help me out in terms of figuring out what build profiles to use to get th=
is
> to play well? Will I be able to update ML-Lib independently of updating t=
he
> rest of spark to 1.2 and beyond? I ran into numerous issues trying to bui=
ld
> 1.2 against CDH=E2=80=99s Hadoop deployment. Alternately, if anyone has m=
anaged to
> get the trunk successfully built and tested against Cloudera=E2=80=99s YA=
RN and
> Hadoop for 5.2 I would love some help. Thanks!
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or
> proprietary to Capital One and/or its affiliates. The information
> transmitted herewith is intended only for use by the individual or entity
> to which it is addressed.  If the reader of this message is not the
> intended recipient, you are hereby notified that any review,
> retransmission, dissemination, distribution, copying or other use of, or
> taking of any action in reliance upon this information is strictly
> prohibited. If you have received this communication in error, please
> contact the sender and delete the material from your computer.
>

--089e014941faa713c7050a0be1e0--

From dev-return-10771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 21:54:58 2014
Return-Path: <dev-return-10771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB3C110AFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 21:54:57 +0000 (UTC)
Received: (qmail 45968 invoked by uid 500); 12 Dec 2014 21:54:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45899 invoked by uid 500); 12 Dec 2014 21:54:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45886 invoked by uid 99); 12 Dec 2014 21:54:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:54:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 21:54:30 +0000
Received: by mail-wg0-f42.google.com with SMTP id z12so10266039wgg.15
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 13:54:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=LWdDF6gP7vV8AVvPJ/EvtH/p/qPnIc2I0Yq1bJ8NYa4=;
        b=TzHnmWkFII9j3XVyqEvrjMC49U8cgvGSdfBmEFOMgEfhISudhUfCujzpvIqg1fzT/1
         XH6hH4zLiq9hO0P4vga8mLyZ7xjyQe7yjj//Uwu5n57Dy9d25spKidq3TC4wL0g/Wkp6
         9kD1uayELzdvSk8syPiWUPTe4Bb/7ox0uc4oLS2kkwxN+OTgcKcfZQ4sYymhYjglIjjb
         ++VvF2/Nls9OXdRfFExP75hbcUD0dGOo97IzKp07qBYUC7qWFQO9+LH1ZqDInJYmzMa1
         t8N6xTGKWq4DYBIPwPKPh6QQSqpd/ccZgLO/KAvyVlMtowGATDtMgm4TM+B1EjknTFsr
         pWOg==
X-Gm-Message-State: ALoCoQnM2Z9Kap/h0Lwmq89k+ZZxtfxgIs9vgOp2o97gnM0F/mpvMj8iFI7v0pTVbvvGVbOSyYBV
X-Received: by 10.194.92.176 with SMTP id cn16mr30321723wjb.62.1418421270072;
 Fri, 12 Dec 2014 13:54:30 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 12 Dec 2014 13:54:09 -0800 (PST)
In-Reply-To: <D0B0A11F.6F63%ilya.ganelin@capitalone.com>
References: <D0B0A11F.6F63%ilya.ganelin@capitalone.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 12 Dec 2014 21:54:09 +0000
Message-ID: <CAMAsSdJmSOX3h6xYEeBPDYJ180dm5BN0o4oXyqdZCRG6uLYLmA@mail.gmail.com>
Subject: Re: Newest ML-Lib on Spark 1.1
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Could you specify what problems you're seeing? there is nothing
special about the CDH distribution at all.

The latest and greatest is 1.1, and that is what is in CDH 5.2. You
can certainly compile even master for CDH and get it to work though.

The safest build flags should be "-Phadoop-2.4 -Dhadoop.version=3D2.5.0-cdh=
5.2.1".

5.3 is just around the corner, and includes 1.2, which is also just
around the corner.

On Fri, Dec 12, 2014 at 9:41 PM, Ganelin, Ilya
<Ilya.Ganelin@capitalone.com> wrote:
> Hi all =E2=80=93 we=E2=80=99re running CDH 5.2 and would be interested in=
 having the latest and greatest ML Lib version on our cluster (with YARN). =
Could anyone help me out in terms of figuring out what build profiles to us=
e to get this to play well? Will I be able to update ML-Lib independently o=
f updating the rest of spark to 1.2 and beyond? I ran into numerous issues =
trying to build 1.2 against CDH=E2=80=99s Hadoop deployment. Alternately, i=
f anyone has managed to get the trunk successfully built and tested against=
 Cloudera=E2=80=99s YARN and Hadoop for 5.2 I would love some help. Thanks!
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or proprieta=
ry to Capital One and/or its affiliates. The information transmitted herewi=
th is intended only for use by the individual or entity to which it is addr=
essed.  If the reader of this message is not the intended recipient, you ar=
e hereby notified that any review, retransmission, dissemination, distribut=
ion, copying or other use of, or taking of any action in reliance upon this=
 information is strictly prohibited. If you have received this communicatio=
n in error, please contact the sender and delete the material from your com=
puter.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 22:17:27 2014
Return-Path: <dev-return-10772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74EF010BCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 22:17:27 +0000 (UTC)
Received: (qmail 6294 invoked by uid 500); 12 Dec 2014 22:17:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6207 invoked by uid 500); 12 Dec 2014 22:17:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6192 invoked by uid 99); 12 Dec 2014 22:17:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:17:25 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rcsenkbe@us.ibm.com designates 32.97.110.158 as permitted sender)
Received: from [32.97.110.158] (HELO e37.co.us.ibm.com) (32.97.110.158)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:16:57 +0000
Received: from /spool/local
	by e37.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <rcsenkbe@us.ibm.com>;
	Fri, 12 Dec 2014 15:16:55 -0700
Received: from d03dlp01.boulder.ibm.com (9.17.202.177)
	by e37.co.us.ibm.com (192.168.1.137) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 12 Dec 2014 15:16:53 -0700
Received: from b03cxnp08025.gho.boulder.ibm.com (b03cxnp08025.gho.boulder.ibm.com [9.17.130.17])
	by d03dlp01.boulder.ibm.com (Postfix) with ESMTP id 1797C1FF001E;
	Fri, 12 Dec 2014 15:05:37 -0700 (MST)
Received: from d03av01.boulder.ibm.com (d03av01.boulder.ibm.com [9.17.195.167])
	by b03cxnp08025.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBCMHEV425952508;
	Fri, 12 Dec 2014 15:17:14 -0700
Received: from d03av01.boulder.ibm.com (localhost [127.0.0.1])
	by d03av01.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBCMGoRp010484;
	Fri, 12 Dec 2014 15:16:51 -0700
Received: from d03nm127.boulder.ibm.com (d03nm127.boulder.ibm.com [9.63.33.48])
	by d03av01.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBCMGoJX010421;
	Fri, 12 Dec 2014 15:16:50 -0700
Subject: IBM open-sources Spark Kernel
X-KeepSent: 981E4085:358A8F34-87257DAC:007A38E7;
 type=4; name=$KeepSent
To: dev@spark.apache.org, user@spark.apache.org
X-Mailer: IBM Notes Release 9.0.1 October 14, 2013
Message-ID: <OF981E4085.358A8F34-ON87257DAC.007A38E7-86257DAC.007A63A7@us.ibm.com>
From: Robert C Senkbeil <rcsenkbe@us.ibm.com>
Date: Fri, 12 Dec 2014 16:16:45 -0600
X-MIMETrack: Serialize by Router on D03NM127/03/M/IBM(Release 9.0.1FP1|April  03, 2014) at
 12/12/2014 15:16:46
MIME-Version: 1.0
Content-type: multipart/alternative; 
	Boundary="0__=08BBF73FDFE9BE778f9e8a93df938690918c08BBF73FDFE9BE77"
Content-Disposition: inline
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121222-0025-0000-0000-000006E1CB7C
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=08BBF73FDFE9BE778f9e8a93df938690918c08BBF73FDFE9BE77
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable




We are happy to announce a developer preview of the Spark Kernel which
enables remote applications to dynamically interact with Spark. You can=

think of the Spark Kernel as a remote Spark Shell that uses the IPython=

notebook interface to provide a common entrypoint for any application. =
The
Spark Kernel obviates the need to submit jars using spark-submit, and c=
an
replace the existing Spark Shell.

You can try out the Spark Kernel today by installing it from our github=

repo at=A0https://github.com/ibm-et/spark-kernel. To help you get a dem=
o
environment up and running quickly, the repository also includes a
Dockerfile and a Vagrantfile to build a Spark Kernel container and conn=
ect
to it from an IPython notebook.

We have included a number of documents with the project to help explain=
 it
and provide how-to information:

* A high-level overview of the Spark Kernel and its client library (
https://issues.apache.org/jira/secure/attachment/12683624/Kernel%20Arch=
itecture.pdf
).

* README (https://github.com/ibm-et/spark-kernel/blob/master/README.md)=
 -
building and testing the kernel, and deployment options including build=
ing
the Docker container and packaging the kernel.

* IPython instructions (
https://github.com/ibm-et/spark-kernel/blob/master/docs/IPYTHON.md) -
setting up the development version of IPython and connecting a Spark
Kernel.

* Client library tutorial (
https://github.com/ibm-et/spark-kernel/blob/master/docs/CLIENT.md) -
building and using the client library to connect to a Spark Kernel.

* Magics documentation (
https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md) - th=
e
magics in the kernel and how to write your own.

We think the Spark Kernel will be useful for developing applications fo=
r
Spark, and we are making it available with the intention of improving t=
hese
capabilities within the context of the Spark community (
https://issues.apache.org/jira/browse/SPARK-4605). We will continue to
develop the codebase and welcome your comments and suggestions.


Signed,

Chip Senkbeil
IBM Emerging Technology Software Engineer=

--0__=08BBF73FDFE9BE778f9e8a93df938690918c08BBF73FDFE9BE77--


From dev-return-10773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 22:33:17 2014
Return-Path: <dev-return-10773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1912010C5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 22:33:17 +0000 (UTC)
Received: (qmail 51018 invoked by uid 500); 12 Dec 2014 22:33:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50942 invoked by uid 500); 12 Dec 2014 22:33:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50520 invoked by uid 99); 12 Dec 2014 22:33:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:33:13 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Ilya.Ganelin@capitalone.com designates 204.63.55.164 as permitted sender)
Received: from [204.63.55.164] (HELO pomail03.capitalone.com) (204.63.55.164)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:33:09 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=10540; q=dns/txt; s=SM2048Apr2013P;
  t=1418423589; x=1418509989;
  h=from:to:cc:date:subject:message-id:references:
   in-reply-to:mime-version;
  bh=0ZYfv4rMaGjFZydvFvPATQV8fpBA3ORTNKWZ0e9XT34=;
  b=A+XuxJl4oDdr1e/vIi12QgmIA/llrBIcEwfv/YR24Cx0q1ia6GRD4hYL
   zOfcHTNV0BWsGfEKEI12XyNDiA7nX8aOHNEJiA5pvMOA51VBQ53Fo/S/W
   eMFh/W8EzL2mWUk4MLrMN+859X84wx4V3q3Lf3CHxqeKKzVtIaCXmqzJk
   wcihsUvfY1LiqiRBU79ELSokTe+pyxyuxVtU1wt+A+9vouCuLfepkneqm
   ydZ0bOiukMFsvT3lSV/ou6g2DRp6ce0SUvA9Yhyo2EpJz5tUTHP/ni7nt
   ETQm3vWX01k9N23rfWv4PebN0c2PNasD9fPT1W2LMcr+puLwBKfZiNs9i
   w==;
X-IronPort-AV: E=McAfee;i="5600,1067,7650"; a="62930890"
X-IronPort-AV: E=Sophos;i="5.07,567,1413259200"; 
   d="scan'208,217";a="62930890"
X-HTML-Disclaimer: True
Received: from kdcpexcasht01.cof.ds.capitalone.com ([10.37.194.11])
  by pomail03.kdc.capitalone.com with ESMTP; 12 Dec 2014 17:32:48 -0500
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.46]) by
 KDCPEXCASHT01.cof.ds.capitalone.com ([10.37.194.11]) with mapi; Fri, 12 Dec
 2014 17:32:47 -0500
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: 'Sean Owen' <sowen@cloudera.com>
CC: 'dev' <dev@spark.apache.org>
Date: Fri, 12 Dec 2014 17:32:46 -0500
Subject: RE: Newest ML-Lib on Spark 1.1
Thread-Topic: Newest ML-Lib on Spark 1.1
Thread-Index: AdAWVjXfE5nNP1HQQkG42xqIQoSTuwABVel/
Message-ID: <6D38021C2F77244EA889B37352DA7C49483971CBF8@KDCPEXCMB01.cof.ds.capitalone.com>
References: <CAMAsSdJmSOX3h6xYEeBPDYJ180dm5BN0o4oXyqdZCRG6uLYLmA@mail.gmail.com>
In-Reply-To: <CAMAsSdJmSOX3h6xYEeBPDYJ180dm5BN0o4oXyqdZCRG6uLYLmA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_6D38021C2F77244EA889B37352DA7C49483971CBF8KDCPEXCMB01co_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_6D38021C2F77244EA889B37352DA7C49483971CBF8KDCPEXCMB01co_
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: base64
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--_000_6D38021C2F77244EA889B37352DA7C49483971CBF8KDCPEXCMB01co_--


From dev-return-10774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 22:35:32 2014
Return-Path: <dev-return-10774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75D4610C65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 22:35:32 +0000 (UTC)
Received: (qmail 57367 invoked by uid 500); 12 Dec 2014 22:35:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57294 invoked by uid 500); 12 Dec 2014 22:35:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57279 invoked by uid 99); 12 Dec 2014 22:35:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:35:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:35:05 +0000
Received: by mail-wi0-f169.google.com with SMTP id r20so5801093wiv.2
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 14:35:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=j1q3Vi12kfC1yL/39mfpGuGysmr7AtI/GNIxor2E+HU=;
        b=c3g9/Al9HWK7nzwcdr+oflp7w0cNKUBrxRRctwpx0Nx1VpEFap6SJyzali2Pde8dh2
         COJyM31Fma8dHQnjDjtwgtnCZWsg49C6JsrZ5bBFVCSjA/9aTgSt79jT8O93s7/AOppr
         ne/MutTXcV0O2BLyXSfd+0FTv+L547S/GGRa/vj3w8+49FsmRZWMytBoQrihUNozGd+W
         Vu6kA4THSOqMfVDhE3QNO/4kVWtYmMFq6bCqrDvLbw8SAwC3sBWzK+4tp17kIzpYsdHv
         XYO7ZNxFjOTWPAGuG2IScS1CB9VTwBOSyVL82f3Sk7n+z2Qr+rvOjrHa+JIBczYQPj2i
         HaZQ==
X-Gm-Message-State: ALoCoQkukGcyK9w04rX/zO8aA0v45M9IRLr2O8rNS/kqmPUy7peTC7PLbouEp5rKiP3SJQWOuejJ
X-Received: by 10.194.191.227 with SMTP id hb3mr31746503wjc.79.1418423704448;
 Fri, 12 Dec 2014 14:35:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 12 Dec 2014 14:34:44 -0800 (PST)
In-Reply-To: <6D38021C2F77244EA889B37352DA7C49483971CBF8@KDCPEXCMB01.cof.ds.capitalone.com>
References: <CAMAsSdJmSOX3h6xYEeBPDYJ180dm5BN0o4oXyqdZCRG6uLYLmA@mail.gmail.com>
 <6D38021C2F77244EA889B37352DA7C49483971CBF8@KDCPEXCMB01.cof.ds.capitalone.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 12 Dec 2014 22:34:44 +0000
Message-ID: <CAMAsSdLKK+D0i9JWCATVFPWRujo=2eQQTFoG-ZWPhMNHCNMgaA@mail.gmail.com>
Subject: Re: Newest ML-Lib on Spark 1.1
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

What errors do you see? protobuf errors usually mean you didn't build
for the right version of Hadoop, but if you are using -Phadoop-2.3 or
better -Phadoop-2.4 that should be fine. Yes, a stack trace would be
good. I'm still not sure what error you are seeing.

On Fri, Dec 12, 2014 at 10:32 PM, Ganelin, Ilya
<Ilya.Ganelin@capitalone.com> wrote:
> Hi Sean - I should clarify : I was able to build the master but when runn=
ing
> I hit really random looking protobuf errors (just starting up a spark
> shell), I can try doing a build later today and give the exact stack trac=
e.
>
> I know that 5.2 is running 1.1 but I believe the latest and greatest Ml L=
ib
> is much fresher than the one in 1.1 and specifically includes fixed for A=
LS
> to help it scale better.
>
> I had built with the exact flags you suggested below. After doing so I tr=
ied
> to run the test suite and run a spark she'll without success. Might you h=
ave
> any other suggestions? Thanks!
>
>
>
> Sent with Good (www.good.com)
>
>
>
> -----Original Message-----
> From: Sean Owen [sowen@cloudera.com]
> Sent: Friday, December 12, 2014 04:54 PM Eastern Standard Time
> To: Ganelin, Ilya
> Cc: dev
> Subject: Re: Newest ML-Lib on Spark 1.1
>
> Could you specify what problems you're seeing? there is nothing
> special about the CDH distribution at all.
>
> The latest and greatest is 1.1, and that is what is in CDH 5.2. You
> can certainly compile even master for CDH and get it to work though.
>
> The safest build flags should be "-Phadoop-2.4
> -Dhadoop.version=3D2.5.0-cdh5.2.1".
>
> 5.3 is just around the corner, and includes 1.2, which is also just
> around the corner.
>
> On Fri, Dec 12, 2014 at 9:41 PM, Ganelin, Ilya
> <Ilya.Ganelin@capitalone.com> wrote:
>> Hi all =E2=80=93 we=E2=80=99re running CDH 5.2 and would be interested i=
n having the
>> latest and greatest ML Lib version on our cluster (with YARN). Could any=
one
>> help me out in terms of figuring out what build profiles to use to get t=
his
>> to play well? Will I be able to update ML-Lib independently of updating =
the
>> rest of spark to 1.2 and beyond? I ran into numerous issues trying to bu=
ild
>> 1.2 against CDH=E2=80=99s Hadoop deployment. Alternately, if anyone has =
managed to
>> get the trunk successfully built and tested against Cloudera=E2=80=99s Y=
ARN and
>> Hadoop for 5.2 I would love some help. Thanks!
>> ________________________________________________________
>>
>> The information contained in this e-mail is confidential and/or
>> proprietary to Capital One and/or its affiliates. The information
>> transmitted herewith is intended only for use by the individual or entit=
y to
>> which it is addressed.  If the reader of this message is not the intende=
d
>> recipient, you are hereby notified that any review, retransmission,
>> dissemination, distribution, copying or other use of, or taking of any
>> action in reliance upon this information is strictly prohibited. If you =
have
>> received this communication in error, please contact the sender and dele=
te
>> the material from your computer.
>
>
> ________________________________
>
> The information contained in this e-mail is confidential and/or proprieta=
ry
> to Capital One and/or its affiliates. The information transmitted herewit=
h
> is intended only for use by the individual or entity to which it is
> addressed.  If the reader of this message is not the intended recipient, =
you
> are hereby notified that any review, retransmission, dissemination,
> distribution, copying or other use of, or taking of any action in relianc=
e
> upon this information is strictly prohibited. If you have received this
> communication in error, please contact the sender and delete the material
> from your computer.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 22:38:09 2014
Return-Path: <dev-return-10775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C4BE10C89
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 22:38:09 +0000 (UTC)
Received: (qmail 63158 invoked by uid 500); 12 Dec 2014 22:38:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63076 invoked by uid 500); 12 Dec 2014 22:38:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63059 invoked by uid 99); 12 Dec 2014 22:38:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:38:07 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:37:41 +0000
Received: by mail-la0-f53.google.com with SMTP id gm9so6705768lab.40
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 14:37:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=YTUDUt9qnFj2578t1ZX/I8SdGFpKntjKzPjKKMIMvWE=;
        b=SrokaWWC2tpd5EiXXLAcnDMtQ6xwjj+wrKTVj9D/aWKF+loW0m34SkUB1Ru1OgqEUA
         ut3WCfUcydiAfU3OY864iTwz/EYbj5OmFeSLfpVxs7PafWoJckfm/2ekwv/2O3x6qWlH
         0dNEmxfOQ4Od3qanpoqOyqTdFIIFrP4ed+vQS8GuPREPRiluxQWCQkRd7OoxA4H8ngJg
         8we1M4HeNLyi9ZauuKiAfQIeCVEjCQxQlpuMXNQM0lD++2jTcXR3pTANgIzo5LwQGWa4
         L5RBdLeQPHjGQoQOZiu8fGrJt6BdJI4Q43vBHcATIuVjwzHa2di/fqxM/Kuxi5owYpB/
         jJZw==
MIME-Version: 1.0
X-Received: by 10.152.25.129 with SMTP id c1mr18128335lag.9.1418423860963;
 Fri, 12 Dec 2014 14:37:40 -0800 (PST)
Received: by 10.25.212.144 with HTTP; Fri, 12 Dec 2014 14:37:40 -0800 (PST)
In-Reply-To: <CAMAsSdLKK+D0i9JWCATVFPWRujo=2eQQTFoG-ZWPhMNHCNMgaA@mail.gmail.com>
References: <CAMAsSdJmSOX3h6xYEeBPDYJ180dm5BN0o4oXyqdZCRG6uLYLmA@mail.gmail.com>
	<6D38021C2F77244EA889B37352DA7C49483971CBF8@KDCPEXCMB01.cof.ds.capitalone.com>
	<CAMAsSdLKK+D0i9JWCATVFPWRujo=2eQQTFoG-ZWPhMNHCNMgaA@mail.gmail.com>
Date: Fri, 12 Dec 2014 14:37:40 -0800
Message-ID: <CA+B-+fyWFCHAq=6REAiJymoC5cgMRSMi3Ce5f0WAVFnaE78N+Q@mail.gmail.com>
Subject: Re: Newest ML-Lib on Spark 1.1
From: Debasish Das <debasish.das83@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160bcb45faf97050a0c8aee
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bcb45faf97050a0c8aee
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

protobuf comes from missing -Phadoop2.3

On Fri, Dec 12, 2014 at 2:34 PM, Sean Owen <sowen@cloudera.com> wrote:
>
> What errors do you see? protobuf errors usually mean you didn't build
> for the right version of Hadoop, but if you are using -Phadoop-2.3 or
> better -Phadoop-2.4 that should be fine. Yes, a stack trace would be
> good. I'm still not sure what error you are seeing.
>
> On Fri, Dec 12, 2014 at 10:32 PM, Ganelin, Ilya
> <Ilya.Ganelin@capitalone.com> wrote:
> > Hi Sean - I should clarify : I was able to build the master but when
> running
> > I hit really random looking protobuf errors (just starting up a spark
> > shell), I can try doing a build later today and give the exact stack
> trace.
> >
> > I know that 5.2 is running 1.1 but I believe the latest and greatest Ml
> Lib
> > is much fresher than the one in 1.1 and specifically includes fixed for
> ALS
> > to help it scale better.
> >
> > I had built with the exact flags you suggested below. After doing so I
> tried
> > to run the test suite and run a spark she'll without success. Might you
> have
> > any other suggestions? Thanks!
> >
> >
> >
> > Sent with Good (www.good.com)
> >
> >
> >
> > -----Original Message-----
> > From: Sean Owen [sowen@cloudera.com]
> > Sent: Friday, December 12, 2014 04:54 PM Eastern Standard Time
> > To: Ganelin, Ilya
> > Cc: dev
> > Subject: Re: Newest ML-Lib on Spark 1.1
> >
> > Could you specify what problems you're seeing? there is nothing
> > special about the CDH distribution at all.
> >
> > The latest and greatest is 1.1, and that is what is in CDH 5.2. You
> > can certainly compile even master for CDH and get it to work though.
> >
> > The safest build flags should be "-Phadoop-2.4
> > -Dhadoop.version=3D2.5.0-cdh5.2.1".
> >
> > 5.3 is just around the corner, and includes 1.2, which is also just
> > around the corner.
> >
> > On Fri, Dec 12, 2014 at 9:41 PM, Ganelin, Ilya
> > <Ilya.Ganelin@capitalone.com> wrote:
> >> Hi all =E2=80=93 we=E2=80=99re running CDH 5.2 and would be interested=
 in having the
> >> latest and greatest ML Lib version on our cluster (with YARN). Could
> anyone
> >> help me out in terms of figuring out what build profiles to use to get
> this
> >> to play well? Will I be able to update ML-Lib independently of updatin=
g
> the
> >> rest of spark to 1.2 and beyond? I ran into numerous issues trying to
> build
> >> 1.2 against CDH=E2=80=99s Hadoop deployment. Alternately, if anyone ha=
s managed
> to
> >> get the trunk successfully built and tested against Cloudera=E2=80=99s=
 YARN and
> >> Hadoop for 5.2 I would love some help. Thanks!
> >> ________________________________________________________
> >>
> >> The information contained in this e-mail is confidential and/or
> >> proprietary to Capital One and/or its affiliates. The information
> >> transmitted herewith is intended only for use by the individual or
> entity to
> >> which it is addressed.  If the reader of this message is not the
> intended
> >> recipient, you are hereby notified that any review, retransmission,
> >> dissemination, distribution, copying or other use of, or taking of any
> >> action in reliance upon this information is strictly prohibited. If yo=
u
> have
> >> received this communication in error, please contact the sender and
> delete
> >> the material from your computer.
> >
> >
> > ________________________________
> >
> > The information contained in this e-mail is confidential and/or
> proprietary
> > to Capital One and/or its affiliates. The information transmitted
> herewith
> > is intended only for use by the individual or entity to which it is
> > addressed.  If the reader of this message is not the intended recipient=
,
> you
> > are hereby notified that any review, retransmission, dissemination,
> > distribution, copying or other use of, or taking of any action in
> reliance
> > upon this information is strictly prohibited. If you have received this
> > communication in error, please contact the sender and delete the materi=
al
> > from your computer.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e0160bcb45faf97050a0c8aee--

From dev-return-10776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 22:56:02 2014
Return-Path: <dev-return-10776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2322110D43
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 22:56:02 +0000 (UTC)
Received: (qmail 10321 invoked by uid 500); 12 Dec 2014 22:55:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10245 invoked by uid 500); 12 Dec 2014 22:55:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9751 invoked by uid 99); 12 Dec 2014 22:55:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:55:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 22:55:32 +0000
Received: by mail-qg0-f54.google.com with SMTP id l89so6063676qgf.41
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 14:53:41 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:content-type;
        bh=Az3+MzcKKs2dJzhCZrF50hdzaA6yGMiOtUvWENMHMhQ=;
        b=YJMsC2mmpGVRgy9m1/j0SwJjDzVuhREG98ryGoJ6m45qQeKoACboaFtN6tuFBjPAnz
         h3tvEf5YDaHhv4z1eGGNy+IYoJrp/sVRclIN6jOwnChMNp8PloyzStaffWpKXFq+U3Qd
         RxCEBh/kK6r0emIAh9YvCxHGFaDjI2UviZL1/Ths003s0xwMcf5lHtdFZ6m1K6N0VrxB
         lPNYtBNJ+uJE57yRXqdCk4ycgVb3lEmdCFIPUAvrLOxXfjMsMb9IuNrQQV4SvwYs2Otj
         E5LYCXFDSbSGroM91obbhhIZPX1At4l0hp4vvHH28rupOmhfGCxceyevG1DbC6J4Zl41
         2Bhg==
X-Gm-Message-State: ALoCoQmIBXCHPgIOe2A0Frar2sdfp7z4RIbDOqvkeqHa0ZrQ03J6ZSkZg1SkG2JkOw0Aqo6xA5qx
MIME-Version: 1.0
X-Received: by 10.224.37.5 with SMTP id v5mr35603327qad.25.1418424821468; Fri,
 12 Dec 2014 14:53:41 -0800 (PST)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.com
Received: by 10.229.15.72 with HTTP; Fri, 12 Dec 2014 14:53:41 -0800 (PST)
In-Reply-To: <CAEYYnxbo_WQZAz+ES34Zi4hL2nbM9RcnZSFHD6ykNqiRx_e0vw@mail.gmail.com>
References: <CAEYYnxbo_WQZAz+ES34Zi4hL2nbM9RcnZSFHD6ykNqiRx_e0vw@mail.gmail.com>
Date: Fri, 12 Dec 2014 14:53:41 -0800
X-Google-Sender-Auth: LAP6uM2N0ns2ksJ3Tl2sKkL-SGo
Message-ID: <CAEYYnxaA_Dewu=ugdp4f+PWBhR+O4km=Yqb2XH7_Vvi5pV9pxQ@mail.gmail.com>
Subject: Re: CrossValidator API in new spark.ml package
From: DB Tsai <dbtsai@dbtsai.com>
To: Xiangrui Meng <mengxr@gmail.com>, joseph@databricks.com, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Okay, I got it. In Estimator, fit(dataset: SchemaRDD, paramMaps:
Array[ParamMap]): Seq[M] can be overwritten to implement
regularization path. Correct me if I'm wrong.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Fri, Dec 12, 2014 at 11:37 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
> Hi Xiangrui,
>
> It seems that it's stateless so will be hard to implement
> regularization path. Any suggestion to extend it? Thanks.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 12 23:30:25 2014
Return-Path: <dev-return-10777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1443410E5C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Dec 2014 23:30:25 +0000 (UTC)
Received: (qmail 88600 invoked by uid 500); 12 Dec 2014 23:30:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88518 invoked by uid 500); 12 Dec 2014 23:30:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88507 invoked by uid 99); 12 Dec 2014 23:30:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 23:30:23 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS,TVD_FW_GRAPHIC_NAME_MID
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rcsenkbe@us.ibm.com designates 32.97.110.153 as permitted sender)
Received: from [32.97.110.153] (HELO e35.co.us.ibm.com) (32.97.110.153)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Dec 2014 23:29:54 +0000
Received: from /spool/local
	by e35.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <rcsenkbe@us.ibm.com>;
	Fri, 12 Dec 2014 16:29:23 -0700
Received: from d03dlp02.boulder.ibm.com (9.17.202.178)
	by e35.co.us.ibm.com (192.168.1.135) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Fri, 12 Dec 2014 16:29:21 -0700
Received: from b03cxnp08028.gho.boulder.ibm.com (b03cxnp08028.gho.boulder.ibm.com [9.17.130.20])
	by d03dlp02.boulder.ibm.com (Postfix) with ESMTP id A3CB03E4003D;
	Fri, 12 Dec 2014 16:29:20 -0700 (MST)
Received: from d03av02.boulder.ibm.com (d03av02.boulder.ibm.com [9.17.195.168])
	by b03cxnp08028.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBCNTiBh33816798;
	Fri, 12 Dec 2014 16:29:44 -0700
Received: from d03av02.boulder.ibm.com (localhost [127.0.0.1])
	by d03av02.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBCNTKJm018493;
	Fri, 12 Dec 2014 16:29:20 -0700
Received: from d03nm127.boulder.ibm.com (d03nm127.boulder.ibm.com [9.63.33.48])
	by d03av02.boulder.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBCNTKLq018481;
	Fri, 12 Dec 2014 16:29:20 -0700
In-Reply-To: <CAN+vPU=-aV=t-y_ptzycrAZtcs17mp6Nw=0PUMEeNkcv4waRwQ@mail.gmail.com>
References: <OF981E4085.358A8F34-ON87257DAC.007A38E7-86257DAC.007A63A7@us.ibm.com> <CAN+vPU=-aV=t-y_ptzycrAZtcs17mp6Nw=0PUMEeNkcv4waRwQ@mail.gmail.com>
Subject: Re: IBM open-sources Spark Kernel
X-KeepSent: 6057ECD0:27933B07-87257DAC:007B2F0A;
 type=4; name=$KeepSent
To: Sam Bessalah <samkiller.oss@gmail.com>
Cc: user@spark.apache.org, dev <dev@spark.apache.org>
X-Mailer: IBM Notes Release 9.0.1 October 14, 2013
Message-ID: <OF6057ECD0.27933B07-ON87257DAC.007B2F0A-86257DAC.008106A7@us.ibm.com>
From: Robert C Senkbeil <rcsenkbe@us.ibm.com>
Date: Fri, 12 Dec 2014 17:29:15 -0600
X-MIMETrack: Serialize by Router on D03NM127/03/M/IBM(Release 9.0.1FP1|April  03, 2014) at
 12/12/2014 16:29:17
MIME-Version: 1.0
Content-type: multipart/related; 
	Boundary="0__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A"
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121223-0013-0000-0000-000007066DC0
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A
Content-type: multipart/alternative; 
	Boundary="1__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A"

--1__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable


Hi Sam,

We developed the Spark Kernel with a focus on the newest version of the=

IPython message protocol (5.0) for the upcoming IPython 3.0 release.

We are building around Apache Spark's REPL, which is used in the curren=
t
Spark Shell implementation.

The Spark Kernel was designed to be extensible through magics (
https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md),
providing functionality that might be needed outside the Scala interpre=
ter.

Finally, a big part of our focus is on application development. Because=
 of
this, we are providing a client library for applications to connect to =
the
Spark Kernel without needing to implement the ZeroMQ protocol.

Signed,
Chip Senkbeil



From:	Sam Bessalah <samkiller.oss@gmail.com>
To:	Robert C Senkbeil/Austin/IBM@IBMUS
Date:	12/12/2014 04:20 PM
Subject:	Re: IBM open-sources Spark Kernel



Wow. Thanks. Can't wait to try this out.
Great job.
How Is it different from Iscala or Ispark?


On Dec 12, 2014 11:17 PM, "Robert C Senkbeil" <rcsenkbe@us.ibm.com> wro=
te:



  We are happy to announce a developer preview of the Spark Kernel whic=
h
  enables remote applications to dynamically interact with Spark. You c=
an
  think of the Spark Kernel as a remote Spark Shell that uses the IPyth=
on
  notebook interface to provide a common entrypoint for any application=
.
  The
  Spark Kernel obviates the need to submit jars using spark-submit, and=
 can
  replace the existing Spark Shell.

  You can try out the Spark Kernel today by installing it from our gith=
ub
  repo at=A0https://github.com/ibm-et/spark-kernel. To help you get a d=
emo
  environment up and running quickly, the repository also includes a
  Dockerfile and a Vagrantfile to build a Spark Kernel container and
  connect
  to it from an IPython notebook.

  We have included a number of documents with the project to help expla=
in
  it
  and provide how-to information:

  * A high-level overview of the Spark Kernel and its client library (
  https://issues.apache.org/jira/secure/attachment/12683624/Kernel%20Ar=
chitecture.pdf

  ).

  * README (https://github.com/ibm-et/spark-kernel/blob/master/README.m=
d) -
  building and testing the kernel, and deployment options including
  building
  the Docker container and packaging the kernel.

  * IPython instructions (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/IPYTHON.md) -=

  setting up the development version of IPython and connecting a Spark
  Kernel.

  * Client library tutorial (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/CLIENT.md) -
  building and using the client library to connect to a Spark Kernel.

  * Magics documentation (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md) - =
the
  magics in the kernel and how to write your own.

  We think the Spark Kernel will be useful for developing applications =
for
  Spark, and we are making it available with the intention of improving=

  these
  capabilities within the context of the Spark community (
  https://issues.apache.org/jira/browse/SPARK-4605). We will continue t=
o
  develop the codebase and welcome your comments and suggestions.


  Signed,

  Chip Senkbeil
  IBM Emerging Technology Software Engineer=

--1__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A
Content-type: text/html; charset=ISO-8859-1
Content-Disposition: inline
Content-transfer-encoding: quoted-printable

<html><body>
<p><font size=3D"2" face=3D"sans-serif">Hi Sam,</font><br>
<br>
<font size=3D"2" face=3D"sans-serif">We developed the Spark Kernel with=
 a focus on the newest version of the IPython message protocol (5.0) fo=
r the upcoming IPython 3.0 release.</font><br>
<br>
<font size=3D"2" face=3D"sans-serif">We are building around Apache Spar=
k's REPL, which is used in the current Spark Shell implementation.</fon=
t><br>
<br>
<font size=3D"2" face=3D"sans-serif">The Spark Kernel was designed to b=
e extensible through magics (</font><a href=3D"https://github.com/ibm-e=
t/spark-kernel/blob/master/docs/MAGICS.md"><font size=3D"2" face=3D"san=
s-serif">https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS=
.md</font></a><font size=3D"2" face=3D"sans-serif">), providing functio=
nality that might be needed outside the Scala interpreter.</font><br>
<br>
<font size=3D"2" face=3D"sans-serif">Finally, a big part of our focus i=
s on application development. Because of this, we are providing a clien=
t library for applications to connect to the Spark Kernel without needi=
ng to implement the ZeroMQ protocol. </font><br>
<br>
<font size=3D"2" face=3D"sans-serif">Signed,</font><br>
<font size=3D"2" face=3D"sans-serif">Chip Senkbeil</font><br>
<br>
<img width=3D"16" height=3D"16" src=3D"cid:1__=3D08BBF73FDFE8A99A8f9e8a=
93df938@us.ibm.com" border=3D"0" alt=3D"Inactive hide details for Sam B=
essalah ---12/12/2014 04:20:41 PM---Wow. Thanks. Can't wait to try this=
 out. Great job."><font size=3D"2" color=3D"#424282" face=3D"sans-serif=
">Sam Bessalah ---12/12/2014 04:20:41 PM---Wow. Thanks. Can't wait to t=
ry this out. Great job.</font><br>
<br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">From:	</font><fo=
nt size=3D"1" face=3D"sans-serif">Sam Bessalah &lt;samkiller.oss@gmail.=
com&gt;</font><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">To:	</font><font=
 size=3D"1" face=3D"sans-serif">Robert C Senkbeil/Austin/IBM@IBMUS</fon=
t><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">Date:	</font><fo=
nt size=3D"1" face=3D"sans-serif">12/12/2014 04:20 PM</font><br>
<font size=3D"1" color=3D"#5F5F5F" face=3D"sans-serif">Subject:	</font>=
<font size=3D"1" face=3D"sans-serif">Re: IBM open-sources Spark Kernel<=
/font><br>
<hr width=3D"100%" size=3D"2" align=3D"left" noshade style=3D"color:#80=
91A5; "><br>
<br>
<br>
<font size=3D"3" face=3D"serif">Wow. Thanks. Can't wait to try this out=
. <br>
Great job.<br>
How Is it different from Iscala or Ispark?</font>
<p><font size=3D"3" face=3D"serif">On Dec 12, 2014 11:17 PM, &quot;Robe=
rt C Senkbeil&quot; &lt;</font><a href=3D"mailto:rcsenkbe@us.ibm.com"><=
font size=3D"3" color=3D"#0000FF" face=3D"serif"><u>rcsenkbe@us.ibm.com=
</u></font></a><font size=3D"3" face=3D"serif">&gt; wrote:</font>
<ul style=3D"padding-left: 9pt"><font size=3D"3" face=3D"serif"><br>
<br>
<br>
We are happy to announce a developer preview of the Spark Kernel which<=
br>
enables remote applications to dynamically interact with Spark. You can=
<br>
think of the Spark Kernel as a remote Spark Shell that uses the IPython=
<br>
notebook interface to provide a common entrypoint for any application. =
The<br>
Spark Kernel obviates the need to submit jars using spark-submit, and c=
an<br>
replace the existing Spark Shell.<br>
<br>
You can try out the Spark Kernel today by installing it from our github=
<br>
repo at=A0</font><a href=3D"https://github.com/ibm-et/spark-kernel" tar=
get=3D"_blank"><font size=3D"3" color=3D"#0000FF" face=3D"serif"><u>htt=
ps://github.com/ibm-et/spark-kernel</u></font></a><font size=3D"3" face=
=3D"serif">. To help you get a demo<br>
environment up and running quickly, the repository also includes a<br>
Dockerfile and a Vagrantfile to build a Spark Kernel container and conn=
ect<br>
to it from an IPython notebook.<br>
<br>
We have included a number of documents with the project to help explain=
 it<br>
and provide how-to information:<br>
<br>
* A high-level overview of the Spark Kernel and its client library (</f=
ont><font size=3D"3" color=3D"#0000FF" face=3D"serif"><u><br>
</u></font><a href=3D"https://issues.apache.org/jira/secure/attachment/=
12683624/Kernel%20Architecture.pdf" target=3D"_blank"><font size=3D"3" =
color=3D"#0000FF" face=3D"serif"><u>https://issues.apache.org/jira/secu=
re/attachment/12683624/Kernel%20Architecture.pdf</u></font></a><font si=
ze=3D"3" face=3D"serif"><br>
).<br>
<br>
* README (</font><a href=3D"https://github.com/ibm-et/spark-kernel/blob=
/master/README.md" target=3D"_blank"><font size=3D"3" color=3D"#0000FF"=
 face=3D"serif"><u>https://github.com/ibm-et/spark-kernel/blob/master/R=
EADME.md</u></font></a><font size=3D"3" face=3D"serif">) -<br>
building and testing the kernel, and deployment options including build=
ing<br>
the Docker container and packaging the kernel.<br>
<br>
* IPython instructions (</font><font size=3D"3" color=3D"#0000FF" face=3D=
"serif"><u><br>
</u></font><a href=3D"https://github.com/ibm-et/spark-kernel/blob/maste=
r/docs/IPYTHON.md" target=3D"_blank"><font size=3D"3" color=3D"#0000FF"=
 face=3D"serif"><u>https://github.com/ibm-et/spark-kernel/blob/master/d=
ocs/IPYTHON.md</u></font></a><font size=3D"3" face=3D"serif">) -<br>
setting up the development version of IPython and connecting a Spark<br=
>
Kernel.<br>
<br>
* Client library tutorial (</font><font size=3D"3" color=3D"#0000FF" fa=
ce=3D"serif"><u><br>
</u></font><a href=3D"https://github.com/ibm-et/spark-kernel/blob/maste=
r/docs/CLIENT.md" target=3D"_blank"><font size=3D"3" color=3D"#0000FF" =
face=3D"serif"><u>https://github.com/ibm-et/spark-kernel/blob/master/do=
cs/CLIENT.md</u></font></a><font size=3D"3" face=3D"serif">) -<br>
building and using the client library to connect to a Spark Kernel.<br>=

<br>
* Magics documentation (</font><font size=3D"3" color=3D"#0000FF" face=3D=
"serif"><u><br>
</u></font><a href=3D"https://github.com/ibm-et/spark-kernel/blob/maste=
r/docs/MAGICS.md" target=3D"_blank"><font size=3D"3" color=3D"#0000FF" =
face=3D"serif"><u>https://github.com/ibm-et/spark-kernel/blob/master/do=
cs/MAGICS.md</u></font></a><font size=3D"3" face=3D"serif">) - the<br>
magics in the kernel and how to write your own.<br>
<br>
We think the Spark Kernel will be useful for developing applications fo=
r<br>
Spark, and we are making it available with the intention of improving t=
hese<br>
capabilities within the context of the Spark community (</font><font si=
ze=3D"3" color=3D"#0000FF" face=3D"serif"><u><br>
</u></font><a href=3D"https://issues.apache.org/jira/browse/SPARK-4605"=
 target=3D"_blank"><font size=3D"3" color=3D"#0000FF" face=3D"serif"><u=
>https://issues.apache.org/jira/browse/SPARK-4605</u></font></a><font s=
ize=3D"3" face=3D"serif">). We will continue to<br>
develop the codebase and welcome your comments and suggestions.<br>
<br>
<br>
Signed,<br>
<br>
Chip Senkbeil<br>
IBM Emerging Technology Software Engineer</font><br>
</ul>
</body></html>=


--1__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A--


--0__=08BBF73FDFE8A99A8f9e8a93df938690918c08BBF73FDFE8A99A--


From dev-return-10778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 02:16:36 2014
Return-Path: <dev-return-10778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 143CF948D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 02:16:36 +0000 (UTC)
Received: (qmail 38226 invoked by uid 500); 13 Dec 2014 02:16:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38028 invoked by uid 500); 13 Dec 2014 02:16:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38000 invoked by uid 99); 13 Dec 2014 02:16:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 02:16:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lochanac@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 02:16:06 +0000
Received: by mail-pa0-f44.google.com with SMTP id et14so8387188pad.3
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 18:16:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject
         :content-type:content-transfer-encoding;
        bh=q6V1q4M338WVgf3SxeGHmLhtRWExMkepAe2IkqRTMU4=;
        b=AbK9jcxwfpS14ns+sUdbtVhb1Sc1jz9LRLFs9VZse5jyuP90B9MEkv4fw45dMHQBoq
         sV94HFiUdFnaRVWW254yUE+x1JyBLuGmrMNNx2n5U4U0uba/FDjoG8HZqF72SZWZDdFe
         ohM4taoXG0JWlOxnNczf02sN6eZPEcf0cINA9lMQJANIaoILMRmbbfCmJGpm+ji7MPqn
         H69+tt5ORy7oBVBboZG32dvrQNScvxcxTR/B/FMhLpaxoPdwtfd5zFZYoHjvuIijqOlc
         jpyNzDQhJceoFTWUuQzy9k8otVaoVGVpMoJ5mzDoqPhiaMuo97vdrxEtPBOIobrS0PRp
         wZtQ==
X-Received: by 10.67.3.165 with SMTP id bx5mr31637914pad.59.1418436964908;
        Fri, 12 Dec 2014 18:16:04 -0800 (PST)
Received: from Lochanas-MacBook-Pro.local ([112.134.133.193])
        by mx.google.com with ESMTPSA id ae4sm2711399pad.16.2014.12.12.18.16.03
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 12 Dec 2014 18:16:04 -0800 (PST)
Message-ID: <548BA161.6090207@gmail.com>
Date: Sat, 13 Dec 2014 07:46:01 +0530
From: Lochana Menikarachchi <lochanac@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: one hot encoding
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Do we have one-hot encoding in spark MLLib 1.1.1 or 1.2.0 ? It wasn't 
available in 1.1.0.
Thanks.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 04:01:57 2014
Return-Path: <dev-return-10779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71F0496CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 04:01:57 +0000 (UTC)
Received: (qmail 17604 invoked by uid 500); 13 Dec 2014 04:01:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17536 invoked by uid 500); 13 Dec 2014 04:01:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17524 invoked by uid 99); 13 Dec 2014 04:01:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 04:01:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 04:01:29 +0000
Received: by mail-oi0-f43.google.com with SMTP id a3so6098229oib.2
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 20:00:43 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=uIQCUfz4ehPblYJUO/FBoMyb2vcQuxwkM/gBJ0aGhZM=;
        b=jGUabzgSMeOGknDfU7NCG7w/0WwIlNVeGeogRSESDfCEtNr7FjYpQakQks4QhPjkcb
         866zs/1JRWsgeBl0COLJc9u12ziIwz4s+1JFJ2H4wazXNG9rm+JXpSja8A1gb45roreu
         5Kd8Svaydwg6nJLRXan4JTdGTYfH4Jbd/wYnPDX9SQGWapxlmd/UDuyuuIupdFucO19W
         qGCC5LuJMKYEbg+u06F4fqKMLyoN66tNRD8zalwEnoODt+oonHO0a+txxFY0+F0deqLo
         k1tM1HNLC6uaFUuUQo+48DoL/TP12M7PSTbVJLPrpiOZphL0AHZEKrjboaIR3vH9zhx7
         e7vg==
X-Received: by 10.182.131.233 with SMTP id op9mr8168102obb.78.1418443243520;
        Fri, 12 Dec 2014 20:00:43 -0800 (PST)
Received: from joshs-mbp.att.net ([2602:306:cdd1:b10:7ca9:3a1a:59f2:6371])
        by mx.google.com with ESMTPSA id dd17sm1543271obb.18.2014.12.12.20.00.40
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 12 Dec 2014 20:00:42 -0800 (PST)
Date: Fri, 12 Dec 2014 20:00:38 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>, Reynold Xin
 <rxin@databricks.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, Patrick
 Wendell <pwendell@gmail.com>
Message-ID: <etPan.548bb9e6.6b8b4567.e35b@joshs-mbp.att.net>
In-Reply-To: <CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
 <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
 <CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="548bb9e6_327b23c6_e35b"
X-Virus-Checked: Checked by ClamAV on apache.org

--548bb9e6_327b23c6_e35b
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

+1. =C2=A0Tested using spark-perf=C2=A0and the Spark EC2 scripts. =C2=A0I=
 didn=E2=80=99t notice any performance regressions that could not be attr=
ibuted to changes of default configurations. =C2=A0To be more specific, w=
hen running Spark 1.2.0 with the Spark 1.1.0 settings of spark.shuffle.ma=
nager=3Dhash and spark.shuffle.blockTransferService=3Dnio, there was no p=
erformance regression and, in fact, there were significant performance im=
provements for some workloads.

In Spark 1.2.0, the new default settings are spark.shuffle.manager=3Dsort=
 and spark.shuffle.blockTransferService=3Dnetty. =C2=A0With these new set=
tings, I noticed a performance regression in the scala-sort-by-key-int sp=
ark-perf test. =C2=A0However, Spark 1.1.0 and 1.1.1 exhibit a similar per=
formance regression for that same test when run with spark.shuffle.manage=
r=3Dsort, so this regression seems explainable by the change of defaults.=
 =C2=A0Besides this, most of the other tests ran at the same speeds or fa=
ster with the new 1.2.0 defaults. =C2=A0Also, keep in mind that this is a=
 somewhat artificial micro benchmark; I have heard anecdotal reports from=
 many users that their real workloads have run faster with 1.2.0.

Based on these results, I=E2=80=99m comfortable giving a +1 on 1.2.0 RC2.=


- Josh

On December 11, 2014 at 9:52:39 AM, Sandy Ryza (sandy.ryza=40cloudera.com=
) wrote:

+1 (non-binding). Tested on Ubuntu against YARN. =20

On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin <rxin=40databricks.com> wrot=
e: =20

> +1 =20
> =20
> Tested on OS X. =20
> =20
> On Wednesday, December 10, 2014, Patrick Wendell <pwendell=40gmail.com>=
 =20
> wrote: =20
> =20
> > Please vote on releasing the following candidate as Apache Spark vers=
ion =20
> > 1.2.0=21 =20
> > =20
> > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2): =20
> > =20
> > =20
> https://git-wip-us.apache.org/repos/asf=3Fp=3Dspark.git;a=3Dcommit;h=3D=
a428c446e23e628b746e0626cc02b7b3cadf588e =20
> > =20
> > The release files, including signatures, digests, etc. can be found a=
t: =20
> > http://people.apache.org/=7Epwendell/spark-1.2.0-rc2/ =20
> > =20
> > Release artifacts are signed with the following key: =20
> > https://people.apache.org/keys/committer/pwendell.asc =20
> > =20
> > The staging repository for this release can be found at: =20
> > https://repository.apache.org/content/repositories/orgapachespark-105=
5/ =20
> > =20
> > The documentation corresponding to this release can be found at: =20
> > http://people.apache.org/=7Epwendell/spark-1.2.0-rc2-docs/ =20
> > =20
> > Please vote on releasing this package as Apache Spark 1.2.0=21 =20
> > =20
> > The vote is open until Saturday, December 13, at 21:00 UTC and passes=
 =20
> > if a majority of at least 3 +1 PMC votes are cast. =20
> > =20
> > =5B =5D +1 Release this package as Apache Spark 1.2.0 =20
> > =5B =5D -1 Do not release this package because ... =20
> > =20
> > To learn more about Apache Spark, please see =20
> > http://spark.apache.org/ =20
> > =20
> > =3D=3D What justifies a -1 vote for this release=3F =3D=3D =20
> > This vote is happening relatively late into the QA period, so =20
> > -1 votes should only occur for significant regressions from =20
> > 1.0.2. Bugs already present in 1.1.X, minor =20
> > regressions, or bugs related to new features will not block this =20
> > release. =20
> > =20
> > =3D=3D What default changes should I be aware of=3F =3D=3D =20
> > 1. The default value of =22spark.shuffle.blockTransferService=22 has =
been =20
> > changed to =22netty=22 =20
> > --> Old behavior can be restored by switching to =22nio=22 =20
> > =20
> > 2. The default value of =22spark.shuffle.manager=22 has been changed =
to =20
> =22sort=22. =20
> > --> Old behavior can be restored by setting =22spark.shuffle.manager=22=
 to =20
> > =22hash=22. =20
> > =20
> > =3D=3D How does this differ from RC1 =3D=3D =20
> > This has fixes for a handful of issues identified - some of the =20
> > notable fixes are: =20
> > =20
> > =5BCore=5D =20
> > SPARK-4498: Standalone Master can fail to recognize completed/failed =
=20
> > applications =20
> > =20
> > =5BSQL=5D =20
> > SPARK-4552: Query for empty parquet table in spark sql hive get =20
> > IllegalArgumentException =20
> > SPARK-4753: Parquet2 does not prune based on OR filters on partition =
=20
> > columns =20
> > SPARK-4761: With JDBC server, set Kryo as default serializer and =20
> > disable reference tracking =20
> > SPARK-4785: When called with arguments referring column fields, PMOD =
=20
> > throws NPE =20
> > =20
> > - Patrick =20
> > =20
> > ---------------------------------------------------------------------=
 =20
> > To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org <javascrip=
t:;> =20
> > =46or additional commands, e-mail: dev-help=40spark.apache.org =20
> <javascript:;> =20
> > =20
> > =20
> =20

--548bb9e6_327b23c6_e35b--


From dev-return-10780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 04:57:16 2014
Return-Path: <dev-return-10780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9031397B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 04:57:16 +0000 (UTC)
Received: (qmail 54843 invoked by uid 500); 13 Dec 2014 04:57:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54773 invoked by uid 500); 13 Dec 2014 04:57:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54759 invoked by uid 99); 13 Dec 2014 04:57:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 04:57:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 04:56:48 +0000
Received: by mail-wi0-f177.google.com with SMTP id l15so4388192wiw.16
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 20:55:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=C6ZV2qXq8lk5tU02pkw1AOwQkHQjrYM76/n2BopvAos=;
        b=AFEtH1wOYwsgJ/WUUm3PBVN+SlhggCNxB4xX8urv359P26Fbg4Yxtrau+w/stD4Cp9
         QMdrIS5Gjz5mW4dspQaJmz+U6T0knjmEYW2N06qhp/fBwwuiPHgMQLFiPcn+YK1ood9A
         tz5ZGrQV0pY05w+ddeIhDTV1n+2ixckT1he/h5JaRhpBCOur5CYRZaGXSm44Xk7iFX1F
         2HsUF1imPQDptfttK/YB7T/9K96ZQk8grlxBMpYPGQPyfutug1GlA3cxAMEU6BmCp6XB
         pdYEo3XQe8d7c9h8ab8kwNYbLxUcvzpZNZC57gnjzUR1bJm3hkiLLEEOIaJ0+yg2kfob
         3kCQ==
X-Gm-Message-State: ALoCoQn4m2Aqrbc7dYHkfL8aht2h8fSEnRv/O5QYXVJ67g+2jCEG1Jw/H9dHWKfIz+OYbp2nW3dc
MIME-Version: 1.0
X-Received: by 10.180.81.169 with SMTP id b9mr4585108wiy.41.1418446517407;
 Fri, 12 Dec 2014 20:55:17 -0800 (PST)
Received: by 10.216.68.137 with HTTP; Fri, 12 Dec 2014 20:55:17 -0800 (PST)
In-Reply-To: <etPan.548bb9e6.6b8b4567.e35b@joshs-mbp.att.net>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
	<CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
	<CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com>
	<etPan.548bb9e6.6b8b4567.e35b@joshs-mbp.att.net>
Date: Fri, 12 Dec 2014 20:55:17 -0800
Message-ID: <CAAsvFPkA2Ui-soi6vpEOhMtywONMNodKrK0g=Gv3Vtmqsfp62A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Mark Hamstra <mark@clearstorydata.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d044401eccda8c9050a11d093
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044401eccda8c9050a11d093
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1

On Fri, Dec 12, 2014 at 8:00 PM, Josh Rosen <rosenville@gmail.com> wrote:
>
> +1.  Tested using spark-perf and the Spark EC2 scripts.  I didn=E2=80=99t=
 notice
> any performance regressions that could not be attributed to changes of
> default configurations.  To be more specific, when running Spark 1.2.0 wi=
th
> the Spark 1.1.0 settings of spark.shuffle.manager=3Dhash and
> spark.shuffle.blockTransferService=3Dnio, there was no performance regres=
sion
> and, in fact, there were significant performance improvements for some
> workloads.
>
> In Spark 1.2.0, the new default settings are spark.shuffle.manager=3Dsort
> and spark.shuffle.blockTransferService=3Dnetty.  With these new settings,=
 I
> noticed a performance regression in the scala-sort-by-key-int spark-perf
> test.  However, Spark 1.1.0 and 1.1.1 exhibit a similar performance
> regression for that same test when run with spark.shuffle.manager=3Dsort,=
 so
> this regression seems explainable by the change of defaults.  Besides thi=
s,
> most of the other tests ran at the same speeds or faster with the new 1.2=
.0
> defaults.  Also, keep in mind that this is a somewhat artificial micro
> benchmark; I have heard anecdotal reports from many users that their real
> workloads have run faster with 1.2.0.
>
> Based on these results, I=E2=80=99m comfortable giving a +1 on 1.2.0 RC2.
>
> - Josh
>
> On December 11, 2014 at 9:52:39 AM, Sandy Ryza (sandy.ryza@cloudera.com)
> wrote:
>
> +1 (non-binding). Tested on Ubuntu against YARN.
>
> On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > +1
> >
> > Tested on OS X.
> >
> > On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> > > Please vote on releasing the following candidate as Apache Spark
> version
> > > 1.2.0!
> > >
> > > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> > >
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Da428=
c446e23e628b746e0626cc02b7b3cadf588e
> > >
> > > The release files, including signatures, digests, etc. can be found a=
t:
> > > http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/pwendell.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1055/
> > >
> > > The documentation corresponding to this release can be found at:
> > > http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> > >
> > > Please vote on releasing this package as Apache Spark 1.2.0!
> > >
> > > The vote is open until Saturday, December 13, at 21:00 UTC and passes
> > > if a majority of at least 3 +1 PMC votes are cast.
> > >
> > > [ ] +1 Release this package as Apache Spark 1.2.0
> > > [ ] -1 Do not release this package because ...
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> > >
> > > =3D=3D What justifies a -1 vote for this release? =3D=3D
> > > This vote is happening relatively late into the QA period, so
> > > -1 votes should only occur for significant regressions from
> > > 1.0.2. Bugs already present in 1.1.X, minor
> > > regressions, or bugs related to new features will not block this
> > > release.
> > >
> > > =3D=3D What default changes should I be aware of? =3D=3D
> > > 1. The default value of "spark.shuffle.blockTransferService" has been
> > > changed to "netty"
> > > --> Old behavior can be restored by switching to "nio"
> > >
> > > 2. The default value of "spark.shuffle.manager" has been changed to
> > "sort".
> > > --> Old behavior can be restored by setting "spark.shuffle.manager" t=
o
> > > "hash".
> > >
> > > =3D=3D How does this differ from RC1 =3D=3D
> > > This has fixes for a handful of issues identified - some of the
> > > notable fixes are:
> > >
> > > [Core]
> > > SPARK-4498: Standalone Master can fail to recognize completed/failed
> > > applications
> > >
> > > [SQL]
> > > SPARK-4552: Query for empty parquet table in spark sql hive get
> > > IllegalArgumentException
> > > SPARK-4753: Parquet2 does not prune based on OR filters on partition
> > > columns
> > > SPARK-4761: With JDBC server, set Kryo as default serializer and
> > > disable reference tracking
> > > SPARK-4785: When called with arguments referring column fields, PMOD
> > > throws NPE
> > >
> > > - Patrick
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> <javascript:;>
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > <javascript:;>
> > >
> > >
> >
>

--f46d044401eccda8c9050a11d093--

From dev-return-10781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 06:48:20 2014
Return-Path: <dev-return-10781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C88A898E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 06:48:20 +0000 (UTC)
Received: (qmail 14015 invoked by uid 500); 13 Dec 2014 06:48:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13935 invoked by uid 500); 13 Dec 2014 06:48:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13923 invoked by uid 99); 13 Dec 2014 06:48:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 06:48:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of denny.g.lee@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 06:47:52 +0000
Received: by mail-ig0-f169.google.com with SMTP id hl2so3635141igb.4
        for <dev@spark.apache.org>; Fri, 12 Dec 2014 22:47:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=UTPVDgSxGTpAto442X2e/PoIptyZr8HfbNPtmvlYxOY=;
        b=G1JzZVz4aEZRhOH4T8OLolQWZStYt8DsR+c3IaCfCMZDDhs1M+kJyxx0RBtElB/JdD
         fAWqUWhNHe5ARigGx8hBjcaiw97KJ5BoKmloTVVySngjKTjHoJMBuL8qpou6UV/eLszh
         7kcdiL8JFchD6RD5bOAOs/BEAFHsSXlkEMva+7t3Jk/uFcCkKRZ8jT55Hc7XZ4eLVzvT
         jLy6Bt7JFnJh2bzx5PlL0bFO1eFhcXFTtSdj0EznvGS/hN61+/tP0xMgPkKekMZ2Yyps
         QLBwgBPiczjYUJGc+Szf/VuDDoEQlgGoRflwRUoxCsukEtPag2GdpJsopiqAbKWi2zpr
         xqwQ==
X-Received: by 10.43.154.138 with SMTP id le10mr5546353icc.50.1418453226443;
 Fri, 12 Dec 2014 22:47:06 -0800 (PST)
MIME-Version: 1.0
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
 <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
 <CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com>
 <etPan.548bb9e6.6b8b4567.e35b@joshs-mbp.att.net> <CAAsvFPkA2Ui-soi6vpEOhMtywONMNodKrK0g=Gv3Vtmqsfp62A@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Sat, 13 Dec 2014 06:47:05 +0000
Message-ID: <CABjYQ380Ui8EvGLDrbXUYEoHwnNjUichkU4vi8SeRtoy00nqJQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
To: Mark Hamstra <mark@clearstorydata.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2d3dab15a65050a136037
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d3dab15a65050a136037
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1 Tested on OSX

Tested Scala 2.10.3, SparkSQL with Hive 0.12 / Hadoop 2.5, Thrift Server,
MLLib SVD


On Fri Dec 12 2014 at 8:57:16 PM Mark Hamstra <mark@clearstorydata.com>
wrote:

> +1
>
> On Fri, Dec 12, 2014 at 8:00 PM, Josh Rosen <rosenville@gmail.com> wrote:
> >
> > +1.  Tested using spark-perf and the Spark EC2 scripts.  I didn=E2=80=
=99t notice
> > any performance regressions that could not be attributed to changes of
> > default configurations.  To be more specific, when running Spark 1.2.0
> with
> > the Spark 1.1.0 settings of spark.shuffle.manager=3Dhash and
> > spark.shuffle.blockTransferService=3Dnio, there was no performance
> regression
> > and, in fact, there were significant performance improvements for some
> > workloads.
> >
> > In Spark 1.2.0, the new default settings are spark.shuffle.manager=3Dso=
rt
> > and spark.shuffle.blockTransferService=3Dnetty.  With these new setting=
s,
> I
> > noticed a performance regression in the scala-sort-by-key-int spark-per=
f
> > test.  However, Spark 1.1.0 and 1.1.1 exhibit a similar performance
> > regression for that same test when run with spark.shuffle.manager=3Dsor=
t,
> so
> > this regression seems explainable by the change of defaults.  Besides
> this,
> > most of the other tests ran at the same speeds or faster with the new
> 1.2.0
> > defaults.  Also, keep in mind that this is a somewhat artificial micro
> > benchmark; I have heard anecdotal reports from many users that their re=
al
> > workloads have run faster with 1.2.0.
> >
> > Based on these results, I=E2=80=99m comfortable giving a +1 on 1.2.0 RC=
2.
> >
> > - Josh
> >
> > On December 11, 2014 at 9:52:39 AM, Sandy Ryza (sandy.ryza@cloudera.com=
)
> > wrote:
> >
> > +1 (non-binding). Tested on Ubuntu against YARN.
> >
> > On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > +1
> > >
> > > Tested on OS X.
> > >
> > > On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > >
> > > > Please vote on releasing the following candidate as Apache Spark
> > version
> > > > 1.2.0!
> > > >
> > > > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> > > >
> > > >
> > >
> > https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D
> a428c446e23e628b746e0626cc02b7b3cadf588e
> > > >
> > > > The release files, including signatures, digests, etc. can be found
> at:
> > > > http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> > > >
> > > > Release artifacts are signed with the following key:
> > > > https://people.apache.org/keys/committer/pwendell.asc
> > > >
> > > > The staging repository for this release can be found at:
> > > >
> > https://repository.apache.org/content/repositories/orgapachespark-1055/
> > > >
> > > > The documentation corresponding to this release can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> > > >
> > > > Please vote on releasing this package as Apache Spark 1.2.0!
> > > >
> > > > The vote is open until Saturday, December 13, at 21:00 UTC and pass=
es
> > > > if a majority of at least 3 +1 PMC votes are cast.
> > > >
> > > > [ ] +1 Release this package as Apache Spark 1.2.0
> > > > [ ] -1 Do not release this package because ...
> > > >
> > > > To learn more about Apache Spark, please see
> > > > http://spark.apache.org/
> > > >
> > > > =3D=3D What justifies a -1 vote for this release? =3D=3D
> > > > This vote is happening relatively late into the QA period, so
> > > > -1 votes should only occur for significant regressions from
> > > > 1.0.2. Bugs already present in 1.1.X, minor
> > > > regressions, or bugs related to new features will not block this
> > > > release.
> > > >
> > > > =3D=3D What default changes should I be aware of? =3D=3D
> > > > 1. The default value of "spark.shuffle.blockTransferService" has
> been
> > > > changed to "netty"
> > > > --> Old behavior can be restored by switching to "nio"
> > > >
> > > > 2. The default value of "spark.shuffle.manager" has been changed to
> > > "sort".
> > > > --> Old behavior can be restored by setting "spark.shuffle.manager"
> to
> > > > "hash".
> > > >
> > > > =3D=3D How does this differ from RC1 =3D=3D
> > > > This has fixes for a handful of issues identified - some of the
> > > > notable fixes are:
> > > >
> > > > [Core]
> > > > SPARK-4498: Standalone Master can fail to recognize completed/faile=
d
> > > > applications
> > > >
> > > > [SQL]
> > > > SPARK-4552: Query for empty parquet table in spark sql hive get
> > > > IllegalArgumentException
> > > > SPARK-4753: Parquet2 does not prune based on OR filters on partitio=
n
> > > > columns
> > > > SPARK-4761: With JDBC server, set Kryo as default serializer and
> > > > disable reference tracking
> > > > SPARK-4785: When called with arguments referring column fields, PMO=
D
> > > > throws NPE
> > > >
> > > > - Patrick
> > > >
> > > > ------------------------------------------------------------
> ---------
> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > <javascript:;>
> > > > For additional commands, e-mail: dev-help@spark.apache.org
> > > <javascript:;>
> > > >
> > > >
> > >
> >
>

--001a11c2d3dab15a65050a136037--

From dev-return-10782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 10:20:06 2014
Return-Path: <dev-return-10782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D140A9BC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 10:20:06 +0000 (UTC)
Received: (qmail 52976 invoked by uid 500); 13 Dec 2014 10:20:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52902 invoked by uid 500); 13 Dec 2014 10:20:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52891 invoked by uid 99); 13 Dec 2014 10:20:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 10:20:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 10:19:38 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so4459243qgd.5
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 02:18:52 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=rJf9XWTLZHnR2P7gsttgLNZxKCUmcc7SKy7EUT8eAgs=;
        b=UHF5T33SL11cHylmc7TJiv+apd5/e95nQ860B9Yb4MrlqzW4ItyXVZyBp5J355RrvR
         nHKH0Qlje2ka7mzJsKplTWW0AQJnxMXFYMlEgVPUMuJiOtATA1paZd7RmuDKvtuidZA8
         HqF++1OW66Xl++aurLXk3BMZrM/O4/BFTcQ486GJFGyOCFl9wFdp8fSZNSNVCukneI9Z
         58Gw9lwJ8/oGLwt+Zp0G4K6a/j3Nyx55jEn/TWRBgEEzNHifED1TaP1KW8/wVxGjy4d3
         yaOEHqs3y3X0O3sIafuwVhQrEf16hi+DhkWTiapVLoRQNHPQRLp67IrlTsiyTVePvHXo
         XxRw==
X-Gm-Message-State: ALoCoQl01gGLpEKaZKUPksQNx6ldcvdjJTsOc1Dwa7Xf8yp7x36Ai+gwedOcbdvRgVSVS2yLsf9p
MIME-Version: 1.0
X-Received: by 10.224.54.2 with SMTP id o2mr38453708qag.34.1418465932014; Sat,
 13 Dec 2014 02:18:52 -0800 (PST)
Received: by 10.140.102.113 with HTTP; Sat, 13 Dec 2014 02:18:51 -0800 (PST)
In-Reply-To: <548BA161.6090207@gmail.com>
References: <548BA161.6090207@gmail.com>
Date: Sat, 13 Dec 2014 02:18:51 -0800
Message-ID: <CACBYxK+N-rQHW-UMX8jB_T6wZ-pc2z=CH5OQQ3yXm-z3dD_VHQ@mail.gmail.com>
Subject: Re: one hot encoding
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Lochana Menikarachchi <lochanac@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132f08001154b050a1656de
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f08001154b050a1656de
Content-Type: text/plain; charset=UTF-8

Hi Lochana,

We haven't yet added this in 1.2.
https://issues.apache.org/jira/browse/SPARK-4081 tracks adding categorical
feature indexing, which one-hot encoding can be built on.
https://issues.apache.org/jira/browse/SPARK-1216 also tracks a version of
this prior to the ML pipelines work.

-Sandy

On Fri, Dec 12, 2014 at 6:16 PM, Lochana Menikarachchi <lochanac@gmail.com>
wrote:
>
> Do we have one-hot encoding in spark MLLib 1.1.1 or 1.2.0 ? It wasn't
> available in 1.1.0.
> Thanks.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1132f08001154b050a1656de--

From dev-return-10783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 13:01:52 2014
Return-Path: <dev-return-10783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DCE79E73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 13:01:52 +0000 (UTC)
Received: (qmail 51034 invoked by uid 500); 13 Dec 2014 13:01:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50962 invoked by uid 500); 13 Dec 2014 13:01:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50949 invoked by uid 99); 13 Dec 2014 13:01:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 13:01:50 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 98.138.121.96 as permitted sender)
Received: from [98.138.121.96] (HELO nm47-vm0.bullet.mail.ne1.yahoo.com) (98.138.121.96)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 13:01:45 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=Jy6AJkokgmlISc3y3EdjM+ZwYZ1XVEG/jo+vLfTRnjphrTk3G+b6bjWU+3fVxoKbZTFr8+NRsKEf+5pPjC/nVXOSFn+qdPAX7AmmnBvyK1DZVgX23h2q8x0sOXuU3oBFfmagZPSP8IzutgtqF5m0EZ4tViox/z/NcaFLVRMpODUjQeo/q34ZGG6WkgfxqLLt/nx1FkE3urxWVmjv7GqJ0QJ7zMOa7IPJB1Ew1xnjkawRLqP+SLsebcB0vCitf1bdiMucJS23vDa8W+tZk1ZAJOP7qOSGYQQl58A7qRQiOgzd1omTOfhs1pB3X6nGtVDwCW9jgkgMVRp8vxH+W1KqnA==;
Received: from [127.0.0.1] by nm47.bullet.mail.ne1.yahoo.com with NNFMP; 13 Dec 2014 13:01:24 -0000
Received: from [98.138.100.116] by nm47.bullet.mail.ne1.yahoo.com with NNFMP; 13 Dec 2014 12:58:26 -0000
Received: from [98.139.212.150] by tm107.bullet.mail.ne1.yahoo.com with NNFMP; 13 Dec 2014 12:58:26 -0000
Received: from [98.139.212.199] by tm7.bullet.mail.bf1.yahoo.com with NNFMP; 13 Dec 2014 12:58:26 -0000
Received: from [127.0.0.1] by omp1008.mail.bf1.yahoo.com with NNFMP; 13 Dec 2014 12:58:26 -0000
X-Yahoo-Newman-Property: ymail-4
X-Yahoo-Newman-Id: 104174.99940.bm@omp1008.mail.bf1.yahoo.com
X-YMail-OSG: KCC5GO4VM1ma_w9zswugyyqGsdgudQmAS0J4GsvcARRIGnc6JHLejv_kKNMRh4s
 wwc6NwNNFZsRqL_sjOvBk1.UWw73qg4LgUILaWs0ko2o6f1Z_6oQ44ABZ9tiTrw0zLogyScpeTuT
 wr9tQdtZoOeX4Pn2lU.RLdQ6zfFkN.pUl1dSS0jY1xaK21FftyWKic2hV3Wf8UtKIzcY_qKVYYRQ
 2O37rhoZaWHuQv2JckTXo8LeOaVp5aZP1NBkHM_ZaEpzBPEfQ3HIwg78Wcky4odMD5anh_xfOqBp
 _2yZKspOBqEltGzOiqO0AWxG3MfDRo9mPg78Y69EujGTvKYiSV7NDN4cdGTljKtMSHR7DKbwRSOC
 z15qKOv0Xq14H9jbgk7nkp.OnyUiNvJ0rehwiomsrAvPnHjlfd_EqBSbpmpib4tTbGZRFJveJHN3
 rZ.MsXCpiUj7EvVG1ljqkwe86ne_3dcmsB12B.2NVXh6v9BKYI4Qi5D3UCRmZIQyHOGs4CQwq3M0
 GTR7BC7pSs1UdUjeCvncnUnst2EKR8h7yDV.Hk35ma81vuByEX6eOJxTRRy1V1BSIE66uCmDj_BY
 Hopzapu_W.2AJCuvGrlXznkfd67Xr9rI-
Received: by 66.196.80.122; Sat, 13 Dec 2014 12:58:25 +0000 
Date: Sat, 13 Dec 2014 12:58:25 +0000 (UTC)
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
To: Denny Lee <denny.g.lee@gmail.com>, 
	Mark Hamstra <mark@clearstorydata.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1411841348.194179.1418475505094.JavaMail.yahoo@jws106132.mail.bf1.yahoo.com>
In-Reply-To: <CABjYQ380Ui8EvGLDrbXUYEoHwnNjUichkU4vi8SeRtoy00nqJQ@mail.gmail.com>
References: <CABjYQ380Ui8EvGLDrbXUYEoHwnNjUichkU4vi8SeRtoy00nqJQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_194178_733974922.1418475505088"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_194178_733974922.1418475505088
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1 built and tested on Yarn on Hadoop 2.x cluster.
Tom=20

     On Saturday, December 13, 2014 12:48 AM, Denny Lee <denny.g.lee@gmail.=
com> wrote:
  =20

 +1 Tested on OSX

Tested Scala 2.10.3, SparkSQL with Hive 0.12 / Hadoop 2.5, Thrift Server,
MLLib SVD


On Fri Dec 12 2014 at 8:57:16 PM Mark Hamstra <mark@clearstorydata.com>
wrote:

> +1
>
> On Fri, Dec 12, 2014 at 8:00 PM, Josh Rosen <rosenville@gmail.com> wrote:
> >
> > +1.=C2=A0 Tested using spark-perf and the Spark EC2 scripts.=C2=A0 I di=
dn=E2=80=99t notice
> > any performance regressions that could not be attributed to changes of
> > default configurations.=C2=A0 To be more specific, when running Spark 1=
.2.0
> with
> > the Spark 1.1.0 settings of spark.shuffle.manager=3Dhash and
> > spark.shuffle.blockTransferService=3Dnio, there was no performance
> regression
> > and, in fact, there were significant performance improvements for some
> > workloads.
> >
> > In Spark 1.2.0, the new default settings are spark.shuffle.manager=3Dso=
rt
> > and spark.shuffle.blockTransferService=3Dnetty.=C2=A0 With these new se=
ttings,
> I
> > noticed a performance regression in the scala-sort-by-key-int spark-per=
f
> > test.=C2=A0 However, Spark 1.1.0 and 1.1.1 exhibit a similar performanc=
e
> > regression for that same test when run with spark.shuffle.manager=3Dsor=
t,
> so
> > this regression seems explainable by the change of defaults.=C2=A0 Besi=
des
> this,
> > most of the other tests ran at the same speeds or faster with the new
> 1.2.0
> > defaults.=C2=A0 Also, keep in mind that this is a somewhat artificial m=
icro
> > benchmark; I have heard anecdotal reports from many users that their re=
al
> > workloads have run faster with 1.2.0.
> >
> > Based on these results, I=E2=80=99m comfortable giving a +1 on 1.2.0 RC=
2.
> >
> > - Josh
> >
> > On December 11, 2014 at 9:52:39 AM, Sandy Ryza (sandy.ryza@cloudera.com=
)
> > wrote:
> >
> > +1 (non-binding). Tested on Ubuntu against YARN.
> >
> > On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > +1
> > >
> > > Tested on OS X.
> > >
> > > On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > >
> > > > Please vote on releasing the following candidate as Apache Spark
> > version
> > > > 1.2.0!
> > > >
> > > > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> > > >
> > > >
> > >
> > https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D
> a428c446e23e628b746e0626cc02b7b3cadf588e
> > > >
> > > > The release files, including signatures, digests, etc. can be found
> at:
> > > > http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> > > >
> > > > Release artifacts are signed with the following key:
> > > > https://people.apache.org/keys/committer/pwendell.asc
> > > >
> > > > The staging repository for this release can be found at:
> > > >
> > https://repository.apache.org/content/repositories/orgapachespark-1055/
> > > >
> > > > The documentation corresponding to this release can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> > > >
> > > > Please vote on releasing this package as Apache Spark 1.2.0!
> > > >
> > > > The vote is open until Saturday, December 13, at 21:00 UTC and pass=
es
> > > > if a majority of at least 3 +1 PMC votes are cast.
> > > >
> > > > [ ] +1 Release this package as Apache Spark 1.2.0
> > > > [ ] -1 Do not release this package because ...
> > > >
> > > > To learn more about Apache Spark, please see
> > > > http://spark.apache.org/
> > > >
> > > > =3D=3D What justifies a -1 vote for this release? =3D=3D
> > > > This vote is happening relatively late into the QA period, so
> > > > -1 votes should only occur for significant regressions from
> > > > 1.0.2. Bugs already present in 1.1.X, minor
> > > > regressions, or bugs related to new features will not block this
> > > > release.
> > > >
> > > > =3D=3D What default changes should I be aware of? =3D=3D
> > > > 1. The default value of "spark.shuffle.blockTransferService" has
> been
> > > > changed to "netty"
> > > > --> Old behavior can be restored by switching to "nio"
> > > >
> > > > 2. The default value of "spark.shuffle.manager" has been changed to
> > > "sort".
> > > > --> Old behavior can be restored by setting "spark.shuffle.manager"
> to
> > > > "hash".
> > > >
> > > > =3D=3D How does this differ from RC1 =3D=3D
> > > > This has fixes for a handful of issues identified - some of the
> > > > notable fixes are:
> > > >
> > > > [Core]
> > > > SPARK-4498: Standalone Master can fail to recognize completed/faile=
d
> > > > applications
> > > >
> > > > [SQL]
> > > > SPARK-4552: Query for empty parquet table in spark sql hive get
> > > > IllegalArgumentException
> > > > SPARK-4753: Parquet2 does not prune based on OR filters on partitio=
n
> > > > columns
> > > > SPARK-4761: With JDBC server, set Kryo as default serializer and
> > > > disable reference tracking
> > > > SPARK-4785: When called with arguments referring column fields, PMO=
D
> > > > throws NPE
> > > >
> > > > - Patrick
> > > >
> > > > ------------------------------------------------------------
> ---------
> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > <javascript:;>
> > > > For additional commands, e-mail: dev-help@spark.apache.org
> > > <javascript:;>
> > > >
> > > >
> > >
> >
>

   
------=_Part_194178_733974922.1418475505088--

From dev-return-10784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 13:12:14 2014
Return-Path: <dev-return-10784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47CEF9E8F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 13:12:14 +0000 (UTC)
Received: (qmail 55679 invoked by uid 500); 13 Dec 2014 13:12:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55607 invoked by uid 500); 13 Dec 2014 13:12:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55594 invoked by uid 99); 13 Dec 2014 13:12:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 13:12:10 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 184.105.206.84 as permitted sender)
Received: from [184.105.206.84] (HELO smtpproxy19.qq.com) (184.105.206.84)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 13:12:05 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1418476299; bh=2goK+IipKlxbT0UcVL/CFqt83thohg8lI9lLo0enoTY=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Cc:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=TRXCKj+mCBGlPHcRcKlFd83x2lG7Dw//siRJzOjVy3b1F1F3dVLH5je3qgMmqhxYG
	 g8CHDq5SdOQQ++vzJyqfTVhVSInhuy2K02aFoY1+zTcR53eH1/776SbJ3Vwk9xs8AP
	 s8svDFIc0qfmT68Qg4W5eef0gssV1TmBg+6atdgI=
X-QQ-FEAT: hSSUgwK6GKUcASjuP8iFzqIHXvQ/iG3x+ZEjHrT+JtcX6WR/C5kxbD+JDuY86
	OtXeqXER/1bGcl/OW8fmth4c602hVoBC8qC9MUhLei1V+r77RxpQdmBDOYlhglYuOzunrPK
	USULH8EYEK594IIbSpH5/KVvE/hCjEca4gL6nQE1UbTGBwpUG9bzDWMi9HS3WERdetQFmMb
	tcvsRe6Vm1YIkBiKdjwy3K8d8jhrDUqw=
X-QQ-SSF: 000000000000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 223.20.24.193
In-Reply-To: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1418476297t1143949
From: "=?gb18030?B?R3VvUWlhbmcgTGk=?=" <witgo@qq.com>
To: "=?gb18030?B?UGF0cmljayBXZW5kZWxs?=" <pwendell@gmail.com>
Cc: "=?gb18030?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_548C3B09_092F0C28_2D18279F"
Content-Transfer-Encoding: 8Bit
Date: Sat, 13 Dec 2014 21:11:37 +0800
X-Priority: 3
Message-ID: <tencent_77384F866EAF4B0D66EFF831@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 3776903200
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_548C3B09_092F0C28_2D18279F
Content-Type: text/plain;
	charset="gb18030"
Content-Transfer-Encoding: base64
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------=_NextPart_548C3B09_092F0C28_2D18279F--




From dev-return-10785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 14:47:50 2014
Return-Path: <dev-return-10785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B2A99FC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 14:47:50 +0000 (UTC)
Received: (qmail 45911 invoked by uid 500); 13 Dec 2014 14:47:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45841 invoked by uid 500); 13 Dec 2014 14:47:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45822 invoked by uid 99); 13 Dec 2014 14:47:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 14:47:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 14:47:22 +0000
Received: by mail-qa0-f46.google.com with SMTP id n8so6350257qaq.33
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 06:47:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:subject
         :content-type;
        bh=2MSEKTJqqQ6Pmn0zVkSjXwM6mzAuN73TFdBrbRGyDIk=;
        b=fINDFzm+Blg32O5kQ9hj8PtU5PfUmHTJQDxzk0Lb0KsbJDo9xdB6NaJyXmD6zpW+DE
         D8fNp8sJjJClsEb+lglCgAWKH7YPZ2lUvC2N5uDl260/oE6SqIucSi4e1jQ9fyxH9DFx
         lKCAj78c0RwQMWb+Mvd2JlAPZcwSPkSNwO1DbevPffqtI4oT0zCoZjE26VpCm05DC7r1
         cMzZzSLsPT7JKit4yTyGiOw169xidHkkyRb+Nh5hibq/uHV7BYhPJVpPAJpJ/wRBjbN3
         GI+O0fXx5t0R2dvzP5f3R7BH+1TCQAYPZN0Ih1GOrzQcVv5FXQ4wUhjPR/mER7zkTYnB
         F2yQ==
X-Received: by 10.224.164.195 with SMTP id f3mr41360536qay.55.1418482041551;
        Sat, 13 Dec 2014 06:47:21 -0800 (PST)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id p35sm4648830qgd.5.2014.12.13.06.47.20
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 13 Dec 2014 06:47:20 -0800 (PST)
Date: Sat, 13 Dec 2014 06:47:20 -0800 (PST)
X-Google-Original-Date: Sat, 13 Dec 2014 14:47:19 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1418482039856.252018a2@Nodemailer>
In-Reply-To: <tencent_77384F866EAF4B0D66EFF831@qq.com>
References: <tencent_77384F866EAF4B0D66EFF831@qq.com>
X-Orchestra-Oid: 72A7B2B2-AA81-4F6F-A03C-73D323771209
X-Orchestra-Sig: 526d87b685e25941aaaabdfb8505779d24e3ad22
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1487138504865805193
X-Orchestra-Thrid-Sig: 14cb9d54f74b45de30cb95e1441db1df2fb56044
X-Orchestra-Account: afbf1a908c78adf1bd57b748c6c0a8dda4f029c4
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1418482040635"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1418482040635
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

+1

=E2=80=94
Sent from Mailbox

On Sat, Dec 13, 2014 at 3:12 PM, GuoQiang Li <witgo@qq.com> wrote:

> +1 (non-binding).  Tested on CentOS 6.4
> ------------------ Original ------------------
> From:  =22Patrick Wendell=22;<pwendell@gmail.com>;
> Date:  Thu, Dec 11, 2014 05:08 AM
> To:  =22dev=E5=8F=91=E9=80=81@spark.apache.org=22<dev@spark.apache.org>;
> Subject:  [VOTE] Release Apache Spark 1.2.0 (RC2)
> Please vote on releasing the following candidate as Apache Spark version =
1.2.0!
> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> https://git-wip-us.apache.org/repos/asf=3Fp=3Dspark.=
git;a=3Dcommit;h=3Da428c446e23e628b746e0626cc02b7b3cadf588e
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1055/
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> Please vote on releasing this package as Apache Spark 1.2.0!
> The vote is open until Saturday, December 13, at 21:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.2.0
> [ ] -1 Do not release this package because ...
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> =3D=3D What justifies a -1 vote for this release=3F =3D=3D
> This vote is happening relatively late into the QA period, so
> -1 votes should only occur for significant regressions from
> 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
> =3D=3D What default changes should I be aware of=3F =3D=3D
> 1. The default value of =22spark.shuffle.blockTransferService=22 has =
been
> changed to =22netty=22
> --> Old behavior can be restored by switching to =22nio=22
> 2. The default value of =22spark.shuffle.manager=22 has been changed to =
=22sort=22.
> --> Old behavior can be restored by setting =22spark.shuffle.manager=22 =
to =22hash=22.
> =3D=3D How does this differ from RC1 =3D=3D
> This has fixes for a handful of issues identified - some of the
> notable fixes are:
> [Core]
> SPARK-4498: Standalone Master can fail to recognize completed/failed
> applications
> [SQL]
> SPARK-4552: Query for empty parquet table in spark sql hive get
> IllegalArgumentException
> SPARK-4753: Parquet2 does not prune based on OR filters on partition =
columns
> SPARK-4761: With JDBC server, set Kryo as default serializer and
> disable reference tracking
> SPARK-4785: When called with arguments referring column fields, PMOD =
throws NPE
> - Patrick
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.=
org
------Nodemailer-0.5.0-?=_1-1418482040635--

From dev-return-10786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 18:34:50 2014
Return-Path: <dev-return-10786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6EBB2C5A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 18:34:50 +0000 (UTC)
Received: (qmail 5391 invoked by uid 500); 13 Dec 2014 18:34:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5302 invoked by uid 500); 13 Dec 2014 18:34:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5286 invoked by uid 99); 13 Dec 2014 18:34:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 18:34:48 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of anant.asty@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 18:34:43 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 7F220D9BE9C
	for <dev@spark.incubator.apache.org>; Sat, 13 Dec 2014 10:32:53 -0800 (PST)
Date: Sat, 13 Dec 2014 11:32:52 -0700 (MST)
From: slcclimber <anant.asty@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418495572515-9770.post@n3.nabble.com>
In-Reply-To: <CABjYQ380Ui8EvGLDrbXUYEoHwnNjUichkU4vi8SeRtoy00nqJQ@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com> <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com> <CACBYxK+nWShi0k3-_XJYc+tX8_bddLxrHTHw3LYrQjNs6J6e5w@mail.gmail.com> <etPan.548bb9e6.6b8b4567.e35b@joshs-mbp.att.net> <CAAsvFPkA2Ui-soi6vpEOhMtywONMNodKrK0g=Gv3Vtmqsfp62A@mail.gmail.com> <CABjYQ380Ui8EvGLDrbXUYEoHwnNjUichkU4vi8SeRtoy00nqJQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am building and testing using sbt.
I get a lot of 
"Job aborted due to stage failure: Master removed our application: FAILED"
did not contain "cancelled", and "Job aborted due to stage failure: Master
removed our application: FAILED" did not contain "killed"
errors trying to run tests.  (JobCancellationSuite.scala:236)
I have never experienced this before so it is concerning.

I  was able to successfully run all the python examples for spark and Mllib
successfully.




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-0-RC2-tp9713p9770.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 20:48:25 2014
Return-Path: <dev-return-10787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 60735C78B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 20:48:25 +0000 (UTC)
Received: (qmail 11545 invoked by uid 500); 13 Dec 2014 20:48:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11479 invoked by uid 500); 13 Dec 2014 20:48:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11468 invoked by uid 99); 13 Dec 2014 20:48:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 20:48:22 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.22 as permitted sender)
Received: from [216.64.169.22] (HELO pdxmta01.webtrends.com) (216.64.169.22)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 20:47:57 +0000
Received: from pdxex1.webtrends.corp (Not Verified[10.61.2.220]) by pdxmta01.webtrends.com with MailMarshal (v7,2,3,6978) (using TLS: SSLv23)
	id <B548ca5d40000>; Sat, 13 Dec 2014 20:47:16 +0000
Received: from PDXEX2.WebTrends.corp ([172.27.3.221]) by pdxex1.webtrends.corp
 ([172.27.5.220]) with mapi id 14.03.0195.001; Sat, 13 Dec 2014 20:47:13 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
Thread-Topic: [VOTE] Release Apache Spark 1.2.0 (RC2)
Thread-Index: AQHQFL2n+rqjNFlBMkSb96tcTemr+5yKqRkAgANZWoA=
Date: Sat, 13 Dec 2014 20:47:11 +0000
Message-ID: <CE681A77-34B0-4A6F-92E9-C7547B16BD61@webtrends.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
 <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
In-Reply-To: <CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <5563559C74186D40A7116BC7C3E7DA3D@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

+1 tested on OS X and deployed+tested our apps via YARN into our staging cl=
uster.

Sean


> On Dec 11, 2014, at 10:40 AM, Reynold Xin <rxin@databricks.com> wrote:
>=20
> +1
>=20
> Tested on OS X.
>=20
> On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com> wro=
te:
>=20
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.2.0!
>>=20
>> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
>>=20
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Da42=
8c446e23e628b746e0626cc02b7b3cadf588e
>>=20
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
>>=20
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>=20
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1055/
>>=20
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
>>=20
>> Please vote on releasing this package as Apache Spark 1.2.0!
>>=20
>> The vote is open until Saturday, December 13, at 21:00 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>>=20
>> [ ] +1 Release this package as Apache Spark 1.2.0
>> [ ] -1 Do not release this package because ...
>>=20
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>=20
>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> This vote is happening relatively late into the QA period, so
>> -1 votes should only occur for significant regressions from
>> 1.0.2. Bugs already present in 1.1.X, minor
>> regressions, or bugs related to new features will not block this
>> release.
>>=20
>> =3D=3D What default changes should I be aware of? =3D=3D
>> 1. The default value of "spark.shuffle.blockTransferService" has been
>> changed to "netty"
>> --> Old behavior can be restored by switching to "nio"
>>=20
>> 2. The default value of "spark.shuffle.manager" has been changed to "sor=
t".
>> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>> "hash".
>>=20
>> =3D=3D How does this differ from RC1 =3D=3D
>> This has fixes for a handful of issues identified - some of the
>> notable fixes are:
>>=20
>> [Core]
>> SPARK-4498: Standalone Master can fail to recognize completed/failed
>> applications
>>=20
>> [SQL]
>> SPARK-4552: Query for empty parquet table in spark sql hive get
>> IllegalArgumentException
>> SPARK-4753: Parquet2 does not prune based on OR filters on partition
>> columns
>> SPARK-4761: With JDBC server, set Kryo as default serializer and
>> disable reference tracking
>> SPARK-4785: When called with arguments referring column fields, PMOD
>> throws NPE
>>=20
>> - Patrick
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
>> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;=
>
>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 13 22:06:27 2014
Return-Path: <dev-return-10788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87D12C993
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 13 Dec 2014 22:06:27 +0000 (UTC)
Received: (qmail 85617 invoked by uid 500); 13 Dec 2014 22:06:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85538 invoked by uid 500); 13 Dec 2014 22:06:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84370 invoked by uid 99); 13 Dec 2014 22:06:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 22:06:23 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 13 Dec 2014 22:05:57 +0000
Received: by mail-pa0-f43.google.com with SMTP id kx10so9440222pab.30
        for <multiple recipients>; Sat, 13 Dec 2014 14:05:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=brsL9oWk/hVTGrvSkCA6I4tlPyYjc7cwLeZjkrSyFwo=;
        b=FyjaJ2PY/hnEiXhOyVVwwRHL1Kbh/Di5w9VbjGNgJ13yFg+EGlB3NQt809k+D5lJlm
         xsNASVfg9GowgxUGA4xJh6inXLRNcl6q20O1hyVIIjpNpIJe45rktqd8CcXH8isgaTG+
         heVF9kiRDvpjcP1VrsxHBEBIDD4kcPXNalPS5Hh1dzSYWkk02VFi8CGcCqH17LOg8oTr
         TpJgu/UQteb5wJWn3j5wtSKPDvsTSxlB2/85vYJdjAz1Wj9bkqNtz96zZybV5Ad9WOdt
         EVowp7IgX6RcMKjvQgv7ajji7+NgRbadsD5Dj2HNCglshEDd0Keao8LiuCDW0uKgPU7D
         L/eg==
MIME-Version: 1.0
X-Received: by 10.68.200.68 with SMTP id jq4mr1402955pbc.30.1418508356323;
 Sat, 13 Dec 2014 14:05:56 -0800 (PST)
Received: by 10.70.41.80 with HTTP; Sat, 13 Dec 2014 14:05:56 -0800 (PST)
Date: Sat, 13 Dec 2014 14:05:56 -0800
Message-ID: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
Subject: Nabble mailing list mirror errors: "This post has NOT been accepted
 by the mailing list yet"
From: Josh Rosen <rosenville@gmail.com>
To: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b10ce69b0920a050a20369d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10ce69b0920a050a20369d
Content-Type: text/plain; charset=UTF-8

I've noticed that several users are attempting to post messages to Spark's
user / dev mailing lists using the Nabble web UI (
http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there are
many posts in Nabble that are not posted to the Apache lists and are
flagged with "This post has NOT been accepted by the mailing list yet."
errors.

I suspect that the issue is that users are not completing the sign-up
confirmation process (
http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
which is preventing their emails from being accepted by the mailing list.

I wanted to mention this issue to the Spark community to see whether there
are any good solutions to address this.  I have spoken to users who think
that our mailing list is unresponsive / inactive because their un-posted
messages haven't received any replies.

- Josh

--047d7b10ce69b0920a050a20369d--

From dev-return-10789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 02:28:43 2014
Return-Path: <dev-return-10789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AD6ACE4E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 02:28:43 +0000 (UTC)
Received: (qmail 75440 invoked by uid 500); 14 Dec 2014 02:28:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75362 invoked by uid 500); 14 Dec 2014 02:28:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73926 invoked by uid 99); 14 Dec 2014 02:28:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 02:28:39 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yana.kadiyska@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 02:28:13 +0000
Received: by mail-wi0-f181.google.com with SMTP id r20so5933576wiv.8
        for <multiple recipients>; Sat, 13 Dec 2014 18:27:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:in-reply-to:references:date:message-id
         :subject:from:to:cc:content-type;
        bh=/SKkCd/aWT8p/99DXksOlAvvj0vmquiHEmjrpdcQXKc=;
        b=06ORMX99+4nRAPLvCtRXLP/xDHAjdaW/db++P1LZx4Pqsn8+nI3yVIjTFldHM4iEk5
         Hho4EbBr4czGJ4BTCWFuE6MhKfJiQikh2vnO3aS+J12HC7kdHXH6DJT9dcjpF3NXgtBf
         zxXV7ite8g+MWjj2k/ccP0eakx25/YCtylRNssgKa05l96ZPF/7kBd81XI2ugkWnMDRd
         yfF6Cr3RKJOZ0OjDj8oYn14/OttLiPXqx0ZlMscWWxgEMhRcdXuNfaFhG+mZucv7Ia2i
         4GNDLXKD9wwSBWE9Y4K31QeP6CntpCedWLynFMz/2ZGIxDdVxAmX3eWH65SXZUIBWX50
         Rsww==
MIME-Version: 1.0
X-Received: by 10.194.2.75 with SMTP id 11mr28644362wjs.78.1418524048005; Sat,
 13 Dec 2014 18:27:28 -0800 (PST)
Received: by 10.216.212.199 with HTTP; Sat, 13 Dec 2014 18:27:27 -0800 (PST)
Reply-To: yana.kadiyska@gmail.com
In-Reply-To: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
Date: Sat, 13 Dec 2014 21:27:27 -0500
Message-ID: <CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
From: Yana Kadiyska <yana.kadiyska@gmail.com>
To: Josh Rosen <rosenville@gmail.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a834cfca03e050a23dd0f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a834cfca03e050a23dd0f
Content-Type: text/plain; charset=UTF-8

Since you mentioned this, I had a related quandry recently -- it also says
that the forum archives "*user@spark.incubator.apache.org
<user@spark.incubator.apache.org>"/* *dev@spark.incubator.apache.org
<dev@spark.incubator.apache.org> *respectively, yet the "Community page"
clearly says to email the @spark.apache.org list (but the nabble archive is
linked right there too). IMO even putting a clear explanation at the top

"Posting here requires that you create an account via the UI. Your message
will be sent to both spark.incubator.apache.org and spark.apache.org (if
that is the case, i'm not sure which alias nabble posts get sent to)" would
make things a lot more clear.

On Sat, Dec 13, 2014 at 5:05 PM, Josh Rosen <rosenville@gmail.com> wrote:
>
> I've noticed that several users are attempting to post messages to Spark's
> user / dev mailing lists using the Nabble web UI (
> http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there
> are many posts in Nabble that are not posted to the Apache lists and are
> flagged with "This post has NOT been accepted by the mailing list yet."
> errors.
>
> I suspect that the issue is that users are not completing the sign-up
> confirmation process (
> http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
> which is preventing their emails from being accepted by the mailing list.
>
> I wanted to mention this issue to the Spark community to see whether there
> are any good solutions to address this.  I have spoken to users who think
> that our mailing list is unresponsive / inactive because their un-posted
> messages haven't received any replies.
>
> - Josh
>

--047d7b3a834cfca03e050a23dd0f--

From dev-return-10790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 07:33:05 2014
Return-Path: <dev-return-10790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0004410211
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 07:33:04 +0000 (UTC)
Received: (qmail 67160 invoked by uid 500); 14 Dec 2014 07:33:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67090 invoked by uid 500); 14 Dec 2014 07:33:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67074 invoked by uid 99); 14 Dec 2014 07:33:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:33:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:32:57 +0000
Received: by mail-ie0-f171.google.com with SMTP id rl12so9144896iec.16
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 23:31:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=Llbj5Ic9a8JCf+n/FLWJJBjPpue6egp40IzJntgCdeQ=;
        b=e7z77lZEWum1A6EuwjnC0KLjk52JxtaTLTPdCwlQYB8Hq8/3hkbeS78NMIUyWkpWbt
         SUmlHq1HPNvJTlR9rKbhmgCgj1iJu9V5hYp4FSfw6bwp/FIsJOcLp4teON/5DBi2OQgA
         WM+Z7wZV/jTI5b3aDp1ddqznx245Dn8kTDTjX4PMXVOvY7sXtn2UQ6sc1ymDulrZAKON
         FmU+ismccca39mP/7MFciS4d/lwMQlYEmLBP8vkhokc9It7cxElIvoh9jyF2d1o/rgGp
         PcSbTjoh8sEqdOpoYYlp6zusFRIXsdLHou2E+OTeg7IYYYQmY/AazXjyJ9+P5GV018It
         6KzQ==
X-Received: by 10.50.66.200 with SMTP id h8mr12596670igt.20.1418542311427;
 Sat, 13 Dec 2014 23:31:51 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 14 Dec 2014 07:31:50 +0000
Message-ID: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
Subject: Spark JIRA Report
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1a6692781d050a281e8c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1a6692781d050a281e8c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

What do y=E2=80=99all think of a report like this emailed out to the dev li=
st on a
monthly basis?

The goal would be to increase visibility into our open issues and encourage
developers to tend to our issue tracker more frequently.

Nick

There are 1,236 unresolved issues
<https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+SPARK+AND+resolut=
ion+%3D+Unresolved+ORDER+BY+updated+DESC>
in the Spark project on JIRA.
Recently Updated Issues
<https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0resolution%20%3D%20Unresolved%20ORDER%20BY%20updated%20DESC>
Type Key Priority Summary Last Updated   Bug SPARK-4841
<https://issues.apache.org/jira/browse/SPARK-4841> Major Batch serializer
bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
<https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
collect Dec 14, 2014  Bug SPARK-785
<https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner not
invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
<https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
creation on VPC Dec 13, 2014  Improvement SPARK-1555
<https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014   St=
ale
Issues
<https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY=
%20updated%20ASC>
Type Key Priority Summary Last Updated   Bug SPARK-560
<https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs /
iterators Oct 22, 2012  New Feature SPARK-540
<https://issues.apache.org/jira/browse/SPARK-540> None Add API to customize
in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
<https://issues.apache.org/jira/browse/SPARK-573> None Clarify semantics of
the parallelized closures Oct 22, 2012  New Feature SPARK-609
<https://issues.apache.org/jira/browse/SPARK-609> Minor Add instructions
for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
<https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism to
run system management/configuration tasks on all workers Dec 17, 2012   Mos=
t
Watched Issues
<https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0resolution%20%3D%20Unresolved%20ORDER%20BY%20watchers%20DESC>
Type Key Priority Summary Watchers   New Feature SPARK-3561
<https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
pluggable execution contexts in Spark 75  New Feature SPARK-2365
<https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD, an
efficient updatable key-value store 33  Improvement SPARK-2044
<https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
interface for shuffles 30  New Feature SPARK-1405
<https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel Latent
Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature SPARK-140=
6
<https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
evaluation support via MLib 21   Most Voted Issues
<https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0resolution%20%3D%20Unresolved%20ORDER%20BY%20votes%20DESC>
Type Key Priority Summary Votes   Bug SPARK-2541
<https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mode
can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
<https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD, an
efficient updatable key-value store 9  Improvement SPARK-3533
<https://issues.apache.org/jira/browse/SPARK-3533> Major Add
saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
<https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Support
for ORCFile format 6  New Feature SPARK-1442
<https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
function support 6
=E2=80=8B

--047d7bdc1a6692781d050a281e8c--

From dev-return-10791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 07:45:54 2014
Return-Path: <dev-return-10791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39A0610231
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 07:45:53 +0000 (UTC)
Received: (qmail 74502 invoked by uid 500); 14 Dec 2014 07:45:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74420 invoked by uid 500); 14 Dec 2014 07:45:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74409 invoked by uid 99); 14 Dec 2014 07:45:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:45:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:45:22 +0000
Received: by mail-qc0-f175.google.com with SMTP id b13so8208897qcw.6
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 23:43:49 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=Ge5RqJvXExHhdUlZUH2fIK+sx8NjKN+ejh8+lOH49HU=;
        b=i0neAxjA/mwdh0e1QtBatLvVpRbs/95rsYbS7xz26muHNhm36+l1eE0dpL6WSAONww
         Pmi79NdT5iHbHRIl5IDALbQbQ7bTx+oByp3O9YK52BHqUUfKelVlj5EdouPrVp/fxjrZ
         XMh0WMm2gEbNkF+zPE7uCgMMXIE3mGsCAv3477927BYr9R9S0JDfaFU/HS+dcVDClTEH
         HeS5LHzeuc/jaU70XHJkogOF4E1cdmRYBYBXH4r+dmIZ7NBHAZ+Mafgz7ZgP/zx9JQo9
         EplbGJMHoGHOqQmhw3fy74thuysGovJsojrEU000JcG7vOHkavZKcoAvWqseenpEglki
         X2Mw==
X-Gm-Message-State: ALoCoQk0q++H7WHfWPgIC/QuWcPmwTha3VNy/UoX+y/pUdoIugCyZnmdTYqgklRwvse/tOHt0pJ3
X-Received: by 10.224.166.67 with SMTP id l3mr45649843qay.15.1418543029516;
        Sat, 13 Dec 2014 23:43:49 -0800 (PST)
Received: from mail-qa0-f54.google.com (mail-qa0-f54.google.com. [209.85.216.54])
        by mx.google.com with ESMTPSA id n1sm6812223qag.21.2014.12.13.23.43.48
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 13 Dec 2014 23:43:48 -0800 (PST)
Received: by mail-qa0-f54.google.com with SMTP id i13so6954137qae.13
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 23:43:48 -0800 (PST)
X-Received: by 10.224.50.75 with SMTP id y11mr45795148qaf.89.1418543028089;
 Sat, 13 Dec 2014 23:43:48 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.97.68 with HTTP; Sat, 13 Dec 2014 23:43:27 -0800 (PST)
From: Andrew Ash <andrew@andrewash.com>
Date: Sat, 13 Dec 2014 23:43:27 -0800
Message-ID: <CA+-p3AGycxJtiRVD8DmPyD_TX4SMSW-VNR+2Jx+avbOu2bAUyA@mail.gmail.com>
Subject: Governance of the Jenkins whitelist
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc878249bdca050a2849d7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc878249bdca050a2849d7
Content-Type: text/plain; charset=UTF-8

Jenkins is a really valuable tool for increasing quality of incoming
patches to Spark, but I've noticed that there are often a lot of patches
waiting for testing because they haven't been approved for testing.

Certain users can instruct Jenkins to run on a PR, or add other users to a
whitelist. How does governance work for that list of admins?  Meaning who
is currently on it, and what are the requirements to be on that list?

Can I be permissioned to allow Jenkins to run on certain PRs?  I've often
come across well-intentioned PRs that are languishing because Jenkins has
yet to run on them.

Andrew

--047d7bdc878249bdca050a2849d7--

From dev-return-10792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 07:49:48 2014
Return-Path: <dev-return-10792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D2E4F10236
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 07:49:48 +0000 (UTC)
Received: (qmail 76523 invoked by uid 500); 14 Dec 2014 07:49:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76442 invoked by uid 500); 14 Dec 2014 07:49:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76427 invoked by uid 99); 14 Dec 2014 07:49:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:49:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 07:49:41 +0000
Received: by mail-qc0-f171.google.com with SMTP id r5so7377026qcx.16
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 23:49:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=LzC1NVmml6Pn/U4sax+S+yzqoGWES4rmuSt1BsJkX3E=;
        b=Y1TcLfFSN4sMj6F11s+l06rMqnTLUvv2w7met87Lstnfh+7t2K60JWcmYZVENlHmW4
         IhKwfedm4m+08rnb2+Cdpjak9+hpW1mEUj3EBWqRAVYF1FaZKOVR8NUJt9a9rcL/J7te
         6223tYIUrkWaXEoXIO/i89AIjSc8ddsfJw3i9fTsfcp1pzBwcXstj0CLKq0tasIyP1HS
         MO3VIQTDRZJj2oSs5+eq3qfATnvcJS8c+qdbXXTDMHLJCa5FbP5f90KXVxfVL/Tf2bOj
         BPHtu5iatg78a9o99vAeEiIqIGWLydwDL72dmpkDBmhp+d2vVM7zRoaUzkUDZwtaH30X
         LbNg==
X-Gm-Message-State: ALoCoQnrmtQVICSInm/y5IqPU3MsVHmZi86xEAMInEGPSOHoqU5AF2VsAik7jIbUXVanqFtBTjGH
X-Received: by 10.140.83.8 with SMTP id i8mr42910643qgd.103.1418543340177;
        Sat, 13 Dec 2014 23:49:00 -0800 (PST)
Received: from mail-qc0-f180.google.com (mail-qc0-f180.google.com. [209.85.216.180])
        by mx.google.com with ESMTPSA id d9sm6815499qam.26.2014.12.13.23.48.59
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 13 Dec 2014 23:48:59 -0800 (PST)
Received: by mail-qc0-f180.google.com with SMTP id i8so7510671qcq.11
        for <dev@spark.apache.org>; Sat, 13 Dec 2014 23:48:58 -0800 (PST)
X-Received: by 10.224.74.132 with SMTP id u4mr45716506qaj.61.1418543338791;
 Sat, 13 Dec 2014 23:48:58 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.97.68 with HTTP; Sat, 13 Dec 2014 23:48:38 -0800 (PST)
In-Reply-To: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sat, 13 Dec 2014 23:48:38 -0800
Message-ID: <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
Subject: Re: Spark JIRA Report
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0129503eceabbf050a285beb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0129503eceabbf050a285beb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The goal of increasing visibility on open issues is a good one.  How is
this different from just a link to Jira though?  Some might say this adds
noise to the mailing list and doesn't contain any information not already
available in Jira.

The idea seems good but the formatting leaves a little to be desired.  If
you aren't opposed to using HTML, I might suggest this more compact format:

SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044>
Pluggable interface
for shuffles
SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
IndexedRDD, an efficient updatable key-value
SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow
for pluggable
execution contexts in Spark

Andrew

On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> What do y=E2=80=99all think of a report like this emailed out to the dev =
list on a
> monthly basis?
>
> The goal would be to increase visibility into our open issues and encoura=
ge
> developers to tend to our issue tracker more frequently.
>
> Nick
>
> There are 1,236 unresolved issues
> <
> https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+SPARK+AND+resolu=
tion+%3D+Unresolved+ORDER+BY+updated+DESC
> >
> in the Spark project on JIRA.
> Recently Updated Issues
> <
> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%=
20resolution%20%3D%20Unresolved%20ORDER%20BY%20updated%20DESC
> >
> Type Key Priority Summary Last Updated   Bug SPARK-4841
> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch serializer
> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
> <https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
> collect Dec 14, 2014  Bug SPARK-785
> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner no=
t
> invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
> creation on VPC Dec 13, 2014  Improvement SPARK-1555
> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
> ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014
>  Stale
> Issues
> <
> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%=
20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20B=
Y%20updated%20ASC
> >
> Type Key Priority Summary Last Updated   Bug SPARK-560
> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs /
> iterators Oct 22, 2012  New Feature SPARK-540
> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to
> customize
> in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify semantics
> of
> the parallelized closures Oct 22, 2012  New Feature SPARK-609
> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add instructions
> for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism to
> run system management/configuration tasks on all workers Dec 17, 2012
>  Most
> Watched Issues
> <
> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%=
20resolution%20%3D%20Unresolved%20ORDER%20BY%20watchers%20DESC
> >
> Type Key Priority Summary Watchers   New Feature SPARK-3561
> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
> pluggable execution contexts in Spark 75  New Feature SPARK-2365
> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
> an
> efficient updatable key-value store 33  Improvement SPARK-2044
> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
> interface for shuffles 30  New Feature SPARK-1405
> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel
> Latent
> Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature
> SPARK-1406
> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
> evaluation support via MLib 21   Most Voted Issues
> <
> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%=
20resolution%20%3D%20Unresolved%20ORDER%20BY%20votes%20DESC
> >
> Type Key Priority Summary Votes   Bug SPARK-2541
> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mode
> can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
> an
> efficient updatable key-value store 9  Improvement SPARK-3533
> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add
> saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Support
> for ORCFile format 6  New Feature SPARK-1442
> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
> function support 6
> =E2=80=8B
>

--089e0129503eceabbf050a285beb--

From dev-return-10793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 08:21:25 2014
Return-Path: <dev-return-10793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 947BF10280
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 08:21:25 +0000 (UTC)
Received: (qmail 89205 invoked by uid 500); 14 Dec 2014 08:21:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89128 invoked by uid 500); 14 Dec 2014 08:21:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89114 invoked by uid 99); 14 Dec 2014 08:21:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 08:21:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.182 as permitted sender)
Received: from [209.85.223.182] (HELO mail-ie0-f182.google.com) (209.85.223.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 08:20:56 +0000
Received: by mail-ie0-f182.google.com with SMTP id x19so9158199ier.41
        for <dev@spark.apache.org>; Sun, 14 Dec 2014 00:20:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=YIO9Wk04Ou1GYGwRZ+bTaFCtKjPCFj7M5gY2449JqCk=;
        b=zzt7sAMkf1zIfahf6hQoKiONYxDRUQO0ZdW9oJXsFQI/eFQ8SgaqLqmZw5Z8+zCuPf
         FEwcpkrL9HWBnVfTWKvUOrbEzj7ynrw2z9adbUzfttwhfbLQ6MGwKFIkmEyNsW+dJrR5
         MDvzKW9ec7M4KdagydG+zwxHBPE97qCKRxQSxaCYbD6rSLX3jLHz+hiT4T/7uAh6ZpJx
         I36SXoOIkOakl7HYJuJFCpfbMiuizab3A1furu/21RPGF0/M2fj03FdUqYZIVqbdjgkp
         LDR6FQ3AOL16w28WH82KOYu6c7A+KA8OPjxdKgQs8eFi27inrCpzo++bEy68bnNDaYip
         PBzA==
X-Received: by 10.42.25.144 with SMTP id a16mr23579368icc.66.1418545255451;
 Sun, 14 Dec 2014 00:20:55 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 14 Dec 2014 08:20:54 +0000
Message-ID: <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
Subject: Re: Spark JIRA Report
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303ea5820c9614050a28ce6e
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303ea5820c9614050a28ce6e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I formatted this report using Markdown; I'm open to changing the structure
or formatting or reducing the amount of information to make the report more
easily consumable.

Regarding just sending links or whether this would just be mailing list
noise, those are a good questions.

I've sent out links before, but I feel from a UX perspective having the
information right in the email itself makes it frictionless for people to
act on the information. For me, that difference is enough to hook me into
spending a few minutes on JIRA vs. just glossing over an email with a link.

I wonder if that's also the case for others on this list.

If you already spend a good amount of time cleaning up on JIRA, then this
report won't be that relevant to you. But given the number and growth of
open issues on our tracker, I suspect we could do with quite a few more
people chipping in and cleaning up where they can.

That's the real problem that this report is intended to help with.

Nick


On Sun Dec 14 2014 at 2:49:00 AM Andrew Ash <andrew@andrewash.com> wrote:

> The goal of increasing visibility on open issues is a good one.  How is
> this different from just a link to Jira though?  Some might say this adds
> noise to the mailing list and doesn't contain any information not already
> available in Jira.
>
> The idea seems good but the formatting leaves a little to be desired.  If
> you aren't opposed to using HTML, I might suggest this more compact forma=
t:
>
> SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044> Pluggable i=
nterface
> for shuffles
> SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
> IndexedRDD, an efficient updatable key-value
> SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow for p=
luggable
> execution contexts in Spark
>
> Andrew
>
> On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> What do y=E2=80=99all think of a report like this emailed out to the dev=
 list on a
>> monthly basis?
>>
>> The goal would be to increase visibility into our open issues and
>> encourage
>> developers to tend to our issue tracker more frequently.
>>
>> Nick
>>
>> There are 1,236 unresolved issues
>>
> <https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+
>> SPARK+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC>
>
>
>> in the Spark project on JIRA.
>> Recently Updated Issues
>>
> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%
>> 3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%
>> 20BY%20updated%20DESC>
>
>
>> Type Key Priority Summary Last Updated   Bug SPARK-4841
>>
> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch serializer
>
>
>> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
>>
> <https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
>
>
>> collect Dec 14, 2014  Bug SPARK-785
>>
> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner no=
t
>
>
>> invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
>>
> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
>
>
>> creation on VPC Dec 13, 2014  Improvement SPARK-1555
>>
> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
>
>
>> ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014
>>  Stale
>> Issues
>>
> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%
>> 3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%
>> 20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
>
>
>> Type Key Priority Summary Last Updated   Bug SPARK-560
>>
> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs /
>
>
>> iterators Oct 22, 2012  New Feature SPARK-540
>>
> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to
>> customize
>
>
>> in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
>>
> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify semantics
>> of
>
>
>> the parallelized closures Oct 22, 2012  New Feature SPARK-609
>>
> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add instructions
>
>
>> for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
>>
> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism to
>
>
>> run system management/configuration tasks on all workers Dec 17, 2012
>>  Most
>> Watched Issues
>>
> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%
>> 3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%
>> 20BY%20watchers%20DESC>
>
>
>> Type Key Priority Summary Watchers   New Feature SPARK-3561
>>
> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
>
>
>> pluggable execution contexts in Spark 75  New Feature SPARK-2365
>>
> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
>> an
>
>
>> efficient updatable key-value store 33  Improvement SPARK-2044
>>
> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
>
>
>> interface for shuffles 30  New Feature SPARK-1405
>>
> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel
>> Latent
>
>
>> Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature
>> SPARK-1406
>>
> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
>
>
>> evaluation support via MLib 21   Most Voted Issues
>>
> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%
>> 3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%
>> 20BY%20votes%20DESC>
>
>
>> Type Key Priority Summary Votes   Bug SPARK-2541
>>
> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mode
>
>
>> can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
>>
> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
>> an
>
>
>> efficient updatable key-value store 9  Improvement SPARK-3533
>>
> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add
>
>
>> saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
>>
> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Support
>
>
>> for ORCFile format 6  New Feature SPARK-1442
>>
> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
>> function support 6
>> =E2=80=8B
>>
>
>

--20cf303ea5820c9614050a28ce6e--

From dev-return-10794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 16:23:17 2014
Return-Path: <dev-return-10794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4243A10C23
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 16:23:17 +0000 (UTC)
Received: (qmail 73188 invoked by uid 500); 14 Dec 2014 16:23:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73120 invoked by uid 500); 14 Dec 2014 16:23:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73109 invoked by uid 99); 14 Dec 2014 16:23:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 16:23:15 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=SPF_FAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: encountered temporary error during SPF processing of domain of barneystinson@aliyun.com)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 16:22:49 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 3916DDAB424
	for <dev@spark.incubator.apache.org>; Sun, 14 Dec 2014 08:21:58 -0800 (PST)
Date: Sun, 14 Dec 2014 09:21:57 -0700 (MST)
From: WangTaoTheTonic <barneystinson@aliyun.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418574117146-9778.post@n3.nabble.com>
In-Reply-To: <CACdU-dQETzji1n1EA5geJnpbXEvqHNxH-PGn2D-NXSOL63w66w@mail.gmail.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com> <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com> <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com> <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com> <CACdU-dQ4wy6V__Vum_eeywqM1MsDxfA3kSvbK=fwoXvivkAE6A@mail.gmail.com> <CACdU-dQETzji1n1EA5geJnpbXEvqHNxH-PGn2D-NXSOL63w66w@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Jenkins is still not available now as some unit tests(about streaming) failed
all the time. Does it have something to do with this update?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/jenkins-downtime-730-930am-12-12-14-tp9583p9778.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 19:38:21 2014
Return-Path: <dev-return-10795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61B1DF221
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 19:38:21 +0000 (UTC)
Received: (qmail 7662 invoked by uid 500); 14 Dec 2014 19:38:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7582 invoked by uid 500); 14 Dec 2014 19:38:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7558 invoked by uid 99); 14 Dec 2014 19:38:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 19:38:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 19:37:54 +0000
Received: by mail-lb0-f170.google.com with SMTP id 10so8217334lbg.29
        for <dev@spark.apache.org>; Sun, 14 Dec 2014 11:36:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=dcfJGDzH+jGQC4RAFyX0GkDYuHR0KC4TVbpg+83sdLM=;
        b=OoCOQ0aIb4CD8JdKsiw+BdFB/qGGgnq6dnw0Wx6xI8nJ/r+e+kUL/Dj0lIMbcyarx/
         RYbJ/kzMIeTZzSjPsZPLWZXw3ysxJ+rY1jyg+2WjBH0ekHsnJ70z4rqfqxvTGSPuDlk6
         c/B1cDIQ1NvtRiPMjFTvnMy65NcDnGP70sELtH8J84TLgxzQymNTeUmzbMz36du4AewI
         7EMUj9i+G4iJXAvfYJ2ykiTkHlZ75c4DM0qqIohrenoEC5cIsarMVwRO45TMhsFFGtsH
         CsD5i0IJIkcsWXyM3rat35L1XeNrK/VM+GQJfh1XpLzwhJGz6qm+SOnr/I7UxJsHLw4k
         KI8g==
X-Gm-Message-State: ALoCoQmNk+8yNSlTNN20WZV9lzvpkVPiWbvVOKq05BordH/WPEFsYlaHQtqaXkamVqzzQM4vlRu8
X-Received: by 10.152.42.198 with SMTP id q6mr25877118lal.48.1418585807491;
 Sun, 14 Dec 2014 11:36:47 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Sun, 14 Dec 2014 11:36:26 -0800 (PST)
In-Reply-To: <548A491E.2050306@gmail.com>
References: <548A491E.2050306@gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Sun, 14 Dec 2014 11:36:26 -0800
Message-ID: <CAAswR-76=BdDhsPdkVBOzZZoicajwCjPKthufWuc2fmS_hEzGg@mail.gmail.com>
Subject: Re: Is there any document to explain how to build the hive jars for spark?
To: Yi Tian <tianyi.asiainfo@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c34dae23abe5050a323fcb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c34dae23abe5050a323fcb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The modified version of hive can be found here:
https://github.com/pwendell/hive

On Thu, Dec 11, 2014 at 5:47 PM, Yi Tian <tianyi.asiainfo@gmail.com> wrote:
>
> Hi, all
>
> We found some bugs in hive-0.12, but we could not wait for hive community
> fixing them.
>
> We want to fix these bugs in our lab and build a new release which could
> be recognized by spark.
>
> As we know, spark depends on a special release of hive, like:
>
> |<dependency>
>   <groupId>org.spark-project.hive</groupId>
>   <artifactId>hive-metastore</artifactId>
>   <version>${hive.version}</version>
> </dependency>
> |
>
> The different between |org.spark-project.hive| and |org.apache.hive| was
> described by Patrick:
>
> |There are two differences:
>
> 1. We publish hive with a shaded protobuf dependency to avoid
> conflicts with some Hadoop versions.
> 2. We publish a proper hive-exec jar that only includes hive packages.
> The upstream version of hive-exec bundles a bunch of other random
> dependencies in it which makes it really hard for third-party projects
> to use it.
> |
>
> Is there any document to guide us how to build the hive jars for spark?
>
> Any help would be greatly appreciated.
>
> =E2=80=8B
>

--001a11c34dae23abe5050a323fcb--

From dev-return-10796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 19:39:31 2014
Return-Path: <dev-return-10796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E83A1F229
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 19:39:30 +0000 (UTC)
Received: (qmail 10023 invoked by uid 500); 14 Dec 2014 19:39:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9950 invoked by uid 500); 14 Dec 2014 19:39:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9939 invoked by uid 99); 14 Dec 2014 19:39:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 19:39:28 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.47 as permitted sender)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 19:39:03 +0000
Received: by mail-la0-f47.google.com with SMTP id hz20so8243469lab.6
        for <dev@spark.incubator.apache.org>; Sun, 14 Dec 2014 11:39:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=TGn8NYKLZkWjcd31L6dgIfx3QZY50fHmFZGRefq5XIs=;
        b=lTZvrIED36rkblDP5sQ7uLfUDbpQ/KQ0PaxHNNi1uyh4MPzb8HDVuqTlQUCnMp81sD
         jx8v2GTylt/RzALZ1gy8EsM84dBKNMlyqBsanKNqWo0JytA6SbX6vMwlV5cGOVq1+uxB
         hcqhF9081buPxIVWJ3f+F7NC54sFwpI1oh7BocIEIs64qfW5D8xkJ4ujXJptqgfEIBbr
         LOQCd1VeVZv9J8WgYz4PwPHH+H+5eZD0N2RgA3DFKH0kcpOQHlginInaOqCaYSp/I3gz
         86OUvIgdARcALTr2Vk7PZ57tGuQEXlS2+DLBc5+Yd0kpFosLwVM9FcX+SaJbwdvnyT7x
         W/vA==
X-Gm-Message-State: ALoCoQlG+cTSsutEJfQYGcD9MEp7tcfrhsJtRGv7vSKdK6eVlTNxlMwBjJHIirC1OsWsdvuRNbWn
X-Received: by 10.112.222.135 with SMTP id qm7mr20763783lbc.19.1418585940495;
 Sun, 14 Dec 2014 11:39:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Sun, 14 Dec 2014 11:38:40 -0800 (PST)
In-Reply-To: <1418574117146-9778.post@n3.nabble.com>
References: <CACdU-dRJTpFhMGa4_=FLP9Q0VwUZ92MubZSV9w-VTLevfnmuxA@mail.gmail.com>
 <CACdU-dQ+JA+BQbXuz+tzquA81+JR=Gf8VspFOVNiUF1DMmkOTQ@mail.gmail.com>
 <CACdU-dR+bxhdd=4go0tYjCGS+uek50864H+dcKGR-MUYkV0fbg@mail.gmail.com>
 <CACdU-dTiH+6SSU-4EuWLkMsZ36f51jrDxT8OW_tXWCjPkG=xdQ@mail.gmail.com>
 <CACdU-dQ4wy6V__Vum_eeywqM1MsDxfA3kSvbK=fwoXvivkAE6A@mail.gmail.com>
 <CACdU-dQETzji1n1EA5geJnpbXEvqHNxH-PGn2D-NXSOL63w66w@mail.gmail.com> <1418574117146-9778.post@n3.nabble.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Sun, 14 Dec 2014 11:38:40 -0800
Message-ID: <CACdU-dQXiu8zBuo1D+grHNH_TeEf6HqVOxBzrUhms9a2miOizg@mail.gmail.com>
Subject: Re: jenkins downtime: 730-930am, 12/12/14
To: WangTaoTheTonic <barneystinson@aliyun.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a1134d228111df2050a3247b0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d228111df2050a3247b0
Content-Type: text/plain; charset=UTF-8

josh rosen has this PR open to address the streaming test failures:

https://github.com/apache/spark/pull/3687

On Sun, Dec 14, 2014 at 8:21 AM, WangTaoTheTonic <barneystinson@aliyun.com>
wrote:

> Jenkins is still not available now as some unit tests(about streaming)
> failed
> all the time. Does it have something to do with this update?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/jenkins-downtime-730-930am-12-12-14-tp9583p9778.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1134d228111df2050a3247b0--

From dev-return-10797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 20:29:57 2014
Return-Path: <dev-return-10797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BDE75F34A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 20:29:57 +0000 (UTC)
Received: (qmail 54697 invoked by uid 500); 14 Dec 2014 20:29:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54607 invoked by uid 500); 14 Dec 2014 20:29:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54595 invoked by uid 99); 14 Dec 2014 20:29:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 20:29:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 20:29:51 +0000
Received: by mail-ig0-f182.google.com with SMTP id hn15so4094991igb.3
        for <dev@spark.apache.org>; Sun, 14 Dec 2014 12:28:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=guFohIrwJDueK1V4dyaQZmHIBPGLvAN4q+pmW+85N24=;
        b=Gc44Xc23y5kyO469qqLB1trEuJVOkTVp0YZMOnVc8VdFYqxEuA7Wf/jHkvKTp4a/Iq
         BGHUK1XJh2kw6Fzuylq7ACGYIrnWwjU5Oh2WhuvpVTD8zgg/+m91yZq4J1lzPwYAVDgd
         kxOtK02qieFI99JskoI5Xa+Bi5A7cGpo46Q5DBdwo38NauUZdKxOccm6GKwwr1y7a/2H
         q3YRcQeLoDQ04U7H4O6xrGaWv5g8EtybW6HDuEstMx+VzJq19jD/7Ivjf2sp2Ps2Fyr5
         i6lDC2Abs4cA7SBT4FV5wMONl0vu710jufpIfQK6gyCBq1jj5E/ly8HLX3ETi3MVm2gc
         xMBQ==
X-Received: by 10.43.89.68 with SMTP id bd4mr24407695icc.63.1418588926013;
 Sun, 14 Dec 2014 12:28:46 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com> <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 14 Dec 2014 20:28:45 +0000
Message-ID: <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
Subject: Re: Spark JIRA Report
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec517c6c004783e050a32f93b
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec517c6c004783e050a32f93b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Taking after Andrew=E2=80=99s suggestion, perhaps the report can just focus=
 on
Stale issues (no updates in > 90 days), since those are probably the
easiest to act on.

For example:
Stale Issues
<https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY=
%20updated%20ASC>

   - [Oct 22, 2012] SPARK-560
   <https://issues.apache.org/jira/browse/SPARK-560>: Specialize RDDs /
   iterators
   - [Oct 22, 2012] SPARK-540
   <https://issues.apache.org/jira/browse/SPARK-540>: Add API to customize
   in-memory representation of RDDs
   - [Oct 22, 2012] SPARK-573
   <https://issues.apache.org/jira/browse/SPARK-573>: Clarify semantics of
   the parallelized closures
   - [Nov 06, 2012] SPARK-609
   <https://issues.apache.org/jira/browse/SPARK-609>: Add instructions for
   enabling Akka debug logging
   - [Dec 17, 2012] SPARK-636
   <https://issues.apache.org/jira/browse/SPARK-636>: Add mechanism to run
   system management/configuration tasks on all workers

Andrew,

Does that seem more useful?

Nick
=E2=80=8B

On Sun Dec 14 2014 at 3:20:54 AM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> I formatted this report using Markdown; I'm open to changing the structur=
e
> or formatting or reducing the amount of information to make the report mo=
re
> easily consumable.
>
> Regarding just sending links or whether this would just be mailing list
> noise, those are a good questions.
>
> I've sent out links before, but I feel from a UX perspective having the
> information right in the email itself makes it frictionless for people to
> act on the information. For me, that difference is enough to hook me into
> spending a few minutes on JIRA vs. just glossing over an email with a lin=
k.
>
> I wonder if that's also the case for others on this list.
>
> If you already spend a good amount of time cleaning up on JIRA, then this
> report won't be that relevant to you. But given the number and growth of
> open issues on our tracker, I suspect we could do with quite a few more
> people chipping in and cleaning up where they can.
>
> That's the real problem that this report is intended to help with.
>
> Nick
>
>
>
> On Sun Dec 14 2014 at 2:49:00 AM Andrew Ash <andrew@andrewash.com> wrote:
>
>> The goal of increasing visibility on open issues is a good one.  How is
>> this different from just a link to Jira though?  Some might say this add=
s
>> noise to the mailing list and doesn't contain any information not alread=
y
>> available in Jira.
>>
>> The idea seems good but the formatting leaves a little to be desired.  I=
f
>> you aren't opposed to using HTML, I might suggest this more compact form=
at:
>>
>> SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044> Pluggable =
interface
>> for shuffles
>> SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
>> IndexedRDD, an efficient updatable key-value
>> SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow for =
pluggable
>> execution contexts in Spark
>>
>> Andrew
>>
>> On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> What do y=E2=80=99all think of a report like this emailed out to the de=
v list on
>>> a
>>> monthly basis?
>>>
>>> The goal would be to increase visibility into our open issues and
>>> encourage
>>> developers to tend to our issue tracker more frequently.
>>>
>>> Nick
>>>
>>> There are 1,236 unresolved issues
>>>
>> <https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+SPAR
>>> K+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC>
>>
>>
>>> in the Spark project on JIRA.
>>> Recently Updated Issues
>>>
>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>> 20updated%20DESC>
>>
>>
>>> Type Key Priority Summary Last Updated   Bug SPARK-4841
>>>
>> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch serialize=
r
>>
>>
>>> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
>>>
>> <https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
>>
>>
>>> collect Dec 14, 2014  Bug SPARK-785
>>>
>> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner
>>> not
>>
>>
>>> invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
>>>
>> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
>>
>>
>>> creation on VPC Dec 13, 2014  Improvement SPARK-1555
>>>
>> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
>>
>>
>>> ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014
>>>  Stale
>>> Issues
>>>
>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20update
>>> d%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
>>
>>
>>> Type Key Priority Summary Last Updated   Bug SPARK-560
>>>
>> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs /
>>
>>
>>> iterators Oct 22, 2012  New Feature SPARK-540
>>>
>> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to
>>> customize
>>
>>
>>> in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
>>>
>> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify semantics
>>> of
>>
>>
>>> the parallelized closures Oct 22, 2012  New Feature SPARK-609
>>>
>> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add instructions
>>
>>
>>> for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
>>>
>> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism to
>>
>>
>>> run system management/configuration tasks on all workers Dec 17, 2012
>>>  Most
>>> Watched Issues
>>>
>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>> 20watchers%20DESC>
>>
>>
>>> Type Key Priority Summary Watchers   New Feature SPARK-3561
>>>
>> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
>>
>>
>>> pluggable execution contexts in Spark 75  New Feature SPARK-2365
>>>
>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
>>> an
>>
>>
>>> efficient updatable key-value store 33  Improvement SPARK-2044
>>>
>> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
>>
>>
>>> interface for shuffles 30  New Feature SPARK-1405
>>>
>> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel
>>> Latent
>>
>>
>>> Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature
>>> SPARK-1406
>>>
>> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
>>
>>
>>> evaluation support via MLib 21   Most Voted Issues
>>>
>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>> 20votes%20DESC>
>>
>>
>>> Type Key Priority Summary Votes   Bug SPARK-2541
>>>
>> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mode
>>
>>
>>> can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
>>>
>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD,
>>> an
>>
>>
>>> efficient updatable key-value store 9  Improvement SPARK-3533
>>>
>> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add
>>
>>
>>> saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
>>>
>> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Support
>>
>>
>>> for ORCFile format 6  New Feature SPARK-1442
>>>
>> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
>>> function support 6
>>> =E2=80=8B
>>>
>>
>>

--bcaec517c6c004783e050a32f93b--

From dev-return-10798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 14 20:42:47 2014
Return-Path: <dev-return-10798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7C7AF403
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 14 Dec 2014 20:42:47 +0000 (UTC)
Received: (qmail 71313 invoked by uid 500); 14 Dec 2014 20:42:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71236 invoked by uid 500); 14 Dec 2014 20:42:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71223 invoked by uid 99); 14 Dec 2014 20:42:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 20:42:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 14 Dec 2014 20:42:19 +0000
Received: by mail-wi0-f182.google.com with SMTP id h11so7183289wiw.3
        for <dev@spark.apache.org>; Sun, 14 Dec 2014 12:41:59 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=f+meq1D274FPLugeowhGu7ur8C/ckdzsE51v5kdmJt4=;
        b=l1kv63yfewn8YpqutHFsfzTbH+CaQEvOcEU0R4k7Qig41OcIvFVzCCq7i6e+88NJkm
         cDqs9UEN3u6+cvcxILUbqlFce1H8QqJb+lheXtyiI2ktvJe0tsQjOxAiilq15DsNQpWc
         FWMsC4Dj9ruBgKgJpKaHrkf3Lg6a9in7Kb15nrA4H7GyZJE7+8kr1KuRYYfLKbL68Hq7
         dcv2gjaC18Em2l7CfUbAnqBXIuXhKpOJaHbMu6oDU6RwhqUsB+plolQ9IUa9u5TcrdE4
         FYmQ2m9XyeOuyqjcUpDw0x7oyRdyc3rP8x9V13ltBFGGQ8Sndngw/fAA9DVXXoBjvIKr
         X6kA==
X-Gm-Message-State: ALoCoQlp8ZwsxMtQH+V5Ajvaei45tqMDBttnUA2ra3PgnYPFKDPRgmEWMjvie3/BY9+fdEd0g/Ho
MIME-Version: 1.0
X-Received: by 10.180.9.229 with SMTP id d5mr26152283wib.22.1418589719058;
 Sun, 14 Dec 2014 12:41:59 -0800 (PST)
Received: by 10.216.134.193 with HTTP; Sun, 14 Dec 2014 12:41:58 -0800 (PST)
X-Originating-IP: [209.150.41.132]
Date: Sun, 14 Dec 2014 15:41:58 -0500
Message-ID: <CANx3uAhx3M5KHLcsHzmi_Qkm6p5-S=VsO7atWKO50vk681vEXA@mail.gmail.com>
Subject: spark kafka batch integration
From: Koert Kuipers <koert@tresata.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2b9e0496f7c050a332859
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b9e0496f7c050a332859
Content-Type: text/plain; charset=UTF-8

hello all,
we at tresata wrote a library to provide for batch integration between
spark and kafka (distributed write of rdd to kafa, distributed read of rdd
from kafka). our main use cases are (in lambda architecture jargon):
* period appends to the immutable master dataset on hdfs from kafka using
spark
* make non-streaming data available in kafka with periodic data drops from
hdfs using spark. this is to facilitate merging the speed and batch layer
in spark-streaming
* distributed writes from spark-streaming

see here:
https://github.com/tresata/spark-kafka

best,
koert

--001a11c2b9e0496f7c050a332859--

From dev-return-10799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 09:36:27 2014
Return-Path: <dev-return-10799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33CDB95B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 09:36:27 +0000 (UTC)
Received: (qmail 99778 invoked by uid 500); 15 Dec 2014 09:36:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99705 invoked by uid 500); 15 Dec 2014 09:36:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99693 invoked by uid 99); 15 Dec 2014 09:36:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 09:36:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 09:35:58 +0000
Received: by mail-ig0-f172.google.com with SMTP id hl2so4662876igb.11
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 01:33:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=LQ0eZE1vlWd3w5hvlvTCEh8Ms2QfwPGBnkkfiOiiKvc=;
        b=uBDsQ9t+q8N5M2UYi+1bo4ogQ3Z1yMsJMUrFsX9qvB+S+tYfp9AG0ZD5SYtLcSpfN9
         dfe8bTkh+Woou90ALfjg5MMCBzqxgZm0dXx3GiUdoLmQst/J71n45lj14gmKqFJcu9In
         blnzITuxnCmx0iPyoFX2xnwKFoxw6P2nN3Qw5T1h/DFwS7bfPCWl0eBlLw17BSMM8wuE
         uwhSl24sUIJGncbL5PBnfohemKAAqpkg21C2Vr0jgFWkL6yKf8rm8eCi0NRcz0Cq2iXB
         fS/WyT9wUZRJ7Lt23YIzhrg62C/uago1Sd6qogQU+YOMSBAhrq924Fu4HPtU3O8Nfdt4
         FKbQ==
X-Received: by 10.50.66.200 with SMTP id h8mr16900206igt.20.1418636022238;
 Mon, 15 Dec 2014 01:33:42 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 15 Dec 2014 09:33:41 +0000
Message-ID: <CAOhmDzdQvt-MfUJAef57cCRTophocTDGXfYyOe6KD7bqWDC-jg@mail.gmail.com>
Subject: Archiving XML test reports for analysis
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1a662bd5c4050a3df04a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1a662bd5c4050a3df04a
Content-Type: text/plain; charset=UTF-8

Every time we run a test cycle on our Jenkins cluster, we generate hundreds
of XML reports covering all the tests we have (e.g.
`streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogSuite.xml`).

These reports contain interesting information about whether tests succeeded
or failed, and how long they took to complete. There is also detailed
information about the environment they ran in.

It might be valuable to have a window into all these reports across all
Jenkins builds and across all time, and use that to track basic statistics
about our tests. That could give us basic insight into what tests are flaky
or slow, and perhaps drive other improvements to our testing infrastructure
that we can't see just yet.

Do people think that would be valuable? Do we already have something like
this?

I'm thinking for starters it might be cool if we automatically uploaded all
the XML test reports from the Master and the Pull Request builders to an S3
bucket and just opened it up for the dev community to analyze.

Nick

--047d7bdc1a662bd5c4050a3df04a--

From dev-return-10800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 15:52:54 2014
Return-Path: <dev-return-10800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44E441014A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 15:52:54 +0000 (UTC)
Received: (qmail 60222 invoked by uid 500); 15 Dec 2014 15:52:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60165 invoked by uid 500); 15 Dec 2014 15:52:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60154 invoked by uid 99); 15 Dec 2014 15:52:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 15:52:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 15:52:26 +0000
Received: by mail-oi0-f54.google.com with SMTP id u20so8146713oif.41
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 07:51:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=mYbfu1Qc+MyklNbAiGv220XNDA9NRA6+ALG3GFONcXs=;
        b=O3BfJ++luBOaEwmey7YLWNU97plsxDav0iGmrBccpuz1ST4I1eyr7lNY86cEYkdP7B
         pP7Y9O9tMgITRDeFUh2libT1G2zh4c8lXbpQmt+3StTc/fU4lGbIePIaLAE6cOofil3H
         A5e6scIGqhI8/SMtOYWdIUWcxUo2PjiRPx4TnHZwbuFCL9yBTZpqQOoU3VP0miRRXDIn
         mWlZksuplEDy6zPGQuaITwgM6ugib4CxjFZySun/Jqhq0SQGuH23sOE7FxKX4dtLR1cV
         fCFUFbFSvy9xyxtOfSeRk2ZQT4gx5pLFDyX8PnlzqV3/xv/5hrooRrPsg5q4XnUD2mBc
         46Hg==
X-Gm-Message-State: ALoCoQlYHSxQOfBSFZL/KzLqdqCI8oJoi4KeQ9/MPhYdmsp/HFUGNKYO8KFVA1NJf4poKLS64/Np
MIME-Version: 1.0
X-Received: by 10.202.172.5 with SMTP id v5mr18288952oie.48.1418658678926;
 Mon, 15 Dec 2014 07:51:18 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Mon, 15 Dec 2014 07:51:18 -0800 (PST)
In-Reply-To: <CANx3uAhx3M5KHLcsHzmi_Qkm6p5-S=VsO7atWKO50vk681vEXA@mail.gmail.com>
References: <CANx3uAhx3M5KHLcsHzmi_Qkm6p5-S=VsO7atWKO50vk681vEXA@mail.gmail.com>
Date: Mon, 15 Dec 2014 09:51:18 -0600
Message-ID: <CAKWX9VUSzedkiyyUZhKDVwGhJbn78aYzm2EzpLGYCcEo3ruZwA@mail.gmail.com>
Subject: Re: spark kafka batch integration
From: Cody Koeninger <cody@koeninger.org>
To: Koert Kuipers <koert@tresata.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113c38009d85b0050a4336bf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113c38009d85b0050a4336bf
Content-Type: text/plain; charset=UTF-8

For an alternative take on a similar idea, see

https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka/src/main/scala/org/apache/spark/rdd/kafka

An advantage of the approach I'm taking is that the lower and upper offsets
of the RDD are known in advance, so it's deterministic.

I haven't had a need to write to kafka from spark yet, so that's an obvious
advantage of your library.

I think the existing kafka dstream is inadequate for a number of use cases,
and would really like to see some combination of these approaches make it
into the spark codebase.


On Sun, Dec 14, 2014 at 2:41 PM, Koert Kuipers <koert@tresata.com> wrote:
>
> hello all,
> we at tresata wrote a library to provide for batch integration between
> spark and kafka (distributed write of rdd to kafa, distributed read of rdd
> from kafka). our main use cases are (in lambda architecture jargon):
> * period appends to the immutable master dataset on hdfs from kafka using
> spark
> * make non-streaming data available in kafka with periodic data drops from
> hdfs using spark. this is to facilitate merging the speed and batch layer
> in spark-streaming
> * distributed writes from spark-streaming
>
> see here:
> https://github.com/tresata/spark-kafka
>
> best,
> koert
>

--001a113c38009d85b0050a4336bf--

From dev-return-10801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 16:26:15 2014
Return-Path: <dev-return-10801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CBE110339
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 16:26:15 +0000 (UTC)
Received: (qmail 35774 invoked by uid 500); 15 Dec 2014 16:26:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35695 invoked by uid 500); 15 Dec 2014 16:26:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35452 invoked by uid 99); 15 Dec 2014 16:26:14 -0000
Received: from mail-relay.apache.org (HELO mail-relay.apache.org) (140.211.11.15)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 16:26:13 +0000
Received: from mail-ie0-f181.google.com (mail-ie0-f181.google.com [209.85.223.181])
	by mail-relay.apache.org (ASF Mail Server at mail-relay.apache.org) with ESMTPSA id C48AF1A00D1
	for <dev@spark.apache.org>; Mon, 15 Dec 2014 16:26:13 +0000 (UTC)
Received: by mail-ie0-f181.google.com with SMTP id tp5so10729717ieb.26
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 08:26:13 -0800 (PST)
MIME-Version: 1.0
X-Received: by 10.50.114.97 with SMTP id jf1mr18328344igb.29.1418660773329;
 Mon, 15 Dec 2014 08:26:13 -0800 (PST)
Received: by 10.50.228.161 with HTTP; Mon, 15 Dec 2014 08:26:13 -0800 (PST)
Date: Mon, 15 Dec 2014 11:26:13 -0500
Message-ID: <CAPqz87rRLd-P6t7+uZZJp9u6Uzn0irf5Z=i9-P-hEO35Y_0OAQ@mail.gmail.com>
Subject: Spark Web Site
From: Pei Chen <chenpei@apache.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a94d6738564050a43b335

--047d7b3a94d6738564050a43b335
Content-Type: text/plain; charset=UTF-8

Hi Spark Dev,
The cTAKES community was looking at revamping their web site and really
liked the clean look of the Spark one.  Is it just using Bootstrap and
static html pages checked into SVN?  Or are you using the Apache CMS
somehow?  Mind if we borrow the layout?

--Pei

--047d7b3a94d6738564050a43b335--

From dev-return-10802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 16:49:13 2014
Return-Path: <dev-return-10802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D9FE710489
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 16:49:13 +0000 (UTC)
Received: (qmail 83516 invoked by uid 500); 15 Dec 2014 16:49:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83441 invoked by uid 500); 15 Dec 2014 16:49:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83173 invoked by uid 99); 15 Dec 2014 16:49:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 16:49:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.175 as permitted sender)
Received: from [209.85.217.175] (HELO mail-lb0-f175.google.com) (209.85.217.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 16:48:45 +0000
Received: by mail-lb0-f175.google.com with SMTP id u10so9283015lbd.6
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 08:48:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=vgeuwPjRTbGchyMI6U8x9UFm7ABvokQmlNAqmaGnN3k=;
        b=VNG3ynrSiKvmZuoh8ShEqVApMKzw6+s3DSqqABMYi8m/Tk/0UGT0turkh9zv9eXapO
         +v9jbK1M6EJzF4A3IrBUPEacEusUcg9eOuWsaKSBZU+n3yKye89PXHGTsMH5+pNTClz4
         fv4OotbGJTX3Dtxc0oaPCU/qqLeGnRxOFFsxtyOEajsY6yk7HS9zHvWuNWdGJ7mQX13l
         mvwvZQoJ2RM9JO5DhXKNT7/0PsESwP6qLqgWN+9HCSUkqJtlFzToDT49KVnQCg7MAG32
         YVfN776Uquh+Lb7Z1QI1cBFAioo0FrrUbk0m5XCsySUaFrGNttimN0p7edpbhZfPHGvW
         1XNA==
X-Gm-Message-State: ALoCoQnvaObmn0hf950ti/RVEHHz+N4JqR/FATurMCyGbaYPXU+TjOkU1BGHjh3FYsTj5CJ0sKTC
X-Received: by 10.112.136.69 with SMTP id py5mr17442316lbb.56.1418662123492;
 Mon, 15 Dec 2014 08:48:43 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Mon, 15 Dec 2014 08:48:23 -0800 (PST)
In-Reply-To: <CAOhmDzdQvt-MfUJAef57cCRTophocTDGXfYyOe6KD7bqWDC-jg@mail.gmail.com>
References: <CAOhmDzdQvt-MfUJAef57cCRTophocTDGXfYyOe6KD7bqWDC-jg@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 15 Dec 2014 08:48:23 -0800
Message-ID: <CACdU-dQyY1HK-pkbKQsngoEiqLejbEnaZSZjTW6wbo39_1xANg@mail.gmail.com>
Subject: Re: Archiving XML test reports for analysis
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01183b86ed6740050a440373
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01183b86ed6740050a440373
Content-Type: text/plain; charset=UTF-8

right now, the following logs are archived on to the master:

  local log_files=$(
    find .\
      -name "unit-tests.log" -o\
      -path "./sql/hive/target/HiveCompatibilitySuite.failed" -o\
      -path "./sql/hive/target/HiveCompatibilitySuite.hiveFailed" -o\
      -path "./sql/hive/target/HiveCompatibilitySuite.wrong"
  )

regarding dumping stuff to S3 -- thankfully, since we're not looking at a
lot of disk usage, i don't see a problem w/this.  we could tar/zip up the
XML for each build and just dump it there.

what builds are we thinking about?  spark pull request builder?  what
others?

On Mon, Dec 15, 2014 at 1:33 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:
>
> Every time we run a test cycle on our Jenkins cluster, we generate hundreds
> of XML reports covering all the tests we have (e.g.
>
> `streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogSuite.xml`).
>
> These reports contain interesting information about whether tests succeeded
> or failed, and how long they took to complete. There is also detailed
> information about the environment they ran in.
>
> It might be valuable to have a window into all these reports across all
> Jenkins builds and across all time, and use that to track basic statistics
> about our tests. That could give us basic insight into what tests are flaky
> or slow, and perhaps drive other improvements to our testing infrastructure
> that we can't see just yet.
>
> Do people think that would be valuable? Do we already have something like
> this?
>
> I'm thinking for starters it might be cool if we automatically uploaded all
> the XML test reports from the Master and the Pull Request builders to an S3
> bucket and just opened it up for the dev community to analyze.
>
> Nick
>

--089e01183b86ed6740050a440373--

From dev-return-10803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 17:35:25 2014
Return-Path: <dev-return-10803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF9271098D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 17:35:25 +0000 (UTC)
Received: (qmail 70275 invoked by uid 500); 15 Dec 2014 17:35:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70205 invoked by uid 500); 15 Dec 2014 17:35:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70188 invoked by uid 99); 15 Dec 2014 17:35:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 17:35:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 17:34:56 +0000
Received: by mail-pa0-f44.google.com with SMTP id et14so12244028pad.17
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 09:34:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=B28qibkvQmvSKhSnKggr/NMhSU6JNIiY80QKSMu/NXM=;
        b=v/UWIuUvCIj03rEyf4/WBYX4b+RYSKlsU838xfhgXZIJnbA47MniqeZFbeLwQsx/ew
         5d9DpNBe1k5py859TCAPlYT4r8hEaUsSHwWNFnlOjQQQjq0x+g8wQE7lPrtu3wT+jXXG
         kq3AYWzPsCMCplBgja+KYZDvGFnFVV+nIltRPdW6ku+LdH/rJzaxpH4+Y1lkR4QtguCH
         D7aw8whgctk+9vw9/ogO8mpXqN5NVWpOG0pqHVbIIVB7+/Ko4qhyGbt+tEW0FX2VCbgm
         7kRuH79ELTDUg4NffBUYCpVF968Lsg77aWILl2hmcmBZW4DaxakURoT4+4JuITG9eGDr
         4JCg==
X-Received: by 10.66.66.68 with SMTP id d4mr50628599pat.79.1418664894803;
        Mon, 15 Dec 2014 09:34:54 -0800 (PST)
Received: from [192.168.1.100] (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id ej12sm9925627pac.28.2014.12.15.09.34.53
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 15 Dec 2014 09:34:53 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: Spark Web Site
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAPqz87rRLd-P6t7+uZZJp9u6Uzn0irf5Z=i9-P-hEO35Y_0OAQ@mail.gmail.com>
Date: Mon, 15 Dec 2014 09:34:52 -0800
Cc: dev@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <0C3C57FB-8D2D-45AA-919E-2C47F0B19E32@gmail.com>
References: <CAPqz87rRLd-P6t7+uZZJp9u6Uzn0irf5Z=i9-P-hEO35Y_0OAQ@mail.gmail.com>
To: Pei Chen <chenpei@apache.org>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

It's just Bootstrap checked into SVN and built using Jekyll. You can =
check out the raw source files from SVN from =
https://svn.apache.org/repos/asf/spark. IMO it's fine if you guys use =
the layout, but just make sure it doesn't look exactly the same because =
otherwise both sites will look like they're copied from some standard =
template. We used a slightly customized Bootstrap theme for it.

Matei

> On Dec 15, 2014, at 8:26 AM, Pei Chen <chenpei@apache.org> wrote:
>=20
> Hi Spark Dev,
> The cTAKES community was looking at revamping their web site and =
really
> liked the clean look of the Spark one.  Is it just using Bootstrap and
> static html pages checked into SVN?  Or are you using the Apache CMS
> somehow?  Mind if we borrow the layout?
>=20
> --Pei


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 17:55:49 2014
Return-Path: <dev-return-10804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2548C10AAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 17:55:49 +0000 (UTC)
Received: (qmail 33737 invoked by uid 500); 15 Dec 2014 17:55:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33657 invoked by uid 500); 15 Dec 2014 17:55:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33644 invoked by uid 99); 15 Dec 2014 17:55:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 17:55:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 17:55:42 +0000
Received: by mail-qc0-f169.google.com with SMTP id w7so9084109qcr.14
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 09:55:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=RbAIcrEPbza2hgTJeII1cBpK7cOld0Amwli8JDSCwuU=;
        b=OnzL/q/002Fa01nLFV/PhMRsYFirBno3k1ht6Wu533mIrpg6CpkNJm8UJ+P6S31yh7
         LPMp7YU2FavurxUuIu3qtlorYl2sTP27yN9crS4eNepWkfpCIzBDdN8Q6m5C0l40hWPU
         ofH9z0EgQUHKpyxLuA5Z/pndmytM1EPSNKl00TrUb9rTAgiT6YtXsdx1eyGU6F54xgbG
         iTRA3Fr40pYB1kCu1C1KRhMAPlqQT1OicAD3wRTkC/ZrCDV9InCptmfYwfOP56j0khp6
         2H6s3zNGW0n/X4Hehs28J3VNCLefN5LLvCxhX/xW3XXUoLP4hJJA7LguVwmCFi0LlNhU
         zO6A==
X-Gm-Message-State: ALoCoQmEN/k7VQ41ACN3VdeS3nhblsYjSv+Gh4PlihFRr6LyqWbfUH7y/A8hvrks1KhdLaMo1NgD
X-Received: by 10.140.109.132 with SMTP id l4mr54372368qgf.91.1418666106258;
        Mon, 15 Dec 2014 09:55:06 -0800 (PST)
Received: from mail-qg0-f51.google.com (mail-qg0-f51.google.com. [209.85.192.51])
        by mx.google.com with ESMTPSA id 77sm6662190qgx.43.2014.12.15.09.55.04
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 15 Dec 2014 09:55:04 -0800 (PST)
Received: by mail-qg0-f51.google.com with SMTP id e89so8725355qgf.24
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 09:55:04 -0800 (PST)
X-Received: by 10.224.74.132 with SMTP id u4mr57729971qaj.61.1418666104345;
 Mon, 15 Dec 2014 09:55:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.97.68 with HTTP; Mon, 15 Dec 2014 09:54:44 -0800 (PST)
In-Reply-To: <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
 <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com> <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 15 Dec 2014 09:54:44 -0800
Message-ID: <CA+-p3AHqvVzuf4HqftTu8jQQSpk7TEFUp9sDbqw48hNAnO2AoA@mail.gmail.com>
Subject: Re: Spark JIRA Report
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0129503e345aab050a44f1ea
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0129503e345aab050a44f1ea
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Nick,

Putting the N most stale issues into a report like your latest one does
seem like a good way to tackle the wall of text effect that I'm worried
about.

On Sun, Dec 14, 2014 at 12:28 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Taking after Andrew=E2=80=99s suggestion, perhaps the report can just foc=
us on
> Stale issues (no updates in > 90 days), since those are probably the
> easiest to act on.
>
> For example:
> Stale Issues
> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND=
%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20=
BY%20updated%20ASC>
>
>    - [Oct 22, 2012] SPARK-560
>    <https://issues.apache.org/jira/browse/SPARK-560>: Specialize RDDs /
>    iterators
>    - [Oct 22, 2012] SPARK-540
>    <https://issues.apache.org/jira/browse/SPARK-540>: Add API to
>    customize in-memory representation of RDDs
>    - [Oct 22, 2012] SPARK-573
>    <https://issues.apache.org/jira/browse/SPARK-573>: Clarify semantics
>    of the parallelized closures
>    - [Nov 06, 2012] SPARK-609
>    <https://issues.apache.org/jira/browse/SPARK-609>: Add instructions
>    for enabling Akka debug logging
>    - [Dec 17, 2012] SPARK-636
>    <https://issues.apache.org/jira/browse/SPARK-636>: Add mechanism to
>    run system management/configuration tasks on all workers
>
> Andrew,
>
> Does that seem more useful?
>
> Nick
> =E2=80=8B
>
> On Sun Dec 14 2014 at 3:20:54 AM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> I formatted this report using Markdown; I'm open to changing the
>> structure or formatting or reducing the amount of information to make th=
e
>> report more easily consumable.
>>
>> Regarding just sending links or whether this would just be mailing list
>> noise, those are a good questions.
>>
>> I've sent out links before, but I feel from a UX perspective having the
>> information right in the email itself makes it frictionless for people t=
o
>> act on the information. For me, that difference is enough to hook me int=
o
>> spending a few minutes on JIRA vs. just glossing over an email with a li=
nk.
>>
>> I wonder if that's also the case for others on this list.
>>
>> If you already spend a good amount of time cleaning up on JIRA, then thi=
s
>> report won't be that relevant to you. But given the number and growth of
>> open issues on our tracker, I suspect we could do with quite a few more
>> people chipping in and cleaning up where they can.
>>
>> That's the real problem that this report is intended to help with.
>>
>> Nick
>>
>>
>>
>> On Sun Dec 14 2014 at 2:49:00 AM Andrew Ash <andrew@andrewash.com> wrote=
:
>>
>>> The goal of increasing visibility on open issues is a good one.  How is
>>> this different from just a link to Jira though?  Some might say this ad=
ds
>>> noise to the mailing list and doesn't contain any information not alrea=
dy
>>> available in Jira.
>>>
>>> The idea seems good but the formatting leaves a little to be desired.
>>> If you aren't opposed to using HTML, I might suggest this more compact
>>> format:
>>>
>>> SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044> Pluggable=
 interface
>>> for shuffles
>>> SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
>>> IndexedRDD, an efficient updatable key-value
>>> SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow for=
 pluggable
>>> execution contexts in Spark
>>>
>>> Andrew
>>>
>>> On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> What do y=E2=80=99all think of a report like this emailed out to the d=
ev list
>>>> on a
>>>> monthly basis?
>>>>
>>>> The goal would be to increase visibility into our open issues and
>>>> encourage
>>>> developers to tend to our issue tracker more frequently.
>>>>
>>>> Nick
>>>>
>>>> There are 1,236 unresolved issues
>>>>
>>> <https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+SPAR
>>>> K+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC>
>>>
>>>
>>>> in the Spark project on JIRA.
>>>> Recently Updated Issues
>>>>
>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>> 20updated%20DESC>
>>>
>>>
>>>> Type Key Priority Summary Last Updated   Bug SPARK-4841
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch
>>>> serializer
>>>
>>>
>>>> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
>>>
>>>
>>>> collect Dec 14, 2014  Bug SPARK-785
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner
>>>> not
>>>
>>>
>>>> invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
>>>
>>>
>>>> creation on VPC Dec 13, 2014  Improvement SPARK-1555
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
>>>
>>>
>>>> ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014
>>>>  Stale
>>>> Issues
>>>>
>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20update
>>>> d%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
>>>
>>>
>>>> Type Key Priority Summary Last Updated   Bug SPARK-560
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs =
/
>>>
>>>
>>>> iterators Oct 22, 2012  New Feature SPARK-540
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to
>>>> customize
>>>
>>>
>>>> in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify
>>>> semantics of
>>>
>>>
>>>> the parallelized closures Oct 22, 2012  New Feature SPARK-609
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add instruction=
s
>>>
>>>
>>>> for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism t=
o
>>>
>>>
>>>> run system management/configuration tasks on all workers Dec 17, 2012
>>>>  Most
>>>> Watched Issues
>>>>
>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>> 20watchers%20DESC>
>>>
>>>
>>>> Type Key Priority Summary Watchers   New Feature SPARK-3561
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
>>>
>>>
>>>> pluggable execution contexts in Spark 75  New Feature SPARK-2365
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add
>>>> IndexedRDD, an
>>>
>>>
>>>> efficient updatable key-value store 33  Improvement SPARK-2044
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
>>>
>>>
>>>> interface for shuffles 30  New Feature SPARK-1405
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel
>>>> Latent
>>>
>>>
>>>> Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature
>>>> SPARK-1406
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
>>>
>>>
>>>> evaluation support via MLib 21   Most Voted Issues
>>>>
>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>> 20votes%20DESC>
>>>
>>>
>>>> Type Key Priority Summary Votes   Bug SPARK-2541
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mod=
e
>>>
>>>
>>>> can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add
>>>> IndexedRDD, an
>>>
>>>
>>>> efficient updatable key-value store 9  Improvement SPARK-3533
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add
>>>
>>>
>>>> saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Suppor=
t
>>>
>>>
>>>> for ORCFile format 6  New Feature SPARK-1442
>>>>
>>> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
>>>> function support 6
>>>> =E2=80=8B
>>>>
>>>
>>>

--089e0129503e345aab050a44f1ea--

From dev-return-10805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 18:51:42 2014
Return-Path: <dev-return-10805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 571C510DDB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 18:51:42 +0000 (UTC)
Received: (qmail 47372 invoked by uid 500); 15 Dec 2014 18:51:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47299 invoked by uid 500); 15 Dec 2014 18:51:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47285 invoked by uid 99); 15 Dec 2014 18:51:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 18:51:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 18:51:08 +0000
Received: by mail-oi0-f48.google.com with SMTP id u20so8396575oif.35
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 10:51:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=zs6xK9f4fcem5WH1LiNBtv0/UZYoQeVGef3Kx+x6q7Q=;
        b=Fv9zeAEfxpubF2D9MBDYJJvF8MwMp2wbKVlGzrSBE0ZQwVKCuEMUmZ3CbOlLL7TaQO
         unDHWHr9vi/dvKM2GnL2e7lrQlaY0wBSE5abuOujSlLEBToF0k/ReZOKOu1B0DeD1Ln9
         2iaIP6Yho3hTqFpUxSnUqsNNooQp0/Tv5JCoYJgcUz7IDaUpiA5JZXBSc6lHTY2flIoL
         F9/hReviL1pI3uUNNuQ7X73vO4Ou8jfldisYCEtoXuxyKRjhmlpltur/5Hcl2KMZgb4L
         T97C9RP+ZATuRtRhpbCl92Z2u4ZdnsLfzYulMT5Dqye0yQX2yH4F5p3DNgopDmsgFsaR
         iapw==
MIME-Version: 1.0
X-Received: by 10.182.33.138 with SMTP id r10mr19889690obi.67.1418669467079;
 Mon, 15 Dec 2014 10:51:07 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 15 Dec 2014 10:51:07 -0800 (PST)
Date: Mon, 15 Dec 2014 10:51:07 -0800
Message-ID: <CABPQxsuzNC2JxxbF-SG+dZGyz5fmjsmvOfkmjyswCU1JR+FW_g@mail.gmail.com>
Subject: Test failures after Jenkins upgrade
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

It appears that a single test suite is failing after the jenkins
upgrade: "org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite".
My guess is the suite is not resilient in some way to differences in
the environment (JVM, OS version, or something else).

I'm going to disable the suite to get the build passing. This should
be done in the next 30 minutes or so.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 18:55:47 2014
Return-Path: <dev-return-10806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BAD7D10E0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 18:55:47 +0000 (UTC)
Received: (qmail 60894 invoked by uid 500); 15 Dec 2014 18:55:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60821 invoked by uid 500); 15 Dec 2014 18:55:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60798 invoked by uid 99); 15 Dec 2014 18:55:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 18:55:45 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 18:55:40 +0000
Received: by mail-pd0-f180.google.com with SMTP id w10so12111811pde.25
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 10:54:35 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=NNCj2bxOie8daaM6x9yDXBrt0GFqnnMlfpUu5I8orv0=;
        b=yEG5KH9j9QQV9gBFscPYhipnW1w9bonpsrS0q15PVIRwpfKYZlDgQFIVm0yY7aoa8y
         86A2eY12Rnnhx+fwtaXVEAxqaXYBFd4I0Nc9v4Hi1DkaQWZOC1kYm1hu0QgibmoCwEhT
         ofwEoAwwl1TGUU+7rNRgisN7+bkpYklsnkJ1rlTwTp1QrvG8Vxl7r5wJF1DMn7W1TGOE
         Fw/ck9mj4BKhZAng28i2MrUfEGtFN1fiMjfBgZixKXZVlDi3wi5R/M0XnG3YjSF4ytW3
         nVT9XQdWImMDDAXWEeGa51gHntdbVKnZx6FlZbucU2qh1XKfBLqIgivWV3eJ6VDytCZK
         EmxA==
X-Received: by 10.68.131.39 with SMTP id oj7mr27746993pbb.145.1418669675434;
        Mon, 15 Dec 2014 10:54:35 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id 9sm9984408pdg.38.2014.12.15.10.54.34
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 15 Dec 2014 10:54:34 -0800 (PST)
Date: Mon, 15 Dec 2014 10:54:33 -0800
From: Josh Rosen <rosenville@gmail.com>
To: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>, Patrick
 Wendell <pwendell@gmail.com>
Message-ID: <etPan.548f2e69.6b8b4567.113@joshs-mbp>
In-Reply-To: <CABPQxsuzNC2JxxbF-SG+dZGyz5fmjsmvOfkmjyswCU1JR+FW_g@mail.gmail.com>
References: <CABPQxsuzNC2JxxbF-SG+dZGyz5fmjsmvOfkmjyswCU1JR+FW_g@mail.gmail.com>
Subject: Re: Test failures after Jenkins upgrade
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="548f2e69_327b23c6_113"
X-Virus-Checked: Checked by ClamAV on apache.org

--548f2e69_327b23c6_113
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

There=E2=80=99s a JIRA for this:=C2=A0https://issues.apache.org/jira/brow=
se/SPARK-4826

And two open PRs:

https://github.com/apache/spark/pull/3695
https://github.com/apache/spark/pull/3701

We might be close to fixing this via one of those PRs, so maybe we should=
 try using one of those instead=3F

On December 15, 2014 at 10:51:46 AM, Patrick Wendell (pwendell=40gmail.co=
m) wrote:

Hey All, =20

It appears that a single test suite is failing after the jenkins =20
upgrade: =22org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSui=
te=22. =20
My guess is the suite is not resilient in some way to differences in =20
the environment (JVM, OS version, or something else). =20

I'm going to disable the suite to get the build passing. This should =20
be done in the next 30 minutes or so. =20

- Patrick =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--548f2e69_327b23c6_113--


From dev-return-10807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 19:01:01 2014
Return-Path: <dev-return-10807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33EF710E5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 19:01:01 +0000 (UTC)
Received: (qmail 76037 invoked by uid 500); 15 Dec 2014 19:00:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75958 invoked by uid 500); 15 Dec 2014 19:00:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75946 invoked by uid 99); 15 Dec 2014 19:00:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:00:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:00:32 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so19256709obc.6
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 10:59:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4VsgPvIZO3o9K+A4ArEEeg2hLVVRP1uTeyV5mLPjhx4=;
        b=uPf1w+YqezKPfJ23vPMjxAEXBbt4LJutYvhuoAUWm/cAnYHttthq8Wm9KeyH124uHC
         xsiRzPGiDJapHvwTqdGSA5dNZL44sXiXHfXOaPxcEFhBw38UqkbI9b3i478anm5b/tz+
         /O9mhIKohjNjiqWlgmFdksFBoWuwWzn2FdJ33wotLDbs2KPx3JjKk65OR23SY6SBh5Zn
         VD+MNdR1FbvflpjkmNZXDr0/oAAbqR7iFIgAQXkZrgPB7wNyFbEqMvOuJ+5LiMuRyiQB
         JWNKOOgqmalfj/MXkx17nkaQNMysWSZnp8sx47gmpCt7L22lFiX0/x6PY8ShdPErAyVh
         pULA==
MIME-Version: 1.0
X-Received: by 10.60.56.78 with SMTP id y14mr19664379oep.52.1418669940846;
 Mon, 15 Dec 2014 10:59:00 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 15 Dec 2014 10:59:00 -0800 (PST)
In-Reply-To: <etPan.548f2e69.6b8b4567.113@joshs-mbp>
References: <CABPQxsuzNC2JxxbF-SG+dZGyz5fmjsmvOfkmjyswCU1JR+FW_g@mail.gmail.com>
	<etPan.548f2e69.6b8b4567.113@joshs-mbp>
Date: Mon, 15 Dec 2014 10:59:00 -0800
Message-ID: <CABPQxssHw2v4xJtpEfc2Ky-5i2ow7uC05LfzVCFaJjg1AWp+TQ@mail.gmail.com>
Subject: Re: Test failures after Jenkins upgrade
From: Patrick Wendell <pwendell@gmail.com>
To: Josh Rosen <rosenville@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Ah cool Josh - I think for some reason we are hitting this every time
now. Since this is holding up a bunch of other patches, I just pushed
something ignoring the tests as a hotfix. Even waiting for a couple
hours is really expensive productivity-wise given the frequency with
which we run tests. We should just re-enable them when we merge the
appropriate fix.

On Mon, Dec 15, 2014 at 10:54 AM, Josh Rosen <rosenville@gmail.com> wrote:
> There's a JIRA for this: https://issues.apache.org/jira/browse/SPARK-4826
>
> And two open PRs:
>
> https://github.com/apache/spark/pull/3695
> https://github.com/apache/spark/pull/3701
>
> We might be close to fixing this via one of those PRs, so maybe we should
> try using one of those instead?
>
> On December 15, 2014 at 10:51:46 AM, Patrick Wendell (pwendell@gmail.com)
> wrote:
>
> Hey All,
>
> It appears that a single test suite is failing after the jenkins
> upgrade: "org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite".
> My guess is the suite is not resilient in some way to differences in
> the environment (JVM, OS version, or something else).
>
> I'm going to disable the suite to get the build passing. This should
> be done in the next 30 minutes or so.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 19:31:48 2014
Return-Path: <dev-return-10808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2777210FC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 19:31:48 +0000 (UTC)
Received: (qmail 50255 invoked by uid 500); 15 Dec 2014 19:31:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50178 invoked by uid 500); 15 Dec 2014 19:31:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50160 invoked by uid 99); 15 Dec 2014 19:31:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:31:45 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.171 as permitted sender)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:31:19 +0000
Received: by mail-pd0-f171.google.com with SMTP id y13so12249908pdi.16
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 11:30:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=X6APB5qLP1DjY3rwUjW9w4CuqCT/MVKfe6uhwuRIHEk=;
        b=OZ19pmWyq4I6ocDCnU2tthvqPLPmePZzLKsDF/h59uLFgZNF6C1g8g6i/AgdOlTy2D
         6JUySCoZg08JUX9sWC9lSojQ4HHHcm5/tit9kljAe2hM/kFMeuxc1mJmbqOxZM88W6Sf
         0eNHrcHgIQSGo67adzwF08d+1TY8Ih9OYQMfFJ/8+JbhEbPmyq+pECzkBBXsEFteLGsn
         og1UmNEcJOv6UK9lePar37t5CGNhHlJAFDOFVO4vuUnDz8waC2zwrOqLM9ELBsBLZfTs
         3a4CcbJlEfdyPss4Haa4gRTqU2YHBo3gSrT7y90dWILuSyvRupb4yUhSk/rTYVHskRPH
         hZYQ==
X-Received: by 10.66.139.234 with SMTP id rb10mr53726287pab.146.1418671832555;
        Mon, 15 Dec 2014 11:30:32 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ug6sm9965073pbc.72.2014.12.15.11.30.31
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 15 Dec 2014 11:30:32 -0800 (PST)
Date: Mon, 15 Dec 2014 11:30:31 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.548f36d7.643c9869.113@joshs-mbp>
In-Reply-To: <CABPQxssHw2v4xJtpEfc2Ky-5i2ow7uC05LfzVCFaJjg1AWp+TQ@mail.gmail.com>
References: <CABPQxsuzNC2JxxbF-SG+dZGyz5fmjsmvOfkmjyswCU1JR+FW_g@mail.gmail.com>
 <etPan.548f2e69.6b8b4567.113@joshs-mbp>
 <CABPQxssHw2v4xJtpEfc2Ky-5i2ow7uC05LfzVCFaJjg1AWp+TQ@mail.gmail.com>
Subject: Re: Test failures after Jenkins upgrade
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="548f36d7_66334873_113"
X-Virus-Checked: Checked by ClamAV on apache.org

--548f36d7_66334873_113
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Opened one more PR for this:=C2=A0https://github.com/apache/spark/pull/37=
04


On December 15, 2014 at 10:59:00 AM, Patrick Wendell (pwendell=40gmail.co=
m) wrote:

Ah cool Josh - I think for some reason we are hitting this every time =20
now. Since this is holding up a bunch of other patches, I just pushed =20
something ignoring the tests as a hotfix. Even waiting for a couple =20
hours is really expensive productivity-wise given the frequency with =20
which we run tests. We should just re-enable them when we merge the =20
appropriate fix. =20

On Mon, Dec 15, 2014 at 10:54 AM, Josh Rosen <rosenville=40gmail.com> wro=
te: =20
> There's a JIRA for this: https://issues.apache.org/jira/browse/SPARK-48=
26 =20
> =20
> And two open PRs: =20
> =20
> https://github.com/apache/spark/pull/3695 =20
> https://github.com/apache/spark/pull/3701 =20
> =20
> We might be close to fixing this via one of those PRs, so maybe we shou=
ld =20
> try using one of those instead=3F =20
> =20
> On December 15, 2014 at 10:51:46 AM, Patrick Wendell (pwendell=40gmail.=
com) =20
> wrote: =20
> =20
> Hey All, =20
> =20
> It appears that a single test suite is failing after the jenkins =20
> upgrade: =22org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDS=
uite=22. =20
> My guess is the suite is not resilient in some way to differences in =20
> the environment (JVM, OS version, or something else). =20
> =20
> I'm going to disable the suite to get the build passing. This should =20
> be done in the next 30 minutes or so. =20
> =20
> - Patrick =20
> =20
> --------------------------------------------------------------------- =20
> To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
> =46or additional commands, e-mail: dev-help=40spark.apache.org =20
> =20

--548f36d7_66334873_113--


From dev-return-10809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 19:42:34 2014
Return-Path: <dev-return-10809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3B6F10056
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 19:42:34 +0000 (UTC)
Received: (qmail 88434 invoked by uid 500); 15 Dec 2014 19:42:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88347 invoked by uid 500); 15 Dec 2014 19:42:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88335 invoked by uid 99); 15 Dec 2014 19:42:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:42:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:42:08 +0000
Received: by mail-ig0-f172.google.com with SMTP id hl2so5582189igb.17
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 11:42:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0/igsVB+fJgJ3xctmeAzrxqjWdJNyIxnJsBbR8/K/r8=;
        b=c/Bb2rijT2UHBM+OCCwfo+f9XUMSm4YImWQv6OuXfar+6CVY4PNIzAHc6/oZk3ney2
         uCQvny5SFPZG/W8iaDFqmrSSmuERxoixQK/QHwk4iH9O2yr6UVcudrq+892H/UT0I8vo
         EFrVm/BdBICL+zn/yfvb8qW+XLU8msvaxBrw9wfJX+85smx31t3G5F909ociBWuFtruH
         PkNC7K54pbakR/wKtYZGC4sCsW+o6G5ES3WuDadb5N8QLAso4IEK9Z9/hPqi27g6IiyH
         TASwzRiC19lFKJnkAv0nUCGXv9AKju4NitbG/AwFGvs3ngafmArRwLRQgkhzBPKVOspr
         YiWg==
MIME-Version: 1.0
X-Received: by 10.107.135.163 with SMTP id r35mr30200214ioi.25.1418672527329;
 Mon, 15 Dec 2014 11:42:07 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Mon, 15 Dec 2014 11:42:07 -0800 (PST)
In-Reply-To: <CAEYYnxaA_Dewu=ugdp4f+PWBhR+O4km=Yqb2XH7_Vvi5pV9pxQ@mail.gmail.com>
References: <CAEYYnxbo_WQZAz+ES34Zi4hL2nbM9RcnZSFHD6ykNqiRx_e0vw@mail.gmail.com>
	<CAEYYnxaA_Dewu=ugdp4f+PWBhR+O4km=Yqb2XH7_Vvi5pV9pxQ@mail.gmail.com>
Date: Tue, 16 Dec 2014 03:42:07 +0800
Message-ID: <CAJgQjQ-hWGUBH0+0aAYQ_bhPuFjeTP+Uf3Cg5tOyRfbAH4i4Ag@mail.gmail.com>
Subject: Re: CrossValidator API in new spark.ml package
From: Xiangrui Meng <mengxr@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: joseph <joseph@databricks.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, regularization path could be viewed as training multiple models
at once. -Xiangrui

On Sat, Dec 13, 2014 at 6:53 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
> Okay, I got it. In Estimator, fit(dataset: SchemaRDD, paramMaps:
> Array[ParamMap]): Seq[M] can be overwritten to implement
> regularization path. Correct me if I'm wrong.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Fri, Dec 12, 2014 at 11:37 AM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> Hi Xiangrui,
>>
>> It seems that it's stateless so will be hard to implement
>> regularization path. Any suggestion to extend it? Thanks.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 19:42:39 2014
Return-Path: <dev-return-10810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9AF6B10058
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 19:42:39 +0000 (UTC)
Received: (qmail 90064 invoked by uid 500); 15 Dec 2014 19:42:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89990 invoked by uid 500); 15 Dec 2014 19:42:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89818 invoked by uid 99); 15 Dec 2014 19:42:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:42:37 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.223.173 as permitted sender)
Received: from [209.85.223.173] (HELO mail-ie0-f173.google.com) (209.85.223.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 19:42:32 +0000
Received: by mail-ie0-f173.google.com with SMTP id y20so11460959ier.18
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 11:40:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Zu59e8//13BTaKvmPHR6vXdzx0kz22CcMdNzPR4dO9c=;
        b=dswsrrepNUL+EB3ZXrWyVC+jP/jmq8eTUlhWrk62EduR16mkx2zuxb09VWA4oHRhA0
         UeCRa3xJgd1gXfDv0+u/Zk3RMe9s6V7gcFY5oFyo9+2tV90C1dTDN+f6klJJO4MVqVhQ
         OYd93oO4Zf0sED0lStBUd36Eo6E1kt040PXzFgj+x5qKH29dAESKTUzz0+VVdk6Gut5U
         bbnIZnDeSa3hoWnFYhEKgxidIr0XmS+A3SaP9KgjmD5n413HL5i4lNAaH4kHLsen309y
         7YjLVBj4kgB+oHpn4LWg8igkqHIVvwvnwJYejmsAfhc0DaDZ3i9F7OWFJq44VFZ+sv55
         d6Vg==
MIME-Version: 1.0
X-Received: by 10.107.136.92 with SMTP id k89mr30679869iod.43.1418672441661;
 Mon, 15 Dec 2014 11:40:41 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Mon, 15 Dec 2014 11:40:41 -0800 (PST)
In-Reply-To: <CAOTBr2=0AdSTkMLSyjnD4Wi5qwHRvngHyP2Ec6+kHgFCj=wJGQ@mail.gmail.com>
References: <CABPQxssgKCU0kzD6jhSMxtXUcAbkTKAaVA3hkMnu7HW3a39QOQ@mail.gmail.com>
	<CAOTBr2=5S=cxYfi7OE4g_8nyOWsdFabaSCsJ2fkWsKHY9Nn=6Q@mail.gmail.com>
	<CAJgQjQ-jbPEWXxM4oFKjrxk=i7ANoR1a19-tdRvN31DrOQHqbA@mail.gmail.com>
	<CAOTBr2=0AdSTkMLSyjnD4Wi5qwHRvngHyP2Ec6+kHgFCj=wJGQ@mail.gmail.com>
Date: Tue, 16 Dec 2014 03:40:41 +0800
Message-ID: <CAJgQjQ_Ux__SUyDrkXhnFZuAanKgg6SNMkyqVcMS0ivo5EatGQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Krishna,

Thanks for providing the notebook! I tried and found that the problem
is with PySpark's zip. I created a JIRA to track the issue:
https://issues.apache.org/jira/browse/SPARK-4841

-Xiangrui

On Thu, Dec 11, 2014 at 1:55 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
> K-Means iPython notebook & data attached.
> It is the zip that gives the error ; while one of the RDDs is from the
> prediction, most probably there is no problem with the K-Means.
> Lines 34,35 & 36 essentially are the same. But only 36 works with 1.2.0.
> Interestingly, lines 34,35 & 36 work with 1.1.1 (Checked just now)
>
> The plot thickens!
> In 1.1.1, freq_cluster_map.take(5) prints normally for 34 & 35, but in
> exponential form for 36. So there is some difference even in 1.1.1.
> #34,#35 [(array([28143, 0, 174, 1, 0, 0, 7000]), 1),
>
>  (array([19244,     0,   215,     2,     0,     0,  6968]), 1),
>  (array([41354,     0,  4123,     4,     0,     0,  7034]), 1),
>  (array([14776,     0,   500,     1,     0,     0,  6952]), 1),
>  (array([97752,     0, 43300,    26,  2077,     4,  6935]), 0)]
>
> #36 [(array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,
>
>            1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
>            7.00000000e+03]), 1),
>  (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,
>            2.00000000e+00,   0.00000000e+00,   0.00000000e+00,
>            6.96800000e+03]), 1),
>  (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,
>            4.00000000e+00,   0.00000000e+00,   0.00000000e+00,
>            7.03400000e+03]), 1),
>  (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,
>            1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
>            6.95200000e+03]), 1),
>  (array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,
>            2.60000000e+01,   2.07700000e+03,   4.00000000e+00,
>            6.93500000e+03]), 0)]
>
> I had overwritten the naive bayes example. Will chase the older versions
> down
>
> Cheers
> <k/>
>
> On Wed, Dec 3, 2014 at 4:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> Krishna, could you send me some code snippets for the issues you saw
>> in naive Bayes and k-means? -Xiangrui
>>
>> On Sun, Nov 30, 2014 at 6:49 AM, Krishna Sankar <ksankar42@gmail.com>
>> wrote:
>> > +1
>> > 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
>> > -Dhadoop.version=2.4.0 -DskipTests clean package 16:46 min (slightly
>> > slower
>> > connection)
>> > 2. Tested pyspark, mlib - running as well as compare esults with 1.1.x
>> > 2.1. statistics OK
>> > 2.2. Linear/Ridge/Laso Regression OK
>> >        Slight difference in the print method (vs. 1.1.x) of the model
>> > object - with a label & more details. This is good.
>> > 2.3. Decision Tree, Naive Bayes OK
>> >        Changes in print(model) - now print (model.ToDebugString()) - OK
>> >        Some changes in NaiveBayes. Different from my 1.1.x code - had to
>> > flatten list structures, zip required same number in partitions
>> >        After code changes ran fine.
>> > 2.4. KMeans OK
>> >        zip occasionally fails with error "localhost):
>> > org.apache.spark.SparkException: Can only zip RDDs with same number of
>> > elements in each partition"
>> > Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
>> > Made it work by doing a different transformation ie reusing an original
>> > rdd.
>> > 2.5. rdd operations OK
>> >        State of the Union Texts - MapReduce, Filter,sortByKey (word
>> > count)
>> > 2.6. recommendation OK
>> > 2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
>> > dataset which never worked. Works fine in 1.2.0 !
>> > 3. Scala Mlib - subset of examples as in #2 above, with Scala
>> > 3.1. statistics OK
>> > 3.2. Linear Regression OK
>> > 3.3. Decision Tree OK
>> > 3.4. KMeans OK
>> > Cheers
>> > <k/>
>> > P.S: Plan to add RF and .ml mechanics to this bank
>> >
>> > On Fri, Nov 28, 2014 at 9:16 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >
>> >> Please vote on releasing the following candidate as Apache Spark
>> >> version
>> >> 1.2.0!
>> >>
>> >> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
>> >>
>> >>
>> >> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
>> >>
>> >> The release files, including signatures, digests, etc. can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
>> >>
>> >> Release artifacts are signed with the following key:
>> >> https://people.apache.org/keys/committer/pwendell.asc
>> >>
>> >> The staging repository for this release can be found at:
>> >> https://repository.apache.org/content/repositories/orgapachespark-1048/
>> >>
>> >> The documentation corresponding to this release can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
>> >>
>> >> Please vote on releasing this package as Apache Spark 1.2.0!
>> >>
>> >> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
>> >> if a majority of at least 3 +1 PMC votes are cast.
>> >>
>> >> [ ] +1 Release this package as Apache Spark 1.1.0
>> >> [ ] -1 Do not release this package because ...
>> >>
>> >> To learn more about Apache Spark, please see
>> >> http://spark.apache.org/
>> >>
>> >> == What justifies a -1 vote for this release? ==
>> >> This vote is happening very late into the QA period compared with
>> >> previous votes, so -1 votes should only occur for significant
>> >> regressions from 1.0.2. Bugs already present in 1.1.X, minor
>> >> regressions, or bugs related to new features will not block this
>> >> release.
>> >>
>> >> == What default changes should I be aware of? ==
>> >> 1. The default value of "spark.shuffle.blockTransferService" has been
>> >> changed to "netty"
>> >> --> Old behavior can be restored by switching to "nio"
>> >>
>> >> 2. The default value of "spark.shuffle.manager" has been changed to
>> >> "sort".
>> >> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>> >> "hash".
>> >>
>> >> == Other notes ==
>> >> Because this vote is occurring over a weekend, I will likely extend
>> >> the vote if this RC survives until the end of the vote period.
>> >>
>> >> - Patrick
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> >>
>> >>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 22:37:31 2014
Return-Path: <dev-return-10811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 204DA108E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 22:37:31 +0000 (UTC)
Received: (qmail 84156 invoked by uid 500); 15 Dec 2014 22:37:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84071 invoked by uid 500); 15 Dec 2014 22:37:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84054 invoked by uid 99); 15 Dec 2014 22:37:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 22:37:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 22:37:25 +0000
Received: by mail-ie0-f169.google.com with SMTP id y20so11934462ier.28
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 14:35:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=r9kUsgn69nH0ZLCigKUv3vC28+rpk7rkWrG3ahVPJUY=;
        b=nZDdz8YiD7WqbO3lKtI5rNF0DxsxocUQc+rrOfFwwN6ZFy3AmzppicTcO7/0h4q9m5
         AJV4OtRPZhIzG9zheelNTrDEW5V07EWjXKIHTYLdlWTCEiTnkl9uPkP+Ob39F1FXjB7+
         rkI/cAK/zivvkXDVKb4ZHnrhIOXLK1Fazk6AvB/effWwLVNo1XAb2sdMjpw4s3M7ISe9
         C1BrHGyHUMgZbRJzjJyrLuOR28NGICNcDbw12yhj9gEt8SZhVeOri5LbfF+aDRiCSRmn
         zoH3dI8gtQ9lUtcujUJArQWLDWYbfbTIEPCmQ4dsvkjpuPxDu12DM6YVHMz2lUV27rTH
         6Wkg==
X-Received: by 10.42.25.144 with SMTP id a16mr30250279icc.66.1418682934366;
 Mon, 15 Dec 2014 14:35:34 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
 <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
 <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com> <CA+-p3AHqvVzuf4HqftTu8jQQSpk7TEFUp9sDbqw48hNAnO2AoA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 15 Dec 2014 22:35:33 +0000
Message-ID: <CAOhmDzed1RdFBsLEPjYU+xK5NEA4oy_wgfN4kKgPXrUp2CX9SQ@mail.gmail.com>
Subject: Re: Spark JIRA Report
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303ea5825a18fb050a48dc1b
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303ea5825a18fb050a48dc1b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

OK, that's good.

Another approach we can take to controlling the number of stale JIRA issues
is writing a bot that simply closes issues after N days of inactivity and
prompts people to reopen the issue if it's still valid. I believe Sean Owen
proposed that at one point (?).

I wonder if that might be better since I feel that even a slimmed down
email might not be enough to get already-busy people to spend time on JIRA
management.

Nick

On Mon Dec 15 2014 at 12:55:06 PM Andrew Ash <andrew@andrewash.com> wrote:

> Nick,
>
> Putting the N most stale issues into a report like your latest one does
> seem like a good way to tackle the wall of text effect that I'm worried
> about.
>
> On Sun, Dec 14, 2014 at 12:28 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> Taking after Andrew=E2=80=99s suggestion, perhaps the report can just fo=
cus on
>> Stale issues (no updates in > 90 days), since those are probably the
>> easiest to act on.
>>
>> For example:
>> Stale Issues
>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AN=
D%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%2=
0BY%20updated%20ASC>
>>
>>    - [Oct 22, 2012] SPARK-560
>>    <https://issues.apache.org/jira/browse/SPARK-560>: Specialize RDDs /
>>    iterators
>>    - [Oct 22, 2012] SPARK-540
>>    <https://issues.apache.org/jira/browse/SPARK-540>: Add API to
>>    customize in-memory representation of RDDs
>>    - [Oct 22, 2012] SPARK-573
>>    <https://issues.apache.org/jira/browse/SPARK-573>: Clarify semantics
>>    of the parallelized closures
>>    - [Nov 06, 2012] SPARK-609
>>    <https://issues.apache.org/jira/browse/SPARK-609>: Add instructions
>>    for enabling Akka debug logging
>>    - [Dec 17, 2012] SPARK-636
>>    <https://issues.apache.org/jira/browse/SPARK-636>: Add mechanism to
>>    run system management/configuration tasks on all workers
>>
>> Andrew,
>>
>> Does that seem more useful?
>>
>> Nick
>> =E2=80=8B
>>
>> On Sun Dec 14 2014 at 3:20:54 AM Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> I formatted this report using Markdown; I'm open to changing the
>>> structure or formatting or reducing the amount of information to make t=
he
>>> report more easily consumable.
>>>
>>> Regarding just sending links or whether this would just be mailing list
>>> noise, those are a good questions.
>>>
>>> I've sent out links before, but I feel from a UX perspective having the
>>> information right in the email itself makes it frictionless for people =
to
>>> act on the information. For me, that difference is enough to hook me in=
to
>>> spending a few minutes on JIRA vs. just glossing over an email with a l=
ink.
>>>
>>> I wonder if that's also the case for others on this list.
>>>
>>> If you already spend a good amount of time cleaning up on JIRA, then
>>> this report won't be that relevant to you. But given the number and gro=
wth
>>> of open issues on our tracker, I suspect we could do with quite a few m=
ore
>>> people chipping in and cleaning up where they can.
>>>
>>> That's the real problem that this report is intended to help with.
>>>
>>> Nick
>>>
>>>
>>>
>>> On Sun Dec 14 2014 at 2:49:00 AM Andrew Ash <andrew@andrewash.com>
>>> wrote:
>>>
>>>> The goal of increasing visibility on open issues is a good one.  How i=
s
>>>> this different from just a link to Jira though?  Some might say this a=
dds
>>>> noise to the mailing list and doesn't contain any information not alre=
ady
>>>> available in Jira.
>>>>
>>>> The idea seems good but the formatting leaves a little to be desired.
>>>> If you aren't opposed to using HTML, I might suggest this more compact
>>>> format:
>>>>
>>>> SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044>
>>>>  Pluggable interface for shuffles
>>>> SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
>>>> IndexedRDD, an efficient updatable key-value
>>>> SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow
>>>> for pluggable execution contexts in Spark
>>>>
>>>> Andrew
>>>>
>>>> On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas <
>>>> nicholas.chammas@gmail.com> wrote:
>>>>
>>>>> What do y=E2=80=99all think of a report like this emailed out to the =
dev list
>>>>> on a
>>>>> monthly basis?
>>>>>
>>>>> The goal would be to increase visibility into our open issues and
>>>>> encourage
>>>>> developers to tend to our issue tracker more frequently.
>>>>>
>>>>> Nick
>>>>>
>>>>> There are 1,236 unresolved issues
>>>>>
>>>> <https://issues.apache.org/jira/issues/?jql=3Dproject+%3D+SPAR
>>>>> K+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC>
>>>>
>>>>
>>>>> in the Spark project on JIRA.
>>>>> Recently Updated Issues
>>>>>
>>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>>> 20updated%20DESC>
>>>>
>>>>
>>>>> Type Key Priority Summary Last Updated   Bug SPARK-4841
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch
>>>>> serializer
>>>>
>>>>
>>>>> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014  Question SPARK-4810
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
>>>>
>>>>
>>>>> collect Dec 14, 2014  Bug SPARK-785
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner
>>>>> not
>>>>
>>>>
>>>>> invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
>>>>
>>>>
>>>>> creation on VPC Dec 13, 2014  Improvement SPARK-1555
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
>>>>
>>>>
>>>>> ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13,
>>>>> 2014   Stale
>>>>> Issues
>>>>>
>>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20update
>>>>> d%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
>>>>
>>>>
>>>>> Type Key Priority Summary Last Updated   Bug SPARK-560
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs
>>>>> /
>>>>
>>>>
>>>>> iterators Oct 22, 2012  New Feature SPARK-540
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to
>>>>> customize
>>>>
>>>>
>>>>> in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify
>>>>> semantics of
>>>>
>>>>
>>>>> the parallelized closures Oct 22, 2012  New Feature SPARK-609
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add
>>>>> instructions
>>>>
>>>>
>>>>> for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism
>>>>> to
>>>>
>>>>
>>>>> run system management/configuration tasks on all workers Dec 17, 2012
>>>>>  Most
>>>>> Watched Issues
>>>>>
>>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>>> 20watchers%20DESC>
>>>>
>>>>
>>>>> Type Key Priority Summary Watchers   New Feature SPARK-3561
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
>>>>
>>>>
>>>>> pluggable execution contexts in Spark 75  New Feature SPARK-2365
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add
>>>>> IndexedRDD, an
>>>>
>>>>
>>>>> efficient updatable key-value store 33  Improvement SPARK-2044
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
>>>>
>>>>
>>>>> interface for shuffles 30  New Feature SPARK-1405
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel
>>>>> Latent
>>>>
>>>>
>>>>> Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature
>>>>> SPARK-1406
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
>>>>
>>>>
>>>>> evaluation support via MLib 21   Most Voted Issues
>>>>>
>>>> <https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%
>>>>> 20votes%20DESC>
>>>>
>>>>
>>>>> Type Key Priority Summary Votes   Bug SPARK-2541
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone
>>>>> mode
>>>>
>>>>
>>>>> can=E2=80=99t access secure HDFS anymore 12  New Feature SPARK-2365
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add
>>>>> IndexedRDD, an
>>>>
>>>>
>>>>> efficient updatable key-value store 9  Improvement SPARK-3533
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add
>>>>
>>>>
>>>>> saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark
>>>>> Support
>>>>
>>>>
>>>>> for ORCFile format 6  New Feature SPARK-1442
>>>>>
>>>> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
>>>>> function support 6
>>>>> =E2=80=8B
>>>>>
>>>>
>>>>
>

--20cf303ea5825a18fb050a48dc1b--

From dev-return-10812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 22:39:49 2014
Return-Path: <dev-return-10812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 15C16108FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 22:39:49 +0000 (UTC)
Received: (qmail 99496 invoked by uid 500); 15 Dec 2014 22:39:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99427 invoked by uid 500); 15 Dec 2014 22:39:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99408 invoked by uid 99); 15 Dec 2014 22:39:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 22:39:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 22:39:42 +0000
Received: by mail-ig0-f173.google.com with SMTP id r2so5884544igi.6
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 14:39:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=TaMWVtaaG8qL3WFM76f5FAyXo+pA4Rs6XiTsR9yMUa0=;
        b=ub1irc2UCPhfsaNWCSo64pSk6wWwMyL4v95s/Wdb2k94MgubXpHguErDMHs5+9QmNd
         vqUi0nbU6mbxnFpa3M7qdwp9YpSTCDhgolMkuHyYJswzTQpXNweGoS//9kHXcJCf0dsN
         qcz5ECT6Yc9OK2orUTFwC6vQRcK6M0TtCcr37ekmoYUyk2aHXoH2oZr6c9w8NV1Nh+rM
         7kubrGVXf8/orW5NGG9DG0RMO+tNv5agGTO9haJ0dOuDP78T64Cwt0RA6RqdxeiQegLM
         d666TsRBj0nxuImNhO+ECAKLvzlRDSmW3WkKlKxH+vAavN1+YZ87KdiuRocckVu3Dcru
         NTBg==
X-Received: by 10.107.136.92 with SMTP id k89mr31514764iod.43.1418683161721;
 Mon, 15 Dec 2014 14:39:21 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzdQvt-MfUJAef57cCRTophocTDGXfYyOe6KD7bqWDC-jg@mail.gmail.com>
 <CACdU-dQyY1HK-pkbKQsngoEiqLejbEnaZSZjTW6wbo39_1xANg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 15 Dec 2014 22:39:21 +0000
Message-ID: <CAOhmDzeWGFSyMQNsHsVhu1pne=S0-j-ZUjm26O9txGDitkvFpA@mail.gmail.com>
Subject: Re: Archiving XML test reports for analysis
To: shane knapp <sknapp@berkeley.edu>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ecad4e742e1050a48e9d6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ecad4e742e1050a48e9d6
Content-Type: text/plain; charset=UTF-8

How about all of them <https://amplab.cs.berkeley.edu/jenkins/view/Spark/>? How
much data per day would it roughly be if we uploaded all the logs for all
these builds?

Also, would Databricks be willing to offer up an S3 bucket for this purpose?

Nick

On Mon Dec 15 2014 at 11:48:44 AM shane knapp <sknapp@berkeley.edu> wrote:

> right now, the following logs are archived on to the master:
>
>   local log_files=$(
>     find .\
>       -name "unit-tests.log" -o\
>       -path "./sql/hive/target/HiveCompatibilitySuite.failed" -o\
>       -path "./sql/hive/target/HiveCompatibilitySuite.hiveFailed" -o\
>       -path "./sql/hive/target/HiveCompatibilitySuite.wrong"
>   )
>
> regarding dumping stuff to S3 -- thankfully, since we're not looking at a
> lot of disk usage, i don't see a problem w/this.  we could tar/zip up the
> XML for each build and just dump it there.
>
> what builds are we thinking about?  spark pull request builder?  what
> others?
>
> On Mon, Dec 15, 2014 at 1:33 AM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>>
>> Every time we run a test cycle on our Jenkins cluster, we generate
>> hundreds
>> of XML reports covering all the tests we have (e.g.
>>
>> `streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogSuite.xml`).
>>
>> These reports contain interesting information about whether tests
>> succeeded
>> or failed, and how long they took to complete. There is also detailed
>> information about the environment they ran in.
>>
>> It might be valuable to have a window into all these reports across all
>> Jenkins builds and across all time, and use that to track basic statistics
>> about our tests. That could give us basic insight into what tests are
>> flaky
>> or slow, and perhaps drive other improvements to our testing
>> infrastructure
>> that we can't see just yet.
>>
>> Do people think that would be valuable? Do we already have something like
>> this?
>>
>> I'm thinking for starters it might be cool if we automatically uploaded
>> all
>> the XML test reports from the Master and the Pull Request builders to an
>> S3
>> bucket and just opened it up for the dev community to analyze.
>>
>> Nick
>>
>

--001a113ecad4e742e1050a48e9d6--

From dev-return-10813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 15 23:01:36 2014
Return-Path: <dev-return-10813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB07C109FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 15 Dec 2014 23:01:36 +0000 (UTC)
Received: (qmail 47287 invoked by uid 500); 15 Dec 2014 23:01:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47217 invoked by uid 500); 15 Dec 2014 23:01:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47206 invoked by uid 99); 15 Dec 2014 23:01:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 23:01:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 15 Dec 2014 23:01:09 +0000
Received: by mail-la0-f43.google.com with SMTP id s18so10426586lam.16
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 15:00:21 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/nq6f+SW7+/ZhAX6ypcj8cNcKxb1lH8770v+BPnDboM=;
        b=daCmwoIaahgKw0poidbgcEek5gHz+iWuQstMA0hzKnrEuwFmt3EldQPoyZbcGrLXYq
         bT8Q6+1uB5XN0q5JLbS/2GqF/kazIHMAr5VD4VAt59ZBchquOq+hA0g/ZSByoy27V5+d
         MbIg1+TiUS8WjgcnNUpmhtiaTEnk8HdTojj86UHfYSK+lkiv+ePVywnThgpFj88kzxHa
         z/8NzPDJtNLzcOeSdzxQRGQUX/Hm6PdAk/Rgc1YjJ8gtUYMeZl2KR8OjzAmPhz5CreUj
         yUUtObYHPuFAiMIiBW6t5uruQpBAO6N4+CK6ha6uK5tFeO9HHVVhBwKeMJccMmMrVTya
         VKjA==
X-Gm-Message-State: ALoCoQmMW3FnzVGScIZ511RGJrSsUxqjXWx2O9q3SYi4e50yew67rDx8sLBoux+ud5TAFR0L9RNB
X-Received: by 10.112.24.130 with SMTP id u2mr32372787lbf.57.1418684421213;
 Mon, 15 Dec 2014 15:00:21 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.172.80 with HTTP; Mon, 15 Dec 2014 15:00:01 -0800 (PST)
In-Reply-To: <CAOhmDzeWGFSyMQNsHsVhu1pne=S0-j-ZUjm26O9txGDitkvFpA@mail.gmail.com>
References: <CAOhmDzdQvt-MfUJAef57cCRTophocTDGXfYyOe6KD7bqWDC-jg@mail.gmail.com>
 <CACdU-dQyY1HK-pkbKQsngoEiqLejbEnaZSZjTW6wbo39_1xANg@mail.gmail.com> <CAOhmDzeWGFSyMQNsHsVhu1pne=S0-j-ZUjm26O9txGDitkvFpA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 15 Dec 2014 15:00:01 -0800
Message-ID: <CACdU-dQBwaf8N-HPeNAC_HTO=_btC2C7nT5a=MTBzfXFNOcByQ@mail.gmail.com>
Subject: Re: Archiving XML test reports for analysis
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134babcf9a7a2050a493424
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134babcf9a7a2050a493424
Content-Type: text/plain; charset=UTF-8

i have no problem w/storing all of the logs.  :)

i also have no problem w/donated S3 buckets.  :)

On Mon, Dec 15, 2014 at 2:39 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:
>
> How about all of them <https://amplab.cs.berkeley.edu/jenkins/view/Spark/>? How
> much data per day would it roughly be if we uploaded all the logs for all
> these builds?
>
> Also, would Databricks be willing to offer up an S3 bucket for this
> purpose?
>
> Nick
>
> On Mon Dec 15 2014 at 11:48:44 AM shane knapp <sknapp@berkeley.edu> wrote:
>
>> right now, the following logs are archived on to the master:
>>
>>   local log_files=$(
>>     find .\
>>       -name "unit-tests.log" -o\
>>       -path "./sql/hive/target/HiveCompatibilitySuite.failed" -o\
>>       -path "./sql/hive/target/HiveCompatibilitySuite.hiveFailed" -o\
>>       -path "./sql/hive/target/HiveCompatibilitySuite.wrong"
>>   )
>>
>> regarding dumping stuff to S3 -- thankfully, since we're not looking at a
>> lot of disk usage, i don't see a problem w/this.  we could tar/zip up the
>> XML for each build and just dump it there.
>>
>> what builds are we thinking about?  spark pull request builder?  what
>> others?
>>
>> On Mon, Dec 15, 2014 at 1:33 AM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>>
>>> Every time we run a test cycle on our Jenkins cluster, we generate
>>> hundreds
>>> of XML reports covering all the tests we have (e.g.
>>>
>>> `streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogSuite.xml`).
>>>
>>> These reports contain interesting information about whether tests
>>> succeeded
>>> or failed, and how long they took to complete. There is also detailed
>>> information about the environment they ran in.
>>>
>>> It might be valuable to have a window into all these reports across all
>>> Jenkins builds and across all time, and use that to track basic
>>> statistics
>>> about our tests. That could give us basic insight into what tests are
>>> flaky
>>> or slow, and perhaps drive other improvements to our testing
>>> infrastructure
>>> that we can't see just yet.
>>>
>>> Do people think that would be valuable? Do we already have something like
>>> this?
>>>
>>> I'm thinking for starters it might be cool if we automatically uploaded
>>> all
>>> the XML test reports from the Master and the Pull Request builders to an
>>> S3
>>> bucket and just opened it up for the dev community to analyze.
>>>
>>> Nick
>>>
>>

--001a1134babcf9a7a2050a493424--

From dev-return-10814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 01:23:47 2014
Return-Path: <dev-return-10814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9B28C0F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 01:23:46 +0000 (UTC)
Received: (qmail 80758 invoked by uid 500); 16 Dec 2014 01:23:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80688 invoked by uid 500); 16 Dec 2014 01:23:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80675 invoked by uid 99); 16 Dec 2014 01:23:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 01:23:45 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS,TVD_FW_GRAPHIC_NAME_MID
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liujunf@cn.ibm.com designates 202.81.31.142 as permitted sender)
Received: from [202.81.31.142] (HELO e23smtp09.au.ibm.com) (202.81.31.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 01:23:40 +0000
Received: from /spool/local
	by e23smtp09.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <liujunf@cn.ibm.com>;
	Tue, 16 Dec 2014 11:23:18 +1000
Received: from d23dlp02.au.ibm.com (202.81.31.213)
	by e23smtp09.au.ibm.com (202.81.31.206) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Tue, 16 Dec 2014 11:23:15 +1000
Received: from d23relay06.au.ibm.com (d23relay06.au.ibm.com [9.185.63.219])
	by d23dlp02.au.ibm.com (Postfix) with ESMTP id 8C10E2BB0057
	for <dev@spark.apache.org>; Tue, 16 Dec 2014 12:23:14 +1100 (EST)
Received: from d23av04.au.ibm.com (d23av04.au.ibm.com [9.190.235.139])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id sBG1NEH031719524
	for <dev@spark.apache.org>; Tue, 16 Dec 2014 12:23:14 +1100
Received: from d23av04.au.ibm.com (localhost [127.0.0.1])
	by d23av04.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id sBG1NDRO007952
	for <dev@spark.apache.org>; Tue, 16 Dec 2014 12:23:13 +1100
Received: from d23ml028.cn.ibm.com (d23ml028.cn.ibm.com [9.119.32.184])
	by d23av04.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id sBG1N53D007806;
	Tue, 16 Dec 2014 12:23:13 +1100
In-Reply-To: <CAG2iju1G+HGfzHBUFDi+ohckAPhEfh3ZU00xRLf=R3XD94X-6g@mail.gmail.com>
References: <OF70D8AF83.8078E988-ON48257DAA.004B96A6-48257DAA.004BBA0A@cn.ibm.com> <CA+-p3AGr4nRQ_i9jHxOZJGuf7V2+_KF=73cme4g34-aLO+0Eqw@mail.gmail.com> <CAPh_B=bzhHj9gZvBW66q0Z=tfcO4=eeNa=vX8LzCkcLMs=_PfA@mail.gmail.com> <CAPh_B=askooSZon7fxhiHyf3Q84=KqZ1TVYvZBnGVZ8hwnw8kg@mail.gmail.com> <OFF275D2D7.2DB740E6-ON48257DAC.004710E2-48257DAC.0047F840@cn.ibm.com> <CAG2iju1G+HGfzHBUFDi+ohckAPhEfh3ZU00xRLf=R3XD94X-6g@mail.gmail.com>
Subject: Re: Tachyon in Spark
X-KeepSent: 1FEC53FB:80219014-48257DB0:00076A42;
 type=4; name=$KeepSent
To: Haoyuan Li <haoyuan.li@gmail.com>
Cc: Andrew Ash <andrew@andrewash.com>,
        "dev@spark.apache.org" <dev@spark.apache.org>,
        Reynold Xin <rxin@databricks.com>
X-Mailer: Lotus Notes Release 8.5.3FP3 Septem 15, 2011
Message-ID: <OF1FEC53FB.80219014-ON48257DB0.00076A42-48257DB0.000799FC@cn.ibm.com>
From: Jun Feng Liu <liujunf@cn.ibm.com>
Date: Tue, 16 Dec 2014 09:23:03 +0800
X-MIMETrack: Serialize by Router on d23ml028/23/M/IBM(Release 8.5.3FP6HF882 | August 28, 2014) at
 12/16/2014 09:23:12
MIME-Version: 1.0
Content-type: multipart/related; 
	Boundary="0__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2"
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14121601-0033-0000-0000-000000C04DD2
X-Virus-Checked: Checked by ClamAV on apache.org

--0__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2
Content-type: multipart/alternative; 
	Boundary="1__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2"

--1__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2
Content-type: text/plain; charset=US-ASCII
Content-transfer-encoding: quoted-printable


Thanks  the response. I got the point - sounds like todays Spark linage=

dose not push to Tachyon linage.  Would be good to see how it works.

Jun Feng Liu.



                                                                       =
    
             Haoyuan Li                                                =
    
             <haoyuan.li@gmail                                         =
    
             .com>                                                     =
 To 
                                       Jun Feng Liu/China/IBM@IBMCN,   =
    
             2014-12-13 00:17                                          =
 cc 
                                       Reynold Xin <rxin@databricks.com=
>,  
                                       Andrew Ash <andrew@andrewash.com=
>,  
                                       "dev@spark.apache.org"          =
    
                                       <dev@spark.apache.org>          =
    
                                                                   Subj=
ect 
                                       Re: Tachyon in Spark            =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    
                                                                       =
    




Junfeng, by off the heap solution, did you mean "rdd.persist(OFF_HEAP)"=
?
That feature is different from the lineage feature. You can use this
feature (rdd.persist(OFF_HEAP)) now for any Spark version later than 1.=
0.0
with Tachyon without a problem.

Regarding Reynold's last email, those are good points. Tachyon had prov=
ided
this a while ago. We are working on enhancing this feature and the
integration part with Spark.

Thanks,

Haoyuan

On Fri, Dec 12, 2014 at 5:06 AM, Jun Feng Liu <liujunf@cn.ibm.com> wrot=
e:
>
> I think the linage is the key feature of tachyon to reproduce the RDD=

when
> any error happen. Otherwise, there have to be some data replica among=

> tachyon nodes to ensure the data redundancy for fault tolerant - I th=
ink
> tachyon is avoiding to go to this path. Dose it mean the off-heap
solution
> is not ready yet if tachyon linage dose not work right now?
>
> Best Regards
>
>
> *Jun Feng Liu*
> IBM China Systems & Technology Laboratory in Beijing
>
>   ------------------------------
>  [image: 2D barcode - encoded with contact information] *Phone:
*86-10-82452683
>
> * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> [image: IBM]
>
> BLD 28,ZGC Software Park
> No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> China
>
>
>
>
>
>  *Reynold Xin <rxin@databricks.com <rxin@databricks.com>>*
>
> 2014/12/12 10:22
>   To
> Andrew Ash <andrew@andrewash.com>,
> cc
> Jun Feng Liu/China/IBM@IBMCN, "dev@spark.apache.org"
<dev@spark.apache.org
> >
> Subject
> Re: Tachyon in Spark
>
>
>
>
> Actually HY emailed me offline about this and this is supported in th=
e
> latest version of Tachyon. It is a hard problem to push this into
storage;
> need to think about how to handle isolation, resource allocation, etc=
.
>
>
>
https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyo=
n/master/Dependency.java

>
> On Thu, Dec 11, 2014 at 3:54 PM, Reynold Xin <rxin@databricks.com> wr=
ote:
>
> > I don't think the lineage thing is even turned on in Tachyon - it w=
as
> > mostly a research prototype, so I don't think it'd make sense for u=
s to
> use
> > that.
> >
> >
> > On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> >> I'm interested in understanding this as well.  One of the main way=
s
> >> Tachyon
> >> is supposed to realize performance gains without sacrificing
durability
> is
> >> by storing the lineage of data rather than full copies of it (simi=
lar
to
> >> Spark).  But if Spark isn't sending lineage information into Tachy=
on,
> then
> >> I'm not sure how this isn't a durability concern.
> >>
> >> On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu <liujunf@cn.ibm.com>=

> wrote:
> >>
> >> > Dose Spark today really leverage Tachyon linage to process data?=
 It
> >> seems
> >> > like the application should call createDependency function in
> TachyonFS
> >> > to create a new linage node. But I did not find any place call t=
hat
in
> >> > Spark code. Did I missed anything?
> >> >
> >> > Best Regards
> >> >
> >> >
> >> > *Jun Feng Liu*
> >> > IBM China Systems & Technology Laboratory in Beijing
> >> >
> >> >   ------------------------------
> >> >  [image: 2D barcode - encoded with contact information] *Phone:
> >> *86-10-82452683
> >> >
> >> > * E-mail:* *liujunf@cn.ibm.com* <liujunf@cn.ibm.com>
> >> > [image: IBM]
> >> >
> >> > BLD 28,ZGC Software Park
> >> > No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193
> >> > China
> >> >
> >> >
> >> >
> >> >
> >> >
> >>
> >
> >
>
>

--
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
=

--1__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2
Content-type: text/html; charset=US-ASCII
Content-Disposition: inline
Content-transfer-encoding: quoted-printable

<html><body>
<p><font size=3D"2" face=3D"sans-serif">Thanks &nbsp;the response. I go=
t the point - sounds like todays Spark linage dose not push to Tachyon =
linage. &nbsp;Would be good to see how it works. </font><br>
<br>
<font size=3D"2" face=3D"sans-serif">Jun Feng Liu.</font><br>
<br>
<br>
<img width=3D"16" height=3D"16" src=3D"cid:1__=3DC7BBF723DF94ECD28f9e8a=
93df938@cn.ibm.com" border=3D"0" alt=3D"Inactive hide details for Haoyu=
an Li ---2014-12-13 00:32:39---Haoyuan Li &lt;haoyuan.li@gmail.com&gt;"=
><font size=3D"2" color=3D"#424282" face=3D"sans-serif">Haoyuan Li ---2=
014-12-13 00:32:39---Haoyuan Li &lt;haoyuan.li@gmail.com&gt;</font><br>=

<br>

<table width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0">=

<tr valign=3D"top"><td style=3D"background-image:url(cid:2__=3DC7BBF723=
DF94ECD28f9e8a93df938@cn.ibm.com); background-repeat: no-repeat; " widt=
h=3D"40%">
<ul style=3D"padding-left: 72pt"><font size=3D"1" face=3D"sans-serif"><=
b>Haoyuan Li &lt;haoyuan.li@gmail.com&gt;</b></font><font size=3D"1" fa=
ce=3D"sans-serif">&nbsp;</font>
<p><font size=3D"1" face=3D"sans-serif">2014-12-13 00:17</font></ul>
</td><td width=3D"60%">
<table width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0">=

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">To</font></di=
v></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""><br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Ju=
n Feng Liu/China/IBM@IBMCN, </font></ul>
</td></tr>

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">cc</font></di=
v></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""><br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Re=
ynold Xin &lt;rxin@databricks.com&gt;, Andrew Ash &lt;andrew@andrewash.=
com&gt;, &quot;dev@spark.apache.org&quot; &lt;dev@spark.apache.org&gt;<=
/font></ul>
</td></tr>

<tr valign=3D"top"><td width=3D"1%"><img width=3D"58" height=3D"1" src=3D=
"cid:3__=3DC7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""><br>
<div align=3D"right"><font size=3D"1" face=3D"sans-serif">Subject</font=
></div></td><td width=3D"100%"><img width=3D"1" height=3D"1" src=3D"cid=
:3__=3DC7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D"">=
<br>

<ul style=3D"padding-left: 7pt"><font size=3D"1" face=3D"sans-serif">Re=
: Tachyon in Spark</font></ul>
</td></tr>
</table>

<table border=3D"0" cellspacing=3D"0" cellpadding=3D"0">
<tr valign=3D"top"><td width=3D"58"><img width=3D"1" height=3D"1" src=3D=
"cid:3__=3DC7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D=
""></td><td width=3D"336"><img width=3D"1" height=3D"1" src=3D"cid:3__=3D=
C7BBF723DF94ECD28f9e8a93df938@cn.ibm.com" border=3D"0" alt=3D""></td></=
tr>
</table>
</td></tr>
</table>
<br>
<tt><font size=3D"2">Junfeng, by off the heap solution, did you mean &q=
uot;rdd.persist(OFF_HEAP)&quot;?<br>
That feature is different from the lineage feature. You can use this<br=
>
feature (rdd.persist(OFF_HEAP)) now for any Spark version later than 1.=
0.0<br>
with Tachyon without a problem.<br>
<br>
Regarding Reynold's last email, those are good points. Tachyon had prov=
ided<br>
this a while ago. We are working on enhancing this feature and the<br>
integration part with Spark.<br>
<br>
Thanks,<br>
<br>
Haoyuan<br>
<br>
On Fri, Dec 12, 2014 at 5:06 AM, Jun Feng Liu &lt;liujunf@cn.ibm.com&gt=
; wrote:<br>
&gt;<br>
&gt; I think the linage is the key feature of tachyon to reproduce the =
RDD when<br>
&gt; any error happen. Otherwise, there have to be some data replica am=
ong<br>
&gt; tachyon nodes to ensure the data redundancy for fault tolerant - I=
 think<br>
&gt; tachyon is avoiding to go to this path. Dose it mean the off-heap =
solution<br>
&gt; is not ready yet if tachyon linage dose not work right now?<br>
&gt;<br>
&gt; Best Regards<br>
&gt;<br>
&gt;<br>
&gt; *Jun Feng Liu*<br>
&gt; IBM China Systems &amp; Technology Laboratory in Beijing<br>
&gt;<br>
&gt; &nbsp; ------------------------------<br>
&gt; &nbsp;[image: 2D barcode - encoded with contact information] *Phon=
e: *86-10-82452683<br>
&gt;<br>
&gt; * E-mail:* *liujunf@cn.ibm.com* &lt;liujunf@cn.ibm.com&gt;<br>
&gt; [image: IBM]<br>
&gt;<br>
&gt; BLD 28,ZGC Software Park<br>
&gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193<br>
&gt; China<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; &nbsp;*Reynold Xin &lt;rxin@databricks.com &lt;rxin@databricks.com=
&gt;&gt;*<br>
&gt;<br>
&gt; 2014/12/12 10:22<br>
&gt; &nbsp; To<br>
&gt; Andrew Ash &lt;andrew@andrewash.com&gt;,<br>
&gt; cc<br>
&gt; Jun Feng Liu/China/IBM@IBMCN, &quot;dev@spark.apache.org&quot; &lt=
;dev@spark.apache.org<br>
&gt; &gt;<br>
&gt; Subject<br>
&gt; Re: Tachyon in Spark<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Actually HY emailed me offline about this and this is supported in=
 the<br>
&gt; latest version of Tachyon. It is a hard problem to push this into =
storage;<br>
&gt; need to think about how to handle isolation, resource allocation, =
etc.<br>
&gt;<br>
&gt;<br>
&gt; </font></tt><tt><font size=3D"2"><a href=3D"https://github.com/amp=
lab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.ja=
va">https://github.com/amplab/tachyon/blob/master/core/src/main/java/ta=
chyon/master/Dependency.java</a></font></tt><tt><font size=3D"2"><br>
&gt;<br>
&gt; On Thu, Dec 11, 2014 at 3:54 PM, Reynold Xin &lt;rxin@databricks.c=
om&gt; wrote:<br>
&gt;<br>
&gt; &gt; I don't think the lineage thing is even turned on in Tachyon =
- it was<br>
&gt; &gt; mostly a research prototype, so I don't think it'd make sense=
 for us to<br>
&gt; use<br>
&gt; &gt; that.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Thu, Dec 11, 2014 at 3:51 PM, Andrew Ash &lt;andrew@andrew=
ash.com&gt;<br>
&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; I'm interested in understanding this as well. &nbsp;One o=
f the main ways<br>
&gt; &gt;&gt; Tachyon<br>
&gt; &gt;&gt; is supposed to realize performance gains without sacrific=
ing durability<br>
&gt; is<br>
&gt; &gt;&gt; by storing the lineage of data rather than full copies of=
 it (similar to<br>
&gt; &gt;&gt; Spark). &nbsp;But if Spark isn't sending lineage informat=
ion into Tachyon,<br>
&gt; then<br>
&gt; &gt;&gt; I'm not sure how this isn't a durability concern.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu &lt;liujunf=
@cn.ibm.com&gt;<br>
&gt; wrote:<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; &gt; Dose Spark today really leverage Tachyon linage to p=
rocess data? It<br>
&gt; &gt;&gt; seems<br>
&gt; &gt;&gt; &gt; like the application should call createDependency fu=
nction in<br>
&gt; TachyonFS<br>
&gt; &gt;&gt; &gt; to create a new linage node. But I did not find any =
place call that in<br>
&gt; &gt;&gt; &gt; Spark code. Did I missed anything?<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt; Best Regards<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt; *Jun Feng Liu*<br>
&gt; &gt;&gt; &gt; IBM China Systems &amp; Technology Laboratory in Bei=
jing<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt; &nbsp; ------------------------------<br>
&gt; &gt;&gt; &gt; &nbsp;[image: 2D barcode - encoded with contact info=
rmation] *Phone:<br>
&gt; &gt;&gt; *86-10-82452683<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt; * E-mail:* *liujunf@cn.ibm.com* &lt;liujunf@cn.ibm.c=
om&gt;<br>
&gt; &gt;&gt; &gt; [image: IBM]<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt; BLD 28,ZGC Software Park<br>
&gt; &gt;&gt; &gt; No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100=
193<br>
&gt; &gt;&gt; &gt; China<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt; &gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt;<br>
&gt;<br>
<br>
-- <br>
Haoyuan Li<br>
AMPLab, EECS, UC Berkeley<br>
</font></tt><tt><font size=3D"2"><a href=3D"http://www.cs.berkeley.edu/=
~haoyuan/">http://www.cs.berkeley.edu/~haoyuan/</a></font></tt><tt><fon=
t size=3D"2"><br>
</font></tt><br>
</body></html>=


--1__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2--


--0__=C7BBF723DF94ECD28f9e8a93df938690918cC7BBF723DF94ECD2--


From dev-return-10815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 02:24:13 2014
Return-Path: <dev-return-10815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9A9BC335
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 02:24:12 +0000 (UTC)
Received: (qmail 4895 invoked by uid 500); 16 Dec 2014 02:24:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4824 invoked by uid 500); 16 Dec 2014 02:24:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4812 invoked by uid 99); 16 Dec 2014 02:24:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 02:24:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 02:24:06 +0000
Received: by mail-oi0-f46.google.com with SMTP id h136so8907321oig.5
        for <dev@spark.apache.org>; Mon, 15 Dec 2014 18:23:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=6AmqXbaU9xQlIYD3i+4qFPG/HRzG4WNMh5s+3RjapcU=;
        b=CaO4YIv5MulwXeBjZsNsawD1Cwp56b6ijG+YMd9fg66M12gPa+IwF4UgSpRAHq62eY
         xO3CGYzi3VClL9o5fu7Z2odKbJ/R13WHWbSc9t+qR9DrV760y6Pbwifa7s9kjXKnmQf2
         7BoacSBr8CznJEQTN6M+2pgmhw0MfwDhOAsTUx7z5ZwFzTpU7xsWJEBUf2DoX8mXQoYO
         DkwLYRl+/Eg+mFBOhGWlipkNhky6zGXloFMXO0/E6fcVvaw4ZqGZglg3rYna2r8yzG5S
         fV5jT6lnkDfxt4Pnpy1dcgMxtSCnEvXpNxlvsvb8ajpUxwDAAUN/SVnalmI85T/uVrcY
         1k0A==
MIME-Version: 1.0
X-Received: by 10.202.13.132 with SMTP id 126mr19845818oin.75.1418696580570;
 Mon, 15 Dec 2014 18:23:00 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 15 Dec 2014 18:23:00 -0800 (PST)
In-Reply-To: <CA+-p3AGycxJtiRVD8DmPyD_TX4SMSW-VNR+2Jx+avbOu2bAUyA@mail.gmail.com>
References: <CA+-p3AGycxJtiRVD8DmPyD_TX4SMSW-VNR+2Jx+avbOu2bAUyA@mail.gmail.com>
Date: Mon, 15 Dec 2014 18:23:00 -0800
Message-ID: <CABPQxstv92tVcExMeqcHMcwEnXbX4pyoT5C34+eB_G58ZnyORA@mail.gmail.com>
Subject: Re: Governance of the Jenkins whitelist
From: Patrick Wendell <pwendell@gmail.com>
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

The list of admins is maintained by the Amplab as part of their
donation of this infrastructure. The reason why we need to have admins
is that the pull request builder will fetch and then execute arbitrary
user code, so we need to do a security audit before we can approve
testing new patches. Over time when we get to know users we usually
whitelist them so they can test whatever they want.

I can see offline if the Amplab would be open to adding you as an
admin. I think we've added people over time who are very involved in
the community. Just wanted to send this e-mail so people understand
how it works.

- Patrick

On Sat, Dec 13, 2014 at 11:43 PM, Andrew Ash <andrew@andrewash.com> wrote:
> Jenkins is a really valuable tool for increasing quality of incoming
> patches to Spark, but I've noticed that there are often a lot of patches
> waiting for testing because they haven't been approved for testing.
>
> Certain users can instruct Jenkins to run on a PR, or add other users to a
> whitelist. How does governance work for that list of admins?  Meaning who
> is currently on it, and what are the requirements to be on that list?
>
> Can I be permissioned to allow Jenkins to run on certain PRs?  I've often
> come across well-intentioned PRs that are languishing because Jenkins has
> yet to run on them.
>
> Andrew

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 08:39:13 2014
Return-Path: <dev-return-10816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 279CFCD79
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 08:39:13 +0000 (UTC)
Received: (qmail 86518 invoked by uid 500); 16 Dec 2014 08:39:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86441 invoked by uid 500); 16 Dec 2014 08:39:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86430 invoked by uid 99); 16 Dec 2014 08:39:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 08:39:10 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [157.193.49.127] (HELO smtp3.ugent.be) (157.193.49.127)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 08:38:44 +0000
Received: from localhost (mcheck3.ugent.be [157.193.71.89])
	by smtp3.ugent.be (Postfix) with ESMTP id 9582FC4CA;
	Tue, 16 Dec 2014 09:38:23 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp3.ugent.be ([IPv6:::ffff:157.193.49.127])
	by localhost (mcheck3.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id vpQN9Zp_Qa8Z; Tue, 16 Dec 2014 09:38:19 +0100 (CET)
Received: from [157.193.44.243] (gast044c.ugent.be [157.193.44.243])
	(Authenticated sender: ehiggs)
	by smtp3.ugent.be (Postfix) with ESMTPSA id 6DCE8C4DF;
	Tue, 16 Dec 2014 09:38:19 +0100 (CET)
Message-ID: <548FEF7B.2040302@ugent.be>
Date: Tue, 16 Dec 2014 09:38:19 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.3.0
MIME-Version: 1.0
To: Tim Harsch <tharsch@cray.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: running the Terasort example
References: <D0AF7170.4C6E%tharsch@cray.com>
In-Reply-To: <D0AF7170.4C6E%tharsch@cray.com>
Content-Type: text/plain; charset=windows-1252; format=flowed
Content-Transfer-Encoding: 8bit
X-Miltered: at jchkm1 with ID 548FEF7B.001 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 548FEF7B.001 from gast044c.ugent.be/gast044c.ugent.be/157.193.44.243/[157.193.44.243]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 548FEF7B.001 on smtp3.ugent.be : j-chkmail score : . : R=. U=. O=. B=0.000 -> S=0.000
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Tim,
run-example is here:
https://github.com/ehiggs/spark/blob/terasort/bin/run-example

It should be in the repository that you cloned. So if you were at the 
top level of the checkout, run-example would be run as ./bin/run-example.

Yours,
Ewan Higgs

On 12/12/14 01:06, Tim Harsch wrote:
> Hi all,
> I just joined the list, so I dont have a message history that would allow
> me to reply to this post:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-example-
> td9284.html
>
> I am interested in running the terasort example.  I cloned the repo
> https://github.com/ehiggs/spark and did checkout of the terasort branch.
> In the above referenced post Ewan gives the example
>
> # Generate 1M 100 byte records:
>    ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in
>
>
> I dont see a run-example in that repo.  Im sure I am missing something
> basic, or less likely, maybe some changes werent pushed?
>
> Thanks for any help,
> Tim
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 11:36:34 2014
Return-Path: <dev-return-10817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6CB092D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 11:36:34 +0000 (UTC)
Received: (qmail 9531 invoked by uid 500); 16 Dec 2014 11:36:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9456 invoked by uid 500); 16 Dec 2014 11:36:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9444 invoked by uid 99); 16 Dec 2014 11:36:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 11:36:32 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [208.65.145.66] (HELO p01c12o143.mxlogic.net) (208.65.145.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 11:36:04 +0000
Received: from unknown [203.199.118.205] (EHLO p01c12o143.mxlogic.net)
	by p01c12o143.mxlogic.net(mxl_mta-8.2.0-3)
	with ESMTP id 32910945.2b52aac1d940.21493.00-453.53003.p01c12o143.mxlogic.net (envelope-from <jeniba.johnson@lntinfotech.com>);
	Tue, 16 Dec 2014 04:36:03 -0700 (MST)
X-MXL-Hash: 549019236a00338e-63d1a65eccb4721bae938b2c3f3c6d2a1c372b9d
Received: from unknown [203.199.118.205]
	by p01c12o143.mxlogic.net(mxl_mta-8.2.0-3)
	with SMTP id e4a00945.0.7250.00-389.17754.p01c12o143.mxlogic.net (envelope-from <jeniba.johnson@lntinfotech.com>);
	Tue, 16 Dec 2014 03:32:48 -0700 (MST)
X-MXL-Hash: 54900a5019200525-04cabc879c705d8d8c49b0faee7fd9e62fd81840
Received: from vshinmsmbx01.vshodc.lntinfotech.com ([172.17.24.117]) by
 VSHINMSHTCAS01.vshodc.lntinfotech.com ([172.17.24.112]) with mapi; Tue, 16
 Dec 2014 16:02:24 +0530
From: Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>
To: Hari Shreedharan <hshreedharan@cloudera.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Date: Tue, 16 Dec 2014 16:02:21 +0530
Subject: Data Loss - Spark streaming
Thread-Topic: Data Loss - Spark streaming
Thread-Index: AdAZGZdcXWJrWgxDRQWkmNLY3nemIw==
Message-ID: <FCD65A279D2E8F418B46C97F0FF7A00C1E35D3F752@VSHINMSMBX01.vshodc.lntinfotech.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_FCD65A279D2E8F418B46C97F0FF7A00C1E35D3F752VSHINMSMBX01v_"
MIME-Version: 1.0
X-AnalysisOut: [v=2.1 cv=GodW+yFC c=1 sm=1 tr=0 a=iE9DRkMG2U69If1yqqmj6g==]
X-AnalysisOut: [:117 a=iE9DRkMG2U69If1yqqmj6g==:17 a=BLceEmwcHowA:10 a=tP0]
X-AnalysisOut: [R5P8-AAAA:8 a=YlVTAMxIAAAA:8 a=A92cGCtB03wA:10 a=zL_KwGp0s]
X-AnalysisOut: [Nu2zVz60YwA:9 a=CjuIK1q_8ugA:10 a=yMhMjlubAAAA:8 a=SSmOFEA]
X-AnalysisOut: [CAAAA:8 a=nZTIAECv3W1oykx1CqMA:9 a=q1wSlIe4i-WqLzO8:21 a=g]
X-AnalysisOut: [KO2Hq4RSVkA:10 a=UiCQ7L4-1S4A:10 a=hTZeC7Yk6K0A:10 a=frz4A]
X-AnalysisOut: [uCg-hUA:10]
X-Spam: [F=0.5000000000; CM=0.500; MH=0.500(2014121605); S=0.200(2014051901)]
X-MAIL-FROM: <jeniba.johnson@lntinfotech.com>
X-SOURCE-IP: [203.199.118.205]
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_FCD65A279D2E8F418B46C97F0FF7A00C1E35D3F752VSHINMSMBX01v_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I need a clarification, while running streaming examples, suppose the batch=
 interval is set to 5 minutes, after collecting the data from the input sou=
rce(FLUME) and  processing till 5 minutes.
What will happen to the data which is flowing continuously from the input s=
ource to spark streaming ? Will that data be stored somewhere or else the d=
ata will be lost ?
Or else what is the solution to capture each and every data without any los=
s in Spark streaming.

Awaiting for your kind reply.


Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential =
or privileged information for the intended recipient(s). Unintended recipie=
nts are prohibited from taking action on the basis of information in this e=
-mail and using or disseminating the information, and must notify the sende=
r and delete it from their system. L&T Infotech will not accept responsibil=
ity or liability for the accuracy or completeness of, or the presence of an=
y virus or disabling code in this e-mail"

--_000_FCD65A279D2E8F418B46C97F0FF7A00C1E35D3F752VSHINMSMBX01v_--

From dev-return-10818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 17:11:14 2014
Return-Path: <dev-return-10818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E86E9FD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 17:11:14 +0000 (UTC)
Received: (qmail 66135 invoked by uid 500); 16 Dec 2014 17:11:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66062 invoked by uid 500); 16 Dec 2014 17:11:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66051 invoked by uid 99); 16 Dec 2014 17:11:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 17:11:11 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 17:11:07 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id A976DDE2F94
	for <dev@spark.incubator.apache.org>; Tue, 16 Dec 2014 09:09:54 -0800 (PST)
Date: Tue, 16 Dec 2014 10:09:54 -0700 (MST)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418749794513-9804.post@n3.nabble.com>
Subject: RDD data flow
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I was looking at some of the Partition implementations in core/rdd and
getOrCompute(...) in CacheManager.
It appears that getOrCompute(...) returns an InterruptibleIterator, which
delegates to a wrapped Iterator.
That would imply that Partitions should extend Iterator, but that is not
always the case.
For example, these Partitions for these RDDs do not extend Iterator:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala

Why is that? Shouldn't all Partitions be Iterators? Clearly I'm missing
something.

On a related subject, I was thinking of documenting the data flow of RDDs in
more detail. The code is not hard to follow, but it's nice to have a simple
picture with the major components and some explanation of the flow.  The
declaration of Partition is throwing me off.

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-data-flow-tp9804.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 16 18:28:21 2014
Return-Path: <dev-return-10819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D06410336
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 16 Dec 2014 18:28:21 +0000 (UTC)
Received: (qmail 43432 invoked by uid 500); 16 Dec 2014 18:28:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43353 invoked by uid 500); 16 Dec 2014 18:28:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43278 invoked by uid 99); 16 Dec 2014 18:28:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 18:28:18 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tharsch@cray.com designates 64.18.1.187 as permitted sender)
Received: from [64.18.1.187] (HELO exprod6og104.obsmtp.com) (64.18.1.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 16 Dec 2014 18:27:52 +0000
Received: from CFWEX01.americas.cray.com ([136.162.34.11]) (using TLSv1) by exprod6ob104.postini.com ([64.18.5.12]) with SMTP
	ID DSNKVJB5pdrvq4pUOmLe3c5TI/XnnpPQyD6i@postini.com; Tue, 16 Dec 2014 10:27:51 PST
Received: from CFWEX02.americas.cray.com (172.30.74.25) by
 CFWEX01.americas.cray.com (172.30.88.25) with Microsoft SMTP Server (TLS) id
 14.2.347.0; Tue, 16 Dec 2014 12:27:43 -0600
Received: from CFWEX01.americas.cray.com ([169.254.1.181]) by
 cfwex02.americas.cray.com ([169.254.2.115]) with mapi id 14.02.0387.000; Tue,
 16 Dec 2014 12:27:42 -0600
From: Tim Harsch <tharsch@cray.com>
To: Ewan Higgs <ewan.higgs@ugent.be>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Re: running the Terasort example
Thread-Topic: running the Terasort example
Thread-Index: AQHQFZ91EFW+gRhFhEGi9Qc+FDrHCpySUJ+AgAAengA=
Date: Tue, 16 Dec 2014 18:27:42 +0000
Message-ID: <D0B5B901.5026%tharsch@cray.com>
References: <D0AF7170.4C6E%tharsch@cray.com> <548FEF7B.2040302@ugent.be>
In-Reply-To: <548FEF7B.2040302@ugent.be>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.7.141117
x-originating-ip: [172.26.68.48]
Content-Type: text/plain; charset="euc-kr"
Content-ID: <5096CC4BC642EB4290975124D59512C6@cray.com>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:02:32 2014
Return-Path: <dev-return-10820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB1781080F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:02:31 +0000 (UTC)
Received: (qmail 6802 invoked by uid 500); 17 Dec 2014 02:02:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6736 invoked by uid 500); 17 Dec 2014 02:02:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6724 invoked by uid 99); 17 Dec 2014 02:02:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:02:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.170 as permitted sender)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:02:24 +0000
Received: by mail-ie0-f170.google.com with SMTP id rd18so14313505iec.1
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:02:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=dduZf/nOgQRhCtDZRaGftGtLokTfILDkCqtC+DI3iBE=;
        b=lXhQXUSvB7UHToDfKZ3H+KSLgDJjVKnKDHHIjKu/XbFo14I0aCueBFXjiadtekk6Jz
         j3sgYRltA75FgQxCwTKUZEeN1ie1x8f0PwzvkTMmeF275qzOxIAi+t+Ad9v7O/Pi8s9x
         OzoZ9qWBwg5YMwJWS+krJgjG1pjQBclZBSOmhotI6rsaHhwkpkvmSznpq7ONzd1x8ZkD
         /ZB7pACSWslN8omNbDnpetyMxLNUtl5g4frSmThng+yplu/GOSyjaeZNFmHrPrN7ozVB
         TXjjw7dbicmpLMgyZTxTvQKZXy2hzoxBaEeRt0dImdtSha5xnHGNdETrgFilW4/LMcUl
         GZMw==
X-Received: by 10.42.26.147 with SMTP id f19mr33595543icc.84.1418781724158;
 Tue, 16 Dec 2014 18:02:04 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com> <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 17 Dec 2014 02:02:03 +0000
Message-ID: <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Reynold Xin <rxin@databricks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303f64f6aebe99050a5fdce2
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303f64f6aebe99050a5fdce2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

News flash!

>From the latest version of the GitHub API
<https://developer.github.com/v3/repos/statuses/>:

Note that the repo:status OAuth scope
<https://developer.github.com/v3/oauth/#scopes> grants targeted access to
Statuses *without* also granting access to repository code, while the repo
scope grants permission to code as well as statuses.

As I understand it, ASF Infra has said no in the past to granting access to
statuses because it also granted push access.

If so, this no longer appears to be the case.

1) Did I understand correctly and 2) should I open a new request with ASF
Infra to give us OAuth keys with repo:status access?

Nick

On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

Aww, that's a bummer...
>
>
> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> that would require github hooks permission and unfortunately asf infra
>> wouldn't allow that.
>>
>> Maybe they will change their mind one day, but so far we asked about thi=
s
>> and the answer has been no for security reasons.
>>
>> On Saturday, September 6, 2014, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>>> After reading Erik's email, I found this Scala PR
>>> <https://github.com/scala/scala/pull/3963> and immediately noticed a fe=
w
>>> cool things:
>>>
>>>    - Jenkins is hooked directly into GitHub somehow, so you get the "Al=
l
>>> is
>>>    well" message in the merge status window, presumably based on the
>>> last test
>>>    status
>>>    - Jenkins is also tagging the PR based on its test status or need fo=
r
>>>    review
>>>    - Jenkins is also tagging the PR for a specific milestone
>>>
>>> Do any of these things make sense to add to our setup? Or perhaps
>>> something
>>> inspired by these features?
>>>
>>> Nick
>>>
>>
>  =E2=80=8B

--20cf303f64f6aebe99050a5fdce2--

From dev-return-10821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:07:54 2014
Return-Path: <dev-return-10821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0E5010842
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:07:54 +0000 (UTC)
Received: (qmail 11723 invoked by uid 500); 17 Dec 2014 02:07:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11643 invoked by uid 500); 17 Dec 2014 02:07:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11632 invoked by uid 99); 17 Dec 2014 02:07:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:07:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:07:48 +0000
Received: by mail-qg0-f51.google.com with SMTP id e89so11305550qgf.10
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:07:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=2xhXeZVfs3BaZLupvsZTMc27EDjmMLPCnBvkbl89q7M=;
        b=WYYonFa9pbmhA6KmzE1ENG9/A2uXKAEFjTVKhRy10Di2RG9y3gYPVuss49wekkBPrK
         NyeiUDBaH3WRhYko9YdTNW1tsqfmVOepHf09Xap2pO5bnmPH8saH2x1G0Lqq0BsUIYSr
         HnqzMUzr53k0+8BO9+EkCDCyeuXqBvm+Dj0IBC1ChGugZT5vXt1eV+HBY61UKl7STteD
         1LRuIVAhpV++5sD0+8DCwDIH00bq5YVA1crY3XuXCmP54lO7RIu2bSfTH2Oxgo/cIbdw
         t0XggCAt9wtqMwPGRI9D74jkgiwk+/I8eGnHyNk+Scg6i1WiPuUoYHyXqfAdE8qpwhrt
         UJlA==
X-Gm-Message-State: ALoCoQnYJhc7ycHqKHqZwCPaipYHAIO5qUQIL5L50EEDgr+VKHxm/J6DfrCdERtZcEZVa6XSlm+H
X-Received: by 10.224.128.129 with SMTP id k1mr9637229qas.14.1418782027223;
 Tue, 16 Dec 2014 18:07:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Tue, 16 Dec 2014 18:06:47 -0800 (PST)
In-Reply-To: <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
 <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com> <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 16 Dec 2014 18:06:47 -0800
Message-ID: <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132f694bf2abe050a5fee3f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f694bf2abe050a5fee3f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

It's worth trying :)


On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:
>
> News flash!
>
> From the latest version of the GitHub API
> <https://developer.github.com/v3/repos/statuses/>:
>
> Note that the repo:status OAuth scope
> <https://developer.github.com/v3/oauth/#scopes> grants targeted access to
> Statuses *without* also granting access to repository code, while the rep=
o
> scope grants permission to code as well as statuses.
>
> As I understand it, ASF Infra has said no in the past to granting access
> to statuses because it also granted push access.
>
> If so, this no longer appears to be the case.
>
> 1) Did I understand correctly and 2) should I open a new request with ASF
> Infra to give us OAuth keys with repo:status access?
>
> Nick
>
> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> Aww, that's a bummer...
>>
>>
>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>>
>>> that would require github hooks permission and unfortunately asf infra
>>> wouldn't allow that.
>>>
>>> Maybe they will change their mind one day, but so far we asked about
>>> this and the answer has been no for security reasons.
>>>
>>> On Saturday, September 6, 2014, Nicholas Chammas <
>>> nicholas.chammas@gmail.com> wrote:
>>>
>>>> After reading Erik's email, I found this Scala PR
>>>> <https://github.com/scala/scala/pull/3963> and immediately noticed a
>>>> few
>>>> cool things:
>>>>
>>>>    - Jenkins is hooked directly into GitHub somehow, so you get the
>>>> "All is
>>>>    well" message in the merge status window, presumably based on the
>>>> last test
>>>>    status
>>>>    - Jenkins is also tagging the PR based on its test status or need f=
or
>>>>    review
>>>>    - Jenkins is also tagging the PR for a specific milestone
>>>>
>>>> Do any of these things make sense to add to our setup? Or perhaps
>>>> something
>>>> inspired by these features?
>>>>
>>>> Nick
>>>>
>>>
>>  =E2=80=8B
>

--001a1132f694bf2abe050a5fee3f--

From dev-return-10822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:12:57 2014
Return-Path: <dev-return-10822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D41410876
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:12:57 +0000 (UTC)
Received: (qmail 24055 invoked by uid 500); 17 Dec 2014 02:12:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23981 invoked by uid 500); 17 Dec 2014 02:12:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23968 invoked by uid 99); 17 Dec 2014 02:12:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:12:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:12:30 +0000
Received: by mail-oi0-f47.google.com with SMTP id v63so10511967oia.20
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:10:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Qax9HrlaZcfm5HmCc7xUo4uBDHQYwPFT5xxv0/1HCUM=;
        b=iw7IM6dBtowrao8+zQZJIBO+zLF+wiHbAgu9IgRJcQbMYUjAF2hzRAJyH0GBrZmosu
         2hLlzVIEEa/JizGDbPCJ5lKOTowLP1IhXVUuxRe4gAc0XcpeRTUgntxK7NqJqYZqGBD7
         z3SwUg5Ene0Sg+IKgL80HSRHutsqlcAlZbsWY0DJuNyqqJoSLhJzgU7daotWlAkWTOfq
         yPEv1W8rzHMBUgeU6GV056j4GTC+W4Smqly1ukVrT0/Ql0wsWZFKdz7lX85nsnpqF6fk
         PeuwEqPK5/mJiz9MP1xS9QKzi6cGkjwAUAwqtH+L1+tjDkIRfKrxFbFRf8dd3b/AB4aa
         6H0w==
MIME-Version: 1.0
X-Received: by 10.202.219.198 with SMTP id s189mr13466711oig.72.1418782259317;
 Tue, 16 Dec 2014 18:10:59 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 16 Dec 2014 18:10:59 -0800 (PST)
In-Reply-To: <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
	<CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
	<CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
	<CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
	<CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
Date: Tue, 16 Dec 2014 18:10:59 -0800
Message-ID: <CABPQxsvNrh7G2HEiCfb7FAp6KT1ohhgZOso9=nOY9NEK81FBtQ@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah you can do it - just make sure they understand it is a new
feature so we're asking them to revisit it. They looked at it in the
past and they concluded they couldn't give us access without giving us
push access.

- Patrick

On Tue, Dec 16, 2014 at 6:06 PM, Reynold Xin <rxin@databricks.com> wrote:
> It's worth trying :)
>
>
> On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>>
>> News flash!
>>
>> From the latest version of the GitHub API
>> <https://developer.github.com/v3/repos/statuses/>:
>>
>> Note that the repo:status OAuth scope
>> <https://developer.github.com/v3/oauth/#scopes> grants targeted access to
>> Statuses *without* also granting access to repository code, while the repo
>> scope grants permission to code as well as statuses.
>>
>> As I understand it, ASF Infra has said no in the past to granting access
>> to statuses because it also granted push access.
>>
>> If so, this no longer appears to be the case.
>>
>> 1) Did I understand correctly and 2) should I open a new request with ASF
>> Infra to give us OAuth keys with repo:status access?
>>
>> Nick
>>
>> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>> Aww, that's a bummer...
>>>
>>>
>>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>>>
>>>> that would require github hooks permission and unfortunately asf infra
>>>> wouldn't allow that.
>>>>
>>>> Maybe they will change their mind one day, but so far we asked about
>>>> this and the answer has been no for security reasons.
>>>>
>>>> On Saturday, September 6, 2014, Nicholas Chammas <
>>>> nicholas.chammas@gmail.com> wrote:
>>>>
>>>>> After reading Erik's email, I found this Scala PR
>>>>> <https://github.com/scala/scala/pull/3963> and immediately noticed a
>>>>> few
>>>>> cool things:
>>>>>
>>>>>    - Jenkins is hooked directly into GitHub somehow, so you get the
>>>>> "All is
>>>>>    well" message in the merge status window, presumably based on the
>>>>> last test
>>>>>    status
>>>>>    - Jenkins is also tagging the PR based on its test status or need for
>>>>>    review
>>>>>    - Jenkins is also tagging the PR for a specific milestone
>>>>>
>>>>> Do any of these things make sense to add to our setup? Or perhaps
>>>>> something
>>>>> inspired by these features?
>>>>>
>>>>> Nick
>>>>>
>>>>
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:25:51 2014
Return-Path: <dev-return-10823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 54161108F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:25:51 +0000 (UTC)
Received: (qmail 41018 invoked by uid 500); 17 Dec 2014 02:25:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40945 invoked by uid 500); 17 Dec 2014 02:25:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40933 invoked by uid 99); 17 Dec 2014 02:25:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:25:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.181 as permitted sender)
Received: from [209.85.213.181] (HELO mail-ig0-f181.google.com) (209.85.213.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:25:45 +0000
Received: by mail-ig0-f181.google.com with SMTP id l13so8790452iga.8
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:23:39 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=TBsRGsHoZaXFY8pulDCoJRhkty4ROvNHm76i1q5YxgA=;
        b=h+o/nKsXt88MevzL2Cf5FbFI5Gbi9VzgD1PFIhbr0Y/JdRLJAHTtmk/oHqGskdt1RY
         xrl5XJs55GCTtkYH7YAA2E36mFMEBQId5wSVgbT3zang9IKsYbVJB6jxJ6TPETNhZJRW
         49aGA+8HnV9fledC5YFsSQBNRt+HbbSwKvosgmbf2dAM32VvzuOJBkP9RgdfIyeWcXEg
         980zDlyIiy3pABeg9VuKKX7cwfO4ndoUz06udece/6WKwnKFF+rIzhVTInTMwTCSAL73
         JIor/8JanOkSNyViXx/BtQAURz4TvM3bwdCdE6l1FeZfUOWqBdxGLwxihogdhc5dI2G+
         cKig==
X-Received: by 10.42.26.147 with SMTP id f19mr33645679icc.84.1418783019839;
 Tue, 16 Dec 2014 18:23:39 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
 <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
 <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
 <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com> <CABPQxsvNrh7G2HEiCfb7FAp6KT1ohhgZOso9=nOY9NEK81FBtQ@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 17 Dec 2014 02:23:39 +0000
Message-ID: <CAOhmDzeKovXaC30rrDDt7TkB12mWESRhhfhNsH-33g_=ufG_ew@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303f64f6e938ec050a60296b
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303f64f6e938ec050a60296b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Actually, reading through the existing issue opened for this
<https://issues.apache.org/jira/browse/INFRA-7367> back in February, I
don=E2=80=99t see any explanation from ASF Infra as to why they won=E2=80=
=99t grant
permission against the Status API. They just recommended transitioning to
the Apache Jenkins instance.

Reynold/Patrick, was there any discussion elsewhere about this, or should I
just go ahead and try reopening the issue with the appropriate explanation?

Nick
=E2=80=8B

On Tue Dec 16 2014 at 9:10:59 PM Patrick Wendell <pwendell@gmail.com> wrote=
:

> Yeah you can do it - just make sure they understand it is a new
> feature so we're asking them to revisit it. They looked at it in the
> past and they concluded they couldn't give us access without giving us
> push access.
>
> - Patrick
>
> On Tue, Dec 16, 2014 at 6:06 PM, Reynold Xin <rxin@databricks.com> wrote:
> > It's worth trying :)
> >
> >
> > On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >>
> >> News flash!
> >>
> >> From the latest version of the GitHub API
> >> <https://developer.github.com/v3/repos/statuses/>:
> >>
> >> Note that the repo:status OAuth scope
> >> <https://developer.github.com/v3/oauth/#scopes> grants targeted access
> to
> >> Statuses *without* also granting access to repository code, while the
> repo
> >> scope grants permission to code as well as statuses.
> >>
> >> As I understand it, ASF Infra has said no in the past to granting acce=
ss
> >> to statuses because it also granted push access.
> >>
> >> If so, this no longer appears to be the case.
> >>
> >> 1) Did I understand correctly and 2) should I open a new request with
> ASF
> >> Infra to give us OAuth keys with repo:status access?
> >>
> >> Nick
> >>
> >> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
> >> nicholas.chammas@gmail.com> wrote:
> >>
> >> Aww, that's a bummer...
> >>>
> >>>
> >>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >>>
> >>>> that would require github hooks permission and unfortunately asf inf=
ra
> >>>> wouldn't allow that.
> >>>>
> >>>> Maybe they will change their mind one day, but so far we asked about
> >>>> this and the answer has been no for security reasons.
> >>>>
> >>>> On Saturday, September 6, 2014, Nicholas Chammas <
> >>>> nicholas.chammas@gmail.com> wrote:
> >>>>
> >>>>> After reading Erik's email, I found this Scala PR
> >>>>> <https://github.com/scala/scala/pull/3963> and immediately noticed =
a
> >>>>> few
> >>>>> cool things:
> >>>>>
> >>>>>    - Jenkins is hooked directly into GitHub somehow, so you get the
> >>>>> "All is
> >>>>>    well" message in the merge status window, presumably based on th=
e
> >>>>> last test
> >>>>>    status
> >>>>>    - Jenkins is also tagging the PR based on its test status or nee=
d
> for
> >>>>>    review
> >>>>>    - Jenkins is also tagging the PR for a specific milestone
> >>>>>
> >>>>> Do any of these things make sense to add to our setup? Or perhaps
> >>>>> something
> >>>>> inspired by these features?
> >>>>>
> >>>>> Nick
> >>>>>
> >>>>
> >>>
> >>
>

--20cf303f64f6e938ec050a60296b--

From dev-return-10824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:39:37 2014
Return-Path: <dev-return-10824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7D8410976
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:39:37 +0000 (UTC)
Received: (qmail 63095 invoked by uid 500); 17 Dec 2014 02:39:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63021 invoked by uid 500); 17 Dec 2014 02:39:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63010 invoked by uid 99); 17 Dec 2014 02:39:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:39:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:39:30 +0000
Received: by mail-qc0-f176.google.com with SMTP id i17so11536276qcy.35
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:38:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=nVSqC6IhY84V0xpKAIDsUCkDt5mY8jef6uEXlzqWmHw=;
        b=am/BGsOdkTTDSo/35qqxgS8FXhpw+o1DEFxBsYY0I4o+WwSfIYQIIBHBsbeHhLUR35
         wQvLXDBjLSsr77y+7nKvkLz9ywJNEnrtE2zlesdwK7HTedzH3D2yl0mF+x9Oy8apT/sj
         QhyShJM3EylLx94XBHChCqja331UGwXWByby0IiuplTATTD4RpAQutQ2a/cT87ld57F5
         ASHp+sr6Va3jzQrz/9YnZfY/P1OiW5Oh74fV1nRWQ0MWYE3TT3hr7IEXs27ueB4rWzmD
         G6DvAyg0Dhk3jKERVne3fBxXnrQXoiG4TH1WW80XQDOaHatzLBRI/LhWUNcYjOe0yQkN
         NCeA==
X-Gm-Message-State: ALoCoQmgR4YXVkvrIPPT0z7eXo0vRbuWhS2PitOgpSdcut00r4Er1Cpru2XEgOAv9L0o8HzoAncA
X-Received: by 10.140.43.195 with SMTP id e61mr54476630qga.13.1418783883765;
 Tue, 16 Dec 2014 18:38:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Tue, 16 Dec 2014 18:37:43 -0800 (PST)
In-Reply-To: <CAOhmDzeKovXaC30rrDDt7TkB12mWESRhhfhNsH-33g_=ufG_ew@mail.gmail.com>
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
 <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
 <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
 <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
 <CABPQxsvNrh7G2HEiCfb7FAp6KT1ohhgZOso9=nOY9NEK81FBtQ@mail.gmail.com> <CAOhmDzeKovXaC30rrDDt7TkB12mWESRhhfhNsH-33g_=ufG_ew@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 16 Dec 2014 18:37:43 -0800
Message-ID: <CAPh_B=ZsJQps_w8M=+Ub4G8fs=85vi_RverDtyoXx44XRc5CYA@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a74ea67c7dd050a605d34
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a74ea67c7dd050a605d34
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This was the ticket: https://issues.apache.org/jira/browse/INFRA-7918

On Tue, Dec 16, 2014 at 6:23 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:
>
> Actually, reading through the existing issue opened for this
> <https://issues.apache.org/jira/browse/INFRA-7367> back in February, I
> don=E2=80=99t see any explanation from ASF Infra as to why they won=E2=80=
=99t grant
> permission against the Status API. They just recommended transitioning to
> the Apache Jenkins instance.
>
> Reynold/Patrick, was there any discussion elsewhere about this, or should
> I just go ahead and try reopening the issue with the appropriate
> explanation?
>
> Nick
> =E2=80=8B
>
> On Tue Dec 16 2014 at 9:10:59 PM Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Yeah you can do it - just make sure they understand it is a new
>> feature so we're asking them to revisit it. They looked at it in the
>> past and they concluded they couldn't give us access without giving us
>> push access.
>>
>> - Patrick
>>
>> On Tue, Dec 16, 2014 at 6:06 PM, Reynold Xin <rxin@databricks.com> wrote=
:
>> > It's worth trying :)
>> >
>> >
>> > On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
>> > nicholas.chammas@gmail.com> wrote:
>> >>
>> >> News flash!
>> >>
>> >> From the latest version of the GitHub API
>> >> <https://developer.github.com/v3/repos/statuses/>:
>> >>
>> >> Note that the repo:status OAuth scope
>> >> <https://developer.github.com/v3/oauth/#scopes> grants targeted
>> access to
>> >> Statuses *without* also granting access to repository code, while the
>> repo
>> >> scope grants permission to code as well as statuses.
>> >>
>> >> As I understand it, ASF Infra has said no in the past to granting
>> access
>> >> to statuses because it also granted push access.
>> >>
>> >> If so, this no longer appears to be the case.
>> >>
>> >> 1) Did I understand correctly and 2) should I open a new request with
>> ASF
>> >> Infra to give us OAuth keys with repo:status access?
>> >>
>> >> Nick
>> >>
>> >> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
>> >> nicholas.chammas@gmail.com> wrote:
>> >>
>> >> Aww, that's a bummer...
>> >>>
>> >>>
>> >>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com>
>> wrote:
>> >>>
>> >>>> that would require github hooks permission and unfortunately asf
>> infra
>> >>>> wouldn't allow that.
>> >>>>
>> >>>> Maybe they will change their mind one day, but so far we asked abou=
t
>> >>>> this and the answer has been no for security reasons.
>> >>>>
>> >>>> On Saturday, September 6, 2014, Nicholas Chammas <
>> >>>> nicholas.chammas@gmail.com> wrote:
>> >>>>
>> >>>>> After reading Erik's email, I found this Scala PR
>> >>>>> <https://github.com/scala/scala/pull/3963> and immediately noticed
>> a
>> >>>>> few
>> >>>>> cool things:
>> >>>>>
>> >>>>>    - Jenkins is hooked directly into GitHub somehow, so you get th=
e
>> >>>>> "All is
>> >>>>>    well" message in the merge status window, presumably based on t=
he
>> >>>>> last test
>> >>>>>    status
>> >>>>>    - Jenkins is also tagging the PR based on its test status or
>> need for
>> >>>>>    review
>> >>>>>    - Jenkins is also tagging the PR for a specific milestone
>> >>>>>
>> >>>>> Do any of these things make sense to add to our setup? Or perhaps
>> >>>>> something
>> >>>>> inspired by these features?
>> >>>>>
>> >>>>> Nick
>> >>>>>
>> >>>>
>> >>>
>> >>
>>
>

--001a113a74ea67c7dd050a605d34--

From dev-return-10825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 02:43:39 2014
Return-Path: <dev-return-10825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 151E71099C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 02:43:39 +0000 (UTC)
Received: (qmail 73274 invoked by uid 500); 17 Dec 2014 02:43:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73189 invoked by uid 500); 17 Dec 2014 02:43:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73177 invoked by uid 99); 17 Dec 2014 02:43:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:43:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 02:43:11 +0000
Received: by mail-ie0-f177.google.com with SMTP id rd18so13558065iec.8
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 18:41:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=5O2rPpsXoURPa71YXKrpHQL1GAA4DW/UnfOjulfacQM=;
        b=y4e1JdEircprykafINWXyndLom1TM29P00rM1xU6TzaWIgZlwtd3KgGKVdWzz0m3jX
         7CZMfGVo0Pb2s4x1bUNpCwF4LaiB2Gp97o33V6z5wAIMK8qr+4fubNY9/C2f2ycNgu3Q
         Waf7WEvY+3Rh3RXP04V/X0o1vtUrp9meMvfmj6FaujFLpDLST9+nehDvetSxdok8a4qH
         oJ7luTVxPCyPZ8Trui7uZ8LlfhEgiJjNsOnPagT2AGa/EDIlJn06Pby66JtL0rNhduzN
         DUbPPPo1lO4u1mgO+IiOWWufVxIJbpvy7D++ERqKqU3JgmMWFV18IXRJCWbTfcFb6tJu
         /u6Q==
X-Received: by 10.107.130.212 with SMTP id m81mr29206065ioi.46.1418784100220;
 Tue, 16 Dec 2014 18:41:40 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
 <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
 <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
 <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
 <CABPQxsvNrh7G2HEiCfb7FAp6KT1ohhgZOso9=nOY9NEK81FBtQ@mail.gmail.com>
 <CAOhmDzeKovXaC30rrDDt7TkB12mWESRhhfhNsH-33g_=ufG_ew@mail.gmail.com> <CAPh_B=ZsJQps_w8M=+Ub4G8fs=85vi_RverDtyoXx44XRc5CYA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 17 Dec 2014 02:41:39 +0000
Message-ID: <CAOhmDzfdncSOQLtF0v=ZjaA9sVw62oJ_D_uH3zRb8ZvKSmDpTg@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Reynold Xin <rxin@databricks.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f7d7a4e8452050a606a14
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f7d7a4e8452050a606a14
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I see. That=E2=80=99s a separate discussion about closing PRs vs. just upda=
ting the
CI status on individual commits.

I=E2=80=99ll comment on INFRA-7367
<https://issues.apache.org/jira/browse/INFRA-7367>.

Nick
=E2=80=8B

On Tue Dec 16 2014 at 9:38:04 PM Reynold Xin <rxin@databricks.com> wrote:

> This was the ticket: https://issues.apache.org/jira/browse/INFRA-7918
>
> On Tue, Dec 16, 2014 at 6:23 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>>
>> Actually, reading through the existing issue opened for this
>> <https://issues.apache.org/jira/browse/INFRA-7367> back in February, I
>> don=E2=80=99t see any explanation from ASF Infra as to why they won=E2=
=80=99t grant
>> permission against the Status API. They just recommended transitioning t=
o
>> the Apache Jenkins instance.
>>
>> Reynold/Patrick, was there any discussion elsewhere about this, or shoul=
d
>> I just go ahead and try reopening the issue with the appropriate
>> explanation?
>>
>> Nick
>> =E2=80=8B
>>
>> On Tue Dec 16 2014 at 9:10:59 PM Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Yeah you can do it - just make sure they understand it is a new
>>> feature so we're asking them to revisit it. They looked at it in the
>>> past and they concluded they couldn't give us access without giving us
>>> push access.
>>>
>>> - Patrick
>>>
>>> On Tue, Dec 16, 2014 at 6:06 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>> > It's worth trying :)
>>> >
>>> >
>>> > On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
>>> > nicholas.chammas@gmail.com> wrote:
>>> >>
>>> >> News flash!
>>> >>
>>> >> From the latest version of the GitHub API
>>> >> <https://developer.github.com/v3/repos/statuses/>:
>>> >>
>>> >> Note that the repo:status OAuth scope
>>> >> <https://developer.github.com/v3/oauth/#scopes> grants targeted
>>> access to
>>> >> Statuses *without* also granting access to repository code, while th=
e
>>> repo
>>> >> scope grants permission to code as well as statuses.
>>> >>
>>> >> As I understand it, ASF Infra has said no in the past to granting
>>> access
>>> >> to statuses because it also granted push access.
>>> >>
>>> >> If so, this no longer appears to be the case.
>>> >>
>>> >> 1) Did I understand correctly and 2) should I open a new request wit=
h
>>> ASF
>>> >> Infra to give us OAuth keys with repo:status access?
>>> >>
>>> >> Nick
>>> >>
>>> >> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
>>> >> nicholas.chammas@gmail.com> wrote:
>>> >>
>>> >> Aww, that's a bummer...
>>> >>>
>>> >>>
>>> >>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>> >>>
>>> >>>> that would require github hooks permission and unfortunately asf
>>> infra
>>> >>>> wouldn't allow that.
>>> >>>>
>>> >>>> Maybe they will change their mind one day, but so far we asked abo=
ut
>>> >>>> this and the answer has been no for security reasons.
>>> >>>>
>>> >>>> On Saturday, September 6, 2014, Nicholas Chammas <
>>> >>>> nicholas.chammas@gmail.com> wrote:
>>> >>>>
>>> >>>>> After reading Erik's email, I found this Scala PR
>>> >>>>> <https://github.com/scala/scala/pull/3963> and immediately
>>> noticed a
>>> >>>>> few
>>> >>>>> cool things:
>>> >>>>>
>>> >>>>>    - Jenkins is hooked directly into GitHub somehow, so you get t=
he
>>> >>>>> "All is
>>> >>>>>    well" message in the merge status window, presumably based on
>>> the
>>> >>>>> last test
>>> >>>>>    status
>>> >>>>>    - Jenkins is also tagging the PR based on its test status or
>>> need for
>>> >>>>>    review
>>> >>>>>    - Jenkins is also tagging the PR for a specific milestone
>>> >>>>>
>>> >>>>> Do any of these things make sense to add to our setup? Or perhaps
>>> >>>>> something
>>> >>>>> inspired by these features?
>>> >>>>>
>>> >>>>> Nick
>>> >>>>>
>>> >>>>
>>> >>>
>>> >>
>>>
>>

--001a113f7d7a4e8452050a606a14--

From dev-return-10826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 03:29:52 2014
Return-Path: <dev-return-10826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CEF310B1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 03:29:52 +0000 (UTC)
Received: (qmail 27310 invoked by uid 500); 17 Dec 2014 03:29:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27234 invoked by uid 500); 17 Dec 2014 03:29:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27216 invoked by uid 99); 17 Dec 2014 03:29:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 03:29:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 03:29:24 +0000
Received: by mail-ig0-f178.google.com with SMTP id hl2so8245955igb.17
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 19:27:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=BwKIHs131/9UpQRT0oNrVwfld5vlQTP5kO6C4CJD80g=;
        b=W2fgV/BiCdNQwUUHdoq2OMYLReGwHqdfjzHVFYXuuCYFbNiENyWqifnRRDWYEC71FG
         v27OnhdKEsDHq6p2w4C+35nchuEG1qOO2OrFGQcQJdosfgzWAawhQVFsmrbqtCDPdazh
         uS6M2JPk4OkudWNVFESU61g0++1xsSMUzjP7K2giJAv1q9zkbdOwd/3tYm5caJKJ9GJi
         GwFqp+3NiSK8evONehtt7kti/rZlBoRkg+ixHtmGD55TmM6LHkNRBW0oMvOBAjxekr1n
         G56D9Pc6+ggO2+i3VR8q/ChcnuOY71QjfTn9A0bY4eGInLZFf6eDhfM2oHNG2Dh6E9aw
         Aiow==
X-Received: by 10.43.89.68 with SMTP id bd4mr34486784icc.63.1418786872603;
 Tue, 16 Dec 2014 19:27:52 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzc1SacTjPP24CrVNXiNN1M=Ey5oMaV_z_Yq5jcWCHuLrg@mail.gmail.com>
 <CAPh_B=bTeRfcHch8TEbcpZMKmRqLwBKHWno4MphU4pZUarKSdg@mail.gmail.com>
 <CAOhmDze9wQwvfAzPBD=76KxDEm+ZzBM4ToWeYKsU4GKU_bRhkw@mail.gmail.com>
 <CAOhmDzfg7sLQV=tJd14r7qVSF=L8JZ47ws3NJpX3zJ5usbVs_A@mail.gmail.com>
 <CAPh_B=b69q4K=TFOhRoqDRdM3ko98Ski1jx-X5pd=GWY4ZyO+w@mail.gmail.com>
 <CABPQxsvNrh7G2HEiCfb7FAp6KT1ohhgZOso9=nOY9NEK81FBtQ@mail.gmail.com>
 <CAOhmDzeKovXaC30rrDDt7TkB12mWESRhhfhNsH-33g_=ufG_ew@mail.gmail.com>
 <CAPh_B=ZsJQps_w8M=+Ub4G8fs=85vi_RverDtyoXx44XRc5CYA@mail.gmail.com> <CAOhmDzfdncSOQLtF0v=ZjaA9sVw62oJ_D_uH3zRb8ZvKSmDpTg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 17 Dec 2014 03:27:51 +0000
Message-ID: <CAOhmDzf2SQbQjw5v40yuQs99N_kUvEE+gatZ0zxUNYgmcNpVcQ@mail.gmail.com>
Subject: Re: Scala's Jenkins setup looks neat
To: Reynold Xin <rxin@databricks.com>
Cc: Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec517c6c08dbe28050a610f20
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec517c6c08dbe28050a610f20
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Shot down again.
<https://issues.apache.org/jira/browse/INFRA-7367?focusedCommentId=3D142493=
82&page=3Dcom.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#=
comment-14249382>
=E2=80=8B

On Tue Dec 16 2014 at 9:41:39 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> I see. That=E2=80=99s a separate discussion about closing PRs vs. just up=
dating
> the CI status on individual commits.
>
> I=E2=80=99ll comment on INFRA-7367
> <https://issues.apache.org/jira/browse/INFRA-7367>.
>
> Nick
> =E2=80=8B
>
> On Tue Dec 16 2014 at 9:38:04 PM Reynold Xin <rxin@databricks.com> wrote:
>
>> This was the ticket: https://issues.apache.org/jira/browse/INFRA-7918
>>
>> On Tue, Dec 16, 2014 at 6:23 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>>
>>> Actually, reading through the existing issue opened for this
>>> <https://issues.apache.org/jira/browse/INFRA-7367> back in February, I
>>> don=E2=80=99t see any explanation from ASF Infra as to why they won=E2=
=80=99t grant
>>> permission against the Status API. They just recommended transitioning =
to
>>> the Apache Jenkins instance.
>>>
>>> Reynold/Patrick, was there any discussion elsewhere about this, or
>>> should I just go ahead and try reopening the issue with the appropriate
>>> explanation?
>>>
>>> Nick
>>> =E2=80=8B
>>>
>>> On Tue Dec 16 2014 at 9:10:59 PM Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>>> Yeah you can do it - just make sure they understand it is a new
>>>> feature so we're asking them to revisit it. They looked at it in the
>>>> past and they concluded they couldn't give us access without giving us
>>>> push access.
>>>>
>>>> - Patrick
>>>>
>>>> On Tue, Dec 16, 2014 at 6:06 PM, Reynold Xin <rxin@databricks.com>
>>>> wrote:
>>>> > It's worth trying :)
>>>> >
>>>> >
>>>> > On Tue, Dec 16, 2014 at 6:02 PM, Nicholas Chammas <
>>>> > nicholas.chammas@gmail.com> wrote:
>>>> >>
>>>> >> News flash!
>>>> >>
>>>> >> From the latest version of the GitHub API
>>>> >> <https://developer.github.com/v3/repos/statuses/>:
>>>> >>
>>>> >> Note that the repo:status OAuth scope
>>>> >> <https://developer.github.com/v3/oauth/#scopes> grants targeted
>>>> access to
>>>> >> Statuses *without* also granting access to repository code, while
>>>> the repo
>>>> >> scope grants permission to code as well as statuses.
>>>> >>
>>>> >> As I understand it, ASF Infra has said no in the past to granting
>>>> access
>>>> >> to statuses because it also granted push access.
>>>> >>
>>>> >> If so, this no longer appears to be the case.
>>>> >>
>>>> >> 1) Did I understand correctly and 2) should I open a new request
>>>> with ASF
>>>> >> Infra to give us OAuth keys with repo:status access?
>>>> >>
>>>> >> Nick
>>>> >>
>>>> >> On Sat Sep 06 2014 at 1:29:53 PM Nicholas Chammas <
>>>> >> nicholas.chammas@gmail.com> wrote:
>>>> >>
>>>> >> Aww, that's a bummer...
>>>> >>>
>>>> >>>
>>>> >>> On Sat, Sep 6, 2014 at 1:10 PM, Reynold Xin <rxin@databricks.com>
>>>> wrote:
>>>> >>>
>>>> >>>> that would require github hooks permission and unfortunately asf
>>>> infra
>>>> >>>> wouldn't allow that.
>>>> >>>>
>>>> >>>> Maybe they will change their mind one day, but so far we asked
>>>> about
>>>> >>>> this and the answer has been no for security reasons.
>>>> >>>>
>>>> >>>> On Saturday, September 6, 2014, Nicholas Chammas <
>>>> >>>> nicholas.chammas@gmail.com> wrote:
>>>> >>>>
>>>> >>>>> After reading Erik's email, I found this Scala PR
>>>> >>>>> <https://github.com/scala/scala/pull/3963> and immediately
>>>> noticed a
>>>> >>>>> few
>>>> >>>>> cool things:
>>>> >>>>>
>>>> >>>>>    - Jenkins is hooked directly into GitHub somehow, so you get
>>>> the
>>>> >>>>> "All is
>>>> >>>>>    well" message in the merge status window, presumably based on
>>>> the
>>>> >>>>> last test
>>>> >>>>>    status
>>>> >>>>>    - Jenkins is also tagging the PR based on its test status or
>>>> need for
>>>> >>>>>    review
>>>> >>>>>    - Jenkins is also tagging the PR for a specific milestone
>>>> >>>>>
>>>> >>>>> Do any of these things make sense to add to our setup? Or perhap=
s
>>>> >>>>> something
>>>> >>>>> inspired by these features?
>>>> >>>>>
>>>> >>>>> Nick
>>>> >>>>>
>>>> >>>>
>>>> >>>
>>>> >>
>>>>
>>>

--bcaec517c6c08dbe28050a610f20--

From dev-return-10827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 04:23:26 2014
Return-Path: <dev-return-10827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EA2710C19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 04:23:26 +0000 (UTC)
Received: (qmail 84157 invoked by uid 500); 17 Dec 2014 04:23:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84086 invoked by uid 500); 17 Dec 2014 04:23:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84075 invoked by uid 99); 17 Dec 2014 04:23:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 04:23:24 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: encountered temporary error during SPF processing of domain of gbowyer@fastmail.co.uk)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 04:23:19 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 26751DEFDAE
	for <dev@spark.incubator.apache.org>; Tue, 16 Dec 2014 20:22:40 -0800 (PST)
Date: Tue, 16 Dec 2014 21:22:39 -0700 (MST)
From: GregBowyer <gbowyer@fastmail.co.uk>
To: dev@spark.incubator.apache.org
Message-ID: <1418790159125-9813.post@n3.nabble.com>
In-Reply-To: <CAPh_B=Y45mm=Kq+USZZss5BO411d+fbyiH+csaD4CPUavsxi6A@mail.gmail.com>
References: <CADnDY-X8BpPnX0D7cV_zM9ewiuFNw6eZBuEhrxfpsxketew=fw@mail.gmail.com> <CAPh_B=Y45mm=Kq+USZZss5BO411d+fbyiH+csaD4CPUavsxi6A@mail.gmail.com>
Subject: Re: Interested in contributing to GraphX in Python
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have been thinking about this for a little while and I wonder if it makes
sense to look at forcing off heap mmap storage what can be shared with
python.

The idea would be that java makes a DirectByteBuffer (or similar) with
python doing memoryview over that buffer.

Then for all except for real objects everything could be written in a numpy
friendly format into the mmap ?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Interested-in-contributing-to-GraphX-in-Python-tp7629p9813.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 05:02:26 2014
Return-Path: <dev-return-10828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 34E9810D02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 05:02:26 +0000 (UTC)
Received: (qmail 27748 invoked by uid 500); 17 Dec 2014 05:02:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27669 invoked by uid 500); 17 Dec 2014 05:02:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27652 invoked by uid 99); 17 Dec 2014 05:02:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:02:24 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:02:20 +0000
Received: by mail-ob0-f172.google.com with SMTP id va8so2104581obc.3
        for <dev@spark.incubator.apache.org>; Tue, 16 Dec 2014 21:01:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=46bEzGceTkXQLmdtABnCCKqBRNHXJSJJmVQxT2y8mPE=;
        b=huMW6wy7yQpmp7Y37cAWSTf6HEo2w/EvSYBLJ/dtK/92+ZGgc3/f4OuPco9Js7h1NY
         0W/JewUKlMQY1vCOnHqZ5ZUUnTrGUUUWRhOm+SYjyyBHLJ3EL8GyFW7CICi1Xp9Vnmxh
         BqxCyz9d0Mt4ZnEDbHZxbv2aWdTKV5g1nQaTNcbwdTvX3tvTeICgK6CpSNyj5z5iciF2
         bK5NUB261tR9b9xlaE7a7ssim5ZsNsv+S1/7xGDzB13mjPpopwPg0T5lvw3W2lOJcpTs
         kZiF+RkWRdqqcH5Ci3ELErFsROm1VvunSx0l1ULDOJ/vzgqU5DbrERTjj4msTE5zzL33
         WFgA==
MIME-Version: 1.0
X-Received: by 10.60.50.137 with SMTP id c9mr24552050oeo.83.1418792519643;
 Tue, 16 Dec 2014 21:01:59 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 16 Dec 2014 21:01:59 -0800 (PST)
In-Reply-To: <1418749794513-9804.post@n3.nabble.com>
References: <1418749794513-9804.post@n3.nabble.com>
Date: Tue, 16 Dec 2014 21:01:59 -0800
Message-ID: <CABPQxsvM69K2uFU9Gb8TyCHS3xum5jVP-pxkkPb8ujf_yifXAw@mail.gmail.com>
Subject: Re: RDD data flow
From: Patrick Wendell <pwendell@gmail.com>
To: Madhu <madhu@madhu.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

> Why is that? Shouldn't all Partitions be Iterators? Clearly I'm missing
> something.

The Partition itself doesn't need to be an iterator - the iterator
comes from the result of compute(partition). The Partition is just an
identifier for that partition, not the data itself. Take a look at the
signature for compute() in the RDD class.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L97

>
> On a related subject, I was thinking of documenting the data flow of RDDs in
> more detail. The code is not hard to follow, but it's nice to have a simple
> picture with the major components and some explanation of the flow.  The
> declaration of Partition is throwing me off.
>
> Thanks!
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-data-flow-tp9804.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 05:21:05 2014
Return-Path: <dev-return-10829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74B8F10D6C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 05:21:05 +0000 (UTC)
Received: (qmail 47276 invoked by uid 500); 17 Dec 2014 05:21:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47198 invoked by uid 500); 17 Dec 2014 05:21:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47186 invoked by uid 99); 17 Dec 2014 05:21:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:21:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:20:37 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so2139555obc.6
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 21:20:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=vQAmZtkc9N7fnmiieNLBBq2MS9Qp2eG1A6FSqTS18FU=;
        b=BmzgPxO3IxWFtcxDygNw7RSb6103YXS8ktk2mkCMNgWqAs5wlPt3LyE9GtENyf6oZe
         FcH32qbGMlrZfRnrj8R857095/K/HeMSNW8OtajP2gLaS7iwwpPeyT+MlrkROIAaq5yr
         Qjn7zOrrW714Bzip+Zc++Hdu0FcqEfDda3ZfFA/bLZRcXF5MgJmYdqquXc77Cyncso5k
         Ig1rkPcv9jRn46eK2nGkgIhwz4X7x0IJDvMyt2ATC8YUkv1C/kuFIRicKBN3bHj2tHtD
         GGk7t1EYjazgyLqnKMOKWILEwLPZfKrd4t62WQwQfKv1r3YdscdcS2Hh2yHeFQcb2Vsq
         rJEg==
MIME-Version: 1.0
X-Received: by 10.202.219.198 with SMTP id s189mr13785913oig.72.1418793629657;
 Tue, 16 Dec 2014 21:20:29 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 16 Dec 2014 21:20:29 -0800 (PST)
Date: Tue, 16 Dec 2014 21:20:29 -0800
Message-ID: <CABPQxsvf+tatpfWEUk4JT=19m920gEOOGxJhgXgG7vuFC3pGDA@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote has PASSED with 12 +1 votes (8 binding) and no 0 or -1 votes:

+1:
Matei Zaharia*
Madhu Siddalingaiah
Reynold Xin*
Sandy Ryza
Josh Rozen*
Mark Hamstra*
Denny Lee
Tom Graves*
GuiQiang Li
Nick Pentreath*
Sean McNamara*
Patrick Wendell*

0:

-1:

I'll finalize and package this release in the next 48 hours. Thanks to
everyone who contributed.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 05:22:12 2014
Return-Path: <dev-return-10830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDFDC10D7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 05:22:11 +0000 (UTC)
Received: (qmail 51018 invoked by uid 500); 17 Dec 2014 05:22:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50947 invoked by uid 500); 17 Dec 2014 05:22:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50493 invoked by uid 99); 17 Dec 2014 05:22:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:22:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:21:44 +0000
Received: by mail-ob0-f170.google.com with SMTP id wp18so2156350obc.1
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 21:20:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=GfLraOmNpPaP9aKhcVMSyosi95CRISD5t52lJQU/0eA=;
        b=OJ4qwCf3ackZSjNBgbmdTWLvtrmuwKwfMH7LXogvNUnH0nGZoWSvyxyNEAiuyrmWhK
         0V3ciaAlxAZBvvH9kYkiDo60CNq84Y5XFBxOHfEnOi3nnA8xXKf1QodecPqn9DZjyi6O
         xkriSkjYCyeuoKs9zan0s/LS1PRjUG5yENR8IZNOnrsuh/Joq3tBxi+bqCe0DqByuw8a
         3/l+29AtXxdU+j9SNFht/yLoOXKy9wMhjbSjR3cTZBfA642I7cG/QDJ9tBezTwFK3dL/
         Yw2LQOzycRL9ubjQeUgAfkkfxOEYPgWo+9y03559QtX/J9jUlzxY2QK+8/32Gid/nDqO
         3KdA==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr2307289oit.100.1418793613178;
 Tue, 16 Dec 2014 21:20:13 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 16 Dec 2014 21:20:13 -0800 (PST)
In-Reply-To: <CE681A77-34B0-4A6F-92E9-C7547B16BD61@webtrends.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
	<CAPh_B=aO77txJJo=EKON1EGwsUawocY=eBv9n0_FiAMUxQjsAA@mail.gmail.com>
	<CE681A77-34B0-4A6F-92E9-C7547B16BD61@webtrends.com>
Date: Tue, 16 Dec 2014 21:20:13 -0800
Message-ID: <CABPQxssETXUutrXzAdvzjebQn_+gBbidwihi8ODPPrZETjpc_A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean McNamara <Sean.McNamara@webtrends.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'm closing this vote now, will send results in a new thread.

On Sat, Dec 13, 2014 at 12:47 PM, Sean McNamara
<Sean.McNamara@webtrends.com> wrote:
> +1 tested on OS X and deployed+tested our apps via YARN into our staging cluster.
>
> Sean
>
>
>> On Dec 11, 2014, at 10:40 AM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> +1
>>
>> Tested on OS X.
>>
>> On Wednesday, December 10, 2014, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>>> Please vote on releasing the following candidate as Apache Spark version
>>> 1.2.0!
>>>
>>> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
>>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
>>>
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
>>>
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1055/
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
>>>
>>> Please vote on releasing this package as Apache Spark 1.2.0!
>>>
>>> The vote is open until Saturday, December 13, at 21:00 UTC and passes
>>> if a majority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 1.2.0
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> == What justifies a -1 vote for this release? ==
>>> This vote is happening relatively late into the QA period, so
>>> -1 votes should only occur for significant regressions from
>>> 1.0.2. Bugs already present in 1.1.X, minor
>>> regressions, or bugs related to new features will not block this
>>> release.
>>>
>>> == What default changes should I be aware of? ==
>>> 1. The default value of "spark.shuffle.blockTransferService" has been
>>> changed to "netty"
>>> --> Old behavior can be restored by switching to "nio"
>>>
>>> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
>>> --> Old behavior can be restored by setting "spark.shuffle.manager" to
>>> "hash".
>>>
>>> == How does this differ from RC1 ==
>>> This has fixes for a handful of issues identified - some of the
>>> notable fixes are:
>>>
>>> [Core]
>>> SPARK-4498: Standalone Master can fail to recognize completed/failed
>>> applications
>>>
>>> [SQL]
>>> SPARK-4552: Query for empty parquet table in spark sql hive get
>>> IllegalArgumentException
>>> SPARK-4753: Parquet2 does not prune based on OR filters on partition
>>> columns
>>> SPARK-4761: With JDBC server, set Kryo as default serializer and
>>> disable reference tracking
>>> SPARK-4785: When called with arguments referring column fields, PMOD
>>> throws NPE
>>>
>>> - Patrick
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
>>> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;>
>>>
>>>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 05:29:18 2014
Return-Path: <dev-return-10831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A6FC510DC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 05:29:18 +0000 (UTC)
Received: (qmail 59993 invoked by uid 500); 17 Dec 2014 05:29:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59923 invoked by uid 500); 17 Dec 2014 05:29:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59911 invoked by uid 99); 17 Dec 2014 05:29:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:29:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 05:29:10 +0000
Received: by mail-ob0-f170.google.com with SMTP id wp18so2175625obc.1
        for <dev@spark.apache.org>; Tue, 16 Dec 2014 21:28:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=q81OmP/VZi2DpcyLW27C7mhrD6uRfnYqcQgNUsC3Zp4=;
        b=BUYl4CjHdkIVG5PMeFONJ506FlGoDCoqbgc6DFb8XEJqIYLb+r7jKx3ul7pn/R+WR4
         rkj2U3DYqBfGq1lNR0z44oAfovXokLT17PP+IRVsbk4ThYgW6ZQV+Y91rNhCSl4OtLYW
         f/FmhCDZ79Dx5AYD73QaVdTgveuQqBepHAEYjgOos+Q3HXWnnBVr+YSocwyLWS9L382C
         R8Cd8LJx1bj1uiK0I0wrxzTO7V60vtYhFr1gZhJ0tQ3sCS1OAx7AB8R45eG8Xi4qf1vS
         0JDcSSKPrmGoRMAZVnCjoaDKD3iSb5PrMm/5HoV8sa4Jzg3PLLAR3e3+MKPcmTC4vsql
         2TPA==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr2321025oit.100.1418794085332;
 Tue, 16 Dec 2014 21:28:05 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Tue, 16 Dec 2014 21:28:05 -0800 (PST)
Date: Tue, 16 Dec 2014 21:28:05 -0800
Message-ID: <CABPQxssLQBy_cXhrTkv_FUJxpLHJd6GfL+j1U-jnryhviwAGuA@mail.gmail.com>
Subject: [ANNOUNCE] Requiring JIRA for inclusion in release credits
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Due to the very high volume of contributions, we're switching to an
automated process for generating release credits. This process relies
on JIRA for categorizing contributions, so it's not possible for us to
provide credits in the case where users submit pull requests with no
associated JIRA.

This needed to be automated because, with more than 1000 commits per
release, finding proper names for every commit and summarizing
contributions was taking on the order of days of time.

For 1.2.0 there were around 100 commits that did not have JIRA's. I'll
try to manually merge these into the credits, but please e-mail me
directly if you are not credited once the release notes are posted.
The notes should be posted within 48 hours of right now.

We already ask that users include a JIRA for pull requests, but now it
will be required for proper attribution. I've updated the contributing
guide on the wiki to reflect this.

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 07:42:37 2014
Return-Path: <dev-return-10832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A71790AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 07:42:37 +0000 (UTC)
Received: (qmail 51703 invoked by uid 500); 17 Dec 2014 07:42:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51628 invoked by uid 500); 17 Dec 2014 07:42:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51617 invoked by uid 99); 17 Dec 2014 07:42:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 07:42:36 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [157.193.71.182] (HELO smtp1.ugent.be) (157.193.71.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 07:42:31 +0000
Received: from localhost (mcheck3.ugent.be [157.193.71.89])
	by smtp1.ugent.be (Postfix) with ESMTP id A3BDB80D9;
	Wed, 17 Dec 2014 08:42:08 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp1.ugent.be ([IPv6:::ffff:157.193.71.182])
	by localhost (mcheck3.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id EAQP_hOxkOEh; Wed, 17 Dec 2014 08:42:07 +0100 (CET)
Received: from [192.168.0.149] (78-22-121-243.access.telenet.be [78.22.121.243])
	(Authenticated sender: ehiggs)
	by smtp1.ugent.be (Postfix) with ESMTPSA id 209898031;
	Wed, 17 Dec 2014 08:42:07 +0100 (CET)
Content-Type: text/plain; charset=utf-8
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: running the Terasort example
From: Ewan Higgs <ewan.higgs@ugent.be>
In-Reply-To: <D0B5B901.5026%tharsch@cray.com>
Date: Wed, 17 Dec 2014 08:42:07 +0100
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <9B1E9CD6-D61E-4690-A412-657F2FE28F47@ugent.be>
References: <D0AF7170.4C6E%tharsch@cray.com> <548FEF7B.2040302@ugent.be> <D0B5B901.5026%tharsch@cray.com>
To: Tim Harsch <tharsch@cray.com>
X-Mailer: Apple Mail (2.1993)
X-Miltered: at jchkm1 with ID 549133CE.005 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 549133CE.005 from 78-22-121-243.access.telenet.be/78-22-121-243.access.telenet.be/78.22.121.243/[192.168.0.149]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 549133CE.005 on smtp1.ugent.be : j-chkmail score : X : R=. U=. O=## B=0.000 -> S=0.166
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Tim,

> On 16 Dec 2014, at 19:27, Tim Harsch <tharsch@cray.com> wrote:
>=20
> Hi Ewan,
> Thanks, I think I was just a bit confused at the time, I was looking =
at
> the spark-perf repo when there was the problem (uh.. ok)=E2=80=A6
>=20
The PR that I am working on is indeed for spark-perf.=20

> =E2=80=A6snip...
>=20
>=20
> I can get past this by setting hadoop.version to 2.5.0 in the parent =
pom.
>=20
I wasn=E2=80=99t sure how to get this working across all the Hadoop =
versions so I made it work with 2.4.0 and above. If you have advice on =
back porting this then I=E2=80=99m happy to implement it.

NB, TeraValidate may not be functioning appropriately. If you have =
trouble with it, I recommend using the Hadoop version.

Yours,
Ewan

> Thanks,
> Tim
>=20
>=20
> On 12/16/14, 12:38 AM, "Ewan Higgs" <ewan.higgs@ugent.be> wrote:
>=20
>> Hi Tim,
>> run-example is here:
>> https://github.com/ehiggs/spark/blob/terasort/bin/run-example
>>=20
>> It should be in the repository that you cloned. So if you were at the
>> top level of the checkout, run-example would be run as =
./bin/run-example.
>>=20
>> Yours,
>> Ewan Higgs
>>=20
>> On 12/12/14 01:06, Tim Harsch wrote:
>>> Hi all,
>>> I just joined the list, so I don=C2=B9t have a message history that =
would
>>> allow
>>> me to reply to this post:
>>>=20
>>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-exampl
>>> e-
>>> td9284.html
>>>=20
>>> I am interested in running the terasort example.  I cloned the repo
>>> https://github.com/ehiggs/spark and did checkout of the terasort =
branch.
>>> In the above referenced post Ewan gives the example
>>>=20
>>> # Generate 1M 100 byte records:
>>>   ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in
>>>=20
>>>=20
>>> I don=C2=B9t see a =C2=B3run-example=C2=B2 in that repo.  I=C2=B9m =
sure I am missing
>>> something
>>> basic, or less likely, maybe some changes weren=C2=B9t pushed?
>>>=20
>>> Thanks for any help,
>>> Tim
>>>=20
>>>=20
>>> =
---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>=20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 10:26:50 2014
Return-Path: <dev-return-10833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7938396A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 10:26:50 +0000 (UTC)
Received: (qmail 34860 invoked by uid 500); 17 Dec 2014 10:26:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34784 invoked by uid 500); 17 Dec 2014 10:26:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34771 invoked by uid 99); 17 Dec 2014 10:26:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 10:26:46 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xuelincao@yahoo.com designates 98.138.91.213 as permitted sender)
Received: from [98.138.91.213] (HELO nm25-vm2.bullet.mail.ne1.yahoo.com) (98.138.91.213)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 10:26:39 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=IBDCwtXWLEo89PEJwgafaeQmpmrky6MbNGqXLAoDQ/dDu6tPFqkv84M7QPui4pdKlPrnAgEmJg7dbGarAHy7bp8UCQXV2v21cP1XoBzFKt3TnaGdW3wjwT7YpfyDl0Rg+UQN0ELyDiBMUQWsfgBnAr5b49Q5EsSCy+9n/bHOA5A7RZPH4X3RBCj28i6wqbTBtaR0zbK6Rys5/BAg8qHIwWzKRyRm+FlUgZ8DgwgCP18WSLSggNDAmRLslzLSC1+asLizjrRZVQpVXeA2rioziFWv6I35lplhPJZeOusJxRU8jqreBaDCNqDR7GjuAVTiRPxnNxFjXH10dLJzhmb7ew==;
Received: from [98.138.100.117] by nm25.bullet.mail.ne1.yahoo.com with NNFMP; 17 Dec 2014 10:25:15 -0000
Received: from [98.138.88.238] by tm108.bullet.mail.ne1.yahoo.com with NNFMP; 17 Dec 2014 10:25:14 -0000
Received: from [127.0.0.1] by omp1038.mail.ne1.yahoo.com with NNFMP; 17 Dec 2014 10:25:14 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 920067.63090.bm@omp1038.mail.ne1.yahoo.com
X-YMail-OSG: sgEvpYEVM1n.jMdo8deCXKBSrfFwB4a19DxWm84hlcRh4JFxqBhmSPZrFqgz3KG
 mlQvHzKPqd0q2fP10oLoENP3uCZv7u7HNwuAfd2g7Z2hjI0Q76SfMqhXsYciQ8ci3VEeNONoHFse
 Xucl7wEKOhmNR1R2a9WOYaACdmdDURHFawfGxwxTl5Df5bSYT9dW_NT5JQAmkfP.HsmKQLWxPoqV
 WS_q_9r5qBh_c5w5_4.ptWqsTCvr0K8OWp3zQxCuCJh4dQckgnuvinmHggaNL_dQqrPzwTfe7Op0
 O5JuNsviTKkPUrrRT8Ax_tQxcjKFHSrCNiNdKY3bRmc1dacMHq8bGU481Xubt2hp4jUwei1CfBot
 KJARcJ2aJJLJYvAH3BRJ_XlQsKaR21TzZtnI6RcXGIaDi_fbUanAv82fiwBKO8KP23VgulRsWK9C
 1ooVHdkvxn9_yabfEqgo4fGhcVfTyPEWrnHe4QZHmaafNfeKxrSVyCZDfd5JShSGDjo8GH8G0D6q
 8cMA-
Received: by 98.138.105.244; Wed, 17 Dec 2014 10:25:14 +0000 
Date: Wed, 17 Dec 2014 10:25:14 +0000 (UTC)
From: Xuelin Cao <xuelincao@yahoo.com.INVALID>
Reply-To: Xuelin Cao <xuelincao@yahoo.com>
To: Dev <dev@spark.apache.org>, User <user@spark.apache.org>
Message-ID: <1305577498.102480.1418811914148.JavaMail.yahoo@jws100144.mail.ne1.yahoo.com>
Subject: When will Spark SQL support building DB index natively?
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_102479_366163776.1418811914145"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_102479_366163776.1418811914145
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


Hi,=C2=A0
=C2=A0 =C2=A0 =C2=A0In Spark SQL help document, it says=C2=A0"Some of these=
 (such as indexes) are less important due to Spark SQL=E2=80=99s in-memory =
=C2=A0computational model. Others are slotted for future releases of Spark =
SQL.  =20
   - Block level bitmap indexes and virtual columns (used to build indexes)=
"

=C2=A0 =C2=A0 =C2=A0For our use cases, DB index is quite important. I have =
about 300G data in our database, and we always use "customer id" as a predi=
cate for DB look up. =C2=A0Without DB index, we will have to scan all 300G =
data, and it will take > 1 minute for a simple DB look up, while MySQL only=
 takes 10 seconds. We tried to create an independent table for each "custom=
er id", the result is pretty good, but the logic will be very complex.=C2=
=A0
=C2=A0 =C2=A0 =C2=A0I'm wondering when will Spark SQL supports DB index, an=
d before that, is there an alternative way to support DB index function?
Thanks

------=_Part_102479_366163776.1418811914145--

From dev-return-10834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 14:47:39 2014
Return-Path: <dev-return-10834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B77DBC311
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 14:47:39 +0000 (UTC)
Received: (qmail 4988 invoked by uid 500); 17 Dec 2014 14:47:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4901 invoked by uid 500); 17 Dec 2014 14:47:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4890 invoked by uid 99); 17 Dec 2014 14:47:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 14:47:37 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 14:47:32 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 80B3FDFC4CF
	for <dev@spark.incubator.apache.org>; Wed, 17 Dec 2014 06:45:51 -0800 (PST)
Date: Wed, 17 Dec 2014 07:45:51 -0700 (MST)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1418827551304-9820.post@n3.nabble.com>
In-Reply-To: <CABPQxsvM69K2uFU9Gb8TyCHS3xum5jVP-pxkkPb8ujf_yifXAw@mail.gmail.com>
References: <1418749794513-9804.post@n3.nabble.com> <CABPQxsvM69K2uFU9Gb8TyCHS3xum5jVP-pxkkPb8ujf_yifXAw@mail.gmail.com>
Subject: Re: RDD data flow
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Patrick Wendell wrote
> The Partition itself doesn't need to be an iterator - the iterator
> comes from the result of compute(partition). The Partition is just an
> identifier for that partition, not the data itself.

OK, that makes sense. The docs for Partition are a bit vague on this point.
Maybe I'll add this to the docs.

Thanks Patrick!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-data-flow-tp9804p9820.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 17:44:04 2014
Return-Path: <dev-return-10835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B88FCA54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 17:44:04 +0000 (UTC)
Received: (qmail 19484 invoked by uid 500); 17 Dec 2014 17:44:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19412 invoked by uid 500); 17 Dec 2014 17:44:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19401 invoked by uid 99); 17 Dec 2014 17:44:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 17:44:02 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tharsch@cray.com designates 64.18.1.181 as permitted sender)
Received: from [64.18.1.181] (HELO exprod6og101.obsmtp.com) (64.18.1.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 17:43:58 +0000
Received: from CFWEX01.americas.cray.com ([136.162.34.11]) (using TLSv1) by exprod6ob101.postini.com ([64.18.5.12]) with SMTP
	ID DSNKVJHAkOMJ6OU4zdHfyc93mC2PzJQx5qO+@postini.com; Wed, 17 Dec 2014 09:43:37 PST
Received: from CFWEX02.americas.cray.com (172.30.74.25) by
 CFWEX01.americas.cray.com (172.30.88.25) with Microsoft SMTP Server (TLS) id
 14.2.347.0; Wed, 17 Dec 2014 11:42:39 -0600
Received: from CFWEX01.americas.cray.com ([169.254.1.181]) by
 cfwex02.americas.cray.com ([169.254.2.115]) with mapi id 14.02.0387.000; Wed,
 17 Dec 2014 11:42:37 -0600
From: Tim Harsch <tharsch@cray.com>
To: Ewan Higgs <ewan.higgs@ugent.be>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: running the Terasort example
Thread-Topic: running the Terasort example
Thread-Index: AQHQFZ91EFW+gRhFhEGi9Qc+FDrHCpySUJ+AgAAengCAAWQDgIAAIauA
Date: Wed, 17 Dec 2014 17:42:37 +0000
Message-ID: <D0B6FE0B.50DF%tharsch@cray.com>
References: <D0AF7170.4C6E%tharsch@cray.com> <548FEF7B.2040302@ugent.be>
 <D0B5B901.5026%tharsch@cray.com>
 <9B1E9CD6-D61E-4690-A412-657F2FE28F47@ugent.be>
In-Reply-To: <9B1E9CD6-D61E-4690-A412-657F2FE28F47@ugent.be>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.7.141117
x-originating-ip: [172.26.68.48]
Content-Type: text/plain; charset="euc-kr"
Content-ID: <D9480672BF8EF04E836DAFAF920A73C2@cray.com>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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=
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 21:09:36 2014
Return-Path: <dev-return-10836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5715710697
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 21:09:36 +0000 (UTC)
Received: (qmail 99490 invoked by uid 500); 17 Dec 2014 21:09:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99419 invoked by uid 500); 17 Dec 2014 21:09:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98265 invoked by uid 99); 17 Dec 2014 21:09:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 21:09:32 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 21:09:27 +0000
Received: by mail-pa0-f41.google.com with SMTP id rd3so17215258pab.0
        for <multiple recipients>; Wed, 17 Dec 2014 13:09:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=CktL1jJPK9cIKdtoIXYRWd7hbPytDO+LUdc9wVQ8FLQ=;
        b=jBtSpNmZ7v7lvtZPj6ueYYDWVpU1w5cvfLXrmQylBqK9AdX3OBEwFzAzRuCZKDkrQm
         MACXdMhC0tuQ3xQp5rqp8aAvi5Nxu6uR3himdqT63eWnMho34YduNd44M2F+NRjhH4t4
         l49jQTm9ytZElo/M0IydP2iAJN1rNVZ7hfoWK34aDEWc7AOoDiaxa+bzXSGXBmWRcw4b
         BJ3GzNYvzBaNbFZ+pEnaC8OfpZeuGQABWxsVfCwwLAAZrUAL/kUssVPtEFARvNxS7EU9
         MPAewrYAgYy/vYLQ4GRTQY7feS+1xKvFXxk/rri1PYqrXRxgw5/36VH8eaGCaKfwNqW4
         S0ew==
MIME-Version: 1.0
X-Received: by 10.70.102.193 with SMTP id fq1mr74116907pdb.19.1418850547129;
 Wed, 17 Dec 2014 13:09:07 -0800 (PST)
Received: by 10.70.41.80 with HTTP; Wed, 17 Dec 2014 13:09:06 -0800 (PST)
In-Reply-To: <CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
	<CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com>
Date: Wed, 17 Dec 2014 13:09:06 -0800
Message-ID: <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
From: Josh Rosen <rosenville@gmail.com>
To: yana.kadiyska@gmail.com
Cc: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3270ad9e241050a6fe23a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3270ad9e241050a6fe23a
Content-Type: text/plain; charset=UTF-8

Yeah, it looks like messages that are successfully posted via Nabble end up
on the Apache mailing list, but messages posted directly to Apache aren't
mirrored to Nabble anymore because it's based off the incubator mailing
list.  We should fix this so that Nabble posts to / archives the
non-incubator list.

On Sat, Dec 13, 2014 at 6:27 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
wrote:
>
> Since you mentioned this, I had a related quandry recently -- it also says
> that the forum archives "*user@spark.incubator.apache.org
> <user@spark.incubator.apache.org>"/* *dev@spark.incubator.apache.org
> <dev@spark.incubator.apache.org> *respectively, yet the "Community page"
> clearly says to email the @spark.apache.org list (but the nabble archive
> is linked right there too). IMO even putting a clear explanation at the top
>
> "Posting here requires that you create an account via the UI. Your message
> will be sent to both spark.incubator.apache.org and spark.apache.org (if
> that is the case, i'm not sure which alias nabble posts get sent to)" would
> make things a lot more clear.
>
> On Sat, Dec 13, 2014 at 5:05 PM, Josh Rosen <rosenville@gmail.com> wrote:
>>
>> I've noticed that several users are attempting to post messages to
>> Spark's user / dev mailing lists using the Nabble web UI (
>> http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there
>> are many posts in Nabble that are not posted to the Apache lists and are
>> flagged with "This post has NOT been accepted by the mailing list yet."
>> errors.
>>
>> I suspect that the issue is that users are not completing the sign-up
>> confirmation process (
>> http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
>> which is preventing their emails from being accepted by the mailing list.
>>
>> I wanted to mention this issue to the Spark community to see whether
>> there are any good solutions to address this.  I have spoken to users who
>> think that our mailing list is unresponsive / inactive because their
>> un-posted messages haven't received any replies.
>>
>> - Josh
>>
>

--001a11c3270ad9e241050a6fe23a--

From dev-return-10837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 17 23:26:00 2014
Return-Path: <dev-return-10837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EF1610CDF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 17 Dec 2014 23:26:00 +0000 (UTC)
Received: (qmail 3500 invoked by uid 500); 17 Dec 2014 23:25:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3415 invoked by uid 500); 17 Dec 2014 23:25:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3403 invoked by uid 99); 17 Dec 2014 23:25:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 23:25:59 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ksankar42@gmail.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 17 Dec 2014 23:25:32 +0000
Received: by mail-pd0-f181.google.com with SMTP id v10so100183pde.12
        for <dev@spark.apache.org>; Wed, 17 Dec 2014 15:25:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=aoCE/lSGKST+OaP7xgKN68iZfSBwAIud8YHpbpQQ1+w=;
        b=I0xBqvBrMEGVJAcV3WjAR7P2DUuzzbScVZAWqCA/8G7mJ8PvS8QwGPJo+ly6jo0Q/k
         yJ++kRH2CuZvwHuTKjzrRhkfUz4gJacAoTC0tPELcK1UTFBPjEwFfAl+liI+AZdwgFQ8
         0YEw06xD1lKUyxf3FAZ+74swv5fh8JTKIvLRJ3ZQJeTr/0lyUPNI71RWNEgrHlRTfs21
         oD4zCvF7+p4B7JUHpFDP/fRDBMYn/M+VFpUkVu9UPbGKbmGdJaOotD/zlLm8qAmnoinr
         6bjnRTGBg9JYl4di0GUEokX0GiTvEaTChu3Kk6HK36BP9UC6d67Za7ylzVZv+Z0f2PTo
         Tgeg==
MIME-Version: 1.0
X-Received: by 10.69.26.98 with SMTP id ix2mr73563992pbd.161.1418858730418;
 Wed, 17 Dec 2014 15:25:30 -0800 (PST)
Received: by 10.70.37.230 with HTTP; Wed, 17 Dec 2014 15:25:30 -0800 (PST)
In-Reply-To: <CAOTBr2npHoPdyJS_VWWQ5S_nHyezEqpOyFBfEhBvSNE=x6STEQ@mail.gmail.com>
References: <CABPQxstT84GtV-3pONpjtDtJhmQbE+KEVjqdFGeOL-bZcyvkRg@mail.gmail.com>
	<A7E5DDE3-FC90-484C-8A50-0AB1A5455124@gmail.com>
	<CAOTBr2npHoPdyJS_VWWQ5S_nHyezEqpOyFBfEhBvSNE=x6STEQ@mail.gmail.com>
Date: Wed, 17 Dec 2014 15:25:30 -0800
Message-ID: <CAOTBr2=VnHDNE8WdewVEyME72Us-KOqYJP4BhcifmB9GKB2vfA@mail.gmail.com>
Subject: Fwd: [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Krishna Sankar <ksankar42@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134916a9cf9e8050a71cad2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134916a9cf9e8050a71cad2
Content-Type: text/plain; charset=UTF-8

Forgot Reply To All ;o(
---------- Forwarded message ----------
From: Krishna Sankar <ksankar42@gmail.com>
Date: Wed, Dec 10, 2014 at 9:16 PM
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
To: Matei Zaharia <matei.zaharia@gmail.com>

+1
Works same as RC1
1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package 13:07 min
2. Tested pyspark, mlib - running as well as compare results with 1.1.x
2.1. statistics OK
2.2. Linear/Ridge/Laso Regression OK
       Slight difference in the print method (vs. 1.1.x) of the model
object - with a label & more details. This is good.
2.3. Decision Tree, Naive Bayes OK
       Changes in print(model) - now print (model.ToDebugString()) - OK
       Some changes in NaiveBayes. Different from my 1.1.x code - had to
flatten list structures, zip required same number in partitions
       After code changes ran fine.
2.4. KMeans OK
       Center And Scale OK
       zip occasionally fails with error "localhost):
org.apache.spark.SparkException: Can only zip RDDs with same number of
elements in each partition"
Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
Made it work by doing a different transformation ie reusing an original
rdd.
(Xiangrui, I will end you the iPython Notebook & the dataset by a separate
e-mail)
2.5. rdd operations OK
       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. recommendation OK
2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
dataset which never worked. Works fine in 1.2.0 !
3. Scala Mlib - subset of examples as in #2 above, with Scala
3.1. statistics OK
3.2. Linear Regression OK
3.3. Decision Tree OK
3.4. KMeans OK
Cheers
<k/>

On Wed, Dec 10, 2014 at 3:05 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> +1
>
> Tested on Mac OS X.
>
> Matei
>
> > On Dec 10, 2014, at 1:08 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> >
> > Please vote on releasing the following candidate as Apache Spark version
> 1.2.0!
> >
> > The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1055/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.2.0!
> >
> > The vote is open until Saturday, December 13, at 21:00 UTC and passes
> > if a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.2.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > == What justifies a -1 vote for this release? ==
> > This vote is happening relatively late into the QA period, so
> > -1 votes should only occur for significant regressions from
> > 1.0.2. Bugs already present in 1.1.X, minor
> > regressions, or bugs related to new features will not block this
> > release.
> >
> > == What default changes should I be aware of? ==
> > 1. The default value of "spark.shuffle.blockTransferService" has been
> > changed to "netty"
> > --> Old behavior can be restored by switching to "nio"
> >
> > 2. The default value of "spark.shuffle.manager" has been changed to
> "sort".
> > --> Old behavior can be restored by setting "spark.shuffle.manager" to
> "hash".
> >
> > == How does this differ from RC1 ==
> > This has fixes for a handful of issues identified - some of the
> > notable fixes are:
> >
> > [Core]
> > SPARK-4498: Standalone Master can fail to recognize completed/failed
> > applications
> >
> > [SQL]
> > SPARK-4552: Query for empty parquet table in spark sql hive get
> > IllegalArgumentException
> > SPARK-4753: Parquet2 does not prune based on OR filters on partition
> columns
> > SPARK-4761: With JDBC server, set Kryo as default serializer and
> > disable reference tracking
> > SPARK-4785: When called with arguments referring column fields, PMOD
> throws NPE
> >
> > - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1134916a9cf9e8050a71cad2--

From dev-return-10838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 07:39:55 2014
Return-Path: <dev-return-10838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E4D29187
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 07:39:55 +0000 (UTC)
Received: (qmail 55780 invoked by uid 500); 18 Dec 2014 07:39:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55679 invoked by uid 500); 18 Dec 2014 07:39:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54395 invoked by uid 99); 18 Dec 2014 07:39:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 07:39:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 07:39:22 +0000
Received: by mail-oi0-f49.google.com with SMTP id i138so32147oig.8;
        Wed, 17 Dec 2014 23:38:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=dB0ZNX5W0Oz7/h+P31jLY8Szmj/1PPWgPm1yWSg8WGI=;
        b=vBlxuFfnLeTsou3QRbrgiG8WHK0WPyH4Unm4m5Af21Pjno1g5PM0CG/stbKD9PA9W6
         4K+xsDTnMtUaMDiiSLIdpHYxvaKbbzEUSgsCIZDxkFhyEd4dEaVLS/OIRT4ABuhSM7lm
         4ZAHkyNogS25ON7z6ehpxnB/LdN6BR0FwS33Cb7IOrinaXBz7uKNZmnul1WjTn2rQAdw
         HWKN3BpfI0Z1HhMEOT9yrmfFtpGXRTEke+MAREnb+dCltYGaLQ3srp/YLEjXbbkfct3x
         FfU0+Nx2ySv7zSzHGXK8p/9FPHZwnaDNT6azdxi1Kc3X2DpdupVf+C7Ho2eabSqXBTph
         Vfbw==
X-Received: by 10.202.225.197 with SMTP id y188mr405685oig.94.1418888316546;
 Wed, 17 Dec 2014 23:38:36 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.71.67 with HTTP; Wed, 17 Dec 2014 23:38:16 -0800 (PST)
In-Reply-To: <CAJc_syLw5bV2Zyds_GvB6Qw1K1+5FZREH1kTtQH0jWZoHJ3LsQ@mail.gmail.com>
References: <CAJc_syKW=oga24CE0C7X202RH=B9URW8UYUvPr3u-2cB0cpfNA@mail.gmail.com>
 <CABjYQ39pS5H3NfRmX-PCir-7ueAAPrhSt0nO3PMspu3R=xXUww@mail.gmail.com>
 <CAJc_syJSU3tmA8zZHk+SSOSPrDqbMpnmnw5gLyB_WTTj3jQKmQ@mail.gmail.com>
 <CABjYQ3_faewqDyEdqyigpa9RLFTvFxWZhjVASXZJ2P9w+OwwXg@mail.gmail.com> <CAJc_syLw5bV2Zyds_GvB6Qw1K1+5FZREH1kTtQH0jWZoHJ3LsQ@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Wed, 17 Dec 2014 23:38:16 -0800
Message-ID: <CAJc_syLXR4J6mG+6umNtSUMNduXPz9fud0ix1G7NQLLsT0M4ZA@mail.gmail.com>
Subject: Re: Spark Shell slowness on Google Cloud
To: Denny Lee <denny.g.lee@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Cc: user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d4cb21576bd050a78ae90
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d4cb21576bd050a78ae90
Content-Type: text/plain; charset=UTF-8

Here's another data point: the slow part of my code is the construction of
an RDD as the union of the textFile RDDs representing data from several
distinct google storage directories. So the question becomes the following:
what computation happens when calling the union method on two RDDs?

On Wed, Dec 17, 2014 at 11:24 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:
>
> Well, what do you suggest I run to test this? But more importantly, what
> information would this give me?
>
> On Wed, Dec 17, 2014 at 10:46 PM, Denny Lee <denny.g.lee@gmail.com> wrote:
>>
>> Oh, it makes sense of gsutil scans through this quickly, but I was
>> wondering if running a Hadoop job / bdutil would result in just as fast
>> scans?
>>
>>
>> On Wed Dec 17 2014 at 10:44:45 PM Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>>> Denny,
>>>
>>> No, gsutil scans through the listing of the bucket quickly. See the
>>> following.
>>>
>>> alex@hadoop-m:~/split$ time bash -c "gsutil ls
>>> gs://my-bucket/20141205/csv/*/*/* | wc -l"
>>>
>>> 6860
>>>
>>> real    0m6.971s
>>> user    0m1.052s
>>> sys     0m0.096s
>>>
>>> Alex
>>>
>>>
>>> On Wed, Dec 17, 2014 at 10:29 PM, Denny Lee <denny.g.lee@gmail.com>
>>> wrote:
>>>>
>>>> I'm curious if you're seeing the same thing when using bdutil against
>>>> GCS?  I'm wondering if this may be an issue concerning the transfer rate of
>>>> Spark -> Hadoop -> GCS Connector -> GCS.
>>>>
>>>>
>>>> On Wed Dec 17 2014 at 10:09:17 PM Alessandro Baretta <
>>>> alexbaretta@gmail.com> wrote:
>>>>
>>>>> All,
>>>>>
>>>>> I'm using the Spark shell to interact with a small test deployment of
>>>>> Spark, built from the current master branch. I'm processing a dataset
>>>>> comprising a few thousand objects on Google Cloud Storage, split into a
>>>>> half dozen directories. My code constructs an object--let me call it the
>>>>> Dataset object--that defines a distinct RDD for each directory. The
>>>>> constructor of the object only defines the RDDs; it does not actually
>>>>> evaluate them, so I would expect it to return very quickly. Indeed, the
>>>>> logging code in the constructor prints a line signaling the completion of
>>>>> the code almost immediately after invocation, but the Spark shell does not
>>>>> show the prompt right away. Instead, it spends a few minutes seemingly
>>>>> frozen, eventually producing the following output:
>>>>>
>>>>> 14/12/18 05:52:49 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 9
>>>>>
>>>>> 14/12/18 05:54:15 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 759
>>>>>
>>>>> 14/12/18 05:54:40 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 228
>>>>>
>>>>> 14/12/18 06:00:11 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 3076
>>>>>
>>>>> 14/12/18 06:02:02 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 1013
>>>>>
>>>>> 14/12/18 06:02:21 INFO mapred.FileInputFormat: Total input paths to
>>>>> process : 156
>>>>>
>>>>> This stage is inexplicably slow. What could be happening?
>>>>>
>>>>> Thanks.
>>>>>
>>>>>
>>>>> Alex
>>>>>
>>>>

--001a113d4cb21576bd050a78ae90--

From dev-return-10839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 09:04:55 2014
Return-Path: <dev-return-10839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4764695A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 09:04:55 +0000 (UTC)
Received: (qmail 13867 invoked by uid 500); 18 Dec 2014 09:04:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13791 invoked by uid 500); 18 Dec 2014 09:04:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13769 invoked by uid 99); 18 Dec 2014 09:04:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 09:04:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 09:04:47 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so109527oia.15
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 01:04:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=bx5eD7Z0Zoh3f794m/2s1X55pIJmMfF+4WhZjh3ZDfM=;
        b=PmfwmXGf1dRbfUeJfPnUv6TFU1U1xljeCAptW0zR/BLeJ6ZO968onf+k/kdc2Vt1nF
         i8MFIXbTmrkpAspFH2okxpLsbulnGtrXN8HOF9dirVlBe5/zZd0CcKYOxRQZXk+CJPgJ
         tfTyJ7Zy7/8QKBV/myFAvjhlVBBye+XgrFlYs/wNWrMbVRtmn25TFAgtoUOFcoeraiN9
         Jt4Njb0OHF1kVsh7YjqOJRyPyeuKMLgj+WwuyFZHWIrgX1Osb/F7RS2Qb1ZsyoKjXee4
         3GRhrLOJJGho2tByoFNVJcOjOY3U2LI+ifilGbmUTBHNTealYBvdqqIpOLQ1H4SNDBXP
         tQcA==
X-Received: by 10.202.111.2 with SMTP id m2mr570875oic.106.1418893467158; Thu,
 18 Dec 2014 01:04:27 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.71.67 with HTTP; Thu, 18 Dec 2014 01:04:06 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 18 Dec 2014 01:04:06 -0800
Message-ID: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
Subject: What RDD transformations trigger computations?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114284bc159183050a79e1aa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114284bc159183050a79e1aa
Content-Type: text/plain; charset=UTF-8

All,

I noticed that while some operations that return RDDs are very cheap, such
as map and flatMap, some are quite expensive, such as union and groupByKey.
I'm referring here to the cost of constructing the RDD scala value, not the
cost of collecting the values contained in the RDD. This does not match my
understanding that RDD transformations only set up a computation without
actually running it. Oh, Spark developers, can you please provide some
clarity?

Alex

--001a114284bc159183050a79e1aa--

From dev-return-10840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 09:57:26 2014
Return-Path: <dev-return-10840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7973D9715
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 09:57:26 +0000 (UTC)
Received: (qmail 61123 invoked by uid 500); 18 Dec 2014 09:57:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61053 invoked by uid 500); 18 Dec 2014 09:57:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61039 invoked by uid 99); 18 Dec 2014 09:57:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 09:57:22 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FORGED_YAHOO_RCVD,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of smyee@yahoo.com designates 98.138.120.214 as permitted sender)
Received: from [98.138.120.214] (HELO nm42-vm8.bullet.mail.ne1.yahoo.com) (98.138.120.214)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 09:56:56 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=nlsbllQp/asMUE6q46X62PQkjezhS4/WHq7nRE85BU6HyDjIbMA02/1kAhkeTM9TH2w659THA5m8v7aNOC/kT71bSioVUugvPh9ZoZls7PL//voFAwlI2jWl1CNvHcl3mZo2SlsdW96ke772tiXWQ772oUFcSIRY3Qp+HPe5+4W8CLipkyFHpXKI2WF62LJcKziCnAcu/X3iS84BwBbD0Redj336ZXx1A7M/9TQrGqRj0VNNtM7qNrE8XyaCd2gQepGkE0kuBFy6KjEKa8eraumAbixjFYQDFhcI/pv0jA6zEJZ+tcSHN8VMgPlQ9cy1UkhGHJxpl7gzCZEikv9sdw==;
Received: from [127.0.0.1] by nm42.bullet.mail.ne1.yahoo.com with NNFMP; 18 Dec 2014 09:56:54 -0000
Received: from [98.138.226.177] by nm42.bullet.mail.ne1.yahoo.com with NNFMP; 18 Dec 2014 09:54:14 -0000
Received: from [98.138.104.113] by tm12.bullet.mail.ne1.yahoo.com with NNFMP; 18 Dec 2014 09:54:14 -0000
Received: from [127.0.0.1] by smtp222.mail.ne1.yahoo.com with NNFMP; 18 Dec 2014 09:54:14 -0000
X-Yahoo-Newman-Id: 210087.25152.bm@smtp222.mail.ne1.yahoo.com
X-Yahoo-Newman-Property: ymail-4
X-YMail-OSG: Sykj9EcVM1kbWsUMk07jLXbOr7lgHgRJKg_QPtmkIdRi5ps
 RRY0426Vb17zMNiFEgABYz0PWgHk6pcwIkPrtnxut9QV0FG4IW8jxDqH35zd
 iXYK5.F58WiQu91iA7nUnVmprDX9VxDVgAdd.Ta6.7CitFFVcLmtZWGIEQYF
 .WO6406UBXLKBGFy1rIMAIIAdj.7U5G08.HqpgA6zNiKeC8SOhhvpalCreQx
 oPjJyyU.0MfwMbNhP0dLBxgDg.Elrm8ImM_ztgM8wJoiHoUAbXWIJid8Yu3m
 xDuvteFNZSWTHUE6hXpxs_IL1oJiJ6JrDxJDngmlP4sqXGbMPE302zITVsiM
 TmgfswpUNu7VNnJNAoI6Ipk6x9u_SXOG0h_.OzlUinBPYacxjzKHilKAD8Xo
 VTnwwMMTJm9adLSLsgXOLCy9Z4QLTp5DaK.agqpd8gs9HWKYDw69liMMMQDU
 mIfKTHHfpDpqr5zp3Bp2KQrv6aPv.z0cQVj7D0wlvKjrmDYZejVxv3Dtp4H6
 ._0cPuJbmgWIbIWRCKx1Nh0Y8.1qi2fTW43.slg--
X-Yahoo-SMTP: 6tBLrCuswBCJQ.cTAiCW3hYB
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: one hot encoding
From: "smyee@yahoo.com.INVALID" <smyee@yahoo.com.INVALID>
In-Reply-To: <CACBYxK+N-rQHW-UMX8jB_T6wZ-pc2z=CH5OQQ3yXm-z3dD_VHQ@mail.gmail.com>
Date: Thu, 18 Dec 2014 17:54:10 +0800
Cc: Lochana Menikarachchi <lochanac@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: 7bit
Message-Id: <1641CCCA-3FA9-4E97-B4DA-1AE26F8A56AC@yahoo.com>
References: <548BA161.6090207@gmail.com> <CACBYxK+N-rQHW-UMX8jB_T6wZ-pc2z=CH5OQQ3yXm-z3dD_VHQ@mail.gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

Sandy, will it be available to pyspark use too?

> On Dec 13, 2014, at 6:18 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> 
> Hi Lochana,
> 
> We haven't yet added this in 1.2.
> https://issues.apache.org/jira/browse/SPARK-4081 tracks adding categorical
> feature indexing, which one-hot encoding can be built on.
> https://issues.apache.org/jira/browse/SPARK-1216 also tracks a version of
> this prior to the ML pipelines work.
> 
> -Sandy
> 
> On Fri, Dec 12, 2014 at 6:16 PM, Lochana Menikarachchi <lochanac@gmail.com>
> wrote:
>> 
>> Do we have one-hot encoding in spark MLLib 1.1.1 or 1.2.0 ? It wasn't
>> available in 1.1.0.
>> Thanks.
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 15:07:55 2014
Return-Path: <dev-return-10841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C404710214
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 15:07:55 +0000 (UTC)
Received: (qmail 98585 invoked by uid 500); 18 Dec 2014 15:07:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98518 invoked by uid 500); 18 Dec 2014 15:07:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98507 invoked by uid 99); 18 Dec 2014 15:07:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 15:07:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 15:07:27 +0000
Received: by mail-oi0-f51.google.com with SMTP id e131so492064oig.38
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 07:07:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=RM3GtuYuFpDh7PkKMQPcg0fFrWwf28O9cTDXqsWeykI=;
        b=Pv/iXn6btt21IxxKwiie/zWEO4HxhIfRRPXRhvX0oicPj+gShyCNkdTok5Zv2f0zXB
         SLWwZdSxYZplSgdweka51N1t+caO+JHw2QLYhU+MNlLQufFLsAaTy+d+/zPjRVom4TGo
         i2EBSyo7ucgXzwkJ9cEtEx8DSi0avLxaHasFrwJ47ag+61DXiR8dZs8VgWRXrNcjQBEi
         vA/E7FkwBh3Z5Xk1QVWeIq9Z0NUd4j2lXZ6Sbg8XIGsUTbaBb9Tq9RogDJE6T7Qu1EX0
         i3zuaRuGTctI3hSOMLx0VIsFHMmAdoKNfJ/VrB8FDpkntNun7M1YF8OeCJIoEDOgpnKY
         rnKw==
X-Gm-Message-State: ALoCoQmohxVQWTFfQkZWNPBDStJPQn1nPD82Zfy52mJs8MGU/yiDbTIqD0CLuXGo3IhQMWODqP2L
MIME-Version: 1.0
X-Received: by 10.202.172.5 with SMTP id v5mr1522463oie.48.1418915225416; Thu,
 18 Dec 2014 07:07:05 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Thu, 18 Dec 2014 07:07:05 -0800 (PST)
Date: Thu, 18 Dec 2014 09:07:05 -0600
Message-ID: <CAKWX9VVqn3eE7QmaUj1-SobF9yWGcqaAp7OCyTWG7Sgi8CDgbw@mail.gmail.com>
Subject: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113c3800fa4e07050a7ef19a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113c3800fa4e07050a7ef19a
Content-Type: text/plain; charset=UTF-8

Now that 1.2 is finalized...  who are the go-to people to get some
long-standing Kafka related issues resolved?

The existing api is not sufficiently safe nor flexible for our production
use.  I don't think we're alone in this viewpoint, because I've seen
several different patches and libraries to fix the same things we've been
running into.

Regarding flexibility

https://issues.apache.org/jira/browse/SPARK-3146

has been outstanding since August, and IMHO an equivalent of this is
absolutely necessary.  We wrote a similar patch ourselves, then found that
PR and have been running it in production.  We wouldn't be able to get our
jobs done without it.  It also allows users to solve a whole class of
problems for themselves (e.g. SPARK-2388, arbitrary delay of messages, etc).

Regarding safety, I understand the motivation behind WriteAheadLog as a
general solution for streaming unreliable sources, but Kafka already is a
reliable source.  I think there's a need for an api that treats it as
such.  Even aside from the performance issues of duplicating the
write-ahead log in kafka into another write-ahead log in hdfs, I need
exactly-once semantics in the face of failure (I've had failures that
prevented reloading a spark streaming checkpoint, for instance).

I've got an implementation i've been using

https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
/src/main/scala/org/apache/spark/rdd/kafka

Tresata has something similar at https://github.com/tresata/spark-kafka,
and I know there were earlier attempts based on Storm code.

Trying to distribute these kinds of fixes as libraries rather than patches
to Spark is problematic, because large portions of the implementation are
private[spark].

 I'd like to help, but i need to know whose attention to get.

--001a113c3800fa4e07050a7ef19a--

From dev-return-10842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 19:06:56 2014
Return-Path: <dev-return-10842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 32BD310D4D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 19:06:56 +0000 (UTC)
Received: (qmail 89288 invoked by uid 500); 18 Dec 2014 19:06:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89227 invoked by uid 500); 18 Dec 2014 19:06:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89215 invoked by uid 99); 18 Dec 2014 19:06:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:06:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of francois.garillot@typesafe.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:06:50 +0000
Received: by mail-qc0-f172.google.com with SMTP id m20so1399143qcx.3
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 11:04:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=typesafe.com; s=google;
        h=date:mime-version:message-id:from:to:subject:content-type;
        bh=NwkuuWbNLuAOQn7TCIQhwNU1EmecZGOdNbkFZOygsvI=;
        b=DgzEiBa/Uo1KPK7Zc1gUW9/NHbJL+d6iDbr3wsFqiFgHhv3C7VuVVmciRdo/CmD1Sq
         oUYSrvabaQ0saQPZOV9RkzB4eEzG3M6AQd2g0kx+UTESLwfvDE0jGxivjeoTIe6eqG8F
         N6miTyOD8vQQhsrb+dk3v0qNrNQqYaKXVChwI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:mime-version:message-id:from:to:subject
         :content-type;
        bh=NwkuuWbNLuAOQn7TCIQhwNU1EmecZGOdNbkFZOygsvI=;
        b=bbtD1deasfLHEZa7sGDmHCZV0aQ79kHfexoVMEMQcLEQwXu11dPnG9u4vVjHFMIn0D
         S8NSjqSZ5FkqBXi6WZ/OV0kzEG5B3my3FB0uaUjuuiKT5mE/m8j8/qMKN/7Ml2CrToiS
         2JC8syZ4yJZ6ggeqiZWbQloBXlg80af5XxI9BP7el2d7fLMN73NGs/m05NJ/vVGCBOE5
         dCLSLCfpnsF+niCub1pWtz+OAC9XipMdGiykVTm+N1pBlU9ZA90jT+U0IWaszclb6aF8
         8jXy5B+mkw377pkvbd2CSv3AmettoGPiXCGhzo/jhuI7boAcfeAW0pTN6se/lWJRByCS
         SZaw==
X-Gm-Message-State: ALoCoQnZKPSo2jEFQsN6FMpyXL4ZxNW1DqeqV0qyTlzAdZu9oZ//euDj8y72OyNC8YeOni0FwuCa
X-Received: by 10.224.21.133 with SMTP id j5mr6535869qab.83.1418929499312;
        Thu, 18 Dec 2014 11:04:59 -0800 (PST)
Received: from hedwig-41.prd.orcali.com (ec2-54-85-253-236.compute-1.amazonaws.com. [54.85.253.236])
        by mx.google.com with ESMTPSA id m91sm2244573qge.46.2014.12.18.11.04.58
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 11:04:58 -0800 (PST)
Date: Thu, 18 Dec 2014 11:04:58 -0800 (PST)
X-Google-Original-Date: Thu, 18 Dec 2014 19:04:57 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1418929497786.4ba3e417@Nodemailer>
X-Orchestra-Oid: 91C5C95D-D6D9-43F0-8D6F-5FC6DD75F4BF
X-Orchestra-Sig: 3315c76333dd62707fe1f2b88519c81712d45bd5
X-Orchestra-Thrid: AB4B34E4-5916-467C-A294-F1729EEE4E50
X-Orchestra-Thrid-Sig: 863fe19311163c61e6c91365c5e1b40302af5294
X-Orchestra-Account: c790f2f8247b1e9c15a01483baa60a58f4ef35d8
From: francois.garillot@typesafe.com
To: dev@spark.apache.org
Subject: Spark Streaming Data flow graph
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1418929498468"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1418929498468
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

I=E2=80=99ve been trying to produce an updated box diagram to refresh :
http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathaga=
tadassparkmeetup20130617/26


=E2=80=A6 after the SPARK-3129, and other switches (a surprising number of =
comments still mention NetworkReceiver).


Here=E2=80=99s what I have so far:
https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf=3Fdl=3D0


This is not supposed to respect any particular convention (ER, ORM, =
=E2=80=A6). Data flow up to right before RDD creation is in bold arrows, =
metadata flow is in normal width arrows.


This diagram is still very much a WIP (see below : todo), but I wanted to =
share it to ask:
- what=E2=80=99s wrong =3F
- what are the glaring omissions =3F
- how can I make this better (i.e. what should I add first to the Todo-list=
 below) =3F


I=E2=80=99ll be happy to share this (including sources) with whoever asks =
for it.=C2=A0


Todo :
- mark private/public classes
- mark queues in Receiver, ReceivedBlockHandler, BlockManager
- mark type of info on transport : e.g. Actor message, =
ReceivedBlockInfo=C2=A0



=E2=80=94
Fran=C3=A7ois Garillot
------Nodemailer-0.5.0-?=_1-1418929498468--

From dev-return-10843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 19:14:16 2014
Return-Path: <dev-return-10843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF56B10DB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 19:14:16 +0000 (UTC)
Received: (qmail 10769 invoked by uid 500); 18 Dec 2014 19:14:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10694 invoked by uid 500); 18 Dec 2014 19:14:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10295 invoked by uid 99); 18 Dec 2014 19:14:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:14:13 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:13:48 +0000
Received: by mail-oi0-f49.google.com with SMTP id i138so811830oig.8
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 11:13:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=T/dIUJ5Jvlafoj08GD0W2tHYZlH0teDeNFgvQhXf4XE=;
        b=xw7RfGgr+pAvWzlhGBMQNEyG/LJQwPyyebq5aQZF0R7JmbELdGJMWmHM6yoEzVhx2m
         ee4Mc7XSBbvw2vnpEQSDgPbxZyRhTL6oVF62OpX3AF1sDBCVqHkT9rnysy5hWdmpGOAC
         rGPHU6pXrp9tpgafmNAYw9J2K5PTroJznG7sf/kVn7tQkvYG5vjf5WQbrnLIKKEDqjg1
         /EUynKGrrF4/WQ6vSOh/CyTLyKwd10O61AF/VCB9l2LcrEQiAwgcVsXRKXRgZqvWiS8W
         JZWtEPU+mkUr44iFWJCZqmR3I7oqO04c8JZQRQ4DQ6dbDuMeHjFQKSQDMx05BMoHhoY+
         gmjw==
MIME-Version: 1.0
X-Received: by 10.202.219.198 with SMTP id s189mr2235357oig.72.1418929982098;
 Thu, 18 Dec 2014 11:13:02 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Thu, 18 Dec 2014 11:13:02 -0800 (PST)
In-Reply-To: <CAKWX9VVqn3eE7QmaUj1-SobF9yWGcqaAp7OCyTWG7Sgi8CDgbw@mail.gmail.com>
References: <CAKWX9VVqn3eE7QmaUj1-SobF9yWGcqaAp7OCyTWG7Sgi8CDgbw@mail.gmail.com>
Date: Thu, 18 Dec 2014 11:13:02 -0800
Message-ID: <CABPQxsshp4iXBqgZDTt-EnzimEfVvxoxAE+BMLCJTXXA8hMjFA@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Patrick Wendell <pwendell@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Cody,

Thanks for reaching out with this. The lead on streaming is TD - he is
traveling this week though so I can respond a bit. To the high level
point of whether Kafka is important - it definitely is. Something like
80% of Spark Streaming deployments (anecdotally) ingest data from
Kafka. Also, good support for Kafka is something we generally want in
Spark and not a library. In some cases IIRC there were user libraries
that used unstable Kafka API's and we were somewhat waiting on Kafka
to stabilize them to merge things upstream. Otherwise users wouldn't
be able to use newer Kakfa versions. This is a high level impression
only though, I haven't talked to TD about this recently so it's worth
revisiting given the developments in Kafka.

Please do bring things up like this on the dev list if there are
blockers for your usage - thanks for pinging it.

- Patrick

On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org> wrote:
> Now that 1.2 is finalized...  who are the go-to people to get some
> long-standing Kafka related issues resolved?
>
> The existing api is not sufficiently safe nor flexible for our production
> use.  I don't think we're alone in this viewpoint, because I've seen
> several different patches and libraries to fix the same things we've been
> running into.
>
> Regarding flexibility
>
> https://issues.apache.org/jira/browse/SPARK-3146
>
> has been outstanding since August, and IMHO an equivalent of this is
> absolutely necessary.  We wrote a similar patch ourselves, then found that
> PR and have been running it in production.  We wouldn't be able to get our
> jobs done without it.  It also allows users to solve a whole class of
> problems for themselves (e.g. SPARK-2388, arbitrary delay of messages, etc).
>
> Regarding safety, I understand the motivation behind WriteAheadLog as a
> general solution for streaming unreliable sources, but Kafka already is a
> reliable source.  I think there's a need for an api that treats it as
> such.  Even aside from the performance issues of duplicating the
> write-ahead log in kafka into another write-ahead log in hdfs, I need
> exactly-once semantics in the face of failure (I've had failures that
> prevented reloading a spark streaming checkpoint, for instance).
>
> I've got an implementation i've been using
>
> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
> /src/main/scala/org/apache/spark/rdd/kafka
>
> Tresata has something similar at https://github.com/tresata/spark-kafka,
> and I know there were earlier attempts based on Storm code.
>
> Trying to distribute these kinds of fixes as libraries rather than patches
> to Spark is problematic, because large portions of the implementation are
> private[spark].
>
>  I'd like to help, but i need to know whose attention to get.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 19:46:17 2014
Return-Path: <dev-return-10844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DD801000E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 19:46:17 +0000 (UTC)
Received: (qmail 1768 invoked by uid 500); 18 Dec 2014 19:46:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1693 invoked by uid 500); 18 Dec 2014 19:46:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1677 invoked by uid 99); 18 Dec 2014 19:46:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:46:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:46:11 +0000
Received: by mail-qa0-f52.google.com with SMTP id x12so1311764qac.11
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 11:44:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:mime-version:message-id:in-reply-to
         :references:from:to:cc:subject:content-type;
        bh=qOHcHQDtNtxD+QfJMpiBDpDCOzrIYnWzzYAYBsS/ILI=;
        b=k6rOE7W02eWxDqI3vCbBAnDu5zvKLD71pxbzPnbBU/Swnwp+P4WxAYF1f3dAnxp/Rj
         jsEVkiMjcyJVsVYsuZQLpNRRuYnCBbXopICAlZMaa6XujmYIkHfqQKL1IoCzWOQj8X81
         +HJcoIp9nidhaPKnbc9U8zpXj3l7ZkdtfGo3BY+QsuzM1tOwmB9JxxSg/Gtdb4/eWpBV
         nA+rp0D9oQaUp+gzdY+SaSeda9yhlfY4RUmw3BMOlI3IMcevfm1806EvaSNnyHQKkOTy
         izACrd/zJcvd+GY0EPKVeAU/POuqUQtRdQ4zIqZYyg/Krej+o+u65+QWIO0GZUaoM6+U
         HQqw==
X-Gm-Message-State: ALoCoQkUG1EpkpoMTmonuVPyzlsMt/vLXtLE4kpkenBO5wZnWI8qpqfkyVTo+Kx0UXWU0imr4RUG
X-Received: by 10.140.105.55 with SMTP id b52mr6574792qgf.1.1418931859989;
        Thu, 18 Dec 2014 11:44:19 -0800 (PST)
Received: from hedwig-6.prd.orcali.com (ec2-54-85-253-252.compute-1.amazonaws.com. [54.85.253.252])
        by mx.google.com with ESMTPSA id b109sm7611282qgf.40.2014.12.18.11.44.18
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 11:44:19 -0800 (PST)
Date: Thu, 18 Dec 2014 11:44:19 -0800 (PST)
X-Google-Original-Date: Thu, 18 Dec 2014 19:44:18 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1418931858785.151869ee@Nodemailer>
In-Reply-To: <CABPQxsshp4iXBqgZDTt-EnzimEfVvxoxAE+BMLCJTXXA8hMjFA@mail.gmail.com>
References: <CABPQxsshp4iXBqgZDTt-EnzimEfVvxoxAE+BMLCJTXXA8hMjFA@mail.gmail.com>
X-Orchestra-Oid: BF479C06-6A3D-406A-8FA0-8846F9D57B9D
X-Orchestra-Sig: 3e542d293da6ed2a293d485f676af1e57f548ba0
X-Orchestra-Thrid: TEB56D109-7104-46FE-9747-57A7D6EE5D69_1487840502276290343
X-Orchestra-Thrid-Sig: 257f4f6067e02a56f8c5e6e4a3be5aafec0bf555
X-Orchestra-Account: cb3c0d8c865d266cbc6d7e3bcacc800a1280cd0d
From: "Hari Shreedharan" <hshreedharan@cloudera.com>
To: "Patrick Wendell" <pwendell@gmail.com>
Cc: "Cody Koeninger" <cody@koeninger.org>, dev@spark.apache.org
Subject: Re: Which committers care about Kafka?
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1418931859167"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1418931859167
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi Cody,




I am an absolute +1 on SPARK-3146. I think we can implement something =
pretty simple and lightweight for that one.




For the Kafka DStream skipping the WAL implementation - this is something I=
 discussed with TD a few weeks ago. Though it is a good idea to implement =
this to avoid unnecessary HDFS writes, it is an optimization. For that =
reason, we must be careful in implementation. There are a couple of issues =
that we need to ensure works properly - specifically ordering. To ensure we=
 pull messages from different topics and partitions in the same order after=
 failure, we=E2=80=99d still have to persist the metadata to HDFS (or some =
other system) - this metadata must contain the order of messages consumed, =
so we know how to re-read the messages. I am planning to explore this once =
I have some time (probably in Jan). In addition, we must also ensure =
bucketing functions work fine as well. I will file a placeholder jira for =
this one.=C2=A0




I also wrote an API to write data back to Kafka a while back - =
https://github.com/apache/spark/pull/2994 . I am hoping that this will get =
pulled in soon, as this is something I know people want. I am open to =
feedback on that - anything that I can do to make it better.




Thanks,=C2=A0Hari

On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey Cody,
> Thanks for reaching out with this. The lead on streaming is TD - he is
> traveling this week though so I can respond a bit. To the high level
> point of whether Kafka is important - it definitely is. Something like
> 80% of Spark Streaming deployments (anecdotally) ingest data from
> Kafka. Also, good support for Kafka is something we generally want in
> Spark and not a library. In some cases IIRC there were user libraries
> that used unstable Kafka API's and we were somewhat waiting on Kafka
> to stabilize them to merge things upstream. Otherwise users wouldn't
> be able to use newer Kakfa versions. This is a high level impression
> only though, I haven't talked to TD about this recently so it's worth
> revisiting given the developments in Kafka.
> Please do bring things up like this on the dev list if there are
> blockers for your usage - thanks for pinging it.
> - Patrick
> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org> =
wrote:
>> Now that 1.2 is finalized...  who are the go-to people to get some
>> long-standing Kafka related issues resolved=3F
>>
>> The existing api is not sufficiently safe nor flexible for our =
production
>> use.  I don't think we're alone in this viewpoint, because I've seen
>> several different patches and libraries to fix the same things we've =
been
>> running into.
>>
>> Regarding flexibility
>>
>> https://issues.apache.org/jira/browse/SPARK-3146
>>
>> has been outstanding since August, and IMHO an equivalent of this is
>> absolutely necessary.  We wrote a similar patch ourselves, then found =
that
>> PR and have been running it in production.  We wouldn't be able to get =
our
>> jobs done without it.  It also allows users to solve a whole class of
>> problems for themselves (e.g. SPARK-2388, arbitrary delay of messages, =
etc).
>>
>> Regarding safety, I understand the motivation behind WriteAheadLog as a
>> general solution for streaming unreliable sources, but Kafka already is =
a
>> reliable source.  I think there's a need for an api that treats it as
>> such.  Even aside from the performance issues of duplicating the
>> write-ahead log in kafka into another write-ahead log in hdfs, I need
>> exactly-once semantics in the face of failure (I've had failures that
>> prevented reloading a spark streaming checkpoint, for instance).
>>
>> I've got an implementation i've been using
>>
>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
>> /src/main/scala/org/apache/spark/rdd/kafka
>>
>> Tresata has something similar at https://github.com/tresata/spark-kafka,=

>> and I know there were earlier attempts based on Storm code.
>>
>> Trying to distribute these kinds of fixes as libraries rather than =
patches
>> to Spark is problematic, because large portions of the implementation =
are
>> private[spark].
>>
>>  I'd like to help, but i need to know whose attention to get.
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.=
org
------Nodemailer-0.5.0-?=_1-1418931859167--

From dev-return-10845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 19:54:50 2014
Return-Path: <dev-return-10845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BD901006B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 19:54:50 +0000 (UTC)
Received: (qmail 21174 invoked by uid 500); 18 Dec 2014 19:54:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21102 invoked by uid 500); 18 Dec 2014 19:54:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21089 invoked by uid 99); 18 Dec 2014 19:54:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:54:47 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 19:54:20 +0000
Received: by mail-pa0-f52.google.com with SMTP id eu11so2019778pac.25
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 11:52:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=qK86DaFGmF0ziGOTIpB14qYDhMiuRFMl8tTkrxkZH/w=;
        b=ZgfHzacgGHSVw73NvcHdJoXaLlvrt6bgeziorV4Pk76UUBP/PtKB6S+HrSCFCP1N0R
         AIiEYxsO1WghmpP3E3J76zo8hW7YZDNdOqggXAb7NGQuePhGQF5wu/+P9+R8mBGTG+sp
         bZjkB9/hEOrJoDm+DtdRyHeiOZnCpfyw0JAzKIDyOs+lwtpz3k71JLFgO3DW42I06Av8
         OZB1N01arRxqkqjEfy76MlUo8DcFN+XxKGmEY9K5gt1y7daMzc1nY4lROJGv5MgRrG2b
         uVEBeCzu0HF7yjaobO0wSZA77zHTGskpUrHipuLovhxM0w+pcNQ/P0TDdbDV93/sjvce
         BnUg==
X-Received: by 10.66.100.136 with SMTP id ey8mr6171989pab.93.1418932368691;
        Thu, 18 Dec 2014 11:52:48 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id q10sm7478421pdm.78.2014.12.18.11.52.47
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 11:52:48 -0800 (PST)
Date: Thu, 18 Dec 2014 11:52:46 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Alessandro Baretta <alexbaretta@gmail.com>, 
 "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.5493308e.7545e146.10f@joshs-mbp>
In-Reply-To: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
References: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
Subject: Re: What RDD transformations trigger computations?
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5493308e_515f007c_10f"
X-Virus-Checked: Checked by ClamAV on apache.org

--5493308e_515f007c_10f
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Could you provide an example=3F =C2=A0These operations are lazy, in the s=
ense that they don=E2=80=99t trigger Spark jobs:


scala> val a =3D sc.parallelize(1 to 10000, 1).mapPartitions=7B x =3D> pr=
intln(=22computed a=21=22); x=7D
a: org.apache.spark.rdd.RDD=5BInt=5D =3D MapPartitionsRDD=5B14=5D at mapP=
artitions at <console>:18

scala> a.union(a)
res4: org.apache.spark.rdd.RDD=5BInt=5D =3D UnionRDD=5B15=5D at union at =
<console>:22

scala> a.map(x =3D> (x, x)).groupByKey()
res5: org.apache.spark.rdd.RDD=5B(Int, Iterable=5BInt=5D)=5D =3D Shuffled=
RDD=5B17=5D at groupByKey at <console>:22

scala> a.map(x =3D> (x, x)).groupByKey().count()
computed a=21
res6: Long =3D 10000


On December 18, 2014 at 1:04:54 AM, Alessandro Baretta (alexbaretta=40gma=
il.com) wrote:

All, =20

I noticed that while some operations that return RDDs are very cheap, suc=
h =20
as map and flatMap, some are quite expensive, such as union and groupByKe=
y. =20
I'm referring here to the cost of constructing the RDD scala value, not t=
he =20
cost of collecting the values contained in the RDD. This does not match m=
y =20
understanding that RDD transformations only set up a computation without =
=20
actually running it. Oh, Spark developers, can you please provide some =20
clarity=3F =20

Alex =20

--5493308e_515f007c_10f--


From dev-return-10846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 20:09:47 2014
Return-Path: <dev-return-10846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5950A1012C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 20:09:47 +0000 (UTC)
Received: (qmail 59623 invoked by uid 500); 18 Dec 2014 20:09:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59551 invoked by uid 500); 18 Dec 2014 20:09:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59538 invoked by uid 99); 18 Dec 2014 20:09:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:09:44 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:09:17 +0000
Received: by mail-pa0-f51.google.com with SMTP id ey11so2065744pad.38
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 12:07:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=qSYowGjSg6fgxnnHqHHCxweGMkZzeFbtLzs+3d4B+bM=;
        b=rXpbSbyIye39nUZQFVe8FEGHrLzH8cF1zN+vhz8ionarYmQxGXWnpSzB/BBFKltW2g
         tLyzV9ukkEEZNtfdIMyJa0B0AsnbtWMztYeYO2OQj9MiTCbZb9WNAeHfbehCcPEOhthc
         Grjl2Ls7QudgunWzaMwnfldAxFkV01ikOVPXK7UqBXXG1oP+E4R+OlumXsKdYYTcyE5m
         IWKsAbpZ4ulRx2rv8h3p09qiFTXRRGoDaQpx36AhtrHv4ShYzsXz+KBO1EUIv/7c2kP0
         zsmnrP9MLTL+LDwODQhZnXrU+oBlSda4xoL/puDnXS+5JDOzPkmuqnLZ5369O3zyVCgx
         thTw==
X-Received: by 10.69.26.130 with SMTP id iy2mr6338506pbd.93.1418933266086;
        Thu, 18 Dec 2014 12:07:46 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id oz7sm7581416pbb.14.2014.12.18.12.07.43
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 12:07:45 -0800 (PST)
Date: Thu, 18 Dec 2014 12:07:43 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>, Andrew Ash
 <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Message-ID: <etPan.5493340f.1f16e9e8.10f@joshs-mbp>
In-Reply-To: <CAOhmDzed1RdFBsLEPjYU+xK5NEA4oy_wgfN4kKgPXrUp2CX9SQ@mail.gmail.com>
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
 <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
 <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
 <CA+-p3AHqvVzuf4HqftTu8jQQSpk7TEFUp9sDbqw48hNAnO2AoA@mail.gmail.com>
 <CAOhmDzed1RdFBsLEPjYU+xK5NEA4oy_wgfN4kKgPXrUp2CX9SQ@mail.gmail.com>
Subject: Re: Spark JIRA Report
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5493340f_1190cde7_10f"
X-Virus-Checked: Checked by ClamAV on apache.org

--5493340f_1190cde7_10f
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I don=E2=80=99t think that it makes sense to just close inactive JIRA iss=
ue without any human review. =C2=A0There are many legitimate feature requ=
ests / bug reports that might be inactive for a long time because they=E2=
=80=99re low priorities to fix or because nobody has had time to deal wit=
h them yet.

On December 15, 2014 at 2:37:30 PM, Nicholas Chammas (nicholas.chammas=40=
gmail.com) wrote:

OK, that's good. =20

Another approach we can take to controlling the number of stale JIRA issu=
es =20
is writing a bot that simply closes issues after N days of inactivity and=
 =20
prompts people to reopen the issue if it's still valid. I believe Sean Ow=
en =20
proposed that at one point (=3F). =20

I wonder if that might be better since I feel that even a slimmed down =20
email might not be enough to get already-busy people to spend time on JIR=
A =20
management. =20

Nick =20

On Mon Dec 15 2014 at 12:55:06 PM Andrew Ash <andrew=40andrewash.com> wro=
te: =20

> Nick, =20
> =20
> Putting the N most stale issues into a report like your latest one does=
 =20
> seem like a good way to tackle the wall of text effect that I'm worried=
 =20
> about. =20
> =20
> On Sun, Dec 14, 2014 at 12:28 PM, Nicholas Chammas < =20
> nicholas.chammas=40gmail.com> wrote: =20
> =20
>> Taking after Andrew=E2=80=99s suggestion, perhaps the report can just =
focus on =20
>> Stale issues (no updates in > 90 days), since those are probably the =20
>> easiest to act on. =20
>> =20
>> =46or example: =20
>> Stale Issues =20
>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject%20%3D%20SPARK%=
20AND%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20O=
RDER%20BY%20updated%20ASC> =20
>> =20
>> - =5BOct 22, 2012=5D SPARK-560 =20
>> <https://issues.apache.org/jira/browse/SPARK-560>: Specialize RDDs / =20
>> iterators =20
>> - =5BOct 22, 2012=5D SPARK-540 =20
>> <https://issues.apache.org/jira/browse/SPARK-540>: Add API to =20
>> customize in-memory representation of RDDs =20
>> - =5BOct 22, 2012=5D SPARK-573 =20
>> <https://issues.apache.org/jira/browse/SPARK-573>: Clarify semantics =20
>> of the parallelized closures =20
>> - =5BNov 06, 2012=5D SPARK-609 =20
>> <https://issues.apache.org/jira/browse/SPARK-609>: Add instructions =20
>> for enabling Akka debug logging =20
>> - =5BDec 17, 2012=5D SPARK-636 =20
>> <https://issues.apache.org/jira/browse/SPARK-636>: Add mechanism to =20
>> run system management/configuration tasks on all workers =20
>> =20
>> Andrew, =20
>> =20
>> Does that seem more useful=3F =20
>> =20
>> Nick =20
>> =E2=80=8B =20
>> =20
>> On Sun Dec 14 2014 at 3:20:54 AM Nicholas Chammas < =20
>> nicholas.chammas=40gmail.com> wrote: =20
>> =20
>>> I formatted this report using Markdown; I'm open to changing the =20
>>> structure or formatting or reducing the amount of information to make=
 the =20
>>> report more easily consumable. =20
>>> =20
>>> Regarding just sending links or whether this would just be mailing li=
st =20
>>> noise, those are a good questions. =20
>>> =20
>>> I've sent out links before, but I feel from a UX perspective having t=
he =20
>>> information right in the email itself makes it frictionless for peopl=
e to =20
>>> act on the information. =46or me, that difference is enough to hook m=
e into =20
>>> spending a few minutes on JIRA vs. just glossing over an email with a=
 link. =20
>>> =20
>>> I wonder if that's also the case for others on this list. =20
>>> =20
>>> If you already spend a good amount of time cleaning up on JIRA, then =
=20
>>> this report won't be that relevant to you. But given the number and g=
rowth =20
>>> of open issues on our tracker, I suspect we could do with quite a few=
 more =20
>>> people chipping in and cleaning up where they can. =20
>>> =20
>>> That's the real problem that this report is intended to help with. =20
>>> =20
>>> Nick =20
>>> =20
>>> =20
>>> =20
>>> On Sun Dec 14 2014 at 2:49:00 AM Andrew Ash <andrew=40andrewash.com> =
=20
>>> wrote: =20
>>> =20
>>>> The goal of increasing visibility on open issues is a good one. How =
is =20
>>>> this different from just a link to Jira though=3F Some might say thi=
s adds =20
>>>> noise to the mailing list and doesn't contain any information not al=
ready =20
>>>> available in Jira. =20
>>>> =20
>>>> The idea seems good but the formatting leaves a little to be desired=
. =20
>>>> If you aren't opposed to using HTML, I might suggest this more compa=
ct =20
>>>> format: =20
>>>> =20
>>>> SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044> =20
>>>> Pluggable interface for shuffles =20
>>>> SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add =20
>>>> IndexedRDD, an efficient updatable key-value =20
>>>> SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow =
=20
>>>> for pluggable execution contexts in Spark =20
>>>> =20
>>>> Andrew =20
>>>> =20
>>>> On Sat, Dec 13, 2014 at 11:31 PM, Nicholas Chammas < =20
>>>> nicholas.chammas=40gmail.com> wrote: =20
>>>> =20
>>>>> What do y=E2=80=99all think of a report like this emailed out to th=
e dev list =20
>>>>> on a =20
>>>>> monthly basis=3F =20
>>>>> =20
>>>>> The goal would be to increase visibility into our open issues and =20
>>>>> encourage =20
>>>>> developers to tend to our issue tracker more frequently. =20
>>>>> =20
>>>>> Nick =20
>>>>> =20
>>>>> There are 1,236 unresolved issues =20
>>>>> =20
>>>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject+%3D+SPAR =20
>>>>> K+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC> =20
>>>> =20
>>>> =20
>>>>> in the Spark project on JIRA. =20
>>>>> Recently Updated Issues =20
>>>>> =20
>>>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject%20%3D% =20
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY% =20
>>>>> 20updated%20DESC> =20
>>>> =20
>>>> =20
>>>>> Type Key Priority Summary Last Updated Bug SPARK-4841 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-4841> Major Batch =20
>>>>> serializer =20
>>>> =20
>>>> =20
>>>>> bug in PySpark=E2=80=99s RDD.zip Dec 14, 2014 Question SPARK-4810 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-4810> Major =46ailed to=
 run =20
>>>> =20
>>>> =20
>>>>> collect Dec 14, 2014 Bug SPARK-785 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-785> Major ClosureClean=
er =20
>>>>> not =20
>>>> =20
>>>> =20
>>>>> invoked on most PairRDD=46unctions Dec 14, 2014 New =46eature SPARK=
-3405 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster=
 =20
>>>> =20
>>>> =20
>>>>> creation on VPC Dec 13, 2014 Improvement SPARK-1555 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-1555> Minor enable =20
>>>> =20
>>>> =20
>>>>> ec2/spark=5Fec2.py to stop/delete cluster non-interactively Dec 13,=
 =20
>>>>> 2014 Stale =20
>>>>> Issues =20
>>>>> =20
>>>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject%20%3D% =20
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20update =20
>>>>> d%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC> =20
>>>> =20
>>>> =20
>>>>> Type Key Priority Summary Last Updated Bug SPARK-560 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-560> None Specialize RD=
Ds =20
>>>>> / =20
>>>> =20
>>>> =20
>>>>> iterators Oct 22, 2012 New =46eature SPARK-540 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-540> None Add API to =20
>>>>> customize =20
>>>> =20
>>>> =20
>>>>> in-memory representation of RDDs Oct 22, 2012 Improvement SPARK-573=
 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-573> None Clarify =20
>>>>> semantics of =20
>>>> =20
>>>> =20
>>>>> the parallelized closures Oct 22, 2012 New =46eature SPARK-609 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-609> Minor Add =20
>>>>> instructions =20
>>>> =20
>>>> =20
>>>>> for enabling Akka debug logging Nov 06, 2012 New =46eature SPARK-63=
6 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanis=
m =20
>>>>> to =20
>>>> =20
>>>> =20
>>>>> run system management/configuration tasks on all workers Dec 17, 20=
12 =20
>>>>> Most =20
>>>>> Watched Issues =20
>>>>> =20
>>>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject%20%3D% =20
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY% =20
>>>>> 20watchers%20DESC> =20
>>>> =20
>>>> =20
>>>>> Type Key Priority Summary Watchers New =46eature SPARK-3561 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for =20
>>>> =20
>>>> =20
>>>>> pluggable execution contexts in Spark 75 New =46eature SPARK-2365 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add =20
>>>>> IndexedRDD, an =20
>>>> =20
>>>> =20
>>>>> efficient updatable key-value store 33 Improvement SPARK-2044 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable =20
>>>> =20
>>>> =20
>>>>> interface for shuffles 30 New =46eature SPARK-1405 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel=
 =20
>>>>> Latent =20
>>>> =20
>>>> =20
>>>>> Dirichlet Allocation (LDA) atop of spark in MLlib 26 New =46eature =
=20
>>>>> SPARK-1406 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model =
=20
>>>> =20
>>>> =20
>>>>> evaluation support via MLib 21 Most Voted Issues =20
>>>>> =20
>>>> <https://issues.apache.org/jira/issues/=3Fjql=3Dproject%20%3D% =20
>>>>> 20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY% =20
>>>>> 20votes%20DESC> =20
>>>> =20
>>>> =20
>>>>> Type Key Priority Summary Votes Bug SPARK-2541 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone =
=20
>>>>> mode =20
>>>> =20
>>>> =20
>>>>> can=E2=80=99t access secure HD=46S anymore 12 New =46eature SPARK-2=
365 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-2365> Major Add =20
>>>>> IndexedRDD, an =20
>>>> =20
>>>> =20
>>>>> efficient updatable key-value store 9 Improvement SPARK-3533 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-3533> Major Add =20
>>>> =20
>>>> =20
>>>>> saveAsText=46ileByKey() method to RDDs 8 Bug SPARK-2883 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark =20
>>>>> Support =20
>>>> =20
>>>> =20
>>>>> for ORC=46ile format 6 New =46eature SPARK-1442 =20
>>>>> =20
>>>> <https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window =
=20
>>>>> function support 6 =20
>>>>> =E2=80=8B =20
>>>>> =20
>>>> =20
>>>> =20
> =20

--5493340f_1190cde7_10f--


From dev-return-10847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 20:16:07 2014
Return-Path: <dev-return-10847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D36D1016E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 20:16:07 +0000 (UTC)
Received: (qmail 77406 invoked by uid 500); 18 Dec 2014 20:16:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77332 invoked by uid 500); 18 Dec 2014 20:16:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77321 invoked by uid 99); 18 Dec 2014 20:16:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:16:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:15:38 +0000
Received: by mail-qg0-f54.google.com with SMTP id l89so1449987qgf.27
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 12:14:31 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ApGODfMP6nei2z48IJC3Tkie7wJJqT+1/4TXAGLCA/w=;
        b=UGppATKMz2effVN+BXGRw2NN9TNh/D0kArQKQI3LkarHJfXK0V6xa1kI8bA2c/DAHE
         x10GnxZrpT/884MRrVJPMt8hYnvf5SvRoHU0vKn6DfVCKDZ7qYxN/3c/1jXav90gATk8
         5DQK5ozVll72xvlZT2XiOYU8j9x+fEg3Ywusw2phKvyHYG1LKZHL6gswmUnJpspG9ZKt
         +YG/UWI3xTxsbTqeLTBE4LAbrY7kgVyTMcrKLHCANv+tM5xx2/CHBjX9sar1ZE1qWqhh
         6QVB1eubS5vLV8ggmS+snlKg7oBOIN2C46W2DzjPoCRZaUwzWC5WRBXErhplqyP0YoAn
         h0lQ==
X-Gm-Message-State: ALoCoQmELHfbLx07rjHBqDpUXyeM/13WN/fIjwyIV+9LPMPTorfZxV32KHd3eEl/VzXlxaIUlbPV
X-Received: by 10.140.46.52 with SMTP id j49mr6822338qga.30.1418933671711;
 Thu, 18 Dec 2014 12:14:31 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.173.100 with HTTP; Thu, 18 Dec 2014 12:14:11 -0800 (PST)
In-Reply-To: <etPan.5493308e.7545e146.10f@joshs-mbp>
References: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
 <etPan.5493308e.7545e146.10f@joshs-mbp>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 18 Dec 2014 12:14:11 -0800
Message-ID: <CAPh_B=aqqQoPDFzQeFJ7d2ar0BqaERVHHnFPBkHa8KmcfDR6Zg@mail.gmail.com>
Subject: Re: What RDD transformations trigger computations?
To: Josh Rosen <rosenville@gmail.com>
Cc: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a7d4c7674b9050a833d45
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a7d4c7674b9050a833d45
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alessandro was probably referring to some transformations whose
implementations depend on some actions. For example: sortByKey requires
sampling the data to get the histogram.

There is a ticket tracking this:
https://issues.apache.org/jira/browse/SPARK-2992






On Thu, Dec 18, 2014 at 11:52 AM, Josh Rosen <rosenville@gmail.com> wrote:
>
> Could you provide an example?  These operations are lazy, in the sense
> that they don=E2=80=99t trigger Spark jobs:
>
>
> scala> val a =3D sc.parallelize(1 to 10000, 1).mapPartitions{ x =3D>
> println("computed a!"); x}
> a: org.apache.spark.rdd.RDD[Int] =3D MapPartitionsRDD[14] at mapPartition=
s
> at <console>:18
>
> scala> a.union(a)
> res4: org.apache.spark.rdd.RDD[Int] =3D UnionRDD[15] at union at <console=
>:22
>
> scala> a.map(x =3D> (x, x)).groupByKey()
> res5: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] =3D ShuffledRDD[17] =
at
> groupByKey at <console>:22
>
> scala> a.map(x =3D> (x, x)).groupByKey().count()
> computed a!
> res6: Long =3D 10000
>
>
> On December 18, 2014 at 1:04:54 AM, Alessandro Baretta (
> alexbaretta@gmail.com) wrote:
>
> All,
>
> I noticed that while some operations that return RDDs are very cheap, suc=
h
> as map and flatMap, some are quite expensive, such as union and groupByKe=
y.
> I'm referring here to the cost of constructing the RDD scala value, not t=
he
> cost of collecting the values contained in the RDD. This does not match m=
y
> understanding that RDD transformations only set up a computation without
> actually running it. Oh, Spark developers, can you please provide some
> clarity?
>
> Alex
>

--001a113a7d4c7674b9050a833d45--

From dev-return-10848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 20:26:55 2014
Return-Path: <dev-return-10848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8950E101C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 20:26:55 +0000 (UTC)
Received: (qmail 2600 invoked by uid 500); 18 Dec 2014 20:26:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2525 invoked by uid 500); 18 Dec 2014 20:26:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2503 invoked by uid 99); 18 Dec 2014 20:26:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:26:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:26:49 +0000
Received: by mail-oi0-f47.google.com with SMTP id v63so920322oia.6
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 12:26:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=gfU4kwmL759WKjDcoZ0SxJLbB5EyJzs4KDELljOPfYY=;
        b=UFi1BMLjZJkOCEvvUKmkNDNrYOVwfEl73UOhwHltAIP9ce79t09Gx3O6nrSoolf8OS
         niB6hapbqQVnpSf/jiCItQ6KDX+J2OqHYd8hz4r0Hp+S73+PzlWBiNc9VLgdJZQ4GRR5
         HvAYYwT8spQME38KM0qyxl2nFGwWufPGj6O+i4KDS3ZWw9RvVKTzmEPVrzxsGaUsOALw
         HQzR/6w3C9mTPknCra0TK8PuLxCuyzNuO9MOkbpKeZQWpBza0UDEHeZfwY7KAIYTp6xM
         TRDeJMad8uBUyfvWzZE0ld8ZU3OSiaxsX4RSX90sEPcFx7uvy9V6PbJlFTmq0sPB2D4t
         tbQg==
X-Gm-Message-State: ALoCoQmc0fs9g3mjZYBhf3zq+odNd3awaYntsDuf6ZhfFa2esMG4UAj4ekyT3PZJpW63XLjrIWHa
MIME-Version: 1.0
X-Received: by 10.202.98.10 with SMTP id w10mr2402973oib.104.1418934368772;
 Thu, 18 Dec 2014 12:26:08 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Thu, 18 Dec 2014 12:26:08 -0800 (PST)
In-Reply-To: <1418931858785.151869ee@Nodemailer>
References: <CABPQxsshp4iXBqgZDTt-EnzimEfVvxoxAE+BMLCJTXXA8hMjFA@mail.gmail.com>
	<1418931858785.151869ee@Nodemailer>
Date: Thu, 18 Dec 2014 14:26:08 -0600
Message-ID: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d31cc02ba4e050a8367fd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d31cc02ba4e050a8367fd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the replies.

Regarding skipping WAL, it's not just about optimization.  If you actually
want exactly-once semantics, you need control of kafka offsets as well,
including the ability to not use zookeeper as the system of record for
offsets.  Kafka already is a reliable system that has strong ordering
guarantees (within a partition) and does not mandate the use of zookeeper
to store offsets.  I think there should be a spark api that acts as a very
simple intermediary between Kafka and the user's choice of downstream store=
.

Take a look at the links I posted - if there's already been 2 independent
implementations of the idea, chances are it's something people need.

On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <hshreedharan@cloudera.co=
m
> wrote:
>
> Hi Cody,
>
> I am an absolute +1 on SPARK-3146. I think we can implement something
> pretty simple and lightweight for that one.
>
> For the Kafka DStream skipping the WAL implementation - this is something
> I discussed with TD a few weeks ago. Though it is a good idea to implemen=
t
> this to avoid unnecessary HDFS writes, it is an optimization. For that
> reason, we must be careful in implementation. There are a couple of issue=
s
> that we need to ensure works properly - specifically ordering. To ensure =
we
> pull messages from different topics and partitions in the same order afte=
r
> failure, we=E2=80=99d still have to persist the metadata to HDFS (or some=
 other
> system) - this metadata must contain the order of messages consumed, so w=
e
> know how to re-read the messages. I am planning to explore this once I ha=
ve
> some time (probably in Jan). In addition, we must also ensure bucketing
> functions work fine as well. I will file a placeholder jira for this one.
>
> I also wrote an API to write data back to Kafka a while back -
> https://github.com/apache/spark/pull/2994 . I am hoping that this will
> get pulled in soon, as this is something I know people want. I am open to
> feedback on that - anything that I can do to make it better.
>
> Thanks,
> Hari
>
>
> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey Cody,
>>
>> Thanks for reaching out with this. The lead on streaming is TD - he is
>> traveling this week though so I can respond a bit. To the high level
>> point of whether Kafka is important - it definitely is. Something like
>> 80% of Spark Streaming deployments (anecdotally) ingest data from
>> Kafka. Also, good support for Kafka is something we generally want in
>> Spark and not a library. In some cases IIRC there were user libraries
>> that used unstable Kafka API's and we were somewhat waiting on Kafka
>> to stabilize them to merge things upstream. Otherwise users wouldn't
>> be able to use newer Kakfa versions. This is a high level impression
>> only though, I haven't talked to TD about this recently so it's worth
>> revisiting given the developments in Kafka.
>>
>> Please do bring things up like this on the dev list if there are
>> blockers for your usage - thanks for pinging it.
>>
>> - Patrick
>>
>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>> > Now that 1.2 is finalized... who are the go-to people to get some
>> > long-standing Kafka related issues resolved?
>> >
>> > The existing api is not sufficiently safe nor flexible for our
>> production
>> > use. I don't think we're alone in this viewpoint, because I've seen
>> > several different patches and libraries to fix the same things we've
>> been
>> > running into.
>> >
>> > Regarding flexibility
>> >
>> > https://issues.apache.org/jira/browse/SPARK-3146
>> >
>> > has been outstanding since August, and IMHO an equivalent of this is
>> > absolutely necessary. We wrote a similar patch ourselves, then found
>> that
>> > PR and have been running it in production. We wouldn't be able to get
>> our
>> > jobs done without it. It also allows users to solve a whole class of
>> > problems for themselves (e.g. SPARK-2388, arbitrary delay of messages,
>> etc).
>> >
>> > Regarding safety, I understand the motivation behind WriteAheadLog as =
a
>> > general solution for streaming unreliable sources, but Kafka already i=
s
>> a
>> > reliable source. I think there's a need for an api that treats it as
>> > such. Even aside from the performance issues of duplicating the
>> > write-ahead log in kafka into another write-ahead log in hdfs, I need
>> > exactly-once semantics in the face of failure (I've had failures that
>> > prevented reloading a spark streaming checkpoint, for instance).
>> >
>> > I've got an implementation i've been using
>> >
>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
>> > /src/main/scala/org/apache/spark/rdd/kafka
>> >
>> > Tresata has something similar at https://github.com/tresata/spark-kafk=
a,
>>
>> > and I know there were earlier attempts based on Storm code.
>> >
>> > Trying to distribute these kinds of fixes as libraries rather than
>> patches
>> > to Spark is problematic, because large portions of the implementation
>> are
>> > private[spark].
>> >
>> > I'd like to help, but i need to know whose attention to get.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a113d31cc02ba4e050a8367fd--

From dev-return-10849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 20:30:28 2014
Return-Path: <dev-return-10849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8503010204
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 20:30:28 +0000 (UTC)
Received: (qmail 14094 invoked by uid 500); 18 Dec 2014 20:30:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14020 invoked by uid 500); 18 Dec 2014 20:30:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14004 invoked by uid 99); 18 Dec 2014 20:30:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:30:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 20:30:22 +0000
Received: by mail-wi0-f169.google.com with SMTP id r20so2786920wiv.2
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 12:29:16 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=t3X/nUVQshJTw8mFswYbKuqEw1vI9DYMULVQdQzJmSU=;
        b=IefbeeTQOT4Ghf+ofYs1K+16pPtnGqN4jXHxMz6lNXq3awpTEV6texjwjio/j1cmPS
         X31RDTJXAQhC76BtICxF8VSs07y0MtOYFQAIuVwskvz9DZu7CMhOhBUDc8ZTh7Ib9DSg
         gcm7EKEkIF430alajDHR/DGawnPKh38M9aK8mRWTQxgzvNo2VYnCLBUSPvxwy3xZjWTQ
         +bJJJlRNetyo0m6Mq4Kocn195w1/XyRFXud0oiH3Ew/BNjOgMAWzPBZuMSHrk2s7Vnxq
         AFCxffWHHM5CnxOTndSYUiNiNNtLzjOpBZCNjyVc8wfjRksemDdLuqBxtFx1rJrs8XQk
         TxKA==
X-Gm-Message-State: ALoCoQmCZNoznxrPouHOSkOhkzV8/hugjRmze12fcFDMKIScGsSYyIKfmiFakUtwWJ0cCklnFIxQ
MIME-Version: 1.0
X-Received: by 10.195.13.114 with SMTP id ex18mr7545538wjd.111.1418934556095;
 Thu, 18 Dec 2014 12:29:16 -0800 (PST)
Received: by 10.216.68.132 with HTTP; Thu, 18 Dec 2014 12:29:16 -0800 (PST)
In-Reply-To: <CAPh_B=aqqQoPDFzQeFJ7d2ar0BqaERVHHnFPBkHa8KmcfDR6Zg@mail.gmail.com>
References: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
	<etPan.5493308e.7545e146.10f@joshs-mbp>
	<CAPh_B=aqqQoPDFzQeFJ7d2ar0BqaERVHHnFPBkHa8KmcfDR6Zg@mail.gmail.com>
Date: Thu, 18 Dec 2014 12:29:16 -0800
Message-ID: <CAAsvFPmunHTyhQzsKCfaWvFRdSQU+tDRtM=Z8s8KoPMSTju9VA@mail.gmail.com>
Subject: Re: What RDD transformations trigger computations?
From: Mark Hamstra <mark@clearstorydata.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Josh Rosen <rosenville@gmail.com>, Alessandro Baretta <alexbaretta@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfced1c2d0fec050a8372a1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfced1c2d0fec050a8372a1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

SPARK-2992 is a good start, but it's not exhaustive.  For example,
zipWithIndex is also an eager transformation, and we occasionally see PRs
suggesting additional eager transformations.

On Thu, Dec 18, 2014 at 12:14 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> Alessandro was probably referring to some transformations whose
> implementations depend on some actions. For example: sortByKey requires
> sampling the data to get the histogram.
>
> There is a ticket tracking this:
> https://issues.apache.org/jira/browse/SPARK-2992
>
>
>
>
>
>
> On Thu, Dec 18, 2014 at 11:52 AM, Josh Rosen <rosenville@gmail.com> wrote=
:
> >
> > Could you provide an example?  These operations are lazy, in the sense
> > that they don=E2=80=99t trigger Spark jobs:
> >
> >
> > scala> val a =3D sc.parallelize(1 to 10000, 1).mapPartitions{ x =3D>
> > println("computed a!"); x}
> > a: org.apache.spark.rdd.RDD[Int] =3D MapPartitionsRDD[14] at mapPartiti=
ons
> > at <console>:18
> >
> > scala> a.union(a)
> > res4: org.apache.spark.rdd.RDD[Int] =3D UnionRDD[15] at union at
> <console>:22
> >
> > scala> a.map(x =3D> (x, x)).groupByKey()
> > res5: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] =3D ShuffledRDD[17=
] at
> > groupByKey at <console>:22
> >
> > scala> a.map(x =3D> (x, x)).groupByKey().count()
> > computed a!
> > res6: Long =3D 10000
> >
> >
> > On December 18, 2014 at 1:04:54 AM, Alessandro Baretta (
> > alexbaretta@gmail.com) wrote:
> >
> > All,
> >
> > I noticed that while some operations that return RDDs are very cheap,
> such
> > as map and flatMap, some are quite expensive, such as union and
> groupByKey.
> > I'm referring here to the cost of constructing the RDD scala value, not
> the
> > cost of collecting the values contained in the RDD. This does not match
> my
> > understanding that RDD transformations only set up a computation withou=
t
> > actually running it. Oh, Spark developers, can you please provide some
> > clarity?
> >
> > Alex
> >
>

--047d7bfced1c2d0fec050a8372a1--

From dev-return-10850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 21:27:27 2014
Return-Path: <dev-return-10850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D67E7104F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 21:27:27 +0000 (UTC)
Received: (qmail 48762 invoked by uid 500); 18 Dec 2014 21:27:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48683 invoked by uid 500); 18 Dec 2014 21:27:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48671 invoked by uid 99); 18 Dec 2014 21:27:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:27:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:27:21 +0000
Received: by mail-qc0-f181.google.com with SMTP id m20so1540519qcx.40
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 13:27:01 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:mime-version:message-id:in-reply-to
         :references:from:to:cc:subject:content-type;
        bh=7Cp6Pah6rK2ZK32q7u4hKQQ6c2LLeUvNcbeXOTSEyJE=;
        b=ErLKYsa60yxrIrMyvYRG6eFn2lZZfegN+5dpFWYkHX4sq7QDLJp2rTiwIXIRrMYlK6
         BZlKAXZOAv+tre4/3hIYqL6htmHcrZrnx6uMdIPhcqRSMhinO7RfE7JJt0t/RLZltuyS
         uUFXxvCXOYya749FcbMfu+C3P0eDBbGeir+4OMwXs2md9U/qE3WNMRkdXO7J2Fa0Pqcn
         i/THd90/q7DoUb7vJrLDYMZB1m/xSRpJ0yl+ZkQWXL7alIHY2qfENFrNYcJmkxOwMUBB
         5MvdGoe1cbLGA7PR6W5r9MY3I/HETOGv5xFjtY1qtxlUJxG3pif6nrpJMLcS57wMbxvT
         sU7g==
X-Gm-Message-State: ALoCoQmAaa5e+R2sJs08oLT0Qar0kYEwcnxolTXwP9ZxgCD/wrv/ggS4cT4knHq0xFRaTrzycUfb
X-Received: by 10.140.33.226 with SMTP id j89mr7350770qgj.23.1418938021179;
        Thu, 18 Dec 2014 13:27:01 -0800 (PST)
Received: from hedwig-6.prd.orcali.com (ec2-54-85-253-252.compute-1.amazonaws.com. [54.85.253.252])
        by mx.google.com with ESMTPSA id s3sm7857688qat.32.2014.12.18.13.27.00
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 13:27:00 -0800 (PST)
Date: Thu, 18 Dec 2014 13:27:00 -0800 (PST)
X-Google-Original-Date: Thu, 18 Dec 2014 21:26:59 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1418938019858.7377d5d7@Nodemailer>
In-Reply-To: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
X-Orchestra-Oid: E52C35D9-7A27-4DE7-980A-C48DB5854B3E
X-Orchestra-Sig: 1155fc67eede82c85dd5978c0777fd9768290918
X-Orchestra-Thrid: TEB56D109-7104-46FE-9747-57A7D6EE5D69_1487840502276290343
X-Orchestra-Thrid-Sig: 257f4f6067e02a56f8c5e6e4a3be5aafec0bf555
X-Orchestra-Account: 799e4b3e98e11c51b0fc3eaa274e3e4b1738164a
From: "Hari Shreedharan" <hshreedharan@cloudera.com>
To: "Cody Koeninger" <cody@koeninger.org>
Cc: dev@spark.apache.org, "Patrick Wendell" <pwendell@gmail.com>
Subject: Re: Which committers care about Kafka?
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1418938020242"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1418938020242
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

I get what you are saying. But getting exactly once right is an extremely =
hard problem - especially in presence of failure. The issue is failures can=
 happen in a bunch of places. For example, before the notification of =
downstream store being successful reaches the receiver that updates the =
offsets, the node fails. The store was successful, but duplicates came in =
either way. This is something worth discussing by itself - but without =
uuids etc this might not really be solved even when you think it is.




Anyway, I will look at the links. Even I am interested in all of the =
features you mentioned - no HDFS WAL for Kafka and once-only delivery, but =
I doubt the latter is really possible to guarantee - though I really would =
love to have that!




Thanks,=C2=A0Hari

On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger <cody@koeninger.org>
wrote:

> Thanks for the replies.
> Regarding skipping WAL, it's not just about optimization.  If you =
actually
> want exactly-once semantics, you need control of kafka offsets as well,
> including the ability to not use zookeeper as the system of record for
> offsets.  Kafka already is a reliable system that has strong ordering
> guarantees (within a partition) and does not mandate the use of =
zookeeper
> to store offsets.  I think there should be a spark api that acts as a =
very
> simple intermediary between Kafka and the user's choice of downstream =
store.
> Take a look at the links I posted - if there's already been 2 =
independent
> implementations of the idea, chances are it's something people need.
> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <hshreedharan@cloudera.=
com
>> wrote:
>>
>> Hi Cody,
>>
>> I am an absolute +1 on SPARK-3146. I think we can implement something
>> pretty simple and lightweight for that one.
>>
>> For the Kafka DStream skipping the WAL implementation - this is =
something
>> I discussed with TD a few weeks ago. Though it is a good idea to =
implement
>> this to avoid unnecessary HDFS writes, it is an optimization. For that
>> reason, we must be careful in implementation. There are a couple of =
issues
>> that we need to ensure works properly - specifically ordering. To ensure=
 we
>> pull messages from different topics and partitions in the same order =
after
>> failure, we=E2=80=99d still have to persist the metadata to HDFS (or =
some other
>> system) - this metadata must contain the order of messages consumed, so =
we
>> know how to re-read the messages. I am planning to explore this once I =
have
>> some time (probably in Jan). In addition, we must also ensure bucketing
>> functions work fine as well. I will file a placeholder jira for this one=
.
>>
>> I also wrote an API to write data back to Kafka a while back -
>> https://github.com/apache/spark/pull/2994 . I am hoping that this will
>> get pulled in soon, as this is something I know people want. I am open =
to
>> feedback on that - anything that I can do to make it better.
>>
>> Thanks,
>> Hari
>>
>>
>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Hey Cody,
>>>
>>> Thanks for reaching out with this. The lead on streaming is TD - he is
>>> traveling this week though so I can respond a bit. To the high level
>>> point of whether Kafka is important - it definitely is. Something like
>>> 80% of Spark Streaming deployments (anecdotally) ingest data from
>>> Kafka. Also, good support for Kafka is something we generally want in
>>> Spark and not a library. In some cases IIRC there were user libraries
>>> that used unstable Kafka API's and we were somewhat waiting on Kafka
>>> to stabilize them to merge things upstream. Otherwise users wouldn't
>>> be able to use newer Kakfa versions. This is a high level impression
>>> only though, I haven't talked to TD about this recently so it's worth
>>> revisiting given the developments in Kafka.
>>>
>>> Please do bring things up like this on the dev list if there are
>>> blockers for your usage - thanks for pinging it.
>>>
>>> - Patrick
>>>
>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>> > Now that 1.2 is finalized... who are the go-to people to get some
>>> > long-standing Kafka related issues resolved=3F
>>> >
>>> > The existing api is not sufficiently safe nor flexible for our
>>> production
>>> > use. I don't think we're alone in this viewpoint, because I've seen
>>> > several different patches and libraries to fix the same things we've
>>> been
>>> > running into.
>>> >
>>> > Regarding flexibility
>>> >
>>> > https://issues.apache.org/jira/browse/SPARK-3146
>>> >
>>> > has been outstanding since August, and IMHO an equivalent of this is
>>> > absolutely necessary. We wrote a similar patch ourselves, then found
>>> that
>>> > PR and have been running it in production. We wouldn't be able to =
get
>>> our
>>> > jobs done without it. It also allows users to solve a whole class of
>>> > problems for themselves (e.g. SPARK-2388, arbitrary delay of messages=
,
>>> etc).
>>> >
>>> > Regarding safety, I understand the motivation behind WriteAheadLog as=
 a
>>> > general solution for streaming unreliable sources, but Kafka already =
is
>>> a
>>> > reliable source. I think there's a need for an api that treats it as
>>> > such. Even aside from the performance issues of duplicating the
>>> > write-ahead log in kafka into another write-ahead log in hdfs, I =
need
>>> > exactly-once semantics in the face of failure (I've had failures =
that
>>> > prevented reloading a spark streaming checkpoint, for instance).
>>> >
>>> > I've got an implementation i've been using
>>> >
>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
>>> > /src/main/scala/org/apache/spark/rdd/kafka
>>> >
>>> > Tresata has something similar at https://github.=
com/tresata/spark-kafka,
>>>
>>> > and I know there were earlier attempts based on Storm code.
>>> >
>>> > Trying to distribute these kinds of fixes as libraries rather than
>>> patches
>>> > to Spark is problematic, because large portions of the =
implementation
>>> are
>>> > private[spark].
>>> >
>>> > I'd like to help, but i need to know whose attention to get.
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>
------Nodemailer-0.5.0-?=_1-1418938020242--

From dev-return-10851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 21:52:42 2014
Return-Path: <dev-return-10851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62822106C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 21:52:42 +0000 (UTC)
Received: (qmail 23567 invoked by uid 500); 18 Dec 2014 21:52:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23491 invoked by uid 500); 18 Dec 2014 21:52:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23480 invoked by uid 99); 18 Dec 2014 21:52:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:52:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:52:36 +0000
Received: by mail-ob0-f181.google.com with SMTP id gq1so5988195obb.12
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 13:51:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=KjMBHTNTRjHEgKtAmb3TtBd0bWMTa3UcfU1q7UGdHFI=;
        b=SfKpXvTC5/aPcDcvT1vTeCKxEVQ9eNZmdfKn2TF0H+26uWIpSybo0GDVFLKhrSAr+E
         zp2GsR7vDUh8Tzb/TrpbDxCrd6wyxBfJBoCIVZxb22gGCWxBsyGItmrff18xe9Xtbf3W
         n/LCXuunDQmp5v5xZKfWO8D15/Q8MNzVI1O3fqwrBw0If+3k1Mt3aW5gupGIr9U6GwL9
         Jx+It6HKWdGZKAazyJylVJ0CIGiH8MoO1kkJb4fnHlc9ApikO0JWCjGPJ7mmX3F9MBSk
         q27HTTlgjkfaUHkx4Z8MlTuKIXzyqXOMjDEVO20AER/YlDC5NpHtrje7hIihy9BCiGSl
         3mZQ==
X-Gm-Message-State: ALoCoQn6eyhHyrKfXP6yOjBuS7/PR5rM+RlYN0u/gzkYWObw6Hb/S9WqT2bulvDjeK5HY1IRAi2u
MIME-Version: 1.0
X-Received: by 10.182.213.72 with SMTP id nq8mr2701612obc.11.1418939514989;
 Thu, 18 Dec 2014 13:51:54 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Thu, 18 Dec 2014 13:51:54 -0800 (PST)
In-Reply-To: <1418938019858.7377d5d7@Nodemailer>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
	<1418938019858.7377d5d7@Nodemailer>
Date: Thu, 18 Dec 2014 15:51:54 -0600
Message-ID: <CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c1c848bfc747050a8499f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c848bfc747050a8499f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

If the downstream store for the output data is idempotent or transactional,
and that downstream store also is the system of record for kafka offsets,
then you have exactly-once semantics.  Commit offsets with / after the data
is stored.  On any failure, restart from the last committed offsets.

Yes, this approach is biased towards the etl-like use cases rather than
near-realtime-analytics use cases.

On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <hshreedharan@cloudera.co=
m
> wrote:
>
> I get what you are saying. But getting exactly once right is an extremely
> hard problem - especially in presence of failure. The issue is failures c=
an
> happen in a bunch of places. For example, before the notification of
> downstream store being successful reaches the receiver that updates the
> offsets, the node fails. The store was successful, but duplicates came in
> either way. This is something worth discussing by itself - but without
> uuids etc this might not really be solved even when you think it is.
>
> Anyway, I will look at the links. Even I am interested in all of the
> features you mentioned - no HDFS WAL for Kafka and once-only delivery, bu=
t
> I doubt the latter is really possible to guarantee - though I really woul=
d
> love to have that!
>
> Thanks,
> Hari
>
>
> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
>
>> Thanks for the replies.
>>
>> Regarding skipping WAL, it's not just about optimization.  If you
>> actually want exactly-once semantics, you need control of kafka offsets =
as
>> well, including the ability to not use zookeeper as the system of record
>> for offsets.  Kafka already is a reliable system that has strong orderin=
g
>> guarantees (within a partition) and does not mandate the use of zookeepe=
r
>> to store offsets.  I think there should be a spark api that acts as a ve=
ry
>> simple intermediary between Kafka and the user's choice of downstream st=
ore.
>>
>> Take a look at the links I posted - if there's already been 2 independen=
t
>> implementations of the idea, chances are it's something people need.
>>
>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>> hshreedharan@cloudera.com> wrote:
>>>
>>> Hi Cody,
>>>
>>> I am an absolute +1 on SPARK-3146. I think we can implement something
>>> pretty simple and lightweight for that one.
>>>
>>> For the Kafka DStream skipping the WAL implementation - this is
>>> something I discussed with TD a few weeks ago. Though it is a good idea=
 to
>>> implement this to avoid unnecessary HDFS writes, it is an optimization.=
 For
>>> that reason, we must be careful in implementation. There are a couple o=
f
>>> issues that we need to ensure works properly - specifically ordering. T=
o
>>> ensure we pull messages from different topics and partitions in the sam=
e
>>> order after failure, we=E2=80=99d still have to persist the metadata to=
 HDFS (or
>>> some other system) - this metadata must contain the order of messages
>>> consumed, so we know how to re-read the messages. I am planning to expl=
ore
>>> this once I have some time (probably in Jan). In addition, we must also
>>> ensure bucketing functions work fine as well. I will file a placeholder
>>> jira for this one.
>>>
>>> I also wrote an API to write data back to Kafka a while back -
>>> https://github.com/apache/spark/pull/2994 . I am hoping that this will
>>> get pulled in soon, as this is something I know people want. I am open =
to
>>> feedback on that - anything that I can do to make it better.
>>>
>>> Thanks,
>>> Hari
>>>
>>>
>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>>>  Hey Cody,
>>>>
>>>> Thanks for reaching out with this. The lead on streaming is TD - he is
>>>> traveling this week though so I can respond a bit. To the high level
>>>> point of whether Kafka is important - it definitely is. Something like
>>>> 80% of Spark Streaming deployments (anecdotally) ingest data from
>>>> Kafka. Also, good support for Kafka is something we generally want in
>>>> Spark and not a library. In some cases IIRC there were user libraries
>>>> that used unstable Kafka API's and we were somewhat waiting on Kafka
>>>> to stabilize them to merge things upstream. Otherwise users wouldn't
>>>> be able to use newer Kakfa versions. This is a high level impression
>>>> only though, I haven't talked to TD about this recently so it's worth
>>>> revisiting given the developments in Kafka.
>>>>
>>>> Please do bring things up like this on the dev list if there are
>>>> blockers for your usage - thanks for pinging it.
>>>>
>>>> - Patrick
>>>>
>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org>
>>>> wrote:
>>>> > Now that 1.2 is finalized... who are the go-to people to get some
>>>> > long-standing Kafka related issues resolved?
>>>> >
>>>> > The existing api is not sufficiently safe nor flexible for our
>>>> production
>>>> > use. I don't think we're alone in this viewpoint, because I've seen
>>>> > several different patches and libraries to fix the same things we've
>>>> been
>>>> > running into.
>>>> >
>>>> > Regarding flexibility
>>>> >
>>>> > https://issues.apache.org/jira/browse/SPARK-3146
>>>> >
>>>> > has been outstanding since August, and IMHO an equivalent of this is
>>>> > absolutely necessary. We wrote a similar patch ourselves, then found
>>>> that
>>>> > PR and have been running it in production. We wouldn't be able to ge=
t
>>>> our
>>>> > jobs done without it. It also allows users to solve a whole class of
>>>> > problems for themselves (e.g. SPARK-2388, arbitrary delay of
>>>> messages, etc).
>>>> >
>>>> > Regarding safety, I understand the motivation behind WriteAheadLog a=
s
>>>> a
>>>> > general solution for streaming unreliable sources, but Kafka already
>>>> is a
>>>> > reliable source. I think there's a need for an api that treats it as
>>>> > such. Even aside from the performance issues of duplicating the
>>>> > write-ahead log in kafka into another write-ahead log in hdfs, I nee=
d
>>>> > exactly-once semantics in the face of failure (I've had failures tha=
t
>>>> > prevented reloading a spark streaming checkpoint, for instance).
>>>> >
>>>> > I've got an implementation i've been using
>>>> >
>>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
>>>> > /src/main/scala/org/apache/spark/rdd/kafka
>>>> >
>>>> > Tresata has something similar at
>>>> https://github.com/tresata/spark-kafka,
>>>> > and I know there were earlier attempts based on Storm code.
>>>> >
>>>> > Trying to distribute these kinds of fixes as libraries rather than
>>>> patches
>>>> > to Spark is problematic, because large portions of the implementatio=
n
>>>> are
>>>> > private[spark].
>>>> >
>>>> > I'd like to help, but i need to know whose attention to get.
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>>
>>>
>

--001a11c1c848bfc747050a8499f8--

From dev-return-10852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 21:57:04 2014
Return-Path: <dev-return-10852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B7CB1071E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 21:57:04 +0000 (UTC)
Received: (qmail 36172 invoked by uid 500); 18 Dec 2014 21:57:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36099 invoked by uid 500); 18 Dec 2014 21:57:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36087 invoked by uid 99); 18 Dec 2014 21:57:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:57:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of langel.groups@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 21:56:36 +0000
Received: by mail-wi0-f182.google.com with SMTP id h11so69201wiw.9
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 13:56:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SjKtVUfv1y+C+HffQJE0V53CgsScwvNnrz7eyUHYo0E=;
        b=ZGKrxY/Gs32C0n7SRHFA+a3U7N635GvFETqF2EWo3vM6R+Az/63B3ae9BcfNlTQla1
         +Xa9iyh/s/CH3WFwXPStA2c1rou1TWsbwLK4m9+scQZiUizFtgl4Wb81Ap1kmcATbZO9
         Ybw1fjoSHk/gJPtivYO07vIycbYc8L/D9psb2QkSTOoxnGUBrSeYujuuUCEAQlj5AVtq
         YHWmuyFE6XryPlSQ48wQOpGcsmSESDTFh/PPkwtc5fE+qlfc1iTNswdINnpgRHX2Nh6n
         TUYbB5TA/JujsODWwxEZiyW5SrQH4zStBfl4+LuCOcFk5pWjKHcwmweh+cNSKDSbcX+D
         oBOw==
MIME-Version: 1.0
X-Received: by 10.181.13.106 with SMTP id ex10mr28096940wid.36.1418939795902;
 Thu, 18 Dec 2014 13:56:35 -0800 (PST)
Received: by 10.194.19.234 with HTTP; Thu, 18 Dec 2014 13:56:35 -0800 (PST)
Received: by 10.194.19.234 with HTTP; Thu, 18 Dec 2014 13:56:35 -0800 (PST)
In-Reply-To: <CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
	<1418938019858.7377d5d7@Nodemailer>
	<CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
Date: Thu, 18 Dec 2014 21:56:35 +0000
Message-ID: <CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: =?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043be12c7e192e050a84aad4
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043be12c7e192e050a84aad4
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

But idempotency is not that easy t achieve sometimes. A strong only once
semantic through a proper API would  be superuseful; but I'm not implying
this is easy to achieve.
On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote:

> If the downstream store for the output data is idempotent or transactiona=
l,
> and that downstream store also is the system of record for kafka offsets,
> then you have exactly-once semantics.  Commit offsets with / after the da=
ta
> is stored.  On any failure, restart from the last committed offsets.
>
> Yes, this approach is biased towards the etl-like use cases rather than
> near-realtime-analytics use cases.
>
> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> hshreedharan@cloudera.com
> > wrote:
> >
> > I get what you are saying. But getting exactly once right is an extreme=
ly
> > hard problem - especially in presence of failure. The issue is failures
> can
> > happen in a bunch of places. For example, before the notification of
> > downstream store being successful reaches the receiver that updates the
> > offsets, the node fails. The store was successful, but duplicates came =
in
> > either way. This is something worth discussing by itself - but without
> > uuids etc this might not really be solved even when you think it is.
> >
> > Anyway, I will look at the links. Even I am interested in all of the
> > features you mentioned - no HDFS WAL for Kafka and once-only delivery,
> but
> > I doubt the latter is really possible to guarantee - though I really
> would
> > love to have that!
> >
> > Thanks,
> > Hari
> >
> >
> > On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger <cody@koeninger.org>
> > wrote:
> >
> >> Thanks for the replies.
> >>
> >> Regarding skipping WAL, it's not just about optimization.  If you
> >> actually want exactly-once semantics, you need control of kafka offset=
s
> as
> >> well, including the ability to not use zookeeper as the system of reco=
rd
> >> for offsets.  Kafka already is a reliable system that has strong
> ordering
> >> guarantees (within a partition) and does not mandate the use of
> zookeeper
> >> to store offsets.  I think there should be a spark api that acts as a
> very
> >> simple intermediary between Kafka and the user's choice of downstream
> store.
> >>
> >> Take a look at the links I posted - if there's already been 2
> independent
> >> implementations of the idea, chances are it's something people need.
> >>
> >> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> >> hshreedharan@cloudera.com> wrote:
> >>>
> >>> Hi Cody,
> >>>
> >>> I am an absolute +1 on SPARK-3146. I think we can implement something
> >>> pretty simple and lightweight for that one.
> >>>
> >>> For the Kafka DStream skipping the WAL implementation - this is
> >>> something I discussed with TD a few weeks ago. Though it is a good
> idea to
> >>> implement this to avoid unnecessary HDFS writes, it is an
> optimization. For
> >>> that reason, we must be careful in implementation. There are a couple
> of
> >>> issues that we need to ensure works properly - specifically ordering.
> To
> >>> ensure we pull messages from different topics and partitions in the
> same
> >>> order after failure, we=E2=80=99d still have to persist the metadata =
to HDFS
> (or
> >>> some other system) - this metadata must contain the order of messages
> >>> consumed, so we know how to re-read the messages. I am planning to
> explore
> >>> this once I have some time (probably in Jan). In addition, we must al=
so
> >>> ensure bucketing functions work fine as well. I will file a placehold=
er
> >>> jira for this one.
> >>>
> >>> I also wrote an API to write data back to Kafka a while back -
> >>> https://github.com/apache/spark/pull/2994 . I am hoping that this wil=
l
> >>> get pulled in soon, as this is something I know people want. I am ope=
n
> to
> >>> feedback on that - anything that I can do to make it better.
> >>>
> >>> Thanks,
> >>> Hari
> >>>
> >>>
> >>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell <pwendell@gmail.com=
>
> >>> wrote:
> >>>
> >>>>  Hey Cody,
> >>>>
> >>>> Thanks for reaching out with this. The lead on streaming is TD - he =
is
> >>>> traveling this week though so I can respond a bit. To the high level
> >>>> point of whether Kafka is important - it definitely is. Something li=
ke
> >>>> 80% of Spark Streaming deployments (anecdotally) ingest data from
> >>>> Kafka. Also, good support for Kafka is something we generally want i=
n
> >>>> Spark and not a library. In some cases IIRC there were user librarie=
s
> >>>> that used unstable Kafka API's and we were somewhat waiting on Kafka
> >>>> to stabilize them to merge things upstream. Otherwise users wouldn't
> >>>> be able to use newer Kakfa versions. This is a high level impression
> >>>> only though, I haven't talked to TD about this recently so it's wort=
h
> >>>> revisiting given the developments in Kafka.
> >>>>
> >>>> Please do bring things up like this on the dev list if there are
> >>>> blockers for your usage - thanks for pinging it.
> >>>>
> >>>> - Patrick
> >>>>
> >>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger <cody@koeninger.org>
> >>>> wrote:
> >>>> > Now that 1.2 is finalized... who are the go-to people to get some
> >>>> > long-standing Kafka related issues resolved?
> >>>> >
> >>>> > The existing api is not sufficiently safe nor flexible for our
> >>>> production
> >>>> > use. I don't think we're alone in this viewpoint, because I've see=
n
> >>>> > several different patches and libraries to fix the same things we'=
ve
> >>>> been
> >>>> > running into.
> >>>> >
> >>>> > Regarding flexibility
> >>>> >
> >>>> > https://issues.apache.org/jira/browse/SPARK-3146
> >>>> >
> >>>> > has been outstanding since August, and IMHO an equivalent of this =
is
> >>>> > absolutely necessary. We wrote a similar patch ourselves, then fou=
nd
> >>>> that
> >>>> > PR and have been running it in production. We wouldn't be able to
> get
> >>>> our
> >>>> > jobs done without it. It also allows users to solve a whole class =
of
> >>>> > problems for themselves (e.g. SPARK-2388, arbitrary delay of
> >>>> messages, etc).
> >>>> >
> >>>> > Regarding safety, I understand the motivation behind WriteAheadLog
> as
> >>>> a
> >>>> > general solution for streaming unreliable sources, but Kafka alrea=
dy
> >>>> is a
> >>>> > reliable source. I think there's a need for an api that treats it =
as
> >>>> > such. Even aside from the performance issues of duplicating the
> >>>> > write-ahead log in kafka into another write-ahead log in hdfs, I
> need
> >>>> > exactly-once semantics in the face of failure (I've had failures
> that
> >>>> > prevented reloading a spark streaming checkpoint, for instance).
> >>>> >
> >>>> > I've got an implementation i've been using
> >>>> >
> >>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
> >>>> > /src/main/scala/org/apache/spark/rdd/kafka
> >>>> >
> >>>> > Tresata has something similar at
> >>>> https://github.com/tresata/spark-kafka,
> >>>> > and I know there were earlier attempts based on Storm code.
> >>>> >
> >>>> > Trying to distribute these kinds of fixes as libraries rather than
> >>>> patches
> >>>> > to Spark is problematic, because large portions of the
> implementation
> >>>> are
> >>>> > private[spark].
> >>>> >
> >>>> > I'd like to help, but i need to know whose attention to get.
> >>>>
> >>>> --------------------------------------------------------------------=
-
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>
> >>>>
> >>>
> >
>

--f46d043be12c7e192e050a84aad4--

From dev-return-10853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 22:01:33 2014
Return-Path: <dev-return-10853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 368F51076B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 22:01:33 +0000 (UTC)
Received: (qmail 50188 invoked by uid 500); 18 Dec 2014 22:01:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50110 invoked by uid 500); 18 Dec 2014 22:01:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50090 invoked by uid 99); 18 Dec 2014 22:01:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 22:01:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 22:01:03 +0000
Received: by mail-wi0-f175.google.com with SMTP id l15so80212wiw.8
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 14:01:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type:content-transfer-encoding;
        bh=9azpD94F5AX9pXj8x+S/olMmzi5qz9Zr9YUvt4WeTBA=;
        b=cmczI+tts6C58jti5C0evDFp8L73KH+UEHCrTl950c5YDPHt8IWxZzzPD6nYOLaPNF
         Bgl43YueS2H81p5G3DsuGHQrL7ixrsA9S2qnG/PGBaTqQ9M9ArhxxSHfCp5dzQrIJs5K
         P0ZONm/ZHI/vWoipbOZMYCz6wXrzstUF93h8wOCD9kZ9KesUGE4/FFFOdiptaKcOW/2U
         gKA6/oTdAuVjLRGiNJjmn3LxiUT+iStGoHnQxIoRLYxKKyKkpKiJKUyLTJ8315sloSsQ
         2uL2xas6UCiNL0UeI+qWjTdBJtGl1AJOf7LuYdrAfzmsIGJj+Dg8VE1P5hrNqVwXsUZ+
         rM6A==
X-Gm-Message-State: ALoCoQnQSA7wRbb38TaQAAz6NMAKJFvVZOWFoTFNSpB7KHVpqW0QxFB2ffa1pxQwGnzyzz+KysLo
X-Received: by 10.180.20.242 with SMTP id q18mr8844633wie.80.1418940061556;
 Thu, 18 Dec 2014 14:01:01 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Thu, 18 Dec 2014 14:00:41 -0800 (PST)
In-Reply-To: <etPan.5493340f.1f16e9e8.10f@joshs-mbp>
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
 <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
 <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
 <CA+-p3AHqvVzuf4HqftTu8jQQSpk7TEFUp9sDbqw48hNAnO2AoA@mail.gmail.com>
 <CAOhmDzed1RdFBsLEPjYU+xK5NEA4oy_wgfN4kKgPXrUp2CX9SQ@mail.gmail.com> <etPan.5493340f.1f16e9e8.10f@joshs-mbp>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 18 Dec 2014 22:00:41 +0000
Message-ID: <CAMAsSdLH0aAJrimpv5JkEB7EtfZQnEi8behM7=MoZVgwdRXAnw@mail.gmail.com>
Subject: Fwd: Spark JIRA Report
To: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

In practice, most issues with no activity for, say, 6+ months are
dead. There's down-side in believing they will eventually get done by
somebody, since they almost always don't.

Most is clutter, but if there are important bugs among them, then the
fact they're idling is a different problem: too much demand / not
enough supply of attention, not saying 'no' to enough, fast enough,
and so on.

Sure you can prompt people to at least ping an issue they care about
once every 6 months to keep it alive. Which is essentially the same
as: Resolve and invite anyone who cares to Reopen. If nobody bothers,
can it be important? If the problem is, well, nobody would really be
paying attention to the prompts, that's this different problem again.

So: I think the auto-Resolve idea, or an email blast, is at best a
forcing mechanism to pay attention to a more fundamental issue. I
myself am less interested in that than working on the causes of
long-lived important stuff in a JIRA backlog.

You can see regular process progress like auto-closing PRs,
spark-prs.appspot.com, some big passes at closing stale issues. It's
still my impression that the bulk of existing JIRA does not get
reviewed, so there's more to do. For example, from a recent tour
through the JIRA list, there were ~50 that were even definitively
resolved, and not marked as such. It's not for lack of excellent
effort. The pace of good change outstrips any other project I've seen
by a wide margin, dwarfed only by unprecedented inbound load.

I'd rather the conversation be about more attacks on the supply/demand
problem, like adding committers to offload resolution of the easy and
clear changes more rapidly, docs or tools to help contributors make
better PRs/JIRAs in the first place, stating what is in and out of
scope upfront to direct efforts, and so on. That's a different
discussion from this one though.


On Thu, Dec 18, 2014 at 8:07 PM, Josh Rosen <rosenville@gmail.com> wrote:
> I don=E2=80=99t think that it makes sense to just close inactive JIRA iss=
ue without any human review.  There are many legitimate feature requests / =
bug reports that might be inactive for a long time because they=E2=80=99re =
low priorities to fix or because nobody has had time to deal with them yet.
>
> On December 15, 2014 at 2:37:30 PM, Nicholas Chammas (nicholas.chammas@gm=
ail.com) wrote:
>
> OK, that's good.
>
> Another approach we can take to controlling the number of stale JIRA issu=
es
> is writing a bot that simply closes issues after N days of inactivity and
> prompts people to reopen the issue if it's still valid. I believe Sean Ow=
en
> proposed that at one point (?).
>
> I wonder if that might be better since I feel that even a slimmed down
> email might not be enough to get already-busy people to spend time on JIR=
A
> management.
>
> Nick
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 22:03:27 2014
Return-Path: <dev-return-10854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F3FE010792
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 22:03:26 +0000 (UTC)
Received: (qmail 57215 invoked by uid 500); 18 Dec 2014 22:03:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57140 invoked by uid 500); 18 Dec 2014 22:03:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57128 invoked by uid 99); 18 Dec 2014 22:03:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 22:03:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 22:03:20 +0000
Received: by mail-ob0-f178.google.com with SMTP id gq1so6136850obb.9
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 14:02:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=hyH82GHhpm7nOM4mVX5XE8do89XnIsrxnWHX0F8iUGg=;
        b=pkEnjdWJrgEnLsSfyDimDXhLg18zpLP30YLwWHpKz8Rg2zQT9WVKnOvgpAVL7db5CG
         jVII+R9Bm5z6LIVyUE85roPmoO2H6Sf6DDeh3NgcjSfcIPuqWZ43nPnw2hnYxE3SiyCa
         CdybAt9ax0RKoz19J5IOOwH4hM4fHgsJtGKMrrc2YTRIubENfYZ04E4pVPbvnWrnbnEu
         jOSgwBAndiBK+eEmPB4mYT9r+4sF4YvrKjINLnPg3wFMBLWANneF+ITWpDfBJZ51heyx
         3pZnvR0Xg3clgy+jVLNtqzMQzAuuHHmF/gnlupz9S0hOW0X/HF6+OSh/FEtcVZfOj0YA
         YmNw==
X-Received: by 10.182.118.169 with SMTP id kn9mr2722899obb.57.1418940134878;
 Thu, 18 Dec 2014 14:02:14 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Thu, 18 Dec 2014 14:01:54 -0800 (PST)
In-Reply-To: <CAPh_B=aqqQoPDFzQeFJ7d2ar0BqaERVHHnFPBkHa8KmcfDR6Zg@mail.gmail.com>
References: <CAJc_syLBtPPQGUX4dG1KsUWtmSsn=MG9grgpt4mbG42QkMeYwQ@mail.gmail.com>
 <etPan.5493308e.7545e146.10f@joshs-mbp> <CAPh_B=aqqQoPDFzQeFJ7d2ar0BqaERVHHnFPBkHa8KmcfDR6Zg@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Thu, 18 Dec 2014 14:01:54 -0800
Message-ID: <CAJc_sy+CH1i5M+iE5eApwuAB4QzYoqPRORSvNACjuwcTRcdoXQ@mail.gmail.com>
Subject: Re: What RDD transformations trigger computations?
To: Reynold Xin <rxin@databricks.com>
Cc: Josh Rosen <rosenville@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cdc8b27688050a84be1d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cdc8b27688050a84be1d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Reynold,

Yes, this exactly what I was referring to. I specifically noted this
unexpected behavior with sortByKey. I also noted that union is unexpectedly
very slow, taking several minutes to define the RDD: although it does not
seem to trigger a spark computation per se, it seems to cause the input
files to read by the Hadoop subsystem, which to the console such messages
as these:

14/12/18 05:52:49 INFO mapred.FileInputFormat: Total input paths to process
: 9
14/12/18 05:54:15 INFO mapred.FileInputFormat: Total input paths to process
: 759
14/12/18 05:54:40 INFO mapred.FileInputFormat: Total input paths to process
: 228
14/12/18 06:00:11 INFO mapred.FileInputFormat: Total input paths to process
: 3076
14/12/18 06:02:02 INFO mapred.FileInputFormat: Total input paths to process
: 1013
14/12/18 06:02:21 INFO mapred.FileInputFormat: Total input paths to process
: 156

More generally, it would be important for the documentation to clearly
point out what RDD transformations are eager, otherwise it is easy to
introduce horrible performance bugs by constructing unneeded RDDs, assuming
this is a lazy operation. I would venture to suggest introducing one or
more traits to collect all the eager RDD-to-RDD transformations, so that
the type system can be used to enforce that no eager transformation is used
where a lazy one is intended to be used.

Alex

On Thu, Dec 18, 2014 at 12:14 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> Alessandro was probably referring to some transformations whose
> implementations depend on some actions. For example: sortByKey requires
> sampling the data to get the histogram.
>
> There is a ticket tracking this:
> https://issues.apache.org/jira/browse/SPARK-2992
>
>
>
>
>
>
> On Thu, Dec 18, 2014 at 11:52 AM, Josh Rosen <rosenville@gmail.com> wrote=
:
>>
>> Could you provide an example?  These operations are lazy, in the sense
>> that they don=E2=80=99t trigger Spark jobs:
>>
>>
>> scala> val a =3D sc.parallelize(1 to 10000, 1).mapPartitions{ x =3D>
>> println("computed a!"); x}
>> a: org.apache.spark.rdd.RDD[Int] =3D MapPartitionsRDD[14] at mapPartitio=
ns
>> at <console>:18
>>
>> scala> a.union(a)
>> res4: org.apache.spark.rdd.RDD[Int] =3D UnionRDD[15] at union at
>> <console>:22
>>
>> scala> a.map(x =3D> (x, x)).groupByKey()
>> res5: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] =3D ShuffledRDD[17]=
 at
>> groupByKey at <console>:22
>>
>> scala> a.map(x =3D> (x, x)).groupByKey().count()
>> computed a!
>> res6: Long =3D 10000
>>
>>
>> On December 18, 2014 at 1:04:54 AM, Alessandro Baretta (
>> alexbaretta@gmail.com) wrote:
>>
>> All,
>>
>> I noticed that while some operations that return RDDs are very cheap, su=
ch
>> as map and flatMap, some are quite expensive, such as union and
>> groupByKey.
>> I'm referring here to the cost of constructing the RDD scala value, not
>> the
>> cost of collecting the values contained in the RDD. This does not match =
my
>> understanding that RDD transformations only set up a computation without
>> actually running it. Oh, Spark developers, can you please provide some
>> clarity?
>>
>> Alex
>>
>

--089e0149cdc8b27688050a84be1d--

From dev-return-10855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 18 23:38:41 2014
Return-Path: <dev-return-10855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D805B10C0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 18 Dec 2014 23:38:41 +0000 (UTC)
Received: (qmail 87871 invoked by uid 500); 18 Dec 2014 23:38:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87803 invoked by uid 500); 18 Dec 2014 23:38:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87790 invoked by uid 99); 18 Dec 2014 23:38:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 23:38:39 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 18 Dec 2014 23:38:35 +0000
Received: by mail-pd0-f170.google.com with SMTP id v10so2394156pde.1
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 15:37:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=yIaZ67XNqRCas6XBsteb2b4FSZ/8oetY/Y6qBRTJIRQ=;
        b=avu7Ywxd75W++Bm7Bm+6XEZkIoatPMPNnqf1OyCRQF22HAVBwUh8gUzYjS5U0dP8EV
         v/+IRx5LA046Clz1x0t9VUURQRYHjpflpnu9eBkF5sA3khYJ2hiswSjcIduNQMGzDJzC
         oK6QtaebaNBKMffUNL9qyJmjpcPgM7aD+l9/GckiQA5vadRIem0u0GIItv6RsTDXzpgy
         1zi7siT2cb4AuP5vFdhmlbdkjnz2O8NdbV33SEYnei8WLldrvdFEMWSYP+ta9h3tSAnS
         EAQPsMRrr4qBUJbfP3MkflGE1stYXU1yycxSTbnHtXo0Ub85+nNJV0pbFCYeXdHPR5jt
         E0Bg==
X-Received: by 10.70.98.233 with SMTP id el9mr7691932pdb.132.1418945850046;
        Thu, 18 Dec 2014 15:37:30 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id wt4sm7847370pab.4.2014.12.18.15.37.28
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 18 Dec 2014 15:37:28 -0800 (PST)
Date: Thu, 18 Dec 2014 15:37:27 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Message-ID: <etPan.54936537.3352255a.10f@joshs-mbp>
In-Reply-To: <CAMAsSdLH0aAJrimpv5JkEB7EtfZQnEi8behM7=MoZVgwdRXAnw@mail.gmail.com>
References: <CAOhmDzeqfBH9WGJ_AUMaHe6_OHb5CejRvFNZnWx3FKeKVDw+0g@mail.gmail.com>
 <CA+-p3AERX_FG+QCK3sXGDvHwfPFoYLEbcvMJq=Bz5u3KgA0Hbg@mail.gmail.com>
 <CAOhmDzcZKX803F=NGu_KBM53uKwgZETB4rR=d+9mYd0He8BBhA@mail.gmail.com>
 <CAOhmDzdTH_W7RphoGmxLvj_e_+kUsysUeaaKaSKSffMDiJti_w@mail.gmail.com>
 <CA+-p3AHqvVzuf4HqftTu8jQQSpk7TEFUp9sDbqw48hNAnO2AoA@mail.gmail.com>
 <CAOhmDzed1RdFBsLEPjYU+xK5NEA4oy_wgfN4kKgPXrUp2CX9SQ@mail.gmail.com>
 <etPan.5493340f.1f16e9e8.10f@joshs-mbp>
 <CAMAsSdLH0aAJrimpv5JkEB7EtfZQnEi8behM7=MoZVgwdRXAnw@mail.gmail.com>
Subject: Re: Fwd: Spark JIRA Report
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="54936537_109cf92e_10f"
X-Virus-Checked: Checked by ClamAV on apache.org

--54936537_109cf92e_10f
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Slightly off-topic, but or helping to clear the PR review backlog, I have=
 a proposal to add some =E2=80=9CPR lifecycle=E2=80=9D tools to spark-prs=
.appspot.com=C2=A0to make it easier to track which PRs are blocked on rev=
iewers vs. authors:=C2=A0https://github.com/databricks/spark-pr-dashboard=
/pull/39


On December 18, 2014 at 2:01:31 PM, Sean Owen (sowen=40cloudera.com) wrot=
e:

In practice, most issues with no activity for, say, 6+ months are =20
dead. There's down-side in believing they will eventually get done by =20
somebody, since they almost always don't. =20

Most is clutter, but if there are important bugs among them, then the =20
fact they're idling is a different problem: too much demand / not =20
enough supply of attention, not saying 'no' to enough, fast enough, =20
and so on. =20

Sure you can prompt people to at least ping an issue they care about =20
once every 6 months to keep it alive. Which is essentially the same =20
as: Resolve and invite anyone who cares to Reopen. If nobody bothers, =20
can it be important=3F If the problem is, well, nobody would really be =20
paying attention to the prompts, that's this different problem again. =20

So: I think the auto-Resolve idea, or an email blast, is at best a =20
forcing mechanism to pay attention to a more fundamental issue. I =20
myself am less interested in that than working on the causes of =20
long-lived important stuff in a JIRA backlog. =20

You can see regular process progress like auto-closing PRs, =20
spark-prs.appspot.com, some big passes at closing stale issues. It's =20
still my impression that the bulk of existing JIRA does not get =20
reviewed, so there's more to do. =46or example, from a recent tour =20
through the JIRA list, there were =7E50 that were even definitively =20
resolved, and not marked as such. It's not for lack of excellent =20
effort. The pace of good change outstrips any other project I've seen =20
by a wide margin, dwarfed only by unprecedented inbound load. =20

I'd rather the conversation be about more attacks on the supply/demand =20
problem, like adding committers to offload resolution of the easy and =20
clear changes more rapidly, docs or tools to help contributors make =20
better PRs/JIRAs in the first place, stating what is in and out of =20
scope upfront to direct efforts, and so on. That's a different =20
discussion from this one though. =20


On Thu, Dec 18, 2014 at 8:07 PM, Josh Rosen <rosenville=40gmail.com> wrot=
e: =20
> I don=E2=80=99t think that it makes sense to just close inactive JIRA i=
ssue without any human review. There are many legitimate feature requests=
 / bug reports that might be inactive for a long time because they=E2=80=99=
re low priorities to fix or because nobody has had time to deal with them=
 yet. =20
> =20
> On December 15, 2014 at 2:37:30 PM, Nicholas Chammas (nicholas.chammas=40=
gmail.com) wrote: =20
> =20
> OK, that's good. =20
> =20
> Another approach we can take to controlling the number of stale JIRA is=
sues =20
> is writing a bot that simply closes issues after N days of inactivity a=
nd =20
> prompts people to reopen the issue if it's still valid. I believe Sean =
Owen =20
> proposed that at one point (=3F). =20
> =20
> I wonder if that might be better since I feel that even a slimmed down =
=20
> email might not be enough to get already-busy people to spend time on J=
IRA =20
> management. =20
> =20
> Nick =20
> =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--54936537_109cf92e_10f--


From dev-return-10856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 00:39:16 2014
Return-Path: <dev-return-10856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02C6110FA7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 00:39:16 +0000 (UTC)
Received: (qmail 84960 invoked by uid 500); 19 Dec 2014 00:39:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84889 invoked by uid 500); 19 Dec 2014 00:39:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83907 invoked by uid 99); 19 Dec 2014 00:39:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 00:39:13 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of andykonwinski@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 00:39:08 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 1F42BE25754
	for <dev@spark.apache.org>; Thu, 18 Dec 2014 16:38:19 -0800 (PST)
Date: Thu, 18 Dec 2014 17:38:17 -0700 (MST)
From: andy <andykonwinski@gmail.com>
To: dev@spark.apache.org
Message-ID: <1418949497973-9842.post@n3.nabble.com>
In-Reply-To: <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com> <CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com> <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I just changed the domain name in the mailing list archive settings to remove
".incubator" so maybe it'll work now.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Nabble-mailing-list-mirror-errors-This-post-has-NOT-been-accepted-by-the-mailing-list-yet-tp9772p9842.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 00:40:40 2014
Return-Path: <dev-return-10857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADFC710FB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 00:40:40 +0000 (UTC)
Received: (qmail 90666 invoked by uid 500); 19 Dec 2014 00:40:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90590 invoked by uid 500); 19 Dec 2014 00:40:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90579 invoked by uid 99); 19 Dec 2014 00:40:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 00:40:38 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of andykonwinski@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 00:40:12 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 8DA70E25790
	for <dev@spark.apache.org>; Thu, 18 Dec 2014 16:39:42 -0800 (PST)
Date: Thu, 18 Dec 2014 17:39:41 -0700 (MST)
From: andy <andykonwinski@gmail.com>
To: dev@spark.apache.org
Message-ID: <1418949581426-9843.post@n3.nabble.com>
In-Reply-To: <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com> <CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com> <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I just changed the domain name in the mailing list archive settings to remove
".incubator" so maybe it'll work now.

Andy



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Nabble-mailing-list-mirror-errors-This-post-has-NOT-been-accepted-by-the-mailing-list-yet-tp9772p9843.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 02:04:19 2014
Return-Path: <dev-return-10858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5701FC336
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 02:04:19 +0000 (UTC)
Received: (qmail 4874 invoked by uid 500); 19 Dec 2014 02:04:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4789 invoked by uid 500); 19 Dec 2014 02:04:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4778 invoked by uid 99); 19 Dec 2014 02:04:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 02:04:17 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of saisai.shao@intel.com designates 192.55.52.88 as permitted sender)
Received: from [192.55.52.88] (HELO mga01.intel.com) (192.55.52.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 02:04:13 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga101.fm.intel.com with ESMTP; 18 Dec 2014 18:03:34 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,604,1413270000"; 
   d="scan'208";a="650234366"
Received: from pgsmsx104.gar.corp.intel.com ([10.221.44.91])
  by fmsmga002.fm.intel.com with ESMTP; 18 Dec 2014 18:03:33 -0800
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 PGSMSX104.gar.corp.intel.com (10.221.44.91) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Fri, 19 Dec 2014 10:03:31 +0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.182]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.216]) with mapi id 14.03.0195.001;
 Fri, 19 Dec 2014 10:03:30 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: =?utf-8?B?THVpcyDDgW5nZWwgVmljZW50ZSBTw6FuY2hleg==?=
	<langel.groups@gmail.com>, Cody Koeninger <cody@koeninger.org>
CC: Hari Shreedharan <hshreedharan@cloudera.com>, Patrick Wendell
	<pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Which committers care about Kafka?
Thread-Topic: Which committers care about Kafka?
Thread-Index: AQHQGtXAzrWADJNrIEmzg2LIwJg5n5yVMYEAgAAIvoCAAAuuAIAAEQIAgAAG9QCAAAFPgIAAx1Sw
Date: Fri, 19 Dec 2014 02:03:30 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
	<1418938019858.7377d5d7@Nodemailer>
	<CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
 <CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
In-Reply-To: <CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 02:23:30 2014
Return-Path: <dev-return-10859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B06BC3AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 02:23:30 +0000 (UTC)
Received: (qmail 36037 invoked by uid 500); 19 Dec 2014 02:23:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35959 invoked by uid 500); 19 Dec 2014 02:23:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35943 invoked by uid 99); 19 Dec 2014 02:23:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 02:23:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 02:23:01 +0000
Received: by mail-ob0-f176.google.com with SMTP id vb8so9863070obc.7
        for <dev@spark.apache.org>; Thu, 18 Dec 2014 18:23:00 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MGMaCosNp+5Xmfn2B+mHwkBQ1eZP0/gYUnMSIN7C2L0=;
        b=lXvlJO/zwPGnNi551e5XU+6vjWbLUkbpn0TYFgJ2zw5zYgwVOQe9CUC74iVhj/kEtw
         PNgm1VKnVRAj/owMsdqV+2tFdvDqMeTBkM5F/R+KTy84C3ImFThZHhZPBMrZQ5nFq+fN
         gLM7GbzzARAnl/FLlr059K3t18AGapkWrB4Th75XegLHJ7i03AwczdKa9+onAzIfI3yf
         vnBV+UJ2sAamKYsygu5BNxlsQly51q3fMJSukmkdjzMkInHOqjOID1oQxmoDn0VyYq12
         C761XztvwB7xSiDEWgKw9T4K4dUNtaJgZ4Se/t7JswRSXEQvnCS4SM4wC7nRzOW2GSHp
         K4nw==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr3159558oit.100.1418955780196;
 Thu, 18 Dec 2014 18:23:00 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Thu, 18 Dec 2014 18:23:00 -0800 (PST)
In-Reply-To: <CABPQxsvf+tatpfWEUk4JT=19m920gEOOGxJhgXgG7vuFC3pGDA@mail.gmail.com>
References: <CABPQxsvf+tatpfWEUk4JT=19m920gEOOGxJhgXgG7vuFC3pGDA@mail.gmail.com>
Date: Thu, 18 Dec 2014 18:23:00 -0800
Message-ID: <CABPQxsujRMYefDycYvGjENV3m+E0Ck76oEG+zFKrVxHFT1H-Kw@mail.gmail.com>
Subject: Re: [RESULT] [VOTE] Release Apache Spark 1.2.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Update: An Apache infrastructure issue prevented me from pushing this
last night. The issue was resolved today and I should be able to push
the final release artifacts tonight.

On Tue, Dec 16, 2014 at 9:20 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> This vote has PASSED with 12 +1 votes (8 binding) and no 0 or -1 votes:
>
> +1:
> Matei Zaharia*
> Madhu Siddalingaiah
> Reynold Xin*
> Sandy Ryza
> Josh Rozen*
> Mark Hamstra*
> Denny Lee
> Tom Graves*
> GuiQiang Li
> Nick Pentreath*
> Sean McNamara*
> Patrick Wendell*
>
> 0:
>
> -1:
>
> I'll finalize and package this release in the next 48 hours. Thanks to
> everyone who contributed.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 08:54:18 2014
Return-Path: <dev-return-10860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF3BCCF84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 08:54:18 +0000 (UTC)
Received: (qmail 39713 invoked by uid 500); 19 Dec 2014 08:54:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39617 invoked by uid 500); 19 Dec 2014 08:54:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38219 invoked by uid 99); 19 Dec 2014 08:54:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 08:54:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 08:54:09 +0000
Received: by mail-ob0-f173.google.com with SMTP id uy5so11589440obc.4;
        Fri, 19 Dec 2014 00:52:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=zQWhuRZvdkvAHFtaWI9V+dqkc/V0Y8s0UPsMom+R7xA=;
        b=yr+VEPGm/VvzeCysX9AF7uUHp+ipCA4ijwKjBinli2ElH4uJxdSW2UzrxXB1+gHJrJ
         S2HhvB9SojWYdNxLWbB2Tk4UX428AVIukvDbwKx/XhJupnMyopi2/d4C5oQminJaSvOQ
         Aul1c7neBZjW5ILqotr3iQ4Ifg4LodZMSkgGkiVSGWADK9NwKnt3wMXMJgc8otINmlko
         huT6uDkp2BiBlf0fjU8DkT9bDhlPYzft43n49sON+PSzQSsnNsoVOCN55nLdymJgx6/R
         clbG/wC/nYqZL/c5kSAU0ngYGraHVcSBVxR1RBLGzE9k9piIUyiSCpDuGtmwIaaROAWQ
         Hy7A==
MIME-Version: 1.0
X-Received: by 10.60.103.138 with SMTP id fw10mr4141994oeb.18.1418979139174;
 Fri, 19 Dec 2014 00:52:19 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Fri, 19 Dec 2014 00:52:19 -0800 (PST)
Date: Fri, 19 Dec 2014 00:52:19 -0800
Message-ID: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
Subject: Announcing Spark 1.2!
From: Patrick Wendell <pwendell@gmail.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
the third release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 172 developers and more
than 1,000 commits!

This release brings operational and performance improvements in Spark
core including a new network transport subsytem designed for very
large shuffles. Spark SQL introduces an API for external data sources
along with Hive 13 support, dynamic partitioning, and the
fixed-precision decimal type. MLlib adds a new pipeline-oriented
package (spark.ml) for composing multiple algorithms. Spark Streaming
adds a Python API and a write ahead log for fault tolerance. Finally,
GraphX has graduated from alpha and introduces a stable API along with
performance improvements.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone involved in creating, testing, and documenting this release!

[1] http://spark.apache.org/releases/spark-release-1-2-0.html
[2] http://spark.apache.org/downloads.html

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 09:45:26 2014
Return-Path: <dev-return-10861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CDE9C90F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 09:45:26 +0000 (UTC)
Received: (qmail 66941 invoked by uid 500); 19 Dec 2014 09:45:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66868 invoked by uid 500); 19 Dec 2014 09:45:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66851 invoked by uid 99); 19 Dec 2014 09:45:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:45:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dibyendu.bhattachary@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:44:58 +0000
Received: by mail-oi0-f50.google.com with SMTP id x69so913641oia.9
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 01:44:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=+jj9j9NKykZfkYkeOpZd/yJOyAh8LtXSmO20c0a+eQI=;
        b=DL/LIjhdkFvbKQg+FGIkNzcrMdu7mcrAl9YkfgSl+wfsIS+m45dhIZmbKUtziD9xkd
         YiYj5EpbVqrPxXUNgWw1lhV8/D8XLtG99nsV8a3SGdbakQxxZH/IHjfmqtjRUitq0ZIm
         DnKyDdwfE3ns3MEJGaFwVnx4mLA7hcwncEqxlZr/2k/rcUb1Lb44G9YO3F1d7SdcmkFK
         ecRmqZ96lnCJllfmK+rqvN3Yg74yuRjDxmRflbPmD0ZAIP/GWoudXzaWeYFiGyWilGE6
         PB6nLUpP6tlraHcr5P74u5zt88dtRJpswMKSM2gtW5xGx1CbeIecBNwwLjYpUeRh0Ltl
         vKrw==
MIME-Version: 1.0
X-Received: by 10.202.191.65 with SMTP id p62mr751780oif.38.1418982296866;
 Fri, 19 Dec 2014 01:44:56 -0800 (PST)
Received: by 10.76.109.12 with HTTP; Fri, 19 Dec 2014 01:44:56 -0800 (PST)
In-Reply-To: <64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
	<1418938019858.7377d5d7@Nodemailer>
	<CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
	<CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
	<64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com>
Date: Fri, 19 Dec 2014 15:14:56 +0530
Message-ID: <CAFiYKR-FA3DAH91d6GtmRfAOmYECPzYh3C3qrQBuR4RuC9vZkg@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
To: "Shao, Saisai" <saisai.shao@intel.com>
Cc: =?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Cody Koeninger <cody@koeninger.org>, Hari Shreedharan <hshreedharan@cloudera.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ddf6abf5601050a8e8f0e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ddf6abf5601050a8e8f0e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

Thanks to Jerry for mentioning the Kafka Spout for Trident. The Storm
Trident has done the exact-once guarantee by processing the tuple in a
batch  and assigning same transaction-id for a given batch . The replay for
a given batch with a transaction-id will have exact same set of tuples and
replay of batches happen in exact same order before the failure.

Having this paradigm, if downstream system process data for a given batch
for having a given transaction-id , and if during failure if same batch is
again emitted , you can check if same transaction-id is already processed
or not and hence can guarantee exact once semantics.

And this can only be achieved in Spark if we use Low Level Kafka consumer
API to process the offsets. This low level Kafka Consumer (
https://github.com/dibbhatt/kafka-spark-consumer) has implemented the Spark
Kafka consumer which uses Kafka Low Level APIs . All of the Kafka related
logic has been taken from Storm-Kafka spout and which manages all Kafka
re-balance and fault tolerant aspects and Kafka metadata managements.

Presently this Consumer maintains that during Receiver failure, it will
re-emit the exact same Block with same set of messages . Every message have
the details of its partition, offset and topic related details which can
tackle the SPARK-3146.

As this Low Level consumer has complete control over the Kafka Offsets , we
can implement Trident like feature on top of it like having implement a
transaction-id for a given block , and re-emit the same block with same set
of message during Driver failure.

Regards,
Dibyendu


On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.com> wrote=
:
>
> Hi all,
>
> I agree with Hari that Strong exact-once semantics is very hard to
> guarantee, especially in the failure situation. From my understanding eve=
n
> current implementation of ReliableKafkaReceiver cannot fully guarantee th=
e
> exact once semantics once failed, first is the ordering of data replaying
> from last checkpoint, this is hard to guarantee when multiple partitions
> are injected in; second is the design complexity of achieving this, you c=
an
> refer to the Kafka Spout in Trident, we have to dig into the very details
> of Kafka metadata management system to achieve this, not to say rebalance
> and fault-tolerance.
>
> Thanks
> Jerry
>
> -----Original Message-----
> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups@gmail.co=
m]
> Sent: Friday, December 19, 2014 5:57 AM
> To: Cody Koeninger
> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
> Subject: Re: Which committers care about Kafka?
>
> But idempotency is not that easy t achieve sometimes. A strong only once
> semantic through a proper API would  be superuseful; but I'm not implying
> this is easy to achieve.
> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote:
>
> > If the downstream store for the output data is idempotent or
> > transactional, and that downstream store also is the system of record
> > for kafka offsets, then you have exactly-once semantics.  Commit
> > offsets with / after the data is stored.  On any failure, restart from
> the last committed offsets.
> >
> > Yes, this approach is biased towards the etl-like use cases rather
> > than near-realtime-analytics use cases.
> >
> > On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> > hshreedharan@cloudera.com
> > > wrote:
> > >
> > > I get what you are saying. But getting exactly once right is an
> > > extremely hard problem - especially in presence of failure. The
> > > issue is failures
> > can
> > > happen in a bunch of places. For example, before the notification of
> > > downstream store being successful reaches the receiver that updates
> > > the offsets, the node fails. The store was successful, but
> > > duplicates came in either way. This is something worth discussing by
> > > itself - but without uuids etc this might not really be solved even
> when you think it is.
> > >
> > > Anyway, I will look at the links. Even I am interested in all of the
> > > features you mentioned - no HDFS WAL for Kafka and once-only
> > > delivery,
> > but
> > > I doubt the latter is really possible to guarantee - though I really
> > would
> > > love to have that!
> > >
> > > Thanks,
> > > Hari
> > >
> > >
> > > On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
> > > <cody@koeninger.org>
> > > wrote:
> > >
> > >> Thanks for the replies.
> > >>
> > >> Regarding skipping WAL, it's not just about optimization.  If you
> > >> actually want exactly-once semantics, you need control of kafka
> > >> offsets
> > as
> > >> well, including the ability to not use zookeeper as the system of
> > >> record for offsets.  Kafka already is a reliable system that has
> > >> strong
> > ordering
> > >> guarantees (within a partition) and does not mandate the use of
> > zookeeper
> > >> to store offsets.  I think there should be a spark api that acts as
> > >> a
> > very
> > >> simple intermediary between Kafka and the user's choice of
> > >> downstream
> > store.
> > >>
> > >> Take a look at the links I posted - if there's already been 2
> > independent
> > >> implementations of the idea, chances are it's something people need.
> > >>
> > >> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> > >> hshreedharan@cloudera.com> wrote:
> > >>>
> > >>> Hi Cody,
> > >>>
> > >>> I am an absolute +1 on SPARK-3146. I think we can implement
> > >>> something pretty simple and lightweight for that one.
> > >>>
> > >>> For the Kafka DStream skipping the WAL implementation - this is
> > >>> something I discussed with TD a few weeks ago. Though it is a good
> > idea to
> > >>> implement this to avoid unnecessary HDFS writes, it is an
> > optimization. For
> > >>> that reason, we must be careful in implementation. There are a
> > >>> couple
> > of
> > >>> issues that we need to ensure works properly - specifically orderin=
g.
> > To
> > >>> ensure we pull messages from different topics and partitions in
> > >>> the
> > same
> > >>> order after failure, we=E2=80=99d still have to persist the metadat=
a to
> > >>> HDFS
> > (or
> > >>> some other system) - this metadata must contain the order of
> > >>> messages consumed, so we know how to re-read the messages. I am
> > >>> planning to
> > explore
> > >>> this once I have some time (probably in Jan). In addition, we must
> > >>> also ensure bucketing functions work fine as well. I will file a
> > >>> placeholder jira for this one.
> > >>>
> > >>> I also wrote an API to write data back to Kafka a while back -
> > >>> https://github.com/apache/spark/pull/2994 . I am hoping that this
> > >>> will get pulled in soon, as this is something I know people want.
> > >>> I am open
> > to
> > >>> feedback on that - anything that I can do to make it better.
> > >>>
> > >>> Thanks,
> > >>> Hari
> > >>>
> > >>>
> > >>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
> > >>> <pwendell@gmail.com>
> > >>> wrote:
> > >>>
> > >>>>  Hey Cody,
> > >>>>
> > >>>> Thanks for reaching out with this. The lead on streaming is TD -
> > >>>> he is traveling this week though so I can respond a bit. To the
> > >>>> high level point of whether Kafka is important - it definitely
> > >>>> is. Something like 80% of Spark Streaming deployments
> > >>>> (anecdotally) ingest data from Kafka. Also, good support for
> > >>>> Kafka is something we generally want in Spark and not a library.
> > >>>> In some cases IIRC there were user libraries that used unstable
> > >>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
> > >>>> them to merge things upstream. Otherwise users wouldn't be able
> > >>>> to use newer Kakfa versions. This is a high level impression only
> > >>>> though, I haven't talked to TD about this recently so it's worth
> revisiting given the developments in Kafka.
> > >>>>
> > >>>> Please do bring things up like this on the dev list if there are
> > >>>> blockers for your usage - thanks for pinging it.
> > >>>>
> > >>>> - Patrick
> > >>>>
> > >>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
> > >>>> <cody@koeninger.org>
> > >>>> wrote:
> > >>>> > Now that 1.2 is finalized... who are the go-to people to get
> > >>>> > some long-standing Kafka related issues resolved?
> > >>>> >
> > >>>> > The existing api is not sufficiently safe nor flexible for our
> > >>>> production
> > >>>> > use. I don't think we're alone in this viewpoint, because I've
> > >>>> > seen several different patches and libraries to fix the same
> > >>>> > things we've
> > >>>> been
> > >>>> > running into.
> > >>>> >
> > >>>> > Regarding flexibility
> > >>>> >
> > >>>> > https://issues.apache.org/jira/browse/SPARK-3146
> > >>>> >
> > >>>> > has been outstanding since August, and IMHO an equivalent of
> > >>>> > this is absolutely necessary. We wrote a similar patch
> > >>>> > ourselves, then found
> > >>>> that
> > >>>> > PR and have been running it in production. We wouldn't be able
> > >>>> > to
> > get
> > >>>> our
> > >>>> > jobs done without it. It also allows users to solve a whole
> > >>>> > class of problems for themselves (e.g. SPARK-2388, arbitrary
> > >>>> > delay of
> > >>>> messages, etc).
> > >>>> >
> > >>>> > Regarding safety, I understand the motivation behind
> > >>>> > WriteAheadLog
> > as
> > >>>> a
> > >>>> > general solution for streaming unreliable sources, but Kafka
> > >>>> > already
> > >>>> is a
> > >>>> > reliable source. I think there's a need for an api that treats
> > >>>> > it as such. Even aside from the performance issues of
> > >>>> > duplicating the write-ahead log in kafka into another
> > >>>> > write-ahead log in hdfs, I
> > need
> > >>>> > exactly-once semantics in the face of failure (I've had
> > >>>> > failures
> > that
> > >>>> > prevented reloading a spark streaming checkpoint, for instance).
> > >>>> >
> > >>>> > I've got an implementation i've been using
> > >>>> >
> > >>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> > >>>> > ka /src/main/scala/org/apache/spark/rdd/kafka
> > >>>> >
> > >>>> > Tresata has something similar at
> > >>>> https://github.com/tresata/spark-kafka,
> > >>>> > and I know there were earlier attempts based on Storm code.
> > >>>> >
> > >>>> > Trying to distribute these kinds of fixes as libraries rather
> > >>>> > than
> > >>>> patches
> > >>>> > to Spark is problematic, because large portions of the
> > implementation
> > >>>> are
> > >>>> > private[spark].
> > >>>> >
> > >>>> > I'd like to help, but i need to know whose attention to get.
> > >>>>
> > >>>> -----------------------------------------------------------------
> > >>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
> > >>>> additional commands, e-mail: dev-help@spark.apache.org
> > >>>>
> > >>>>
> > >>>
> > >
> >
>

--001a113ddf6abf5601050a8e8f0e--

From dev-return-10862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 09:52:13 2014
Return-Path: <dev-return-10862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 878CA9118
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 09:52:13 +0000 (UTC)
Received: (qmail 89056 invoked by uid 500); 19 Dec 2014 09:52:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88957 invoked by uid 500); 19 Dec 2014 09:52:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87895 invoked by uid 99); 19 Dec 2014 09:52:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:52:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zsxwing@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:51:59 +0000
Received: by mail-ig0-f177.google.com with SMTP id z20so366040igj.10;
        Fri, 19 Dec 2014 01:50:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rz8yTMeM/zdOoW256QKiG6ouwNUfIoh6Gj1fO9xJOQI=;
        b=RhVrz4wFmRHMhSRrTqcQInnsdKO+01cENajocy10RFZmJglTyBcToQ4gqmylQu+q/N
         XtjPAIeK3Qi03iWp8vHwSWNXxPXxV23hiU032XW6TdIteMXvRPWtg03pMW1SlcSKsYv8
         MBfsxrkEBremAoxoquefzXOrU+GxvsrrogkPhLsi2m/AXCK1fGt9fCr1z6RjZQ1R0XlC
         xhUV3ZDGUHDH2ZCMUUo9e56giY4+AJNGI911jJaBO43jUfYoQ19UDFHK15iJCu1ugCtU
         zHVTC/O77usTylSpXnRoDFeaVGMnRDIhDjVAUChsQ22HyjoPngcUTuK+yZp9egomVG9p
         /v3Q==
MIME-Version: 1.0
X-Received: by 10.107.132.78 with SMTP id g75mr6338260iod.21.1418982608601;
 Fri, 19 Dec 2014 01:50:08 -0800 (PST)
Received: by 10.107.165.203 with HTTP; Fri, 19 Dec 2014 01:50:08 -0800 (PST)
In-Reply-To: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
Date: Fri, 19 Dec 2014 17:50:08 +0800
Message-ID: <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
Subject: Re: Announcing Spark 1.2!
From: Shixiong Zhu <zsxwing@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fb6b25407c5050a8ea2fa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fb6b25407c5050a8ea2fa
Content-Type: text/plain; charset=UTF-8

Congrats!

A little question about this release: Which commit is this release based
on? v1.2.0 and v1.2.0-rc2 are pointed to different commits in
https://github.com/apache/spark/releases

Best Regards,
Shixiong Zhu

2014-12-19 16:52 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
>
> I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
> the third release on the API-compatible 1.X line. It is Spark's
> largest release ever, with contributions from 172 developers and more
> than 1,000 commits!
>
> This release brings operational and performance improvements in Spark
> core including a new network transport subsytem designed for very
> large shuffles. Spark SQL introduces an API for external data sources
> along with Hive 13 support, dynamic partitioning, and the
> fixed-precision decimal type. MLlib adds a new pipeline-oriented
> package (spark.ml) for composing multiple algorithms. Spark Streaming
> adds a Python API and a write ahead log for fault tolerance. Finally,
> GraphX has graduated from alpha and introduces a stable API along with
> performance improvements.
>
> Visit the release notes [1] to read about the new features, or
> download [2] the release today.
>
> For errata in the contributions or release notes, please e-mail me
> *directly* (not on-list).
>
> Thanks to everyone involved in creating, testing, and documenting this
> release!
>
> [1] http://spark.apache.org/releases/spark-release-1-2-0.html
> [2] http://spark.apache.org/downloads.html
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> For additional commands, e-mail: user-help@spark.apache.org
>
>

--001a113fb6b25407c5050a8ea2fa--

From dev-return-10863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 09:56:17 2014
Return-Path: <dev-return-10863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C64E9128
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 09:56:17 +0000 (UTC)
Received: (qmail 2137 invoked by uid 500); 19 Dec 2014 09:56:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2063 invoked by uid 500); 19 Dec 2014 09:56:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2052 invoked by uid 99); 19 Dec 2014 09:56:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:56:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 09:56:11 +0000
Received: by mail-wi0-f181.google.com with SMTP id r20so1150082wiv.14
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 01:55:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ImOzuqzkC3XGSaEg7WUrn+vc2oiCd/8BCFZJ4Pumoyw=;
        b=jzso8Nbi8JnRsogtfnGUF7iJJXFYAaXOw3427RnaO0jB3MS7rq8MnG5ljp9r52ddKn
         hdyAa9GAVHM+B79pgxLQDNeNFs+ncwgnyabMx81x3Dbtr8ac7eKCI9bLkmsRjXFT3J4V
         a0JwnWuvsiys5y0/V3tIJBRxoAKwwY/yCZxPU0rRbjXRe4VMkCnHz/4lqJhezB11Ry5P
         d8KZQKIyRk0kPyEkC0uQjotjvf2XUVaAalIRfukn6Tv5XQqKq460lY7wLMMs4yo9O3wE
         fRmTTgTywzpOSdQ+dK8NDNwuTyCmKmEutmuGVFH54YElCC+/PI7iuVMUWT9r3JuW62kb
         T4Vw==
X-Gm-Message-State: ALoCoQm9gm7dn3muswgPBt00m0IRc2VxcbnKh+hgTbTeV/hEPS2XUSGxryM59CjygsRZ//iMGFzF
X-Received: by 10.180.8.70 with SMTP id p6mr4215023wia.72.1418982950141; Fri,
 19 Dec 2014 01:55:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 19 Dec 2014 01:55:29 -0800 (PST)
In-Reply-To: <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
 <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 19 Dec 2014 09:55:29 +0000
Message-ID: <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com>
Subject: Re: Announcing Spark 1.2!
To: Shixiong Zhu <zsxwing@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get
updated. I assume it's going to be 1.2.0-rc2 plus a few commits
related to the release process.

On Fri, Dec 19, 2014 at 9:50 AM, Shixiong Zhu <zsxwing@gmail.com> wrote:
> Congrats!
>
> A little question about this release: Which commit is this release based on?
> v1.2.0 and v1.2.0-rc2 are pointed to different commits in
> https://github.com/apache/spark/releases
>
> Best Regards,
>
> Shixiong Zhu
>
> 2014-12-19 16:52 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
>>
>> I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
>> the third release on the API-compatible 1.X line. It is Spark's
>> largest release ever, with contributions from 172 developers and more
>> than 1,000 commits!
>>
>> This release brings operational and performance improvements in Spark
>> core including a new network transport subsytem designed for very
>> large shuffles. Spark SQL introduces an API for external data sources
>> along with Hive 13 support, dynamic partitioning, and the
>> fixed-precision decimal type. MLlib adds a new pipeline-oriented
>> package (spark.ml) for composing multiple algorithms. Spark Streaming
>> adds a Python API and a write ahead log for fault tolerance. Finally,
>> GraphX has graduated from alpha and introduces a stable API along with
>> performance improvements.
>>
>> Visit the release notes [1] to read about the new features, or
>> download [2] the release today.
>>
>> For errata in the contributions or release notes, please e-mail me
>> *directly* (not on-list).
>>
>> Thanks to everyone involved in creating, testing, and documenting this
>> release!
>>
>> [1] http://spark.apache.org/releases/spark-release-1-2-0.html
>> [2] http://spark.apache.org/downloads.html
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 11:17:31 2014
Return-Path: <dev-return-10864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24B259626
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 11:17:31 +0000 (UTC)
Received: (qmail 12822 invoked by uid 500); 19 Dec 2014 11:17:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12746 invoked by uid 500); 19 Dec 2014 11:17:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12733 invoked by uid 99); 19 Dec 2014 11:17:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 11:17:28 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wyphao.2007@163.com designates 220.181.13.71 as permitted sender)
Received: from [220.181.13.71] (HELO m13-71.163.com) (220.181.13.71)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 11:17:03 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=Gt4U0
	+ugi3h1X+QC+UirqG4cbXPv8lRzmSp2CJ3O5rU=; b=VEgULy6jipQFNVzTACAx9
	cQ0JawDo0BjmDnPdE9Bw9eRQOBP05vlgpnsZk0u/4Hd3VyyoNR2xXZOyAXv795r8
	UEgB0iZeVtk45BVe9Pw3fYe7LP9Bp49rq6XtaXUG+HCpoSam0yyq1l6b7sArVBkA
	PRN25bHOianQjc+/P2yMQ8=
Received: from wyphao.2007$163.com ( [211.151.238.52] ) by
 ajax-webmail-wmsvr71 (Coremail) ; Fri, 19 Dec 2014 19:16:57 +0800 (CST)
X-Originating-IP: [211.151.238.52]
Date: Fri, 19 Dec 2014 19:16:57 +0800 (CST)
From: "wyphao.2007" <wyphao.2007@163.com>
To: dev@spark.apache.org
Subject: Re:Re: Announcing Spark 1.2!
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20140915(28949.6690) Copyright (c) 2002-2014 www.mailtech.cn 163com
In-Reply-To: <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
 <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
 <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com>
X-CM-CTRLDATA: ongutmZvb3Rlcl9odG09NTE0Njo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_388905_969390108.1418987817587"
MIME-Version: 1.0
Message-ID: <53bc55e2.192f.14a6243ca74.Coremail.wyphao.2007@163.com>
X-CM-TRANSID:R8GowABXXxcqCZRUkYQlAA--.1015W
X-CM-SenderInfo: xz1sxtbrosiiqx6rljoofrz/1tbiXRVnKFEALYiU8QABsX
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_388905_969390108.1418987817587
Content-Type: text/plain; charset=GBK
Content-Transfer-Encoding: base64
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------=_Part_388905_969390108.1418987817587--


From dev-return-10865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 11:21:51 2014
Return-Path: <dev-return-10865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4CA3963C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 11:21:51 +0000 (UTC)
Received: (qmail 26033 invoked by uid 500); 19 Dec 2014 11:21:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25959 invoked by uid 500); 19 Dec 2014 11:21:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25948 invoked by uid 99); 19 Dec 2014 11:21:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 11:21:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 11:21:23 +0000
Received: by mail-wi0-f171.google.com with SMTP id bs8so1549342wib.10
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 03:19:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=bl71ddcUoJuVeP1ssLwmU0p6Pk5dfyPIy/RyIbQDlaE=;
        b=liTviYHI0jxgAhmeHehd0l2HsElOScrWTuCEO4fqkpdwBReXMYTuhIDBka1ejvKlzt
         YL8KNPlKkep8Hnl4EO+a4BYiqNrKIGqJP0yvxZIu4oQk/riIlHN3o5P9kgzoTEgJL190
         Gi654UZREwNTQUEtMBtQCIsKtFeI27W9ZqJTWoe9Wf0vzQxwC6bCbcuO7Nur88kc+17u
         /TnJdDR+lsXgxDx3B5ey9KkuhKY1c+9swDtmrKsqNfbxuCbXwZu3A7B3ihSWsSl0/J/r
         NTUjGecHkAbSSicb22MsoVO1fPKpDDNxhgo3pR0Xq/zz/mSjVts9lDUjKjD4FLndXY3U
         tXRA==
X-Gm-Message-State: ALoCoQlbunDllNcsLdAOMBi3NQNF/6GXJWYhCA4rdfJViu2bVx26fDh8dJlR2senhnvrDouOyTWT
X-Received: by 10.180.96.33 with SMTP id dp1mr4827517wib.13.1418987993042;
 Fri, 19 Dec 2014 03:19:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 19 Dec 2014 03:19:32 -0800 (PST)
In-Reply-To: <53bc55e2.192f.14a6243ca74.Coremail.wyphao.2007@163.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
 <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
 <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com> <53bc55e2.192f.14a6243ca74.Coremail.wyphao.2007@163.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 19 Dec 2014 11:19:32 +0000
Message-ID: <CAMAsSdJsycredriRKYaXvtQXmL-SiK8Az02veTgqycCedR67EA@mail.gmail.com>
Subject: Re: Re: Announcing Spark 1.2!
To: "wyphao.2007" <wyphao.2007@163.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I can download it. Make sure you refresh the page, maybe, so that it
shows the 1.2.0 download as an option.

On Fri, Dec 19, 2014 at 11:16 AM, wyphao.2007 <wyphao.2007@163.com> wrote:
>
>
> In the http://spark.apache.org/downloads.html page,We cann't download the newest Spark release.
>
>
>
>
>
>
> At 2014-12-19 17:55:29,"Sean Owen" <sowen@cloudera.com> wrote:
>>Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get
>>updated. I assume it's going to be 1.2.0-rc2 plus a few commits
>>related to the release process.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 17:11:46 2014
Return-Path: <dev-return-10866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10D43104BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 17:11:46 +0000 (UTC)
Received: (qmail 31867 invoked by uid 500); 19 Dec 2014 17:11:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31792 invoked by uid 500); 19 Dec 2014 17:11:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31202 invoked by uid 99); 19 Dec 2014 17:11:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 17:11:43 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [50.205.35.100] (HELO hera.ccri.com) (50.205.35.100)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 17:11:38 +0000
Received: from [192.168.2.67]
	by hera.ccri.com with esmtpsa (TLSv1:DHE-RSA-AES128-SHA:128)
	(Exim 4.80.1)
	(envelope-from <mcwhorter@ccri.com>)
	id 1Y215C-0002wS-8Z
	for dev@spark.apache.org; Fri, 19 Dec 2014 12:11:18 -0500
Message-ID: <54945BD9.7050109@ccri.com>
Date: Fri, 19 Dec 2014 12:09:45 -0500
From: David McWhorter <mcwhorter@ccri.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.2.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: spark-yarn_2.10 1.2.0 artifacts
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

Thanks for your work on spark!  I am trying to locate spark-yarn jars 
for the new 1.2.0 release.  The jars for spark-core, etc, are on maven 
central, but the spark-yarn jars are missing.

Confusingly and perhaps relatedly, I also can't seem to get the 
spark-yarn artifact to install on my local computer when I run 'mvn 
-Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean install'.  
At the install plugin stage, maven reports:

[INFO] --- maven-install-plugin:2.5.1:install (default-install) @ 
spark-yarn_2.10 ---
[INFO] Skipping artifact installation

Any help or insights into how to use spark-yarn_2.10 1.2.0 in a maven 
build would be appreciated.

David

-- 

David McWhorter
Software Engineer
Commonwealth Computer Research, Inc.
1422 Sachem Place, Unit #1
Charlottesville, VA 22901
mcwhorter@ccri.com | 434.299.0090x204


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 17:15:41 2014
Return-Path: <dev-return-10867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73E34104C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 17:15:41 +0000 (UTC)
Received: (qmail 36545 invoked by uid 500); 19 Dec 2014 17:15:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36473 invoked by uid 500); 19 Dec 2014 17:15:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36462 invoked by uid 99); 19 Dec 2014 17:15:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 17:15:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 17:15:14 +0000
Received: by mail-wi0-f170.google.com with SMTP id bs8so5354875wib.1
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 09:13:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CK3UAAkRbLKEljhnoaGKDzlslb+UE1SqNmUfdZE4uac=;
        b=ZxV7FuN18bDtHHWLdMEU6++YmEaLeTO0+O5Th8sWLbQ7jbeEvfCYuAEF3zcRXIiFFJ
         ULT7Xy5S2v0R5O8zZLj8cNY0U8VrGpMK5qyLvCM3wy4xMb1LlQc3vj4HBN+2YF8c/vOY
         6pN6xfggjHKJfHLxhIfPZI/hx6wz14wHCSnD7GwR2uGwZyko2qMLSw/TlorfFxXuLDu5
         b8Jg4tu3MGaSCkKhPIkWZt9gBi5zYyT/hYN6Ewwa/4yAemfa38I4d59YsyTt9mY3OsEL
         F+JGKU+DfFS/Jt1UgoNStafudJWUOzI5q1IAqkCQII1AZd/zrQdik48jAb0u2T6nL/Qj
         TY4A==
X-Gm-Message-State: ALoCoQmNXaRch32JeFMRSuo62FAP5N14p0FHnNoLzeY5WmaI2Uvo1KwFCgPOt8Ssfu4jC0R9yOwz
X-Received: by 10.194.80.193 with SMTP id t1mr16736530wjx.8.1419009218363;
 Fri, 19 Dec 2014 09:13:38 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Fri, 19 Dec 2014 09:13:18 -0800 (PST)
In-Reply-To: <54945BD9.7050109@ccri.com>
References: <54945BD9.7050109@ccri.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 19 Dec 2014 17:13:18 +0000
Message-ID: <CAMAsSdKfJYRTgY2j=Th0KCMG_DqPvxeTxyudVht4=hpYiRmXgg@mail.gmail.com>
Subject: Re: spark-yarn_2.10 1.2.0 artifacts
To: David McWhorter <mcwhorter@ccri.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I believe spark-yarn does not exist from 1.2 onwards. Have a look at
spark-network-yarn for where some of that went, I believe.

On Fri, Dec 19, 2014 at 5:09 PM, David McWhorter <mcwhorter@ccri.com> wrote:
> Hi all,
>
> Thanks for your work on spark!  I am trying to locate spark-yarn jars for
> the new 1.2.0 release.  The jars for spark-core, etc, are on maven central,
> but the spark-yarn jars are missing.
>
> Confusingly and perhaps relatedly, I also can't seem to get the spark-yarn
> artifact to install on my local computer when I run 'mvn -Pyarn -Phadoop-2.2
> -Dhadoop.version=2.2.0 -DskipTests clean install'.  At the install plugin
> stage, maven reports:
>
> [INFO] --- maven-install-plugin:2.5.1:install (default-install) @
> spark-yarn_2.10 ---
> [INFO] Skipping artifact installation
>
> Any help or insights into how to use spark-yarn_2.10 1.2.0 in a maven build
> would be appreciated.
>
> David
>
> --
>
> David McWhorter
> Software Engineer
> Commonwealth Computer Research, Inc.
> 1422 Sachem Place, Unit #1
> Charlottesville, VA 22901
> mcwhorter@ccri.com | 434.299.0090x204
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 18:53:34 2014
Return-Path: <dev-return-10868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 08E22108FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 18:53:34 +0000 (UTC)
Received: (qmail 84314 invoked by uid 500); 19 Dec 2014 18:53:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84197 invoked by uid 500); 19 Dec 2014 18:53:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83169 invoked by uid 99); 19 Dec 2014 18:53:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 18:53:30 +0000
X-ASF-Spam-Status: No, hits=0.4 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,TRACKER_ID
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 18:53:26 +0000
Received: by mail-oi0-f53.google.com with SMTP id g201so2811821oib.12;
        Fri, 19 Dec 2014 10:50:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=c6xTGh0SbIIPWvziips5Ozmax/Zwa3svFkGnRHlddKo=;
        b=hvhAkrbduj9aMXNfwM0LMLBNV3DQuMIZy2geZgkL9IKM+97sLqigO0Ya2C1epM+pxj
         MvFDO8dNPysY0GnfhSNYstUBro/jUl4BzQKNYDl5QS/0/3uKW2azrPuJUCqkEnnh8rhS
         55XsUTaQYYX49SEcvqAVZQ9OcybeTcPvIcEvAPw6GmuNp0ry+GpmF5LHeyF7PwtKdSW0
         4vZvWjvqGV6/LCZ82rCMfvbD6/tfEjRaAXuons8ieMoQiJYLSTH3cAZ9oyHMK0kzsCXJ
         FOWoy16B1hqFgYQR1VGk8Ernj+0/MQ/EccncvQlcLshrQLZIgEGoTmlO7xKZl3fdoTPa
         +YLA==
MIME-Version: 1.0
X-Received: by 10.60.50.137 with SMTP id c9mr5664047oeo.83.1419015050890; Fri,
 19 Dec 2014 10:50:50 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Fri, 19 Dec 2014 10:50:50 -0800 (PST)
In-Reply-To: <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com>
	<CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com>
	<CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com>
Date: Fri, 19 Dec 2014 10:50:50 -0800
Message-ID: <CABPQxssUOU8uJ_NUeKFdAhrfOCAbWQubt+0f71S4=eD_YEKJMg@mail.gmail.com>
Subject: Re: Announcing Spark 1.2!
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Shixiong Zhu <zsxwing@gmail.com>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for pointing out the tag issue. I've updated all links to point
to the correct tag (from the vote thread):

a428c446e23e628b746e0626cc02b7b3cadf588e

On Fri, Dec 19, 2014 at 1:55 AM, Sean Owen <sowen@cloudera.com> wrote:
> Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get
> updated. I assume it's going to be 1.2.0-rc2 plus a few commits
> related to the release process.
>
> On Fri, Dec 19, 2014 at 9:50 AM, Shixiong Zhu <zsxwing@gmail.com> wrote:
>> Congrats!
>>
>> A little question about this release: Which commit is this release based on?
>> v1.2.0 and v1.2.0-rc2 are pointed to different commits in
>> https://github.com/apache/spark/releases
>>
>> Best Regards,
>>
>> Shixiong Zhu
>>
>> 2014-12-19 16:52 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
>>>
>>> I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
>>> the third release on the API-compatible 1.X line. It is Spark's
>>> largest release ever, with contributions from 172 developers and more
>>> than 1,000 commits!
>>>
>>> This release brings operational and performance improvements in Spark
>>> core including a new network transport subsytem designed for very
>>> large shuffles. Spark SQL introduces an API for external data sources
>>> along with Hive 13 support, dynamic partitioning, and the
>>> fixed-precision decimal type. MLlib adds a new pipeline-oriented
>>> package (spark.ml) for composing multiple algorithms. Spark Streaming
>>> adds a Python API and a write ahead log for fault tolerance. Finally,
>>> GraphX has graduated from alpha and introduces a stable API along with
>>> performance improvements.
>>>
>>> Visit the release notes [1] to read about the new features, or
>>> download [2] the release today.
>>>
>>> For errata in the contributions or release notes, please e-mail me
>>> *directly* (not on-list).
>>>
>>> Thanks to everyone involved in creating, testing, and documenting this
>>> release!
>>>
>>> [1] http://spark.apache.org/releases/spark-release-1-2-0.html
>>> [2] http://spark.apache.org/downloads.html
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: user-help@spark.apache.org
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 18:57:42 2014
Return-Path: <dev-return-10869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ACB8B10928
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 18:57:42 +0000 (UTC)
Received: (qmail 3001 invoked by uid 500); 19 Dec 2014 18:57:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2927 invoked by uid 500); 19 Dec 2014 18:57:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 81334 invoked by uid 99); 19 Dec 2014 18:52:40 -0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: encountered temporary error during SPF processing of domain of tiong@ooyala.com)
Date: Fri, 19 Dec 2014 11:51:53 -0700 (MST)
From: thlee <tiong@ooyala.com>
To: dev@spark.apache.org
Message-ID: <1419015113315-9855.post@n3.nabble.com>
In-Reply-To: <1418686647469-9798.post@n3.nabble.com>
References: <1418686647469-9798.post@n3.nabble.com>
Subject: Re: Confirming race condition in DagScheduler
 (NoSuchElementException)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

any comments?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Confirming-race-condition-in-DagScheduler-NoSuchElementException-tp9798p9855.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 18:58:34 2014
Return-Path: <dev-return-10870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F0B7810932
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 18:58:33 +0000 (UTC)
Received: (qmail 6653 invoked by uid 500); 19 Dec 2014 18:58:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6570 invoked by uid 500); 19 Dec 2014 18:58:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6559 invoked by uid 99); 19 Dec 2014 18:58:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 18:58:31 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 18:58:05 +0000
Received: from EX02-WEST.YOJOE.local ([169.254.1.145]) by
 EX03-WEST.YOJOE.local ([169.254.2.196]) with mapi id 14.03.0195.001; Fri, 19
 Dec 2014 10:57:42 -0800
From: Mingyu Kim <mkim@palantir.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: Matt Cheah <mcheah@palantir.com>
Subject: Spark master OOMs with exception stack trace stored in
 JobProgressListener (SPARK-4906)
Thread-Topic: Spark master OOMs with exception stack trace stored in
 JobProgressListener (SPARK-4906)
Thread-Index: AQHQG72quZTt66PgHEOJMVhWrjk5Fg==
Date: Fri, 19 Dec 2014 18:57:41 +0000
Message-ID: <D0B9B524.176F1%mkim@palantir.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.150.25.165]
Content-Type: multipart/alternative;
	boundary="_000_D0B9B524176F1mkimpalantircom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D0B9B524176F1mkimpalantircom_
Content-Type: text/plain; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi,

I just filed a bug SPARK-4906<https://issues.apache.org/jira/browse/SPARK-4=
906>, regarding Spark master OOMs. If I understand correctly, the UI states=
 for all running applications are kept in memory retained by JobProgressLis=
tener, and when there are a lot of exception stack traces, this UI states c=
an take up a significant amount of heap. This seems very bad especially for=
 long-running applications.

Can you correct me if I=92m misunderstanding anything? If my understanding =
is correct, is there any work being done to make sure the UI states don=92t=
 grow indefinitely over time? Would it make sense to spill some states to d=
isk or work with what spark.eventLog is doing so Spark master doesn=92t nee=
d to keep things in memory?

Thanks,
Mingyu

--_000_D0B9B524176F1mkimpalantircom_--

From dev-return-10871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 19:06:55 2014
Return-Path: <dev-return-10871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF2AD1099D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 19:06:55 +0000 (UTC)
Received: (qmail 33325 invoked by uid 500); 19 Dec 2014 19:06:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33240 invoked by uid 500); 19 Dec 2014 19:06:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33229 invoked by uid 99); 19 Dec 2014 19:06:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:06:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.223.176 as permitted sender)
Received: from [209.85.223.176] (HELO mail-ie0-f176.google.com) (209.85.223.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:06:50 +0000
Received: by mail-ie0-f176.google.com with SMTP id tr6so1293166ieb.35
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 11:06:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=PBKkYb5Ax8Bse2gmXJTG7LJo3twFi80NGMcy5be40E4=;
        b=UZFo22mq3prgrCQDlOegfOLqrKbohll2eRZE3UxBUGtSFqK6TsIR4QvnJLrxZBVVHL
         q0wQpIocNcDS8MtQCJXibCSx+Nej4q0bi98bSfDueDSNZFu74UETlRgI0NuVrH0ybSU1
         egmdjcjAK9Ab0b/NjA5mL9fLIcQ2W5n0+l9FCJEXm5SSaD/dcbJM4WKzU0jkQAmWOazy
         yz+OvevXZGr2tHmbs6AFyOcWePM0p9UaSvi2LnJCd2jYMoKA8l1iEfBMGJlP2oxqnGlW
         ThX88SoNd7PMdNQ4sX/UxrRPUwj99eAf3kTO0s8xww4ZeetxV5a6239vbBqctBTHbt9m
         v1Gg==
X-Gm-Message-State: ALoCoQkgiAdZSDa0u6rITygyKy9O0Bi0wc8fp4etAffHd4HcPw4Udj+enJkOFBofkkVXU5PGPkmE
X-Received: by 10.50.153.109 with SMTP id vf13mr4626719igb.41.1419015989805;
 Fri, 19 Dec 2014 11:06:29 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.128.194 with HTTP; Fri, 19 Dec 2014 11:06:09 -0800 (PST)
In-Reply-To: <CAFiYKR-FA3DAH91d6GtmRfAOmYECPzYh3C3qrQBuR4RuC9vZkg@mail.gmail.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
 <1418938019858.7377d5d7@Nodemailer> <CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
 <CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
 <64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com> <CAFiYKR-FA3DAH91d6GtmRfAOmYECPzYh3C3qrQBuR4RuC9vZkg@mail.gmail.com>
From: Hari Shreedharan <hshreedharan@cloudera.com>
Date: Fri, 19 Dec 2014 11:06:09 -0800
Message-ID: <CAHbPYVZwhnNB_xO+WLx6xaiNds=pgJtUh6tobc=tTW=O6Xa+ww@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
To: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>
Cc: "Shao, Saisai" <saisai.shao@intel.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Cody Koeninger <cody@koeninger.org>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0153667200db00050a966832
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0153667200db00050a966832
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Dibyendu,

Thanks for the details on the implementation. But I still do not believe
that it is no duplicates - what they achieve is that the same batch is
processed exactly the same way every time (but see it may be processed more
than once) - so it depends on the operation being idempotent. I believe
Trident uses ZK to keep track of the transactions - a batch can be
processed multiple times in failure scenarios (for example, the transaction
is processed but before ZK is updated the machine fails, causing a "new"
node to process it again).

I don't think it is impossible to do this in Spark Streaming as well and
I'd be really interested in working on it at some point in the near future.

On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
dibyendu.bhattachary@gmail.com> wrote:

> Hi,
>
> Thanks to Jerry for mentioning the Kafka Spout for Trident. The Storm
> Trident has done the exact-once guarantee by processing the tuple in a
> batch  and assigning same transaction-id for a given batch . The replay f=
or
> a given batch with a transaction-id will have exact same set of tuples an=
d
> replay of batches happen in exact same order before the failure.
>
> Having this paradigm, if downstream system process data for a given batch
> for having a given transaction-id , and if during failure if same batch i=
s
> again emitted , you can check if same transaction-id is already processed
> or not and hence can guarantee exact once semantics.
>
> And this can only be achieved in Spark if we use Low Level Kafka consumer
> API to process the offsets. This low level Kafka Consumer (
> https://github.com/dibbhatt/kafka-spark-consumer) has implemented the
> Spark Kafka consumer which uses Kafka Low Level APIs . All of the Kafka
> related logic has been taken from Storm-Kafka spout and which manages all
> Kafka re-balance and fault tolerant aspects and Kafka metadata management=
s.
>
> Presently this Consumer maintains that during Receiver failure, it will
> re-emit the exact same Block with same set of messages . Every message ha=
ve
> the details of its partition, offset and topic related details which can
> tackle the SPARK-3146.
>
> As this Low Level consumer has complete control over the Kafka Offsets ,
> we can implement Trident like feature on top of it like having implement =
a
> transaction-id for a given block , and re-emit the same block with same s=
et
> of message during Driver failure.
>
> Regards,
> Dibyendu
>
>
> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.com>
> wrote:
>>
>> Hi all,
>>
>> I agree with Hari that Strong exact-once semantics is very hard to
>> guarantee, especially in the failure situation. From my understanding ev=
en
>> current implementation of ReliableKafkaReceiver cannot fully guarantee t=
he
>> exact once semantics once failed, first is the ordering of data replayin=
g
>> from last checkpoint, this is hard to guarantee when multiple partitions
>> are injected in; second is the design complexity of achieving this, you =
can
>> refer to the Kafka Spout in Trident, we have to dig into the very detail=
s
>> of Kafka metadata management system to achieve this, not to say rebalanc=
e
>> and fault-tolerance.
>>
>> Thanks
>> Jerry
>>
>> -----Original Message-----
>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups@gmail.c=
om]
>> Sent: Friday, December 19, 2014 5:57 AM
>> To: Cody Koeninger
>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>> Subject: Re: Which committers care about Kafka?
>>
>> But idempotency is not that easy t achieve sometimes. A strong only once
>> semantic through a proper API would  be superuseful; but I'm not implyin=
g
>> this is easy to achieve.
>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote:
>>
>> > If the downstream store for the output data is idempotent or
>> > transactional, and that downstream store also is the system of record
>> > for kafka offsets, then you have exactly-once semantics.  Commit
>> > offsets with / after the data is stored.  On any failure, restart from
>> the last committed offsets.
>> >
>> > Yes, this approach is biased towards the etl-like use cases rather
>> > than near-realtime-analytics use cases.
>> >
>> > On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>> > hshreedharan@cloudera.com
>> > > wrote:
>> > >
>> > > I get what you are saying. But getting exactly once right is an
>> > > extremely hard problem - especially in presence of failure. The
>> > > issue is failures
>> > can
>> > > happen in a bunch of places. For example, before the notification of
>> > > downstream store being successful reaches the receiver that updates
>> > > the offsets, the node fails. The store was successful, but
>> > > duplicates came in either way. This is something worth discussing by
>> > > itself - but without uuids etc this might not really be solved even
>> when you think it is.
>> > >
>> > > Anyway, I will look at the links. Even I am interested in all of the
>> > > features you mentioned - no HDFS WAL for Kafka and once-only
>> > > delivery,
>> > but
>> > > I doubt the latter is really possible to guarantee - though I really
>> > would
>> > > love to have that!
>> > >
>> > > Thanks,
>> > > Hari
>> > >
>> > >
>> > > On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>> > > <cody@koeninger.org>
>> > > wrote:
>> > >
>> > >> Thanks for the replies.
>> > >>
>> > >> Regarding skipping WAL, it's not just about optimization.  If you
>> > >> actually want exactly-once semantics, you need control of kafka
>> > >> offsets
>> > as
>> > >> well, including the ability to not use zookeeper as the system of
>> > >> record for offsets.  Kafka already is a reliable system that has
>> > >> strong
>> > ordering
>> > >> guarantees (within a partition) and does not mandate the use of
>> > zookeeper
>> > >> to store offsets.  I think there should be a spark api that acts as
>> > >> a
>> > very
>> > >> simple intermediary between Kafka and the user's choice of
>> > >> downstream
>> > store.
>> > >>
>> > >> Take a look at the links I posted - if there's already been 2
>> > independent
>> > >> implementations of the idea, chances are it's something people need=
.
>> > >>
>> > >> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>> > >> hshreedharan@cloudera.com> wrote:
>> > >>>
>> > >>> Hi Cody,
>> > >>>
>> > >>> I am an absolute +1 on SPARK-3146. I think we can implement
>> > >>> something pretty simple and lightweight for that one.
>> > >>>
>> > >>> For the Kafka DStream skipping the WAL implementation - this is
>> > >>> something I discussed with TD a few weeks ago. Though it is a good
>> > idea to
>> > >>> implement this to avoid unnecessary HDFS writes, it is an
>> > optimization. For
>> > >>> that reason, we must be careful in implementation. There are a
>> > >>> couple
>> > of
>> > >>> issues that we need to ensure works properly - specifically
>> ordering.
>> > To
>> > >>> ensure we pull messages from different topics and partitions in
>> > >>> the
>> > same
>> > >>> order after failure, we=E2=80=99d still have to persist the metada=
ta to
>> > >>> HDFS
>> > (or
>> > >>> some other system) - this metadata must contain the order of
>> > >>> messages consumed, so we know how to re-read the messages. I am
>> > >>> planning to
>> > explore
>> > >>> this once I have some time (probably in Jan). In addition, we must
>> > >>> also ensure bucketing functions work fine as well. I will file a
>> > >>> placeholder jira for this one.
>> > >>>
>> > >>> I also wrote an API to write data back to Kafka a while back -
>> > >>> https://github.com/apache/spark/pull/2994 . I am hoping that this
>> > >>> will get pulled in soon, as this is something I know people want.
>> > >>> I am open
>> > to
>> > >>> feedback on that - anything that I can do to make it better.
>> > >>>
>> > >>> Thanks,
>> > >>> Hari
>> > >>>
>> > >>>
>> > >>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>> > >>> <pwendell@gmail.com>
>> > >>> wrote:
>> > >>>
>> > >>>>  Hey Cody,
>> > >>>>
>> > >>>> Thanks for reaching out with this. The lead on streaming is TD -
>> > >>>> he is traveling this week though so I can respond a bit. To the
>> > >>>> high level point of whether Kafka is important - it definitely
>> > >>>> is. Something like 80% of Spark Streaming deployments
>> > >>>> (anecdotally) ingest data from Kafka. Also, good support for
>> > >>>> Kafka is something we generally want in Spark and not a library.
>> > >>>> In some cases IIRC there were user libraries that used unstable
>> > >>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
>> > >>>> them to merge things upstream. Otherwise users wouldn't be able
>> > >>>> to use newer Kakfa versions. This is a high level impression only
>> > >>>> though, I haven't talked to TD about this recently so it's worth
>> revisiting given the developments in Kafka.
>> > >>>>
>> > >>>> Please do bring things up like this on the dev list if there are
>> > >>>> blockers for your usage - thanks for pinging it.
>> > >>>>
>> > >>>> - Patrick
>> > >>>>
>> > >>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>> > >>>> <cody@koeninger.org>
>> > >>>> wrote:
>> > >>>> > Now that 1.2 is finalized... who are the go-to people to get
>> > >>>> > some long-standing Kafka related issues resolved?
>> > >>>> >
>> > >>>> > The existing api is not sufficiently safe nor flexible for our
>> > >>>> production
>> > >>>> > use. I don't think we're alone in this viewpoint, because I've
>> > >>>> > seen several different patches and libraries to fix the same
>> > >>>> > things we've
>> > >>>> been
>> > >>>> > running into.
>> > >>>> >
>> > >>>> > Regarding flexibility
>> > >>>> >
>> > >>>> > https://issues.apache.org/jira/browse/SPARK-3146
>> > >>>> >
>> > >>>> > has been outstanding since August, and IMHO an equivalent of
>> > >>>> > this is absolutely necessary. We wrote a similar patch
>> > >>>> > ourselves, then found
>> > >>>> that
>> > >>>> > PR and have been running it in production. We wouldn't be able
>> > >>>> > to
>> > get
>> > >>>> our
>> > >>>> > jobs done without it. It also allows users to solve a whole
>> > >>>> > class of problems for themselves (e.g. SPARK-2388, arbitrary
>> > >>>> > delay of
>> > >>>> messages, etc).
>> > >>>> >
>> > >>>> > Regarding safety, I understand the motivation behind
>> > >>>> > WriteAheadLog
>> > as
>> > >>>> a
>> > >>>> > general solution for streaming unreliable sources, but Kafka
>> > >>>> > already
>> > >>>> is a
>> > >>>> > reliable source. I think there's a need for an api that treats
>> > >>>> > it as such. Even aside from the performance issues of
>> > >>>> > duplicating the write-ahead log in kafka into another
>> > >>>> > write-ahead log in hdfs, I
>> > need
>> > >>>> > exactly-once semantics in the face of failure (I've had
>> > >>>> > failures
>> > that
>> > >>>> > prevented reloading a spark streaming checkpoint, for instance)=
.
>> > >>>> >
>> > >>>> > I've got an implementation i've been using
>> > >>>> >
>> > >>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>> > >>>> > ka /src/main/scala/org/apache/spark/rdd/kafka
>> > >>>> >
>> > >>>> > Tresata has something similar at
>> > >>>> https://github.com/tresata/spark-kafka,
>> > >>>> > and I know there were earlier attempts based on Storm code.
>> > >>>> >
>> > >>>> > Trying to distribute these kinds of fixes as libraries rather
>> > >>>> > than
>> > >>>> patches
>> > >>>> > to Spark is problematic, because large portions of the
>> > implementation
>> > >>>> are
>> > >>>> > private[spark].
>> > >>>> >
>> > >>>> > I'd like to help, but i need to know whose attention to get.
>> > >>>>
>> > >>>> -----------------------------------------------------------------
>> > >>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
>> > >>>> additional commands, e-mail: dev-help@spark.apache.org
>> > >>>>
>> > >>>>
>> > >>>
>> > >
>> >
>>
>

--089e0153667200db00050a966832--

From dev-return-10872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 19:43:31 2014
Return-Path: <dev-return-10872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8376410B75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 19:43:31 +0000 (UTC)
Received: (qmail 63033 invoked by uid 500); 19 Dec 2014 19:43:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62965 invoked by uid 500); 19 Dec 2014 19:43:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62652 invoked by uid 99); 19 Dec 2014 19:43:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:43:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of harikrishna.kamepalli@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:43:24 +0000
Received: by mail-qg0-f42.google.com with SMTP id q108so1128244qgd.15
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 11:43:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=iOxKdVy9LXc5ur5qYwGxA6xXGHKs/nlmxCbPdJDmo7Q=;
        b=pc5d9Sc1NhjQT/lb7ZgpILBrDubtsnLyZg2sieGAYRLIh8vMonoV9Frq3OaKRti3vU
         j1xILEKE9PQO3uZX+486fweErAcA1Ve8T4HB24HlK+MJvmv+wfmfCRPjObqvYTArvzyQ
         ggVqLcYHXR9KO7l25Iw90J1m7S5DqrtjtkJ2c907Hb4AFyd961DqEaujl8l2GS5Mv/xG
         933jzICbOgQdPgwZnLbd1lmJYajrGBBG+m/3rFx3PNQGrzyVf84aWoLMrVyKeWvR1t5g
         b30mc6Tug8Osbpyrf4fxzAOxkmMvGZfOOevPBukptgdRPryvSnTNZbn4B77O0vRxFslO
         DHWQ==
MIME-Version: 1.0
X-Received: by 10.224.136.7 with SMTP id p7mr16779037qat.65.1419018183693;
 Fri, 19 Dec 2014 11:43:03 -0800 (PST)
Received: by 10.140.19.147 with HTTP; Fri, 19 Dec 2014 11:43:03 -0800 (PST)
Date: Sat, 20 Dec 2014 01:13:03 +0530
Message-ID: <CAPBrMRQ=eZYfCk8nOnRDDqX6xjzeOeCEfT=092DDxtJQNmyJCw@mail.gmail.com>
Subject: Spark Dev
From: Harikrishna Kamepalli <harikrishna.kamepalli@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c299a4c4db4b050a96ea72
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c299a4c4db4b050a96ea72
Content-Type: text/plain; charset=UTF-8

i am interested to contribute to spark

--001a11c299a4c4db4b050a96ea72--

From dev-return-10873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 19:55:29 2014
Return-Path: <dev-return-10873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E8E7510C82
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 19:55:28 +0000 (UTC)
Received: (qmail 99019 invoked by uid 500); 19 Dec 2014 19:55:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98938 invoked by uid 500); 19 Dec 2014 19:55:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98927 invoked by uid 99); 19 Dec 2014 19:55:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:55:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 19:54:59 +0000
Received: by mail-qc0-f175.google.com with SMTP id b13so1272828qcw.20
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 11:54:58 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=DHf+htzqQce/DKKIwi9j7fPpFKSNypAWBE0Vh07nPLM=;
        b=I/UoR8NYzMflh5TiKgXS9pNtYDCTmupKi7nilSr8jae+0waA/6jlypjmzQZy5Uek7m
         7ESwcZN7tEbJMeDP9UGNYCdd8+F+dDYYuSXOsjAie6H6Fc9TWbhoTFlyG3/JukvXQXsB
         1Pv6vczfVSRxfnw59eGf88W8OdVcLpj73E2oXzslRJy/iCiYGdC9IqkAMJvUMTqkNG+c
         pqXd+qBSWXQrQ+JKyN6xwbAsoVZAxwS90wWl2ahT2jOs7LTm+AROWq51Vhmb3ReHJf1o
         NWbCN0n5tuWY1B4+6qIQCCHv2hqJanti+jvuCIpPfFFah1SZvkRo8hgraAIn1Qq8vZW/
         q4LA==
X-Gm-Message-State: ALoCoQkrCEA0hcBeI3Qm1BD/5AOyVZLve1ISxQf1ZzQQbdnjQbRPxWG4nrHFxSkjuF7dy8uJAZum
MIME-Version: 1.0
X-Received: by 10.140.97.35 with SMTP id l32mr4620744qge.11.1419018898609;
 Fri, 19 Dec 2014 11:54:58 -0800 (PST)
Received: by 10.140.102.113 with HTTP; Fri, 19 Dec 2014 11:54:58 -0800 (PST)
In-Reply-To: <CAPBrMRQ=eZYfCk8nOnRDDqX6xjzeOeCEfT=092DDxtJQNmyJCw@mail.gmail.com>
References: <CAPBrMRQ=eZYfCk8nOnRDDqX6xjzeOeCEfT=092DDxtJQNmyJCw@mail.gmail.com>
Date: Fri, 19 Dec 2014 14:54:58 -0500
Message-ID: <CACBYxKKtZJUsD1OqfpCP0jVyDJCho7Th-txZEnXuoGMi+fmR9g@mail.gmail.com>
Subject: Re: Spark Dev
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Harikrishna Kamepalli <harikrishna.kamepalli@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a9c2861b381050a97150d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9c2861b381050a97150d
Content-Type: text/plain; charset=UTF-8

Hi Harikrishna,

A good place to start is taking a look at the wiki page on contributing:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

-Sandy

On Fri, Dec 19, 2014 at 2:43 PM, Harikrishna Kamepalli <
harikrishna.kamepalli@gmail.com> wrote:
>
> i am interested to contribute to spark
>

--001a113a9c2861b381050a97150d--

From dev-return-10874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 20:28:19 2014
Return-Path: <dev-return-10874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 80A8A10E6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 20:28:19 +0000 (UTC)
Received: (qmail 89979 invoked by uid 500); 19 Dec 2014 20:28:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89923 invoked by uid 500); 19 Dec 2014 20:28:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89912 invoked by uid 99); 19 Dec 2014 20:28:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 20:28:12 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.23 as permitted sender)
Received: from [216.64.169.23] (HELO pdxmta02.webtrends.com) (216.64.169.23)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 20:27:47 +0000
Received: from pdxex2.webtrends.corp (Not Verified[10.61.2.221]) by pdxmta02.webtrends.com with MailMarshal (v7,2,3,6978) (using TLS: SSLv23)
	id <B54948a430000>; Fri, 19 Dec 2014 20:27:47 +0000
Received: from PDXEX1.WebTrends.corp ([172.27.5.220]) by pdxex2.webtrends.corp
 ([172.27.3.221]) with mapi id 14.03.0195.001; Fri, 19 Dec 2014 20:27:45 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: Hari Shreedharan <hshreedharan@cloudera.com>
CC: Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, "Shao, Saisai"
	<saisai.shao@intel.com>, =?utf-8?B?THVpcyDDgW5nZWwgVmljZW50ZSBTw6FuY2hleg==?=
	<langel.groups@gmail.com>, Cody Koeninger <cody@koeninger.org>, "Patrick
 Wendell" <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Which committers care about Kafka?
Thread-Topic: Which committers care about Kafka?
Thread-Index: AQHQGtRmNJ3KUkVY7kW7uFiYgKK1LZyVt6AAgAAIvoCAAAuuAIAAEQIAgAAG9QCAAAFPgIAARP0AgACA7ACAAJzNgIAAFsiA
Date: Fri, 19 Dec 2014 20:27:44 +0000
Message-ID: <13A3B428-1BC4-4F1D-A057-60DE099F11AD@webtrends.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
 <1418938019858.7377d5d7@Nodemailer>
 <CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
 <CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
 <64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com>
 <CAFiYKR-FA3DAH91d6GtmRfAOmYECPzYh3C3qrQBuR4RuC9vZkg@mail.gmail.com>
 <CAHbPYVZwhnNB_xO+WLx6xaiNds=pgJtUh6tobc=tTW=O6Xa+ww@mail.gmail.com>
In-Reply-To: <CAHbPYVZwhnNB_xO+WLx6xaiNds=pgJtUh6tobc=tTW=O6Xa+ww@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="utf-8"
Content-ID: <1895519FAF3F184BBE6A0299A6EAE6C0@WebTrends.com>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 20:52:28 2014
Return-Path: <dev-return-10875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D44C10F65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 20:52:28 +0000 (UTC)
Received: (qmail 46280 invoked by uid 500); 19 Dec 2014 20:52:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46196 invoked by uid 500); 19 Dec 2014 20:52:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44947 invoked by uid 99); 19 Dec 2014 20:52:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 20:52:18 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 20:52:13 +0000
Received: by mail-lb0-f172.google.com with SMTP id u10so1457843lbd.17;
        Fri, 19 Dec 2014 12:51:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=k0RCdoNBE6SbeBt28rj8zNmLXwb1sF9cQg2TaFidevY=;
        b=pUI9B6NXbRDV2elM9Q7Va/3kCOnZwNjU95Q9Va4h6TUpWb17IwuQC7fZcbHBXpCyFx
         lrXArI+4tpm09NghRbzHDDHrqHg1IKqENA0N+gM9ldkkSOR50SGxLE5Bdl+qR+R3BYav
         lssDC4pjLI4PD/lfU6leBV81VOQWn0p/32+AENMET1b3nRmx3QKd1a6UWQ2Yvw7L4dQ8
         m5YWP1MRocw2R/CQm62x6iy9uCkdDBbW09yGr7Ta34NPPrUZeVyurd1aFZNRK9pT5tLp
         tQNSJMhc4PrPX0f8OIgORvHaV7gr1hLZg7SSBIKvw6pFkPmgVD0Rq5LTvLUsiAaQ4CuA
         Htrw==
MIME-Version: 1.0
X-Received: by 10.112.157.104 with SMTP id wl8mr9788758lbb.96.1419022267474;
 Fri, 19 Dec 2014 12:51:07 -0800 (PST)
Received: by 10.112.126.166 with HTTP; Fri, 19 Dec 2014 12:51:07 -0800 (PST)
In-Reply-To: <CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
	<CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com>
	<CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
Date: Fri, 19 Dec 2014 12:51:07 -0800
Message-ID: <CALEZFQxMdtQhtw6et_QcygyxEXUvgj6w3m9bbkYQ5NCdNfJJXA@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
From: Andy Konwinski <andykonwinski@gmail.com>
To: Josh Rosen <rosenville@gmail.com>
Cc: Yana Kadiyska <yana.kadiyska@gmail.com>, 
	"user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c32e342e6870050a97de1b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c32e342e6870050a97de1b
Content-Type: text/plain; charset=UTF-8

Yesterday, I changed the domain name in the mailing list archive settings
to remove ".incubator" so maybe it'll work now.

However, I also sent two emails about this through the nabble interface (in
this same thread) yesterday and they don't appear to have made it through
so not sure if it actually worked after all.

Andy

On Wed, Dec 17, 2014 at 1:09 PM, Josh Rosen <rosenville@gmail.com> wrote:
>
> Yeah, it looks like messages that are successfully posted via Nabble end
> up on the Apache mailing list, but messages posted directly to Apache
> aren't mirrored to Nabble anymore because it's based off the incubator
> mailing list.  We should fix this so that Nabble posts to / archives the
> non-incubator list.
>
> On Sat, Dec 13, 2014 at 6:27 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
> wrote:
>>
>> Since you mentioned this, I had a related quandry recently -- it also
>> says that the forum archives "*user@spark.incubator.apache.org
>> <user@spark.incubator.apache.org>"/* *dev@spark.incubator.apache.org
>> <dev@spark.incubator.apache.org> *respectively, yet the "Community page"
>> clearly says to email the @spark.apache.org list (but the nabble archive
>> is linked right there too). IMO even putting a clear explanation at the top
>>
>> "Posting here requires that you create an account via the UI. Your
>> message will be sent to both spark.incubator.apache.org and
>> spark.apache.org (if that is the case, i'm not sure which alias nabble
>> posts get sent to)" would make things a lot more clear.
>>
>> On Sat, Dec 13, 2014 at 5:05 PM, Josh Rosen <rosenville@gmail.com> wrote:
>>>
>>> I've noticed that several users are attempting to post messages to
>>> Spark's user / dev mailing lists using the Nabble web UI (
>>> http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there
>>> are many posts in Nabble that are not posted to the Apache lists and are
>>> flagged with "This post has NOT been accepted by the mailing list yet."
>>> errors.
>>>
>>> I suspect that the issue is that users are not completing the sign-up
>>> confirmation process (
>>> http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
>>> which is preventing their emails from being accepted by the mailing list.
>>>
>>> I wanted to mention this issue to the Spark community to see whether
>>> there are any good solutions to address this.  I have spoken to users who
>>> think that our mailing list is unresponsive / inactive because their
>>> un-posted messages haven't received any replies.
>>>
>>> - Josh
>>>
>>

--001a11c32e342e6870050a97de1b--

From dev-return-10876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 21:38:56 2014
Return-Path: <dev-return-10876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0308D1013B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 21:38:56 +0000 (UTC)
Received: (qmail 40930 invoked by uid 500); 19 Dec 2014 21:38:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40825 invoked by uid 500); 19 Dec 2014 21:38:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38921 invoked by uid 99); 19 Dec 2014 21:38:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:38:52 +0000
X-ASF-Spam-Status: No, hits=2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.42 as permitted sender)
Received: from [209.85.213.42] (HELO mail-yh0-f42.google.com) (209.85.213.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:38:47 +0000
Received: by mail-yh0-f42.google.com with SMTP id v1so794500yhn.15;
        Fri, 19 Dec 2014 13:36:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=VKFLlCmASXBV3kKzJ3yZpdOHvJocGj8cHtz7phO7zp4=;
        b=TR1jYGt1CwInewqgQngPr0RUcayVmXf3Pxwk2sedY3S8DROWJZgYhd7ksFAtJQWsKy
         KOqKIm7QOMRGBMCyg/ddFux63FpYYYe+CmG08zqcqzp7RlWzKOlVRBBD7jVDOboGHDbm
         DAL9pYJKDl8z71JdqjK6FM7NsKJtVumsqRK0z+gefjyTRCUHfOBpnEpVxBGXlXrpqc00
         EQCZoH8U+d76sXck5B0Its7J8tEHKERXGUzVVF0uVjiTNlrlXMHmvMMW8rHtm/k467LH
         4hsP1yfJ2tsL2FrSLDga/nQzsfxZKExY+gIMwporCtCHdd3pcY3Ti4yAq40vMTZZKOld
         sUnA==
MIME-Version: 1.0
X-Received: by 10.170.188.8 with SMTP id f8mr9294407yke.114.1419025016821;
 Fri, 19 Dec 2014 13:36:56 -0800 (PST)
Received: by 10.170.145.67 with HTTP; Fri, 19 Dec 2014 13:36:56 -0800 (PST)
In-Reply-To: <CALEZFQxMdtQhtw6et_QcygyxEXUvgj6w3m9bbkYQ5NCdNfJJXA@mail.gmail.com>
References: <CAOEPXP6ikStDUO28CPscNkwd2uaYRUNNkR2-qZ6EdPj0rd47mA@mail.gmail.com>
	<CAJ4HpHGdra9ON1pA-WVLuFba3WvSjjqrnT36aPW4JTtQO=r4Xw@mail.gmail.com>
	<CAOEPXP6jCMcHE+abV9yanbNrJCX1tE54pqGN5MsEYLN+z8+=Sg@mail.gmail.com>
	<CALEZFQxMdtQhtw6et_QcygyxEXUvgj6w3m9bbkYQ5NCdNfJJXA@mail.gmail.com>
Date: Fri, 19 Dec 2014 13:36:56 -0800
Message-ID: <CALte62x8yX-+LjnRpUwoxiWtEnRMh2eQYYyQ1ZPvGG-JSf1dyg@mail.gmail.com>
Subject: Re: Nabble mailing list mirror errors: "This post has NOT been
 accepted by the mailing list yet"
From: Ted Yu <yuzhihong@gmail.com>
To: Andy Konwinski <andykonwinski@gmail.com>
Cc: Josh Rosen <rosenville@gmail.com>, Yana Kadiyska <yana.kadiyska@gmail.com>, 
	"user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139709c0e1bfe050a988299
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139709c0e1bfe050a988299
Content-Type: text/plain; charset=UTF-8

Andy:
I saw two emails from you from yesterday.

See this thread: http://search-hadoop.com/m/JW1q5opRsY1

Cheers

On Fri, Dec 19, 2014 at 12:51 PM, Andy Konwinski <andykonwinski@gmail.com>
wrote:

> Yesterday, I changed the domain name in the mailing list archive settings
> to remove ".incubator" so maybe it'll work now.
>
> However, I also sent two emails about this through the nabble interface
> (in this same thread) yesterday and they don't appear to have made it
> through so not sure if it actually worked after all.
>
> Andy
>
> On Wed, Dec 17, 2014 at 1:09 PM, Josh Rosen <rosenville@gmail.com> wrote:
>>
>> Yeah, it looks like messages that are successfully posted via Nabble end
>> up on the Apache mailing list, but messages posted directly to Apache
>> aren't mirrored to Nabble anymore because it's based off the incubator
>> mailing list.  We should fix this so that Nabble posts to / archives the
>> non-incubator list.
>>
>> On Sat, Dec 13, 2014 at 6:27 PM, Yana Kadiyska <yana.kadiyska@gmail.com>
>> wrote:
>>>
>>> Since you mentioned this, I had a related quandry recently -- it also
>>> says that the forum archives "*user@spark.incubator.apache.org
>>> <user@spark.incubator.apache.org>"/* *dev@spark.incubator.apache.org
>>> <dev@spark.incubator.apache.org> *respectively, yet the "Community
>>> page" clearly says to email the @spark.apache.org list (but the nabble
>>> archive is linked right there too). IMO even putting a clear explanation at
>>> the top
>>>
>>> "Posting here requires that you create an account via the UI. Your
>>> message will be sent to both spark.incubator.apache.org and
>>> spark.apache.org (if that is the case, i'm not sure which alias nabble
>>> posts get sent to)" would make things a lot more clear.
>>>
>>> On Sat, Dec 13, 2014 at 5:05 PM, Josh Rosen <rosenville@gmail.com>
>>> wrote:
>>>>
>>>> I've noticed that several users are attempting to post messages to
>>>> Spark's user / dev mailing lists using the Nabble web UI (
>>>> http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there
>>>> are many posts in Nabble that are not posted to the Apache lists and are
>>>> flagged with "This post has NOT been accepted by the mailing list yet."
>>>> errors.
>>>>
>>>> I suspect that the issue is that users are not completing the sign-up
>>>> confirmation process (
>>>> http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
>>>> which is preventing their emails from being accepted by the mailing list.
>>>>
>>>> I wanted to mention this issue to the Spark community to see whether
>>>> there are any good solutions to address this.  I have spoken to users who
>>>> think that our mailing list is unresponsive / inactive because their
>>>> un-posted messages haven't received any replies.
>>>>
>>>> - Josh
>>>>
>>>

--001a1139709c0e1bfe050a988299--

From dev-return-10877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 21:50:03 2014
Return-Path: <dev-return-10877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5AE7110189
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 21:50:03 +0000 (UTC)
Received: (qmail 58901 invoked by uid 500); 19 Dec 2014 21:50:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58840 invoked by uid 500); 19 Dec 2014 21:50:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58803 invoked by uid 99); 19 Dec 2014 21:50:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:50:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:49:56 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so3413107oig.11
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 13:48:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=jxGY8gXNwUiMP0OfhX6HIqSzJyVy2zMo+cW7PpvTddg=;
        b=frjjgze8V1RbZ8wS9TL7XUwhJP4KF2RioLF9lKLX34U/4XQGUzyb02GGL5lwE0qwjs
         fOzSY0rIf4lrgcpWj1sxNRNQ7RaIDG815jdW09uHME4cmEfXfWEGI7PBrXIut+guT8NE
         mCzKGnUYSJ5jq5yJlffmlOxxmZ019axQr0Cl2g8ER4uJPdp4dG0A4ZseVPr7r1qj1WAi
         RWo/Th9I0m6cLLj5dournO9OaUqSmK9ufCtam1jZmXuCGOzuAjBNraIP+KLec1BSIypY
         sCCQy9iLkuQAtEuWoFC8xZQkkwj9r2mfZVcceKaQExW9epxcipa63PlTBFku6Hot8Wte
         oWbw==
X-Gm-Message-State: ALoCoQmyp1ug+VZwmvEFogBqbeXVbczRuaVjBlo6wbqNNJluoBgC9BKbn9NDxrmlTBlmlCdrlJQ5
MIME-Version: 1.0
X-Received: by 10.202.61.9 with SMTP id k9mr5863849oia.116.1419025709653; Fri,
 19 Dec 2014 13:48:29 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Fri, 19 Dec 2014 13:48:29 -0800 (PST)
In-Reply-To: <13A3B428-1BC4-4F1D-A057-60DE099F11AD@webtrends.com>
References: <CAKWX9VXGBS56O4NNp6idKtAoHPwKdvDbi3ygLM7kU9dpqSiePg@mail.gmail.com>
	<1418938019858.7377d5d7@Nodemailer>
	<CAKWX9VXZjsNxKCsb31gAdGqE16tkNqyxn+GQfsSgSxOrWYhzaw@mail.gmail.com>
	<CAGzJ1gTkweAgXMq=4-Kkgdjxd7-fYd_SmEKR48xGt9vQ=594xQ@mail.gmail.com>
	<64474308D680D540A4D8151B0F7C03F70276AEAE@SHSMSX104.ccr.corp.intel.com>
	<CAFiYKR-FA3DAH91d6GtmRfAOmYECPzYh3C3qrQBuR4RuC9vZkg@mail.gmail.com>
	<CAHbPYVZwhnNB_xO+WLx6xaiNds=pgJtUh6tobc=tTW=O6Xa+ww@mail.gmail.com>
	<13A3B428-1BC4-4F1D-A057-60DE099F11AD@webtrends.com>
Date: Fri, 19 Dec 2014 15:48:29 -0600
Message-ID: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Sean McNamara <Sean.McNamara@webtrends.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, "Shao, Saisai" <saisai.shao@intel.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113cd2a859f898050a98abad
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cd2a859f898050a98abad
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The problems you guys are discussing come from trying to store state in
spark, so don't do that.  Spark isn't a distributed database.

Just map kafka partitions directly to rdds, llet user code specify the
range of offsets explicitly, and let them be in charge of committing
offsets.

Using the simple consumer isn't that bad, I'm already using this in
production with the code I linked to, and tresata apparently has been as
well.  Again, for everyone saying this is impossible, have you read either
of those implementations and looked at the approach?



On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <Sean.McNamara@webtrends.com=
>
wrote:

> Please feel free to correct me if I=E2=80=99m wrong, but I think the exac=
tly once
> spark streaming semantics can easily be solved using updateStateByKey. Ma=
ke
> the key going into updateStateByKey be a hash of the event, or pluck off
> some uuid from the message.  The updateFunc would only emit the message i=
f
> the key did not exist, and the user has complete control over the window =
of
> time / state lifecycle for detecting duplicates.  It also makes it really
> easy to detect and take action (alert?) when you DO see a duplicate, or
> make memory tradeoffs within an error bound using a sketch algorithm.  Th=
e
> kafka simple consumer is insanely complex, if possible I think it would b=
e
> better (and vastly more flexible) to get reliability using the primitives
> that spark so elegantly provides.
>
> Cheers,
>
> Sean
>
>
> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
> hshreedharan@cloudera.com> wrote:
> >
> > Hi Dibyendu,
> >
> > Thanks for the details on the implementation. But I still do not believ=
e
> > that it is no duplicates - what they achieve is that the same batch is
> > processed exactly the same way every time (but see it may be processed
> more
> > than once) - so it depends on the operation being idempotent. I believe
> > Trident uses ZK to keep track of the transactions - a batch can be
> > processed multiple times in failure scenarios (for example, the
> transaction
> > is processed but before ZK is updated the machine fails, causing a "new=
"
> > node to process it again).
> >
> > I don't think it is impossible to do this in Spark Streaming as well an=
d
> > I'd be really interested in working on it at some point in the near
> future.
> >
> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
> > dibyendu.bhattachary@gmail.com> wrote:
> >
> >> Hi,
> >>
> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The Storm
> >> Trident has done the exact-once guarantee by processing the tuple in a
> >> batch  and assigning same transaction-id for a given batch . The repla=
y
> for
> >> a given batch with a transaction-id will have exact same set of tuples
> and
> >> replay of batches happen in exact same order before the failure.
> >>
> >> Having this paradigm, if downstream system process data for a given
> batch
> >> for having a given transaction-id , and if during failure if same batc=
h
> is
> >> again emitted , you can check if same transaction-id is already
> processed
> >> or not and hence can guarantee exact once semantics.
> >>
> >> And this can only be achieved in Spark if we use Low Level Kafka
> consumer
> >> API to process the offsets. This low level Kafka Consumer (
> >> https://github.com/dibbhatt/kafka-spark-consumer) has implemented the
> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the Kafk=
a
> >> related logic has been taken from Storm-Kafka spout and which manages
> all
> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
> managements.
> >>
> >> Presently this Consumer maintains that during Receiver failure, it wil=
l
> >> re-emit the exact same Block with same set of messages . Every message
> have
> >> the details of its partition, offset and topic related details which c=
an
> >> tackle the SPARK-3146.
> >>
> >> As this Low Level consumer has complete control over the Kafka Offsets=
 ,
> >> we can implement Trident like feature on top of it like having
> implement a
> >> transaction-id for a given block , and re-emit the same block with sam=
e
> set
> >> of message during Driver failure.
> >>
> >> Regards,
> >> Dibyendu
> >>
> >>
> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.com>
> >> wrote:
> >>>
> >>> Hi all,
> >>>
> >>> I agree with Hari that Strong exact-once semantics is very hard to
> >>> guarantee, especially in the failure situation. From my understanding
> even
> >>> current implementation of ReliableKafkaReceiver cannot fully guarante=
e
> the
> >>> exact once semantics once failed, first is the ordering of data
> replaying
> >>> from last checkpoint, this is hard to guarantee when multiple
> partitions
> >>> are injected in; second is the design complexity of achieving this,
> you can
> >>> refer to the Kafka Spout in Trident, we have to dig into the very
> details
> >>> of Kafka metadata management system to achieve this, not to say
> rebalance
> >>> and fault-tolerance.
> >>>
> >>> Thanks
> >>> Jerry
> >>>
> >>> -----Original Message-----
> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups@gmai=
l.com]
> >>> Sent: Friday, December 19, 2014 5:57 AM
> >>> To: Cody Koeninger
> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
> >>> Subject: Re: Which committers care about Kafka?
> >>>
> >>> But idempotency is not that easy t achieve sometimes. A strong only
> once
> >>> semantic through a proper API would  be superuseful; but I'm not
> implying
> >>> this is easy to achieve.
> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote:
> >>>
> >>>> If the downstream store for the output data is idempotent or
> >>>> transactional, and that downstream store also is the system of recor=
d
> >>>> for kafka offsets, then you have exactly-once semantics.  Commit
> >>>> offsets with / after the data is stored.  On any failure, restart fr=
om
> >>> the last committed offsets.
> >>>>
> >>>> Yes, this approach is biased towards the etl-like use cases rather
> >>>> than near-realtime-analytics use cases.
> >>>>
> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> >>>> hshreedharan@cloudera.com
> >>>>> wrote:
> >>>>>
> >>>>> I get what you are saying. But getting exactly once right is an
> >>>>> extremely hard problem - especially in presence of failure. The
> >>>>> issue is failures
> >>>> can
> >>>>> happen in a bunch of places. For example, before the notification o=
f
> >>>>> downstream store being successful reaches the receiver that updates
> >>>>> the offsets, the node fails. The store was successful, but
> >>>>> duplicates came in either way. This is something worth discussing b=
y
> >>>>> itself - but without uuids etc this might not really be solved even
> >>> when you think it is.
> >>>>>
> >>>>> Anyway, I will look at the links. Even I am interested in all of th=
e
> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
> >>>>> delivery,
> >>>> but
> >>>>> I doubt the latter is really possible to guarantee - though I reall=
y
> >>>> would
> >>>>> love to have that!
> >>>>>
> >>>>> Thanks,
> >>>>> Hari
> >>>>>
> >>>>>
> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
> >>>>> <cody@koeninger.org>
> >>>>> wrote:
> >>>>>
> >>>>>> Thanks for the replies.
> >>>>>>
> >>>>>> Regarding skipping WAL, it's not just about optimization.  If you
> >>>>>> actually want exactly-once semantics, you need control of kafka
> >>>>>> offsets
> >>>> as
> >>>>>> well, including the ability to not use zookeeper as the system of
> >>>>>> record for offsets.  Kafka already is a reliable system that has
> >>>>>> strong
> >>>> ordering
> >>>>>> guarantees (within a partition) and does not mandate the use of
> >>>> zookeeper
> >>>>>> to store offsets.  I think there should be a spark api that acts a=
s
> >>>>>> a
> >>>> very
> >>>>>> simple intermediary between Kafka and the user's choice of
> >>>>>> downstream
> >>>> store.
> >>>>>>
> >>>>>> Take a look at the links I posted - if there's already been 2
> >>>> independent
> >>>>>> implementations of the idea, chances are it's something people nee=
d.
> >>>>>>
> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> >>>>>> hshreedharan@cloudera.com> wrote:
> >>>>>>>
> >>>>>>> Hi Cody,
> >>>>>>>
> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
> >>>>>>> something pretty simple and lightweight for that one.
> >>>>>>>
> >>>>>>> For the Kafka DStream skipping the WAL implementation - this is
> >>>>>>> something I discussed with TD a few weeks ago. Though it is a goo=
d
> >>>> idea to
> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
> >>>> optimization. For
> >>>>>>> that reason, we must be careful in implementation. There are a
> >>>>>>> couple
> >>>> of
> >>>>>>> issues that we need to ensure works properly - specifically
> >>> ordering.
> >>>> To
> >>>>>>> ensure we pull messages from different topics and partitions in
> >>>>>>> the
> >>>> same
> >>>>>>> order after failure, we=E2=80=99d still have to persist the metad=
ata to
> >>>>>>> HDFS
> >>>> (or
> >>>>>>> some other system) - this metadata must contain the order of
> >>>>>>> messages consumed, so we know how to re-read the messages. I am
> >>>>>>> planning to
> >>>> explore
> >>>>>>> this once I have some time (probably in Jan). In addition, we mus=
t
> >>>>>>> also ensure bucketing functions work fine as well. I will file a
> >>>>>>> placeholder jira for this one.
> >>>>>>>
> >>>>>>> I also wrote an API to write data back to Kafka a while back -
> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that this
> >>>>>>> will get pulled in soon, as this is something I know people want.
> >>>>>>> I am open
> >>>> to
> >>>>>>> feedback on that - anything that I can do to make it better.
> >>>>>>>
> >>>>>>> Thanks,
> >>>>>>> Hari
> >>>>>>>
> >>>>>>>
> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
> >>>>>>> <pwendell@gmail.com>
> >>>>>>> wrote:
> >>>>>>>
> >>>>>>>> Hey Cody,
> >>>>>>>>
> >>>>>>>> Thanks for reaching out with this. The lead on streaming is TD -
> >>>>>>>> he is traveling this week though so I can respond a bit. To the
> >>>>>>>> high level point of whether Kafka is important - it definitely
> >>>>>>>> is. Something like 80% of Spark Streaming deployments
> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support for
> >>>>>>>> Kafka is something we generally want in Spark and not a library.
> >>>>>>>> In some cases IIRC there were user libraries that used unstable
> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be able
> >>>>>>>> to use newer Kakfa versions. This is a high level impression onl=
y
> >>>>>>>> though, I haven't talked to TD about this recently so it's worth
> >>> revisiting given the developments in Kafka.
> >>>>>>>>
> >>>>>>>> Please do bring things up like this on the dev list if there are
> >>>>>>>> blockers for your usage - thanks for pinging it.
> >>>>>>>>
> >>>>>>>> - Patrick
> >>>>>>>>
> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
> >>>>>>>> <cody@koeninger.org>
> >>>>>>>> wrote:
> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to get
> >>>>>>>>> some long-standing Kafka related issues resolved?
> >>>>>>>>>
> >>>>>>>>> The existing api is not sufficiently safe nor flexible for our
> >>>>>>>> production
> >>>>>>>>> use. I don't think we're alone in this viewpoint, because I've
> >>>>>>>>> seen several different patches and libraries to fix the same
> >>>>>>>>> things we've
> >>>>>>>> been
> >>>>>>>>> running into.
> >>>>>>>>>
> >>>>>>>>> Regarding flexibility
> >>>>>>>>>
> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
> >>>>>>>>>
> >>>>>>>>> has been outstanding since August, and IMHO an equivalent of
> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
> >>>>>>>>> ourselves, then found
> >>>>>>>> that
> >>>>>>>>> PR and have been running it in production. We wouldn't be able
> >>>>>>>>> to
> >>>> get
> >>>>>>>> our
> >>>>>>>>> jobs done without it. It also allows users to solve a whole
> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitrary
> >>>>>>>>> delay of
> >>>>>>>> messages, etc).
> >>>>>>>>>
> >>>>>>>>> Regarding safety, I understand the motivation behind
> >>>>>>>>> WriteAheadLog
> >>>> as
> >>>>>>>> a
> >>>>>>>>> general solution for streaming unreliable sources, but Kafka
> >>>>>>>>> already
> >>>>>>>> is a
> >>>>>>>>> reliable source. I think there's a need for an api that treats
> >>>>>>>>> it as such. Even aside from the performance issues of
> >>>>>>>>> duplicating the write-ahead log in kafka into another
> >>>>>>>>> write-ahead log in hdfs, I
> >>>> need
> >>>>>>>>> exactly-once semantics in the face of failure (I've had
> >>>>>>>>> failures
> >>>> that
> >>>>>>>>> prevented reloading a spark streaming checkpoint, for instance)=
.
> >>>>>>>>>
> >>>>>>>>> I've got an implementation i've been using
> >>>>>>>>>
> >>>>>>>>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
> >>>>>>>>>
> >>>>>>>>> Tresata has something similar at
> >>>>>>>> https://github.com/tresata/spark-kafka,
> >>>>>>>>> and I know there were earlier attempts based on Storm code.
> >>>>>>>>>
> >>>>>>>>> Trying to distribute these kinds of fixes as libraries rather
> >>>>>>>>> than
> >>>>>>>> patches
> >>>>>>>>> to Spark is problematic, because large portions of the
> >>>> implementation
> >>>>>>>> are
> >>>>>>>>> private[spark].
> >>>>>>>>>
> >>>>>>>>> I'd like to help, but i need to know whose attention to get.
> >>>>>>>>
> >>>>>>>> ----------------------------------------------------------------=
-
> >>>>>>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org Fo=
r
> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>>
> >>>
> >>
>
>

--001a113cd2a859f898050a98abad--

From dev-return-10878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 21:57:32 2014
Return-Path: <dev-return-10878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B48D101E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 21:57:32 +0000 (UTC)
Received: (qmail 79291 invoked by uid 500); 19 Dec 2014 21:57:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79209 invoked by uid 500); 19 Dec 2014 21:57:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79198 invoked by uid 99); 19 Dec 2014 21:57:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:57:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.192.51 as permitted sender)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 21:57:26 +0000
Received: by mail-qg0-f51.google.com with SMTP id i50so1234326qgf.38
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 13:57:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:mime-version:message-id:in-reply-to
         :references:from:to:cc:subject:content-type;
        bh=1TVf6tLzN3E801A3h2cz9VKWecr+NevrEFx4QqwH6Hc=;
        b=JyuFhheJ9pB8ZZj/CD0hO2LP4VbNmv+klqOl3ml4Uno1472sCmrHBlJoMuDh9ubaso
         ZuIjkdRw++JVo0pdjpIoH1Km9/RVGgzEGkktIjN57NcGrDKdQqBlNhiQPwsTL4GtkopK
         Uy+2TiCRqOs/JvjumZPqxS75YXgzF5wsDgR0NIr5Rd6Z5HcBKAsGvPYKds7IHTeLqVa/
         1HhU59R4APb4MMMnCJt62RZGvSBgcmS5jJ6IbzONaC+QqMzpnTJp9YBiy4kv2H6LIHge
         zM9ZdXe/GKJkLUSI3i12mXjWkWPwMn6BJ3oLajvifImBTcDCf/tpNQcfhn1AFVo0Hepe
         9oqg==
X-Gm-Message-State: ALoCoQmSP8baxWo9mmAqY3RCeCsCINaKEoqchTmaLtyxgQsozDPPE5xDaPrd2rndV+2g2iMilfVr
X-Received: by 10.224.66.200 with SMTP id o8mr17924209qai.13.1419026225293;
        Fri, 19 Dec 2014 13:57:05 -0800 (PST)
Received: from hedwig-6.prd.orcali.com (ec2-54-85-253-252.compute-1.amazonaws.com. [54.85.253.252])
        by mx.google.com with ESMTPSA id u14sm10539082qac.15.2014.12.19.13.57.04
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 19 Dec 2014 13:57:04 -0800 (PST)
Date: Fri, 19 Dec 2014 13:57:04 -0800 (PST)
X-Google-Original-Date: Fri, 19 Dec 2014 21:57:04 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1419026224285.80417be1@Nodemailer>
In-Reply-To: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
References: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
X-Orchestra-Oid: 2966663E-5598-416E-B4B9-751904F3EA6B
X-Orchestra-Sig: 5c1ba0880861e91dd21facebe2b6a80aab6b43ee
X-Orchestra-Thrid: TEB56D109-7104-46FE-9747-57A7D6EE5D69_1487840502276290343
X-Orchestra-Thrid-Sig: 257f4f6067e02a56f8c5e6e4a3be5aafec0bf555
X-Orchestra-Account: 848a6421f1fff786534519cf9a6ec8029f524104
From: "Hari Shreedharan" <hshreedharan@cloudera.com>
To: "Cody Koeninger" <cody@koeninger.org>
Cc: "Sean McNamara" <sean.mcnamara@webtrends.com>, dev@spark.apache.org,
 "Dibyendu Bhattacharya" <dibyendu.bhattachary@gmail.com>, "Luis
 =?UTF-8?Q?=C3=81ngel?= Vicente =?UTF-8?Q?S=C3=A1nchez?="
 <langel.groups@gmail.com>, "Patrick Wendell" <pwendell@gmail.com>,
 "ShaoSaisai" <saisai.shao@intel.com>
Subject: Re: Which committers care about Kafka?
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1419026224707"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1419026224707
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Can you explain your basic algorithm for the once-only-delivery=3F It is =
quite a bit of very Kafka-specific code, that would take more time to read =
than I can currently afford=3F If you can explain your algorithm a bit, it =
might help.




Thanks,=C2=A0Hari

On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
wrote:

> The problems you guys are discussing come from trying to store state in
> spark, so don't do that.  Spark isn't a distributed database.
> Just map kafka partitions directly to rdds, llet user code specify the
> range of offsets explicitly, and let them be in charge of committing
> offsets.
> Using the simple consumer isn't that bad, I'm already using this in
> production with the code I linked to, and tresata apparently has been as
> well.  Again, for everyone saying this is impossible, have you read =
either
> of those implementations and looked at the approach=3F
> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <Sean.McNamara@webtrends.=
com>
> wrote:
>> Please feel free to correct me if I=E2=80=99m wrong, but I think the =
exactly once
>> spark streaming semantics can easily be solved using updateStateByKey. =
Make
>> the key going into updateStateByKey be a hash of the event, or pluck =
off
>> some uuid from the message.  The updateFunc would only emit the message =
if
>> the key did not exist, and the user has complete control over the window=
 of
>> time / state lifecycle for detecting duplicates.  It also makes it =
really
>> easy to detect and take action (alert=3F) when you DO see a duplicate, =
or
>> make memory tradeoffs within an error bound using a sketch algorithm.  =
The
>> kafka simple consumer is insanely complex, if possible I think it would =
be
>> better (and vastly more flexible) to get reliability using the =
primitives
>> that spark so elegantly provides.
>>
>> Cheers,
>>
>> Sean
>>
>>
>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>> hshreedharan@cloudera.com> wrote:
>> >
>> > Hi Dibyendu,
>> >
>> > Thanks for the details on the implementation. But I still do not =
believe
>> > that it is no duplicates - what they achieve is that the same batch =
is
>> > processed exactly the same way every time (but see it may be =
processed
>> more
>> > than once) - so it depends on the operation being idempotent. I =
believe
>> > Trident uses ZK to keep track of the transactions - a batch can be
>> > processed multiple times in failure scenarios (for example, the
>> transaction
>> > is processed but before ZK is updated the machine fails, causing a =
=22new=22
>> > node to process it again).
>> >
>> > I don't think it is impossible to do this in Spark Streaming as well =
and
>> > I'd be really interested in working on it at some point in the near
>> future.
>> >
>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>> > dibyendu.bhattachary@gmail.com> wrote:
>> >
>> >> Hi,
>> >>
>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The =
Storm
>> >> Trident has done the exact-once guarantee by processing the tuple in =
a
>> >> batch  and assigning same transaction-id for a given batch . The =
replay
>> for
>> >> a given batch with a transaction-id will have exact same set of =
tuples
>> and
>> >> replay of batches happen in exact same order before the failure.
>> >>
>> >> Having this paradigm, if downstream system process data for a given
>> batch
>> >> for having a given transaction-id , and if during failure if same =
batch
>> is
>> >> again emitted , you can check if same transaction-id is already
>> processed
>> >> or not and hence can guarantee exact once semantics.
>> >>
>> >> And this can only be achieved in Spark if we use Low Level Kafka
>> consumer
>> >> API to process the offsets. This low level Kafka Consumer (
>> >> https://github.com/dibbhatt/kafka-spark-consumer) has implemented =
the
>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the =
Kafka
>> >> related logic has been taken from Storm-Kafka spout and which =
manages
>> all
>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>> managements.
>> >>
>> >> Presently this Consumer maintains that during Receiver failure, it =
will
>> >> re-emit the exact same Block with same set of messages . Every =
message
>> have
>> >> the details of its partition, offset and topic related details which =
can
>> >> tackle the SPARK-3146.
>> >>
>> >> As this Low Level consumer has complete control over the Kafka =
Offsets ,
>> >> we can implement Trident like feature on top of it like having
>> implement a
>> >> transaction-id for a given block , and re-emit the same block with =
same
>> set
>> >> of message during Driver failure.
>> >>
>> >> Regards,
>> >> Dibyendu
>> >>
>> >>
>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.=
com>
>> >> wrote:
>> >>>
>> >>> Hi all,
>> >>>
>> >>> I agree with Hari that Strong exact-once semantics is very hard to
>> >>> guarantee, especially in the failure situation. From my =
understanding
>> even
>> >>> current implementation of ReliableKafkaReceiver cannot fully =
guarantee
>> the
>> >>> exact once semantics once failed, first is the ordering of data
>> replaying
>> >>> from last checkpoint, this is hard to guarantee when multiple
>> partitions
>> >>> are injected in; second is the design complexity of achieving this,
>> you can
>> >>> refer to the Kafka Spout in Trident, we have to dig into the very
>> details
>> >>> of Kafka metadata management system to achieve this, not to say
>> rebalance
>> >>> and fault-tolerance.
>> >>>
>> >>> Thanks
>> >>> Jerry
>> >>>
>> >>> -----Original Message-----
>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.=
groups@gmail.com]
>> >>> Sent: Friday, December 19, 2014 5:57 AM
>> >>> To: Cody Koeninger
>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>> >>> Subject: Re: Which committers care about Kafka=3F
>> >>>
>> >>> But idempotency is not that easy t achieve sometimes. A strong only
>> once
>> >>> semantic through a proper API would  be superuseful; but I'm not
>> implying
>> >>> this is easy to achieve.
>> >>> On 18 Dec 2014 21:52, =22Cody Koeninger=22 <cody@koeninger.org> =
wrote:
>> >>>
>> >>>> If the downstream store for the output data is idempotent or
>> >>>> transactional, and that downstream store also is the system of =
record
>> >>>> for kafka offsets, then you have exactly-once semantics.  Commit
>> >>>> offsets with / after the data is stored.  On any failure, restart =
from
>> >>> the last committed offsets.
>> >>>>
>> >>>> Yes, this approach is biased towards the etl-like use cases rather
>> >>>> than near-realtime-analytics use cases.
>> >>>>
>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>> >>>> hshreedharan@cloudera.com
>> >>>>> wrote:
>> >>>>>
>> >>>>> I get what you are saying. But getting exactly once right is an
>> >>>>> extremely hard problem - especially in presence of failure. The
>> >>>>> issue is failures
>> >>>> can
>> >>>>> happen in a bunch of places. For example, before the notification =
of
>> >>>>> downstream store being successful reaches the receiver that =
updates
>> >>>>> the offsets, the node fails. The store was successful, but
>> >>>>> duplicates came in either way. This is something worth discussing =
by
>> >>>>> itself - but without uuids etc this might not really be solved =
even
>> >>> when you think it is.
>> >>>>>
>> >>>>> Anyway, I will look at the links. Even I am interested in all of =
the
>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>> >>>>> delivery,
>> >>>> but
>> >>>>> I doubt the latter is really possible to guarantee - though I =
really
>> >>>> would
>> >>>>> love to have that!
>> >>>>>
>> >>>>> Thanks,
>> >>>>> Hari
>> >>>>>
>> >>>>>
>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>> >>>>> <cody@koeninger.org>
>> >>>>> wrote:
>> >>>>>
>> >>>>>> Thanks for the replies.
>> >>>>>>
>> >>>>>> Regarding skipping WAL, it's not just about optimization.  If =
you
>> >>>>>> actually want exactly-once semantics, you need control of kafka
>> >>>>>> offsets
>> >>>> as
>> >>>>>> well, including the ability to not use zookeeper as the system =
of
>> >>>>>> record for offsets.  Kafka already is a reliable system that has
>> >>>>>> strong
>> >>>> ordering
>> >>>>>> guarantees (within a partition) and does not mandate the use of
>> >>>> zookeeper
>> >>>>>> to store offsets.  I think there should be a spark api that acts =
as
>> >>>>>> a
>> >>>> very
>> >>>>>> simple intermediary between Kafka and the user's choice of
>> >>>>>> downstream
>> >>>> store.
>> >>>>>>
>> >>>>>> Take a look at the links I posted - if there's already been 2
>> >>>> independent
>> >>>>>> implementations of the idea, chances are it's something people =
need.
>> >>>>>>
>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>> >>>>>> hshreedharan@cloudera.com> wrote:
>> >>>>>>>
>> >>>>>>> Hi Cody,
>> >>>>>>>
>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
>> >>>>>>> something pretty simple and lightweight for that one.
>> >>>>>>>
>> >>>>>>> For the Kafka DStream skipping the WAL implementation - this is
>> >>>>>>> something I discussed with TD a few weeks ago. Though it is a =
good
>> >>>> idea to
>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>> >>>> optimization. For
>> >>>>>>> that reason, we must be careful in implementation. There are a
>> >>>>>>> couple
>> >>>> of
>> >>>>>>> issues that we need to ensure works properly - specifically
>> >>> ordering.
>> >>>> To
>> >>>>>>> ensure we pull messages from different topics and partitions in
>> >>>>>>> the
>> >>>> same
>> >>>>>>> order after failure, we=E2=80=99d still have to persist the =
metadata to
>> >>>>>>> HDFS
>> >>>> (or
>> >>>>>>> some other system) - this metadata must contain the order of
>> >>>>>>> messages consumed, so we know how to re-read the messages. I am
>> >>>>>>> planning to
>> >>>> explore
>> >>>>>>> this once I have some time (probably in Jan). In addition, we =
must
>> >>>>>>> also ensure bucketing functions work fine as well. I will file =
a
>> >>>>>>> placeholder jira for this one.
>> >>>>>>>
>> >>>>>>> I also wrote an API to write data back to Kafka a while back -
>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that =
this
>> >>>>>>> will get pulled in soon, as this is something I know people want=
.
>> >>>>>>> I am open
>> >>>> to
>> >>>>>>> feedback on that - anything that I can do to make it better.
>> >>>>>>>
>> >>>>>>> Thanks,
>> >>>>>>> Hari
>> >>>>>>>
>> >>>>>>>
>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>> >>>>>>> <pwendell@gmail.com>
>> >>>>>>> wrote:
>> >>>>>>>
>> >>>>>>>> Hey Cody,
>> >>>>>>>>
>> >>>>>>>> Thanks for reaching out with this. The lead on streaming is TD =
-
>> >>>>>>>> he is traveling this week though so I can respond a bit. To =
the
>> >>>>>>>> high level point of whether Kafka is important - it definitely
>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support for
>> >>>>>>>> Kafka is something we generally want in Spark and not a library=
.
>> >>>>>>>> In some cases IIRC there were user libraries that used =
unstable
>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be =
able
>> >>>>>>>> to use newer Kakfa versions. This is a high level impression =
only
>> >>>>>>>> though, I haven't talked to TD about this recently so it's =
worth
>> >>> revisiting given the developments in Kafka.
>> >>>>>>>>
>> >>>>>>>> Please do bring things up like this on the dev list if there =
are
>> >>>>>>>> blockers for your usage - thanks for pinging it.
>> >>>>>>>>
>> >>>>>>>> - Patrick
>> >>>>>>>>
>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>> >>>>>>>> <cody@koeninger.org>
>> >>>>>>>> wrote:
>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to get
>> >>>>>>>>> some long-standing Kafka related issues resolved=3F
>> >>>>>>>>>
>> >>>>>>>>> The existing api is not sufficiently safe nor flexible for =
our
>> >>>>>>>> production
>> >>>>>>>>> use. I don't think we're alone in this viewpoint, because =
I've
>> >>>>>>>>> seen several different patches and libraries to fix the same
>> >>>>>>>>> things we've
>> >>>>>>>> been
>> >>>>>>>>> running into.
>> >>>>>>>>>
>> >>>>>>>>> Regarding flexibility
>> >>>>>>>>>
>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>> >>>>>>>>>
>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent of
>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>> >>>>>>>>> ourselves, then found
>> >>>>>>>> that
>> >>>>>>>>> PR and have been running it in production. We wouldn't be =
able
>> >>>>>>>>> to
>> >>>> get
>> >>>>>>>> our
>> >>>>>>>>> jobs done without it. It also allows users to solve a whole
>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitrary
>> >>>>>>>>> delay of
>> >>>>>>>> messages, etc).
>> >>>>>>>>>
>> >>>>>>>>> Regarding safety, I understand the motivation behind
>> >>>>>>>>> WriteAheadLog
>> >>>> as
>> >>>>>>>> a
>> >>>>>>>>> general solution for streaming unreliable sources, but Kafka
>> >>>>>>>>> already
>> >>>>>>>> is a
>> >>>>>>>>> reliable source. I think there's a need for an api that =
treats
>> >>>>>>>>> it as such. Even aside from the performance issues of
>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>> >>>>>>>>> write-ahead log in hdfs, I
>> >>>> need
>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>> >>>>>>>>> failures
>> >>>> that
>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for =
instance).
>> >>>>>>>>>
>> >>>>>>>>> I've got an implementation i've been using
>> >>>>>>>>>
>> >>>>>>>>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/ka=
f
>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>> >>>>>>>>>
>> >>>>>>>>> Tresata has something similar at
>> >>>>>>>> https://github.com/tresata/spark-kafka,
>> >>>>>>>>> and I know there were earlier attempts based on Storm code.
>> >>>>>>>>>
>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries rather
>> >>>>>>>>> than
>> >>>>>>>> patches
>> >>>>>>>>> to Spark is problematic, because large portions of the
>> >>>> implementation
>> >>>>>>>> are
>> >>>>>>>>> private[spark].
>> >>>>>>>>>
>> >>>>>>>>> I'd like to help, but i need to know whose attention to get.
>> >>>>>>>>
>> >>>>>>>> ---------------------------------------------------------------=
--
>> >>>>>>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org =
For
>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>> >>>>>>>>
>> >>>>>>>>
>> >>>>>>>
>> >>>>>
>> >>>>
>> >>>
>> >>
>>
>>
------Nodemailer-0.5.0-?=_1-1419026224707--

From dev-return-10879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 22:33:20 2014
Return-Path: <dev-return-10879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02B1610440
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 22:33:19 +0000 (UTC)
Received: (qmail 77868 invoked by uid 500); 19 Dec 2014 22:33:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77793 invoked by uid 500); 19 Dec 2014 22:33:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77781 invoked by uid 99); 19 Dec 2014 22:33:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 22:33:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 22:32:51 +0000
Received: by mail-ob0-f180.google.com with SMTP id wp4so14574032obc.11
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 14:32:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=GODuVBpzkRNzVKnFYJ/cvjix8KYnRSIeiOf7Sge1JIY=;
        b=h6Dr/EtCWAQk840UxeMCjAvIMwiP5k4LxtIAw/wnOqc1k8TCLK4SSSBgd0moJyaS+X
         zFg0p0s3bxeTlotYlGmJdJ4RprCnR1CBLbk8xn/DMEG5L1e9RwARCDeR1kv0EGFiyP9+
         tiRTytO0+rqxSIY7aOT486xyAfRRZE9vc59UvJivEudAqqE8urUHsb9DaNyKUyCD2HWS
         FO2pu6uLwXD4ITDN9cbIrz30wlDSJ7uS5///NxJdPs7C528qBe848Gb4RYYu1pZE6DW/
         Nz1g5mTuFKZDMyDbsb9i0+22X165cSLLbkvWS5kOnpWm+xpsqCCTiLUFm79Ba5bmjIfw
         lz5g==
X-Gm-Message-State: ALoCoQn8pm2pWRNLWVPsdF6P32uuYa7btZtk/wS752S/I5bFIGa0V5Y/355f6GEVolNoPuMEWkL3
MIME-Version: 1.0
X-Received: by 10.60.123.14 with SMTP id lw14mr6178921oeb.31.1419028349185;
 Fri, 19 Dec 2014 14:32:29 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Fri, 19 Dec 2014 14:32:29 -0800 (PST)
In-Reply-To: <1419026224285.80417be1@Nodemailer>
References: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
	<1419026224285.80417be1@Nodemailer>
Date: Fri, 19 Dec 2014 16:32:29 -0600
Message-ID: <CAKWX9VW42TNGt0NOYSmHtMHDoBEzsAtDv3XSfBPz0LKzpu=0NA@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: Sean McNamara <sean.mcnamara@webtrends.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, ShaoSaisai <saisai.shao@intel.com>
Content-Type: multipart/alternative; boundary=047d7b5d47d8ae0429050a994858
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d47d8ae0429050a994858
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That KafkaRDD code is dead simple.

Given a user specified map

(topic1, partition0) -> (startingOffset, endingOffset)
(topic1, partition1) -> (startingOffset, endingOffset)
...
turn each one of those entries into a partition of an rdd, using the simple
consumer.
That's it.  No recovery logic, no state, nothing - for any failures, bail
on the rdd and let it retry.
Spark stays out of the business of being a distributed database.

The client code does any transformation it wants, then stores the data and
offsets.  There are two ways of doing this, either based on idempotence or
a transactional data store.

For idempotent stores:

1.manipulate data
2.save data to store
3.save ending offsets to the same store

If you fail between 2 and 3, the offsets haven't been stored, you start
again at the same beginning offsets, do the same calculations in the same
order, overwrite the same data, all is good.


For transactional stores:

1. manipulate data
2. begin transaction
3. save data to the store
4. save offsets
5. commit transaction

If you fail before 5, the transaction rolls back.  To make this less
heavyweight, you can write the data outside the transaction and then update
a pointer to the current data inside the transaction.


Again, spark has nothing much to do with guaranteeing exactly once.  In
fact, the current streaming api actively impedes my ability to do the
above.  I'm just suggesting providing an api that doesn't get in the way of
exactly-once.





On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <hshreedharan@cloudera.co=
m
> wrote:

> Can you explain your basic algorithm for the once-only-delivery? It is
> quite a bit of very Kafka-specific code, that would take more time to rea=
d
> than I can currently afford? If you can explain your algorithm a bit, it
> might help.
>
> Thanks,
> Hari
>
>
> On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
>
>>
>> The problems you guys are discussing come from trying to store state in
>> spark, so don't do that.  Spark isn't a distributed database.
>>
>> Just map kafka partitions directly to rdds, llet user code specify the
>> range of offsets explicitly, and let them be in charge of committing
>> offsets.
>>
>> Using the simple consumer isn't that bad, I'm already using this in
>> production with the code I linked to, and tresata apparently has been as
>> well.  Again, for everyone saying this is impossible, have you read eith=
er
>> of those implementations and looked at the approach?
>>
>>
>>
>> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>> Sean.McNamara@webtrends.com> wrote:
>>
>>> Please feel free to correct me if I=E2=80=99m wrong, but I think the ex=
actly
>>> once spark streaming semantics can easily be solved using updateStateBy=
Key.
>>> Make the key going into updateStateByKey be a hash of the event, or plu=
ck
>>> off some uuid from the message.  The updateFunc would only emit the mes=
sage
>>> if the key did not exist, and the user has complete control over the wi=
ndow
>>> of time / state lifecycle for detecting duplicates.  It also makes it
>>> really easy to detect and take action (alert?) when you DO see a duplic=
ate,
>>> or make memory tradeoffs within an error bound using a sketch algorithm=
.
>>> The kafka simple consumer is insanely complex, if possible I think it w=
ould
>>> be better (and vastly more flexible) to get reliability using the
>>> primitives that spark so elegantly provides.
>>>
>>> Cheers,
>>>
>>> Sean
>>>
>>>
>>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>>> hshreedharan@cloudera.com> wrote:
>>> >
>>> > Hi Dibyendu,
>>> >
>>> > Thanks for the details on the implementation. But I still do not
>>> believe
>>> > that it is no duplicates - what they achieve is that the same batch i=
s
>>> > processed exactly the same way every time (but see it may be processe=
d
>>> more
>>> > than once) - so it depends on the operation being idempotent. I belie=
ve
>>> > Trident uses ZK to keep track of the transactions - a batch can be
>>> > processed multiple times in failure scenarios (for example, the
>>> transaction
>>> > is processed but before ZK is updated the machine fails, causing a
>>> "new"
>>> > node to process it again).
>>> >
>>> > I don't think it is impossible to do this in Spark Streaming as well
>>> and
>>> > I'd be really interested in working on it at some point in the near
>>> future.
>>> >
>>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>>> > dibyendu.bhattachary@gmail.com> wrote:
>>> >
>>> >> Hi,
>>> >>
>>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The Stor=
m
>>> >> Trident has done the exact-once guarantee by processing the tuple in=
 a
>>> >> batch  and assigning same transaction-id for a given batch . The
>>> replay for
>>> >> a given batch with a transaction-id will have exact same set of
>>> tuples and
>>> >> replay of batches happen in exact same order before the failure.
>>> >>
>>> >> Having this paradigm, if downstream system process data for a given
>>> batch
>>> >> for having a given transaction-id , and if during failure if same
>>> batch is
>>> >> again emitted , you can check if same transaction-id is already
>>> processed
>>> >> or not and hence can guarantee exact once semantics.
>>> >>
>>> >> And this can only be achieved in Spark if we use Low Level Kafka
>>> consumer
>>> >> API to process the offsets. This low level Kafka Consumer (
>>> >> https://github.com/dibbhatt/kafka-spark-consumer) has implemented th=
e
>>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the
>>> Kafka
>>> >> related logic has been taken from Storm-Kafka spout and which manage=
s
>>> all
>>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>>> managements.
>>> >>
>>> >> Presently this Consumer maintains that during Receiver failure, it
>>> will
>>> >> re-emit the exact same Block with same set of messages . Every
>>> message have
>>> >> the details of its partition, offset and topic related details which
>>> can
>>> >> tackle the SPARK-3146.
>>> >>
>>> >> As this Low Level consumer has complete control over the Kafka
>>> Offsets ,
>>> >> we can implement Trident like feature on top of it like having
>>> implement a
>>> >> transaction-id for a given block , and re-emit the same block with
>>> same set
>>> >> of message during Driver failure.
>>> >>
>>> >> Regards,
>>> >> Dibyendu
>>> >>
>>> >>
>>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.com=
>
>>> >> wrote:
>>> >>>
>>> >>> Hi all,
>>> >>>
>>> >>> I agree with Hari that Strong exact-once semantics is very hard to
>>> >>> guarantee, especially in the failure situation. From my
>>> understanding even
>>> >>> current implementation of ReliableKafkaReceiver cannot fully
>>> guarantee the
>>> >>> exact once semantics once failed, first is the ordering of data
>>> replaying
>>> >>> from last checkpoint, this is hard to guarantee when multiple
>>> partitions
>>> >>> are injected in; second is the design complexity of achieving this,
>>> you can
>>> >>> refer to the Kafka Spout in Trident, we have to dig into the very
>>> details
>>> >>> of Kafka metadata management system to achieve this, not to say
>>> rebalance
>>> >>> and fault-tolerance.
>>> >>>
>>> >>> Thanks
>>> >>> Jerry
>>> >>>
>>> >>> -----Original Message-----
>>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups@gm=
ail.com]
>>> >>> Sent: Friday, December 19, 2014 5:57 AM
>>> >>> To: Cody Koeninger
>>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>>> >>> Subject: Re: Which committers care about Kafka?
>>> >>>
>>> >>> But idempotency is not that easy t achieve sometimes. A strong only
>>> once
>>> >>> semantic through a proper API would  be superuseful; but I'm not
>>> implying
>>> >>> this is easy to achieve.
>>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote:
>>> >>>
>>> >>>> If the downstream store for the output data is idempotent or
>>> >>>> transactional, and that downstream store also is the system of
>>> record
>>> >>>> for kafka offsets, then you have exactly-once semantics.  Commit
>>> >>>> offsets with / after the data is stored.  On any failure, restart
>>> from
>>> >>> the last committed offsets.
>>> >>>>
>>> >>>> Yes, this approach is biased towards the etl-like use cases rather
>>> >>>> than near-realtime-analytics use cases.
>>> >>>>
>>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>> >>>> hshreedharan@cloudera.com
>>> >>>>> wrote:
>>> >>>>>
>>> >>>>> I get what you are saying. But getting exactly once right is an
>>> >>>>> extremely hard problem - especially in presence of failure. The
>>> >>>>> issue is failures
>>> >>>> can
>>> >>>>> happen in a bunch of places. For example, before the notification
>>> of
>>> >>>>> downstream store being successful reaches the receiver that updat=
es
>>> >>>>> the offsets, the node fails. The store was successful, but
>>> >>>>> duplicates came in either way. This is something worth discussing
>>> by
>>> >>>>> itself - but without uuids etc this might not really be solved ev=
en
>>> >>> when you think it is.
>>> >>>>>
>>> >>>>> Anyway, I will look at the links. Even I am interested in all of
>>> the
>>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>>> >>>>> delivery,
>>> >>>> but
>>> >>>>> I doubt the latter is really possible to guarantee - though I
>>> really
>>> >>>> would
>>> >>>>> love to have that!
>>> >>>>>
>>> >>>>> Thanks,
>>> >>>>> Hari
>>> >>>>>
>>> >>>>>
>>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>> >>>>> <cody@koeninger.org>
>>> >>>>> wrote:
>>> >>>>>
>>> >>>>>> Thanks for the replies.
>>> >>>>>>
>>> >>>>>> Regarding skipping WAL, it's not just about optimization.  If yo=
u
>>> >>>>>> actually want exactly-once semantics, you need control of kafka
>>> >>>>>> offsets
>>> >>>> as
>>> >>>>>> well, including the ability to not use zookeeper as the system o=
f
>>> >>>>>> record for offsets.  Kafka already is a reliable system that has
>>> >>>>>> strong
>>> >>>> ordering
>>> >>>>>> guarantees (within a partition) and does not mandate the use of
>>> >>>> zookeeper
>>> >>>>>> to store offsets.  I think there should be a spark api that acts
>>> as
>>> >>>>>> a
>>> >>>> very
>>> >>>>>> simple intermediary between Kafka and the user's choice of
>>> >>>>>> downstream
>>> >>>> store.
>>> >>>>>>
>>> >>>>>> Take a look at the links I posted - if there's already been 2
>>> >>>> independent
>>> >>>>>> implementations of the idea, chances are it's something people
>>> need.
>>> >>>>>>
>>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>> >>>>>> hshreedharan@cloudera.com> wrote:
>>> >>>>>>>
>>> >>>>>>> Hi Cody,
>>> >>>>>>>
>>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
>>> >>>>>>> something pretty simple and lightweight for that one.
>>> >>>>>>>
>>> >>>>>>> For the Kafka DStream skipping the WAL implementation - this is
>>> >>>>>>> something I discussed with TD a few weeks ago. Though it is a
>>> good
>>> >>>> idea to
>>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>> >>>> optimization. For
>>> >>>>>>> that reason, we must be careful in implementation. There are a
>>> >>>>>>> couple
>>> >>>> of
>>> >>>>>>> issues that we need to ensure works properly - specifically
>>> >>> ordering.
>>> >>>> To
>>> >>>>>>> ensure we pull messages from different topics and partitions in
>>> >>>>>>> the
>>> >>>> same
>>> >>>>>>> order after failure, we=E2=80=99d still have to persist the met=
adata to
>>> >>>>>>> HDFS
>>> >>>> (or
>>> >>>>>>> some other system) - this metadata must contain the order of
>>> >>>>>>> messages consumed, so we know how to re-read the messages. I am
>>> >>>>>>> planning to
>>> >>>> explore
>>> >>>>>>> this once I have some time (probably in Jan). In addition, we
>>> must
>>> >>>>>>> also ensure bucketing functions work fine as well. I will file =
a
>>> >>>>>>> placeholder jira for this one.
>>> >>>>>>>
>>> >>>>>>> I also wrote an API to write data back to Kafka a while back -
>>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that
>>> this
>>> >>>>>>> will get pulled in soon, as this is something I know people wan=
t.
>>> >>>>>>> I am open
>>> >>>> to
>>> >>>>>>> feedback on that - anything that I can do to make it better.
>>> >>>>>>>
>>> >>>>>>> Thanks,
>>> >>>>>>> Hari
>>> >>>>>>>
>>> >>>>>>>
>>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>> >>>>>>> <pwendell@gmail.com>
>>> >>>>>>> wrote:
>>> >>>>>>>
>>> >>>>>>>> Hey Cody,
>>> >>>>>>>>
>>> >>>>>>>> Thanks for reaching out with this. The lead on streaming is TD=
 -
>>> >>>>>>>> he is traveling this week though so I can respond a bit. To th=
e
>>> >>>>>>>> high level point of whether Kafka is important - it definitely
>>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support for
>>> >>>>>>>> Kafka is something we generally want in Spark and not a librar=
y.
>>> >>>>>>>> In some cases IIRC there were user libraries that used unstabl=
e
>>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
>>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be abl=
e
>>> >>>>>>>> to use newer Kakfa versions. This is a high level impression
>>> only
>>> >>>>>>>> though, I haven't talked to TD about this recently so it's wor=
th
>>> >>> revisiting given the developments in Kafka.
>>> >>>>>>>>
>>> >>>>>>>> Please do bring things up like this on the dev list if there a=
re
>>> >>>>>>>> blockers for your usage - thanks for pinging it.
>>> >>>>>>>>
>>> >>>>>>>> - Patrick
>>> >>>>>>>>
>>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>> >>>>>>>> <cody@koeninger.org>
>>> >>>>>>>> wrote:
>>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to get
>>> >>>>>>>>> some long-standing Kafka related issues resolved?
>>> >>>>>>>>>
>>> >>>>>>>>> The existing api is not sufficiently safe nor flexible for ou=
r
>>> >>>>>>>> production
>>> >>>>>>>>> use. I don't think we're alone in this viewpoint, because I'v=
e
>>> >>>>>>>>> seen several different patches and libraries to fix the same
>>> >>>>>>>>> things we've
>>> >>>>>>>> been
>>> >>>>>>>>> running into.
>>> >>>>>>>>>
>>> >>>>>>>>> Regarding flexibility
>>> >>>>>>>>>
>>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>> >>>>>>>>>
>>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent of
>>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>> >>>>>>>>> ourselves, then found
>>> >>>>>>>> that
>>> >>>>>>>>> PR and have been running it in production. We wouldn't be abl=
e
>>> >>>>>>>>> to
>>> >>>> get
>>> >>>>>>>> our
>>> >>>>>>>>> jobs done without it. It also allows users to solve a whole
>>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitrary
>>> >>>>>>>>> delay of
>>> >>>>>>>> messages, etc).
>>> >>>>>>>>>
>>> >>>>>>>>> Regarding safety, I understand the motivation behind
>>> >>>>>>>>> WriteAheadLog
>>> >>>> as
>>> >>>>>>>> a
>>> >>>>>>>>> general solution for streaming unreliable sources, but Kafka
>>> >>>>>>>>> already
>>> >>>>>>>> is a
>>> >>>>>>>>> reliable source. I think there's a need for an api that treat=
s
>>> >>>>>>>>> it as such. Even aside from the performance issues of
>>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>>> >>>>>>>>> write-ahead log in hdfs, I
>>> >>>> need
>>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>>> >>>>>>>>> failures
>>> >>>> that
>>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>>> instance).
>>> >>>>>>>>>
>>> >>>>>>>>> I've got an implementation i've been using
>>> >>>>>>>>>
>>> >>>>>>>>>
>>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>> >>>>>>>>>
>>> >>>>>>>>> Tresata has something similar at
>>> >>>>>>>> https://github.com/tresata/spark-kafka,
>>> >>>>>>>>> and I know there were earlier attempts based on Storm code.
>>> >>>>>>>>>
>>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries rather
>>> >>>>>>>>> than
>>> >>>>>>>> patches
>>> >>>>>>>>> to Spark is problematic, because large portions of the
>>> >>>> implementation
>>> >>>>>>>> are
>>> >>>>>>>>> private[spark].
>>> >>>>>>>>>
>>> >>>>>>>>> I'd like to help, but i need to know whose attention to get.
>>> >>>>>>>>
>>> >>>>>>>>
>>> -----------------------------------------------------------------
>>> >>>>>>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For
>>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>>> >>>>>>>>
>>> >>>>>>>>
>>> >>>>>>>
>>> >>>>>
>>> >>>>
>>> >>>
>>> >>
>>>
>>>
>>
>

--047d7b5d47d8ae0429050a994858--

From dev-return-10880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 19 22:38:07 2014
Return-Path: <dev-return-10880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C21210476
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 19 Dec 2014 22:38:07 +0000 (UTC)
Received: (qmail 92764 invoked by uid 500); 19 Dec 2014 22:38:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92694 invoked by uid 500); 19 Dec 2014 22:38:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92683 invoked by uid 99); 19 Dec 2014 22:38:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 22:38:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 19 Dec 2014 22:38:01 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so3220957wiv.1
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 14:36:34 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=lorENuhQM9wIKy7y7ICbsoei4aSnY4JsBd/OkjlN+ms=;
        b=Kwwcz8xwiwgeMMPUqTsr28R4EJ7sUzeHXm3DA/hB4IXsfPrzHSSQdOQZI0iI01+xov
         e7HWIYoqDFltMoDEroVi+A6I7I/bI6kd2WNcdWMC/7eMk/rPkjJvGAqcEn5DMPmr3JG+
         pB+OP5PiMshSyL5fEjlsiOsrdjJkROgybcJ0YDTDHQAoBWFVwmjHrGL+gilP8rDKff6x
         5dJ+6o2FINXxCZkEhewyXJ3B9coJeCl7RVS3/HGcyvrWhSl7WUMa2ZiGLt+49LFxQCNq
         CJtpGnVrJi0lfCxbYEydAnBwegbrsm5SuqB4EZNP/aj1x4urjNHx31PmLG0pkP37xGMy
         TXDg==
X-Gm-Message-State: ALoCoQmlDU+F8e6NG4o9P/Awo7WNMAriCwa1E/8IlztkrgRh4bTfsynt1Y9oxP/rWdfMfPUG/oKM
MIME-Version: 1.0
X-Received: by 10.195.12.35 with SMTP id en3mr19232357wjd.129.1419028594444;
 Fri, 19 Dec 2014 14:36:34 -0800 (PST)
Received: by 10.216.134.193 with HTTP; Fri, 19 Dec 2014 14:36:34 -0800 (PST)
X-Originating-IP: [92.109.128.8]
In-Reply-To: <CAKWX9VW42TNGt0NOYSmHtMHDoBEzsAtDv3XSfBPz0LKzpu=0NA@mail.gmail.com>
References: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
	<1419026224285.80417be1@Nodemailer>
	<CAKWX9VW42TNGt0NOYSmHtMHDoBEzsAtDv3XSfBPz0LKzpu=0NA@mail.gmail.com>
Date: Fri, 19 Dec 2014 17:36:34 -0500
Message-ID: <CANx3uAi44S_H_qVouAQcbgNDWLnygPt+fiJQe8FT5P2j80qomg@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Koert Kuipers <koert@tresata.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Sean McNamara <sean.mcnamara@webtrends.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, ShaoSaisai <saisai.shao@intel.com>
Content-Type: multipart/alternative; boundary=047d7bfcf7884c6483050a9957ee
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf7884c6483050a9957ee
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

yup, we at tresata do the idempotent store the same way. very simple
approach.

On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org> wrote:
>
> That KafkaRDD code is dead simple.
>
> Given a user specified map
>
> (topic1, partition0) -> (startingOffset, endingOffset)
> (topic1, partition1) -> (startingOffset, endingOffset)
> ...
> turn each one of those entries into a partition of an rdd, using the simp=
le
> consumer.
> That's it.  No recovery logic, no state, nothing - for any failures, bail
> on the rdd and let it retry.
> Spark stays out of the business of being a distributed database.
>
> The client code does any transformation it wants, then stores the data an=
d
> offsets.  There are two ways of doing this, either based on idempotence o=
r
> a transactional data store.
>
> For idempotent stores:
>
> 1.manipulate data
> 2.save data to store
> 3.save ending offsets to the same store
>
> If you fail between 2 and 3, the offsets haven't been stored, you start
> again at the same beginning offsets, do the same calculations in the same
> order, overwrite the same data, all is good.
>
>
> For transactional stores:
>
> 1. manipulate data
> 2. begin transaction
> 3. save data to the store
> 4. save offsets
> 5. commit transaction
>
> If you fail before 5, the transaction rolls back.  To make this less
> heavyweight, you can write the data outside the transaction and then upda=
te
> a pointer to the current data inside the transaction.
>
>
> Again, spark has nothing much to do with guaranteeing exactly once.  In
> fact, the current streaming api actively impedes my ability to do the
> above.  I'm just suggesting providing an api that doesn't get in the way =
of
> exactly-once.
>
>
>
>
>
> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
> hshreedharan@cloudera.com
> > wrote:
>
> > Can you explain your basic algorithm for the once-only-delivery? It is
> > quite a bit of very Kafka-specific code, that would take more time to
> read
> > than I can currently afford? If you can explain your algorithm a bit, i=
t
> > might help.
> >
> > Thanks,
> > Hari
> >
> >
> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
> > wrote:
> >
> >>
> >> The problems you guys are discussing come from trying to store state i=
n
> >> spark, so don't do that.  Spark isn't a distributed database.
> >>
> >> Just map kafka partitions directly to rdds, llet user code specify the
> >> range of offsets explicitly, and let them be in charge of committing
> >> offsets.
> >>
> >> Using the simple consumer isn't that bad, I'm already using this in
> >> production with the code I linked to, and tresata apparently has been =
as
> >> well.  Again, for everyone saying this is impossible, have you read
> either
> >> of those implementations and looked at the approach?
> >>
> >>
> >>
> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
> >> Sean.McNamara@webtrends.com> wrote:
> >>
> >>> Please feel free to correct me if I=E2=80=99m wrong, but I think the =
exactly
> >>> once spark streaming semantics can easily be solved using
> updateStateByKey.
> >>> Make the key going into updateStateByKey be a hash of the event, or
> pluck
> >>> off some uuid from the message.  The updateFunc would only emit the
> message
> >>> if the key did not exist, and the user has complete control over the
> window
> >>> of time / state lifecycle for detecting duplicates.  It also makes it
> >>> really easy to detect and take action (alert?) when you DO see a
> duplicate,
> >>> or make memory tradeoffs within an error bound using a sketch
> algorithm.
> >>> The kafka simple consumer is insanely complex, if possible I think it
> would
> >>> be better (and vastly more flexible) to get reliability using the
> >>> primitives that spark so elegantly provides.
> >>>
> >>> Cheers,
> >>>
> >>> Sean
> >>>
> >>>
> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
> >>> hshreedharan@cloudera.com> wrote:
> >>> >
> >>> > Hi Dibyendu,
> >>> >
> >>> > Thanks for the details on the implementation. But I still do not
> >>> believe
> >>> > that it is no duplicates - what they achieve is that the same batch
> is
> >>> > processed exactly the same way every time (but see it may be
> processed
> >>> more
> >>> > than once) - so it depends on the operation being idempotent. I
> believe
> >>> > Trident uses ZK to keep track of the transactions - a batch can be
> >>> > processed multiple times in failure scenarios (for example, the
> >>> transaction
> >>> > is processed but before ZK is updated the machine fails, causing a
> >>> "new"
> >>> > node to process it again).
> >>> >
> >>> > I don't think it is impossible to do this in Spark Streaming as wel=
l
> >>> and
> >>> > I'd be really interested in working on it at some point in the near
> >>> future.
> >>> >
> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
> >>> > dibyendu.bhattachary@gmail.com> wrote:
> >>> >
> >>> >> Hi,
> >>> >>
> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The
> Storm
> >>> >> Trident has done the exact-once guarantee by processing the tuple
> in a
> >>> >> batch  and assigning same transaction-id for a given batch . The
> >>> replay for
> >>> >> a given batch with a transaction-id will have exact same set of
> >>> tuples and
> >>> >> replay of batches happen in exact same order before the failure.
> >>> >>
> >>> >> Having this paradigm, if downstream system process data for a give=
n
> >>> batch
> >>> >> for having a given transaction-id , and if during failure if same
> >>> batch is
> >>> >> again emitted , you can check if same transaction-id is already
> >>> processed
> >>> >> or not and hence can guarantee exact once semantics.
> >>> >>
> >>> >> And this can only be achieved in Spark if we use Low Level Kafka
> >>> consumer
> >>> >> API to process the offsets. This low level Kafka Consumer (
> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has implemented
> the
> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the
> >>> Kafka
> >>> >> related logic has been taken from Storm-Kafka spout and which
> manages
> >>> all
> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
> >>> managements.
> >>> >>
> >>> >> Presently this Consumer maintains that during Receiver failure, it
> >>> will
> >>> >> re-emit the exact same Block with same set of messages . Every
> >>> message have
> >>> >> the details of its partition, offset and topic related details whi=
ch
> >>> can
> >>> >> tackle the SPARK-3146.
> >>> >>
> >>> >> As this Low Level consumer has complete control over the Kafka
> >>> Offsets ,
> >>> >> we can implement Trident like feature on top of it like having
> >>> implement a
> >>> >> transaction-id for a given block , and re-emit the same block with
> >>> same set
> >>> >> of message during Driver failure.
> >>> >>
> >>> >> Regards,
> >>> >> Dibyendu
> >>> >>
> >>> >>
> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
> saisai.shao@intel.com>
> >>> >> wrote:
> >>> >>>
> >>> >>> Hi all,
> >>> >>>
> >>> >>> I agree with Hari that Strong exact-once semantics is very hard t=
o
> >>> >>> guarantee, especially in the failure situation. From my
> >>> understanding even
> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
> >>> guarantee the
> >>> >>> exact once semantics once failed, first is the ordering of data
> >>> replaying
> >>> >>> from last checkpoint, this is hard to guarantee when multiple
> >>> partitions
> >>> >>> are injected in; second is the design complexity of achieving thi=
s,
> >>> you can
> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the very
> >>> details
> >>> >>> of Kafka metadata management system to achieve this, not to say
> >>> rebalance
> >>> >>> and fault-tolerance.
> >>> >>>
> >>> >>> Thanks
> >>> >>> Jerry
> >>> >>>
> >>> >>> -----Original Message-----
> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups@=
gmail.com]
> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
> >>> >>> To: Cody Koeninger
> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
> >>> >>> Subject: Re: Which committers care about Kafka?
> >>> >>>
> >>> >>> But idempotency is not that easy t achieve sometimes. A strong on=
ly
> >>> once
> >>> >>> semantic through a proper API would  be superuseful; but I'm not
> >>> implying
> >>> >>> this is easy to achieve.
> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org> wrote=
:
> >>> >>>
> >>> >>>> If the downstream store for the output data is idempotent or
> >>> >>>> transactional, and that downstream store also is the system of
> >>> record
> >>> >>>> for kafka offsets, then you have exactly-once semantics.  Commit
> >>> >>>> offsets with / after the data is stored.  On any failure, restar=
t
> >>> from
> >>> >>> the last committed offsets.
> >>> >>>>
> >>> >>>> Yes, this approach is biased towards the etl-like use cases rath=
er
> >>> >>>> than near-realtime-analytics use cases.
> >>> >>>>
> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> >>> >>>> hshreedharan@cloudera.com
> >>> >>>>> wrote:
> >>> >>>>>
> >>> >>>>> I get what you are saying. But getting exactly once right is an
> >>> >>>>> extremely hard problem - especially in presence of failure. The
> >>> >>>>> issue is failures
> >>> >>>> can
> >>> >>>>> happen in a bunch of places. For example, before the notificati=
on
> >>> of
> >>> >>>>> downstream store being successful reaches the receiver that
> updates
> >>> >>>>> the offsets, the node fails. The store was successful, but
> >>> >>>>> duplicates came in either way. This is something worth discussi=
ng
> >>> by
> >>> >>>>> itself - but without uuids etc this might not really be solved
> even
> >>> >>> when you think it is.
> >>> >>>>>
> >>> >>>>> Anyway, I will look at the links. Even I am interested in all o=
f
> >>> the
> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
> >>> >>>>> delivery,
> >>> >>>> but
> >>> >>>>> I doubt the latter is really possible to guarantee - though I
> >>> really
> >>> >>>> would
> >>> >>>>> love to have that!
> >>> >>>>>
> >>> >>>>> Thanks,
> >>> >>>>> Hari
> >>> >>>>>
> >>> >>>>>
> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
> >>> >>>>> <cody@koeninger.org>
> >>> >>>>> wrote:
> >>> >>>>>
> >>> >>>>>> Thanks for the replies.
> >>> >>>>>>
> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.  If
> you
> >>> >>>>>> actually want exactly-once semantics, you need control of kafk=
a
> >>> >>>>>> offsets
> >>> >>>> as
> >>> >>>>>> well, including the ability to not use zookeeper as the system
> of
> >>> >>>>>> record for offsets.  Kafka already is a reliable system that h=
as
> >>> >>>>>> strong
> >>> >>>> ordering
> >>> >>>>>> guarantees (within a partition) and does not mandate the use o=
f
> >>> >>>> zookeeper
> >>> >>>>>> to store offsets.  I think there should be a spark api that ac=
ts
> >>> as
> >>> >>>>>> a
> >>> >>>> very
> >>> >>>>>> simple intermediary between Kafka and the user's choice of
> >>> >>>>>> downstream
> >>> >>>> store.
> >>> >>>>>>
> >>> >>>>>> Take a look at the links I posted - if there's already been 2
> >>> >>>> independent
> >>> >>>>>> implementations of the idea, chances are it's something people
> >>> need.
> >>> >>>>>>
> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> >>> >>>>>> hshreedharan@cloudera.com> wrote:
> >>> >>>>>>>
> >>> >>>>>>> Hi Cody,
> >>> >>>>>>>
> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
> >>> >>>>>>> something pretty simple and lightweight for that one.
> >>> >>>>>>>
> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation - this =
is
> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it is a
> >>> good
> >>> >>>> idea to
> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
> >>> >>>> optimization. For
> >>> >>>>>>> that reason, we must be careful in implementation. There are =
a
> >>> >>>>>>> couple
> >>> >>>> of
> >>> >>>>>>> issues that we need to ensure works properly - specifically
> >>> >>> ordering.
> >>> >>>> To
> >>> >>>>>>> ensure we pull messages from different topics and partitions =
in
> >>> >>>>>>> the
> >>> >>>> same
> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist the m=
etadata to
> >>> >>>>>>> HDFS
> >>> >>>> (or
> >>> >>>>>>> some other system) - this metadata must contain the order of
> >>> >>>>>>> messages consumed, so we know how to re-read the messages. I =
am
> >>> >>>>>>> planning to
> >>> >>>> explore
> >>> >>>>>>> this once I have some time (probably in Jan). In addition, we
> >>> must
> >>> >>>>>>> also ensure bucketing functions work fine as well. I will fil=
e
> a
> >>> >>>>>>> placeholder jira for this one.
> >>> >>>>>>>
> >>> >>>>>>> I also wrote an API to write data back to Kafka a while back =
-
> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that
> >>> this
> >>> >>>>>>> will get pulled in soon, as this is something I know people
> want.
> >>> >>>>>>> I am open
> >>> >>>> to
> >>> >>>>>>> feedback on that - anything that I can do to make it better.
> >>> >>>>>>>
> >>> >>>>>>> Thanks,
> >>> >>>>>>> Hari
> >>> >>>>>>>
> >>> >>>>>>>
> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
> >>> >>>>>>> <pwendell@gmail.com>
> >>> >>>>>>> wrote:
> >>> >>>>>>>
> >>> >>>>>>>> Hey Cody,
> >>> >>>>>>>>
> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming is
> TD -
> >>> >>>>>>>> he is traveling this week though so I can respond a bit. To
> the
> >>> >>>>>>>> high level point of whether Kafka is important - it definite=
ly
> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support for
> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
> library.
> >>> >>>>>>>> In some cases IIRC there were user libraries that used
> unstable
> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to stabili=
ze
> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be
> able
> >>> >>>>>>>> to use newer Kakfa versions. This is a high level impression
> >>> only
> >>> >>>>>>>> though, I haven't talked to TD about this recently so it's
> worth
> >>> >>> revisiting given the developments in Kafka.
> >>> >>>>>>>>
> >>> >>>>>>>> Please do bring things up like this on the dev list if there
> are
> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
> >>> >>>>>>>>
> >>> >>>>>>>> - Patrick
> >>> >>>>>>>>
> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
> >>> >>>>>>>> <cody@koeninger.org>
> >>> >>>>>>>> wrote:
> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to ge=
t
> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
> >>> >>>>>>>>>
> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible for
> our
> >>> >>>>>>>> production
> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, because
> I've
> >>> >>>>>>>>> seen several different patches and libraries to fix the sam=
e
> >>> >>>>>>>>> things we've
> >>> >>>>>>>> been
> >>> >>>>>>>>> running into.
> >>> >>>>>>>>>
> >>> >>>>>>>>> Regarding flexibility
> >>> >>>>>>>>>
> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
> >>> >>>>>>>>>
> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent o=
f
> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
> >>> >>>>>>>>> ourselves, then found
> >>> >>>>>>>> that
> >>> >>>>>>>>> PR and have been running it in production. We wouldn't be
> able
> >>> >>>>>>>>> to
> >>> >>>> get
> >>> >>>>>>>> our
> >>> >>>>>>>>> jobs done without it. It also allows users to solve a whole
> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitrar=
y
> >>> >>>>>>>>> delay of
> >>> >>>>>>>> messages, etc).
> >>> >>>>>>>>>
> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
> >>> >>>>>>>>> WriteAheadLog
> >>> >>>> as
> >>> >>>>>>>> a
> >>> >>>>>>>>> general solution for streaming unreliable sources, but Kafk=
a
> >>> >>>>>>>>> already
> >>> >>>>>>>> is a
> >>> >>>>>>>>> reliable source. I think there's a need for an api that
> treats
> >>> >>>>>>>>> it as such. Even aside from the performance issues of
> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
> >>> >>>>>>>>> write-ahead log in hdfs, I
> >>> >>>> need
> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
> >>> >>>>>>>>> failures
> >>> >>>> that
> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
> >>> instance).
> >>> >>>>>>>>>
> >>> >>>>>>>>> I've got an implementation i've been using
> >>> >>>>>>>>>
> >>> >>>>>>>>>
> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
> >>> >>>>>>>>>
> >>> >>>>>>>>> Tresata has something similar at
> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
> >>> >>>>>>>>> and I know there were earlier attempts based on Storm code.
> >>> >>>>>>>>>
> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries rath=
er
> >>> >>>>>>>>> than
> >>> >>>>>>>> patches
> >>> >>>>>>>>> to Spark is problematic, because large portions of the
> >>> >>>> implementation
> >>> >>>>>>>> are
> >>> >>>>>>>>> private[spark].
> >>> >>>>>>>>>
> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to get=
.
> >>> >>>>>>>>
> >>> >>>>>>>>
> >>> -----------------------------------------------------------------
> >>> >>>>>>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.or=
g
> >>> For
> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
> >>> >>>>>>>>
> >>> >>>>>>>>
> >>> >>>>>>>
> >>> >>>>>
> >>> >>>>
> >>> >>>
> >>> >>
> >>>
> >>>
> >>
> >
>

--047d7bfcf7884c6483050a9957ee--

From dev-return-10881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 20 00:21:35 2014
Return-Path: <dev-return-10881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E512109C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 20 Dec 2014 00:21:35 +0000 (UTC)
Received: (qmail 11319 invoked by uid 500); 20 Dec 2014 00:21:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11235 invoked by uid 500); 20 Dec 2014 00:21:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11223 invoked by uid 99); 20 Dec 2014 00:21:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 00:21:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jayunit100.apache@gmail.com designates 209.85.212.193 as permitted sender)
Received: from [209.85.212.193] (HELO mail-wi0-f193.google.com) (209.85.212.193)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 00:21:27 +0000
Received: by mail-wi0-f193.google.com with SMTP id bs8so1130354wib.8
        for <dev@spark.apache.org>; Fri, 19 Dec 2014 16:21:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=fMRy+pKXDRBiWPtH+xZ/jDjczCRvuuufZRpHi57teeI=;
        b=MoM8As2kUgnha1jNY4XKRsh3ZBrckLwXPlGbSRiGoCVDZG/5SKdrcctS9gv9RyIahm
         dL3gYpi6JNJ9bt1pjoouhtA/vY+CfOV++egOzX37mQ90PalmqxD8EEVo/Lddx3lbBkBn
         dp8hM2uq5CGkptNGhhsBvW7wI9sEOIGVJ4k5GTSuOrgl5wf3ZarZDi9Vp++XNIy/YtEF
         c4P0uMOV3JhPzZIhnztZIMrINxyFMNssjkMueASHXkcVLUUWMbKXSK2foe3Z+mvlIv1K
         YIzRED3gWwZ/HX29Yi7EVWBT2vxOgQhmOKuPGV0t6KmVRpTq2zcQf/L3ifuxoYCxdqlx
         6W2w==
MIME-Version: 1.0
X-Received: by 10.194.88.131 with SMTP id bg3mr18679912wjb.99.1419034866438;
 Fri, 19 Dec 2014 16:21:06 -0800 (PST)
Received: by 10.27.126.135 with HTTP; Fri, 19 Dec 2014 16:21:06 -0800 (PST)
Date: Fri, 19 Dec 2014 19:21:06 -0500
Message-ID: <CACVCA=fMnR6Lnm9+dh3Qn7wdYVBf=O88Jb7+0qzqymt3daBr0Q@mail.gmail.com>
Subject: EndpointWriter : Dropping message failure ReliableDeliverySupervisor errors...
From: jay vyas <jayunit100.apache@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e010d849e2358e3050a9acdd8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e010d849e2358e3050a9acdd8
Content-Type: text/plain; charset=UTF-8

Hi spark.   Im trying to understand the akka debug messages when networking
doesnt work properly.  any hints would be great on this.

SIMPLE TESTS I RAN

- i tried a ping, works.
- i tried a telnet to the 7077 port of master, from slave, also works.

LOGS

1) On the master I see this WARN log buried:

ReliableDeliverySupervisor: Association with remote system
[akka.tcp://sparkWorker@s2.docker:45477] has failed, address is now gated
for [500] ms  Reason is: [Disassociated].

2) I also see a periodic, repeated ERROR message :

 ERROR EndpointWriter: dropping message [class
akka.actor.ActorSelectionMessage] for non-local recipient [Actor[akka.tcp://
sparkMaster@172.17.0.12:7077


Any idea what these folks mean?   From what i can tel, i can telnet from
s2.docker to my master server.

Any thoughts for more debugging of this would be appreciated! im out of
ideas for the time being ....

-- 
jay vyas

--089e010d849e2358e3050a9acdd8--

From dev-return-10882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 20 12:30:50 2014
Return-Path: <dev-return-10882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FEBA969F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 20 Dec 2014 12:30:50 +0000 (UTC)
Received: (qmail 53990 invoked by uid 500); 20 Dec 2014 12:30:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53918 invoked by uid 500); 20 Dec 2014 12:30:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53901 invoked by uid 99); 20 Dec 2014 12:30:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 12:30:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of putta.sreenivas@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 12:30:21 +0000
Received: by mail-ig0-f177.google.com with SMTP id z20so1984705igj.16
        for <dev@spark.apache.org>; Sat, 20 Dec 2014 04:30:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=vDmuUfrfdjOjjh5PsAwx7s7ySOjyXLfyumKUoTzV1b8=;
        b=W5gs7+Bw0B/WnCcpjNGso82T7+MAwAnrkTA9sH6NIF81hvcDChasHDihPCWu7P330S
         z/4MRMNEzOtr/lJyB6KnKlRwwxgA4/fAK9s6avuNM7SXdlt6uxa5SC0KEb+qF1vpFXh7
         yMU0h/DXXqXaaJF6kf0B27MHi2gkUEoNEwPkFAYIyJDrx6solmCl/0gIDuibPmuJbdTs
         WQ4ZtL149B5hNldWlljvFx0Ha8hbNryb4mcWnVTjBeLeSi27XgBUkKEFpmSEHpwZlA9g
         iTU4cfKHFJYkRkAhTFDBhqVI1lD2kL4B9QbmB/RQJ2/WWWm43mr+pKnpyYV6H/rwm9FE
         zXhQ==
MIME-Version: 1.0
X-Received: by 10.107.35.134 with SMTP id j128mr12182325ioj.4.1419078620529;
 Sat, 20 Dec 2014 04:30:20 -0800 (PST)
Received: by 10.64.96.9 with HTTP; Sat, 20 Dec 2014 04:30:20 -0800 (PST)
Date: Sat, 20 Dec 2014 18:00:20 +0530
Message-ID: <CAK-=xs6FsxBgtFpH3YvLF7uvQE_+3TEoNtFPEcYFGNE+z2pcXQ@mail.gmail.com>
Subject: Contribution in java
From: sreenivas putta <putta.sreenivas@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1140f5e015ca8d050aa4fd57
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140f5e015ca8d050aa4fd57
Content-Type: text/plain; charset=UTF-8

Hi,

I want to contribute for spark in java. Does it support java? please let me
know.

Thanks,
Sreenivas

--001a1140f5e015ca8d050aa4fd57--

From dev-return-10883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 20 16:34:14 2014
Return-Path: <dev-return-10883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E8DA49BAA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 20 Dec 2014 16:34:13 +0000 (UTC)
Received: (qmail 60661 invoked by uid 500); 20 Dec 2014 16:34:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60588 invoked by uid 500); 20 Dec 2014 16:34:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60577 invoked by uid 99); 20 Dec 2014 16:34:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 16:34:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 16:33:44 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so4607062wiv.5
        for <dev@spark.apache.org>; Sat, 20 Dec 2014 08:31:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=q/mTbSANNXaBVw0GLn+XsdUu1Ia17o41RnwcJ06BFPA=;
        b=PoubNrrP7hp1K3QQasiWYt2t8P3WO9OtFOEpyyz7coDqVBsL3BOCT1WcDCjnJq0m6R
         VtnopKxe33qbHRYqZMeswSEyVQO34VfY+Zrl+MN1cZsSH2gPfsZGFEHwfe9876+pWusL
         kyaR4Op5T/3Xfbjy2PrBeZaEr2RgEB6RyTQdKPN/ITk4zx0pZb+KCfa/z/AEak/38WK7
         NgZMhscuqptbIdheFZCFp4LWC74C2Md6TrtzgxP4ggksSFcwjF48qKpTzXEmYL4Ogl1W
         X6WLAD6gYHDsiQeWOMtbIcnGGPbRHYLgZhN5z2/0jExlRH1k4OmDdNa8GqO280k6dyG5
         ifxw==
X-Gm-Message-State: ALoCoQkry1DX8DhOsIkgQzFzxbqZMw9TiOlE+Jts9tRx+mOLovadZWznuWWNETGDyGbCh245x85C
MIME-Version: 1.0
X-Received: by 10.180.74.212 with SMTP id w20mr16075598wiv.22.1419093113551;
 Sat, 20 Dec 2014 08:31:53 -0800 (PST)
Received: by 10.216.134.193 with HTTP; Sat, 20 Dec 2014 08:31:53 -0800 (PST)
X-Originating-IP: [92.109.128.8]
In-Reply-To: <CAK-=xs6FsxBgtFpH3YvLF7uvQE_+3TEoNtFPEcYFGNE+z2pcXQ@mail.gmail.com>
References: <CAK-=xs6FsxBgtFpH3YvLF7uvQE_+3TEoNtFPEcYFGNE+z2pcXQ@mail.gmail.com>
Date: Sat, 20 Dec 2014 11:31:53 -0500
Message-ID: <CANx3uAiC4rBO9VWzK-VZX4pq3CsgY-419MUr+S-dy7Pir1Ltwg@mail.gmail.com>
Subject: Re: Contribution in java
From: Koert Kuipers <koert@tresata.com>
To: sreenivas putta <putta.sreenivas@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c7eb8efd5e6050aa85c8a
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c7eb8efd5e6050aa85c8a
Content-Type: text/plain; charset=UTF-8

yes it does. although the core of spark is written in scala it also
maintains java and python apis, and there is plenty of work for those to
contribute to.

On Sat, Dec 20, 2014 at 7:30 AM, sreenivas putta <putta.sreenivas@gmail.com>
wrote:
>
> Hi,
>
> I want to contribute for spark in java. Does it support java? please let me
> know.
>
> Thanks,
> Sreenivas
>

--f46d043c7eb8efd5e6050aa85c8a--

From dev-return-10884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 20 18:35:22 2014
Return-Path: <dev-return-10884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 40D4B9ED1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 20 Dec 2014 18:35:22 +0000 (UTC)
Received: (qmail 75077 invoked by uid 500); 20 Dec 2014 18:35:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75002 invoked by uid 500); 20 Dec 2014 18:35:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74990 invoked by uid 99); 20 Dec 2014 18:35:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 18:35:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vaquar.khan@gmail.com designates 209.85.213.53 as permitted sender)
Received: from [209.85.213.53] (HELO mail-yh0-f53.google.com) (209.85.213.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 20 Dec 2014 18:35:14 +0000
Received: by mail-yh0-f53.google.com with SMTP id i57so1291093yha.40
        for <dev@spark.apache.org>; Sat, 20 Dec 2014 10:34:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0H311N4CrrJ2QXc3N5Y5I7BPwVSaGtufBOoaNZtTSRg=;
        b=YgWDtbFTf8WF3cSWXx2goeolOdosVA3D9I1SgIMXMuww+h6MY0YeSoG9npxb56viI9
         juHTv9GFXBLbLX0TMS7gIb3WeOJKzL0oWqj2Rax0oLuLUfjHUqYKHaCf/MEq+1WJjRlG
         x5o2Pa+pdbAEFATLfIuJ7+60LazX/YTG4ekvF7vyAPU7vIkMs8rL2VawGtLDBf0X/hcV
         26FLIkWe7W3Sk2e/R8SBUYfHSMLv8KDyCd8dte62XSX7MeagTNEZStNgx/58LZJKU34J
         sMRPb7zRS8RcYkn6oGi6Nw4tfmIeh6Apgj8iGZ5xUerRv3kouK9aUAMGjg5iFI75BCtu
         cMHQ==
MIME-Version: 1.0
X-Received: by 10.236.228.198 with SMTP id f66mr11623557yhq.40.1419100449012;
 Sat, 20 Dec 2014 10:34:09 -0800 (PST)
Received: by 10.170.49.80 with HTTP; Sat, 20 Dec 2014 10:34:08 -0800 (PST)
In-Reply-To: <CAK-=xs6FsxBgtFpH3YvLF7uvQE_+3TEoNtFPEcYFGNE+z2pcXQ@mail.gmail.com>
References: <CAK-=xs6FsxBgtFpH3YvLF7uvQE_+3TEoNtFPEcYFGNE+z2pcXQ@mail.gmail.com>
Date: Sun, 21 Dec 2014 00:04:08 +0530
Message-ID: <CANF+cm+SHLdOxHtWW24+CUZ4zGK=auewzHgxZS5K96CSLSwpFA@mail.gmail.com>
Subject: Re: Contribution in java
From: vaquar khan <vaquar.khan@gmail.com>
To: sreenivas putta <putta.sreenivas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ef7c2a0109050aaa1220
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ef7c2a0109050aaa1220
Content-Type: text/plain; charset=ISO-8859-1

Hi Sreenivas,

Please read Spark doc first, everything mention in doc , without reading
doc how can you contribute ?

regards,
vaquar khan

On Sat, Dec 20, 2014 at 6:00 PM, sreenivas putta <putta.sreenivas@gmail.com>
wrote:

> Hi,
>
> I want to contribute for spark in java. Does it support java? please let me
> know.
>
> Thanks,
> Sreenivas
>



-- 
Regards,
Vaquar Khan
+91 830-851-1500

--001a1132ef7c2a0109050aaa1220--

From dev-return-10885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 21 00:26:38 2014
Return-Path: <dev-return-10885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E5D110324
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 21 Dec 2014 00:26:38 +0000 (UTC)
Received: (qmail 20118 invoked by uid 500); 21 Dec 2014 00:26:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20038 invoked by uid 500); 21 Dec 2014 00:26:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20026 invoked by uid 99); 21 Dec 2014 00:26:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 00:26:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jayunit100.apache@gmail.com designates 74.125.82.66 as permitted sender)
Received: from [74.125.82.66] (HELO mail-wg0-f66.google.com) (74.125.82.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 00:26:31 +0000
Received: by mail-wg0-f66.google.com with SMTP id y19so1112752wgg.1
        for <dev@spark.apache.org>; Sat, 20 Dec 2014 16:26:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Yxf3fo53vd8jcbUjRznX6McGdINJ++f4Z1nSysfWv3A=;
        b=uDsg5Zizr3p/fa9j5hBSiZE2PeG0qZRBBLO1koh86KGtofH2YtkHcCmFDdcq/aC80y
         9TZ02Ol6g2pzfPIVF6FCaFuCF0b698UWLuEUVVmpHsmdXqSll88PWfKLeH+0XyHJfdcu
         4Y+MpdF8QGBFcxoi15g+hQZQXuLeLhBLySGnh2oT9R9WFrsyrPWSiTUose9blmRLRCyq
         BG4eCBGNKSu/oxzFwcVYTKe3AEmSmJGWxr38jJRezAcpuZdnUEKuQoFTsjCFl4jn/2X0
         BgPPBPL090lctxyJoA09XeBTI1/uSfK6gwKFAZyDPvVMZOlTzsrKAq/KPMsN/I7V9Lnd
         tBHQ==
MIME-Version: 1.0
X-Received: by 10.194.236.1 with SMTP id uq1mr10347204wjc.28.1419121570152;
 Sat, 20 Dec 2014 16:26:10 -0800 (PST)
Received: by 10.27.126.135 with HTTP; Sat, 20 Dec 2014 16:26:09 -0800 (PST)
In-Reply-To: <CACVCA=fMnR6Lnm9+dh3Qn7wdYVBf=O88Jb7+0qzqymt3daBr0Q@mail.gmail.com>
References: <CACVCA=fMnR6Lnm9+dh3Qn7wdYVBf=O88Jb7+0qzqymt3daBr0Q@mail.gmail.com>
Date: Sat, 20 Dec 2014 19:26:10 -0500
Message-ID: <CACVCA=c0KOVGDaH40+D5ViSAqpi6Vsc-H9++-4YYT7KW4XOO0g@mail.gmail.com>
Subject: Re: EndpointWriter : Dropping message failure ReliableDeliverySupervisor
 errors...
From: jay vyas <jayunit100.apache@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013d14a2150868050aaefd1b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d14a2150868050aaefd1b
Content-Type: text/plain; charset=UTF-8

Hi folks.

In the end, I found that the problem was that I was using IP Addresses
instead of hostnames.

I guess, maybe,  reverse dns is a requirement for spark slave -> master
communications...  ?



On Fri, Dec 19, 2014 at 7:21 PM, jay vyas <jayunit100.apache@gmail.com>
wrote:

> Hi spark.   Im trying to understand the akka debug messages when
> networking doesnt work properly.  any hints would be great on this.
>
> SIMPLE TESTS I RAN
>
> - i tried a ping, works.
> - i tried a telnet to the 7077 port of master, from slave, also works.
>
> LOGS
>
> 1) On the master I see this WARN log buried:
>
> ReliableDeliverySupervisor: Association with remote system
> [akka.tcp://sparkWorker@s2.docker:45477] has failed, address is now gated
> for [500] ms  Reason is: [Disassociated].
>
> 2) I also see a periodic, repeated ERROR message :
>
>  ERROR EndpointWriter: dropping message [class
> akka.actor.ActorSelectionMessage] for non-local recipient [Actor[akka.tcp://
> sparkMaster@172.17.0.12:7077
>
>
> Any idea what these folks mean?   From what i can tel, i can telnet from
> s2.docker to my master server.
>
> Any thoughts for more debugging of this would be appreciated! im out of
> ideas for the time being ....
>
> --
> jay vyas
>



-- 
jay vyas

--089e013d14a2150868050aaefd1b--

From dev-return-10886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 21 08:42:19 2014
Return-Path: <dev-return-10886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB3F51089B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 21 Dec 2014 08:42:19 +0000 (UTC)
Received: (qmail 62469 invoked by uid 500); 21 Dec 2014 08:42:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62389 invoked by uid 500); 21 Dec 2014 08:42:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62373 invoked by uid 99); 21 Dec 2014 08:42:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 08:42:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 08:42:11 +0000
Received: by mail-ig0-f174.google.com with SMTP id hn15so2685680igb.13
        for <dev@spark.apache.org>; Sun, 21 Dec 2014 00:41:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=Dn+3BrSYY2CU6SQBLMOrdfCqssshzrjtbh3x9Wrw39U=;
        b=B+AXBLg/BbHMhtZmXA9ZmGMbdSIP78tm0KCqIHaBbwZH9tZv/joqHPpBr9Po97+okF
         JuP6cHTm4GJZ0ShzpEc8QzLSnKAsoT3jt1ZzcEN1PLbxrm5K+HD+X0uFU9C+lOBbgSSO
         zjEMz34iiIAJSicx8LJPa7j3LIrd2PSDeuinDfMMjxGV8PZJrcqTIjrVxYwxeMRcEsqJ
         c59P8JN2GP122IHUQdiRKEVKGr64s3AlwcGxD93uSMpfNYvTByjzurc6qanOtyH86m3D
         DtarLoJwSAeasKfI3NCvzpFkQk3vA+OQW472bqKQSVOgPSp0cd7ZXzTNwhZTHc3ginSq
         HXbg==
X-Received: by 10.50.66.200 with SMTP id h8mr11191458igt.20.1419151311350;
 Sun, 21 Dec 2014 00:41:51 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzdLf3LbeZR4p+EcjA1FjGJgHFhspAFwv=Zvin+3M+77dQ@mail.gmail.com>
 <etPan.53fc13d4.737b8ddc.9a@mbp-3.local> <CABPQxsubhEpY1k3udbk3xjJfZxANy8VXURnnchY4+ZK9zA8iQg@mail.gmail.com>
 <CAOhmDzeAO2QojA5pFGJ_4FiSjswG2KmVhhwZM=cczZ0GYFEZKA@mail.gmail.com> <CAOhmDzfRrwe41rGXKMv2RGjKqW-YW-2ZYr9BHvXRqnq+7rsWeg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 21 Dec 2014 08:41:50 +0000
Message-ID: <CAOhmDzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com>
Subject: Re: Handling stale PRs
To: Patrick Wendell <pwendell@gmail.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1a66cbb220050ab5e950
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1a66cbb220050ab5e950
Content-Type: text/plain; charset=UTF-8

Shout-out to Michael and other Spark SQL contributors for really trimming
down the number of open/stale Spark SQL PRs
<https://spark-prs.appspot.com/#sql>.

As of right now, the least recently updated open Spark SQL PR goes back
only 11 days.

Nice work!

Nick


On Mon Dec 08 2014 at 2:58:08 PM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> I recently came across this blog post, which reminded me of this thread.
>
> How to Discourage Open Source Contributions
> <http://danluu.com/discourage-oss/>
>
> We are currently at 320+ open PRs, many of which haven't been updated in
> over a month. We have quite a few PRs that haven't been touched in 3-5
> months.
>
> *If you have the time and interest, please hop on over to the Spark PR
> Dashboard <https://spark-prs.appspot.com/>, sort the PRs by
> least-recently-updated, and update them where you can.*
>
> I share the blog author's opinion that letting PRs go stale discourages
> contributions, especially from first-time contributors, and especially more
> so when the PR author is waiting on feedback from a committer or
> contributor.
>
> I've been thinking about simple ways to make it easier for all of us to
> chip in on controlling stale PRs in an incremental way. For starters, would
> it help if an automated email went out to the dev list once a week that a)
> reported the number of stale PRs, and b) directly linked to the 5 least
> recently updated PRs?
>
> Nick
>
> On Sat Aug 30 2014 at 3:41:39 AM Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> On Tue, Aug 26, 2014 at 2:02 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> it's actually precedurally difficult for us to close pull requests
>>
>>
>> Just an FYI: Seems like the GitHub-sanctioned work-around to having
>> issues-only permissions is to have a second, issues-only repository
>> <https://help.github.com/articles/issues-only-access-permissions>. Not a
>> very attractive work-around...
>>
>> Nick
>>
>

--047d7bdc1a66cbb220050ab5e950--

From dev-return-10887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 21 16:02:56 2014
Return-Path: <dev-return-10887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE1FF10E7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 21 Dec 2014 16:02:56 +0000 (UTC)
Received: (qmail 94872 invoked by uid 500); 21 Dec 2014 16:02:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94792 invoked by uid 500); 21 Dec 2014 16:02:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94780 invoked by uid 99); 21 Dec 2014 16:02:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 16:02:55 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of anant.asty@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 21 Dec 2014 16:02:29 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 2FF77E5D7E7
	for <dev@spark.apache.org>; Sun, 21 Dec 2014 08:02:29 -0800 (PST)
Date: Sun, 21 Dec 2014 09:02:27 -0700 (MST)
From: slcclimber <anant.asty@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419177747944-9874.post@n3.nabble.com>
In-Reply-To: <CABPQxssUOU8uJ_NUeKFdAhrfOCAbWQubt+0f71S4=eD_YEKJMg@mail.gmail.com>
References: <CABPQxstuqgycNfM1NvCU0qDUbPOtwybHqbp0L_f_71ZTM9+aeA@mail.gmail.com> <CAPn6-YTyF+6jsvdhp2FBJAnmoFAXVoRiDHctALvKRW3-g5sBwQ@mail.gmail.com> <CAMAsSdJUJPfTivbfFJnsi667nthyJK-ixDB01QteyBYFzUrL_w@mail.gmail.com> <CABPQxssUOU8uJ_NUeKFdAhrfOCAbWQubt+0f71S4=eD_YEKJMg@mail.gmail.com>
Subject: Re: Announcing Spark 1.2!
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Congratulations. This is quite exciting.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Announcing-Spark-1-2-tp9847p9874.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 02:44:04 2014
Return-Path: <dev-return-10888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5308C931
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 02:44:04 +0000 (UTC)
Received: (qmail 9432 invoked by uid 500); 22 Dec 2014 02:44:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9351 invoked by uid 500); 22 Dec 2014 02:44:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9340 invoked by uid 99); 22 Dec 2014 02:44:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 02:44:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ramana.gollamudi@huawei.com designates 119.145.14.64 as permitted sender)
Received: from [119.145.14.64] (HELO szxga01-in.huawei.com) (119.145.14.64)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 02:43:57 +0000
Received: from 172.24.2.119 (EHLO SZXEMA412-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id CGK57934;
	Mon, 22 Dec 2014 10:43:34 +0800 (CST)
Received: from BLREML405-HUB.china.huawei.com (10.20.4.41) by
 SZXEMA412-HUB.china.huawei.com (10.82.72.71) with Microsoft SMTP Server (TLS)
 id 14.3.158.1; Mon, 22 Dec 2014 10:43:32 +0800
Received: from BLREML502-MBX.china.huawei.com ([10.18.97.34]) by
 BLREML405-HUB.china.huawei.com ([10.20.4.41]) with mapi id 14.03.0158.001;
 Mon, 22 Dec 2014 08:13:26 +0530
From: Venkata ramana gollamudi <ramana.gollamudi@huawei.com>
To: dev <dev@spark.apache.org>
Subject: Data source interface for making multiple tables available for query
Thread-Topic: Data source interface for making multiple tables available for
 query
Thread-Index: AQHQHZEOAgERk89FM0K+2KK8Q1XMMg==
Date: Mon, 22 Dec 2014 02:43:25 +0000
Message-ID: <6545898BE5B3B545A96ECAE83927C38E2E50153C@blreml502-mbx>
Accept-Language: en-US, zh-CN
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.18.146.98]
Content-Type: multipart/alternative;
	boundary="_000_6545898BE5B3B545A96ECAE83927C38E2E50153Cblreml502mbx_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_6545898BE5B3B545A96ECAE83927C38E2E50153Cblreml502mbx_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Hi,

Data source ddl.scala, CREATE TEMPORARY TABLE makes one table at time avail=
able to temp tables, how about the case if multiple/all tables from some da=
ta source needs to be available for query, just like hive tables. I think w=
e also need that interface to connect such data sources. Please comment.

Regards,
Ramana

--_000_6545898BE5B3B545A96ECAE83927C38E2E50153Cblreml502mbx_--

From dev-return-10889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 04:12:31 2014
Return-Path: <dev-return-10889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0975ACA4A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 04:12:31 +0000 (UTC)
Received: (qmail 53286 invoked by uid 500); 22 Dec 2014 04:12:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53210 invoked by uid 500); 22 Dec 2014 04:12:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53198 invoked by uid 99); 22 Dec 2014 04:12:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 04:12:29 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wyphao.2007@163.com designates 220.181.13.91 as permitted sender)
Received: from [220.181.13.91] (HELO m13-91.163.com) (220.181.13.91)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 04:12:03 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=+f2RU
	8LyczgVNrBfh8DCBeQMVkp0ZJV77bl6SDDSuYY=; b=VTU1H5zEbMrGMgD0k6X8v
	vxjcIKE+iNT/iXokl5XFTE98kGDvI5COjw2joAHK8TVdwmF0nRBGHkbNYVaMoaBy
	74y5mjSPZF0V3L92OQCSgscarzEVR4OeuEVTNSvvaHAalL63qblRtcScNU3fnqjT
	UNyL3unK8kGkO8sz6o25lY=
Received: from wyphao.2007$163.com ( [211.151.238.52] ) by
 ajax-webmail-wmsvr91 (Coremail) ; Mon, 22 Dec 2014 12:10:18 +0800 (CST)
X-Originating-IP: [211.151.238.52]
Date: Mon, 22 Dec 2014 12:10:18 +0800 (CST)
From: "wyphao.2007" <wyphao.2007@163.com>
To: dev@spark.apache.org
Subject: Use mvn to build Spark 1.2.0  failed
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20140915(28949.6690) Copyright (c) 2002-2014 www.mailtech.cn 163com
X-CM-CTRLDATA: Tk6QwWZvb3Rlcl9odG09ODIwMTo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_390368_692354662.1419221418344"
MIME-Version: 1.0
Message-ID: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
X-CM-TRANSID:W8GowEAZJ0OrmZdUGowpAA--.1454W
X-CM-SenderInfo: xz1sxtbrosiiqx6rljoofrz/1tbiXAtqKFEALihdbQAAsy
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_390368_692354662.1419221418344
Content-Type: text/plain; charset=GBK
Content-Transfer-Encoding: base64
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------=_Part_390368_692354662.1419221418344--


From dev-return-10890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 10:25:13 2014
Return-Path: <dev-return-10890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17C219219
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 10:25:13 +0000 (UTC)
Received: (qmail 94028 invoked by uid 500); 22 Dec 2014 10:25:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93945 invoked by uid 500); 22 Dec 2014 10:25:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93932 invoked by uid 99); 22 Dec 2014 10:25:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 10:25:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 10:24:45 +0000
Received: by mail-wi0-f178.google.com with SMTP id em10so7426402wid.11
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 02:24:44 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=SgPaOLqLhNuTmgPralrxpxBBqM0uFvlnCPFAuyTIbpU=;
        b=E75ijOGhgg5z5Rq7fFRi5SBP15le2gGpS1TbNl4osiDoSWijveuAoh49Db6MimVqOe
         6GqMiRp7pB2dAusGsugc7Gi5SnhD1xGr38zBCt/VMQSIYHQj6EWazKnaezWTheV5znKI
         c4X39JuygaWRp4asGdR/e1U+C1RM33pr7v/1dtsHfdVPApCO9u3e0utnbC89fOAQ519c
         o/y8ucEhvyIocXJMK4l16qDSjhbHqE01vZGBa9tzAMkK9F8/tKDrl9MJXKmx+GJHGVKN
         AE3e5U//efnLNJpckuRdVf2sL77icL7kheALRRDiduQoteKDQj1hWdkpXgkD74Pa4l/b
         iwng==
X-Gm-Message-State: ALoCoQlc72ejHcELbgWijxHmcdQrCqn7TqutZ3k78HBi/nV5pt4KwPno2AldfjUXHfVx2wfPDUJP
X-Received: by 10.180.198.209 with SMTP id je17mr29427470wic.17.1419243884586;
 Mon, 22 Dec 2014 02:24:44 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Mon, 22 Dec 2014 02:24:24 -0800 (PST)
In-Reply-To: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 22 Dec 2014 10:24:24 +0000
Message-ID: <CAMAsSdLXA6MMWL6q7Tq=LKWO8c47MDqH_+zgehevbp5zncx71w@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
To: "wyphao.2007" <wyphao.2007@163.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I just tried the exact same command and do not see any error. Maybe
you can make sure you're starting from a clean extraction of the
distro, and check your environment. I'm on OSX, Maven 3.2, Java 8 but
I don't know that any of those would be relevant.

On Mon, Dec 22, 2014 at 4:10 AM, wyphao.2007 <wyphao.2007@163.com> wrote:
> Hi all, Today download Spark source from http://spark.apache.org/download=
s.html page, and I use
>
>
>  ./make-distribution.sh --tgz -Phadoop-2.2 -Pyarn -DskipTests -Dhadoop.ve=
rsion=3D2.2.0 -Phive
>
>
> to build the release, but I encountered an exception as follow:
>
>
> [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @=
 spark-parent ---
> [INFO] Source directory: /home/q/spark/spark-1.2.0/src/main/scala added.
> [INFO]
> [INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-pa=
rent ---
> [INFO] ------------------------------------------------------------------=
------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM .......................... FAILURE [1.015=
s]
> [INFO] Spark Project Networking .......................... SKIPPED
> [INFO] Spark Project Shuffle Streaming Service ........... SKIPPED
> [INFO] Spark Project Core ................................ SKIPPED
> [INFO] Spark Project Bagel ............................... SKIPPED
> [INFO] Spark Project GraphX .............................. SKIPPED
> [INFO] Spark Project Streaming ........................... SKIPPED
> [INFO] Spark Project Catalyst ............................ SKIPPED
> [INFO] Spark Project SQL ................................. SKIPPED
> [INFO] Spark Project ML Library .......................... SKIPPED
> [INFO] Spark Project Tools ............................... SKIPPED
> [INFO] Spark Project Hive ................................ SKIPPED
> [INFO] Spark Project REPL ................................ SKIPPED
> [INFO] Spark Project YARN Parent POM ..................... SKIPPED
> [INFO] Spark Project YARN Stable API ..................... SKIPPED
> [INFO] Spark Project Assembly ............................ SKIPPED
> [INFO] Spark Project External Twitter .................... SKIPPED
> [INFO] Spark Project External Flume Sink ................. SKIPPED
> [INFO] Spark Project External Flume ...................... SKIPPED
> [INFO] Spark Project External MQTT ....................... SKIPPED
> [INFO] Spark Project External ZeroMQ ..................... SKIPPED
> [INFO] Spark Project External Kafka ...................... SKIPPED
> [INFO] Spark Project Examples ............................ SKIPPED
> [INFO] Spark Project YARN Shuffle Service ................ SKIPPED
> [INFO] ------------------------------------------------------------------=
------
> [INFO] BUILD FAILURE
> [INFO] ------------------------------------------------------------------=
------
> [INFO] Total time: 1.644s
> [INFO] Finished at: Mon Dec 22 10:56:35 CST 2014
> [INFO] Final Memory: 21M/481M
> [INFO] ------------------------------------------------------------------=
------
> [ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-reso=
urces-plugin:1.5:process (default) on project spark-parent: Error finding r=
emote resources manifests: /home/q/spark/spark-1.2.0/target/maven-shared-ar=
chive-resources/META-INF/NOTICE (No such file or directory) -> [Help 1]
> [ERROR]
> [ERROR] To see the full stack trace of the errors, re-run Maven with the =
-e switch.
> [ERROR] Re-run Maven using the -X switch to enable full debug logging.
> [ERROR]
> [ERROR] For more information about the errors and possible solutions, ple=
ase read the following articles:
> [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExe=
cutionException
>
>
> but the NOTICE file is in the download spark release:
>
>
> [wyp@spark  /home/q/spark/spark-1.2.0]$ ll
> total 248
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 assembly
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 bagel
> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 bin
> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 conf
> -rw-rw-r-- 1 1000 1000   663 Dec 10 18:02 CONTRIBUTING.md
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 core
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 data
> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 dev
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 docker
> drwxrwxr-x 7 1000 1000  4096 Dec 10 18:02 docs
> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 ec2
> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 examples
> drwxrwxr-x 8 1000 1000  4096 Dec 10 18:02 external
> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 extras
> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 graphx
> -rw-rw-r-- 1 1000 1000 45242 Dec 10 18:02 LICENSE
> -rwxrwxr-x 1 1000 1000  7941 Dec 10 18:02 make-distribution.sh
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 mllib
> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 network
> -rw-rw-r-- 1 1000 1000 22559 Dec 10 18:02 NOTICE
> -rw-rw-r-- 1 1000 1000 49002 Dec 10 18:02 pom.xml
> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 project
> drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 python
> -rw-rw-r-- 1 1000 1000  3645 Dec 10 18:02 README.md
> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 repl
> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbin
> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbt
> -rw-rw-r-- 1 1000 1000  7804 Dec 10 18:02 scalastyle-config.xml
> drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 sql
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 streaming
> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 tools
> -rw-rw-r-- 1 1000 1000   838 Dec 10 18:02 tox.ini
> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 yarn
>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 13:46:54 2014
Return-Path: <dev-return-10891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D76C79C41
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 13:46:54 +0000 (UTC)
Received: (qmail 48950 invoked by uid 500); 22 Dec 2014 13:46:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48844 invoked by uid 500); 22 Dec 2014 13:46:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47384 invoked by uid 99); 22 Dec 2014 13:46:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 13:46:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of learnings.chitturi@gmail.com designates 209.85.216.195 as permitted sender)
Received: from [209.85.216.195] (HELO mail-qc0-f195.google.com) (209.85.216.195)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 13:46:24 +0000
Received: by mail-qc0-f195.google.com with SMTP id i8so1193320qcq.2;
        Mon, 22 Dec 2014 05:45:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ndyAcY5le3ZSaRrfWE3CiL8IQ3/zl6S807qPD5fP+PA=;
        b=HmO1ApbCVbjvq00vfUwuGbFlh+DHLKkEcA7t+tfdhY6DOH1aNSYv3X9hu2JNPZ/FWs
         +TWC6ng+YjZJ3VTm89jxsCL9r8DAJ4kiVsuDHCv5WpCEo54gggQIs3Nyb1C7e9kx619e
         TUzGVxAARqbc4jkKApa1Eh/FgiKUXq5Mjk/CIo0urzbOui312MDNsN6npzy+5U0enAGe
         CTSm1WP44JLofUtT/wQF7g+vdT0BHfg5WEKn8VpKXMS9gGVGvyvcDUHKuFaQ/+634Ygc
         fSAgTPUatfpP1BsszcEYo1OupQ6JrvHGbhejUwIl88dVC4GJS/0W84sj0/iW5ibhWT6g
         bqdQ==
MIME-Version: 1.0
X-Received: by 10.224.95.67 with SMTP id c3mr35955870qan.3.1419255928357; Mon,
 22 Dec 2014 05:45:28 -0800 (PST)
Received: by 10.229.155.1 with HTTP; Mon, 22 Dec 2014 05:45:28 -0800 (PST)
Date: Mon, 22 Dec 2014 19:15:28 +0530
Message-ID: <CABXsDPqcJm_1p__bJSCa-CJujs56iY-+oYKZ2+eod=DR0MKVgA@mail.gmail.com>
Subject: Spark exception when sending message to akka actor
From: Priya Ch <learnings.chitturi@gmail.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c356a07488a6050ace45f3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c356a07488a6050ace45f3
Content-Type: text/plain; charset=UTF-8

Hi All,

I have akka remote actors running on 2 nodes. I submitted spark application
from node1. In the spark code, in one of the rdd, i am sending message to
actor running on node1. My Spark code is as follows:




class ActorClient extends Actor with Serializable
{
  import context._

  val currentActor: ActorSelection =
context.system.actorSelection("akka.tcp://
ActorSystem@192.168.145.183:2551/user/MasterActor")
  implicit val timeout = Timeout(10 seconds)


  def receive =
  {
  case msg:String => { if(msg.contains("Spark"))
                       { currentActor ! msg
                         sender ! "Local"
                       }
                       else
                       {
                        println("Received.."+msg)
                        val future=currentActor ? msg
                        val result = Await.result(future,
timeout.duration).asInstanceOf[String]
                        if(result.contains("ACK"))
                              sender ! "OK"
                       }
                     }
  case PoisonPill => context.stop(self)
  }
}

object SparkExec extends Serializable
{

  implicit val timeout = Timeout(10 seconds)
   val actorSystem=ActorSystem("ClientActorSystem")
   val
actor=actorSystem.actorOf(Props(classOf[ActorClient]),name="ClientActor")

 def main(args:Array[String]) =
  {

     val conf = new SparkConf().setAppName("DeepLearningSpark")

     val sc=new SparkContext(conf)

    val
textrdd=sc.textFile("hdfs://IMPETUS-DSRV02:9000/deeplearning/sample24k.csv")
    val rdd1=textrddmap{ line => println("In Map...")

                                   val future = actor ? "Hello..Spark"
                                   val result =
Await.result(future,timeout.duration).asInstanceOf[String]
                                   if(result.contains("Local")){
                                     println("Recieved in map...."+result)
                                      //actorSystem.shutdown
                                      }
                                      (10)
                                     }


     val rdd2=rdd1.map{ x =>
                             val future=actor ? "Done"
                             val result = Await.result(future,
timeout.duration).asInstanceOf[String]
                              if(result.contains("OK"))
                              {
                               actorSystem.stop(remoteActor)
                               actorSystem.shutdown
                              }
                             (2) }
     rdd2.saveAsTextFile("/home/padma/SparkAkkaOut")
}

}

In my ActorClientActor, through actorSelection, identifying the remote
actor and sending the message. Once the messages are sent, in *rdd2*, after
receiving ack from remote actor, i am killing the actor ActorClient and
shutting down the ActorSystem.

The above code is throwing the following exception:




14/12/22 19:04:36 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 0.0
(TID 1, IMPETUS-DSRV05.impetus.co.in):
java.lang.ExceptionInInitializerError:
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:166)
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:159)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:984)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722)
14/12/22 19:04:36 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0
(TID 0, IMPETUS-DSRV05.impetus.co.in): java.lang.NoClassDefFoundError:
Could not initialize class com.impetus.spark.SparkExec$
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:166)
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:159)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:984)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722)


Please anyone could help me on this ? My concern is i want to send message
to an actor within a spark rdd and after sending the messages the
actorsystem need to be shutdown.

Thanks,
Padma Ch

--001a11c356a07488a6050ace45f3--

From dev-return-10892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 16:34:17 2014
Return-Path: <dev-return-10892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 80436103EA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 16:34:17 +0000 (UTC)
Received: (qmail 45781 invoked by uid 500); 22 Dec 2014 16:34:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45667 invoked by uid 500); 22 Dec 2014 16:34:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43902 invoked by uid 99); 22 Dec 2014 16:34:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 16:34:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 16:33:48 +0000
Received: by mail-wi0-f169.google.com with SMTP id r20so10965299wiv.4;
        Mon, 22 Dec 2014 08:33:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:cc:content-type;
        bh=Fk7o3w+cvqRGFl+QgrSC8AfdGxUwiQ+S/ZDVBVwIrzQ=;
        b=Xgkv6xqR7Nt7zl9PymIUiUFPDGtcudpyVxrgz2fHgMbE53nN4EhtmE0kNlG5tSyZLA
         SoQfWsqheelxaCjyydrdlRG5KGZRrt9o7YrpGV7Mzv478/A2PONkxJKVwvDCcckup7pL
         cV//4s1aTRBprWTQEq5R0AR/5RlNQlaJxRs9d1SC1k/GGdKv9TGaB3x8gMDO0G270ix9
         k74vKakHTYXr84/U9aQ1kJhz03pnMxnmPD5COUgDL21VP0+U4DLNLV4zLEpLVs+vlSZd
         aIs/8hW+eIylHW14VwJxz6upcSnw+sjwaJc0OLXTFYNsRJ78ZWtcLvUCQIyp38xQBlUS
         OOFQ==
X-Received: by 10.194.71.203 with SMTP id x11mr43565811wju.131.1419266027982;
 Mon, 22 Dec 2014 08:33:47 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.189.8 with HTTP; Mon, 22 Dec 2014 08:33:17 -0800 (PST)
From: Gerard Maas <gerard.maas@gmail.com>
Date: Mon, 22 Dec 2014 17:33:17 +0100
Message-ID: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
Subject: Tuning Spark Streaming jobs
To: spark users <user@spark.apache.org>, dev@spark.apache.org, 
	Tathagata Das <tathagata.das1565@gmail.com>
Cc: andy petrella <andy.petrella@gmail.com>, Xavier Tordoir <xavier@silicocloud.eu>, 
	=?UTF-8?Q?Lieven_Gesqui=C3=A8re?= <lieven.gesquiere@virdata.com>
Content-Type: multipart/alternative; boundary=047d7bfd0bd2709702050ad09f01
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0bd2709702050ad09f01
Content-Type: text/plain; charset=UTF-8

Hi,

After facing issues with the performance of some of our Spark Streaming
 jobs, we invested quite some effort figuring out the factors that affect
the performance characteristics of a Streaming job. We  defined an
empirical model that helps us reason about Streaming jobs and applied it to
tune the jobs in order to maximize throughput.

We have summarized our findings in a blog post with the intention of
collecting feedback and hoping that it is useful to other Spark Streaming
users facing similar issues.

 http://www.virdata.com/tuning-spark/

Your feedback is welcome.

With kind regards,

Gerard.
Data Processing Team Lead
Virdata.com
@maasg

--047d7bfd0bd2709702050ad09f01--

From dev-return-10893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 17:01:32 2014
Return-Path: <dev-return-10893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 439CB1062B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 17:01:32 +0000 (UTC)
Received: (qmail 49758 invoked by uid 500); 22 Dec 2014 17:01:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49669 invoked by uid 500); 22 Dec 2014 17:01:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49632 invoked by uid 99); 22 Dec 2014 17:01:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 17:01:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tnachen@gmail.com designates 209.85.192.171 as permitted sender)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 17:01:24 +0000
Received: by mail-pd0-f171.google.com with SMTP id y13so6191985pdi.30;
        Mon, 22 Dec 2014 09:01:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=87vORWhNsl7oQNTJH4GjSOS1jRYeCRC7zSUlCbIRTE4=;
        b=NtqSi0+zCpp5rP02oXL9RhSEyL8e2kNrgLSjsa0M4XDLIC2sm4/WOE7dF5r0NZoev6
         V80FHqIhF5EbakfgkBVAz/dxyxvA+eJ2zy0nMTE9T0VL6/6h2g2H4HuxCg0mhvwOhDUP
         AEuz3axL59IEPVyED7PKcFl+qOd3Y7v0WbvEh6+T4S1aX52xepsahts+E6dsWz3Qm5B0
         sVXF/772xDGh/LOWPwHx0oTFyv6thUEzj2t2HdD8FTkeUmI4a4MLBjKd5KH+t/zJH/Pw
         PjPCe4f/VRwHT4mNPjY4dDF0l0U7Kh2Y5YkEtiXm4If1nl092nY4KETQBRiKnyxZVEZz
         dJrg==
X-Received: by 10.66.164.200 with SMTP id ys8mr36701753pab.130.1419267664250;
        Mon, 22 Dec 2014 09:01:04 -0800 (PST)
Received: from ?IPv6:2601:9:6180:8c4:483:4f87:568e:7df5? ([2601:9:6180:8c4:483:4f87:568e:7df5])
        by mx.google.com with ESMTPSA id cr6sm10006238pdb.32.2014.12.22.09.01.03
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 22 Dec 2014 09:01:03 -0800 (PST)
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Tuning Spark Streaming jobs
From: Timothy Chen <tnachen@gmail.com>
X-Mailer: iPhone Mail (12B440)
In-Reply-To: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
Date: Mon, 22 Dec 2014 09:01:02 -0800
Cc: spark users <user@spark.apache.org>,
 "dev@spark.apache.org" <dev@spark.apache.org>,
 Tathagata Das <tathagata.das1565@gmail.com>,
 andy petrella <andy.petrella@gmail.com>,
 Xavier Tordoir <xavier@silicocloud.eu>,
 =?GB2312?Q?Lieven_Gesqui=A8=A8re?= <lieven.gesquiere@virdata.com>
Content-Transfer-Encoding: quoted-printable
Message-Id: <59D086C1-3E76-4754-9770-64273DAF2E54@gmail.com>
References: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
To: Gerard Maas <gerard.maas@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Gerard,

Really nice guide!

I'm particularly interested in the Mesos scheduling side to more evenly dist=
ribute cores across cluster.

I wonder if you are using coarse grain mode or fine grain mode?=20

I'm making changes to the spark mesos scheduler and I think we can propose a=
 best way to achieve what you mentioned.

Tim

Sent from my iPhone

> On Dec 22, 2014, at 8:33 AM, Gerard Maas <gerard.maas@gmail.com> wrote:
>=20
> Hi,
>=20
> After facing issues with the performance of some of our Spark Streaming
> jobs, we invested quite some effort figuring out the factors that affect
> the performance characteristics of a Streaming job. We  defined an
> empirical model that helps us reason about Streaming jobs and applied it t=
o
> tune the jobs in order to maximize throughput.
>=20
> We have summarized our findings in a blog post with the intention of
> collecting feedback and hoping that it is useful to other Spark Streaming
> users facing similar issues.
>=20
> http://www.virdata.com/tuning-spark/
>=20
> Your feedback is welcome.
>=20
> With kind regards,
>=20
> Gerard.
> Data Processing Team Lead
> Virdata.com
> @maasg

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 17:47:42 2014
Return-Path: <dev-return-10894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 684D01084D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 17:47:42 +0000 (UTC)
Received: (qmail 53263 invoked by uid 500); 22 Dec 2014 17:47:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53182 invoked by uid 500); 22 Dec 2014 17:47:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53168 invoked by uid 99); 22 Dec 2014 17:47:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 17:47:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 17:47:35 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so22179554obc.0
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 09:46:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=JGoS3DTnm2nmwp8pEeKrl2/aA/8RFAwDkPTP/2D1Swg=;
        b=KZaIyM+YST+DwMP7EOl/VUe6LNBm21f66+59pDdRKKdtmYRuLw0R0dRAxovQZeRpjh
         0N+NlBWt80LLJe+ZH89QpSLyY4wYh1Gs8Bspp1bMMAEQFnrYw8+6j9qHzQT/MeAHC9vc
         M5M8rtkb7luSk7ll4pbaC7Ft7Ap85Lwd58xUDQv6JCDJdvrYfQOOK+Q5ElktHH2bN3dk
         mcvff29XdxmHBzk3QxLNjNTdNYUm3Ya9Kka6vzuVpBq2BLKzhta/8HKKB0WMLLkmiYZI
         oWhOnNF/h2182PKQfEhD0xrQGcAT7tnalo6zoBzYsS9WAq0Iy+XcgA+zJMy5bxiCDQC6
         ZK0w==
MIME-Version: 1.0
X-Received: by 10.202.192.11 with SMTP id q11mr6822322oif.75.1419270390583;
 Mon, 22 Dec 2014 09:46:30 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 09:46:30 -0800 (PST)
In-Reply-To: <CAMAsSdLXA6MMWL6q7Tq=LKWO8c47MDqH_+zgehevbp5zncx71w@mail.gmail.com>
References: <6ccfbb56.21ee.14a70304168.Coremail.wyphao.2007@163.com>
	<CAMAsSdLXA6MMWL6q7Tq=LKWO8c47MDqH_+zgehevbp5zncx71w@mail.gmail.com>
Date: Mon, 22 Dec 2014 09:46:30 -0800
Message-ID: <CABPQxssj+qyYcJ3MRLqMpMvkOO=vx3jONzZKR-3Qo2GqyKCDhA@mail.gmail.com>
Subject: Re: Use mvn to build Spark 1.2.0 failed
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "wyphao.2007" <wyphao.2007@163.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I also couldn't reproduce this issued.

On Mon, Dec 22, 2014 at 2:24 AM, Sean Owen <sowen@cloudera.com> wrote:
> I just tried the exact same command and do not see any error. Maybe
> you can make sure you're starting from a clean extraction of the
> distro, and check your environment. I'm on OSX, Maven 3.2, Java 8 but
> I don't know that any of those would be relevant.
>
> On Mon, Dec 22, 2014 at 4:10 AM, wyphao.2007 <wyphao.2007@163.com> wrote:
>> Hi all, Today download Spark source from http://spark.apache.org/downloa=
ds.html page, and I use
>>
>>
>>  ./make-distribution.sh --tgz -Phadoop-2.2 -Pyarn -DskipTests -Dhadoop.v=
ersion=3D2.2.0 -Phive
>>
>>
>> to build the release, but I encountered an exception as follow:
>>
>>
>> [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) =
@ spark-parent ---
>> [INFO] Source directory: /home/q/spark/spark-1.2.0/src/main/scala added.
>> [INFO]
>> [INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-p=
arent ---
>> [INFO] -----------------------------------------------------------------=
-------
>> [INFO] Reactor Summary:
>> [INFO]
>> [INFO] Spark Project Parent POM .......................... FAILURE [1.01=
5s]
>> [INFO] Spark Project Networking .......................... SKIPPED
>> [INFO] Spark Project Shuffle Streaming Service ........... SKIPPED
>> [INFO] Spark Project Core ................................ SKIPPED
>> [INFO] Spark Project Bagel ............................... SKIPPED
>> [INFO] Spark Project GraphX .............................. SKIPPED
>> [INFO] Spark Project Streaming ........................... SKIPPED
>> [INFO] Spark Project Catalyst ............................ SKIPPED
>> [INFO] Spark Project SQL ................................. SKIPPED
>> [INFO] Spark Project ML Library .......................... SKIPPED
>> [INFO] Spark Project Tools ............................... SKIPPED
>> [INFO] Spark Project Hive ................................ SKIPPED
>> [INFO] Spark Project REPL ................................ SKIPPED
>> [INFO] Spark Project YARN Parent POM ..................... SKIPPED
>> [INFO] Spark Project YARN Stable API ..................... SKIPPED
>> [INFO] Spark Project Assembly ............................ SKIPPED
>> [INFO] Spark Project External Twitter .................... SKIPPED
>> [INFO] Spark Project External Flume Sink ................. SKIPPED
>> [INFO] Spark Project External Flume ...................... SKIPPED
>> [INFO] Spark Project External MQTT ....................... SKIPPED
>> [INFO] Spark Project External ZeroMQ ..................... SKIPPED
>> [INFO] Spark Project External Kafka ...................... SKIPPED
>> [INFO] Spark Project Examples ............................ SKIPPED
>> [INFO] Spark Project YARN Shuffle Service ................ SKIPPED
>> [INFO] -----------------------------------------------------------------=
-------
>> [INFO] BUILD FAILURE
>> [INFO] -----------------------------------------------------------------=
-------
>> [INFO] Total time: 1.644s
>> [INFO] Finished at: Mon Dec 22 10:56:35 CST 2014
>> [INFO] Final Memory: 21M/481M
>> [INFO] -----------------------------------------------------------------=
-------
>> [ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-res=
ources-plugin:1.5:process (default) on project spark-parent: Error finding =
remote resources manifests: /home/q/spark/spark-1.2.0/target/maven-shared-a=
rchive-resources/META-INF/NOTICE (No such file or directory) -> [Help 1]
>> [ERROR]
>> [ERROR] To see the full stack trace of the errors, re-run Maven with the=
 -e switch.
>> [ERROR] Re-run Maven using the -X switch to enable full debug logging.
>> [ERROR]
>> [ERROR] For more information about the errors and possible solutions, pl=
ease read the following articles:
>> [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoEx=
ecutionException
>>
>>
>> but the NOTICE file is in the download spark release:
>>
>>
>> [wyp@spark  /home/q/spark/spark-1.2.0]$ ll
>> total 248
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 assembly
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 bagel
>> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 bin
>> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 conf
>> -rw-rw-r-- 1 1000 1000   663 Dec 10 18:02 CONTRIBUTING.md
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 core
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 data
>> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 dev
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 docker
>> drwxrwxr-x 7 1000 1000  4096 Dec 10 18:02 docs
>> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 ec2
>> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 examples
>> drwxrwxr-x 8 1000 1000  4096 Dec 10 18:02 external
>> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 extras
>> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 graphx
>> -rw-rw-r-- 1 1000 1000 45242 Dec 10 18:02 LICENSE
>> -rwxrwxr-x 1 1000 1000  7941 Dec 10 18:02 make-distribution.sh
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 mllib
>> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 network
>> -rw-rw-r-- 1 1000 1000 22559 Dec 10 18:02 NOTICE
>> -rw-rw-r-- 1 1000 1000 49002 Dec 10 18:02 pom.xml
>> drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 project
>> drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 python
>> -rw-rw-r-- 1 1000 1000  3645 Dec 10 18:02 README.md
>> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 repl
>> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbin
>> drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbt
>> -rw-rw-r-- 1 1000 1000  7804 Dec 10 18:02 scalastyle-config.xml
>> drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 sql
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 streaming
>> drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 tools
>> -rw-rw-r-- 1 1000 1000   838 Dec 10 18:02 tox.ini
>> drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 yarn
>>
>>
>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 18:20:32 2014
Return-Path: <dev-return-10895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBBFC10941
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 18:20:32 +0000 (UTC)
Received: (qmail 18135 invoked by uid 500); 22 Dec 2014 18:20:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18056 invoked by uid 500); 22 Dec 2014 18:20:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18044 invoked by uid 99); 22 Dec 2014 18:20:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 18:20:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 18:20:04 +0000
Received: by mail-oi0-f43.google.com with SMTP id i138so6365512oig.2
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 10:19:42 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=slQCEcczKpZDhrtD3DI+pUVtSAXsRNi+MnUcCYqEqEk=;
        b=NDTv6p0s7npSrcknJF8jQaM/hx7dSgucuVDc6Pqu7qfEp6JAf4sL+4pIkZogrj+rW+
         6tdRexXEl2DPPX2/DwAHM7bKjbOLBkynm9sYwSvvCwZglDtRnc++gt4B98C8J07nEl6u
         FUDgjFYXKBuBnkCL9knNcZ91EudrDmn7BQ2O3YKR8Ks2TKR1dYvhZoUCvZ/vJZopGl4S
         /UvJA11unSz+iwZpI0BfO6XPFQf72AURk6zh+poWjc3Iw7vEdLd4GGtJEdE0DS3KJeUZ
         +0PDOwBDunTTusQvUxf5vv7gFLinE1az8DZQLRxGG1ZEfa7P/IfPSydTiZDsEaLOuehU
         g9BA==
X-Gm-Message-State: ALoCoQmUIUIcul00d1oBdacSLvFNiJw8UUdwggdbf8K302JuLo2V4PhWf3OcQdYekwrSQIGD2lR1
MIME-Version: 1.0
X-Received: by 10.60.230.6 with SMTP id su6mr6732375oec.44.1419272382759; Mon,
 22 Dec 2014 10:19:42 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Mon, 22 Dec 2014 10:19:42 -0800 (PST)
Date: Mon, 22 Dec 2014 12:19:42 -0600
Message-ID: <CAKWX9VVEUkz7mf74=X=44sr2SjVx=rB6RHmvu9ck9O4Gah1ZJA@mail.gmail.com>
Subject: cleaning up cache files left by SPARK-2713
From: Cody Koeninger <cody@koeninger.org>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136400a36efa7050ad21a91
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136400a36efa7050ad21a91
Content-Type: text/plain; charset=UTF-8

Is there a reason not to go ahead and move the _cache and _lock files
created by Utils.fetchFiles into the work directory, so they can be cleaned
up more easily?  I saw comments to that effect in the discussion of the PR
for 2713, but it doesn't look like it got done.

And no, I didn't just have a machine fill up the /tmp directory, why do you
ask?  :)

--001a1136400a36efa7050ad21a91--

From dev-return-10896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 18:40:35 2014
Return-Path: <dev-return-10896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9130710A2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 18:40:35 +0000 (UTC)
Received: (qmail 61234 invoked by uid 500); 22 Dec 2014 18:40:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61156 invoked by uid 500); 22 Dec 2014 18:40:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61140 invoked by uid 99); 22 Dec 2014 18:40:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 18:40:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 18:40:08 +0000
Received: by mail-qc0-f180.google.com with SMTP id i8so3643470qcq.25
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 10:39:22 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ekRS+baf/alMOZN0FxXHt7eIyGsqWXoFZkg6EBYeB+Q=;
        b=QX50HSmbTN7D62pe3nrkdZcEh39ZfnymJb60xOAw4kDUAzy42IHBA5fmrizxMBSsfs
         ueSDT64cwd5RJN2FFj9+riO/eVhatQgjTBPmKv2ZOKuiJ8Mp8JE2Go6mv9Kl/bzevNo2
         g6AI1OYmCTl2otV6w41s8cZ/4hP/5veivYrYKIzaw6qCcVlW9m0HDUCgjyp3flDL1Uas
         tHDhh7mj7QSwZPVdqwR5OcLUE/yFLNutEsCDAbYxhTEAxN7evvK7lh5dsKRvKF8uUtPt
         pkKltsaQG0i1PFJn//XqScTOp70CIKJlFQQr3QHq68H5zI3Z9Kd5EUR8lZHICkqnLZmf
         f4jw==
X-Gm-Message-State: ALoCoQmRjMHUPfUIJurRahqyMk26ebqz93NNsG3YuE/ePN46vGAowYUNrKgU4RjGe46Loon3qx4y
MIME-Version: 1.0
X-Received: by 10.224.13.145 with SMTP id c17mr39020462qaa.96.1419273562434;
 Mon, 22 Dec 2014 10:39:22 -0800 (PST)
Received: by 10.229.216.196 with HTTP; Mon, 22 Dec 2014 10:39:22 -0800 (PST)
In-Reply-To: <CAKWX9VVEUkz7mf74=X=44sr2SjVx=rB6RHmvu9ck9O4Gah1ZJA@mail.gmail.com>
References: <CAKWX9VVEUkz7mf74=X=44sr2SjVx=rB6RHmvu9ck9O4Gah1ZJA@mail.gmail.com>
Date: Mon, 22 Dec 2014 10:39:22 -0800
Message-ID: <CAAOnQ7s0Lk4s4nLN719TGNO3O6RUzB-y_Sgx3URmx+rh+_A1qQ@mail.gmail.com>
Subject: Re: cleaning up cache files left by SPARK-2713
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

https://github.com/apache/spark/pull/3705

On Mon, Dec 22, 2014 at 10:19 AM, Cody Koeninger <cody@koeninger.org> wrote:
> Is there a reason not to go ahead and move the _cache and _lock files
> created by Utils.fetchFiles into the work directory, so they can be cleaned
> up more easily?  I saw comments to that effect in the discussion of the PR
> for 2713, but it doesn't look like it got done.
>
> And no, I didn't just have a machine fill up the /tmp directory, why do you
> ask?  :)



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 19:11:33 2014
Return-Path: <dev-return-10897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB79910B29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 19:11:33 +0000 (UTC)
Received: (qmail 30836 invoked by uid 500); 22 Dec 2014 19:11:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30766 invoked by uid 500); 22 Dec 2014 19:11:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30753 invoked by uid 99); 22 Dec 2014 19:11:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:11:31 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [50.205.35.100] (HELO hera.ccri.com) (50.205.35.100)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:11:26 +0000
Received: from [192.168.2.67]
	by hera.ccri.com with esmtpsa (TLSv1:DHE-RSA-AES128-SHA:128)
	(Exim 4.80.1)
	(envelope-from <mcwhorter@ccri.com>)
	id 1Y38MJ-0003TI-BX; Mon, 22 Dec 2014 14:09:35 -0500
Message-ID: <54986C12.3030806@ccri.com>
Date: Mon, 22 Dec 2014 14:08:02 -0500
From: David McWhorter <mcwhorter@ccri.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.2.0
MIME-Version: 1.0
To: Sean Owen <sowen@cloudera.com>
CC: dev <dev@spark.apache.org>
Subject: Re: spark-yarn_2.10 1.2.0 artifacts
References: <54945BD9.7050109@ccri.com> <CAMAsSdKfJYRTgY2j=Th0KCMG_DqPvxeTxyudVht4=hpYiRmXgg@mail.gmail.com>
In-Reply-To: <CAMAsSdKfJYRTgY2j=Th0KCMG_DqPvxeTxyudVht4=hpYiRmXgg@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you, Sean, using spark-network-yarn seems to do the trick.

On 12/19/2014 12:13 PM, Sean Owen wrote:
> I believe spark-yarn does not exist from 1.2 onwards. Have a look at
> spark-network-yarn for where some of that went, I believe.
>
> On Fri, Dec 19, 2014 at 5:09 PM, David McWhorter <mcwhorter@ccri.com> wrote:
>> Hi all,
>>
>> Thanks for your work on spark!  I am trying to locate spark-yarn jars for
>> the new 1.2.0 release.  The jars for spark-core, etc, are on maven central,
>> but the spark-yarn jars are missing.
>>
>> Confusingly and perhaps relatedly, I also can't seem to get the spark-yarn
>> artifact to install on my local computer when I run 'mvn -Pyarn -Phadoop-2.2
>> -Dhadoop.version=2.2.0 -DskipTests clean install'.  At the install plugin
>> stage, maven reports:
>>
>> [INFO] --- maven-install-plugin:2.5.1:install (default-install) @
>> spark-yarn_2.10 ---
>> [INFO] Skipping artifact installation
>>
>> Any help or insights into how to use spark-yarn_2.10 1.2.0 in a maven build
>> would be appreciated.
>>
>> David
>>
>> --
>>
>> David McWhorter
>> Software Engineer
>> Commonwealth Computer Research, Inc.
>> 1422 Sachem Place, Unit #1
>> Charlottesville, VA 22901
>> mcwhorter@ccri.com | 434.299.0090x204
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>

-- 

David McWhorter
Software Engineer
Commonwealth Computer Research, Inc.
1422 Sachem Place, Unit #1
Charlottesville, VA 22901
mcwhorter@ccri.com | 434.299.0090x204


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 19:18:11 2014
Return-Path: <dev-return-10898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D74F10B5C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 19:18:11 +0000 (UTC)
Received: (qmail 45816 invoked by uid 500); 22 Dec 2014 19:18:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45700 invoked by uid 500); 22 Dec 2014 19:18:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44350 invoked by uid 99); 22 Dec 2014 19:18:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:18:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:17:41 +0000
Received: by mail-wg0-f43.google.com with SMTP id l18so7466655wgh.16;
        Mon, 22 Dec 2014 11:16:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=YQgfWEgyJlV60tz+nzWIN0Gmr0U6ebQP6BCN73a4XMY=;
        b=y1P7bS+j9eXMgoApnsu5rO/CDMPIllx84Gqfn0hQbQyLW3MJ3brMv965vv7mJiwsUd
         jJwe8emizUw/RKsjqFOPLwJAjUozHLFBy2n2n/5WpXVaJs0kesXL0Ty5WEPZMn+WJpcw
         uX0YjqRijp98u0vecH27ECYYaT1raPQiRIvJVNTcuj6d2ZU0GuUmyAdgwphOBm4AdGvh
         Q6zj5TW9/DeqejOWGrY7qDURAcCNOfFzlynaZP5CCJ/JmYl0xeFSHtpfAGm6hiOr1Hpl
         5AwE3W1TtAG1g0SzkaAkWGbFKcdHaIA/spJb36izusPc0ndJtutgGHMpiIqIp3A992k0
         A2+A==
X-Received: by 10.180.108.143 with SMTP id hk15mr34825656wib.6.1419275815855;
 Mon, 22 Dec 2014 11:16:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.189.8 with HTTP; Mon, 22 Dec 2014 11:16:25 -0800 (PST)
In-Reply-To: <59D086C1-3E76-4754-9770-64273DAF2E54@gmail.com>
References: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
 <59D086C1-3E76-4754-9770-64273DAF2E54@gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Mon, 22 Dec 2014 20:16:25 +0100
Message-ID: <CAMc-71nGbEkoCV9ZWGusxCe-XDpyi=i6poDzTrN6BgtZxb2wmA@mail.gmail.com>
Subject: Re: Tuning Spark Streaming jobs
To: Timothy Chen <tnachen@gmail.com>
Cc: spark users <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Tathagata Das <tathagata.das1565@gmail.com>, andy petrella <andy.petrella@gmail.com>, 
	Xavier Tordoir <xavier@silicocloud.eu>, =?UTF-8?Q?Lieven_Gesqui=C3=A8re?= <lieven.gesquiere@virdata.com>
Content-Type: multipart/alternative; boundary=e89a8f3ba2b1d7acc3050ad2e606
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3ba2b1d7acc3050ad2e606
Content-Type: text/plain; charset=UTF-8

Hi Tim,

That would be awesome. We have seen some really disparate Mesos allocations
for our Spark Streaming jobs. (like (7,4,1) over 3 executors for 4 kafka
consumer instead of the ideal (3,3,3,3))
For network dependent consumers, achieving an even deployment would
 provide a reliable and reproducible streaming job execution from the
performance point of view.
We're deploying in coarse grain mode. Not sure Spark Streaming would work
well in fine-grained given the added latency to acquire a worker.

You mention that you're changing the Mesos scheduler. Is there a Jira where
this job is taking place?

-kr, Gerard.


On Mon, Dec 22, 2014 at 6:01 PM, Timothy Chen <tnachen@gmail.com> wrote:

> Hi Gerard,
>
> Really nice guide!
>
> I'm particularly interested in the Mesos scheduling side to more evenly
> distribute cores across cluster.
>
> I wonder if you are using coarse grain mode or fine grain mode?
>
> I'm making changes to the spark mesos scheduler and I think we can propose
> a best way to achieve what you mentioned.
>
> Tim
>
> Sent from my iPhone
>
> > On Dec 22, 2014, at 8:33 AM, Gerard Maas <gerard.maas@gmail.com> wrote:
> >
> > Hi,
> >
> > After facing issues with the performance of some of our Spark Streaming
> > jobs, we invested quite some effort figuring out the factors that affect
> > the performance characteristics of a Streaming job. We  defined an
> > empirical model that helps us reason about Streaming jobs and applied it
> to
> > tune the jobs in order to maximize throughput.
> >
> > We have summarized our findings in a blog post with the intention of
> > collecting feedback and hoping that it is useful to other Spark Streaming
> > users facing similar issues.
> >
> > http://www.virdata.com/tuning-spark/
> >
> > Your feedback is welcome.
> >
> > With kind regards,
> >
> > Gerard.
> > Data Processing Team Lead
> > Virdata.com
> > @maasg
>

--e89a8f3ba2b1d7acc3050ad2e606--

From dev-return-10899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 19:34:46 2014
Return-Path: <dev-return-10899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD50310BF8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 19:34:46 +0000 (UTC)
Received: (qmail 78640 invoked by uid 500); 22 Dec 2014 19:34:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78565 invoked by uid 500); 22 Dec 2014 19:34:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78554 invoked by uid 99); 22 Dec 2014 19:34:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:34:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.41] (HELO mail-la0-f41.google.com) (209.85.215.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 19:34:40 +0000
Received: by mail-la0-f41.google.com with SMTP id hv19so4543424lab.28
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 11:33:58 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=LNTAUzmrmwaBYLZRUTNisdXmUiy/ueUwyUTnwr564yc=;
        b=e2pXwlXpx62P9QRMHLStMafapyxO+gmx1JhrMdU4oV7mkS4lPvzDHa1tPMjnULt77G
         4QS02ThHVGk0E3VlCgOMNoF21cGSWq92zLOrLgBI6RG4ftglnAFg5doypJqUIF26Qq6Z
         fAXAUMJqsfAfZvIipUztPrWYpc28bJuX74SwtgoKmybev66NzrMrhsxpVugg6XSVkZ4I
         G2nNw1f5VyRU3qJxiObBJIa9edOj4RUbJR608RW3phB4jTEpwC7MukQIcOp6q216JFfF
         2Qc3B4YjQoMLlQwihUKSoFIWNhXBxR/QaHF2T6Gpx7LyiwwZziA9nVKNbbQWzIlWRbTO
         8ZTA==
X-Gm-Message-State: ALoCoQnPO9roneFhcrfDjrzobCJ/UDYUpwuLOKDk3ZvsdR6reCisKqXP6oVUsdL8nicA/DL02Top
X-Received: by 10.112.147.10 with SMTP id tg10mr23528001lbb.92.1419276838410;
 Mon, 22 Dec 2014 11:33:58 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 22 Dec 2014 11:33:38 -0800 (PST)
In-Reply-To: <6545898BE5B3B545A96ECAE83927C38E2E50153C@blreml502-mbx>
References: <6545898BE5B3B545A96ECAE83927C38E2E50153C@blreml502-mbx>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 22 Dec 2014 11:33:38 -0800
Message-ID: <CAAswR-7+4eA=aTRpM3CnboePxmoBUtBatoY0Gx1_tmw1r5aLwQ@mail.gmail.com>
Subject: Re: Data source interface for making multiple tables available for query
To: Venkata ramana gollamudi <ramana.gollamudi@huawei.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a8ec0cab242050ad3231a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8ec0cab242050ad3231a
Content-Type: text/plain; charset=UTF-8

I agree and this is something that we have discussed in the past.
Essentially I think instead of creating a RelationProvider that returns a
single table, we'll have something like an external catalog that can return
multiple base relations.

On Sun, Dec 21, 2014 at 6:43 PM, Venkata ramana gollamudi <
ramana.gollamudi@huawei.com> wrote:

> Hi,
>
> Data source ddl.scala, CREATE TEMPORARY TABLE makes one table at time
> available to temp tables, how about the case if multiple/all tables from
> some data source needs to be available for query, just like hive tables. I
> think we also need that interface to connect such data sources. Please
> comment.
>
> Regards,
> Ramana
>

--047d7b3a8ec0cab242050ad3231a--

From dev-return-10900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 20:39:41 2014
Return-Path: <dev-return-10900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB52C10E7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 20:39:41 +0000 (UTC)
Received: (qmail 97455 invoked by uid 500); 22 Dec 2014 20:39:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97363 invoked by uid 500); 22 Dec 2014 20:39:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95815 invoked by uid 99); 22 Dec 2014 20:39:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 20:39:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 20:39:33 +0000
Received: by mail-ie0-f171.google.com with SMTP id ar1so5006176iec.30;
        Mon, 22 Dec 2014 12:37:43 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type
         :content-transfer-encoding;
        bh=lMadRv32zPvV5QuHLdE3xNkniEAviLZVnCTy4geEZ7g=;
        b=WOXVVaaTCjyWdOjIBYUFzPuTAOynN/wzT8sf/yWKRnB1WeAfaKPYHPV1rUrOkw0ply
         ufBBFW+3nTbQOQ3vvE2S0qvq3X3xIJ8+OCQEp8OecDLAKqDIAO4lcWtXP+3B06VSyGnD
         2SF8hlbjj8Jj66LItEfdPwET98KRYa7PEOfqeFhcseFPAtOYYPCl3MHXHrrDF9E5sqUR
         mj5u+lIm1iyIezUIszXCHPfIFuifpdLtxxHD3w3dT7b0KDZHttZpDKtRG3wNAYzsbHoT
         JRPwQE+yBKjnksElqKEc40aAEAnSXmBMKPJDCneWURJTNJ7X2PzbkMCKtWD0nF1d5E8G
         vdjg==
MIME-Version: 1.0
X-Received: by 10.42.107.211 with SMTP id e19mr18658060icp.27.1419280663323;
 Mon, 22 Dec 2014 12:37:43 -0800 (PST)
Received: by 10.107.167.148 with HTTP; Mon, 22 Dec 2014 12:37:43 -0800 (PST)
Date: Mon, 22 Dec 2014 12:37:43 -0800
Message-ID: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
Subject: Announcing Spark Packages
From: Xiangrui Meng <mengxr@gmail.com>
To: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Dear Spark users and developers,

I=E2=80=99m happy to announce Spark Packages (http://spark-packages.org), a
community package index to track the growing number of open source
packages and libraries that work with Apache Spark. Spark Packages
makes it easy for users to find, discuss, rate, and install packages
for any version of Spark, and makes it easy for developers to
contribute packages.

Spark Packages will feature integrations with various data sources,
management tools, higher level domain-specific libraries, machine
learning algorithms, code samples, and other Spark content. Thanks to
the package authors, the initial listing of packages includes
scientific computing libraries, a job execution server, a connector
for importing Avro data, tools for launching Spark on Google Compute
Engine, and many others.

I=E2=80=99d like to invite you to contribute and use Spark Packages and
provide feedback! As a disclaimer: Spark Packages is a community index
maintained by Databricks and (by design) will include packages outside
of the ASF Spark project. We are excited to help showcase and support
all of the great work going on in the broader Spark community!

Cheers,
Xiangrui

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 21:36:05 2014
Return-Path: <dev-return-10901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DAD3010102
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 21:36:05 +0000 (UTC)
Received: (qmail 4053 invoked by uid 500); 22 Dec 2014 21:36:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3981 invoked by uid 500); 22 Dec 2014 21:36:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3810 invoked by uid 99); 22 Dec 2014 21:35:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 21:35:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 21:35:52 +0000
Received: by mail-oi0-f48.google.com with SMTP id u20so11531997oif.7
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 13:33:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=Hcv/jO7zBQfkKoRtl8mZqqPFZKLu2YIEGJpUak744JU=;
        b=qr3bM7zbj0iaT+hen8X/LJ5HGysYr/zDduAYe7RmaywooXuTvXG+I5BY9gy59Hm6Ca
         zSSB2mex1DUC/7o/F9TkpO/5bqiLhMIiJul7jB0jL08uZDd97c/bTE1YOTWjjFUnZeWz
         QtroEzjIL+mwIu2h8DmNzhvfdyNDIpiuVodsq+J/UrMKZyMDgkDoENFZuzT48f4sbIv8
         JQ5Z8DMI6ihqSCEPPL+Lh5tX8kDIj8+NxvXVsY1GkzUMVovwlqyD2+AmW1stfYG6YpCG
         gDen5US5yTB3YeITdYatHqgqk+uDVnCJ8QFGEIqbPKavrh6uMHTDDlYW7BhxZM3/zGIf
         HREw==
X-Received: by 10.182.19.167 with SMTP id g7mr7847416obe.75.1419283996966;
 Mon, 22 Dec 2014 13:33:16 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Mon, 22 Dec 2014 13:32:56 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Mon, 22 Dec 2014 13:32:56 -0800
Message-ID: <CAJc_syK_3DcHrMC6NOo-RtQLppxjTo3ZMRUBYgUP93fCRY+Z6Q@mail.gmail.com>
Subject: More general submitJob API
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ea207987a8050ad4ce91
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ea207987a8050ad4ce91
Content-Type: text/plain; charset=UTF-8

Fellow Sparkers,

I'm rather puzzled at the submitJob API. I can't quite figure out how it is
supposed to be used. Is there any more documentation about it?

Also, is there any simpler way to multiplex jobs on the cluster, such as
starting multiple computations in as many threads in the driver and reaping
all the results when they are available?

Thanks,

Alex

--001a11c2ea207987a8050ad4ce91--

From dev-return-10902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 23:16:02 2014
Return-Path: <dev-return-10902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B39E104DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 23:16:02 +0000 (UTC)
Received: (qmail 84706 invoked by uid 500); 22 Dec 2014 23:16:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84632 invoked by uid 500); 22 Dec 2014 23:16:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84610 invoked by uid 99); 22 Dec 2014 23:16:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 23:16:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 23:15:50 +0000
Received: by mail-qa0-f41.google.com with SMTP id s7so3861571qap.14
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 15:14:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=4l//lqMb9wJ0pTuLjdQr92PaFR7Jed71eV7+lDByds4=;
        b=UZ8AFIqYO4ZbvbnQyeKch/szH2Cn5K+9tSaUNcxkBZBx/o2B4KWhkRXhvrY5/UxJXJ
         3s5I6y/bR7RJenS9xxklKldfjDcQWjL8FhAcKcD7eywBC8uftvYme2tHPIlgZ0uCZhVJ
         5d+exWQYn/lcWrBsK35KXr2H6EQPNCo/VqtVbZiR6AvOmjnGe7f57IyPMZ2QCAp1Qbem
         rMT5Ng28bvmQbF5bYr/aSXdAJKzLLj9a8MFuRLsMUvsQd+v7cccshJzAS7FkjjFUp2Wx
         2oNPnS7i+QPuCSb6k7j+J/bsz48et12ijOhArfwTq81F+qPxOaQG9fDEPSWwIBulPNpd
         tD3Q==
X-Gm-Message-State: ALoCoQky0HFAsyrZbfMYQqT5q4fArOhQciqjMk5MGwKauhnJv7bRTwaxCF0qd78+Wjw3cXyI67Hf
X-Received: by 10.229.130.65 with SMTP id r1mr40882339qcs.16.1419290064072;
        Mon, 22 Dec 2014 15:14:24 -0800 (PST)
Received: from mail-qa0-f46.google.com (mail-qa0-f46.google.com. [209.85.216.46])
        by mx.google.com with ESMTPSA id k6sm17524022qaz.41.2014.12.22.15.14.22
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 22 Dec 2014 15:14:22 -0800 (PST)
Received: by mail-qa0-f46.google.com with SMTP id w8so3816386qac.19;
        Mon, 22 Dec 2014 15:14:22 -0800 (PST)
X-Received: by 10.224.74.132 with SMTP id u4mr40888612qaj.61.1419290062330;
 Mon, 22 Dec 2014 15:14:22 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.89.11 with HTTP; Mon, 22 Dec 2014 15:14:02 -0800 (PST)
In-Reply-To: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 22 Dec 2014 15:14:02 -0800
Message-ID: <CA+-p3AEEhEBDcB_y3nVoBhYGXjZYPQOk_HpAhU3FoOn9TGEv6Q@mail.gmail.com>
Subject: Re: Announcing Spark Packages
To: Xiangrui Meng <mengxr@gmail.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0129503effa2c9050ad637d0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0129503effa2c9050ad637d0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Xiangrui,

That link is currently returning a 503 Over Quota error message.  Would you
mind pinging back out when the page is back up?

Thanks!
Andrew

On Mon, Dec 22, 2014 at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Dear Spark users and developers,
>
> I=E2=80=99m happy to announce Spark Packages (http://spark-packages.org),=
 a
> community package index to track the growing number of open source
> packages and libraries that work with Apache Spark. Spark Packages
> makes it easy for users to find, discuss, rate, and install packages
> for any version of Spark, and makes it easy for developers to
> contribute packages.
>
> Spark Packages will feature integrations with various data sources,
> management tools, higher level domain-specific libraries, machine
> learning algorithms, code samples, and other Spark content. Thanks to
> the package authors, the initial listing of packages includes
> scientific computing libraries, a job execution server, a connector
> for importing Avro data, tools for launching Spark on Google Compute
> Engine, and many others.
>
> I=E2=80=99d like to invite you to contribute and use Spark Packages and
> provide feedback! As a disclaimer: Spark Packages is a community index
> maintained by Databricks and (by design) will include packages outside
> of the ASF Spark project. We are excited to help showcase and support
> all of the great work going on in the broader Spark community!
>
> Cheers,
> Xiangrui
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> For additional commands, e-mail: user-help@spark.apache.org
>
>

--089e0129503effa2c9050ad637d0--

From dev-return-10903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 23:40:08 2014
Return-Path: <dev-return-10903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F41710585
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 23:40:08 +0000 (UTC)
Received: (qmail 18778 invoked by uid 500); 22 Dec 2014 23:40:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18707 invoked by uid 500); 22 Dec 2014 23:40:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16348 invoked by uid 99); 22 Dec 2014 23:40:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 23:40:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 23:39:36 +0000
Received: by mail-ob0-f172.google.com with SMTP id va8so23351955obc.3;
        Mon, 22 Dec 2014 15:38:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=yYhJ9sEsiKJURP5wFf6tS4aDMO662wAgYeNO+E321Lg=;
        b=U9cYI8vRUL4PILQ/qOQjxRBR0ZEOA0ttG3/rIACqbn47C/3DiN5bQMUjaM4HFkhX+r
         MWYXfovpoUwDhwoGMfPhefG4t21QN25ZacCafrkSAW1YzTpxRNi1pATJDeeXHkMvxm4V
         kGfrPR6i11n89XW/2elM4HhSVXUuVHrML2FhyLqN1rPTiB1knali53lUw0ZpZq23sluC
         MFIJhA9+/4HCP22brA2DffmiDUSXAhx6JPBO04XwhVgTYgA3iPhW3tu7iGzHw+BvWcMZ
         SO/ucYKfqZTNM+72x41DBusB2spATJ9pWTTRYX5/ksGuY68Z4Jrp72OisSNw2e7vwxtm
         UcGw==
MIME-Version: 1.0
X-Received: by 10.202.187.132 with SMTP id l126mr13562282oif.82.1419291529626;
 Mon, 22 Dec 2014 15:38:49 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 15:38:49 -0800 (PST)
In-Reply-To: <5498A761.8080305@uowmail.edu.au>
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
	<CA+-p3AEEhEBDcB_y3nVoBhYGXjZYPQOk_HpAhU3FoOn9TGEv6Q@mail.gmail.com>
	<5498A761.8080305@uowmail.edu.au>
Date: Mon, 22 Dec 2014 15:38:49 -0800
Message-ID: <CABPQxsvcZ2gGCn9Vwgv=msUANndSkRpfTAU7RrvbScY5wzWRWg@mail.gmail.com>
Subject: Re: Announcing Spark Packages
From: Patrick Wendell <pwendell@gmail.com>
To: peng <pc175@uowmail.edu.au>
Cc: Andrew Ash <andrew@andrewash.com>, "user@spark.apache.org" <user@spark.apache.org>, 
	dev <dev@spark.apache.org>, Xiangrui Meng <mengxr@gmail.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Xiangrui asked me to report that it's back and running :)

On Mon, Dec 22, 2014 at 3:21 PM, peng <pc175@uowmail.edu.au> wrote:
> Me 2 :)
>
>
> On 12/22/2014 06:14 PM, Andrew Ash wrote:
>
> Hi Xiangrui,
>
> That link is currently returning a 503 Over Quota error message.  Would you
> mind pinging back out when the page is back up?
>
> Thanks!
> Andrew
>
> On Mon, Dec 22, 2014 at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> Dear Spark users and developers,
>>
>> I'm happy to announce Spark Packages (http://spark-packages.org), a
>> community package index to track the growing number of open source
>> packages and libraries that work with Apache Spark. Spark Packages
>> makes it easy for users to find, discuss, rate, and install packages
>> for any version of Spark, and makes it easy for developers to
>> contribute packages.
>>
>> Spark Packages will feature integrations with various data sources,
>> management tools, higher level domain-specific libraries, machine
>> learning algorithms, code samples, and other Spark content. Thanks to
>> the package authors, the initial listing of packages includes
>> scientific computing libraries, a job execution server, a connector
>> for importing Avro data, tools for launching Spark on Google Compute
>> Engine, and many others.
>>
>> I'd like to invite you to contribute and use Spark Packages and
>> provide feedback! As a disclaimer: Spark Packages is a community index
>> maintained by Databricks and (by design) will include packages outside
>> of the ASF Spark project. We are excited to help showcase and support
>> all of the great work going on in the broader Spark community!
>>
>> Cheers,
>> Xiangrui
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 22 23:40:13 2014
Return-Path: <dev-return-10904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EAD010586
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 22 Dec 2014 23:40:13 +0000 (UTC)
Received: (qmail 20783 invoked by uid 500); 22 Dec 2014 23:40:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20660 invoked by uid 500); 22 Dec 2014 23:40:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20638 invoked by uid 99); 22 Dec 2014 23:40:09 -0000
Received: from mail-relay.apache.org (HELO mail-relay.apache.org) (140.211.11.15)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 22 Dec 2014 23:40:09 +0000
Received: from [10.11.4.89] (outbound.hortonworks.com [192.175.27.2])
	by mail-relay.apache.org (ASF Mail Server at mail-relay.apache.org) with ESMTPSA id D2A061A0118;
	Mon, 22 Dec 2014 23:40:06 +0000 (UTC)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Announcing Spark Packages
From: Hitesh Shah <hitesh@apache.org>
In-Reply-To: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
Date: Mon, 22 Dec 2014 15:40:00 -0800
Cc: dev@spark.apache.org,
 user@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <FBD2077C-7A29-4078-AEF3-8B203A0AB393@apache.org>
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
X-Mailer: Apple Mail (2.1878.6)

Hello Xiangrui,=20

If you have not already done so, you should look at =
http://www.apache.org/foundation/marks/#domains for the policy on use of =
ASF trademarked terms in domain names.=20

thanks
=97 Hitesh

On Dec 22, 2014, at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Dear Spark users and developers,
>=20
> I=92m happy to announce Spark Packages (http://spark-packages.org), a
> community package index to track the growing number of open source
> packages and libraries that work with Apache Spark. Spark Packages
> makes it easy for users to find, discuss, rate, and install packages
> for any version of Spark, and makes it easy for developers to
> contribute packages.
>=20
> Spark Packages will feature integrations with various data sources,
> management tools, higher level domain-specific libraries, machine
> learning algorithms, code samples, and other Spark content. Thanks to
> the package authors, the initial listing of packages includes
> scientific computing libraries, a job execution server, a connector
> for importing Avro data, tools for launching Spark on Google Compute
> Engine, and many others.
>=20
> I=92d like to invite you to contribute and use Spark Packages and
> provide feedback! As a disclaimer: Spark Packages is a community index
> maintained by Databricks and (by design) will include packages outside
> of the ASF Spark project. We are excited to help showcase and support
> all of the great work going on in the broader Spark community!
>=20
> Cheers,
> Xiangrui
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 00:05:16 2014
Return-Path: <dev-return-10905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CC8710741
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 00:05:16 +0000 (UTC)
Received: (qmail 72097 invoked by uid 500); 23 Dec 2014 00:05:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72008 invoked by uid 500); 23 Dec 2014 00:05:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71997 invoked by uid 99); 23 Dec 2014 00:05:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:05:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:05:07 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so4062885qcv.15
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 16:04:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=epWK5JhyMMuT2ISOVKqtAWqvHq81z1zz/bPcoqhNsFE=;
        b=OKzfm0u8O5lsqq/FD7tTBuVQVn4FT2uiHz7WbJay9mMjEHYBU55cyIrEX20VnZq+h4
         JCtbdOOOy5YAbui9tdjSxXkphmiBdoJSgeRAu3PghJH9HwzQO3cfqxZD6tBU/m2CwntS
         Z77d7z7+k1Cu5I/eY/3qGlxU2arHJaPHRO8ddksFZgQFJf7Th6YyyPcGBm2eHLHvFA8E
         zUhVcoge74OeZfeVeEDod3B8c+fsMjiAj9Dvb2MSnOT4cIqsqfgi1DCCFXGzamDbl2rt
         o/gpdNIWroLlNT8sdJ5Y84HxgR6eikPUTzCwH1uMqQSK6qIByVGt7o1V5OdmtfbtzKfW
         5GQA==
X-Gm-Message-State: ALoCoQmqDmeHlshHBBeILyBustbXrs8GVStzV52vEQkucP2ZkSJSPTL1aXjtjt6+TtFjEdk+y6vT
X-Received: by 10.229.61.5 with SMTP id r5mr41116941qch.28.1419293066917;
        Mon, 22 Dec 2014 16:04:26 -0800 (PST)
Received: from mail-qa0-f48.google.com (mail-qa0-f48.google.com. [209.85.216.48])
        by mx.google.com with ESMTPSA id t5sm17598749qge.16.2014.12.22.16.04.25
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 22 Dec 2014 16:04:25 -0800 (PST)
Received: by mail-qa0-f48.google.com with SMTP id k15so1524745qaq.21
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 16:04:25 -0800 (PST)
X-Received: by 10.140.23.244 with SMTP id 107mr39527505qgp.14.1419293065353;
 Mon, 22 Dec 2014 16:04:25 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.89.11 with HTTP; Mon, 22 Dec 2014 16:04:05 -0800 (PST)
In-Reply-To: <CAJc_syK_3DcHrMC6NOo-RtQLppxjTo3ZMRUBYgUP93fCRY+Z6Q@mail.gmail.com>
References: <CAJc_syK_3DcHrMC6NOo-RtQLppxjTo3ZMRUBYgUP93fCRY+Z6Q@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 22 Dec 2014 16:04:05 -0800
Message-ID: <CA+-p3AFBNFWHe7c-FpxKkUUEfC3dhFvR7e_HFqxEh8Ha0MqoZg@mail.gmail.com>
Subject: Re: More general submitJob API
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c14752fe229d050ad6eaaf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c14752fe229d050ad6eaaf
Content-Type: text/plain; charset=UTF-8

Hi Alex,

SparkContext.submitJob() is marked as experimental -- most client programs
shouldn't be using it.  What are you looking to do?

For multiplexing jobs, one thing you can do is have multiple threads in
your client JVM each submit jobs on your SparkContext job.  This is
described here in the docs:
http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application

Andrew

On Mon, Dec 22, 2014 at 1:32 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Fellow Sparkers,
>
> I'm rather puzzled at the submitJob API. I can't quite figure out how it is
> supposed to be used. Is there any more documentation about it?
>
> Also, is there any simpler way to multiplex jobs on the cluster, such as
> starting multiple computations in as many threads in the driver and reaping
> all the results when they are available?
>
> Thanks,
>
> Alex
>

--001a11c14752fe229d050ad6eaaf--

From dev-return-10906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 00:17:19 2014
Return-Path: <dev-return-10906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74111107FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 00:17:19 +0000 (UTC)
Received: (qmail 93253 invoked by uid 500); 23 Dec 2014 00:17:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93174 invoked by uid 500); 23 Dec 2014 00:17:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93162 invoked by uid 99); 23 Dec 2014 00:17:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:17:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:16:52 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so11927404oia.1
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 16:16:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=U/iKM44p0iBEXtxOptFdhZx6psrgi1jwOM4up1Utwh4=;
        b=dKSXthRDKF0lEDx99X9LPGwNiIp+HlidvDYVO7Uaw8lt3xcQmfrU+HrbqrDIdoVGXb
         FblAk53u77BMZG6QFCFkBhOtPOc1QylMUHLL/TNfvAriUAWQXP/sdNkeG0MuvDMUj12+
         Kg8Pn8e40PEXgq5O0WTegdbJqWnVCzVopSUNWGLKjuUwiZxMxDw6Z57wLzpIDdCKkwbr
         rQsdD1qx1dx53gyW7KtzIMXOG+9mOauD3z8/FOuo/08Q0OF93gb2mSR4ILXZY1D49Wv5
         FM2Vx7YyqQGRGUzqrxTSbcqraeHCK0uAMNQh8ziv8FemzAoH49G8FiUF/C9CU+KYAWzx
         ahEA==
X-Received: by 10.202.135.78 with SMTP id j75mr8070728oid.106.1419293765356;
 Mon, 22 Dec 2014 16:16:05 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Mon, 22 Dec 2014 16:15:45 -0800 (PST)
In-Reply-To: <CA+-p3AFBNFWHe7c-FpxKkUUEfC3dhFvR7e_HFqxEh8Ha0MqoZg@mail.gmail.com>
References: <CAJc_syK_3DcHrMC6NOo-RtQLppxjTo3ZMRUBYgUP93fCRY+Z6Q@mail.gmail.com>
 <CA+-p3AFBNFWHe7c-FpxKkUUEfC3dhFvR7e_HFqxEh8Ha0MqoZg@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Mon, 22 Dec 2014 16:15:45 -0800
Message-ID: <CAJc_sy+xW_fkk2eCK-SQCd2_jPMeQEpfdrW1BZP0ZL2-8PzawQ@mail.gmail.com>
Subject: Re: More general submitJob API
To: Andrew Ash <andrew@andrewash.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ef56ab753c5050ad7142c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ef56ab753c5050ad7142c
Content-Type: text/plain; charset=UTF-8

Andrew,

Thanks, yes, this is what I wanted: basically just to start multiple jobs
concurrently in threads.

Alex

On Mon, Dec 22, 2014 at 4:04 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
> Hi Alex,
>
> SparkContext.submitJob() is marked as experimental -- most client programs
> shouldn't be using it.  What are you looking to do?
>
> For multiplexing jobs, one thing you can do is have multiple threads in
> your client JVM each submit jobs on your SparkContext job.  This is
> described here in the docs:
> http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application
>
> Andrew
>
> On Mon, Dec 22, 2014 at 1:32 PM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Fellow Sparkers,
>>
>> I'm rather puzzled at the submitJob API. I can't quite figure out how it
>> is
>> supposed to be used. Is there any more documentation about it?
>>
>> Also, is there any simpler way to multiplex jobs on the cluster, such as
>> starting multiple computations in as many threads in the driver and
>> reaping
>> all the results when they are available?
>>
>> Thanks,
>>
>> Alex
>>
>
>

--001a113ef56ab753c5050ad7142c--

From dev-return-10907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 00:19:40 2014
Return-Path: <dev-return-10907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0AC41080E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 00:19:40 +0000 (UTC)
Received: (qmail 99596 invoked by uid 500); 23 Dec 2014 00:19:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99522 invoked by uid 500); 23 Dec 2014 00:19:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99507 invoked by uid 99); 23 Dec 2014 00:19:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:19:38 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 00:19:13 +0000
Received: by mail-ob0-f176.google.com with SMTP id vb8so23438228obc.7
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 16:18:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=udKQa4D6yW1xg7FQ9OGYUsMrZmZowp+vCb1gCHXM/5M=;
        b=XauzVBWihb0jKa98K379gu2YiWZk9FSNjZ/E4JOjspJxJoKSFxoTHl//+2N5x5acdc
         vqQklHlWRFYQDzxrH+hzUexfppQFkGPaMFpFg6eyeGvtersFsuuD3sosQ57aFIVqX8ed
         37IT4UzdFHoQEfIWC1QEEijG34fJHXVhMpcsx/AQivdZcs+5Kgrl/8kHphkjCwX1voZz
         vRhKM5BdqI87ObYzkKa0u7n/FnqGGFGq+vki3aJrtX9grgZUKv/W0DH+Io7IR+a74gGM
         s2xTqNWrJrpUloPtDoriueylexMvYHMmzNLawNEicqtvy7LzqC6XYEkcPpl7krRdnXFS
         QLaA==
MIME-Version: 1.0
X-Received: by 10.202.45.79 with SMTP id t76mr13951440oit.100.1419293907319;
 Mon, 22 Dec 2014 16:18:27 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 16:18:27 -0800 (PST)
In-Reply-To: <CAJc_sy+xW_fkk2eCK-SQCd2_jPMeQEpfdrW1BZP0ZL2-8PzawQ@mail.gmail.com>
References: <CAJc_syK_3DcHrMC6NOo-RtQLppxjTo3ZMRUBYgUP93fCRY+Z6Q@mail.gmail.com>
	<CA+-p3AFBNFWHe7c-FpxKkUUEfC3dhFvR7e_HFqxEh8Ha0MqoZg@mail.gmail.com>
	<CAJc_sy+xW_fkk2eCK-SQCd2_jPMeQEpfdrW1BZP0ZL2-8PzawQ@mail.gmail.com>
Date: Mon, 22 Dec 2014 16:18:27 -0800
Message-ID: <CABPQxsuogMDnX7ggqRP0zk-sdxPjA0BAaDqnmWJiPJHGaPO+3w@mail.gmail.com>
Subject: Re: More general submitJob API
From: Patrick Wendell <pwendell@gmail.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: Andrew Ash <andrew@andrewash.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

A SparkContext is thread safe, so you can just have different threads
that create their own RDD's and do actions, etc.

- Patrick

On Mon, Dec 22, 2014 at 4:15 PM, Alessandro Baretta
<alexbaretta@gmail.com> wrote:
> Andrew,
>
> Thanks, yes, this is what I wanted: basically just to start multiple jobs
> concurrently in threads.
>
> Alex
>
> On Mon, Dec 22, 2014 at 4:04 PM, Andrew Ash <andrew@andrewash.com> wrote:
>>
>> Hi Alex,
>>
>> SparkContext.submitJob() is marked as experimental -- most client programs
>> shouldn't be using it.  What are you looking to do?
>>
>> For multiplexing jobs, one thing you can do is have multiple threads in
>> your client JVM each submit jobs on your SparkContext job.  This is
>> described here in the docs:
>> http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application
>>
>> Andrew
>>
>> On Mon, Dec 22, 2014 at 1:32 PM, Alessandro Baretta <alexbaretta@gmail.com
>> > wrote:
>>
>>> Fellow Sparkers,
>>>
>>> I'm rather puzzled at the submitJob API. I can't quite figure out how it
>>> is
>>> supposed to be used. Is there any more documentation about it?
>>>
>>> Also, is there any simpler way to multiplex jobs on the cluster, such as
>>> starting multiple computations in as many threads in the driver and
>>> reaping
>>> all the results when they are available?
>>>
>>> Thanks,
>>>
>>> Alex
>>>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 01:30:23 2014
Return-Path: <dev-return-10908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4190510A4B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 01:30:23 +0000 (UTC)
Received: (qmail 11797 invoked by uid 500); 23 Dec 2014 01:30:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11706 invoked by uid 500); 23 Dec 2014 01:30:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10308 invoked by uid 99); 23 Dec 2014 01:30:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:30:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.173 as permitted sender)
Received: from [209.85.223.173] (HELO mail-ie0-f173.google.com) (209.85.223.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:30:15 +0000
Received: by mail-ie0-f173.google.com with SMTP id y20so5298038ier.32;
        Mon, 22 Dec 2014 17:29:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=+sNzgKiAJ5QDc/UIFG7Ol1vnjKcJegHdUEBh8Kpm73U=;
        b=LEUEDdY6p1QTbvjzsiCkyFfhfsfrFQ+F18jWKscSTVVCZYcXf+wfBptAJVQr6tMYGw
         yvnByA9ws695ZKm5Daa8dpoklXWarIuz6p3M6CuibpW0kgNrY50o3uCaMam69O63P5Yy
         BjkkYFSjEerhuPGQrCbmjj/t+7bGltmGGaKWZxpeADjiVgbrI11wWp/61MK3U8fJbf33
         d1u7gKsLP82JPWYJz9DiA/8nGhxBDdE2TQwDPb8SSxRuafHc8Pjqvb7s58jXD/nzJAJK
         E/iZlkrP5BxhgB1LVdV7ghPnOf7ND0pX/OROB13XgRiau2ieJRFmZTgf3h7SuVMVcMOq
         Yheg==
X-Received: by 10.107.17.169 with SMTP id 41mr22694421ior.90.1419298149206;
 Mon, 22 Dec 2014 17:29:09 -0800 (PST)
MIME-Version: 1.0
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
 <FBD2077C-7A29-4078-AEF3-8B203A0AB393@apache.org>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 23 Dec 2014 01:29:08 +0000
Message-ID: <CAOhmDzfRr3TQNGAaayxbE9eS770OBL+Fn=LtkX5qx3=FPdVs7w@mail.gmail.com>
Subject: Re: Announcing Spark Packages
To: Hitesh Shah <hitesh@apache.org>, Xiangrui Meng <mengxr@gmail.com>
Cc: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ed7fc0390a6050ad81a69
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ed7fc0390a6050ad81a69
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hitesh,

>From your link <http://www.apache.org/foundation/marks/#domains>:

You may not use ASF trademarks such as =E2=80=9CApache=E2=80=9D or =E2=80=
=9CApacheFoo=E2=80=9D or =E2=80=9CFoo=E2=80=9D in
your own domain names if that use would be likely to confuse a relevant
consumer about the source of software or services provided through your
website, without written approval of the VP, Apache Brand Management or
designee.

The title on the packages website is =E2=80=9CA community index of packages=
 for
Apache Spark.=E2=80=9D Furthermore, the footnote of the website reads =E2=
=80=9CSpark
Packages is a community site hosting modules that are not part of Apache
Spark.=E2=80=9D

I think there=E2=80=99s nothing on there that would =E2=80=9Cconfuse a rele=
vant consumer
about the source of software=E2=80=9D. It=E2=80=99s pretty clear that the S=
park Packages
name is well within the ASF=E2=80=99s guidelines.

Have I misunderstood the ASF=E2=80=99s policy?

Nick
=E2=80=8B

On Mon Dec 22 2014 at 6:40:10 PM Hitesh Shah <hitesh@apache.org> wrote:

> Hello Xiangrui,
>
> If you have not already done so, you should look at http://www.apache.org=
/
> foundation/marks/#domains for the policy on use of ASF trademarked terms
> in domain names.
>
> thanks
> =E2=80=94 Hitesh
>
> On Dec 22, 2014, at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Dear Spark users and developers,
> >
> > I=E2=80=99m happy to announce Spark Packages (http://spark-packages.org=
), a
> > community package index to track the growing number of open source
> > packages and libraries that work with Apache Spark. Spark Packages
> > makes it easy for users to find, discuss, rate, and install packages
> > for any version of Spark, and makes it easy for developers to
> > contribute packages.
> >
> > Spark Packages will feature integrations with various data sources,
> > management tools, higher level domain-specific libraries, machine
> > learning algorithms, code samples, and other Spark content. Thanks to
> > the package authors, the initial listing of packages includes
> > scientific computing libraries, a job execution server, a connector
> > for importing Avro data, tools for launching Spark on Google Compute
> > Engine, and many others.
> >
> > I=E2=80=99d like to invite you to contribute and use Spark Packages and
> > provide feedback! As a disclaimer: Spark Packages is a community index
> > maintained by Databricks and (by design) will include packages outside
> > of the ASF Spark project. We are excited to help showcase and support
> > all of the great work going on in the broader Spark community!
> >
> > Cheers,
> > Xiangrui
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> For additional commands, e-mail: user-help@spark.apache.org
>
>

--001a113ed7fc0390a6050ad81a69--

From dev-return-10909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 01:35:06 2014
Return-Path: <dev-return-10909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3208C10A86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 01:35:06 +0000 (UTC)
Received: (qmail 27375 invoked by uid 500); 23 Dec 2014 01:35:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27278 invoked by uid 500); 23 Dec 2014 01:35:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25327 invoked by uid 99); 23 Dec 2014 01:35:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:35:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:34:59 +0000
Received: by mail-oi0-f47.google.com with SMTP id v63so12127184oia.6;
        Mon, 22 Dec 2014 17:33:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=m865PUSkZXuusT8v9RTAZ974zLwVzopW6yDmO869svQ=;
        b=BmCqPNEh5QoJH3NxfULXCDPsF0JgmwG+xvTZ+WStQFCxaceX/kpdvktweRaoY3ywBC
         sqX6nqZxVz04KzL4k2cppUR8keN0hI70a1O0DYzHV8FmLnUtM+3MDihb3YZiPYM1NecC
         1ini82baI89kJicTTeCYDpuQJf+vEztueSlMU2kOkihaN+acKZNY7pHNOmX/7OWLd2u3
         so7m+SJ4kCm8leaxjgdrE717r6VzilkArhrvlkoA0uCfzFD7VJGkITDvPNQZXiZoYtKo
         DXFhxMS5jgB9J/rgH6rwA4bLGXS0Qo9Ujvh6FXHCKzXLeglCtwtoo59/j4fgWRiBFvAq
         WHRQ==
MIME-Version: 1.0
X-Received: by 10.182.79.41 with SMTP id g9mr14552622obx.14.1419298433954;
 Mon, 22 Dec 2014 17:33:53 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 17:33:53 -0800 (PST)
In-Reply-To: <CAOhmDzfRr3TQNGAaayxbE9eS770OBL+Fn=LtkX5qx3=FPdVs7w@mail.gmail.com>
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
	<FBD2077C-7A29-4078-AEF3-8B203A0AB393@apache.org>
	<CAOhmDzfRr3TQNGAaayxbE9eS770OBL+Fn=LtkX5qx3=FPdVs7w@mail.gmail.com>
Date: Mon, 22 Dec 2014 17:33:53 -0800
Message-ID: <CABPQxstuYDX=SAvQamKhcVfU4Fs0sKkOP_semcJm05RGPrqAkw@mail.gmail.com>
Subject: Re: Announcing Spark Packages
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Hitesh Shah <hitesh@apache.org>, Xiangrui Meng <mengxr@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nick,

I think Hitesh was just trying to be helpful and point out the policy
- not necessarily saying there was an issue. We've taken a close look
at this and I think we're in good shape her vis-a-vis this policy.

- Patrick

On Mon, Dec 22, 2014 at 5:29 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Hitesh,
>
> From your link:
>
> You may not use ASF trademarks such as "Apache" or "ApacheFoo" or "Foo" in
> your own domain names if that use would be likely to confuse a relevant
> consumer about the source of software or services provided through your
> website, without written approval of the VP, Apache Brand Management or
> designee.
>
> The title on the packages website is "A community index of packages for
> Apache Spark." Furthermore, the footnote of the website reads "Spark
> Packages is a community site hosting modules that are not part of Apache
> Spark."
>
> I think there's nothing on there that would "confuse a relevant consumer
> about the source of software". It's pretty clear that the Spark Packages
> name is well within the ASF's guidelines.
>
> Have I misunderstood the ASF's policy?
>
> Nick
>
>
> On Mon Dec 22 2014 at 6:40:10 PM Hitesh Shah <hitesh@apache.org> wrote:
>>
>> Hello Xiangrui,
>>
>> If you have not already done so, you should look at
>> http://www.apache.org/foundation/marks/#domains for the policy on use of ASF
>> trademarked terms in domain names.
>>
>> thanks
>> -- Hitesh
>>
>> On Dec 22, 2014, at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> > Dear Spark users and developers,
>> >
>> > I'm happy to announce Spark Packages (http://spark-packages.org), a
>> > community package index to track the growing number of open source
>> > packages and libraries that work with Apache Spark. Spark Packages
>> > makes it easy for users to find, discuss, rate, and install packages
>> > for any version of Spark, and makes it easy for developers to
>> > contribute packages.
>> >
>> > Spark Packages will feature integrations with various data sources,
>> > management tools, higher level domain-specific libraries, machine
>> > learning algorithms, code samples, and other Spark content. Thanks to
>> > the package authors, the initial listing of packages includes
>> > scientific computing libraries, a job execution server, a connector
>> > for importing Avro data, tools for launching Spark on Google Compute
>> > Engine, and many others.
>> >
>> > I'd like to invite you to contribute and use Spark Packages and
>> > provide feedback! As a disclaimer: Spark Packages is a community index
>> > maintained by Databricks and (by design) will include packages outside
>> > of the ASF Spark project. We are excited to help showcase and support
>> > all of the great work going on in the broader Spark community!
>> >
>> > Cheers,
>> > Xiangrui
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 01:38:11 2014
Return-Path: <dev-return-10910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 86E6D10AAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 01:38:11 +0000 (UTC)
Received: (qmail 35096 invoked by uid 500); 23 Dec 2014 01:38:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34980 invoked by uid 500); 23 Dec 2014 01:38:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33315 invoked by uid 99); 23 Dec 2014 01:38:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:38:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 01:37:41 +0000
Received: by mail-ie0-f172.google.com with SMTP id tr6so5332484ieb.17;
        Mon, 22 Dec 2014 17:36:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=nVbZkQ+1Y0oWjkZMNatc8XYIQnLaRBfgHAPaQd+4Zbw=;
        b=cIQVRhf5TJVSo9XGgZ6K6XJUwgNdKqujKLycg+lNk6utOsEesp4r+dw8ejvsLldpSp
         QDXNpMysQAobtXK5LIOl3Mv9FHyQSwXis/xZhoEESQv5IGSRDxfMAjMja0iKQxaJayt6
         NIjBs5B4YflAh2YtR4bU0YjHmL6spy3TsIA8Jih7JNYKIIyt/N5U2VTpBLW+JRcrjK7R
         wxFf2Ep81qV+KLOraLwN/R2sM5LTtjn9qDFb7eepsJ8sk3hLIr2hGFW0n35/EBPfu6bL
         Lfd8tBCW2rm0up7uM73ggtwDqzqpohl8HLUc2gmrXPUBq0n6QKGI2/PnahNPG1lSjVdr
         i95A==
X-Received: by 10.107.17.169 with SMTP id 41mr22713171ior.90.1419298615357;
 Mon, 22 Dec 2014 17:36:55 -0800 (PST)
MIME-Version: 1.0
References: <CAJgQjQ8ghn_td3bcV9UiYtKygjzg30sD6aT6dfsvmpyW++FzgA@mail.gmail.com>
 <FBD2077C-7A29-4078-AEF3-8B203A0AB393@apache.org> <CAOhmDzfRr3TQNGAaayxbE9eS770OBL+Fn=LtkX5qx3=FPdVs7w@mail.gmail.com>
 <CABPQxstuYDX=SAvQamKhcVfU4Fs0sKkOP_semcJm05RGPrqAkw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 23 Dec 2014 01:36:55 +0000
Message-ID: <CAOhmDzd6P+hn5Vj2QGkGyL-vXVYkk7sOzi1rZeSeMK7ysUSWCw@mail.gmail.com>
Subject: Re: Announcing Spark Packages
To: Patrick Wendell <pwendell@gmail.com>
Cc: Hitesh Shah <hitesh@apache.org>, Xiangrui Meng <mengxr@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ed7fccc77e7050ad835a5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ed7fccc77e7050ad835a5
Content-Type: text/plain; charset=UTF-8

Okie doke! (I just assumed there was an issue since the policy was brought
up.)

On Mon Dec 22 2014 at 8:33:53 PM Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Nick,
>
> I think Hitesh was just trying to be helpful and point out the policy
> - not necessarily saying there was an issue. We've taken a close look
> at this and I think we're in good shape her vis-a-vis this policy.
>
> - Patrick
>
> On Mon, Dec 22, 2014 at 5:29 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > Hitesh,
> >
> > From your link:
> >
> > You may not use ASF trademarks such as "Apache" or "ApacheFoo" or "Foo"
> in
> > your own domain names if that use would be likely to confuse a relevant
> > consumer about the source of software or services provided through your
> > website, without written approval of the VP, Apache Brand Management or
> > designee.
> >
> > The title on the packages website is "A community index of packages for
> > Apache Spark." Furthermore, the footnote of the website reads "Spark
> > Packages is a community site hosting modules that are not part of Apache
> > Spark."
> >
> > I think there's nothing on there that would "confuse a relevant consumer
> > about the source of software". It's pretty clear that the Spark Packages
> > name is well within the ASF's guidelines.
> >
> > Have I misunderstood the ASF's policy?
> >
> > Nick
> >
> >
> > On Mon Dec 22 2014 at 6:40:10 PM Hitesh Shah <hitesh@apache.org> wrote:
> >>
> >> Hello Xiangrui,
> >>
> >> If you have not already done so, you should look at
> >> http://www.apache.org/foundation/marks/#domains for the policy on use
> of ASF
> >> trademarked terms in domain names.
> >>
> >> thanks
> >> -- Hitesh
> >>
> >> On Dec 22, 2014, at 12:37 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >>
> >> > Dear Spark users and developers,
> >> >
> >> > I'm happy to announce Spark Packages (http://spark-packages.org), a
> >> > community package index to track the growing number of open source
> >> > packages and libraries that work with Apache Spark. Spark Packages
> >> > makes it easy for users to find, discuss, rate, and install packages
> >> > for any version of Spark, and makes it easy for developers to
> >> > contribute packages.
> >> >
> >> > Spark Packages will feature integrations with various data sources,
> >> > management tools, higher level domain-specific libraries, machine
> >> > learning algorithms, code samples, and other Spark content. Thanks to
> >> > the package authors, the initial listing of packages includes
> >> > scientific computing libraries, a job execution server, a connector
> >> > for importing Avro data, tools for launching Spark on Google Compute
> >> > Engine, and many others.
> >> >
> >> > I'd like to invite you to contribute and use Spark Packages and
> >> > provide feedback! As a disclaimer: Spark Packages is a community index
> >> > maintained by Databricks and (by design) will include packages outside
> >> > of the ASF Spark project. We are excited to help showcase and support
> >> > all of the great work going on in the broader Spark community!
> >> >
> >> > Cheers,
> >> > Xiangrui
> >> >
> >> > ---------------------------------------------------------------------
> >> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > For additional commands, e-mail: dev-help@spark.apache.org
> >> >
> >>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: user-help@spark.apache.org
> >>
> >
>

--001a113ed7fccc77e7050ad835a5--

From dev-return-10911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 02:17:44 2014
Return-Path: <dev-return-10911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 330ED10BC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 02:17:44 +0000 (UTC)
Received: (qmail 94471 invoked by uid 500); 23 Dec 2014 02:17:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94400 invoked by uid 500); 23 Dec 2014 02:17:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94379 invoked by uid 99); 23 Dec 2014 02:17:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 02:17:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 02:17:16 +0000
Received: by mail-ig0-f178.google.com with SMTP id hl2so4913490igb.11
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 18:17:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=FnOCiExSNP+Q42BWt9iEba0sp4FJ80Wu8geMIObRsMY=;
        b=MrFjWarGjSNIvlD/xs1Ub07a3Bg0AWQ7ymBboBbV4YozWwH0AlAS0VpS7ijuc+jiM8
         dckLRog8FSGRUJpnM4HkrdJyaRUgLZ+oOKx9+Ycradhh+yRkqTx94q7EzlHfVUPmC2YW
         HnfDe98OTeYcYoQtvVRz1lYwn2nErb5S5Pr2EaM50hMn6/zieVuuZDhJDiTxhxW99Bxb
         xhvPCBpA3FD8gqBAqcjg0SfB3r2GTtzxMGGVTqUieOdptYOZmhqLAIwFAvqkv0WhdPqK
         ARXI8vuXDB51h47BIOAv4N2+UQOfFKCg5Qpad74Vl9J2sGtSNMNoQr6zXiHC7vhhnCEn
         57dg==
X-Received: by 10.107.130.212 with SMTP id m81mr23169274ioi.46.1419301034732;
 Mon, 22 Dec 2014 18:17:14 -0800 (PST)
MIME-Version: 1.0
References: <CABPQxssLQBy_cXhrTkv_FUJxpLHJd6GfL+j1U-jnryhviwAGuA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 23 Dec 2014 02:17:14 +0000
Message-ID: <CAOhmDzeBd9RmLtKnPYES2Kou7HypVPL9iCAaFAiyHkDuE4MGog@mail.gmail.com>
Subject: Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits
To: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f7d7a0134e5050ad8c664
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f7d7a0134e5050ad8c664
Content-Type: text/plain; charset=UTF-8

Does this include contributions made against the spark-ec2
<https://github.com/mesos/spark-ec2> repo?

On Wed Dec 17 2014 at 12:29:19 AM Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey All,
>
> Due to the very high volume of contributions, we're switching to an
> automated process for generating release credits. This process relies
> on JIRA for categorizing contributions, so it's not possible for us to
> provide credits in the case where users submit pull requests with no
> associated JIRA.
>
> This needed to be automated because, with more than 1000 commits per
> release, finding proper names for every commit and summarizing
> contributions was taking on the order of days of time.
>
> For 1.2.0 there were around 100 commits that did not have JIRA's. I'll
> try to manually merge these into the credits, but please e-mail me
> directly if you are not credited once the release notes are posted.
> The notes should be posted within 48 hours of right now.
>
> We already ask that users include a JIRA for pull requests, but now it
> will be required for proper attribution. I've updated the contributing
> guide on the wiki to reflect this.
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113f7d7a0134e5050ad8c664--

From dev-return-10912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 06:52:44 2014
Return-Path: <dev-return-10912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 965091049F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 06:52:44 +0000 (UTC)
Received: (qmail 44584 invoked by uid 500); 23 Dec 2014 06:52:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44509 invoked by uid 500); 23 Dec 2014 06:52:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44485 invoked by uid 99); 23 Dec 2014 06:52:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 06:52:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 06:52:15 +0000
Received: by mail-oi0-f53.google.com with SMTP id g201so12725080oib.12
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 22:52:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rCmuH1mQweD5J1QtE3c6qacX1ObvjfEdqcgxCWCa16o=;
        b=kb+xSUFqrpFHEyDxOU7kB+NUsXC5dEadghn77EIg8b8TNVhu9Bd4XkpDFUnybTCcW0
         Tr4E4VrHNpASZF8+ewLXIaz/w6z9UYZ/86v0YD++FhEtg7CKGTBm7wgmReqFdr82MjcO
         u4vHhyFIuo+k5EzuxdR9xZsf56sKhzOD5f8wIyBlj0XFkhwMHbinzv5DbmsSZ1TwTTWD
         dggla3QqGRtEkgOoP2iaflC1xi1/mOIE262KizAYrArBSa+0pBOyjQZgTj6z/OwdnZzK
         JiIMboPYXS1WF/i3+KTs3RamDro/3UrgGuJHBg0W/cj8RpwXkIkkZ3kZX2nm1G0lcEeT
         wmWg==
MIME-Version: 1.0
X-Received: by 10.202.219.198 with SMTP id s189mr14607312oig.72.1419317534236;
 Mon, 22 Dec 2014 22:52:14 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 22:52:14 -0800 (PST)
In-Reply-To: <CAOhmDzeBd9RmLtKnPYES2Kou7HypVPL9iCAaFAiyHkDuE4MGog@mail.gmail.com>
References: <CABPQxssLQBy_cXhrTkv_FUJxpLHJd6GfL+j1U-jnryhviwAGuA@mail.gmail.com>
	<CAOhmDzeBd9RmLtKnPYES2Kou7HypVPL9iCAaFAiyHkDuE4MGog@mail.gmail.com>
Date: Mon, 22 Dec 2014 22:52:14 -0800
Message-ID: <CABPQxssY_Gnqrj47KFCFX2uYXu7zE0iATXsq=83YPT8qtr7_vw@mail.gmail.com>
Subject: Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Josh,

We don't explicitly track contributions to spark-ec2 in the Apache
Spark release notes. The main reason is that usually updates to
spark-ec2 include a corresponding update to spark so we get it there.
This may not always be the case though, so let me know if you think
there is something missing we should add.

- Patrick

On Mon, Dec 22, 2014 at 6:17 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Does this include contributions made against the spark-ec2 repo?
>
> On Wed Dec 17 2014 at 12:29:19 AM Patrick Wendell <pwendell@gmail.com>
> wrote:
>>
>> Hey All,
>>
>> Due to the very high volume of contributions, we're switching to an
>> automated process for generating release credits. This process relies
>> on JIRA for categorizing contributions, so it's not possible for us to
>> provide credits in the case where users submit pull requests with no
>> associated JIRA.
>>
>> This needed to be automated because, with more than 1000 commits per
>> release, finding proper names for every commit and summarizing
>> contributions was taking on the order of days of time.
>>
>> For 1.2.0 there were around 100 commits that did not have JIRA's. I'll
>> try to manually merge these into the credits, but please e-mail me
>> directly if you are not credited once the release notes are posted.
>> The notes should be posted within 48 hours of right now.
>>
>> We already ask that users include a JIRA for pull requests, but now it
>> will be required for proper attribution. I've updated the contributing
>> guide on the wiki to reflect this.
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 06:52:54 2014
Return-Path: <dev-return-10913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4656104A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 06:52:54 +0000 (UTC)
Received: (qmail 46113 invoked by uid 500); 23 Dec 2014 06:52:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46040 invoked by uid 500); 23 Dec 2014 06:52:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46022 invoked by uid 99); 23 Dec 2014 06:52:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 06:52:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 06:52:26 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so12678930oia.1
        for <dev@spark.apache.org>; Mon, 22 Dec 2014 22:52:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=El9Gk2VrV/LSzrtf8KX4Ui1nTlxv7qWIpRmeIiOqb6Q=;
        b=lt0mapKBUwNWg7J1BvH+ZPvsZdEVkYXJdZ9MTA/RutTGaKNxrKy92bSt8g96hDq+Bv
         uto/JDQaqufuC7u4mBn5+xjWhi53TViILC+mrjQzfYnoXgLkIbu+znP9hAiIKLAHJlPX
         tJtst01lgNKyHy/sUwYz/Jy7o3xgwoMbNrvpJlKhdiSP6EtnJASOA5GTz4Hk+caFse+P
         5DFvOBtbEb1uxnVaA+fB6dNZSWcftvuKGOFaaQkHWEJUpLuZN7WcktlH1w5ah5KCeEth
         vOs7PS3iiVphhWkZh+s0+axP8zWVpGp/9L+3FfI6ixXpDsfH9BvXFJ5e9TgLdW/go3Iq
         +YtA==
MIME-Version: 1.0
X-Received: by 10.60.133.141 with SMTP id pc13mr15438625oeb.68.1419317545251;
 Mon, 22 Dec 2014 22:52:25 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Mon, 22 Dec 2014 22:52:25 -0800 (PST)
In-Reply-To: <CABPQxssY_Gnqrj47KFCFX2uYXu7zE0iATXsq=83YPT8qtr7_vw@mail.gmail.com>
References: <CABPQxssLQBy_cXhrTkv_FUJxpLHJd6GfL+j1U-jnryhviwAGuA@mail.gmail.com>
	<CAOhmDzeBd9RmLtKnPYES2Kou7HypVPL9iCAaFAiyHkDuE4MGog@mail.gmail.com>
	<CABPQxssY_Gnqrj47KFCFX2uYXu7zE0iATXsq=83YPT8qtr7_vw@mail.gmail.com>
Date: Mon, 22 Dec 2014 22:52:25 -0800
Message-ID: <CABPQxsu+30mZY2UFJi634JYOAdeo3X2xxxcPhPzuZFFBgageRA@mail.gmail.com>
Subject: Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits
From: Patrick Wendell <pwendell@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

s/Josh/Nick/ - sorry!

On Mon, Dec 22, 2014 at 10:52 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Josh,
>
> We don't explicitly track contributions to spark-ec2 in the Apache
> Spark release notes. The main reason is that usually updates to
> spark-ec2 include a corresponding update to spark so we get it there.
> This may not always be the case though, so let me know if you think
> there is something missing we should add.
>
> - Patrick
>
> On Mon, Dec 22, 2014 at 6:17 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
>> Does this include contributions made against the spark-ec2 repo?
>>
>> On Wed Dec 17 2014 at 12:29:19 AM Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>>
>>> Hey All,
>>>
>>> Due to the very high volume of contributions, we're switching to an
>>> automated process for generating release credits. This process relies
>>> on JIRA for categorizing contributions, so it's not possible for us to
>>> provide credits in the case where users submit pull requests with no
>>> associated JIRA.
>>>
>>> This needed to be automated because, with more than 1000 commits per
>>> release, finding proper names for every commit and summarizing
>>> contributions was taking on the order of days of time.
>>>
>>> For 1.2.0 there were around 100 commits that did not have JIRA's. I'll
>>> try to manually merge these into the credits, but please e-mail me
>>> directly if you are not credited once the release notes are posted.
>>> The notes should be posted within 48 hours of right now.
>>>
>>> We already ask that users include a JIRA for pull requests, but now it
>>> will be required for proper attribution. I've updated the contributing
>>> guide on the wiki to reflect this.
>>>
>>> - Patrick
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 23 19:03:01 2014
Return-Path: <dev-return-10914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 082A8CFFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Dec 2014 19:03:01 +0000 (UTC)
Received: (qmail 93555 invoked by uid 500); 23 Dec 2014 19:02:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93406 invoked by uid 500); 23 Dec 2014 19:02:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92894 invoked by uid 99); 23 Dec 2014 19:02:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 19:02:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tnachen@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Dec 2014 19:02:51 +0000
Received: by mail-oi0-f53.google.com with SMTP id g201so14885747oib.12;
        Tue, 23 Dec 2014 11:02:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=A2ye2sYvoh/LeZXwwC9TCpu5jRwOE/4V09CJUPqN2iE=;
        b=wmI8U+SSrQyid+351+cCqF72M0yM3+fMH2kzo4Db/BSJw7NVtuqcjFmLaFokd6mfVS
         pVz6VcqU+lYz+Z247o3k44EVs80TYljJl+dVCviNMGJzuhUIVLqB3Fl70k9gHKPbboXB
         enYicQi8RJ4dF8jtpdFSnyLtB84TzME4EgBeHYbmlUqZnx8OK9cdTrgvYZyZ4CKcCLip
         sTP4pVNdschY9HNVucZPJQDKer1Su3sV+Ig/SnbYcbYK0LmRTy4zHS6npKW8fi70/Wkb
         WMaAVgZHa1uwv/8vy/+IiFihu/o0dx6ylDk3zj5nCINz8Ujsx3LNMugad14CYC8h4M4g
         XKsA==
MIME-Version: 1.0
X-Received: by 10.60.92.193 with SMTP id co1mr17223275oeb.5.1419361350670;
 Tue, 23 Dec 2014 11:02:30 -0800 (PST)
Received: by 10.60.150.142 with HTTP; Tue, 23 Dec 2014 11:02:30 -0800 (PST)
In-Reply-To: <CAMc-71nGbEkoCV9ZWGusxCe-XDpyi=i6poDzTrN6BgtZxb2wmA@mail.gmail.com>
References: <CAMc-71k6o1MnPKCF3zObdd-Fmk=VBDWXp5W6vTXcKEi6jDjL6g@mail.gmail.com>
	<59D086C1-3E76-4754-9770-64273DAF2E54@gmail.com>
	<CAMc-71nGbEkoCV9ZWGusxCe-XDpyi=i6poDzTrN6BgtZxb2wmA@mail.gmail.com>
Date: Tue, 23 Dec 2014 11:02:30 -0800
Message-ID: <CAFx0iW_rO43PkS6n1xKzu45zdDhvB2HDzTyBFYVZVBP-py1+YA@mail.gmail.com>
Subject: Re: Tuning Spark Streaming jobs
From: Timothy Chen <tnachen@gmail.com>
To: Gerard Maas <gerard.maas@gmail.com>
Cc: spark users <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Tathagata Das <tathagata.das1565@gmail.com>, andy petrella <andy.petrella@gmail.com>, 
	Xavier Tordoir <xavier@silicocloud.eu>, =?UTF-8?Q?Lieven_Gesqui=C3=A8re?= <lieven.gesquiere@virdata.com>
Content-Type: multipart/alternative; boundary=047d7b33d3d21d62d9050ae6d1ca
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d3d21d62d9050ae6d1ca
Content-Type: text/plain; charset=UTF-8

Hi Gerard,

SPARK-4286 is the ticket I am working on, which besides supporting shuffle
service it also supports the executor scaling callbacks (kill/request
total) for coarse grain mode.

I created SPARK-4940 to discuss more about the distribution problem, and
let's bring our discussions there.

Tim



On Dec 22, 2014, at 11:16 AM, Gerard Maas <gerard.maas@gmail.com> wrote:

Hi Tim,

That would be awesome. We have seen some really disparate Mesos allocations
for our Spark Streaming jobs. (like (7,4,1) over 3 executors for 4 kafka
consumer instead of the ideal (3,3,3,3))
For network dependent consumers, achieving an even deployment would
 provide a reliable and reproducible streaming job execution from the
performance point of view.
We're deploying in coarse grain mode. Not sure Spark Streaming would work
well in fine-grained given the added latency to acquire a worker.

You mention that you're changing the Mesos scheduler. Is there a Jira where
this job is taking place?

-kr, Gerard.


On Mon, Dec 22, 2014 at 6:01 PM, Timothy Chen <tnachen@gmail.com> wrote:

> Hi Gerard,
>
> Really nice guide!
>
> I'm particularly interested in the Mesos scheduling side to more evenly
> distribute cores across cluster.
>
> I wonder if you are using coarse grain mode or fine grain mode?
>
> I'm making changes to the spark mesos scheduler and I think we can propose
> a best way to achieve what you mentioned.
>
> Tim
>
> Sent from my iPhone
>
> > On Dec 22, 2014, at 8:33 AM, Gerard Maas <gerard.maas@gmail.com> wrote:
> >
> > Hi,
> >
> > After facing issues with the performance of some of our Spark Streaming
> > jobs, we invested quite some effort figuring out the factors that affect
> > the performance characteristics of a Streaming job. We  defined an
> > empirical model that helps us reason about Streaming jobs and applied it
> to
> > tune the jobs in order to maximize throughput.
> >
> > We have summarized our findings in a blog post with the intention of
> > collecting feedback and hoping that it is useful to other Spark Streaming
> > users facing similar issues.
> >
> > http://www.virdata.com/tuning-spark/
> >
> > Your feedback is welcome.
> >
> > With kind regards,
> >
> > Gerard.
> > Data Processing Team Lead
> > Virdata.com
> > @maasg
>

--047d7b33d3d21d62d9050ae6d1ca--

From dev-return-10915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 24 07:14:40 2014
Return-Path: <dev-return-10915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C148E107C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 24 Dec 2014 07:14:40 +0000 (UTC)
Received: (qmail 17523 invoked by uid 500); 24 Dec 2014 07:14:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17444 invoked by uid 500); 24 Dec 2014 07:14:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16707 invoked by uid 99); 24 Dec 2014 07:14:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 07:14:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of jianshi.huang@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 07:14:09 +0000
Received: by mail-lb0-f177.google.com with SMTP id b6so6241982lbj.22;
        Tue, 23 Dec 2014 23:12:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=/8ODsIc0kOQ4Ex8yseu0KQnCoqEtfNEtZoN5vYJaaKc=;
        b=r0uedF5Zk1Ezjjb5yrd7mHuC5ETyx+XvnEEcRB8GzmD+f5Gnmx3CnLQBvNDwOLTW5O
         zr+WjUDB5i7Eu+YMYClgJs1tI+ybyXDlajEjEzPydzE53oy4xFXDWS+apSxoSHSbRaua
         LOy6o+e5Y/QfmsbQbp82RQeAuCV+xC3z7HSvZuPKD9Y4QV/varwj/cbe0C94MqYsWsQ3
         +roz1li8we2OGLrFK754Iqc4PGHJqquh7da+BHjM3Kkx45ZLOpcnbqb8KVTEdgUr+aWu
         XcYYjOVCWnpHzUmXew/tEduHOxFdOsc32fQgFi+B7uXpzmegEOUJssEtwVn8HJRc9xi5
         hBgA==
X-Received: by 10.152.18.225 with SMTP id z1mr32832628lad.0.1419405157452;
 Tue, 23 Dec 2014 23:12:37 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.31.72 with HTTP; Tue, 23 Dec 2014 23:12:17 -0800 (PST)
In-Reply-To: <CAAswR-4YtZHTRDVjEhKERPvjfPmmHup+JoeYLFFnTFYPnKP9mw@mail.gmail.com>
References: <CACA1tWJFdVw8+Mh20HZ8Dedjsq-L-fP9_jiFO=GCm7L=ryYCjg@mail.gmail.com>
 <CACA1tW+=HAmTtu0GgmZSgRj0bYY9v4uLBMrmFtroD_Snivh=KQ@mail.gmail.com>
 <CACA1tW+f4m+=8Pc2sXJTR9S6Jxxx-an1Dhqp=vhgJ4AWVXFmtw@mail.gmail.com>
 <CACA1tW+rfWyVPwa78saBpNR6a-1oTNK6jTH3xcEoH0xhAB-8Ag@mail.gmail.com>
 <CACA1tW+zO3AZSStG6ML5dh2Ss2MjejQVQbf=FPVBWWs-2tiO5A@mail.gmail.com>
 <CAAswR-7=doK5JkUoxvPfTT0g0UmQ6hUMWHUZhxuA9WdhUobWNA@mail.gmail.com>
 <CACA1tWLGnZ_RjtnZOdz+ZLDWMQiO7oA53o8J7G3FWz8EOSnbuA@mail.gmail.com> <CAAswR-4YtZHTRDVjEhKERPvjfPmmHup+JoeYLFFnTFYPnKP9mw@mail.gmail.com>
From: Jianshi Huang <jianshi.huang@gmail.com>
Date: Wed, 24 Dec 2014 15:12:17 +0800
Message-ID: <CACA1tWLTMRaSE1MKRqN4AwNaNCLUDaF-u5d7rQYtxT9wprh2iQ@mail.gmail.com>
Subject: Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)
To: Michael Armbrust <michael@databricks.com>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e014940ae33d4f2050af10433
X-Virus-Checked: Checked by ClamAV on apache.org

--089e014940ae33d4f2050af10433
Content-Type: text/plain; charset=UTF-8

FYI,

Latest hive 0.14/parquet will have column renaming support.

Jianshi

On Wed, Dec 10, 2014 at 3:37 AM, Michael Armbrust <michael@databricks.com>
wrote:

> You might also try out the recently added support for views.
>
> On Mon, Dec 8, 2014 at 9:31 PM, Jianshi Huang <jianshi.huang@gmail.com>
> wrote:
>
>> Ah... I see. Thanks for pointing it out.
>>
>> Then it means we cannot mount external table using customized column
>> names. hmm...
>>
>> Then the only option left is to use a subquery to add a bunch of column
>> alias. I'll try it later.
>>
>> Thanks,
>> Jianshi
>>
>> On Tue, Dec 9, 2014 at 3:34 AM, Michael Armbrust <michael@databricks.com>
>> wrote:
>>
>>> This is by hive's design.  From the Hive documentation:
>>>
>>> The column change command will only modify Hive's metadata, and will not
>>>> modify data. Users should make sure the actual data layout of the
>>>> table/partition conforms with the metadata definition.
>>>
>>>
>>>
>>> On Sat, Dec 6, 2014 at 8:28 PM, Jianshi Huang <jianshi.huang@gmail.com>
>>> wrote:
>>>
>>>> Ok, found another possible bug in Hive.
>>>>
>>>> My current solution is to use ALTER TABLE CHANGE to rename the column
>>>> names.
>>>>
>>>> The problem is after renaming the column names, the value of the
>>>> columns became all NULL.
>>>>
>>>> Before renaming:
>>>> scala> sql("select `sorted::cre_ts` from pmt limit 1").collect
>>>> res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])
>>>>
>>>> Execute renaming:
>>>> scala> sql("alter table pmt change `sorted::cre_ts` cre_ts string")
>>>> res13: org.apache.spark.sql.SchemaRDD =
>>>> SchemaRDD[972] at RDD at SchemaRDD.scala:108
>>>> == Query Plan ==
>>>> <Native command: executed by Hive>
>>>>
>>>> After renaming:
>>>> scala> sql("select cre_ts from pmt limit 1").collect
>>>> res16: Array[org.apache.spark.sql.Row] = Array([null])
>>>>
>>>> I created a JIRA for it:
>>>>
>>>>   https://issues.apache.org/jira/browse/SPARK-4781
>>>>
>>>>
>>>> Jianshi
>>>>
>>>> On Sun, Dec 7, 2014 at 1:06 AM, Jianshi Huang <jianshi.huang@gmail.com>
>>>> wrote:
>>>>
>>>>> Hmm... another issue I found doing this approach is that ANALYZE TABLE
>>>>> ... COMPUTE STATISTICS will fail to attach the metadata to the table, and
>>>>> later broadcast join and such will fail...
>>>>>
>>>>> Any idea how to fix this issue?
>>>>>
>>>>> Jianshi
>>>>>
>>>>> On Sat, Dec 6, 2014 at 9:10 PM, Jianshi Huang <jianshi.huang@gmail.com
>>>>> > wrote:
>>>>>
>>>>>> Very interesting, the line doing drop table will throws an exception.
>>>>>> After removing it all works.
>>>>>>
>>>>>> Jianshi
>>>>>>
>>>>>> On Sat, Dec 6, 2014 at 9:11 AM, Jianshi Huang <
>>>>>> jianshi.huang@gmail.com> wrote:
>>>>>>
>>>>>>> Here's the solution I got after talking with Liancheng:
>>>>>>>
>>>>>>> 1) using backquote `..` to wrap up all illegal characters
>>>>>>>
>>>>>>>     val rdd = parquetFile(file)
>>>>>>>     val schema = rdd.schema.fields.map(f => s"`${f.name}`
>>>>>>> ${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
>>>>>>>
>>>>>>>     val ddl_13 = s"""
>>>>>>>       |CREATE EXTERNAL TABLE $name (
>>>>>>>       |  $schema
>>>>>>>       |)
>>>>>>>       |STORED AS PARQUET
>>>>>>>       |LOCATION '$file'
>>>>>>>       """.stripMargin
>>>>>>>
>>>>>>>     sql(ddl_13)
>>>>>>>
>>>>>>> 2) create a new Schema and do applySchema to generate a new
>>>>>>> SchemaRDD, had to drop and register table
>>>>>>>
>>>>>>>     val t = table(name)
>>>>>>>     val newSchema = StructType(t.schema.fields.map(s => s.copy(name
>>>>>>> = s.name.replaceAll(".*?::", ""))))
>>>>>>>     sql(s"drop table $name")
>>>>>>>     applySchema(t, newSchema).registerTempTable(name)
>>>>>>>
>>>>>>> I'm testing it for now.
>>>>>>>
>>>>>>> Thanks for the help!
>>>>>>>
>>>>>>>
>>>>>>> Jianshi
>>>>>>>
>>>>>>> On Sat, Dec 6, 2014 at 8:41 AM, Jianshi Huang <
>>>>>>> jianshi.huang@gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> I had to use Pig for some preprocessing and to generate Parquet
>>>>>>>> files for Spark to consume.
>>>>>>>>
>>>>>>>> However, due to Pig's limitation, the generated schema contains
>>>>>>>> Pig's identifier
>>>>>>>>
>>>>>>>> e.g.
>>>>>>>> sorted::id, sorted::cre_ts, ...
>>>>>>>>
>>>>>>>> I tried to put the schema inside CREATE EXTERNAL TABLE, e.g.
>>>>>>>>
>>>>>>>>   create external table pmt (
>>>>>>>>     sorted::id bigint
>>>>>>>>   )
>>>>>>>>   stored as parquet
>>>>>>>>   location '...'
>>>>>>>>
>>>>>>>> Obviously it didn't work, I also tried removing the identifier
>>>>>>>> sorted::, but the resulting rows contain only nulls.
>>>>>>>>
>>>>>>>> Any idea how to create a table in HiveContext from these Parquet
>>>>>>>> files?
>>>>>>>>
>>>>>>>> Thanks,
>>>>>>>> Jianshi
>>>>>>>> --
>>>>>>>> Jianshi Huang
>>>>>>>>
>>>>>>>> LinkedIn: jianshi
>>>>>>>> Twitter: @jshuang
>>>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Jianshi Huang
>>>>>>>
>>>>>>> LinkedIn: jianshi
>>>>>>> Twitter: @jshuang
>>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Jianshi Huang
>>>>>>
>>>>>> LinkedIn: jianshi
>>>>>> Twitter: @jshuang
>>>>>> Github & Blog: http://huangjs.github.com/
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Jianshi Huang
>>>>>
>>>>> LinkedIn: jianshi
>>>>> Twitter: @jshuang
>>>>> Github & Blog: http://huangjs.github.com/
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jianshi Huang
>>>>
>>>> LinkedIn: jianshi
>>>> Twitter: @jshuang
>>>> Github & Blog: http://huangjs.github.com/
>>>>
>>>
>>>
>>
>>
>> --
>> Jianshi Huang
>>
>> LinkedIn: jianshi
>> Twitter: @jshuang
>> Github & Blog: http://huangjs.github.com/
>>
>
>


-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

--089e014940ae33d4f2050af10433--

From dev-return-10916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 24 08:50:21 2014
Return-Path: <dev-return-10916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FB1A109CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 24 Dec 2014 08:50:21 +0000 (UTC)
Received: (qmail 36560 invoked by uid 500); 24 Dec 2014 08:50:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36480 invoked by uid 500); 24 Dec 2014 08:50:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36469 invoked by uid 99); 24 Dec 2014 08:50:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 08:50:18 +0000
X-ASF-Spam-Status: No, hits=3.0 required=10.0
	tests=FORGED_YAHOO_RCVD,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: encountered temporary error during SPF processing of domain of tanejagagan@yahoo.com)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 08:49:52 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id A033CE9DC6D
	for <dev@spark.apache.org>; Wed, 24 Dec 2014 00:48:30 -0800 (PST)
Date: Wed, 24 Dec 2014 01:48:30 -0700 (MST)
From: tanejagagan <tanejagagan@yahoo.com>
To: dev@spark.apache.org
Message-ID: <1419410910615-9905.post@n3.nabble.com>
In-Reply-To: <CAAswR-7qPemUBGJd1CNCd_8sR0hGaxnLsnvtMd_tbbq5pwkPmA@mail.gmail.com>
References: <CAKWX9VV4R73ae239ZEFtLR1WXPOCAKwXaEEUeZT8fz3GrhiC0A@mail.gmail.com> <CAAswR-7qPemUBGJd1CNCd_8sR0hGaxnLsnvtMd_tbbq5pwkPmA@mail.gmail.com>
Subject: Re: Support for Hive buckets
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

(for example, you might be 
able to avoid a shuffle when doing joins on tables that are already 
bucketed by exposing more metastore information to the planner). 

Can you provide more input on how to implement this functionality so that i
can speed up join between 2 hive tables, both with few billion rows 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Support-for-Hive-buckets-tp8421p9905.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 24 21:15:14 2014
Return-Path: <dev-return-10917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72BEF10CDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 24 Dec 2014 21:15:14 +0000 (UTC)
Received: (qmail 83693 invoked by uid 500); 24 Dec 2014 21:15:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83625 invoked by uid 500); 24 Dec 2014 21:15:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83608 invoked by uid 99); 24 Dec 2014 21:15:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 21:15:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 21:14:43 +0000
Received: by mail-pd0-f178.google.com with SMTP id r10so10465091pdi.37
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 13:13:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=nV7EEIkzvGgzGGkVU0UT+r1xPPPrmPYiDzAzPr0iBd0=;
        b=cMSdFNONDzC48qBZqx5rI35ybzf6uVyDm7JqH0ZvZ9wkjoXhbcJrg+tI/Swi9zFLWd
         XO+4hw6IraUae9W0Gvha7XKnyuxymxRXReqXKuW5+JieyA9w6s3VmN66DGC20aGt9q4/
         /sY1d00u4thXEmrod4gSKJqFmLivWEGGMtzf+jDhXwysyKLqmIqbU7m0MNrk9y3AigWa
         DDz8to46XE8LaurfnllLcPGq4qlGn00KgKwS68wqOF/t4wcVegxXPh6d8MrQFyC9HLTF
         I+xMVnYcXRh1yLI8TVU/euOENTCN/UvWgMuLOfTmQBZvFtm6KPGKqN4Yde8IRzzGSZK3
         xI8Q==
X-Received: by 10.70.52.33 with SMTP id q1mr52728863pdo.64.1419455637287;
        Wed, 24 Dec 2014 13:13:57 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id oz7sm23709257pbb.14.2014.12.24.13.13.56
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 24 Dec 2014 13:13:56 -0800 (PST)
Date: Wed, 24 Dec 2014 13:13:55 -0800
From: Josh Rosen <rosenville@gmail.com>
To: Cody Koeninger <cody@koeninger.org>, Marcelo Vanzin
 <vanzin@cloudera.com>
Cc: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <etPan.549b2c93.327b23c6.116@joshs-mbp>
In-Reply-To: <CAAOnQ7s0Lk4s4nLN719TGNO3O6RUzB-y_Sgx3URmx+rh+_A1qQ@mail.gmail.com>
References: <CAKWX9VVEUkz7mf74=X=44sr2SjVx=rB6RHmvu9ck9O4Gah1ZJA@mail.gmail.com>
 <CAAOnQ7s0Lk4s4nLN719TGNO3O6RUzB-y_Sgx3URmx+rh+_A1qQ@mail.gmail.com>
Subject: Re: cleaning up cache files left by SPARK-2713
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="549b2c93_643c9869_116"
X-Virus-Checked: Checked by ClamAV on apache.org

--549b2c93_643c9869_116
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

I reviewed and merged that PR, in case you want to try out the fix.

- Josh

On December 22, 2014 at 10:40:35 AM, Marcelo Vanzin (vanzin@cloudera.com) wrote:

https://github.com/apache/spark/pull/3705 

On Mon, Dec 22, 2014 at 10:19 AM, Cody Koeninger <cody@koeninger.org> wrote: 
> Is there a reason not to go ahead and move the _cache and _lock files 
> created by Utils.fetchFiles into the work directory, so they can be cleaned 
> up more easily? I saw comments to that effect in the discussion of the PR 
> for 2713, but it doesn't look like it got done. 
> 
> And no, I didn't just have a machine fill up the /tmp directory, why do you 
> ask? :) 



-- 
Marcelo 

--------------------------------------------------------------------- 
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
For additional commands, e-mail: dev-help@spark.apache.org 


--549b2c93_643c9869_116--


From dev-return-10918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 24 21:21:05 2014
Return-Path: <dev-return-10918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D05AB10D02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 24 Dec 2014 21:21:05 +0000 (UTC)
Received: (qmail 91428 invoked by uid 500); 24 Dec 2014 21:21:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91344 invoked by uid 500); 24 Dec 2014 21:21:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91089 invoked by uid 99); 24 Dec 2014 21:21:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 21:21:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Dec 2014 21:20:36 +0000
Received: by mail-pa0-f48.google.com with SMTP id rd3so10589796pab.35
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 13:19:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=U7vnruEViIVT9nESuwoUq5gKWtE+PfKy7X5KpkoXoQI=;
        b=Bils1rQrzlpduwVjuDrHtN6Ig4MRsQ1Iq6Mh+/wXmeI+fOVxX1cYbgGzxWHiLkn1Eu
         5OJRffFKPMuY+amYljJk0dCOYCM46+Rfz5Wvtl2lBMgQ+k3rGgFZf5DXhEluodDibsaG
         KtnLUO9dw7Z2Y1eeZXpQAfZdO+m/WOFRHrZ7yc1JoWvaKo/AEfH2JVxo9y6MUmJlcKwE
         iaxmftnLuA+WLk8YSc++z6FMr4qaM1+JqxVPABF+8aPu9iHmph5dTRW0jeWCq9LxCsx0
         ceKkRrbvt7UunoWuoA/eq5O/G5yy9DZFUhKc426nfGTmofntVgtFf9JnqXBkD+F6pqxg
         Ck9g==
X-Received: by 10.68.171.97 with SMTP id at1mr47078981pbc.54.1419455944353;
        Wed, 24 Dec 2014 13:19:04 -0800 (PST)
Received: from joshs-mbp (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id nb5sm23698226pbc.25.2014.12.24.13.18.58
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 24 Dec 2014 13:18:58 -0800 (PST)
Date: Wed, 24 Dec 2014 13:18:57 -0800
From: Josh Rosen <rosenville@gmail.com>
To: thlee <tiong@ooyala.com>, dev@spark.apache.org
Message-ID: <etPan.549b2dc1.327b23c6.787d@joshs-mbp>
In-Reply-To: <1419015113315-9855.post@n3.nabble.com>
References: <1418686647469-9798.post@n3.nabble.com>
 <1419015113315-9855.post@n3.nabble.com>
Subject: Re: Confirming race condition in DagScheduler
 (NoSuchElementException)
X-Mailer: Airmail (284)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="549b2dc1_643c9869_787d"
X-Virus-Checked: Checked by ClamAV on apache.org

--549b2dc1_643c9869_787d
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I=E2=80=99m investigating this issue and left some comments on the propos=
ed fix:=C2=A0https://github.com/apache/spark/pull/3345=23issuecomment-680=
14353

To summarize, I agree with your description of the problem but think that=
 the right fix may be a bit more involved than what=E2=80=99s proposed in=
 that PR (that PR=E2=80=99s fix shouldn=E2=80=99t actually work, as far a=
s I can tell).

- Josh

On December 19, 2014 at 10:57:41 AM, thlee (tiong=40ooyala.com) wrote:

any comments=3F =20



-- =20
View this message in context: http://apache-spark-developers-list.1001551=
.n3.nabble.com/Confirming-race-condition-in-DagScheduler-NoSuchElementExc=
eption-tp9798p9855.html =20
Sent from the Apache Spark Developers List mailing list archive at Nabble=
.com. =20

--------------------------------------------------------------------- =20
To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org =20
=46or additional commands, e-mail: dev-help=40spark.apache.org =20


--549b2dc1_643c9869_787d--


From dev-return-10919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 05:00:54 2014
Return-Path: <dev-return-10919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B2FDC55E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 05:00:54 +0000 (UTC)
Received: (qmail 2050 invoked by uid 500); 25 Dec 2014 05:00:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1787 invoked by uid 500); 25 Dec 2014 05:00:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1775 invoked by uid 99); 25 Dec 2014 05:00:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:00:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:00:23 +0000
Received: by mail-qc0-f175.google.com with SMTP id b13so7069300qcw.20
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 20:59:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=7wfQbNmWLVN0Bk6KjM4ESRnBaAVMUBG4SHimgFmg9dU=;
        b=limXWICLfp/xBiZY1Ln9CoU6z7tqKRIeaxJIeaB5hqdlTmZI7PHGGUD7DOBKXcNpy/
         r2lyACKVkmAa3IVUjDcQXtCS1Xkmb3G27w+O/FwUPjCHJKofgeiUmtLBcizOByKqx3t7
         uILb2r3FMOgVfd6ehzcHIfjINbB9gpAUSQM4pcr3sHXL1sjJzW1p0zN83TEArQDEC7iU
         xFK5GjVQkfQCXXCGT7NkoTfbg8ySTKxCEAIM2eTqt7wsesWQHhiQ7kZdNGfVK392WZyC
         GDul6s8EaHV4uJzyoBFU2TNFs6p1hblb1q1EFLO3o+Yfn5SNCtqKMmR7fDllWVxdvInZ
         hmJA==
X-Gm-Message-State: ALoCoQl0EmqGrhiGUsmBRXWZxl+6Rq/DeN833VBHunbm+I52OyxCURf3KbomVps8cAtELJF0WSy3
MIME-Version: 1.0
X-Received: by 10.224.74.70 with SMTP id t6mr8253848qaj.15.1419483566123; Wed,
 24 Dec 2014 20:59:26 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Wed, 24 Dec 2014 20:59:26 -0800 (PST)
Date: Wed, 24 Dec 2014 23:59:26 -0500
Message-ID: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
Subject: Starting with Spark
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3beaeb93f40050b034524
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3beaeb93f40050b034524
Content-Type: text/plain; charset=UTF-8

Hi All,

I am starting to use Spark. I am having trouble getting the latest code
from git.
I am using Intellij as suggested in the below link,

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-StarterTasks


The below link isn't working as well,

http://spark.apache.org/building-spark.html


Does anyone know any useful links to get spark running on local laptop

Please help.


Thanks

--001a11c3beaeb93f40050b034524--

From dev-return-10920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 05:08:53 2014
Return-Path: <dev-return-10920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6EA1FC59B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 05:08:53 +0000 (UTC)
Received: (qmail 11146 invoked by uid 500); 25 Dec 2014 05:08:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11071 invoked by uid 500); 25 Dec 2014 05:08:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11053 invoked by uid 99); 25 Dec 2014 05:08:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:08:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tnachen@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:08:23 +0000
Received: by mail-pa0-f45.google.com with SMTP id lf10so11238085pab.4
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 21:08:22 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=bWow+wfQn6vzebLKtllfZCc6a+f/05XDMUkg9u50AjQ=;
        b=sW/LRlhmTj3yFjmCsSsLKJeq/ZG03CErWIGFY8MM0IAUsC31i7y3N0k50hERJ2pUfO
         RkTRp1L/EzEBX9Pq+332AlVLTJoTbaQKtT6IGtfse/uDAb998rCDB/PRxAPAQfXRHGoc
         jRvT/u7TuYySyf/iORzX7pB61E4WUX1/zeszP4aDikcrYamuAkxpMhqwHPn8Ck1Qcn00
         x6XMmRSLJQ4MSgfM2z4jMBv9Zzu2ej/AgzETiRXPPIXkyQ/mHj2RKV0ZoXvYJVnAZHpC
         vwI6ONniQmCU0L1HH4hexa4XKQj4EjV3NQphNmggI6NmDi77NKjNT6wObCKOV9UK6Oo/
         mfTA==
X-Received: by 10.68.226.166 with SMTP id rt6mr7224507pbc.120.1419484102218;
        Wed, 24 Dec 2014 21:08:22 -0800 (PST)
Received: from [10.184.4.21] ([166.170.40.239])
        by mx.google.com with ESMTPSA id v4sm24297636pdc.61.2014.12.24.21.08.21
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 24 Dec 2014 21:08:21 -0800 (PST)
Content-Type: text/plain;
	charset=us-ascii
Mime-Version: 1.0 (1.0)
Subject: Re: Starting with Spark
From: Timothy Chen <tnachen@gmail.com>
X-Mailer: iPhone Mail (12B440)
In-Reply-To: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
Date: Wed, 24 Dec 2014 21:08:20 -0800
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <5EEA2347-1148-4AD4-956B-9703578E9078@gmail.com>
References: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
To: Naveen Madhire <vmadhire@umail.iu.edu>
X-Virus-Checked: Checked by ClamAV on apache.org

What error are you getting?

Tim

Sent from my iPhone

> On Dec 24, 2014, at 8:59 PM, Naveen Madhire <vmadhire@umail.iu.edu> wrote:=

>=20
> Hi All,
>=20
> I am starting to use Spark. I am having trouble getting the latest code
> from git.
> I am using Intellij as suggested in the below link,
>=20
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#Co=
ntributingtoSpark-StarterTasks
>=20
>=20
> The below link isn't working as well,
>=20
> http://spark.apache.org/building-spark.html
>=20
>=20
> Does anyone know any useful links to get spark running on local laptop
>=20
> Please help.
>=20
>=20
> Thanks

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 05:37:25 2014
Return-Path: <dev-return-10921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C5FA0C5CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 05:37:25 +0000 (UTC)
Received: (qmail 20733 invoked by uid 500); 25 Dec 2014 05:37:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20654 invoked by uid 500); 25 Dec 2014 05:37:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20642 invoked by uid 99); 25 Dec 2014 05:37:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:37:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of era.yeung@gmail.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 05:37:16 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so19402304oia.1
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 21:34:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=bNJreKleIjlBhdw7UTLmkNKXqX+DGRGkKzbHjZNVf+w=;
        b=UIoz7HNttJIFZCaM23g7JADkiJfnesghhqCrMwRM0bUWKQYZFmd4+84iT2pI2xOnNA
         yhDDARPxRDkvvmvJ+2xXAaD3woYX+lw9Rf6awwqV8VZmWSry4F690myIUz2qEdAZfOwF
         +rO54eJZKsseKGOCbxnwdvQoPAAW55oZHQBrrCFqOMFzbWZ8yPbz1J0CXInypnwL3de7
         jN02vaDJp9k/b9TZZCUn10VRFmrDOBVK3zvHyTEOWeqJAHIrofBfXDh506sJ2tfQdG0w
         NXxqzVlgMVsXOVw/BtqR+VEHzCm04aWBGEP4hU22RDKAQBKbTo00quuxwgW5AKB0Ij+R
         LHJQ==
MIME-Version: 1.0
X-Received: by 10.182.66.8 with SMTP id b8mr14534815obt.69.1419485680786; Wed,
 24 Dec 2014 21:34:40 -0800 (PST)
Received: by 10.76.127.177 with HTTP; Wed, 24 Dec 2014 21:34:40 -0800 (PST)
Date: Thu, 25 Dec 2014 13:34:40 +0800
Message-ID: <CAKDPS92xiL9jOpTiDye-+crfX2Fh5a2aS3FyN3J5Fg-Ar6c87g@mail.gmail.com>
Subject: Problems with large dataset using collect() and broadcast()
From: Will Yang <era.yeung@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f923faac460ca050b03c3d6
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f923faac460ca050b03c3d6
Content-Type: text/plain; charset=UTF-8

Hi all,
In my occasion, I have a huge HashMap[(Int, Long), (Double, Double,
Double)], say several GB to tens of GB, after each iteration, I need to
collect() this HashMap and perform some calculation, and then broadcast()
it to every node. Now I have 20GB for each executor and after it
performances collect(), it gets stuck at "Added rdd_xx_xx", no further
respond showed on the Application UI.

I've tried to lower the spark.shuffle.memoryFraction and
spark.storage.memoryFraction, but it seems that it can only deal with as
much as 2GB HashMap. What should I optimize for such conditions.

(ps: sorry for my bad English & Grammar)


Thanks

--e89a8f923faac460ca050b03c3d6--

From dev-return-10922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 06:43:32 2014
Return-Path: <dev-return-10922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DEAEC69D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 06:43:32 +0000 (UTC)
Received: (qmail 48205 invoked by uid 500); 25 Dec 2014 06:43:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48132 invoked by uid 500); 25 Dec 2014 06:43:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48116 invoked by uid 99); 25 Dec 2014 06:43:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:43:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.48 as permitted sender)
Received: from [209.85.218.48] (HELO mail-oi0-f48.google.com) (209.85.218.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:43:02 +0000
Received: by mail-oi0-f48.google.com with SMTP id u20so19535614oif.7
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 22:42:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=HsBazmzetPswZzDhOmj/UL+9ljhiab1BC0KRavBpMUo=;
        b=Stf6l+Gfj2L2P2qSg6sh+L8z7Mru35KC+BJ2rUp24T43pjegkcAt7mbuItRINX0Zf0
         UR68TcfgtwLbdseeJ6x4JXGVRxPgE0SgEQLuRpvKfRceNVii2/AcKwaZElk0XvQIclDk
         Z2IqHFTxhgnCugWbXU47g9+fZkB9PIWOf6+Itvbf9s6b9hHVpXKwl4ggRsCCeMwztr1Y
         X1xRIgrsjTQ+0vL+Y9jJ5NB9b9ghKU8kF+sfd+RXS8807T0MthpSBaSag+6hsN1xlbyJ
         RL7q5yTz+GBycBxFKHTowATrza6QpaDW2RLXxiEY169BTHP0gBbjOaJyNslXHp+w2YOB
         mhvg==
MIME-Version: 1.0
X-Received: by 10.60.103.138 with SMTP id fw10mr21876621oeb.18.1419489736125;
 Wed, 24 Dec 2014 22:42:16 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 24 Dec 2014 22:42:16 -0800 (PST)
In-Reply-To: <CAKDPS92xiL9jOpTiDye-+crfX2Fh5a2aS3FyN3J5Fg-Ar6c87g@mail.gmail.com>
References: <CAKDPS92xiL9jOpTiDye-+crfX2Fh5a2aS3FyN3J5Fg-Ar6c87g@mail.gmail.com>
Date: Wed, 24 Dec 2014 22:42:16 -0800
Message-ID: <CABPQxssjT22-pLQrE6h78TGOdvEMuZfF2nK_q=ugp7MT2DcTuw@mail.gmail.com>
Subject: Re: Problems with large dataset using collect() and broadcast()
From: Patrick Wendell <pwendell@gmail.com>
To: Will Yang <era.yeung@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Will,

When you call collect() the item you are collecting needs to fit in
memory on the driver. Is it possible your driver program does not have
enough memory?

- Patrick

On Wed, Dec 24, 2014 at 9:34 PM, Will Yang <era.yeung@gmail.com> wrote:
> Hi all,
> In my occasion, I have a huge HashMap[(Int, Long), (Double, Double,
> Double)], say several GB to tens of GB, after each iteration, I need to
> collect() this HashMap and perform some calculation, and then broadcast()
> it to every node. Now I have 20GB for each executor and after it
> performances collect(), it gets stuck at "Added rdd_xx_xx", no further
> respond showed on the Application UI.
>
> I've tried to lower the spark.shuffle.memoryFraction and
> spark.storage.memoryFraction, but it seems that it can only deal with as
> much as 2GB HashMap. What should I optimize for such conditions.
>
> (ps: sorry for my bad English & Grammar)
>
>
> Thanks

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 06:49:31 2014
Return-Path: <dev-return-10923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A087CC6B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 06:49:31 +0000 (UTC)
Received: (qmail 51531 invoked by uid 500); 25 Dec 2014 06:49:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51454 invoked by uid 500); 25 Dec 2014 06:49:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51441 invoked by uid 99); 25 Dec 2014 06:49:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:49:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.176 as permitted sender)
Received: from [209.85.213.176] (HELO mail-ig0-f176.google.com) (209.85.213.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:49:02 +0000
Received: by mail-ig0-f176.google.com with SMTP id l13so8229965iga.3
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 22:49:01 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=ImJLGgrLT7FpHw5MfDv4HlIk+7dbJPiuLwUtuanvXKQ=;
        b=WUGGBEXyOfzCm/NZIuqjczefZbPY4lgm1axXFlmWqH8H3IQJW9pLny7Puo1d4vavn5
         aW87YIE7nCDq1LDk0GqcSc72tyqB5B07WnD+f11TrH4y28dL25uvYc9lzmYKU7wUFtZy
         jpNRUsDABr7RTbVXm1k4ZXVoM84e75Wgd4HtCBIkwPgw6ADhXPzTPtrXMPLzih4QL+rL
         f7x6lmn92VeslzHwAbnRS1ANX/6984FrQuBJtV/I1dJC/uqKInFZUgfoE9SbJ4kqooJ0
         VP87A3nLWMUMN/yR68VIFbsDFmyB/TW3tWfbccTNgqnDRhcLLy9xb61rxvqUWIowzEEf
         gw0g==
X-Received: by 10.50.36.103 with SMTP id p7mr29499366igj.20.1419490141325;
 Wed, 24 Dec 2014 22:49:01 -0800 (PST)
MIME-Version: 1.0
References: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 25 Dec 2014 06:49:00 +0000
Message-ID: <CAOhmDzeOOi9ZNtvcutpVoWPSbzJeAqgJyjsBMqYi_7zJhuVn3A@mail.gmail.com>
Subject: Re: Starting with Spark
To: Naveen Madhire <vmadhire@umail.iu.edu>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01184d5ca2d34f050b04cd93
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184d5ca2d34f050b04cd93
Content-Type: text/plain; charset=UTF-8

The correct docs link is:
https://spark.apache.org/docs/1.2.0/building-spark.html

Where did you get that bad link from?

Nick


On Thu Dec 25 2014 at 12:00:53 AM Naveen Madhire <vmadhire@umail.iu.edu>
wrote:

> Hi All,
>
> I am starting to use Spark. I am having trouble getting the latest code
> from git.
> I am using Intellij as suggested in the below link,
>
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#
> ContributingtoSpark-StarterTasks
>
>
> The below link isn't working as well,
>
> http://spark.apache.org/building-spark.html
>
>
> Does anyone know any useful links to get spark running on local laptop
>
> Please help.
>
>
> Thanks
>

--089e01184d5ca2d34f050b04cd93--

From dev-return-10924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 06:53:15 2014
Return-Path: <dev-return-10924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9077FC6C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 06:53:15 +0000 (UTC)
Received: (qmail 59305 invoked by uid 500); 25 Dec 2014 06:53:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59214 invoked by uid 500); 25 Dec 2014 06:53:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56690 invoked by uid 99); 25 Dec 2014 06:53:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:53:13 +0000
X-ASF-Spam-Status: No, hits=-1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saisai.shao@intel.com designates 134.134.136.24 as permitted sender)
Received: from [134.134.136.24] (HELO mga09.intel.com) (134.134.136.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 06:52:47 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by orsmga102.jf.intel.com with ESMTP; 24 Dec 2014 22:50:49 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,642,1413270000"; 
   d="scan'208,217";a="659872721"
Received: from pgsmsx103.gar.corp.intel.com ([10.221.44.82])
  by orsmga002.jf.intel.com with ESMTP; 24 Dec 2014 22:52:44 -0800
Received: from shsmsx152.ccr.corp.intel.com (10.239.6.52) by
 PGSMSX103.gar.corp.intel.com (10.221.44.82) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 25 Dec 2014 14:52:43 +0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.182]) by
 SHSMSX152.ccr.corp.intel.com ([169.254.6.5]) with mapi id 14.03.0195.001;
 Thu, 25 Dec 2014 14:52:42 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Question on saveAsTextFile with overwrite option
Thread-Topic: Question on saveAsTextFile with overwrite option
Thread-Index: AdAgDs0pWmZvFtc5T6OySf5rRQKSww==
Date: Thu, 25 Dec 2014 06:52:41 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_64474308D680D540A4D8151B0F7C03F70276C5E9SHSMSX104ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_64474308D680D540A4D8151B0F7C03F70276C5E9SHSMSX104ccrcor_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

We have such requirements to save RDD output to HDFS with saveAsTextFile li=
ke API, but need to overwrite the data if existed. I'm not sure if current =
Spark support such kind of operations, or I need to check this manually?

There's a thread in mailing list discussed about this (http://apache-spark-=
user-list.1001560.n3.nabble.com/How-can-I-make-Spark-1-0-saveAsTextFile-to-=
overwrite-existing-file-td6696.html), I'm not sure this feature is enabled =
or not, or with some configurations?

Appreciate your suggestions.

Thanks a lot
Jerry

--_000_64474308D680D540A4D8151B0F7C03F70276C5E9SHSMSX104ccrcor_--

From dev-return-10925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:23:12 2014
Return-Path: <dev-return-10925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D87A0C721
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:23:12 +0000 (UTC)
Received: (qmail 75234 invoked by uid 500); 25 Dec 2014 07:23:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75145 invoked by uid 500); 25 Dec 2014 07:23:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73157 invoked by uid 99); 25 Dec 2014 07:23:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:23:08 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:23:04 +0000
Received: by mail-ob0-f171.google.com with SMTP id uz6so31526550obc.2;
        Wed, 24 Dec 2014 23:21:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=5gkzCXg2G4I8oJ4L2HxcCEQXaaPmD7WsTFmM++osg9M=;
        b=uaNEMQuAePn4pGPxvyKjeZzQFoU+6gXu0y0c6FkVTwIOsa37hv+c58ht7s4kWMkXRT
         8qqnLRsEW+xqtJj9vp7cXcWrdPzo2hQ/yDVnWeB4lm4ZL/1y7F/PcrwLr9T4ByZktlOu
         vRtJL4P7P4oVx222w5YSsfY1KRQOH+wmdwaFmjQNL+Ray3AwFlzuERhEBPoalaHqwdNK
         Ck3ErzsZInELXWBGH1GSc2Z1FHA4YdeDJCjSu1WBLJTl6UPaG/QcY6CxkvkDbAnwhb0J
         YEAg8lnmqPQoXke/CrT3ciEU5QwC9wdEtLNwvb43M+EWKB4nlXaBcB0Y3VEsNiqyohQI
         mZYA==
MIME-Version: 1.0
X-Received: by 10.202.174.198 with SMTP id x189mr20388859oie.78.1419492119076;
 Wed, 24 Dec 2014 23:21:59 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 24 Dec 2014 23:21:59 -0800 (PST)
In-Reply-To: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
References: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
Date: Wed, 24 Dec 2014 23:21:59 -0800
Message-ID: <CABPQxst4Xde987eoMmLBnA+Gau-m6FD9AwNjJ4=ZV728oLPU+w@mail.gmail.com>
Subject: Re: Question on saveAsTextFile with overwrite option
From: Patrick Wendell <pwendell@gmail.com>
To: "Shao, Saisai" <saisai.shao@intel.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Is it sufficient to set "spark.hadoop.validateOutputSpecs" to false?

http://spark.apache.org/docs/latest/configuration.html

- Patrick

On Wed, Dec 24, 2014 at 10:52 PM, Shao, Saisai <saisai.shao@intel.com> wrote:
> Hi,
>
>
>
> We have such requirements to save RDD output to HDFS with saveAsTextFile
> like API, but need to overwrite the data if existed. I'm not sure if current
> Spark support such kind of operations, or I need to check this manually?
>
>
>
> There's a thread in mailing list discussed about this
> (http://apache-spark-user-list.1001560.n3.nabble.com/How-can-I-make-Spark-1-0-saveAsTextFile-to-overwrite-existing-file-td6696.html),
> I'm not sure this feature is enabled or not, or with some configurations?
>
>
>
> Appreciate your suggestions.
>
>
>
> Thanks a lot
>
> Jerry

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:29:22 2014
Return-Path: <dev-return-10926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 459C4C740
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:29:22 +0000 (UTC)
Received: (qmail 87698 invoked by uid 500); 25 Dec 2014 07:29:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87626 invoked by uid 500); 25 Dec 2014 07:29:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84991 invoked by uid 99); 25 Dec 2014 07:29:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:29:14 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hao.cheng@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:29:10 +0000
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
  by fmsmga102.fm.intel.com with ESMTP; 24 Dec 2014 23:28:49 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,862,1389772800"; 
   d="scan'208";a="433225565"
Received: from pgsmsx104.gar.corp.intel.com ([10.221.44.91])
  by FMSMGA003.fm.intel.com with ESMTP; 24 Dec 2014 23:17:11 -0800
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 PGSMSX104.gar.corp.intel.com (10.221.44.91) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 25 Dec 2014 15:28:48 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.216]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.182]) with mapi id 14.03.0195.001;
 Thu, 25 Dec 2014 15:28:46 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Patrick Wendell <pwendell@gmail.com>, "Shao, Saisai"
	<saisai.shao@intel.com>
CC: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Question on saveAsTextFile with overwrite option
Thread-Topic: Question on saveAsTextFile with overwrite option
Thread-Index: AdAgDs0pWmZvFtc5T6OySf5rRQKSw///gzqA//94ZyA=
Date: Thu, 25 Dec 2014 07:28:45 +0000
Message-ID: <80833ADD533E324CA05C160E41B63661027BDEB7@shsmsx102.ccr.corp.intel.com>
References: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
 <CABPQxst4Xde987eoMmLBnA+Gau-m6FD9AwNjJ4=ZV728oLPU+w@mail.gmail.com>
In-Reply-To: <CABPQxst4Xde987eoMmLBnA+Gau-m6FD9AwNjJ4=ZV728oLPU+w@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

I am wondering if we can provide more friendly API, other than configuratio=
n for this purpose. What do you think Patrick?

Cheng Hao

-----Original Message-----
From: Patrick Wendell [mailto:pwendell@gmail.com]=20
Sent: Thursday, December 25, 2014 3:22 PM
To: Shao, Saisai
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: Question on saveAsTextFile with overwrite option

Is it sufficient to set "spark.hadoop.validateOutputSpecs" to false?

http://spark.apache.org/docs/latest/configuration.html

- Patrick

On Wed, Dec 24, 2014 at 10:52 PM, Shao, Saisai <saisai.shao@intel.com> wrot=
e:
> Hi,
>
>
>
> We have such requirements to save RDD output to HDFS with=20
> saveAsTextFile like API, but need to overwrite the data if existed.=20
> I'm not sure if current Spark support such kind of operations, or I need =
to check this manually?
>
>
>
> There's a thread in mailing list discussed about this=20
> (http://apache-spark-user-list.1001560.n3.nabble.com/How-can-I-make-Sp
> ark-1-0-saveAsTextFile-to-overwrite-existing-file-td6696.html),
> I'm not sure this feature is enabled or not, or with some configurations?
>
>
>
> Appreciate your suggestions.
>
>
>
> Thanks a lot
>
> Jerry

---------------------------------------------------------------------
To unsubscribe, e-mail: user-unsubscribe@spark.apache.org For additional co=
mmands, e-mail: user-help@spark.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:44:10 2014
Return-Path: <dev-return-10927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58AFEC768
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:44:10 +0000 (UTC)
Received: (qmail 95439 invoked by uid 500); 25 Dec 2014 07:44:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95369 invoked by uid 500); 25 Dec 2014 07:44:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92678 invoked by uid 99); 25 Dec 2014 07:44:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:44:05 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:44:00 +0000
Received: by mail-oi0-f49.google.com with SMTP id a141so19054294oig.8;
        Wed, 24 Dec 2014 23:42:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=IQ7rZWCJzoaM0rar9WmNQeJFo8lG9e771R+Nd5BIxYY=;
        b=Qaqfp3nath69Pcn0+NzSJOxHTXyYuohz0LnyOOHc1fB6xVr3enzbA8/fQwDfozdT52
         QqaR7S++7eS7iwH5s1ZwcUdEAI/LvvVrUNfXo24LRVVZUmnzmOrfDtmSLwPU8704d6h5
         w8yICBTo1WBSO7k1I1756TtnJtySdOa12KoQ5Q+rCeFylHAdMfG0uYcpPfgVbjri7s5U
         oPWaJXNdlk/NQbxd1tMOm6TRTuuVeT5J1yumXryG/gGEW84dZq3dVWM6RwBc7F1D27/k
         pu5ghU46pGps+9x9bPMFkxPJZlzdvU2crQ7rn+rCEV2P6PfevgqFTdIR6MPd4ZnvJo5J
         apBQ==
MIME-Version: 1.0
X-Received: by 10.182.46.134 with SMTP id v6mr21471593obm.34.1419493375236;
 Wed, 24 Dec 2014 23:42:55 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Wed, 24 Dec 2014 23:42:55 -0800 (PST)
In-Reply-To: <80833ADD533E324CA05C160E41B63661027BDEB7@shsmsx102.ccr.corp.intel.com>
References: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
	<CABPQxst4Xde987eoMmLBnA+Gau-m6FD9AwNjJ4=ZV728oLPU+w@mail.gmail.com>
	<80833ADD533E324CA05C160E41B63661027BDEB7@shsmsx102.ccr.corp.intel.com>
Date: Wed, 24 Dec 2014 23:42:55 -0800
Message-ID: <CABPQxsutNo++nbFUfij_LBf=1pZJEbATiCeDP-DxH3iU49AQ7w@mail.gmail.com>
Subject: Re: Question on saveAsTextFile with overwrite option
From: Patrick Wendell <pwendell@gmail.com>
To: "Cheng, Hao" <hao.cheng@intel.com>
Cc: "Shao, Saisai" <saisai.shao@intel.com>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

So the behavior of overwriting existing directories IMO is something
we don't want to encourage. The reason why the Hadoop client has these
checks is that it's very easy for users to do unsafe things without
them. For instance, a user could overwrite an RDD that had 100
partitions with an RDD that has 10 partitions... and if they read back
the RDD they would get a corrupted RDD that has a combination of data
from the old and new RDD.

If users want to circumvent these safety checks, we need to make them
explicitly disable them. Given this, I think a config option is as
reasonable as any alternatives. This is already pretty easy IMO.

- Patrick

On Wed, Dec 24, 2014 at 11:28 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
> I am wondering if we can provide more friendly API, other than configuration for this purpose. What do you think Patrick?
>
> Cheng Hao
>
> -----Original Message-----
> From: Patrick Wendell [mailto:pwendell@gmail.com]
> Sent: Thursday, December 25, 2014 3:22 PM
> To: Shao, Saisai
> Cc: user@spark.apache.org; dev@spark.apache.org
> Subject: Re: Question on saveAsTextFile with overwrite option
>
> Is it sufficient to set "spark.hadoop.validateOutputSpecs" to false?
>
> http://spark.apache.org/docs/latest/configuration.html
>
> - Patrick
>
> On Wed, Dec 24, 2014 at 10:52 PM, Shao, Saisai <saisai.shao@intel.com> wrote:
>> Hi,
>>
>>
>>
>> We have such requirements to save RDD output to HDFS with
>> saveAsTextFile like API, but need to overwrite the data if existed.
>> I'm not sure if current Spark support such kind of operations, or I need to check this manually?
>>
>>
>>
>> There's a thread in mailing list discussed about this
>> (http://apache-spark-user-list.1001560.n3.nabble.com/How-can-I-make-Sp
>> ark-1-0-saveAsTextFile-to-overwrite-existing-file-td6696.html),
>> I'm not sure this feature is enabled or not, or with some configurations?
>>
>>
>>
>> Appreciate your suggestions.
>>
>>
>>
>> Thanks a lot
>>
>> Jerry
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org For additional commands, e-mail: user-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:48:16 2014
Return-Path: <dev-return-10928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB061C76E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:48:16 +0000 (UTC)
Received: (qmail 97806 invoked by uid 500); 25 Dec 2014 07:48:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97734 invoked by uid 500); 25 Dec 2014 07:48:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97712 invoked by uid 99); 25 Dec 2014 07:48:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:48:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:48:08 +0000
Received: by mail-ob0-f178.google.com with SMTP id gq1so31562427obb.9
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 23:46:41 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=sPL0Xx7f1o6OgjnBA3TOyEgaX5Yx1EfIOD6FiG589rg=;
        b=LnXmUntmBe3h30d5RGUQq14kUcY9o8f0TtOngP4ZNxohCGqIbuZVp9cgSWJD/pn9sP
         W998T+nLz5Wq1r79GT+vcyp8tHUokawRHASuHB2ZRyYLcE/333wdoTTSeTuVCS0oDW7O
         eXvAGJ9GtWcp7bC1auIGd5CMBa868pMgR88eXiNL/Q++YFcWmthmYyJShDjT7tt8mJWd
         Z4751Ml4ue/iSFTXo6X+8ndMRBkOYKvyVYkBkGEwNBRnypSsQxBzzILl5KyvNRdidPAH
         pyXMCWK5tglAZ3xcBD9lxpuxfrFxNvXEjHPsy/sdZCPAPLaGTOm+CkyYU5deJih1R0vS
         eIlg==
X-Gm-Message-State: ALoCoQml0iqlruz5/w79qECO+bEp5oyALqJpgad1P6Feclkr0jsN//IMH0KI62nNqO0dezH4enoM
MIME-Version: 1.0
X-Received: by 10.202.61.9 with SMTP id k9mr20751836oia.116.1419493599462;
 Wed, 24 Dec 2014 23:46:39 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Wed, 24 Dec 2014 23:46:39 -0800 (PST)
In-Reply-To: <CANx3uAi44S_H_qVouAQcbgNDWLnygPt+fiJQe8FT5P2j80qomg@mail.gmail.com>
References: <CAKWX9VVi0sNtDx0YQyQhV5oG08_pDSBp7z0DV8+3q7RniaX54w@mail.gmail.com>
	<1419026224285.80417be1@Nodemailer>
	<CAKWX9VW42TNGt0NOYSmHtMHDoBEzsAtDv3XSfBPz0LKzpu=0NA@mail.gmail.com>
	<CANx3uAi44S_H_qVouAQcbgNDWLnygPt+fiJQe8FT5P2j80qomg@mail.gmail.com>
Date: Thu, 25 Dec 2014 01:46:39 -0600
Message-ID: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Koert Kuipers <koert@tresata.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Sean McNamara <sean.mcnamara@webtrends.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, ShaoSaisai <saisai.shao@intel.com>
Content-Type: multipart/alternative; boundary=001a113cd2a8c1df2d050b059bb1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cd2a8c1df2d050b059bb1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

After a long talk with Patrick and TD (thanks guys), I opened the following
jira

https://issues.apache.org/jira/browse/SPARK-4964

Sample PR has an impementation for the batch and the dstream case, and a
link to a project with example usage.

On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com> wrote:

> yup, we at tresata do the idempotent store the same way. very simple
> approach.
>
> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
>>
>> That KafkaRDD code is dead simple.
>>
>> Given a user specified map
>>
>> (topic1, partition0) -> (startingOffset, endingOffset)
>> (topic1, partition1) -> (startingOffset, endingOffset)
>> ...
>> turn each one of those entries into a partition of an rdd, using the
>> simple
>> consumer.
>> That's it.  No recovery logic, no state, nothing - for any failures, bai=
l
>> on the rdd and let it retry.
>> Spark stays out of the business of being a distributed database.
>>
>> The client code does any transformation it wants, then stores the data a=
nd
>> offsets.  There are two ways of doing this, either based on idempotence =
or
>> a transactional data store.
>>
>> For idempotent stores:
>>
>> 1.manipulate data
>> 2.save data to store
>> 3.save ending offsets to the same store
>>
>> If you fail between 2 and 3, the offsets haven't been stored, you start
>> again at the same beginning offsets, do the same calculations in the sam=
e
>> order, overwrite the same data, all is good.
>>
>>
>> For transactional stores:
>>
>> 1. manipulate data
>> 2. begin transaction
>> 3. save data to the store
>> 4. save offsets
>> 5. commit transaction
>>
>> If you fail before 5, the transaction rolls back.  To make this less
>> heavyweight, you can write the data outside the transaction and then
>> update
>> a pointer to the current data inside the transaction.
>>
>>
>> Again, spark has nothing much to do with guaranteeing exactly once.  In
>> fact, the current streaming api actively impedes my ability to do the
>> above.  I'm just suggesting providing an api that doesn't get in the way
>> of
>> exactly-once.
>>
>>
>>
>>
>>
>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
>> hshreedharan@cloudera.com
>> > wrote:
>>
>> > Can you explain your basic algorithm for the once-only-delivery? It is
>> > quite a bit of very Kafka-specific code, that would take more time to
>> read
>> > than I can currently afford? If you can explain your algorithm a bit, =
it
>> > might help.
>> >
>> > Thanks,
>> > Hari
>> >
>> >
>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
>> > wrote:
>> >
>> >>
>> >> The problems you guys are discussing come from trying to store state =
in
>> >> spark, so don't do that.  Spark isn't a distributed database.
>> >>
>> >> Just map kafka partitions directly to rdds, llet user code specify th=
e
>> >> range of offsets explicitly, and let them be in charge of committing
>> >> offsets.
>> >>
>> >> Using the simple consumer isn't that bad, I'm already using this in
>> >> production with the code I linked to, and tresata apparently has been
>> as
>> >> well.  Again, for everyone saying this is impossible, have you read
>> either
>> >> of those implementations and looked at the approach?
>> >>
>> >>
>> >>
>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>> >> Sean.McNamara@webtrends.com> wrote:
>> >>
>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I think the=
 exactly
>> >>> once spark streaming semantics can easily be solved using
>> updateStateByKey.
>> >>> Make the key going into updateStateByKey be a hash of the event, or
>> pluck
>> >>> off some uuid from the message.  The updateFunc would only emit the
>> message
>> >>> if the key did not exist, and the user has complete control over the
>> window
>> >>> of time / state lifecycle for detecting duplicates.  It also makes i=
t
>> >>> really easy to detect and take action (alert?) when you DO see a
>> duplicate,
>> >>> or make memory tradeoffs within an error bound using a sketch
>> algorithm.
>> >>> The kafka simple consumer is insanely complex, if possible I think i=
t
>> would
>> >>> be better (and vastly more flexible) to get reliability using the
>> >>> primitives that spark so elegantly provides.
>> >>>
>> >>> Cheers,
>> >>>
>> >>> Sean
>> >>>
>> >>>
>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>> >>> hshreedharan@cloudera.com> wrote:
>> >>> >
>> >>> > Hi Dibyendu,
>> >>> >
>> >>> > Thanks for the details on the implementation. But I still do not
>> >>> believe
>> >>> > that it is no duplicates - what they achieve is that the same batc=
h
>> is
>> >>> > processed exactly the same way every time (but see it may be
>> processed
>> >>> more
>> >>> > than once) - so it depends on the operation being idempotent. I
>> believe
>> >>> > Trident uses ZK to keep track of the transactions - a batch can be
>> >>> > processed multiple times in failure scenarios (for example, the
>> >>> transaction
>> >>> > is processed but before ZK is updated the machine fails, causing a
>> >>> "new"
>> >>> > node to process it again).
>> >>> >
>> >>> > I don't think it is impossible to do this in Spark Streaming as we=
ll
>> >>> and
>> >>> > I'd be really interested in working on it at some point in the nea=
r
>> >>> future.
>> >>> >
>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>> >>> > dibyendu.bhattachary@gmail.com> wrote:
>> >>> >
>> >>> >> Hi,
>> >>> >>
>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The
>> Storm
>> >>> >> Trident has done the exact-once guarantee by processing the tuple
>> in a
>> >>> >> batch  and assigning same transaction-id for a given batch . The
>> >>> replay for
>> >>> >> a given batch with a transaction-id will have exact same set of
>> >>> tuples and
>> >>> >> replay of batches happen in exact same order before the failure.
>> >>> >>
>> >>> >> Having this paradigm, if downstream system process data for a giv=
en
>> >>> batch
>> >>> >> for having a given transaction-id , and if during failure if same
>> >>> batch is
>> >>> >> again emitted , you can check if same transaction-id is already
>> >>> processed
>> >>> >> or not and hence can guarantee exact once semantics.
>> >>> >>
>> >>> >> And this can only be achieved in Spark if we use Low Level Kafka
>> >>> consumer
>> >>> >> API to process the offsets. This low level Kafka Consumer (
>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has implemented
>> the
>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the
>> >>> Kafka
>> >>> >> related logic has been taken from Storm-Kafka spout and which
>> manages
>> >>> all
>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>> >>> managements.
>> >>> >>
>> >>> >> Presently this Consumer maintains that during Receiver failure, i=
t
>> >>> will
>> >>> >> re-emit the exact same Block with same set of messages . Every
>> >>> message have
>> >>> >> the details of its partition, offset and topic related details
>> which
>> >>> can
>> >>> >> tackle the SPARK-3146.
>> >>> >>
>> >>> >> As this Low Level consumer has complete control over the Kafka
>> >>> Offsets ,
>> >>> >> we can implement Trident like feature on top of it like having
>> >>> implement a
>> >>> >> transaction-id for a given block , and re-emit the same block wit=
h
>> >>> same set
>> >>> >> of message during Driver failure.
>> >>> >>
>> >>> >> Regards,
>> >>> >> Dibyendu
>> >>> >>
>> >>> >>
>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
>> saisai.shao@intel.com>
>> >>> >> wrote:
>> >>> >>>
>> >>> >>> Hi all,
>> >>> >>>
>> >>> >>> I agree with Hari that Strong exact-once semantics is very hard =
to
>> >>> >>> guarantee, especially in the failure situation. From my
>> >>> understanding even
>> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
>> >>> guarantee the
>> >>> >>> exact once semantics once failed, first is the ordering of data
>> >>> replaying
>> >>> >>> from last checkpoint, this is hard to guarantee when multiple
>> >>> partitions
>> >>> >>> are injected in; second is the design complexity of achieving
>> this,
>> >>> you can
>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the ver=
y
>> >>> details
>> >>> >>> of Kafka metadata management system to achieve this, not to say
>> >>> rebalance
>> >>> >>> and fault-tolerance.
>> >>> >>>
>> >>> >>> Thanks
>> >>> >>> Jerry
>> >>> >>>
>> >>> >>> -----Original Message-----
>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.groups=
@gmail.com]
>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
>> >>> >>> To: Cody Koeninger
>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>> >>> >>> Subject: Re: Which committers care about Kafka?
>> >>> >>>
>> >>> >>> But idempotency is not that easy t achieve sometimes. A strong
>> only
>> >>> once
>> >>> >>> semantic through a proper API would  be superuseful; but I'm not
>> >>> implying
>> >>> >>> this is easy to achieve.
>> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org>
>> wrote:
>> >>> >>>
>> >>> >>>> If the downstream store for the output data is idempotent or
>> >>> >>>> transactional, and that downstream store also is the system of
>> >>> record
>> >>> >>>> for kafka offsets, then you have exactly-once semantics.  Commi=
t
>> >>> >>>> offsets with / after the data is stored.  On any failure, resta=
rt
>> >>> from
>> >>> >>> the last committed offsets.
>> >>> >>>>
>> >>> >>>> Yes, this approach is biased towards the etl-like use cases
>> rather
>> >>> >>>> than near-realtime-analytics use cases.
>> >>> >>>>
>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>> >>> >>>> hshreedharan@cloudera.com
>> >>> >>>>> wrote:
>> >>> >>>>>
>> >>> >>>>> I get what you are saying. But getting exactly once right is a=
n
>> >>> >>>>> extremely hard problem - especially in presence of failure. Th=
e
>> >>> >>>>> issue is failures
>> >>> >>>> can
>> >>> >>>>> happen in a bunch of places. For example, before the
>> notification
>> >>> of
>> >>> >>>>> downstream store being successful reaches the receiver that
>> updates
>> >>> >>>>> the offsets, the node fails. The store was successful, but
>> >>> >>>>> duplicates came in either way. This is something worth
>> discussing
>> >>> by
>> >>> >>>>> itself - but without uuids etc this might not really be solved
>> even
>> >>> >>> when you think it is.
>> >>> >>>>>
>> >>> >>>>> Anyway, I will look at the links. Even I am interested in all =
of
>> >>> the
>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>> >>> >>>>> delivery,
>> >>> >>>> but
>> >>> >>>>> I doubt the latter is really possible to guarantee - though I
>> >>> really
>> >>> >>>> would
>> >>> >>>>> love to have that!
>> >>> >>>>>
>> >>> >>>>> Thanks,
>> >>> >>>>> Hari
>> >>> >>>>>
>> >>> >>>>>
>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>> >>> >>>>> <cody@koeninger.org>
>> >>> >>>>> wrote:
>> >>> >>>>>
>> >>> >>>>>> Thanks for the replies.
>> >>> >>>>>>
>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.  If
>> you
>> >>> >>>>>> actually want exactly-once semantics, you need control of kaf=
ka
>> >>> >>>>>> offsets
>> >>> >>>> as
>> >>> >>>>>> well, including the ability to not use zookeeper as the syste=
m
>> of
>> >>> >>>>>> record for offsets.  Kafka already is a reliable system that
>> has
>> >>> >>>>>> strong
>> >>> >>>> ordering
>> >>> >>>>>> guarantees (within a partition) and does not mandate the use =
of
>> >>> >>>> zookeeper
>> >>> >>>>>> to store offsets.  I think there should be a spark api that
>> acts
>> >>> as
>> >>> >>>>>> a
>> >>> >>>> very
>> >>> >>>>>> simple intermediary between Kafka and the user's choice of
>> >>> >>>>>> downstream
>> >>> >>>> store.
>> >>> >>>>>>
>> >>> >>>>>> Take a look at the links I posted - if there's already been 2
>> >>> >>>> independent
>> >>> >>>>>> implementations of the idea, chances are it's something peopl=
e
>> >>> need.
>> >>> >>>>>>
>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
>> >>> >>>>>>>
>> >>> >>>>>>> Hi Cody,
>> >>> >>>>>>>
>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
>> >>> >>>>>>> something pretty simple and lightweight for that one.
>> >>> >>>>>>>
>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation - this
>> is
>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it is =
a
>> >>> good
>> >>> >>>> idea to
>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>> >>> >>>> optimization. For
>> >>> >>>>>>> that reason, we must be careful in implementation. There are=
 a
>> >>> >>>>>>> couple
>> >>> >>>> of
>> >>> >>>>>>> issues that we need to ensure works properly - specifically
>> >>> >>> ordering.
>> >>> >>>> To
>> >>> >>>>>>> ensure we pull messages from different topics and partitions
>> in
>> >>> >>>>>>> the
>> >>> >>>> same
>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist the =
metadata
>> to
>> >>> >>>>>>> HDFS
>> >>> >>>> (or
>> >>> >>>>>>> some other system) - this metadata must contain the order of
>> >>> >>>>>>> messages consumed, so we know how to re-read the messages. I
>> am
>> >>> >>>>>>> planning to
>> >>> >>>> explore
>> >>> >>>>>>> this once I have some time (probably in Jan). In addition, w=
e
>> >>> must
>> >>> >>>>>>> also ensure bucketing functions work fine as well. I will
>> file a
>> >>> >>>>>>> placeholder jira for this one.
>> >>> >>>>>>>
>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while back=
 -
>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that
>> >>> this
>> >>> >>>>>>> will get pulled in soon, as this is something I know people
>> want.
>> >>> >>>>>>> I am open
>> >>> >>>> to
>> >>> >>>>>>> feedback on that - anything that I can do to make it better.
>> >>> >>>>>>>
>> >>> >>>>>>> Thanks,
>> >>> >>>>>>> Hari
>> >>> >>>>>>>
>> >>> >>>>>>>
>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>> >>> >>>>>>> <pwendell@gmail.com>
>> >>> >>>>>>> wrote:
>> >>> >>>>>>>
>> >>> >>>>>>>> Hey Cody,
>> >>> >>>>>>>>
>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming is
>> TD -
>> >>> >>>>>>>> he is traveling this week though so I can respond a bit. To
>> the
>> >>> >>>>>>>> high level point of whether Kafka is important - it
>> definitely
>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support fo=
r
>> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
>> library.
>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
>> unstable
>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
>> stabilize
>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be
>> able
>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level impressio=
n
>> >>> only
>> >>> >>>>>>>> though, I haven't talked to TD about this recently so it's
>> worth
>> >>> >>> revisiting given the developments in Kafka.
>> >>> >>>>>>>>
>> >>> >>>>>>>> Please do bring things up like this on the dev list if ther=
e
>> are
>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
>> >>> >>>>>>>>
>> >>> >>>>>>>> - Patrick
>> >>> >>>>>>>>
>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>> >>> >>>>>>>> <cody@koeninger.org>
>> >>> >>>>>>>> wrote:
>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to g=
et
>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible for
>> our
>> >>> >>>>>>>> production
>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, because
>> I've
>> >>> >>>>>>>>> seen several different patches and libraries to fix the sa=
me
>> >>> >>>>>>>>> things we've
>> >>> >>>>>>>> been
>> >>> >>>>>>>>> running into.
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> Regarding flexibility
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent =
of
>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>> >>> >>>>>>>>> ourselves, then found
>> >>> >>>>>>>> that
>> >>> >>>>>>>>> PR and have been running it in production. We wouldn't be
>> able
>> >>> >>>>>>>>> to
>> >>> >>>> get
>> >>> >>>>>>>> our
>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a whol=
e
>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitra=
ry
>> >>> >>>>>>>>> delay of
>> >>> >>>>>>>> messages, etc).
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
>> >>> >>>>>>>>> WriteAheadLog
>> >>> >>>> as
>> >>> >>>>>>>> a
>> >>> >>>>>>>>> general solution for streaming unreliable sources, but Kaf=
ka
>> >>> >>>>>>>>> already
>> >>> >>>>>>>> is a
>> >>> >>>>>>>>> reliable source. I think there's a need for an api that
>> treats
>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>> >>> >>>>>>>>> write-ahead log in hdfs, I
>> >>> >>>> need
>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>> >>> >>>>>>>>> failures
>> >>> >>>> that
>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>> >>> instance).
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> I've got an implementation i've been using
>> >>> >>>>>>>>>
>> >>> >>>>>>>>>
>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> Tresata has something similar at
>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm code=
.
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries
>> rather
>> >>> >>>>>>>>> than
>> >>> >>>>>>>> patches
>> >>> >>>>>>>>> to Spark is problematic, because large portions of the
>> >>> >>>> implementation
>> >>> >>>>>>>> are
>> >>> >>>>>>>>> private[spark].
>> >>> >>>>>>>>>
>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to ge=
t.
>> >>> >>>>>>>>
>> >>> >>>>>>>>
>> >>> -----------------------------------------------------------------
>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
>> dev-unsubscribe@spark.apache.org
>> >>> For
>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>> >>> >>>>>>>>
>> >>> >>>>>>>>
>> >>> >>>>>>>
>> >>> >>>>>
>> >>> >>>>
>> >>> >>>
>> >>> >>
>> >>>
>> >>>
>> >>
>> >
>>
>

--001a113cd2a8c1df2d050b059bb1--

From dev-return-10929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:55:56 2014
Return-Path: <dev-return-10929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AAAAFC787
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:55:56 +0000 (UTC)
Received: (qmail 2236 invoked by uid 500); 25 Dec 2014 07:55:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2182 invoked by uid 500); 25 Dec 2014 07:55:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2168 invoked by uid 99); 25 Dec 2014 07:55:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:55:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hshreedharan@cloudera.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:55:25 +0000
Received: by mail-qc0-f177.google.com with SMTP id x3so6476700qcv.36
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 23:54:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:mime-version:message-id:in-reply-to
         :references:from:to:cc:subject:content-type;
        bh=BOxoS0tdBPbEG5u67Rl5AyR4PYDsHxutn44GFvdoUMg=;
        b=Y8mVu9WuEHBPPwOPXCZo7wtMARWAvQHGJurcsMoICHSQlRVXfwU5c/wLOoXvTjlSc2
         7OfT2n4D2jSe1Hk37MEc0+J8PCvv5Yv1caKQ969/6Gn2wXfGhQcx8CruCRTQAlewZYMA
         eErZK32n7B1T6Du6zUaa7aLnDwBfAxNnxaQjM+20MNLRAosTvrJmD8pav6pzl+7zI+PT
         FsXEVdXNgwqcXa6bs/T5szC9vcM831WriYSh2jGioYUTYOsemr25T+pBq0CF+G4B7cqa
         fd61al3Av8NMR9rGxiaOPOvnZVHMIuR9Qim+NZeTO1CYYEbh4jjVO2Jp5Edbm+apCvt0
         Vc+A==
X-Gm-Message-State: ALoCoQkUhjh9+a9crhoavP0MJb9r5QPWn5DBLGLkdkD5K4FRUyMysBanfbG9hla8FmEN9JIISx0Y
X-Received: by 10.140.43.195 with SMTP id e61mr55492541qga.13.1419494078636;
        Wed, 24 Dec 2014 23:54:38 -0800 (PST)
Received: from hedwig-6.prd.orcali.com (ec2-54-85-253-252.compute-1.amazonaws.com. [54.85.253.252])
        by mx.google.com with ESMTPSA id p49sm23170056qgp.30.2014.12.24.23.54.37
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 24 Dec 2014 23:54:37 -0800 (PST)
Date: Wed, 24 Dec 2014 23:54:37 -0800 (PST)
X-Google-Original-Date: Thu, 25 Dec 2014 07:54:37 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1419494077309.b1eff13f@Nodemailer>
In-Reply-To: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
X-Orchestra-Oid: EE462992-F0E7-402B-ABC4-925544BA744F
X-Orchestra-Sig: 758773d54a6bb6239d3453cc0e5ad240571e18a6
X-Orchestra-Thrid: TEB56D109-7104-46FE-9747-57A7D6EE5D69_1487840502276290343
X-Orchestra-Thrid-Sig: 257f4f6067e02a56f8c5e6e4a3be5aafec0bf555
X-Orchestra-Account: 33e1f4347435e70fd584aa6bbd99bf4412e93d20
From: "Hari Shreedharan" <hshreedharan@cloudera.com>
To: "Cody Koeninger" <cody@koeninger.org>
Cc: "Koert Kuipers" <koert@tresata.com>, dev@spark.apache.org, "Dibyendu
 Bhattacharya" <dibyendu.bhattachary@gmail.com>, "Luis
 =?UTF-8?Q?=C3=81ngel?= Vicente =?UTF-8?Q?S=C3=A1nchez?="
 <langel.groups@gmail.com>, "Patrick Wendell" <pwendell@gmail.com>,
 "ShaoSaisai" <saisai.shao@intel.com>, "Sean McNamara"
 <sean.mcnamara@webtrends.com>
Subject: Re: Which committers care about Kafka?
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1419494077742"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1419494077742
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

In general such discussions happen or is posted on the dev lists. Could you=
 please post a summary=3F Thanks.



Thanks,=C2=A0Hari

On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org>
wrote:

> After a long talk with Patrick and TD (thanks guys), I opened the =
following
> jira
> https://issues.apache.org/jira/browse/SPARK-4964
> Sample PR has an impementation for the batch and the dstream case, and a
> link to a project with example usage.
> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com> =
wrote:
>> yup, we at tresata do the idempotent store the same way. very simple
>> approach.
>>
>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>>>
>>> That KafkaRDD code is dead simple.
>>>
>>> Given a user specified map
>>>
>>> (topic1, partition0) -> (startingOffset, endingOffset)
>>> (topic1, partition1) -> (startingOffset, endingOffset)
>>> ...
>>> turn each one of those entries into a partition of an rdd, using the
>>> simple
>>> consumer.
>>> That's it.  No recovery logic, no state, nothing - for any failures, =
bail
>>> on the rdd and let it retry.
>>> Spark stays out of the business of being a distributed database.
>>>
>>> The client code does any transformation it wants, then stores the data =
and
>>> offsets.  There are two ways of doing this, either based on idempotence=
 or
>>> a transactional data store.
>>>
>>> For idempotent stores:
>>>
>>> 1.manipulate data
>>> 2.save data to store
>>> 3.save ending offsets to the same store
>>>
>>> If you fail between 2 and 3, the offsets haven't been stored, you =
start
>>> again at the same beginning offsets, do the same calculations in the =
same
>>> order, overwrite the same data, all is good.
>>>
>>>
>>> For transactional stores:
>>>
>>> 1. manipulate data
>>> 2. begin transaction
>>> 3. save data to the store
>>> 4. save offsets
>>> 5. commit transaction
>>>
>>> If you fail before 5, the transaction rolls back.  To make this less
>>> heavyweight, you can write the data outside the transaction and then
>>> update
>>> a pointer to the current data inside the transaction.
>>>
>>>
>>> Again, spark has nothing much to do with guaranteeing exactly once.  =
In
>>> fact, the current streaming api actively impedes my ability to do the
>>> above.  I'm just suggesting providing an api that doesn't get in the =
way
>>> of
>>> exactly-once.
>>>
>>>
>>>
>>>
>>>
>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
>>> hshreedharan@cloudera.com
>>> > wrote:
>>>
>>> > Can you explain your basic algorithm for the once-only-delivery=3F It=
 is
>>> > quite a bit of very Kafka-specific code, that would take more time =
to
>>> read
>>> > than I can currently afford=3F If you can explain your algorithm a =
bit, it
>>> > might help.
>>> >
>>> > Thanks,
>>> > Hari
>>> >
>>> >
>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
>>> > wrote:
>>> >
>>> >>
>>> >> The problems you guys are discussing come from trying to store state=
 in
>>> >> spark, so don't do that.  Spark isn't a distributed database.
>>> >>
>>> >> Just map kafka partitions directly to rdds, llet user code specify =
the
>>> >> range of offsets explicitly, and let them be in charge of =
committing
>>> >> offsets.
>>> >>
>>> >> Using the simple consumer isn't that bad, I'm already using this in
>>> >> production with the code I linked to, and tresata apparently has =
been
>>> as
>>> >> well.  Again, for everyone saying this is impossible, have you read
>>> either
>>> >> of those implementations and looked at the approach=3F
>>> >>
>>> >>
>>> >>
>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>>> >> Sean.McNamara@webtrends.com> wrote:
>>> >>
>>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I think =
the exactly
>>> >>> once spark streaming semantics can easily be solved using
>>> updateStateByKey.
>>> >>> Make the key going into updateStateByKey be a hash of the event, =
or
>>> pluck
>>> >>> off some uuid from the message.  The updateFunc would only emit =
the
>>> message
>>> >>> if the key did not exist, and the user has complete control over =
the
>>> window
>>> >>> of time / state lifecycle for detecting duplicates.  It also makes =
it
>>> >>> really easy to detect and take action (alert=3F) when you DO see a
>>> duplicate,
>>> >>> or make memory tradeoffs within an error bound using a sketch
>>> algorithm.
>>> >>> The kafka simple consumer is insanely complex, if possible I think =
it
>>> would
>>> >>> be better (and vastly more flexible) to get reliability using the
>>> >>> primitives that spark so elegantly provides.
>>> >>>
>>> >>> Cheers,
>>> >>>
>>> >>> Sean
>>> >>>
>>> >>>
>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>>> >>> hshreedharan@cloudera.com> wrote:
>>> >>> >
>>> >>> > Hi Dibyendu,
>>> >>> >
>>> >>> > Thanks for the details on the implementation. But I still do not
>>> >>> believe
>>> >>> > that it is no duplicates - what they achieve is that the same =
batch
>>> is
>>> >>> > processed exactly the same way every time (but see it may be
>>> processed
>>> >>> more
>>> >>> > than once) - so it depends on the operation being idempotent. I
>>> believe
>>> >>> > Trident uses ZK to keep track of the transactions - a batch can =
be
>>> >>> > processed multiple times in failure scenarios (for example, the
>>> >>> transaction
>>> >>> > is processed but before ZK is updated the machine fails, causing =
a
>>> >>> =22new=22
>>> >>> > node to process it again).
>>> >>> >
>>> >>> > I don't think it is impossible to do this in Spark Streaming as =
well
>>> >>> and
>>> >>> > I'd be really interested in working on it at some point in the =
near
>>> >>> future.
>>> >>> >
>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>>> >>> > dibyendu.bhattachary@gmail.com> wrote:
>>> >>> >
>>> >>> >> Hi,
>>> >>> >>
>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The
>>> Storm
>>> >>> >> Trident has done the exact-once guarantee by processing the =
tuple
>>> in a
>>> >>> >> batch  and assigning same transaction-id for a given batch . =
The
>>> >>> replay for
>>> >>> >> a given batch with a transaction-id will have exact same set of
>>> >>> tuples and
>>> >>> >> replay of batches happen in exact same order before the failure.=

>>> >>> >>
>>> >>> >> Having this paradigm, if downstream system process data for a =
given
>>> >>> batch
>>> >>> >> for having a given transaction-id , and if during failure if =
same
>>> >>> batch is
>>> >>> >> again emitted , you can check if same transaction-id is already
>>> >>> processed
>>> >>> >> or not and hence can guarantee exact once semantics.
>>> >>> >>
>>> >>> >> And this can only be achieved in Spark if we use Low Level =
Kafka
>>> >>> consumer
>>> >>> >> API to process the offsets. This low level Kafka Consumer (
>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has =
implemented
>>> the
>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of =
the
>>> >>> Kafka
>>> >>> >> related logic has been taken from Storm-Kafka spout and which
>>> manages
>>> >>> all
>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>>> >>> managements.
>>> >>> >>
>>> >>> >> Presently this Consumer maintains that during Receiver failure, =
it
>>> >>> will
>>> >>> >> re-emit the exact same Block with same set of messages . Every
>>> >>> message have
>>> >>> >> the details of its partition, offset and topic related details
>>> which
>>> >>> can
>>> >>> >> tackle the SPARK-3146.
>>> >>> >>
>>> >>> >> As this Low Level consumer has complete control over the Kafka
>>> >>> Offsets ,
>>> >>> >> we can implement Trident like feature on top of it like having
>>> >>> implement a
>>> >>> >> transaction-id for a given block , and re-emit the same block =
with
>>> >>> same set
>>> >>> >> of message during Driver failure.
>>> >>> >>
>>> >>> >> Regards,
>>> >>> >> Dibyendu
>>> >>> >>
>>> >>> >>
>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
>>> saisai.shao@intel.com>
>>> >>> >> wrote:
>>> >>> >>>
>>> >>> >>> Hi all,
>>> >>> >>>
>>> >>> >>> I agree with Hari that Strong exact-once semantics is very hard=
 to
>>> >>> >>> guarantee, especially in the failure situation. From my
>>> >>> understanding even
>>> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
>>> >>> guarantee the
>>> >>> >>> exact once semantics once failed, first is the ordering of =
data
>>> >>> replaying
>>> >>> >>> from last checkpoint, this is hard to guarantee when multiple
>>> >>> partitions
>>> >>> >>> are injected in; second is the design complexity of achieving
>>> this,
>>> >>> you can
>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the =
very
>>> >>> details
>>> >>> >>> of Kafka metadata management system to achieve this, not to =
say
>>> >>> rebalance
>>> >>> >>> and fault-tolerance.
>>> >>> >>>
>>> >>> >>> Thanks
>>> >>> >>> Jerry
>>> >>> >>>
>>> >>> >>> -----Original Message-----
>>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:langel.=
groups@gmail.com]
>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
>>> >>> >>> To: Cody Koeninger
>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>>> >>> >>> Subject: Re: Which committers care about Kafka=3F
>>> >>> >>>
>>> >>> >>> But idempotency is not that easy t achieve sometimes. A strong
>>> only
>>> >>> once
>>> >>> >>> semantic through a proper API would  be superuseful; but I'm =
not
>>> >>> implying
>>> >>> >>> this is easy to achieve.
>>> >>> >>> On 18 Dec 2014 21:52, =22Cody Koeninger=22 <cody@koeninger.=
org>
>>> wrote:
>>> >>> >>>
>>> >>> >>>> If the downstream store for the output data is idempotent or
>>> >>> >>>> transactional, and that downstream store also is the system =
of
>>> >>> record
>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.  =
Commit
>>> >>> >>>> offsets with / after the data is stored.  On any failure, =
restart
>>> >>> from
>>> >>> >>> the last committed offsets.
>>> >>> >>>>
>>> >>> >>>> Yes, this approach is biased towards the etl-like use cases
>>> rather
>>> >>> >>>> than near-realtime-analytics use cases.
>>> >>> >>>>
>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>> >>> >>>> hshreedharan@cloudera.com
>>> >>> >>>>> wrote:
>>> >>> >>>>>
>>> >>> >>>>> I get what you are saying. But getting exactly once right is =
an
>>> >>> >>>>> extremely hard problem - especially in presence of failure. =
The
>>> >>> >>>>> issue is failures
>>> >>> >>>> can
>>> >>> >>>>> happen in a bunch of places. For example, before the
>>> notification
>>> >>> of
>>> >>> >>>>> downstream store being successful reaches the receiver that
>>> updates
>>> >>> >>>>> the offsets, the node fails. The store was successful, but
>>> >>> >>>>> duplicates came in either way. This is something worth
>>> discussing
>>> >>> by
>>> >>> >>>>> itself - but without uuids etc this might not really be =
solved
>>> even
>>> >>> >>> when you think it is.
>>> >>> >>>>>
>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in all=
 of
>>> >>> the
>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>>> >>> >>>>> delivery,
>>> >>> >>>> but
>>> >>> >>>>> I doubt the latter is really possible to guarantee - though =
I
>>> >>> really
>>> >>> >>>> would
>>> >>> >>>>> love to have that!
>>> >>> >>>>>
>>> >>> >>>>> Thanks,
>>> >>> >>>>> Hari
>>> >>> >>>>>
>>> >>> >>>>>
>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>> >>> >>>>> <cody@koeninger.org>
>>> >>> >>>>> wrote:
>>> >>> >>>>>
>>> >>> >>>>>> Thanks for the replies.
>>> >>> >>>>>>
>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.  =
If
>>> you
>>> >>> >>>>>> actually want exactly-once semantics, you need control of =
kafka
>>> >>> >>>>>> offsets
>>> >>> >>>> as
>>> >>> >>>>>> well, including the ability to not use zookeeper as the =
system
>>> of
>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system =
that
>>> has
>>> >>> >>>>>> strong
>>> >>> >>>> ordering
>>> >>> >>>>>> guarantees (within a partition) and does not mandate the use=
 of
>>> >>> >>>> zookeeper
>>> >>> >>>>>> to store offsets.  I think there should be a spark api that
>>> acts
>>> >>> as
>>> >>> >>>>>> a
>>> >>> >>>> very
>>> >>> >>>>>> simple intermediary between Kafka and the user's choice of
>>> >>> >>>>>> downstream
>>> >>> >>>> store.
>>> >>> >>>>>>
>>> >>> >>>>>> Take a look at the links I posted - if there's already been =
2
>>> >>> >>>> independent
>>> >>> >>>>>> implementations of the idea, chances are it's something =
people
>>> >>> need.
>>> >>> >>>>>>
>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
>>> >>> >>>>>>>
>>> >>> >>>>>>> Hi Cody,
>>> >>> >>>>>>>
>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can =
implement
>>> >>> >>>>>>> something pretty simple and lightweight for that one.
>>> >>> >>>>>>>
>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation - =
this
>>> is
>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it is=
 a
>>> >>> good
>>> >>> >>>> idea to
>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>> >>> >>>> optimization. For
>>> >>> >>>>>>> that reason, we must be careful in implementation. There =
are a
>>> >>> >>>>>>> couple
>>> >>> >>>> of
>>> >>> >>>>>>> issues that we need to ensure works properly - =
specifically
>>> >>> >>> ordering.
>>> >>> >>>> To
>>> >>> >>>>>>> ensure we pull messages from different topics and =
partitions
>>> in
>>> >>> >>>>>>> the
>>> >>> >>>> same
>>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist the=
 metadata
>>> to
>>> >>> >>>>>>> HDFS
>>> >>> >>>> (or
>>> >>> >>>>>>> some other system) - this metadata must contain the order =
of
>>> >>> >>>>>>> messages consumed, so we know how to re-read the messages. =
I
>>> am
>>> >>> >>>>>>> planning to
>>> >>> >>>> explore
>>> >>> >>>>>>> this once I have some time (probably in Jan). In addition, =
we
>>> >>> must
>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I will
>>> file a
>>> >>> >>>>>>> placeholder jira for this one.
>>> >>> >>>>>>>
>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while =
back -
>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping =
that
>>> >>> this
>>> >>> >>>>>>> will get pulled in soon, as this is something I know =
people
>>> want.
>>> >>> >>>>>>> I am open
>>> >>> >>>> to
>>> >>> >>>>>>> feedback on that - anything that I can do to make it better=
.
>>> >>> >>>>>>>
>>> >>> >>>>>>> Thanks,
>>> >>> >>>>>>> Hari
>>> >>> >>>>>>>
>>> >>> >>>>>>>
>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>> >>> >>>>>>> <pwendell@gmail.com>
>>> >>> >>>>>>> wrote:
>>> >>> >>>>>>>
>>> >>> >>>>>>>> Hey Cody,
>>> >>> >>>>>>>>
>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming =
is
>>> TD -
>>> >>> >>>>>>>> he is traveling this week though so I can respond a bit. =
To
>>> the
>>> >>> >>>>>>>> high level point of whether Kafka is important - it
>>> definitely
>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support =
for
>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
>>> library.
>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
>>> unstable
>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
>>> stabilize
>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't =
be
>>> able
>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level =
impression
>>> >>> only
>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so =
it's
>>> worth
>>> >>> >>> revisiting given the developments in Kafka.
>>> >>> >>>>>>>>
>>> >>> >>>>>>>> Please do bring things up like this on the dev list if =
there
>>> are
>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
>>> >>> >>>>>>>>
>>> >>> >>>>>>>> - Patrick
>>> >>> >>>>>>>>
>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>> >>> >>>>>>>> <cody@koeninger.org>
>>> >>> >>>>>>>> wrote:
>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to =
get
>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved=3F
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible =
for
>>> our
>>> >>> >>>>>>>> production
>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, =
because
>>> I've
>>> >>> >>>>>>>>> seen several different patches and libraries to fix the =
same
>>> >>> >>>>>>>>> things we've
>>> >>> >>>>>>>> been
>>> >>> >>>>>>>>> running into.
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> Regarding flexibility
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent=
 of
>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>> >>> >>>>>>>>> ourselves, then found
>>> >>> >>>>>>>> that
>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn't =
be
>>> able
>>> >>> >>>>>>>>> to
>>> >>> >>>> get
>>> >>> >>>>>>>> our
>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a =
whole
>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388, =
arbitrary
>>> >>> >>>>>>>>> delay of
>>> >>> >>>>>>>> messages, etc).
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
>>> >>> >>>>>>>>> WriteAheadLog
>>> >>> >>>> as
>>> >>> >>>>>>>> a
>>> >>> >>>>>>>>> general solution for streaming unreliable sources, but =
Kafka
>>> >>> >>>>>>>>> already
>>> >>> >>>>>>>> is a
>>> >>> >>>>>>>>> reliable source. I think there's a need for an api that
>>> treats
>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>>> >>> >>>>>>>>> write-ahead log in hdfs, I
>>> >>> >>>> need
>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>>> >>> >>>>>>>>> failures
>>> >>> >>>> that
>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>>> >>> instance).
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> I've got an implementation i've been using
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>>
>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> Tresata has something similar at
>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm =
code.
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries
>>> rather
>>> >>> >>>>>>>>> than
>>> >>> >>>>>>>> patches
>>> >>> >>>>>>>>> to Spark is problematic, because large portions of the
>>> >>> >>>> implementation
>>> >>> >>>>>>>> are
>>> >>> >>>>>>>>> private[spark].
>>> >>> >>>>>>>>>
>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to =
get.
>>> >>> >>>>>>>>
>>> >>> >>>>>>>>
>>> >>> -----------------------------------------------------------------
>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
>>> dev-unsubscribe@spark.apache.org
>>> >>> For
>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>>> >>> >>>>>>>>
>>> >>> >>>>>>>>
>>> >>> >>>>>>>
>>> >>> >>>>>
>>> >>> >>>>
>>> >>> >>>
>>> >>> >>
>>> >>>
>>> >>>
>>> >>
>>> >
>>>
>>
------Nodemailer-0.5.0-?=_1-1419494077742--

From dev-return-10930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 07:58:57 2014
Return-Path: <dev-return-10930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D929CC793
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 07:58:57 +0000 (UTC)
Received: (qmail 8062 invoked by uid 500); 25 Dec 2014 07:58:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7994 invoked by uid 500); 25 Dec 2014 07:58:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5392 invoked by uid 99); 25 Dec 2014 07:58:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:58:53 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saisai.shao@intel.com designates 134.134.136.65 as permitted sender)
Received: from [134.134.136.65] (HELO mga03.intel.com) (134.134.136.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:58:28 +0000
Received: from orsmga001.jf.intel.com ([10.7.209.18])
  by orsmga103.jf.intel.com with ESMTP; 24 Dec 2014 23:55:45 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,642,1413270000"; 
   d="scan'208";a="628976205"
Received: from kmsmsx153.gar.corp.intel.com ([172.21.73.88])
  by orsmga001.jf.intel.com with ESMTP; 24 Dec 2014 23:58:25 -0800
Received: from shsmsx103.ccr.corp.intel.com (10.239.110.14) by
 KMSMSX153.gar.corp.intel.com (172.21.73.88) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 25 Dec 2014 15:58:24 +0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.182]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.240]) with mapi id 14.03.0195.001;
 Thu, 25 Dec 2014 15:58:22 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: Patrick Wendell <pwendell@gmail.com>, "Cheng, Hao" <hao.cheng@intel.com>
CC: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Question on saveAsTextFile with overwrite option
Thread-Topic: Question on saveAsTextFile with overwrite option
Thread-Index: AdAgDs0pWmZvFtc5T6OySf5rRQKSw///gzqA//94ZyCAAI1ygP//d4Dw
Date: Thu, 25 Dec 2014 07:58:22 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F70276C633@SHSMSX104.ccr.corp.intel.com>
References: <64474308D680D540A4D8151B0F7C03F70276C5E9@SHSMSX104.ccr.corp.intel.com>
	<CABPQxst4Xde987eoMmLBnA+Gau-m6FD9AwNjJ4=ZV728oLPU+w@mail.gmail.com>
	<80833ADD533E324CA05C160E41B63661027BDEB7@shsmsx102.ccr.corp.intel.com>
 <CABPQxsutNo++nbFUfij_LBf=1pZJEbATiCeDP-DxH3iU49AQ7w@mail.gmail.com>
In-Reply-To: <CABPQxsutNo++nbFUfij_LBf=1pZJEbATiCeDP-DxH3iU49AQ7w@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Patrick for your detailed explanation.

BR
Jerry

-----Original Message-----
From: Patrick Wendell [mailto:pwendell@gmail.com]=20
Sent: Thursday, December 25, 2014 3:43 PM
To: Cheng, Hao
Cc: Shao, Saisai; user@spark.apache.org; dev@spark.apache.org
Subject: Re: Question on saveAsTextFile with overwrite option

So the behavior of overwriting existing directories IMO is something we don=
't want to encourage. The reason why the Hadoop client has these checks is =
that it's very easy for users to do unsafe things without them. For instanc=
e, a user could overwrite an RDD that had 100 partitions with an RDD that h=
as 10 partitions... and if they read back the RDD they would get a corrupte=
d RDD that has a combination of data from the old and new RDD.

If users want to circumvent these safety checks, we need to make them expli=
citly disable them. Given this, I think a config option is as reasonable as=
 any alternatives. This is already pretty easy IMO.

- Patrick

On Wed, Dec 24, 2014 at 11:28 PM, Cheng, Hao <hao.cheng@intel.com> wrote:
> I am wondering if we can provide more friendly API, other than configurat=
ion for this purpose. What do you think Patrick?
>
> Cheng Hao
>
> -----Original Message-----
> From: Patrick Wendell [mailto:pwendell@gmail.com]
> Sent: Thursday, December 25, 2014 3:22 PM
> To: Shao, Saisai
> Cc: user@spark.apache.org; dev@spark.apache.org
> Subject: Re: Question on saveAsTextFile with overwrite option
>
> Is it sufficient to set "spark.hadoop.validateOutputSpecs" to false?
>
> http://spark.apache.org/docs/latest/configuration.html
>
> - Patrick
>
> On Wed, Dec 24, 2014 at 10:52 PM, Shao, Saisai <saisai.shao@intel.com> wr=
ote:
>> Hi,
>>
>>
>>
>> We have such requirements to save RDD output to HDFS with=20
>> saveAsTextFile like API, but need to overwrite the data if existed.
>> I'm not sure if current Spark support such kind of operations, or I need=
 to check this manually?
>>
>>
>>
>> There's a thread in mailing list discussed about this=20
>> (http://apache-spark-user-list.1001560.n3.nabble.com/How-can-I-make-S
>> p ark-1-0-saveAsTextFile-to-overwrite-existing-file-td6696.html),
>> I'm not sure this feature is enabled or not, or with some configurations=
?
>>
>>
>>
>> Appreciate your suggestions.
>>
>>
>>
>> Thanks a lot
>>
>> Jerry
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org For=20
> additional commands, e-mail: user-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 08:00:26 2014
Return-Path: <dev-return-10931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39653C79E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 08:00:26 +0000 (UTC)
Received: (qmail 12750 invoked by uid 500); 25 Dec 2014 08:00:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12666 invoked by uid 500); 25 Dec 2014 08:00:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12636 invoked by uid 99); 25 Dec 2014 08:00:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 08:00:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 07:59:44 +0000
Received: by mail-oi0-f46.google.com with SMTP id h136so19691100oig.5
        for <dev@spark.apache.org>; Wed, 24 Dec 2014 23:59:22 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=fRqoh5u70S8E8NuuAJ3Zmp5EkpxYgeIVV5GB0Kz2VUU=;
        b=kc4WxLf0h+zPKiYxo6nufpuv4o0wV7iGVMlLNjyBrNymd7B99Xw7Nw1YVBceg0ibc8
         1TjY+PHnGmD3BfqF6/EQkc1d/kWzxe2jFw5YP5J5TZnivo0y3BPx1Er75m5b27ambAel
         1MSuDRjytB++RUe0nrsZp8y0MakdcTSGNq54L61lOxwA9GfQ46L7r8d0I+uOy9VHnueO
         e5hM7IIl5iD0dXmKwg2L3/S00ijptiffBfOoo2BYeIRnSBCUSBkQqXxIY6JbH8/dtRQy
         BgUFkPMBdO2agr2kRM05iA8Ou4HJcxc27Rz9i5EpRIPH9DX6o3jB97tEBG+U5IM3lc4V
         QxeA==
X-Gm-Message-State: ALoCoQkUOeeVpdhCKda/x2BICKH1k22+WqzR5k92xQPWyQFgxlSodnYYekG777f3M1xrQQI1QTeO
MIME-Version: 1.0
X-Received: by 10.202.80.21 with SMTP id e21mr20944424oib.65.1419494362062;
 Wed, 24 Dec 2014 23:59:22 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Wed, 24 Dec 2014 23:59:21 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Wed, 24 Dec 2014 23:59:21 -0800 (PST)
In-Reply-To: <1419494077309.b1eff13f@Nodemailer>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
	<1419494077309.b1eff13f@Nodemailer>
Date: Thu, 25 Dec 2014 01:59:21 -0600
Message-ID: <CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: Saisai Shao <saisai.shao@intel.com>, Sean McNamara <sean.mcnamara@webtrends.com>, 
	Patrick Wendell <pwendell@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, dev@spark.apache.org, 
	Koert Kuipers <koert@tresata.com>
Content-Type: multipart/alternative; boundary=001a113d8408363a33050b05c9c0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d8408363a33050b05c9c0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The conversation was mostly getting TD up to speed on this thread since he
had just gotten back from his trip and hadn't seen it.

The jira has a summary of the requirements we discussed, I'm sure TD or
Patrick can add to the ticket if I missed something.
On Dec 25, 2014 1:54 AM, "Hari Shreedharan" <hshreedharan@cloudera.com>
wrote:

> In general such discussions happen or is posted on the dev lists. Could
> you please post a summary? Thanks.
>
> Thanks,
> Hari
>
>
> On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
>
>>  After a long talk with Patrick and TD (thanks guys), I opened the
>> following jira
>>
>> https://issues.apache.org/jira/browse/SPARK-4964
>>
>> Sample PR has an impementation for the batch and the dstream case, and a
>> link to a project with example usage.
>>
>> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com> wrote=
:
>>
>>> yup, we at tresata do the idempotent store the same way. very simple
>>> approach.
>>>
>>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>>>
>>>> That KafkaRDD code is dead simple.
>>>>
>>>> Given a user specified map
>>>>
>>>> (topic1, partition0) -> (startingOffset, endingOffset)
>>>> (topic1, partition1) -> (startingOffset, endingOffset)
>>>> ...
>>>> turn each one of those entries into a partition of an rdd, using the
>>>> simple
>>>> consumer.
>>>> That's it.  No recovery logic, no state, nothing - for any failures,
>>>> bail
>>>> on the rdd and let it retry.
>>>> Spark stays out of the business of being a distributed database.
>>>>
>>>> The client code does any transformation it wants, then stores the data
>>>> and
>>>> offsets.  There are two ways of doing this, either based on idempotenc=
e
>>>> or
>>>> a transactional data store.
>>>>
>>>> For idempotent stores:
>>>>
>>>> 1.manipulate data
>>>> 2.save data to store
>>>> 3.save ending offsets to the same store
>>>>
>>>> If you fail between 2 and 3, the offsets haven't been stored, you star=
t
>>>> again at the same beginning offsets, do the same calculations in the
>>>> same
>>>> order, overwrite the same data, all is good.
>>>>
>>>>
>>>> For transactional stores:
>>>>
>>>> 1. manipulate data
>>>> 2. begin transaction
>>>> 3. save data to the store
>>>> 4. save offsets
>>>> 5. commit transaction
>>>>
>>>> If you fail before 5, the transaction rolls back.  To make this less
>>>> heavyweight, you can write the data outside the transaction and then
>>>> update
>>>> a pointer to the current data inside the transaction.
>>>>
>>>>
>>>> Again, spark has nothing much to do with guaranteeing exactly once.  I=
n
>>>> fact, the current streaming api actively impedes my ability to do the
>>>> above.  I'm just suggesting providing an api that doesn't get in the
>>>> way of
>>>> exactly-once.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
>>>> hshreedharan@cloudera.com
>>>> > wrote:
>>>>
>>>> > Can you explain your basic algorithm for the once-only-delivery? It =
is
>>>> > quite a bit of very Kafka-specific code, that would take more time t=
o
>>>> read
>>>> > than I can currently afford? If you can explain your algorithm a bit=
,
>>>> it
>>>> > might help.
>>>> >
>>>> > Thanks,
>>>> > Hari
>>>> >
>>>> >
>>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org>
>>>> > wrote:
>>>> >
>>>> >>
>>>> >> The problems you guys are discussing come from trying to store stat=
e
>>>> in
>>>> >> spark, so don't do that.  Spark isn't a distributed database.
>>>> >>
>>>> >> Just map kafka partitions directly to rdds, llet user code specify
>>>> the
>>>> >> range of offsets explicitly, and let them be in charge of committin=
g
>>>> >> offsets.
>>>> >>
>>>> >> Using the simple consumer isn't that bad, I'm already using this in
>>>> >> production with the code I linked to, and tresata apparently has
>>>> been as
>>>> >> well.  Again, for everyone saying this is impossible, have you read
>>>> either
>>>> >> of those implementations and looked at the approach?
>>>> >>
>>>> >>
>>>> >>
>>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>>>> >> Sean.McNamara@webtrends.com> wrote:
>>>> >>
>>>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I think t=
he exactly
>>>> >>> once spark streaming semantics can easily be solved using
>>>> updateStateByKey.
>>>> >>> Make the key going into updateStateByKey be a hash of the event, o=
r
>>>> pluck
>>>> >>> off some uuid from the message.  The updateFunc would only emit th=
e
>>>> message
>>>> >>> if the key did not exist, and the user has complete control over
>>>> the window
>>>> >>> of time / state lifecycle for detecting duplicates.  It also makes
>>>> it
>>>> >>> really easy to detect and take action (alert?) when you DO see a
>>>> duplicate,
>>>> >>> or make memory tradeoffs within an error bound using a sketch
>>>> algorithm.
>>>> >>> The kafka simple consumer is insanely complex, if possible I think
>>>> it would
>>>> >>> be better (and vastly more flexible) to get reliability using the
>>>> >>> primitives that spark so elegantly provides.
>>>> >>>
>>>> >>> Cheers,
>>>> >>>
>>>> >>> Sean
>>>> >>>
>>>> >>>
>>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>>>> >>> hshreedharan@cloudera.com> wrote:
>>>> >>> >
>>>> >>> > Hi Dibyendu,
>>>> >>> >
>>>> >>> > Thanks for the details on the implementation. But I still do not
>>>> >>> believe
>>>> >>> > that it is no duplicates - what they achieve is that the same
>>>> batch is
>>>> >>> > processed exactly the same way every time (but see it may be
>>>> processed
>>>> >>> more
>>>> >>> > than once) - so it depends on the operation being idempotent. I
>>>> believe
>>>> >>> > Trident uses ZK to keep track of the transactions - a batch can =
be
>>>> >>> > processed multiple times in failure scenarios (for example, the
>>>> >>> transaction
>>>> >>> > is processed but before ZK is updated the machine fails, causing=
 a
>>>> >>> "new"
>>>> >>> > node to process it again).
>>>> >>> >
>>>> >>> > I don't think it is impossible to do this in Spark Streaming as
>>>> well
>>>> >>> and
>>>> >>> > I'd be really interested in working on it at some point in the
>>>> near
>>>> >>> future.
>>>> >>> >
>>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>>>> >>> > dibyendu.bhattachary@gmail.com> wrote:
>>>> >>> >
>>>> >>> >> Hi,
>>>> >>> >>
>>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The
>>>> Storm
>>>> >>> >> Trident has done the exact-once guarantee by processing the
>>>> tuple in a
>>>> >>> >> batch  and assigning same transaction-id for a given batch . Th=
e
>>>> >>> replay for
>>>> >>> >> a given batch with a transaction-id will have exact same set of
>>>> >>> tuples and
>>>> >>> >> replay of batches happen in exact same order before the failure=
.
>>>> >>> >>
>>>> >>> >> Having this paradigm, if downstream system process data for a
>>>> given
>>>> >>> batch
>>>> >>> >> for having a given transaction-id , and if during failure if sa=
me
>>>> >>> batch is
>>>> >>> >> again emitted , you can check if same transaction-id is already
>>>> >>> processed
>>>> >>> >> or not and hence can guarantee exact once semantics.
>>>> >>> >>
>>>> >>> >> And this can only be achieved in Spark if we use Low Level Kafk=
a
>>>> >>> consumer
>>>> >>> >> API to process the offsets. This low level Kafka Consumer (
>>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has
>>>> implemented the
>>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of t=
he
>>>> >>> Kafka
>>>> >>> >> related logic has been taken from Storm-Kafka spout and which
>>>> manages
>>>> >>> all
>>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>>>> >>> managements.
>>>> >>> >>
>>>> >>> >> Presently this Consumer maintains that during Receiver failure,
>>>> it
>>>> >>> will
>>>> >>> >> re-emit the exact same Block with same set of messages . Every
>>>> >>> message have
>>>> >>> >> the details of its partition, offset and topic related details
>>>> which
>>>> >>> can
>>>> >>> >> tackle the SPARK-3146.
>>>> >>> >>
>>>> >>> >> As this Low Level consumer has complete control over the Kafka
>>>> >>> Offsets ,
>>>> >>> >> we can implement Trident like feature on top of it like having
>>>> >>> implement a
>>>> >>> >> transaction-id for a given block , and re-emit the same block
>>>> with
>>>> >>> same set
>>>> >>> >> of message during Driver failure.
>>>> >>> >>
>>>> >>> >> Regards,
>>>> >>> >> Dibyendu
>>>> >>> >>
>>>> >>> >>
>>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
>>>> saisai.shao@intel.com>
>>>> >>> >> wrote:
>>>> >>> >>>
>>>> >>> >>> Hi all,
>>>> >>> >>>
>>>> >>> >>> I agree with Hari that Strong exact-once semantics is very har=
d
>>>> to
>>>> >>> >>> guarantee, especially in the failure situation. From my
>>>> >>> understanding even
>>>> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
>>>> >>> guarantee the
>>>> >>> >>> exact once semantics once failed, first is the ordering of dat=
a
>>>> >>> replaying
>>>> >>> >>> from last checkpoint, this is hard to guarantee when multiple
>>>> >>> partitions
>>>> >>> >>> are injected in; second is the design complexity of achieving
>>>> this,
>>>> >>> you can
>>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the
>>>> very
>>>> >>> details
>>>> >>> >>> of Kafka metadata management system to achieve this, not to sa=
y
>>>> >>> rebalance
>>>> >>> >>> and fault-tolerance.
>>>> >>> >>>
>>>> >>> >>> Thanks
>>>> >>> >>> Jerry
>>>> >>> >>>
>>>> >>> >>> -----Original Message-----
>>>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:
>>>> langel.groups@gmail.com]
>>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
>>>> >>> >>> To: Cody Koeninger
>>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>>>> >>> >>> Subject: Re: Which committers care about Kafka?
>>>> >>> >>>
>>>> >>> >>> But idempotency is not that easy t achieve sometimes. A strong
>>>> only
>>>> >>> once
>>>> >>> >>> semantic through a proper API would  be superuseful; but I'm n=
ot
>>>> >>> implying
>>>> >>> >>> this is easy to achieve.
>>>> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org>
>>>> wrote:
>>>> >>> >>>
>>>> >>> >>>> If the downstream store for the output data is idempotent or
>>>> >>> >>>> transactional, and that downstream store also is the system o=
f
>>>> >>> record
>>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.
>>>> Commit
>>>> >>> >>>> offsets with / after the data is stored.  On any failure,
>>>> restart
>>>> >>> from
>>>> >>> >>> the last committed offsets.
>>>> >>> >>>>
>>>> >>> >>>> Yes, this approach is biased towards the etl-like use cases
>>>> rather
>>>> >>> >>>> than near-realtime-analytics use cases.
>>>> >>> >>>>
>>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>>> >>> >>>> hshreedharan@cloudera.com
>>>> >>> >>>>> wrote:
>>>> >>> >>>>>
>>>> >>> >>>>> I get what you are saying. But getting exactly once right is
>>>> an
>>>> >>> >>>>> extremely hard problem - especially in presence of failure.
>>>> The
>>>> >>> >>>>> issue is failures
>>>> >>> >>>> can
>>>> >>> >>>>> happen in a bunch of places. For example, before the
>>>> notification
>>>> >>> of
>>>> >>> >>>>> downstream store being successful reaches the receiver that
>>>> updates
>>>> >>> >>>>> the offsets, the node fails. The store was successful, but
>>>> >>> >>>>> duplicates came in either way. This is something worth
>>>> discussing
>>>> >>> by
>>>> >>> >>>>> itself - but without uuids etc this might not really be
>>>> solved even
>>>> >>> >>> when you think it is.
>>>> >>> >>>>>
>>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in al=
l
>>>> of
>>>> >>> the
>>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>>>> >>> >>>>> delivery,
>>>> >>> >>>> but
>>>> >>> >>>>> I doubt the latter is really possible to guarantee - though =
I
>>>> >>> really
>>>> >>> >>>> would
>>>> >>> >>>>> love to have that!
>>>> >>> >>>>>
>>>> >>> >>>>> Thanks,
>>>> >>> >>>>> Hari
>>>> >>> >>>>>
>>>> >>> >>>>>
>>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>>> >>> >>>>> <cody@koeninger.org>
>>>> >>> >>>>> wrote:
>>>> >>> >>>>>
>>>> >>> >>>>>> Thanks for the replies.
>>>> >>> >>>>>>
>>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.
>>>> If you
>>>> >>> >>>>>> actually want exactly-once semantics, you need control of
>>>> kafka
>>>> >>> >>>>>> offsets
>>>> >>> >>>> as
>>>> >>> >>>>>> well, including the ability to not use zookeeper as the
>>>> system of
>>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system tha=
t
>>>> has
>>>> >>> >>>>>> strong
>>>> >>> >>>> ordering
>>>> >>> >>>>>> guarantees (within a partition) and does not mandate the us=
e
>>>> of
>>>> >>> >>>> zookeeper
>>>> >>> >>>>>> to store offsets.  I think there should be a spark api that
>>>> acts
>>>> >>> as
>>>> >>> >>>>>> a
>>>> >>> >>>> very
>>>> >>> >>>>>> simple intermediary between Kafka and the user's choice of
>>>> >>> >>>>>> downstream
>>>> >>> >>>> store.
>>>> >>> >>>>>>
>>>> >>> >>>>>> Take a look at the links I posted - if there's already been=
 2
>>>> >>> >>>> independent
>>>> >>> >>>>>> implementations of the idea, chances are it's something
>>>> people
>>>> >>> need.
>>>> >>> >>>>>>
>>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> Hi Cody,
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implemen=
t
>>>> >>> >>>>>>> something pretty simple and lightweight for that one.
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation -
>>>> this is
>>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it i=
s
>>>> a
>>>> >>> good
>>>> >>> >>>> idea to
>>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>>> >>> >>>> optimization. For
>>>> >>> >>>>>>> that reason, we must be careful in implementation. There
>>>> are a
>>>> >>> >>>>>>> couple
>>>> >>> >>>> of
>>>> >>> >>>>>>> issues that we need to ensure works properly - specificall=
y
>>>> >>> >>> ordering.
>>>> >>> >>>> To
>>>> >>> >>>>>>> ensure we pull messages from different topics and
>>>> partitions in
>>>> >>> >>>>>>> the
>>>> >>> >>>> same
>>>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist th=
e
>>>> metadata to
>>>> >>> >>>>>>> HDFS
>>>> >>> >>>> (or
>>>> >>> >>>>>>> some other system) - this metadata must contain the order =
of
>>>> >>> >>>>>>> messages consumed, so we know how to re-read the messages.
>>>> I am
>>>> >>> >>>>>>> planning to
>>>> >>> >>>> explore
>>>> >>> >>>>>>> this once I have some time (probably in Jan). In addition,
>>>> we
>>>> >>> must
>>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I will
>>>> file a
>>>> >>> >>>>>>> placeholder jira for this one.
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while
>>>> back -
>>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping
>>>> that
>>>> >>> this
>>>> >>> >>>>>>> will get pulled in soon, as this is something I know peopl=
e
>>>> want.
>>>> >>> >>>>>>> I am open
>>>> >>> >>>> to
>>>> >>> >>>>>>> feedback on that - anything that I can do to make it bette=
r.
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> Thanks,
>>>> >>> >>>>>>> Hari
>>>> >>> >>>>>>>
>>>> >>> >>>>>>>
>>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>>> >>> >>>>>>> <pwendell@gmail.com>
>>>> >>> >>>>>>> wrote:
>>>> >>> >>>>>>>
>>>> >>> >>>>>>>> Hey Cody,
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming
>>>> is TD -
>>>> >>> >>>>>>>> he is traveling this week though so I can respond a bit.
>>>> To the
>>>> >>> >>>>>>>> high level point of whether Kafka is important - it
>>>> definitely
>>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support
>>>> for
>>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
>>>> library.
>>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
>>>> unstable
>>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
>>>> stabilize
>>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't b=
e
>>>> able
>>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level
>>>> impression
>>>> >>> only
>>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so it'=
s
>>>> worth
>>>> >>> >>> revisiting given the developments in Kafka.
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>> Please do bring things up like this on the dev list if
>>>> there are
>>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>> - Patrick
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>>> >>> >>>>>>>> <cody@koeninger.org>
>>>> >>> >>>>>>>> wrote:
>>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to
>>>> get
>>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible
>>>> for our
>>>> >>> >>>>>>>> production
>>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, becaus=
e
>>>> I've
>>>> >>> >>>>>>>>> seen several different patches and libraries to fix the
>>>> same
>>>> >>> >>>>>>>>> things we've
>>>> >>> >>>>>>>> been
>>>> >>> >>>>>>>>> running into.
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> Regarding flexibility
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivalen=
t
>>>> of
>>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>>> >>> >>>>>>>>> ourselves, then found
>>>> >>> >>>>>>>> that
>>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn't b=
e
>>>> able
>>>> >>> >>>>>>>>> to
>>>> >>> >>>> get
>>>> >>> >>>>>>>> our
>>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a
>>>> whole
>>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388,
>>>> arbitrary
>>>> >>> >>>>>>>>> delay of
>>>> >>> >>>>>>>> messages, etc).
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
>>>> >>> >>>>>>>>> WriteAheadLog
>>>> >>> >>>> as
>>>> >>> >>>>>>>> a
>>>> >>> >>>>>>>>> general solution for streaming unreliable sources, but
>>>> Kafka
>>>> >>> >>>>>>>>> already
>>>> >>> >>>>>>>> is a
>>>> >>> >>>>>>>>> reliable source. I think there's a need for an api that
>>>> treats
>>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
>>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>>>> >>> >>>>>>>>> write-ahead log in hdfs, I
>>>> >>> >>>> need
>>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>>>> >>> >>>>>>>>> failures
>>>> >>> >>>> that
>>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>>>> >>> instance).
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> I've got an implementation i've been using
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>>
>>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> Tresata has something similar at
>>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
>>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm
>>>> code.
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries
>>>> rather
>>>> >>> >>>>>>>>> than
>>>> >>> >>>>>>>> patches
>>>> >>> >>>>>>>>> to Spark is problematic, because large portions of the
>>>> >>> >>>> implementation
>>>> >>> >>>>>>>> are
>>>> >>> >>>>>>>>> private[spark].
>>>> >>> >>>>>>>>>
>>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to
>>>> get.
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>>
>>>> >>> -----------------------------------------------------------------
>>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
>>>> dev-unsubscribe@spark.apache.org
>>>> >>> For
>>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>>
>>>> >>> >>>>>>>
>>>> >>> >>>>>
>>>> >>> >>>>
>>>> >>> >>>
>>>> >>> >>
>>>> >>>
>>>> >>>
>>>> >>
>>>> >
>>>>
>>>
>>
>

--001a113d8408363a33050b05c9c0--

From dev-return-10932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 10:43:20 2014
Return-Path: <dev-return-10932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B5D1C9DC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 10:43:19 +0000 (UTC)
Received: (qmail 15684 invoked by uid 500); 25 Dec 2014 10:43:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15593 invoked by uid 500); 25 Dec 2014 10:43:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13611 invoked by uid 99); 25 Dec 2014 10:43:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 10:43:17 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [203.81.22.165] (HELO mail1.qilinsoft.com) (203.81.22.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 10:42:50 +0000
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
Subject: Do you know any Spark modeling tool?
Date: Thu, 25 Dec 2014 18:42:01 +0800
Message-ID: <2EB23AF5EEEA2140946B8F292EB2EB9F13B717@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: Do you know any Spark modeling tool?
Thread-Index: AdAgL2posF6qv9fYRXuxOdEywsaFNg==
From: "Haopu Wang" <HWang@qilinsoft.com>
To: "user" <user@spark.apache.org>,
	<dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, I think a modeling tool may be helpful because sometimes it's
hard/tricky to program Spark. I don't know if there is already such a
tool.

Thanks!

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 12:35:17 2014
Return-Path: <dev-return-10933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BBC9CBB9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 12:35:15 +0000 (UTC)
Received: (qmail 62366 invoked by uid 500); 25 Dec 2014 12:35:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62310 invoked by uid 500); 25 Dec 2014 12:35:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62289 invoked by uid 99); 25 Dec 2014 12:35:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 12:35:06 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of ranamitabh@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 12:34:40 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 0245DEB2412
	for <dev@spark.apache.org>; Thu, 25 Dec 2014 04:34:08 -0800 (PST)
Date: Thu, 25 Dec 2014 05:34:08 -0700 (MST)
From: ranamitabh <ranamitabh@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419510848516-9922.post@n3.nabble.com>
Subject: =?UTF-8?Q?MapR_distribution_Spark_throwing_=E2=80=9Cno_?=
 =?UTF-8?Q?MapRClient_in_java.library.path=E2=80=9D_error?=
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am a big data developer and for past some time I have been using apache
spark of cloudera distribution. I have successfully written all my codes on
Cloudera-Spark but when I am trying my hands on MapR distribution of Spark
then I am getting the following error.

===============================================
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more
==========Unable to find library in jar due to exception. ==============
java.lang.RuntimeException: no native library is found for os.name=Windows
and os.arch=x86_64
    at com.mapr.fs.ShimLoader.findNativeLibrary(ShimLoader.java:496)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:318)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more
Exception in thread "main" java.lang.ExceptionInInitializerError
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:214)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    ... 24 more
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more

====================================

I have written a simple word count code. *When I am removing MapR-Spark jar
and replacing it with Cloudera-Spark jar then the same code is executing
perfectly*. Kindly help.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/MapR-distribution-Spark-throwing-no-MapRClient-in-java-library-path-error-tp9922.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 15:37:38 2014
Return-Path: <dev-return-10934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 067D9CECB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 15:37:38 +0000 (UTC)
Received: (qmail 90254 invoked by uid 500); 25 Dec 2014 15:37:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90173 invoked by uid 500); 25 Dec 2014 15:37:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90155 invoked by uid 99); 25 Dec 2014 15:37:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 15:37:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 15:37:31 +0000
Received: by mail-qc0-f174.google.com with SMTP id c9so6677946qcz.33
        for <dev@spark.apache.org>; Thu, 25 Dec 2014 07:35:19 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=HhtRXpgCNHEKXzCQo4ZXAfSKDwPn/X5QoG2cqnpzSD4=;
        b=WRhAPUKURvol6v4nmx+kZZmfh8pwAK08H1zwMxGwES2by8Y5VN+Bpl4Ws9tqyT6AS6
         n3yNCUBHE5H5yHju2BUw1R9rX7+5ue5IjMTurDF2Z+7ydyQmSykuFt4y5n4HYthX0pdg
         cxX0tOzO5wowf3fXb3Dr/rrIQck+lleTD4HYkM+2AyJbunsqt8Q4K7t3B06orDxtfCfb
         jImcpIywonVsFDIkYNdKCw1ZqKiZrHa2XNkOoiU7ipcIpCU8JXHvz0iWUdxNn8ub7E/i
         Z2m5hwuNawrCkJO51ZhpOIKG7hflTXbT/JEu/1UQ1l/86Dp9iIOEIR7iOfSgEEbEdxBF
         Xyhg==
X-Gm-Message-State: ALoCoQnw+z78GIe4u7MVGDKqa5PC4KkYmrIQfGWRKhVOMpIbhkIUjdQ+4odYobjrPCJL2LsM8BIK
MIME-Version: 1.0
X-Received: by 10.224.121.142 with SMTP id h14mr63366480qar.80.1419521719616;
 Thu, 25 Dec 2014 07:35:19 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Thu, 25 Dec 2014 07:35:19 -0800 (PST)
In-Reply-To: <CAOhmDzeOOi9ZNtvcutpVoWPSbzJeAqgJyjsBMqYi_7zJhuVn3A@mail.gmail.com>
References: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
	<CAOhmDzeOOi9ZNtvcutpVoWPSbzJeAqgJyjsBMqYi_7zJhuVn3A@mail.gmail.com>
Date: Thu, 25 Dec 2014 10:35:19 -0500
Message-ID: <CAPEc=JttO-TvyFxs0oAQ0XL8TZ96Lv1cjk5WA_=SeAONSDG0+g@mail.gmail.com>
Subject: Re: Starting with Spark
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0141a8bed95f85050b0c27d2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0141a8bed95f85050b0c27d2
Content-Type: text/plain; charset=UTF-8

Thanks. I will work on this today and try to setup.

The bad link is present in the below github REAMME file,

https://github.com/apache/spark

Search with "Build Spark with Maven"

On Thu, Dec 25, 2014 at 1:49 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> The correct docs link is:
> https://spark.apache.org/docs/1.2.0/building-spark.html
>
> Where did you get that bad link from?
>
> Nick
>
>
>
> On Thu Dec 25 2014 at 12:00:53 AM Naveen Madhire <vmadhire@umail.iu.edu>
> wrote:
>
>> Hi All,
>>
>> I am starting to use Spark. I am having trouble getting the latest code
>> from git.
>> I am using Intellij as suggested in the below link,
>>
>> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#
>> ContributingtoSpark-StarterTasks
>>
>>
>> The below link isn't working as well,
>>
>> http://spark.apache.org/building-spark.html
>>
>>
>> Does anyone know any useful links to get spark running on local laptop
>>
>> Please help.
>>
>>
>> Thanks
>>
>

--089e0141a8bed95f85050b0c27d2--

From dev-return-10935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 18:10:43 2014
Return-Path: <dev-return-10935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5107B101CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 18:10:43 +0000 (UTC)
Received: (qmail 2666 invoked by uid 500); 25 Dec 2014 18:10:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2557 invoked by uid 500); 25 Dec 2014 18:10:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2459 invoked by uid 99); 25 Dec 2014 18:10:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:10:40 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of anant.asty@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:10:13 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 2A781EB5BAE
	for <dev@spark.apache.org>; Thu, 25 Dec 2014 10:08:40 -0800 (PST)
Date: Thu, 25 Dec 2014 11:08:40 -0700 (MST)
From: slcclimber <anant.asty@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419530920055-9924.post@n3.nabble.com>
In-Reply-To: <1419326747136-9902.post@n3.nabble.com>
References: <1419326747136-9902.post@n3.nabble.com>
Subject: Re: Apache Spark (Data Aggregation) using Java API
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

You could convert your csv file to an rdd of vectors.
Then use stats from mllib.

Also this should be in the user list not the developer list.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-Data-Aggregation-using-Java-API-tp9902p9924.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 18:47:18 2014
Return-Path: <dev-return-10936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74CFF1026F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 18:47:18 +0000 (UTC)
Received: (qmail 26061 invoked by uid 500); 25 Dec 2014 18:47:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25971 invoked by uid 500); 25 Dec 2014 18:47:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25959 invoked by uid 99); 25 Dec 2014 18:47:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:47:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:47:11 +0000
Received: by mail-ie0-f177.google.com with SMTP id rd18so8450758iec.8
        for <dev@spark.apache.org>; Thu, 25 Dec 2014 10:46:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=wpo4B0Vx8kmy1Il2X0IjIPpmY3j0WYZkQnjTs/f23nY=;
        b=WgcKbVmEzJYSzAjfUcoJquS8NSMl/4wWEsX9WbGstYZB4jy9YwB4SRVgVRDp9fqeAs
         e8yDl+RfWjCMms78Bzks0hlYr9/cjeoetsS/to0Bg2LJH1wqKeEGtHiCarfr19ehdy11
         rsmIb3eAevvNkEDX8q66jlN6rrFRXK9OguD10+sIHjVlS1GkWVk9pzVGFXtexnNv9jQ4
         TUPhkKQa6ncZQDMXmlnIbDc5RcrUR2iDs48hbFiMot9E2yaa24P6ZfPXUdVMsLe8pF2M
         h+GBmWb7s0NK4ue7xS1jIgXs1P2wMJJ2HBs8ZW7eMcCN8h61HIZsxlTrqsqAS49cCVKu
         aL1g==
X-Received: by 10.50.79.230 with SMTP id m6mr11089564igx.20.1419533165566;
 Thu, 25 Dec 2014 10:46:05 -0800 (PST)
MIME-Version: 1.0
References: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
 <CAOhmDzeOOi9ZNtvcutpVoWPSbzJeAqgJyjsBMqYi_7zJhuVn3A@mail.gmail.com> <CAPEc=JttO-TvyFxs0oAQ0XL8TZ96Lv1cjk5WA_=SeAONSDG0+g@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 25 Dec 2014 18:46:04 +0000
Message-ID: <CAOhmDzdoEU8frX9OAiY8j5mVSZO25QxOFd2ACkggoNTX2+LaBw@mail.gmail.com>
Subject: Re: Starting with Spark
To: Naveen Madhire <vmadhire@umail.iu.edu>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01229aaa14a332050b0ed282
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01229aaa14a332050b0ed282
Content-Type: text/plain; charset=UTF-8

Thanks for the pointer. This will be fixed in this PR
<https://github.com/apache/spark/pull/3802>.

On Thu Dec 25 2014 at 10:35:20 AM Naveen Madhire <vmadhire@umail.iu.edu>
wrote:

> Thanks. I will work on this today and try to setup.
>
> The bad link is present in the below github REAMME file,
>
> https://github.com/apache/spark
>
> Search with "Build Spark with Maven"
>
> On Thu, Dec 25, 2014 at 1:49 AM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> The correct docs link is:
>> https://spark.apache.org/docs/1.2.0/building-spark.html
>>
>> Where did you get that bad link from?
>>
>> Nick
>>
>>
>>
>> On Thu Dec 25 2014 at 12:00:53 AM Naveen Madhire <vmadhire@umail.iu.edu>
>> wrote:
>>
>>> Hi All,
>>>
>>> I am starting to use Spark. I am having trouble getting the latest code
>>> from git.
>>> I am using Intellij as suggested in the below link,
>>>
>>> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#
>>> ContributingtoSpark-StarterTasks
>>>
>>>
>>> The below link isn't working as well,
>>>
>>> http://spark.apache.org/building-spark.html
>>>
>>>
>>> Does anyone know any useful links to get spark running on local laptop
>>>
>>> Please help.
>>>
>>>
>>> Thanks
>>>
>>
>

--089e01229aaa14a332050b0ed282--

From dev-return-10937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Dec 25 18:55:00 2014
Return-Path: <dev-return-10937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D86C1029D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 25 Dec 2014 18:55:00 +0000 (UTC)
Received: (qmail 32267 invoked by uid 500); 25 Dec 2014 18:54:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32184 invoked by uid 500); 25 Dec 2014 18:54:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32172 invoked by uid 99); 25 Dec 2014 18:54:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:54:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of denny.g.lee@gmail.com designates 209.85.223.170 as permitted sender)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 25 Dec 2014 18:54:32 +0000
Received: by mail-ie0-f170.google.com with SMTP id rd18so8994652iec.1
        for <dev@spark.apache.org>; Thu, 25 Dec 2014 10:53:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=a6t+rAtTFKOq7ys6wOPWox7/V6FWM+8NLSE+eVxxEZ0=;
        b=wLbt/8j9Qe0FK7rxRDGodIyZ3bBeK7qMgsjhi2SvmKSiQwO8HeM0tNC3VPesAIZr4o
         Xagq6z07MbTsn9WNFZ1U0k2EQlWALFNRCmqG3RxitAP13aIW8WLlU97cbuZcsmHZt/Mb
         J85wYNE0umtSucSUhPnqicKqVBTTGnoWs5SgcCdrcRQDwb5mn92yh/0auN664aXEw9lq
         mDXTnVdAjE6WLs1tyoTgzJtZgDBohB975g1Fnix4TXn67F35ivibJUHgikT5PdUGanda
         w/1JksF1655N4ySb283m0efftsSV6b0qGf+VZ3ZQWARKUrCr/bhmK5Kn4fRS3nlwUDDz
         TniA==
X-Received: by 10.42.169.197 with SMTP id c5mr29788961icz.72.1419533626107;
 Thu, 25 Dec 2014 10:53:46 -0800 (PST)
MIME-Version: 1.0
References: <CAPEc=Jt6aQDKpH-AweXOB7=oRftRbFzoK33_zD+-ry9BmORcpA@mail.gmail.com>
 <CAOhmDzeOOi9ZNtvcutpVoWPSbzJeAqgJyjsBMqYi_7zJhuVn3A@mail.gmail.com>
 <CAPEc=JttO-TvyFxs0oAQ0XL8TZ96Lv1cjk5WA_=SeAONSDG0+g@mail.gmail.com> <CAOhmDzdoEU8frX9OAiY8j5mVSZO25QxOFd2ACkggoNTX2+LaBw@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Thu, 25 Dec 2014 18:53:45 +0000
Message-ID: <CABjYQ389s9zgu+Lrpgy6L+K8kkZU__XrfrniAQ=2ee4moirENA@mail.gmail.com>
Subject: Re: Starting with Spark
To: Nicholas Chammas <nicholas.chammas@gmail.com>, Naveen Madhire <vmadhire@umail.iu.edu>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=90e6ba6e8a5c87ef89050b0eedff
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e8a5c87ef89050b0eedff
Content-Type: text/plain; charset=UTF-8

Thanks for the catch Naveen!

On Thu Dec 25 2014 at 10:47:18 AM Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Thanks for the pointer. This will be fixed in this PR
> <https://github.com/apache/spark/pull/3802>.
>
> On Thu Dec 25 2014 at 10:35:20 AM Naveen Madhire <vmadhire@umail.iu.edu>
> wrote:
>
> > Thanks. I will work on this today and try to setup.
> >
> > The bad link is present in the below github REAMME file,
> >
> > https://github.com/apache/spark
> >
> > Search with "Build Spark with Maven"
> >
> > On Thu, Dec 25, 2014 at 1:49 AM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >
> >> The correct docs link is:
> >> https://spark.apache.org/docs/1.2.0/building-spark.html
> >>
> >> Where did you get that bad link from?
> >>
> >> Nick
> >>
> >>
> >>
> >> On Thu Dec 25 2014 at 12:00:53 AM Naveen Madhire <vmadhire@umail.iu.edu
> >
> >> wrote:
> >>
> >>> Hi All,
> >>>
> >>> I am starting to use Spark. I am having trouble getting the latest code
> >>> from git.
> >>> I am using Intellij as suggested in the below link,
> >>>
> >>> https://cwiki.apache.org/confluence/display/SPARK/
> Contributing+to+Spark#
> >>> ContributingtoSpark-StarterTasks
> >>>
> >>>
> >>> The below link isn't working as well,
> >>>
> >>> http://spark.apache.org/building-spark.html
> >>>
> >>>
> >>> Does anyone know any useful links to get spark running on local laptop
> >>>
> >>> Please help.
> >>>
> >>>
> >>> Thanks
> >>>
> >>
> >
>

--90e6ba6e8a5c87ef89050b0eedff--

From dev-return-10938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 26 14:09:21 2014
Return-Path: <dev-return-10938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7D5510673
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 26 Dec 2014 14:09:21 +0000 (UTC)
Received: (qmail 23766 invoked by uid 500); 26 Dec 2014 14:09:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23680 invoked by uid 500); 26 Dec 2014 14:09:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22243 invoked by uid 99); 26 Dec 2014 14:09:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 14:09:17 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 14:08:50 +0000
Received: by mail-pd0-f180.google.com with SMTP id w10so13162264pde.39
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 06:08:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:cc:content-type;
        bh=PyCA25ZJJL8qSHgCdyU0GvCwaL7qVb7CPYkl/gt7yNA=;
        b=T1rfo3HtzdtRuGQXQMsDT0p45iFSay7HvIpyjnpSG4UbBFE6kDy+kGsgyVKCAV5tRt
         1DUsRzst871izoB5jnGArmCf+d0JhIlStniHugHXbAtUdbdEU+xgrfkHzomCBeQCUk6b
         64EkdwNhB7nBeZF0gtQDkAcnUWojawwiqZRUs/8B4USd97SJko5uwWvZpR+Y0voBotNx
         60qEv3prcLUsbiR8aLeRRB/tUWNal/bvYZy54YDD5kUGFvuDiuNqmvp2tD/TypY+Ihl7
         bRyseqmXtV8kBqcZwxWOIplB1tdzhez9vwpbKoyG/GYR9pgVflOgNZNxL8h3FGV94Awy
         Yb4Q==
X-Received: by 10.68.95.3 with SMTP id dg3mr69331282pbb.12.1419602929202; Fri,
 26 Dec 2014 06:08:49 -0800 (PST)
MIME-Version: 1.0
Received: by 10.66.156.39 with HTTP; Fri, 26 Dec 2014 06:08:08 -0800 (PST)
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Fri, 26 Dec 2014 12:08:08 -0200
Message-ID: <CAO4-Pq8hR-OTGuPGmnH2QhQD3SC5u2V+g_sf+nDnXWHsKy97hA@mail.gmail.com>
Subject: Spark 1.2.0 Repl
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b675d7c514079050b1f105a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b675d7c514079050b1f105a
Content-Type: text/plain; charset=UTF-8

Hello,
Is there any reason in not publishing spark repl in the version 1.2.0?
In repl/pom.xml the deploy and publish are been skipped.

Regards,
Dirceu

--047d7b675d7c514079050b1f105a--

From dev-return-10939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 26 19:45:35 2014
Return-Path: <dev-return-10939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 60E1010EA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 26 Dec 2014 19:45:35 +0000 (UTC)
Received: (qmail 27351 invoked by uid 500); 26 Dec 2014 19:45:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27273 invoked by uid 500); 26 Dec 2014 19:45:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27262 invoked by uid 99); 26 Dec 2014 19:45:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 19:45:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 19:45:26 +0000
Received: by mail-wg0-f44.google.com with SMTP id b13so14928508wgh.17
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 11:43:35 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ZKkxh1M41zlg0aGDxkLpue2j2waIXO0RiN0ymhRSe44=;
        b=O3GqRv0P9TGo9tpQhyLGQrKSlvHNOyYiMWaOW4hv8dMnQPOe/gZCZThNoF9CYNDCUD
         y1GzNueT6sHOREWm4iZgADZ1cJ+nm8uT28wzYcKlJTVhKQCdTSX+rS6oan48Af2Hp78P
         qhklmv75n7G5dl1WMOXrquG2NAEO2Oc3h5Sv4cmLqXMBtZcKM3IbWQZDlxATMpeDUOqo
         GshM2HNWrXZQTmmUIipGtfZCkJ30IIaKVEAf2RLsduO6ta09tXX28glKxSg9nGKTD6kr
         PrKCEvZe4pzNl+qRs8QFaf7zB28t9a3EJZrqMXMNMKg77MYbyi6iSh/aCUU6LQ+QqtTQ
         4mug==
X-Gm-Message-State: ALoCoQnt+IaVeXTTxMdfx9n69AmSWf5bUb4Ez4DrUuIaRquboeS9Wj38+rJ54BYZI2WYReyB/CiP
MIME-Version: 1.0
X-Received: by 10.194.191.227 with SMTP id hb3mr85800091wjc.79.1419623015389;
 Fri, 26 Dec 2014 11:43:35 -0800 (PST)
Received: by 10.27.83.204 with HTTP; Fri, 26 Dec 2014 11:43:35 -0800 (PST)
Received: by 10.27.83.204 with HTTP; Fri, 26 Dec 2014 11:43:35 -0800 (PST)
In-Reply-To: <CAO4-Pq8hR-OTGuPGmnH2QhQD3SC5u2V+g_sf+nDnXWHsKy97hA@mail.gmail.com>
References: <CAO4-Pq8hR-OTGuPGmnH2QhQD3SC5u2V+g_sf+nDnXWHsKy97hA@mail.gmail.com>
Date: Fri, 26 Dec 2014 19:43:35 +0000
Message-ID: <CAMAsSdJ2qUcs9SoskH5VMFBmMbVhKcGbg0SbzUT95oiWm9XYvA@mail.gmail.com>
Subject: Re: Spark 1.2.0 Repl
From: Sean Owen <sowen@cloudera.com>
To: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba97f2a8c3685050b23bda5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba97f2a8c3685050b23bda5
Content-Type: text/plain; charset=UTF-8

It was not intended to be a public API but there is a request to keep
publishing it as a developer API:
https://issues.apache.org/jira/browse/SPARK-4923
On Dec 26, 2014 2:09 PM, "Dirceu Semighini Filho" <
dirceu.semighini@gmail.com> wrote:

> Hello,
> Is there any reason in not publishing spark repl in the version 1.2.0?
> In repl/pom.xml the deploy and publish are been skipped.
>
> Regards,
> Dirceu
>

--047d7ba97f2a8c3685050b23bda5--

From dev-return-10940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Dec 26 21:20:25 2014
Return-Path: <dev-return-10940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5624810063
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 26 Dec 2014 21:20:25 +0000 (UTC)
Received: (qmail 5099 invoked by uid 500); 26 Dec 2014 21:20:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5025 invoked by uid 500); 26 Dec 2014 21:20:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5005 invoked by uid 99); 26 Dec 2014 21:20:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 21:20:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 26 Dec 2014 21:20:17 +0000
Received: by mail-ig0-f182.google.com with SMTP id hn15so9227462igb.3
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 13:18:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=OM/LMAMwD09T+ttRNjqWi63ar0Wi8fAQT8xWdqvDBMo=;
        b=Xv8w575muYa6w0kkBtwa8riPt2JlEqNM1ATe9RUqQn0bifQg9fAuwzjD719aZe8S92
         RD9SoxntnWIRoeRRxllMtOF4/2KNJwb5aXPk9KxqfxlCo2inLTs8FS2FgKcXZC7XkGdd
         QDivvifOCdXii4yTBO+ynEc8T1QWyza0DwmNHwVaKxOuEPcoTgXb7vExHbEXGXVffdgz
         qF2eZmusIeHhZXlmSFHkw+B/H6x2HJFPLu4Qp/fi7lwAuehKYxSM5cEOl7ShE8kPoQWQ
         3nIEa8VDtmzw9+9mGz/ql6aURy1peKjK4b9i5aEY7FomLPqlwycmJA53ysEg1tfpPmnz
         z3aw==
X-Received: by 10.43.89.68 with SMTP id bd4mr33702341icc.63.1419628706562;
 Fri, 26 Dec 2014 13:18:26 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 26 Dec 2014 21:18:25 +0000
Message-ID: <CAOhmDzfV3M8_3G9Fzqa2qnvjwiC-NfUBbummvXYxVxirXnOvHw@mail.gmail.com>
Subject: SQL specification for reference during Spark SQL development
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec517c6c0c48d8a050b2510ee
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec517c6c0c48d8a050b2510ee
Content-Type: text/plain; charset=UTF-8

Do we have access to the SQL specification (say, SQL-92) for reference
during Spark SQL development? I know it's not freely available on the web.
Usually, you can only access drafts.

I know that, generally, we look to other systems (especially Hive) when
figuring out how something in Spark SQL should work, but it might be nice
to have the standard available for reference.

Nick

--bcaec517c6c0c48d8a050b2510ee--

From dev-return-10941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 00:11:56 2014
Return-Path: <dev-return-10941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1A864102F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 00:11:56 +0000 (UTC)
Received: (qmail 81340 invoked by uid 500); 27 Dec 2014 00:11:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81263 invoked by uid 500); 27 Dec 2014 00:11:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81251 invoked by uid 99); 27 Dec 2014 00:11:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 00:11:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 00:11:27 +0000
Received: by mail-ob0-f178.google.com with SMTP id gq1so35677962obb.9
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 16:11:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=mIl5PahXqBUxykKbEI3F/PT3H7hZk5sG5BiYXOomM0A=;
        b=FbvS6pUGOSAfKi7OYGkcCM8R+iXAkFEL0IeSwpkLI/4kCvIHJqixmJeg2Mf+03Bqjm
         9Pqvjy9CgQ0AozuuEfbk2xCNarP1SeYNnGrfxLYKKLRDyZusG6UY9A8OPQWiUXE7GQN+
         xW17ZATNiAS/jEXsG+F7CT8Grl9qtK+SjYJdU/W1lmLAYBOeWJ9Wsjyr8RrVYSbtzlzg
         32y/5D6d/YPlxkyM2aq7M5Kr5XdpSPUSZgUuErXLh2NFp+NIPVVS76NA8WAkQMAC9JPe
         IEizH8CIOhb9Iw6BzDrAGGb4A2ewWf5ryrnZswjGs86wxwKXNdhcOYAYH4Ii25YO+1+J
         mGLQ==
X-Received: by 10.60.144.194 with SMTP id so2mr23297190oeb.65.1419639086105;
 Fri, 26 Dec 2014 16:11:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Fri, 26 Dec 2014 16:11:05 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 26 Dec 2014 16:11:05 -0800
Message-ID: <CAJc_syK1H62oZ8fu9NNOnN7JyWxnBC0dH8qr88KY8Q157GC2ew@mail.gmail.com>
Subject: SQLContext is Serializable, SparkContext is not
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a94fe6fc89e050b277bcc
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a94fe6fc89e050b277bcc
Content-Type: text/plain; charset=UTF-8

How, O how can this be? Doesn't the SQLContext hold a reference to the
SparkContext?

Alex

--047d7b3a94fe6fc89e050b277bcc--

From dev-return-10942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 02:17:23 2014
Return-Path: <dev-return-10942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7D4A1051A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 02:17:23 +0000 (UTC)
Received: (qmail 40870 invoked by uid 500); 27 Dec 2014 02:17:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40776 invoked by uid 500); 27 Dec 2014 02:17:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40762 invoked by uid 99); 27 Dec 2014 02:17:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 02:17:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 02:16:52 +0000
Received: by mail-ob0-f171.google.com with SMTP id uz6so35861883obc.2
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 18:16:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=ioQ0j+rBeBm/ATiBq7bt6FQoZvOzAYfYnMGpfOoWQ7c=;
        b=lBNiGtrX42MRsIpDOu7SE7vpzsGC/yE171H+99hmzU0ePVo9f9OYCIP1vKamZBkVad
         /iQzuAHWHjr5pHodI/9cnuhqh2s5B7VyEff6k81NlFnE+J7rE2aJCnpBdsRz1N2iQ73J
         3r9/IMYtBmB8vM5BCxllOhuWc4Fz6JX/+FJQjy3AgKv9uTMKdAp9Ke/1lNXbheicM/Tg
         p7y33PcfTNVgBDbLvZkn1nL2U15WBvU62GgyhpZDacWVDs5+9UBRycsB7rALiNHpMSuT
         x3EFgMGs87FMN6O5DfGN1qDhkmtvAWsN2B0Wl7jnnFMc/+LjdfcKJTD9CZW0yl54QGW/
         O92A==
X-Received: by 10.60.98.240 with SMTP id el16mr21580968oeb.4.1419646565463;
 Fri, 26 Dec 2014 18:16:05 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Fri, 26 Dec 2014 18:15:45 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 26 Dec 2014 18:15:45 -0800
Message-ID: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
Subject: Assembly jar file name does not match profile selection
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011832a63dbe21050b29391e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011832a63dbe21050b29391e
Content-Type: text/plain; charset=UTF-8

I am building spark with sbt off of branch 1.2. I'm using the following
command:

sbt/sbt -Pyarn -Phadoop-2.3 assembly

(http://spark.apache.org/docs/latest/building-spark.html#building-with-sbt)

Although the jar file I obtain does contain the proper version of the
hadoop libraries (v. 2.4), the assembly jar file name refers to hadoop
v.1.0.4:

./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar

Any idea why?


Alex

--089e011832a63dbe21050b29391e--

From dev-return-10943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 04:43:34 2014
Return-Path: <dev-return-10943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3882B106A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 04:43:34 +0000 (UTC)
Received: (qmail 82582 invoked by uid 500); 27 Dec 2014 04:43:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82506 invoked by uid 500); 27 Dec 2014 04:43:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82494 invoked by uid 99); 27 Dec 2014 04:43:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 04:43:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.178 as permitted sender)
Received: from [209.85.160.178] (HELO mail-yk0-f178.google.com) (209.85.160.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 04:43:05 +0000
Received: by mail-yk0-f178.google.com with SMTP id 20so5341876yks.23
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 20:41:33 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=UwHWzEfxo+t+6FswIDOyb3ylaHthK7FCVlkcLdtCM6o=;
        b=dZfQkLsH9aA0jgEi2RPxxund67MCxny7xGCsArvA5VT+h5yX6ybf6OVWLPiFkJ65RG
         epN525hoMBGTeGMJwXZwUIewzo7Dp1ooHn4GHsfc/ocS0c/BPRmcabwWl/UkNGtBEWtF
         rTk/iNlAYjXWCnL4M6cLhW1OejOV5u6Zm+kCqw0PQCaeYpfh7OMX7ybqp+lcSOUts1+4
         5S2hPMrB/vFFYkex/m+UJ9UmIYkrKL0lH+HrBWIYQMSx8XV39i9suQVOJI3OhvGP4DZK
         XDFYxMKOGP8hgZgkB/ZHnqvWzGU9Efm/5HRkfReh3F3pgW0vB8qWuI7EPq/OW3hZvWlg
         D1tQ==
MIME-Version: 1.0
X-Received: by 10.170.141.6 with SMTP id i6mr38086951ykc.122.1419655293635;
 Fri, 26 Dec 2014 20:41:33 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Fri, 26 Dec 2014 20:41:33 -0800 (PST)
In-Reply-To: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
References: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
Date: Fri, 26 Dec 2014 20:41:33 -0800
Message-ID: <CALte62xaEB_E+ckkAP5Di21YyaeZhRDE66QsG1Gou5tK6edw3w@mail.gmail.com>
Subject: Re: Assembly jar file name does not match profile selection
From: Ted Yu <yuzhihong@gmail.com>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139f3ca7b12f8050b2b4129
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139f3ca7b12f8050b2b4129
Content-Type: text/plain; charset=UTF-8

Can you try this command ?

sbt/sbt -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive assembly

On Fri, Dec 26, 2014 at 6:15 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> I am building spark with sbt off of branch 1.2. I'm using the following
> command:
>
> sbt/sbt -Pyarn -Phadoop-2.3 assembly
>
> (http://spark.apache.org/docs/latest/building-spark.html#building-with-sbt
> )
>
> Although the jar file I obtain does contain the proper version of the
> hadoop libraries (v. 2.4), the assembly jar file name refers to hadoop
> v.1.0.4:
>
> ./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar
>
> Any idea why?
>
>
> Alex
>

--001a1139f3ca7b12f8050b2b4129--

From dev-return-10944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 05:10:58 2014
Return-Path: <dev-return-10944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7617C10736
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 05:10:58 +0000 (UTC)
Received: (qmail 89798 invoked by uid 500); 27 Dec 2014 05:10:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89724 invoked by uid 500); 27 Dec 2014 05:10:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89664 invoked by uid 99); 27 Dec 2014 05:10:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 05:10:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 05:10:50 +0000
Received: by mail-ob0-f180.google.com with SMTP id wp4so36117075obc.11
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 21:08:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=RJ6d3YMoCfVBmuIBKtjZCv/dP1WXcrZ6PZFgNw1sdj0=;
        b=LN/Ab+q/wa//OihrE9S0rCoaxih+a99sGZrAZKmhnN+noYPfE6XtSLkQObhU1aDxy/
         8vCLNno1oQRYn/dP0u4w7gJBJS/kauV9/iss5XtastMxjHfy4wNFc14Gto1SE3qQsFsW
         gLBTosnVBtoeYARFXnLUG9yPT0DiOaX1lCAjtdSDMuh9s+/eIostZJ2dKYVW5u1bxzyG
         Fu3+juI4LQLJVi1NijmaSCcrEDKa1e4Ng1N/8nEOMssYWQ5At/Nn+hYd55tlJLSMwSVi
         OeyX8AJ1z7maHTn+E6ClxCN7OpoeCeY0DMKDxLdrcgWXBIEjaSKj0eEh/z1aEwRvpvPq
         Zr9w==
X-Gm-Message-State: ALoCoQnDie2ir3Xkh2jdOh6wJ4YSe4nvum4CDWT9diPux798q9sEgzWdQtJ00UTdnxYbDt2J1vjU
MIME-Version: 1.0
X-Received: by 10.202.80.21 with SMTP id e21mr25795148oib.65.1419656919459;
 Fri, 26 Dec 2014 21:08:39 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Fri, 26 Dec 2014 21:08:39 -0800 (PST)
In-Reply-To: <CAJc_syK1H62oZ8fu9NNOnN7JyWxnBC0dH8qr88KY8Q157GC2ew@mail.gmail.com>
References: <CAJc_syK1H62oZ8fu9NNOnN7JyWxnBC0dH8qr88KY8Q157GC2ew@mail.gmail.com>
Date: Fri, 26 Dec 2014 23:08:39 -0600
Message-ID: <CAKWX9VUbyUTSGCZRLMDMkEy3uzfi1ceUOj82g5WE=r6myMc8NA@mail.gmail.com>
Subject: Re: SQLContext is Serializable, SparkContext is not
From: Cody Koeninger <cody@koeninger.org>
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d840863403c050b2ba2a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d840863403c050b2ba2a1
Content-Type: text/plain; charset=UTF-8

The spark context reference is transient.

On Fri, Dec 26, 2014 at 6:11 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> How, O how can this be? Doesn't the SQLContext hold a reference to the
> SparkContext?
>
> Alex
>

--001a113d840863403c050b2ba2a1--

From dev-return-10945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 05:15:47 2014
Return-Path: <dev-return-10945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63DE71073B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 05:15:46 +0000 (UTC)
Received: (qmail 91852 invoked by uid 500); 27 Dec 2014 05:15:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91770 invoked by uid 500); 27 Dec 2014 05:15:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91756 invoked by uid 99); 27 Dec 2014 05:15:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 05:15:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 05:15:11 +0000
Received: by mail-oi0-f53.google.com with SMTP id g201so24078662oib.12
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 21:12:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Y4f+lKGJY15duXj+ZDv7HG1xOL5IFODVWlX1IlrWsnw=;
        b=YszGEvZIrAvDZAJ9kV7q0YIyNKGFJR4MSzZhXIpiyO7f/0qYDEZmzfTXU5CEaRLEKQ
         Vg8uwvkIsyIs94CVsi0rBUpk4gH68LCoEPT3WVImHV/8KtHepkCmI0u8gQUhV21je17I
         fD4f8c3rMeD3Hl2qNmCZVGUupz3E99AKH0yXH537NiyFIeWPO/pmheAO1DA6fqIq1+IU
         B+8e/P/hilsD618OZNZL77Vf7ZtHYAzSC0LHVAjyfY4lquJhYN7dgTPzTDd/48L+K2Ty
         3K/uTAQzAT9lq40lNpcKxwazThw29Ii/XZZGmboSgMrkjK83JOoyRVmA2MM09yMhtORi
         RJUw==
X-Received: by 10.202.135.78 with SMTP id j75mr19406539oid.106.1419657174686;
 Fri, 26 Dec 2014 21:12:54 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Fri, 26 Dec 2014 21:12:34 -0800 (PST)
In-Reply-To: <CALte62xaEB_E+ckkAP5Di21YyaeZhRDE66QsG1Gou5tK6edw3w@mail.gmail.com>
References: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
 <CALte62xaEB_E+ckkAP5Di21YyaeZhRDE66QsG1Gou5tK6edw3w@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 26 Dec 2014 21:12:34 -0800
Message-ID: <CAJc_sy+yaUDRcKPYJ8EXEWLn1_cN1jdxV6sFhG6WHeNik4H7Ow@mail.gmail.com>
Subject: Re: Assembly jar file name does not match profile selection
To: Ted Yu <yuzhihong@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ef56a99a17d050b2bb1c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ef56a99a17d050b2bb1c3
Content-Type: text/plain; charset=UTF-8

Here's what I get:

./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.6.0.jar

Alex

On Fri, Dec 26, 2014 at 8:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> Can you try this command ?
>
> sbt/sbt -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive assembly
>
> On Fri, Dec 26, 2014 at 6:15 PM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> I am building spark with sbt off of branch 1.2. I'm using the following
>> command:
>>
>> sbt/sbt -Pyarn -Phadoop-2.3 assembly
>>
>> (
>> http://spark.apache.org/docs/latest/building-spark.html#building-with-sbt
>> )
>>
>> Although the jar file I obtain does contain the proper version of the
>> hadoop libraries (v. 2.4), the assembly jar file name refers to hadoop
>> v.1.0.4:
>>
>> ./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar
>>
>> Any idea why?
>>
>>
>> Alex
>>
>
>

--001a113ef56a99a17d050b2bb1c3--

From dev-return-10946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 06:50:25 2014
Return-Path: <dev-return-10946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9087B10850
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 06:50:25 +0000 (UTC)
Received: (qmail 19664 invoked by uid 500); 27 Dec 2014 06:50:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19583 invoked by uid 500); 27 Dec 2014 06:50:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19571 invoked by uid 99); 27 Dec 2014 06:50:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 06:50:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 06:50:16 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so36273735obc.6
        for <dev@spark.apache.org>; Fri, 26 Dec 2014 22:47:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=JGgUWGdkLtZ5pUfguN/x2Bvxy4SSmWFNX3A8p1SFXss=;
        b=LRgzshOWb2YrmGTNOIlsGSB3ZvRZJ1rhF+G2xCFstu/ViKQyFFwp9y+3O1fe36h56a
         JvnOm3wydMxgec2EbkYUA8lbocF/RKliu0Zm3GeoBUwrlccwVKH7M07wdTmbcRFD0hd8
         tZCugv2Q/a6BcLnhM/leuO+KHLyKovMGAvyrpe91+M7jskDIgj3XrUQjkdktEVbq4XS9
         RUjCgWmwCNuekXBJBeDJYwbnYhlmIZBp8ps5VonGYykORzclu+wUS3C6fUbreBJv6G4q
         PARKQdvL62Nt7efVmx81XPJo3apImPW51faKdhPheKs2imlUT2zR931xk1ugdIp+AAZD
         NMTA==
X-Received: by 10.202.135.78 with SMTP id j75mr19530553oid.106.1419662860933;
 Fri, 26 Dec 2014 22:47:40 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Fri, 26 Dec 2014 22:47:20 -0800 (PST)
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Fri, 26 Dec 2014 22:47:20 -0800
Message-ID: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
Subject: Unsupported Catalyst types in Parquet
To: "dev@spark.apache.org" <dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>
Content-Type: multipart/alternative; boundary=001a113ef56a86e0ec050b2d0496
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ef56a86e0ec050b2d0496
Content-Type: text/plain; charset=UTF-8

Michael,

I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
due to my RDDs having having DateType and DecimalType fields. What would it
take to add Parquet support for these Catalyst? Are there any other
Catalyst types for which there is no Catalyst support?

Alex

--001a113ef56a86e0ec050b2d0496--

From dev-return-10947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 07:13:30 2014
Return-Path: <dev-return-10947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 394E51089B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 07:13:30 +0000 (UTC)
Received: (qmail 30719 invoked by uid 500); 27 Dec 2014 07:13:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30644 invoked by uid 500); 27 Dec 2014 07:13:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30627 invoked by uid 99); 27 Dec 2014 07:13:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 07:13:27 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [49.212.34.109] (HELO oss.nttdata.co.jp) (49.212.34.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 07:13:20 +0000
Received: from Kousuke-no-MacBook-Pro.local (113x43x180x139.ap113.ftth.ucom.ne.jp [113.43.180.139])
	by oss.nttdata.co.jp (Postfix) with ESMTP id 71A4617EE04;
	Sat, 27 Dec 2014 16:11:50 +0900 (JST)
Message-ID: <549E5BB3.8090407@oss.nttdata.co.jp>
Date: Sat, 27 Dec 2014 16:11:47 +0900
From: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: Alessandro Baretta <alexbaretta@gmail.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Assembly jar file name does not match profile selection
References: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
In-Reply-To: <CAJc_syK++Pat0M3_4M0as31buuT8Dg1T6GRdD-8hauiFOTmH0w@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: clamav-milter 0.98.5 at oss.nttdata.co.jp
X-Virus-Status: Clean
X-Spam-Checker-Version: SpamAssassin 3.2.5 (2008-06-10) on oss.nttdata.co.jp
X-Virus-Checked: Checked by ClamAV on apache.org
X-Old-Spam-Status: No, score=-99.7 required=13.0 tests=AWL,CONTENT_TYPE_PRESENT,
	UNPARSEABLE_RELAY,USER_IN_WHITELIST autolearn=ham version=3.2.5

Hi Alessandro,

It's fixed by SPARK-3787 and will be applied to 1.2.1 and 1.3.0.

https://issues.apache.org/jira/browse/SPARK-3787

- Kousuke

(2014/12/27 11:15), Alessandro Baretta wrote:
> I am building spark with sbt off of branch 1.2. I'm using the following
> command:
>
> sbt/sbt -Pyarn -Phadoop-2.3 assembly
>
> (http://spark.apache.org/docs/latest/building-spark.html#building-with-sbt)
>
> Although the jar file I obtain does contain the proper version of the
> hadoop libraries (v. 2.4), the assembly jar file name refers to hadoop
> v.1.0.4:
>
> ./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar
>
> Any idea why?
>
>
> Alex
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 21:46:29 2014
Return-Path: <dev-return-10948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25348C92D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 21:46:26 +0000 (UTC)
Received: (qmail 46984 invoked by uid 500); 27 Dec 2014 21:46:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46904 invoked by uid 500); 27 Dec 2014 21:46:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46890 invoked by uid 99); 27 Dec 2014 21:46:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 21:46:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 21:46:15 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so37560943obc.0
        for <dev@spark.apache.org>; Sat, 27 Dec 2014 13:44:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=UFWVw00ZXTlh/7LRukJhMhPE8viJYwhg7vTjpEKE4Tw=;
        b=hIORyC7ShuqjzldCFac8qIMLvznz4+6mhqzH2EFbH1UfpGEtJgurpBiW8IQsJQ+7kF
         Kcc6gKRUQ2C8vdobWEKFCdXPq/W3QuAxxby+Ug6422lLOb4lejS0L1W5pI17MHzM8ALa
         5360ukKfaM0lZ0g3z3gekKTYxgoEDBGOpRGx0zvbjcnA4I72AnE59q9NLVqoxucRYPXD
         Ycc1Yi1KOc08lop3iYYXOh+Cg3wv22u1U9NPZv970gwDfzWA+11q9OigR01AXedLT9QO
         dt70hbKRFAdbxeZnvhytGT8wPUmINIT9837j9Q/eY3taCMblVZY0FSEXeisudlcp7ACF
         GAqA==
MIME-Version: 1.0
X-Received: by 10.182.33.138 with SMTP id r10mr28736362obi.67.1419716664655;
 Sat, 27 Dec 2014 13:44:24 -0800 (PST)
Received: by 10.202.198.65 with HTTP; Sat, 27 Dec 2014 13:44:23 -0800 (PST)
Date: Sat, 27 Dec 2014 13:44:23 -0800
Message-ID: <CABPQxsv-0rY=wg9DOXrUnn9d991xdUBh829QMZ9hb3ak0ZgNzg@mail.gmail.com>
Subject: ANNOUNCE: New build script ./build/mvn
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,

A consistent piece of feedback from Spark developers has been that the
Maven build is very slow. Typesafe provides a tool called Zinc which
improves Scala complication speed substantially with Maven, but is
difficult to install and configure, especially for platforms other
than Mac OS.

I've just merged a patch (authored by Brennon York) that provides an
automatically configured Maven instance with Zinc embedded in Spark.
E.g.:

    ./build/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 package

It is hard to test changes like this across all environments, so
please give this a spin and report any issues on the Spark JIRA. It is
working correctly if you see the following message during compilation:

    [INFO] Using zinc server for incremental compilation

Note that developers preferring their own Maven installation are
unaffected by this and can just ignore this new feature.

Cheers,
- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 21:54:26 2014
Return-Path: <dev-return-10949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BBB4BC954
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 21:54:26 +0000 (UTC)
Received: (qmail 50419 invoked by uid 500); 27 Dec 2014 21:54:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50330 invoked by uid 500); 27 Dec 2014 21:54:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50318 invoked by uid 99); 27 Dec 2014 21:54:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 21:54:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 21:54:19 +0000
Received: by mail-ie0-f180.google.com with SMTP id rp18so10863647iec.11
        for <dev@spark.apache.org>; Sat, 27 Dec 2014 13:51:43 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=pAwzngavxeuGJ5G2a3UIp5ksr1KLXhS6HFuvfK1lwE0=;
        b=fm4v63aEsggi3ZksD6rZmY2bHIdIyvLWgdpikmSYuThyPjVqQOqByQz9d8r19kxzSe
         gvEK9qMHu2S3/mbzA/zuCxQGrfdAiM77yLnOoOxsICGo0OkRb8PAnLbMZea6fRQSTut5
         JOXcQyOChahtZFFQvhZqln9A5XDgt6w+xN5kHa2JsBvxcvXTDIM3Fuw6FWgIGWeWcbmv
         RYvPTqVhv+emNIW+OPGLw7smocbYODwIfik1gS3DQTEDdodasPH124X+v3JAfN0SG1P8
         GQVt5Owpu3hjnbL1Vkk+P0uWeMvm/WedOs7FXRVYXAEMXuGLfn4kAS266Zyfy/Guk4KJ
         3Fpg==
X-Received: by 10.107.17.169 with SMTP id 41mr42816211ior.90.1419717103569;
 Sat, 27 Dec 2014 13:51:43 -0800 (PST)
MIME-Version: 1.0
References: <CABPQxsv-0rY=wg9DOXrUnn9d991xdUBh829QMZ9hb3ak0ZgNzg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 27 Dec 2014 21:51:43 +0000
Message-ID: <CAOhmDze90C1UeAtmLQwYFZH9C0am-=-ZYVjDh758GUUdGKqPzw@mail.gmail.com>
Subject: Re: ANNOUNCE: New build script ./build/mvn
To: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ed7fca3d236050b39a535
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ed7fca3d236050b39a535
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Linkies for the curious:

   - SPARK-4501 <https://issues.apache.org/jira/browse/SPARK-4501>: Create
   build/mvn to automatically download maven/zinc/scalac
   - https://github.com/apache/spark/pull/3707
   - New build folder (mvn and sbt):
   https://github.com/apache/spark/tree/master/build

Nick
=E2=80=8B

On Sat Dec 27 2014 at 4:46:23 PM Patrick Wendell <pwendell@gmail.com> wrote=
:

> Hi All,
>
> A consistent piece of feedback from Spark developers has been that the
> Maven build is very slow. Typesafe provides a tool called Zinc which
> improves Scala complication speed substantially with Maven, but is
> difficult to install and configure, especially for platforms other
> than Mac OS.
>
> I've just merged a patch (authored by Brennon York) that provides an
> automatically configured Maven instance with Zinc embedded in Spark.
> E.g.:
>
>     ./build/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 package
>
> It is hard to test changes like this across all environments, so
> please give this a spin and report any issues on the Spark JIRA. It is
> working correctly if you see the following message during compilation:
>
>     [INFO] Using zinc server for incremental compilation
>
> Note that developers preferring their own Maven installation are
> unaffected by this and can just ignore this new feature.
>
> Cheers,
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113ed7fca3d236050b39a535--

From dev-return-10950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Dec 27 23:45:12 2014
Return-Path: <dev-return-10950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 564A7CAEA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 27 Dec 2014 23:45:12 +0000 (UTC)
Received: (qmail 7947 invoked by uid 500); 27 Dec 2014 23:45:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7866 invoked by uid 500); 27 Dec 2014 23:45:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7855 invoked by uid 99); 27 Dec 2014 23:45:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 23:45:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 27 Dec 2014 23:44:37 +0000
Received: by mail-wi0-f171.google.com with SMTP id bs8so19366309wib.4
        for <dev@spark.apache.org>; Sat, 27 Dec 2014 15:44:36 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=hed8gyQ0iK8eH1X9tRf01Y53HmkKHw8VVcaEBlSIceY=;
        b=KPhYNK6SbALP8Qx4YkhCl0A+rA1OreXTdGmy7KZ1GbHpmrt7qPADS2kiW2qPWwpVW3
         vXNBLpXdjGvBmxVc5THYDej26doZz/OCqgU88O9lyjh6bYvm186OImpn/NaFifXdC4e1
         VtI+WxsScmHCQ7xQ9xmcoqBeoc4jEY2V86Fwy/sCDUbj4veRCLzIFiTr8MacKXmA9t5O
         lrq+yPY77HSY3KVAeuzpw3puVN8n9KkSe5ifmvCNyTqjZhaXNC+mZAuU85O2+oak3Vt+
         Agz7fBCdFWWa9NaCDMWLDpYwSu1Icsz36GVHjCC9D2VYowdfv+3Fymyg6uxc2PDidV78
         KD2Q==
X-Gm-Message-State: ALoCoQn9wYMp5JwRNwb+opT2HAglCWlDTzPzFF4x75FjW752le1lBtxJcSBOTnu+zv1PMk5E4OKL
MIME-Version: 1.0
X-Received: by 10.194.185.243 with SMTP id ff19mr91882809wjc.126.1419723875882;
 Sat, 27 Dec 2014 15:44:35 -0800 (PST)
Received: by 10.216.68.132 with HTTP; Sat, 27 Dec 2014 15:44:35 -0800 (PST)
In-Reply-To: <CAOhmDze90C1UeAtmLQwYFZH9C0am-=-ZYVjDh758GUUdGKqPzw@mail.gmail.com>
References: <CABPQxsv-0rY=wg9DOXrUnn9d991xdUBh829QMZ9hb3ak0ZgNzg@mail.gmail.com>
	<CAOhmDze90C1UeAtmLQwYFZH9C0am-=-ZYVjDh758GUUdGKqPzw@mail.gmail.com>
Date: Sat, 27 Dec 2014 15:44:35 -0800
Message-ID: <CAAsvFPnf1f3jrLFqp8Ak+8zbkC2Rrj-wJqyZrPGKwZpDXs7PjQ@mail.gmail.com>
Subject: Re: ANNOUNCE: New build script ./build/mvn
From: Mark Hamstra <mark@clearstorydata.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb03f124d3311050b3b39ae
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03f124d3311050b3b39ae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

>
> Scala complication speed


Heh.  I like that.

On Sat, Dec 27, 2014 at 1:51 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Linkies for the curious:
>
>    - SPARK-4501 <https://issues.apache.org/jira/browse/SPARK-4501>: Creat=
e
>    build/mvn to automatically download maven/zinc/scalac
>    - https://github.com/apache/spark/pull/3707
>    - New build folder (mvn and sbt):
>    https://github.com/apache/spark/tree/master/build
>
> Nick
> =E2=80=8B
>
> On Sat Dec 27 2014 at 4:46:23 PM Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Hi All,
> >
> > A consistent piece of feedback from Spark developers has been that the
> > Maven build is very slow. Typesafe provides a tool called Zinc which
> > improves Scala complication speed substantially with Maven, but is
> > difficult to install and configure, especially for platforms other
> > than Mac OS.
> >
> > I've just merged a patch (authored by Brennon York) that provides an
> > automatically configured Maven instance with Zinc embedded in Spark.
> > E.g.:
> >
> >     ./build/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 package
> >
> > It is hard to test changes like this across all environments, so
> > please give this a spin and report any issues on the Spark JIRA. It is
> > working correctly if you see the following message during compilation:
> >
> >     [INFO] Using zinc server for incremental compilation
> >
> > Note that developers preferring their own Maven installation are
> > unaffected by this and can just ignore this new feature.
> >
> > Cheers,
> > - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7bb03f124d3311050b3b39ae--

From dev-return-10951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 28 23:12:44 2014
Return-Path: <dev-return-10951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3520E10F6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 28 Dec 2014 23:12:44 +0000 (UTC)
Received: (qmail 86935 invoked by uid 500); 28 Dec 2014 23:12:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86851 invoked by uid 500); 28 Dec 2014 23:12:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86840 invoked by uid 99); 28 Dec 2014 23:12:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 28 Dec 2014 23:12:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 28 Dec 2014 23:12:16 +0000
Received: by mail-qc0-f169.google.com with SMTP id w7so8823713qcr.0
        for <dev@spark.apache.org>; Sun, 28 Dec 2014 15:10:23 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=oLewgmdCZNnOuLuE+buSq51VTTpKYal7KQzDGzjAztA=;
        b=OS6fILeV3Kjaffs6ROx8LWOdceTzfJnoWY0XXaWE6LbD5mEPpN+f9grs5Mq7raw/4W
         7bTwHoLgdSfQMsUuEuMT98w48w/dIAwc5hZvRlJYn+OTYEN5kG/cRoUFzE4nbfFH2/zN
         zlYeO7QEBUwpR1I9u1W5m+qwW37mIQ/htK6JNraBcEIX/yVtUxo46CtsiNbh3u3L7YXq
         CkpOW//OG/oFSahFGSk/uOA6sGEXkHLVr0nECxhJjQklyEKXyi06MuwvckhW3JjcWYPx
         OKZRdUdRpD5SrB6WA6y8rZpI1ARStm/DlwzqeLXJ09hbJnZMFBWOV9Y9G7LrlNXJ5uhf
         kItQ==
X-Gm-Message-State: ALoCoQlGpHyKEyivtd0lwXW1BJH/1+BgT1WSXjLH2SFSD4agsuNjgWm+UY11pACLNU9AZWFULrsO
MIME-Version: 1.0
X-Received: by 10.224.61.145 with SMTP id t17mr13398587qah.49.1419808223582;
 Sun, 28 Dec 2014 15:10:23 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Sun, 28 Dec 2014 15:10:23 -0800 (PST)
Date: Sun, 28 Dec 2014 18:10:23 -0500
Message-ID: <CAPEc=JtvAQqn2VWKh0CT23KqFuEaW29qFVxMFdpT27bmUn5i4A@mail.gmail.com>
Subject: Spark Error - Failed to locate the winutils binary in the hadoop
 binary path
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc8d66d0e63f050b4edc34
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc8d66d0e63f050b4edc34
Content-Type: text/plain; charset=UTF-8

Hi All,

I am getting the below error while running a simple spark application from
Eclipse.

I am using Eclipse, Maven, Java.

I've spark running locally on my Windows laptop. I copied the spark files
from the spark summit 2014 training
http://databricks.com/spark-training-resources#itas

I can run sample commands and small programs using the spark shell. But
getting the below error while running from Eclipse.

14/12/28 18:01:59 ERROR Shell: Failed to locate the winutils binary in the
hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in
the Hadoop binaries.
at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:362)
at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
at scala.Option.map(Option.scala:145)
at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:170)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
at org.apache.spark.rdd.FilteredRDD.getPartitions(FilteredRDD.scala:29)


Please suggest if I am doing something wrong.


Thanks for help

-Naveen

--047d7bdc8d66d0e63f050b4edc34--

From dev-return-10952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Dec 28 23:36:41 2014
Return-Path: <dev-return-10952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4565D10071
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 28 Dec 2014 23:36:40 +0000 (UTC)
Received: (qmail 9089 invoked by uid 500); 28 Dec 2014 23:36:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9005 invoked by uid 500); 28 Dec 2014 23:36:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8994 invoked by uid 99); 28 Dec 2014 23:36:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 28 Dec 2014 23:36:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 28 Dec 2014 23:36:11 +0000
Received: by mail-qa0-f45.google.com with SMTP id f12so7266773qad.18
        for <dev@spark.apache.org>; Sun, 28 Dec 2014 15:35:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=lcHcsfhPLFG/9oof+ENvDdi4noiOXEHleRr/AJTrUkU=;
        b=TDlosspNAH+Mn2tmbuIOXXCeBmUgXvyIQxknRiKoZbSrnO3Qbnx4oNBk896hXQLtWm
         VYZNTl+7EP/rR+0fIOY7EweFaYIjh2Y56SR9olsnnQm76hOR2R9lbXq4c2+cC4cOiIGJ
         5D+hcwVuqXHbWvm8x0oPsNqKiTQuOOpfukstCN0chFm5ASudxHEkh/XNRnGAq/jJK9gI
         ldOLsWAeuBNM99XH0nNc+hBZoBA+Jn4zB72tuR/RJnb6r0+nfW6ooeHa9VkT1C/OqC/2
         +XHyl5MG97T4heE62aF2SBkW41vLp05FTDO6nxeoyOSk/egd27O8W7ZFyFyZpr8bSQmv
         hQLg==
X-Gm-Message-State: ALoCoQk1i1kBuYAylIvt1/8cv/Ykf4eeQhj1t4I+IsdUapOAttx8ILnSEVjlaV9buzpV7+wrmTkx
MIME-Version: 1.0
X-Received: by 10.224.121.142 with SMTP id h14mr87045565qar.80.1419809750195;
 Sun, 28 Dec 2014 15:35:50 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Sun, 28 Dec 2014 15:35:50 -0800 (PST)
In-Reply-To: <CAPEc=JtvAQqn2VWKh0CT23KqFuEaW29qFVxMFdpT27bmUn5i4A@mail.gmail.com>
References: <CAPEc=JtvAQqn2VWKh0CT23KqFuEaW29qFVxMFdpT27bmUn5i4A@mail.gmail.com>
Date: Sun, 28 Dec 2014 18:35:50 -0500
Message-ID: <CAPEc=Ju0qp1tgDu8_Spk81b0JjfgzwP6L27RTT-iznz6vq+1BQ@mail.gmail.com>
Subject: Re: Spark Error - Failed to locate the winutils binary in the hadoop
 binary path
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0141a8becf2e6f050b4f37d8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0141a8becf2e6f050b4f37d8
Content-Type: text/plain; charset=UTF-8

Hi All,

Sorry, I should have checked the JIRA issue tracker before sending the
email.

I found this is an already existing issue,

https://issues.apache.org/jira/browse/SPARK-2356

And the solution is present in the below location,
http://qnalist.com/questions/4994960/run-spark-unit-test-on-windows-7


Now it is working fine.

Thanks all.


On Sun, Dec 28, 2014 at 6:10 PM, Naveen Madhire <vmadhire@umail.iu.edu>
wrote:

> Hi All,
>
> I am getting the below error while running a simple spark application from
> Eclipse.
>
> I am using Eclipse, Maven, Java.
>
> I've spark running locally on my Windows laptop. I copied the spark files
> from the spark summit 2014 training
> http://databricks.com/spark-training-resources#itas
>
> I can run sample commands and small programs using the spark shell. But
> getting the below error while running from Eclipse.
>
> 14/12/28 18:01:59 ERROR Shell: Failed to locate the winutils binary in the
> hadoop binary path
> java.io.IOException: Could not locate executable null\bin\winutils.exe in
> the Hadoop binaries.
> at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
> at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
> at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
> at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
> at
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:362)
> at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
> at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
> at
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
> at
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
> at scala.Option.map(Option.scala:145)
> at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:170)
> at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)
> at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
> at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> at scala.Option.getOrElse(Option.scala:120)
> at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
> at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
> at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
> at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> at scala.Option.getOrElse(Option.scala:120)
> at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
> at org.apache.spark.rdd.FilteredRDD.getPartitions(FilteredRDD.scala:29)
>
>
> Please suggest if I am doing something wrong.
>
>
> Thanks for help
>
> -Naveen
>

--089e0141a8becf2e6f050b4f37d8--

From dev-return-10953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 03:22:19 2014
Return-Path: <dev-return-10953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0E8510503
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 03:22:19 +0000 (UTC)
Received: (qmail 29929 invoked by uid 500); 29 Dec 2014 03:22:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29848 invoked by uid 500); 29 Dec 2014 03:22:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29837 invoked by uid 99); 29 Dec 2014 03:22:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 03:22:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 03:22:11 +0000
Received: by mail-qc0-f170.google.com with SMTP id x3so9137309qcv.29
        for <dev@spark.apache.org>; Sun, 28 Dec 2014 19:21:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=2wfv0LC55zKJzWFNvFS1hj+VoAkyatg3Xh6cIqASn+c=;
        b=EVFNWofqxcydJCz8D4k3Yvfl25YPH5AzgSygwNUvMRhEoRCXw+pdxIvSwWvUZ1eVOj
         uB2Na6Q/MMMgOeFQw1bQx6g++3o0n6GpY6q8l8JhJc/bHIqJj9buKyN72ZI4AySKuYVv
         fv043qf0/OoTFpwNbW3bdkyeZQq+o/XlCaIiapDJOq2s7MSYgYXNeNPSOkNp5P3dwI7K
         Uj+FvQELl6OS+nCBT+WQrgSf/F7bAR2MCwN/1bl+EZwygGPuEBIV00xVys5GysLKaFD5
         /68FjSiNFhbS+qfi/I30J4YkMpClZGMKr5WIQeqtpek8j0GlvNSukaKslwnbZOdpRItY
         w/YA==
X-Gm-Message-State: ALoCoQlkVHBgL6YXkAo8DCdiKChfpcn8bAohWzww9aJXbRySd1JlBD1fhascv3Q4Rg5XABRFYLyM
MIME-Version: 1.0
X-Received: by 10.140.85.100 with SMTP id m91mr81307937qgd.7.1419823290539;
 Sun, 28 Dec 2014 19:21:30 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Sun, 28 Dec 2014 19:21:30 -0800 (PST)
Date: Sun, 28 Dec 2014 22:21:30 -0500
Message-ID: <CAPEc=JujFQAmwoxDWj83DghtvYcvF=TWYxsJu4SHsQOH9zSkxA@mail.gmail.com>
Subject: Spark 1.2.0 build error
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c14d06e07a25050b525edc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c14d06e07a25050b525edc
Content-Type: text/plain; charset=UTF-8

Hi,

I am follow the below link for building Spark 1.2.0

https://spark.apache.org/docs/1.2.0/building-spark.html

I am getting the below error during the Maven build. I am using IntelliJ
IDE.

The build is failing in the scalatest plugin,

[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [3.355s]
[INFO] Spark Project Networking .......................... SUCCESS [4.017s]
[INFO] Spark Project Shuffle Streaming Service ........... SUCCESS [2.914s]
[INFO] Spark Project Core ................................ FAILURE [9.678s]
[INFO] Spark Project Bagel ............................... SKIPPED
[INFO] Spark Project GraphX .............................. SKIPPED



[ERROR] Failed to execute goal
org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
spark-core_2.10: There are test failures -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
spark-core_2.10: There are test failures
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)


Is there something which I am missing during the build process. Please
suggest.

Using
IntelliJ 13.1.6
Maven 3.1.1
Scala 2.10.4
Spark 1.2.0


Thanks
Naveen

--001a11c14d06e07a25050b525edc--

From dev-return-10954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 08:09:28 2014
Return-Path: <dev-return-10954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ACFBF10962
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 08:09:28 +0000 (UTC)
Received: (qmail 47894 invoked by uid 500); 29 Dec 2014 08:09:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47813 invoked by uid 500); 29 Dec 2014 08:09:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47778 invoked by uid 99); 29 Dec 2014 08:09:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 08:09:26 +0000
X-ASF-Spam-Status: No, hits=-3.4 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of daoyuan.wang@intel.com designates 134.134.136.24 as permitted sender)
Received: from [134.134.136.24] (HELO mga09.intel.com) (134.134.136.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 08:09:20 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by orsmga102.jf.intel.com with ESMTP; 29 Dec 2014 00:04:53 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,659,1413270000"; 
   d="scan'208";a="661827815"
Received: from kmsmsx151.gar.corp.intel.com ([172.21.73.86])
  by orsmga002.jf.intel.com with ESMTP; 29 Dec 2014 00:06:46 -0800
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 KMSMSX151.gar.corp.intel.com (172.21.73.86) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Mon, 29 Dec 2014 16:06:45 +0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.110]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.182]) with mapi id 14.03.0195.001;
 Mon, 29 Dec 2014 16:06:43 +0800
From: "Wang, Daoyuan" <daoyuan.wang@intel.com>
To: Alessandro Baretta <alexbaretta@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>
Subject: RE: Unsupported Catalyst types in Parquet
Thread-Topic: Unsupported Catalyst types in Parquet
Thread-Index: AQHQIaF0vtW/afAdlkCSX17MyudGDJyl2VLg
Date: Mon, 29 Dec 2014 08:06:43 +0000
Message-ID: <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
In-Reply-To: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

SGkgQWxleCwNCg0KSSdsbCBjcmVhdGUgSklSQSBTUEFSSy00OTg1IGZvciBkYXRlIHR5cGUgc3Vw
cG9ydCBpbiBwYXJxdWV0LCBhbmQgU1BBUkstNDk4NyBmb3IgdGltZXN0YW1wIHR5cGUgc3VwcG9y
dC4gRm9yIGRlY2ltYWwgdHlwZSwgSSB0aGluayB3ZSBvbmx5IHN1cHBvcnQgZGVjaW1hbHMgdGhh
dCBmaXRzIGluIGEgbG9uZy4NCg0KVGhhbmtzLA0KRGFveXVhbg0KDQotLS0tLU9yaWdpbmFsIE1l
c3NhZ2UtLS0tLQ0KRnJvbTogQWxlc3NhbmRybyBCYXJldHRhIFttYWlsdG86YWxleGJhcmV0dGFA
Z21haWwuY29tXSANClNlbnQ6IFNhdHVyZGF5LCBEZWNlbWJlciAyNywgMjAxNCAyOjQ3IFBNDQpU
bzogZGV2QHNwYXJrLmFwYWNoZS5vcmc7IE1pY2hhZWwgQXJtYnJ1c3QNClN1YmplY3Q6IFVuc3Vw
cG9ydGVkIENhdGFseXN0IHR5cGVzIGluIFBhcnF1ZXQNCg0KTWljaGFlbCwNCg0KSSdtIGhhdmlu
ZyB0cm91YmxlIHN0b3JpbmcgbXkgU2NoZW1hUkREcyBpbiBQYXJxdWV0IGZvcm1hdCB3aXRoIFNw
YXJrU1FMLCBkdWUgdG8gbXkgUkREcyBoYXZpbmcgaGF2aW5nIERhdGVUeXBlIGFuZCBEZWNpbWFs
VHlwZSBmaWVsZHMuIFdoYXQgd291bGQgaXQgdGFrZSB0byBhZGQgUGFycXVldCBzdXBwb3J0IGZv
ciB0aGVzZSBDYXRhbHlzdD8gQXJlIHRoZXJlIGFueSBvdGhlciBDYXRhbHlzdCB0eXBlcyBmb3Ig
d2hpY2ggdGhlcmUgaXMgbm8gQ2F0YWx5c3Qgc3VwcG9ydD8NCg0KQWxleA0K
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-10955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 08:52:35 2014
Return-Path: <dev-return-10955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 523F2109F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 08:52:35 +0000 (UTC)
Received: (qmail 89238 invoked by uid 500); 29 Dec 2014 08:52:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89155 invoked by uid 500); 29 Dec 2014 08:52:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88678 invoked by uid 99); 29 Dec 2014 08:52:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 08:52:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 08:52:28 +0000
Received: by mail-wi0-f181.google.com with SMTP id r20so21410625wiv.2
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 00:52:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=37KTVA1Szm6HAE+qMlqejluRNQEvI9e7xJkJH6ITkYc=;
        b=Xq4KBLWtncgcN7xbO+ATXHagVepDzdeLD+yOozrZ17mrkEA75raIfXjIqdmqfbYYfb
         INLmoVoU4vxvJ43+28VGm8kMdFHd17ynN1o1Add/RU4PTZxcx5YWjjJX6EDFVbKJ0utC
         XZeYY+lAzXODIgvou1jWDTWQclM9KEIsG2JE3z28VMziTlaQ4UX7gimtkLM3qeo9Gblq
         19KRiCWrYH0E/G4izmrFlkcCGErt4iBMPEO8dF4wIKhzc/l+YQcQZ+hXJfHxf+LFpssx
         q1yuiMdueLzYF4UhSxiHhIw3Dy/R+RyFOEhnwtimyRYSV7PONzqaWmOxG6gMFrOnLrFL
         y7Jg==
X-Gm-Message-State: ALoCoQkVrji/0Im5ABBD+mdaDIwx0i2kdW8vvrDAd5CTjSznAKbX2Vqj2eGCC0X1SrSttVaD19vY
MIME-Version: 1.0
X-Received: by 10.195.12.35 with SMTP id en3mr109952217wjd.129.1419843127777;
 Mon, 29 Dec 2014 00:52:07 -0800 (PST)
Received: by 10.27.83.204 with HTTP; Mon, 29 Dec 2014 00:52:07 -0800 (PST)
Received: by 10.27.83.204 with HTTP; Mon, 29 Dec 2014 00:52:07 -0800 (PST)
In-Reply-To: <CAPEc=JujFQAmwoxDWj83DghtvYcvF=TWYxsJu4SHsQOH9zSkxA@mail.gmail.com>
References: <CAPEc=JujFQAmwoxDWj83DghtvYcvF=TWYxsJu4SHsQOH9zSkxA@mail.gmail.com>
Date: Mon, 29 Dec 2014 08:52:07 +0000
Message-ID: <CAMAsSdKs3inqAU-+yBH98XHuk8TLYn2sm5qeo+b7AvKec7=NeA@mail.gmail.com>
Subject: Re: Spark 1.2.0 build error
From: Sean Owen <sowen@cloudera.com>
To: Naveen Madhire <vmadhire@umail.iu.edu>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf78844b5fc050b56fd21
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf78844b5fc050b56fd21
Content-Type: text/plain; charset=UTF-8

It means a test failed but you have not shown the test failure. This would
have been logged earlier. You would need to say how you ran tests too. The
tests for 1.2.0 pass for me on several common permutations.
On Dec 29, 2014 3:22 AM, "Naveen Madhire" <vmadhire@umail.iu.edu> wrote:

> Hi,
>
> I am follow the below link for building Spark 1.2.0
>
> https://spark.apache.org/docs/1.2.0/building-spark.html
>
> I am getting the below error during the Maven build. I am using IntelliJ
> IDE.
>
> The build is failing in the scalatest plugin,
>
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM .......................... SUCCESS [3.355s]
> [INFO] Spark Project Networking .......................... SUCCESS [4.017s]
> [INFO] Spark Project Shuffle Streaming Service ........... SUCCESS [2.914s]
> [INFO] Spark Project Core ................................ FAILURE [9.678s]
> [INFO] Spark Project Bagel ............................... SKIPPED
> [INFO] Spark Project GraphX .............................. SKIPPED
>
>
>
> [ERROR] Failed to execute goal
> org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
> spark-core_2.10: There are test failures -> [Help 1]
> org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
> goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
> spark-core_2.10: There are test failures
> at
>
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
> at
>
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
> at
>
> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
> at
>
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
> at
>
> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
> at
>
> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
> at
>
> org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
>
>
> Is there something which I am missing during the build process. Please
> suggest.
>
> Using
> IntelliJ 13.1.6
> Maven 3.1.1
> Scala 2.10.4
> Spark 1.2.0
>
>
> Thanks
> Naveen
>

--047d7bfcf78844b5fc050b56fd21--

From dev-return-10956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 15:59:50 2014
Return-Path: <dev-return-10956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EE42107C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 15:59:50 +0000 (UTC)
Received: (qmail 78545 invoked by uid 500); 29 Dec 2014 15:59:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78465 invoked by uid 500); 29 Dec 2014 15:59:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78453 invoked by uid 99); 29 Dec 2014 15:59:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 15:59:47 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.123 as permitted sender)
Received: from [77.75.72.123] (HELO mxf1.seznam.cz) (77.75.72.123)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 15:59:22 +0000
Received: from email.seznam.cz
	by email-smtpc7a.go.seznam.cz (email-smtpc7a.go.seznam.cz [192.168.92.44])
	id 4ec61505c9dff3c34d5e1047;
	Mon, 29 Dec 2014 16:59:20 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1419868760; bh=GZscAv9BSoFOchhbIzG5dCIzjvVXQo4Bcbv/1/gpPXU=;
	h=Received:From:To:Subject:Date:Message-Id:Mime-Version:X-Mailer:
	 Content-Type;
	b=YSHt+MwaJj4zdZ3fS+GnKguvehig6wgMCo67jmkdSyTU7ImQh1JdKMMR+R5QEKT3B
	 nKjNDeUvHwYR/rnpdERR2xY1YxlcVeencUKTzVEXSOJzDLxprCwWxoP1USfxZYnQWZ
	 rC/QE/7L6yKistwI6utavKMtQbISSxI8Qck2N0Kk=
Received: from cust.68.12.presnet.sk (cust.68.12.presnet.sk [178.18.68.12])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Mon, 29 Dec 2014 16:59:20 +0100 (CET)
From: "Jakub Dubovsky" <spark.dubovsky.jakub@seznam.cz>
To: <dev@spark.apache.org>
Subject: How to become spark developer in jira?
Date: Mon, 29 Dec 2014 16:59:20 +0100 (CET)
Message-Id: <2fmF.3c14H.7c}N4niPtSR.1KeNfO@seznam.cz>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_1da361a67ee8da5e581b6717=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_1da361a67ee8da5e581b6717=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi devs,=0A=
=0A=
=C2=A0 I'd like to ask what are the procedures/conditions for being assign=
ed a =0A=
role of a developer on spark jira? My motivation is to be able to assign =
=0A=
issues to myself. Only related resource I have found is jira permission =
=0A=
scheme [1].=0A=
=0A=
=C2=A0 regards=0A=
=C2=A0 Jakub=0A=
=0A=
=C2=A0[1] https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissio=
ns+=0A=
Scheme=0A=
--=_1da361a67ee8da5e581b6717=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-10957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 16:15:13 2014
Return-Path: <dev-return-10957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1BF51082C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 16:15:13 +0000 (UTC)
Received: (qmail 5845 invoked by uid 500); 29 Dec 2014 16:15:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5764 invoked by uid 500); 29 Dec 2014 16:15:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5742 invoked by uid 99); 29 Dec 2014 16:15:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 16:15:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.180 as permitted sender)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 16:14:59 +0000
Received: by mail-ob0-f180.google.com with SMTP id wp4so41723284obc.11
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 08:13:53 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=TfpFjah0VCHcVOIx0BpOHLHM6if5DojsIiXnX/kdpbE=;
        b=zLCEPYigNX649FNGterEYV/uFoG9m/QIboaAL9peU+2Mjv3o/DYbNu0SWL//aQdWSC
         r826X9MQtRd2fI76amLuLMT2MAq/yRyWgDTGWvgqzOF62vuszL1LuEIQe01xfHTpbI4d
         EZPRvR4kbVPOHol8bL0WikXU0RuX5vdMWCFxqSunjpSrx43gTDmZg6qlon4RWTXJtI05
         hYrY3gghlQme4H1gWpsmqykD3thXQ+mWR/cdChGpZQA5VGikdimFI5ebtiPTscJc6t6s
         DnngbBOL7fwOC8bazYLH3w3V255Zn7HsjAcPF0M+EGpEUZD2Y/AD4USdcX4kw/hpFEP7
         H+jQ==
MIME-Version: 1.0
X-Received: by 10.182.58.81 with SMTP id o17mr33545975obq.82.1419869633860;
 Mon, 29 Dec 2014 08:13:53 -0800 (PST)
Received: by 10.76.95.195 with HTTP; Mon, 29 Dec 2014 08:13:53 -0800 (PST)
Received: by 10.76.95.195 with HTTP; Mon, 29 Dec 2014 08:13:53 -0800 (PST)
In-Reply-To: <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
	<CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
Date: Mon, 29 Dec 2014 08:13:53 -0800
Message-ID: <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
Subject: RE: Unsupported Catalyst types in Parquet
From: Alessandro Baretta <alexbaretta@gmail.com>
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f83a83b275a0c050b5d29a6
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f83a83b275a0c050b5d29a6
Content-Type: text/plain; charset=UTF-8

Daoyuan,

Thanks for creating the jiras. I need these features by... last week, so
I'd be happy to take care of this myself, if only you or someone more
experienced than me in the SparkSQL codebase could provide some guidance.

Alex
On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:

> Hi Alex,
>
> I'll create JIRA SPARK-4985 for date type support in parquet, and
> SPARK-4987 for timestamp type support. For decimal type, I think we only
> support decimals that fits in a long.
>
> Thanks,
> Daoyuan
>
> -----Original Message-----
> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
> Sent: Saturday, December 27, 2014 2:47 PM
> To: dev@spark.apache.org; Michael Armbrust
> Subject: Unsupported Catalyst types in Parquet
>
> Michael,
>
> I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
> due to my RDDs having having DateType and DecimalType fields. What would it
> take to add Parquet support for these Catalyst? Are there any other
> Catalyst types for which there is no Catalyst support?
>
> Alex
>

--e89a8f83a83b275a0c050b5d29a6--

From dev-return-10958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 18:31:50 2014
Return-Path: <dev-return-10958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA6B110B9C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 18:31:50 +0000 (UTC)
Received: (qmail 83451 invoked by uid 500); 29 Dec 2014 18:31:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83374 invoked by uid 500); 29 Dec 2014 18:31:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83355 invoked by uid 99); 29 Dec 2014 18:31:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 18:31:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 18:31:43 +0000
Received: by mail-pd0-f179.google.com with SMTP id fp1so17663136pdb.24
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 10:30:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=QF95+qJO8mF9zvihLPzSyN96wV6EpUj4iv+pzoxklpA=;
        b=OsbagYkfekZ+/BQId/GvrxHeWZL7ROMgic9ezdJ6Mduj3eQSdJGKhvTuudS3RgDjvP
         bvhiY8xUt8hci+fX/9qQdKT/47H3WxVj7irwoS0v9SP+JikG+RIz7zPy1oY3ecn4Xbnz
         Ly//ox9NfDCTMYjQjzTrhN3ZRgYZsMNIdQm85x5rx7+TtTxX27nfLhdJ/i1mdUKdSoWW
         og1OQvOKaC/Z2Fi2PW9CBoYUHNSZN3FBficy9EiqgbKk5Yq6nSB8WRBYLFVqqI4V8Y3A
         OfWbFxD6OqCjZzW6YaLsGiZbn/I7zrHh+jeG9LP5IvzdDUILw8/7myhqroJ2P8VWHpOk
         mKtA==
X-Received: by 10.70.138.37 with SMTP id qn5mr93171725pdb.118.1419877837985;
        Mon, 29 Dec 2014 10:30:37 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ig1sm36239448pbc.41.2014.12.29.10.30.36
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 29 Dec 2014 10:30:37 -0800 (PST)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.1 \(1993\))
Subject: Re: How to become spark developer in jira?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <2fmF.3c14H.7c}N4niPtSR.1KeNfO@seznam.cz>
Date: Mon, 29 Dec 2014 10:30:34 -0800
Cc: dev@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <D1CC020A-B723-4234-B05D-F77EDD4F0604@gmail.com>
References: <2fmF.3c14H.7c}N4niPtSR.1KeNfO@seznam.cz>
To: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
X-Mailer: Apple Mail (2.1993)
X-Virus-Checked: Checked by ClamAV on apache.org

Please ask someone else to assign them for now, and just comment on them =
that you're working on them. Over time if you contribute a bunch we'll =
add you to that list. The problem is that in the past, people would =
assign issues to themselves and never actually work on them, making it =
confusing for others.

Matei

> On Dec 29, 2014, at 7:59 AM, Jakub Dubovsky =
<spark.dubovsky.jakub@seznam.cz> wrote:
>=20
> Hi devs,
>=20
>   I'd like to ask what are the procedures/conditions for being =
assigned a=20
> role of a developer on spark jira? My motivation is to be able to =
assign=20
> issues to myself. Only related resource I have found is jira =
permission=20
> scheme [1].
>=20
>   regards
>   Jakub
>=20
>  [1] =
https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+
> Scheme


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 18:47:45 2014
Return-Path: <dev-return-10959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BC4D10C21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 18:47:45 +0000 (UTC)
Received: (qmail 13800 invoked by uid 500); 29 Dec 2014 18:47:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13738 invoked by uid 500); 29 Dec 2014 18:47:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13724 invoked by uid 99); 29 Dec 2014 18:47:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 18:47:42 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of spark.dubovsky.jakub@seznam.cz designates 77.75.72.43 as permitted sender)
Received: from [77.75.72.43] (HELO smtp1.seznam.cz) (77.75.72.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 18:47:36 +0000
Received: from email.seznam.cz
	by email-smtpc13a.go.seznam.cz (email-smtpc13a.go.seznam.cz [192.168.92.56])
	id 14ca9c8a93d37a4c175299c8;
	Mon, 29 Dec 2014 19:46:13 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=seznam.cz; s=beta;
	t=1419878773; bh=FS6QY7WN7LToFsivILTqB3Bigpx+hxUydCQP2iawPyA=;
	h=Received:From:To:Cc:Subject:Date:Message-Id:References:
	 Mime-Version:X-Mailer:Content-Type;
	b=UmTSv+cfql29Tlc9ng+C9yl3IAYlHJPLorsYyTFrhAXrgPCfrKaw5B3ZCgvxzfrT+
	 1dFK+eJkTShwQoAyMMpx4CkqVzilA2O8mhrguIs+2GeXkJcyNzxrZKT8EMHApaj6YZ
	 DOpXSgnh1o3B6GK3AsgDdo0eEefXn624ztkRiTL8=
Received: from cust.68.12.presnet.sk (cust.68.12.presnet.sk [178.18.68.12])
	by email.seznam.cz (szn-ebox-4.4.247) with HTTP;
	Mon, 29 Dec 2014 19:46:13 +0100 (CET)
From: "Jakub Dubovsky" <spark.dubovsky.jakub@seznam.cz>
To: "Matei Zaharia" <matei.zaharia@gmail.com>
Cc: <dev@spark.apache.org>
Subject: Re: How to become spark developer in jira?
Date: Mon, 29 Dec 2014 19:46:13 +0100 (CET)
Message-Id: <2gnA.3c179.2apA34W9NVl.1KeQ5r@seznam.cz>
References: <2fmF.3c14H.7c}N4niPtSR.1KeNfO@seznam.cz>
	<D1CC020A-B723-4234-B05D-F77EDD4F0604@gmail.com>
Mime-Version: 1.0 (szn-mime-2.0.1)
X-Mailer: szn-ebox-4.4.247
Content-Type: multipart/alternative;
	boundary="=_784e04c832b7ae62741fd103=17919d76-71f0-58a2-a00e-19349e34696f_="
X-Virus-Checked: Checked by ClamAV on apache.org

--=_784e04c832b7ae62741fd103=17919d76-71f0-58a2-a00e-19349e34696f_=
Content-Type: text/plain;
	charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi Matei,=0A=
=0A=
=C2=A0 that makes sense. Thanks a lot!=0A=
=0A=
=C2=A0 Jakub=0A=
=0A=
=0A=
---------- P=C5=AFvodn=C3=AD zpr=C3=A1va ----------=0A=
Od: Matei Zaharia <matei.zaharia@gmail.com>=0A=
Komu: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>=0A=
Datum: 29. 12. 2014 19:31:57=0A=
P=C5=99edm=C4=9Bt: Re: How to become spark developer in jira?=0A=
=0A=
"Please ask someone else to assign them for now, and just comment on them =
=0A=
that you're working on them. Over time if you contribute a bunch we'll add=
 =0A=
you to that list. The problem is that in the past, people would assign =
=0A=
issues to themselves and never actually work on them, making it confusing =
=0A=
for others.=0A=
=0A=
Matei=0A=
=0A=
> On Dec 29, 2014, at 7:59 AM, Jakub Dubovsky <spark.dubovsky.jakub@seznam=
.=0A=
cz> wrote:=0A=
> =0A=
> Hi devs,=0A=
> =0A=
> I'd like to ask what are the procedures/conditions for being assigned a =
=0A=
> role of a developer on spark jira? My motivation is to be able to assign=
 =0A=
> issues to myself. Only related resource I have found is jira permission =
=0A=
> scheme [1].=0A=
> =0A=
> regards=0A=
> Jakub=0A=
> =0A=
> [1] https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+=
=0A=
> Scheme=0A=
=0A=
=0A=
---------------------------------------------------------------------=
=0A=
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org=0A=
For additional commands, e-mail: dev-help@spark.apache.org"=
--=_784e04c832b7ae62741fd103=17919d76-71f0-58a2-a00e-19349e34696f_=--


From dev-return-10960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 22:34:18 2014
Return-Path: <dev-return-10960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 166CA102BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 22:34:18 +0000 (UTC)
Received: (qmail 17153 invoked by uid 500); 29 Dec 2014 22:34:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17073 invoked by uid 500); 29 Dec 2014 22:34:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16800 invoked by uid 99); 29 Dec 2014 22:34:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 22:34:15 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 22:33:50 +0000
Received: by mail-ig0-f179.google.com with SMTP id r2so11895503igi.0
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 14:33:48 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type:content-transfer-encoding;
        bh=3eqrJGOHFs/sHs3gaknGAEvYkaTbN4SMugc/X2mA23g=;
        b=YgstF95V4XtpwuvjOCtlZPxhi2YXcazAUhsOKXr729OHo6PfK58XZ0/gyuAD2D8qRR
         nXxs8gcuytK8LjSnCxYQnhiiJiRMyyEixCQyV9dIvvXZcPX4vwU8nFN9YE/DyPmiadQ4
         R3ApuKQ1PgPpoFODqOEetYzAtmpBWef6pCaaeLErJ9olgkJegDma1/aoHX/6JtGLM9GO
         uW4aUjCVspk/Jgzk6k7YUYmUanpfAUs3j1CoHq540nu692M+G5GAfv5W/gl72HNa7s8k
         GQF4FsRnM1zv8yBHE6JVXJoQk24IfNo5I8LUViI28k6/7re5asNwW1TYKTg+uGdLOByx
         ZUng==
X-Received: by 10.50.107.36 with SMTP id gz4mr47916125igb.25.1419892428767;
 Mon, 29 Dec 2014 14:33:48 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.12.162 with HTTP; Mon, 29 Dec 2014 14:33:18 -0800 (PST)
In-Reply-To: <CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
 <1419494077309.b1eff13f@Nodemailer> <CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Mon, 29 Dec 2014 14:33:18 -0800
Message-ID: <CAMwrk0=CaXp9i8=6kzpYEZN7tGgf62cJjmiUOnCSemXnq77pDg@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
To: Cody Koeninger <cody@koeninger.org>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Saisai Shao <saisai.shao@intel.com>, 
	Sean McNamara <sean.mcnamara@webtrends.com>, Patrick Wendell <pwendell@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Koert Kuipers <koert@tresata.com>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey all,

Some wrap up thoughts on this thread.

Let me first reiterate what Patrick said, that Kafka is super super
important as it forms the largest fraction of Spark Streaming user
base. So we really want to improve the Kafka + Spark Streaming
integration. To this end, some of the things that needs to be
considered can be broadly classified into the following to sort
facilitate the discussion.

1. Data rate control
2. Receiver failure semantics - partially achieving this gives
at-least once, completely achieving this gives exactly-once
3. Driver failure semantics - partially achieving this gives at-least
once, completely achieving this gives exactly-once

Here is a run down of what is achieved by different implementations
(based on what I think).

1. Prior to WAL in Spark 1.2, the KafkaReceiver could handle 3, could
handle 1 partially (some duplicate data), and could NOT handle 2 (all
previously received data lost).

2. In Spark 1.2 with WAL enabled, the Saisai's ReliableKafkaReceiver
can handle 3, can almost completely handle 1 and 2 (except few corner
cases which prevents it from completely guaranteeing exactly-once).

3. I believe Dibyendu's solution (correct me if i am wrong) can handle
1 and 2 perfectly. And 3 can be partially solved with WAL, or possibly
completely solved by extending the solution further.

4. Cody's solution (again, correct me if I am wrong) does not use
receivers at all (so eliminates 2). It can handle 3 completely for
simple operations like map and filter, but not sure if it works
completely for stateful ops like windows and updateStateByKey. Also it
does not handle 1.

The real challenge for Kafka is in achieving 3 completely for stateful
operations while also handling 1.  (i.e., use receivers, but still get
driver failure guarantees). Solving this will give us our holy grail
solution, and this is what I want to achieve.

On that note, Cody submitted a PR on his style of achieving
exactly-once semantics - https://github.com/apache/spark/pull/3798 . I
am reviewing it. Please follow the PR if you are interested.

TD

On Wed, Dec 24, 2014 at 11:59 PM, Cody Koeninger <cody@koeninger.org> wrote=
:
> The conversation was mostly getting TD up to speed on this thread since h=
e
> had just gotten back from his trip and hadn't seen it.
>
> The jira has a summary of the requirements we discussed, I'm sure TD or
> Patrick can add to the ticket if I missed something.
> On Dec 25, 2014 1:54 AM, "Hari Shreedharan" <hshreedharan@cloudera.com>
> wrote:
>
>> In general such discussions happen or is posted on the dev lists. Could
>> you please post a summary? Thanks.
>>
>> Thanks,
>> Hari
>>
>>
>> On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>>
>>>  After a long talk with Patrick and TD (thanks guys), I opened the
>>> following jira
>>>
>>> https://issues.apache.org/jira/browse/SPARK-4964
>>>
>>> Sample PR has an impementation for the batch and the dstream case, and =
a
>>> link to a project with example usage.
>>>
>>> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com> wrot=
e:
>>>
>>>> yup, we at tresata do the idempotent store the same way. very simple
>>>> approach.
>>>>
>>>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
>>>> wrote:
>>>>>
>>>>> That KafkaRDD code is dead simple.
>>>>>
>>>>> Given a user specified map
>>>>>
>>>>> (topic1, partition0) -> (startingOffset, endingOffset)
>>>>> (topic1, partition1) -> (startingOffset, endingOffset)
>>>>> ...
>>>>> turn each one of those entries into a partition of an rdd, using the
>>>>> simple
>>>>> consumer.
>>>>> That's it.  No recovery logic, no state, nothing - for any failures,
>>>>> bail
>>>>> on the rdd and let it retry.
>>>>> Spark stays out of the business of being a distributed database.
>>>>>
>>>>> The client code does any transformation it wants, then stores the dat=
a
>>>>> and
>>>>> offsets.  There are two ways of doing this, either based on idempoten=
ce
>>>>> or
>>>>> a transactional data store.
>>>>>
>>>>> For idempotent stores:
>>>>>
>>>>> 1.manipulate data
>>>>> 2.save data to store
>>>>> 3.save ending offsets to the same store
>>>>>
>>>>> If you fail between 2 and 3, the offsets haven't been stored, you sta=
rt
>>>>> again at the same beginning offsets, do the same calculations in the
>>>>> same
>>>>> order, overwrite the same data, all is good.
>>>>>
>>>>>
>>>>> For transactional stores:
>>>>>
>>>>> 1. manipulate data
>>>>> 2. begin transaction
>>>>> 3. save data to the store
>>>>> 4. save offsets
>>>>> 5. commit transaction
>>>>>
>>>>> If you fail before 5, the transaction rolls back.  To make this less
>>>>> heavyweight, you can write the data outside the transaction and then
>>>>> update
>>>>> a pointer to the current data inside the transaction.
>>>>>
>>>>>
>>>>> Again, spark has nothing much to do with guaranteeing exactly once.  =
In
>>>>> fact, the current streaming api actively impedes my ability to do the
>>>>> above.  I'm just suggesting providing an api that doesn't get in the
>>>>> way of
>>>>> exactly-once.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
>>>>> hshreedharan@cloudera.com
>>>>> > wrote:
>>>>>
>>>>> > Can you explain your basic algorithm for the once-only-delivery? It=
 is
>>>>> > quite a bit of very Kafka-specific code, that would take more time =
to
>>>>> read
>>>>> > than I can currently afford? If you can explain your algorithm a bi=
t,
>>>>> it
>>>>> > might help.
>>>>> >
>>>>> > Thanks,
>>>>> > Hari
>>>>> >
>>>>> >
>>>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org=
>
>>>>> > wrote:
>>>>> >
>>>>> >>
>>>>> >> The problems you guys are discussing come from trying to store sta=
te
>>>>> in
>>>>> >> spark, so don't do that.  Spark isn't a distributed database.
>>>>> >>
>>>>> >> Just map kafka partitions directly to rdds, llet user code specify
>>>>> the
>>>>> >> range of offsets explicitly, and let them be in charge of committi=
ng
>>>>> >> offsets.
>>>>> >>
>>>>> >> Using the simple consumer isn't that bad, I'm already using this i=
n
>>>>> >> production with the code I linked to, and tresata apparently has
>>>>> been as
>>>>> >> well.  Again, for everyone saying this is impossible, have you rea=
d
>>>>> either
>>>>> >> of those implementations and looked at the approach?
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>>>>> >> Sean.McNamara@webtrends.com> wrote:
>>>>> >>
>>>>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I think =
the exactly
>>>>> >>> once spark streaming semantics can easily be solved using
>>>>> updateStateByKey.
>>>>> >>> Make the key going into updateStateByKey be a hash of the event, =
or
>>>>> pluck
>>>>> >>> off some uuid from the message.  The updateFunc would only emit t=
he
>>>>> message
>>>>> >>> if the key did not exist, and the user has complete control over
>>>>> the window
>>>>> >>> of time / state lifecycle for detecting duplicates.  It also make=
s
>>>>> it
>>>>> >>> really easy to detect and take action (alert?) when you DO see a
>>>>> duplicate,
>>>>> >>> or make memory tradeoffs within an error bound using a sketch
>>>>> algorithm.
>>>>> >>> The kafka simple consumer is insanely complex, if possible I thin=
k
>>>>> it would
>>>>> >>> be better (and vastly more flexible) to get reliability using the
>>>>> >>> primitives that spark so elegantly provides.
>>>>> >>>
>>>>> >>> Cheers,
>>>>> >>>
>>>>> >>> Sean
>>>>> >>>
>>>>> >>>
>>>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>>>>> >>> hshreedharan@cloudera.com> wrote:
>>>>> >>> >
>>>>> >>> > Hi Dibyendu,
>>>>> >>> >
>>>>> >>> > Thanks for the details on the implementation. But I still do no=
t
>>>>> >>> believe
>>>>> >>> > that it is no duplicates - what they achieve is that the same
>>>>> batch is
>>>>> >>> > processed exactly the same way every time (but see it may be
>>>>> processed
>>>>> >>> more
>>>>> >>> > than once) - so it depends on the operation being idempotent. I
>>>>> believe
>>>>> >>> > Trident uses ZK to keep track of the transactions - a batch can=
 be
>>>>> >>> > processed multiple times in failure scenarios (for example, the
>>>>> >>> transaction
>>>>> >>> > is processed but before ZK is updated the machine fails, causin=
g a
>>>>> >>> "new"
>>>>> >>> > node to process it again).
>>>>> >>> >
>>>>> >>> > I don't think it is impossible to do this in Spark Streaming as
>>>>> well
>>>>> >>> and
>>>>> >>> > I'd be really interested in working on it at some point in the
>>>>> near
>>>>> >>> future.
>>>>> >>> >
>>>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>>>>> >>> > dibyendu.bhattachary@gmail.com> wrote:
>>>>> >>> >
>>>>> >>> >> Hi,
>>>>> >>> >>
>>>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. Th=
e
>>>>> Storm
>>>>> >>> >> Trident has done the exact-once guarantee by processing the
>>>>> tuple in a
>>>>> >>> >> batch  and assigning same transaction-id for a given batch . T=
he
>>>>> >>> replay for
>>>>> >>> >> a given batch with a transaction-id will have exact same set o=
f
>>>>> >>> tuples and
>>>>> >>> >> replay of batches happen in exact same order before the failur=
e.
>>>>> >>> >>
>>>>> >>> >> Having this paradigm, if downstream system process data for a
>>>>> given
>>>>> >>> batch
>>>>> >>> >> for having a given transaction-id , and if during failure if s=
ame
>>>>> >>> batch is
>>>>> >>> >> again emitted , you can check if same transaction-id is alread=
y
>>>>> >>> processed
>>>>> >>> >> or not and hence can guarantee exact once semantics.
>>>>> >>> >>
>>>>> >>> >> And this can only be achieved in Spark if we use Low Level Kaf=
ka
>>>>> >>> consumer
>>>>> >>> >> API to process the offsets. This low level Kafka Consumer (
>>>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has
>>>>> implemented the
>>>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of =
the
>>>>> >>> Kafka
>>>>> >>> >> related logic has been taken from Storm-Kafka spout and which
>>>>> manages
>>>>> >>> all
>>>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>>>>> >>> managements.
>>>>> >>> >>
>>>>> >>> >> Presently this Consumer maintains that during Receiver failure=
,
>>>>> it
>>>>> >>> will
>>>>> >>> >> re-emit the exact same Block with same set of messages . Every
>>>>> >>> message have
>>>>> >>> >> the details of its partition, offset and topic related details
>>>>> which
>>>>> >>> can
>>>>> >>> >> tackle the SPARK-3146.
>>>>> >>> >>
>>>>> >>> >> As this Low Level consumer has complete control over the Kafka
>>>>> >>> Offsets ,
>>>>> >>> >> we can implement Trident like feature on top of it like having
>>>>> >>> implement a
>>>>> >>> >> transaction-id for a given block , and re-emit the same block
>>>>> with
>>>>> >>> same set
>>>>> >>> >> of message during Driver failure.
>>>>> >>> >>
>>>>> >>> >> Regards,
>>>>> >>> >> Dibyendu
>>>>> >>> >>
>>>>> >>> >>
>>>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
>>>>> saisai.shao@intel.com>
>>>>> >>> >> wrote:
>>>>> >>> >>>
>>>>> >>> >>> Hi all,
>>>>> >>> >>>
>>>>> >>> >>> I agree with Hari that Strong exact-once semantics is very ha=
rd
>>>>> to
>>>>> >>> >>> guarantee, especially in the failure situation. From my
>>>>> >>> understanding even
>>>>> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
>>>>> >>> guarantee the
>>>>> >>> >>> exact once semantics once failed, first is the ordering of da=
ta
>>>>> >>> replaying
>>>>> >>> >>> from last checkpoint, this is hard to guarantee when multiple
>>>>> >>> partitions
>>>>> >>> >>> are injected in; second is the design complexity of achieving
>>>>> this,
>>>>> >>> you can
>>>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the
>>>>> very
>>>>> >>> details
>>>>> >>> >>> of Kafka metadata management system to achieve this, not to s=
ay
>>>>> >>> rebalance
>>>>> >>> >>> and fault-tolerance.
>>>>> >>> >>>
>>>>> >>> >>> Thanks
>>>>> >>> >>> Jerry
>>>>> >>> >>>
>>>>> >>> >>> -----Original Message-----
>>>>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:
>>>>> langel.groups@gmail.com]
>>>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
>>>>> >>> >>> To: Cody Koeninger
>>>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>>>>> >>> >>> Subject: Re: Which committers care about Kafka?
>>>>> >>> >>>
>>>>> >>> >>> But idempotency is not that easy t achieve sometimes. A stron=
g
>>>>> only
>>>>> >>> once
>>>>> >>> >>> semantic through a proper API would  be superuseful; but I'm =
not
>>>>> >>> implying
>>>>> >>> >>> this is easy to achieve.
>>>>> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org>
>>>>> wrote:
>>>>> >>> >>>
>>>>> >>> >>>> If the downstream store for the output data is idempotent or
>>>>> >>> >>>> transactional, and that downstream store also is the system =
of
>>>>> >>> record
>>>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.
>>>>> Commit
>>>>> >>> >>>> offsets with / after the data is stored.  On any failure,
>>>>> restart
>>>>> >>> from
>>>>> >>> >>> the last committed offsets.
>>>>> >>> >>>>
>>>>> >>> >>>> Yes, this approach is biased towards the etl-like use cases
>>>>> rather
>>>>> >>> >>>> than near-realtime-analytics use cases.
>>>>> >>> >>>>
>>>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>>>> >>> >>>> hshreedharan@cloudera.com
>>>>> >>> >>>>> wrote:
>>>>> >>> >>>>>
>>>>> >>> >>>>> I get what you are saying. But getting exactly once right i=
s
>>>>> an
>>>>> >>> >>>>> extremely hard problem - especially in presence of failure.
>>>>> The
>>>>> >>> >>>>> issue is failures
>>>>> >>> >>>> can
>>>>> >>> >>>>> happen in a bunch of places. For example, before the
>>>>> notification
>>>>> >>> of
>>>>> >>> >>>>> downstream store being successful reaches the receiver that
>>>>> updates
>>>>> >>> >>>>> the offsets, the node fails. The store was successful, but
>>>>> >>> >>>>> duplicates came in either way. This is something worth
>>>>> discussing
>>>>> >>> by
>>>>> >>> >>>>> itself - but without uuids etc this might not really be
>>>>> solved even
>>>>> >>> >>> when you think it is.
>>>>> >>> >>>>>
>>>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in a=
ll
>>>>> of
>>>>> >>> the
>>>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-onl=
y
>>>>> >>> >>>>> delivery,
>>>>> >>> >>>> but
>>>>> >>> >>>>> I doubt the latter is really possible to guarantee - though=
 I
>>>>> >>> really
>>>>> >>> >>>> would
>>>>> >>> >>>>> love to have that!
>>>>> >>> >>>>>
>>>>> >>> >>>>> Thanks,
>>>>> >>> >>>>> Hari
>>>>> >>> >>>>>
>>>>> >>> >>>>>
>>>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>>>> >>> >>>>> <cody@koeninger.org>
>>>>> >>> >>>>> wrote:
>>>>> >>> >>>>>
>>>>> >>> >>>>>> Thanks for the replies.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.
>>>>> If you
>>>>> >>> >>>>>> actually want exactly-once semantics, you need control of
>>>>> kafka
>>>>> >>> >>>>>> offsets
>>>>> >>> >>>> as
>>>>> >>> >>>>>> well, including the ability to not use zookeeper as the
>>>>> system of
>>>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system th=
at
>>>>> has
>>>>> >>> >>>>>> strong
>>>>> >>> >>>> ordering
>>>>> >>> >>>>>> guarantees (within a partition) and does not mandate the u=
se
>>>>> of
>>>>> >>> >>>> zookeeper
>>>>> >>> >>>>>> to store offsets.  I think there should be a spark api tha=
t
>>>>> acts
>>>>> >>> as
>>>>> >>> >>>>>> a
>>>>> >>> >>>> very
>>>>> >>> >>>>>> simple intermediary between Kafka and the user's choice of
>>>>> >>> >>>>>> downstream
>>>>> >>> >>>> store.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> Take a look at the links I posted - if there's already bee=
n 2
>>>>> >>> >>>> independent
>>>>> >>> >>>>>> implementations of the idea, chances are it's something
>>>>> people
>>>>> >>> need.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>>>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> Hi Cody,
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can impleme=
nt
>>>>> >>> >>>>>>> something pretty simple and lightweight for that one.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation -
>>>>> this is
>>>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it =
is
>>>>> a
>>>>> >>> good
>>>>> >>> >>>> idea to
>>>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>>>> >>> >>>> optimization. For
>>>>> >>> >>>>>>> that reason, we must be careful in implementation. There
>>>>> are a
>>>>> >>> >>>>>>> couple
>>>>> >>> >>>> of
>>>>> >>> >>>>>>> issues that we need to ensure works properly - specifical=
ly
>>>>> >>> >>> ordering.
>>>>> >>> >>>> To
>>>>> >>> >>>>>>> ensure we pull messages from different topics and
>>>>> partitions in
>>>>> >>> >>>>>>> the
>>>>> >>> >>>> same
>>>>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist t=
he
>>>>> metadata to
>>>>> >>> >>>>>>> HDFS
>>>>> >>> >>>> (or
>>>>> >>> >>>>>>> some other system) - this metadata must contain the order=
 of
>>>>> >>> >>>>>>> messages consumed, so we know how to re-read the messages=
.
>>>>> I am
>>>>> >>> >>>>>>> planning to
>>>>> >>> >>>> explore
>>>>> >>> >>>>>>> this once I have some time (probably in Jan). In addition=
,
>>>>> we
>>>>> >>> must
>>>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I will
>>>>> file a
>>>>> >>> >>>>>>> placeholder jira for this one.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while
>>>>> back -
>>>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping
>>>>> that
>>>>> >>> this
>>>>> >>> >>>>>>> will get pulled in soon, as this is something I know peop=
le
>>>>> want.
>>>>> >>> >>>>>>> I am open
>>>>> >>> >>>> to
>>>>> >>> >>>>>>> feedback on that - anything that I can do to make it bett=
er.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> Thanks,
>>>>> >>> >>>>>>> Hari
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>>>> >>> >>>>>>> <pwendell@gmail.com>
>>>>> >>> >>>>>>> wrote:
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>>> Hey Cody,
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming
>>>>> is TD -
>>>>> >>> >>>>>>>> he is traveling this week though so I can respond a bit.
>>>>> To the
>>>>> >>> >>>>>>>> high level point of whether Kafka is important - it
>>>>> definitely
>>>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>>>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support
>>>>> for
>>>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
>>>>> library.
>>>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
>>>>> unstable
>>>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
>>>>> stabilize
>>>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't =
be
>>>>> able
>>>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level
>>>>> impression
>>>>> >>> only
>>>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so it=
's
>>>>> worth
>>>>> >>> >>> revisiting given the developments in Kafka.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> Please do bring things up like this on the dev list if
>>>>> there are
>>>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> - Patrick
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>>>> >>> >>>>>>>> <cody@koeninger.org>
>>>>> >>> >>>>>>>> wrote:
>>>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people t=
o
>>>>> get
>>>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible
>>>>> for our
>>>>> >>> >>>>>>>> production
>>>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, becau=
se
>>>>> I've
>>>>> >>> >>>>>>>>> seen several different patches and libraries to fix the
>>>>> same
>>>>> >>> >>>>>>>>> things we've
>>>>> >>> >>>>>>>> been
>>>>> >>> >>>>>>>>> running into.
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Regarding flexibility
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivale=
nt
>>>>> of
>>>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>>>> >>> >>>>>>>>> ourselves, then found
>>>>> >>> >>>>>>>> that
>>>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn't =
be
>>>>> able
>>>>> >>> >>>>>>>>> to
>>>>> >>> >>>> get
>>>>> >>> >>>>>>>> our
>>>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a
>>>>> whole
>>>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388,
>>>>> arbitrary
>>>>> >>> >>>>>>>>> delay of
>>>>> >>> >>>>>>>> messages, etc).
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
>>>>> >>> >>>>>>>>> WriteAheadLog
>>>>> >>> >>>> as
>>>>> >>> >>>>>>>> a
>>>>> >>> >>>>>>>>> general solution for streaming unreliable sources, but
>>>>> Kafka
>>>>> >>> >>>>>>>>> already
>>>>> >>> >>>>>>>> is a
>>>>> >>> >>>>>>>>> reliable source. I think there's a need for an api that
>>>>> treats
>>>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
>>>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>>>>> >>> >>>>>>>>> write-ahead log in hdfs, I
>>>>> >>> >>>> need
>>>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>>>>> >>> >>>>>>>>> failures
>>>>> >>> >>>> that
>>>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>>>>> >>> instance).
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> I've got an implementation i've been using
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>>
>>>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Tresata has something similar at
>>>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
>>>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm
>>>>> code.
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries
>>>>> rather
>>>>> >>> >>>>>>>>> than
>>>>> >>> >>>>>>>> patches
>>>>> >>> >>>>>>>>> to Spark is problematic, because large portions of the
>>>>> >>> >>>> implementation
>>>>> >>> >>>>>>>> are
>>>>> >>> >>>>>>>>> private[spark].
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to
>>>>> get.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>>
>>>>> >>> -----------------------------------------------------------------
>>>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
>>>>> dev-unsubscribe@spark.apache.org
>>>>> >>> For
>>>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>
>>>>> >>> >>>>
>>>>> >>> >>>
>>>>> >>> >>
>>>>> >>>
>>>>> >>>
>>>>> >>
>>>>> >
>>>>>
>>>>
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 22:51:55 2014
Return-Path: <dev-return-10961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4439310315
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 22:51:55 +0000 (UTC)
Received: (qmail 44150 invoked by uid 500); 29 Dec 2014 22:51:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44079 invoked by uid 500); 29 Dec 2014 22:51:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44066 invoked by uid 99); 29 Dec 2014 22:51:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 22:51:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 22:51:22 +0000
Received: by mail-oi0-f42.google.com with SMTP id v63so30621470oia.1
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 14:49:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=oqjIj8vrBQSN+B6ZPWC+9j+fJznlcjjYMBiG9rMi5JU=;
        b=RBB5JM/Ft3mBcRWcfc4IixhWvyCcZYMOFJ5iQTR7xXU0iKm34a3OeFMpyZGo4vFFOV
         IA0Uk66y6xDLKiNmVszFpoROSib3L99hAdilVJ0EIn31LjlPjL0vj+u/qxD55kCKxGc0
         PYIP21lq7LPuHN+bLY5dbZjxUSXPalUOtbuPYx7VvR/BIO4vjW6r3FtFE80upRIC4Tqb
         GXzMOw2zjmxXAZhw5/60cHK3YsEr++S9r3jRwGnw81AvEumbahFF7LA5NbjkMicJZIts
         csrc91L5tVKy/V7Xhg6fn88Zqm1JR3PH2tOyjM40FEtNKqHjch6uNZsT9DlcJrDZ3cyJ
         S01w==
X-Gm-Message-State: ALoCoQmjvSnWOufUWlpGAMT41wOMs/jlB3gO6z6cJlg3j3Z+aJXl1PUMnNKEzuLrr9WLbmUlXMKn
MIME-Version: 1.0
X-Received: by 10.182.215.136 with SMTP id oi8mr34582166obc.18.1419893370724;
 Mon, 29 Dec 2014 14:49:30 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Mon, 29 Dec 2014 14:49:30 -0800 (PST)
In-Reply-To: <CAMwrk0=CaXp9i8=6kzpYEZN7tGgf62cJjmiUOnCSemXnq77pDg@mail.gmail.com>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
	<1419494077309.b1eff13f@Nodemailer>
	<CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
	<CAMwrk0=CaXp9i8=6kzpYEZN7tGgf62cJjmiUOnCSemXnq77pDg@mail.gmail.com>
Date: Mon, 29 Dec 2014 16:49:30 -0600
Message-ID: <CAKWX9VWfjHmJFicPJNUhhN39TWSub_p_cwrg5D=oqFsOW_xioQ@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: Tathagata Das <tathagata.das1565@gmail.com>
Cc: Hari Shreedharan <hshreedharan@cloudera.com>, Saisai Shao <saisai.shao@intel.com>, 
	Sean McNamara <sean.mcnamara@webtrends.com>, Patrick Wendell <pwendell@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Koert Kuipers <koert@tresata.com>
Content-Type: multipart/alternative; boundary=001a11c24f28fb39de050b62af1e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24f28fb39de050b62af1e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Can you give a little more clarification on exactly what is meant by

1. Data rate control

If someone wants to clamp the maximum number of messages per RDD partition
in my solution, it would be very straightforward to do so.

Regarding the holy grail, I'm pretty certain you can't have end-to-end
transactional semantics without the client code being in charge of offset
state.  That means the client code is going to also need to be in charge of
setting up an initial state for updateStateByKey that makes sense; as long
as they can do that, the job should be safe to restart from arbitrary
failures.

On Mon, Dec 29, 2014 at 4:33 PM, Tathagata Das <tathagata.das1565@gmail.com=
>
wrote:

> Hey all,
>
> Some wrap up thoughts on this thread.
>
> Let me first reiterate what Patrick said, that Kafka is super super
> important as it forms the largest fraction of Spark Streaming user
> base. So we really want to improve the Kafka + Spark Streaming
> integration. To this end, some of the things that needs to be
> considered can be broadly classified into the following to sort
> facilitate the discussion.
>
> 1. Data rate control
> 2. Receiver failure semantics - partially achieving this gives
> at-least once, completely achieving this gives exactly-once
> 3. Driver failure semantics - partially achieving this gives at-least
> once, completely achieving this gives exactly-once
>
> Here is a run down of what is achieved by different implementations
> (based on what I think).
>
> 1. Prior to WAL in Spark 1.2, the KafkaReceiver could handle 3, could
> handle 1 partially (some duplicate data), and could NOT handle 2 (all
> previously received data lost).
>
> 2. In Spark 1.2 with WAL enabled, the Saisai's ReliableKafkaReceiver
> can handle 3, can almost completely handle 1 and 2 (except few corner
> cases which prevents it from completely guaranteeing exactly-once).
>
> 3. I believe Dibyendu's solution (correct me if i am wrong) can handle
> 1 and 2 perfectly. And 3 can be partially solved with WAL, or possibly
> completely solved by extending the solution further.
>
> 4. Cody's solution (again, correct me if I am wrong) does not use
> receivers at all (so eliminates 2). It can handle 3 completely for
> simple operations like map and filter, but not sure if it works
> completely for stateful ops like windows and updateStateByKey. Also it
> does not handle 1.
>
> The real challenge for Kafka is in achieving 3 completely for stateful
> operations while also handling 1.  (i.e., use receivers, but still get
> driver failure guarantees). Solving this will give us our holy grail
> solution, and this is what I want to achieve.
>
> On that note, Cody submitted a PR on his style of achieving
> exactly-once semantics - https://github.com/apache/spark/pull/3798 . I
> am reviewing it. Please follow the PR if you are interested.
>
> TD
>
> On Wed, Dec 24, 2014 at 11:59 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> > The conversation was mostly getting TD up to speed on this thread since
> he
> > had just gotten back from his trip and hadn't seen it.
> >
> > The jira has a summary of the requirements we discussed, I'm sure TD or
> > Patrick can add to the ticket if I missed something.
> > On Dec 25, 2014 1:54 AM, "Hari Shreedharan" <hshreedharan@cloudera.com>
> > wrote:
> >
> >> In general such discussions happen or is posted on the dev lists. Coul=
d
> >> you please post a summary? Thanks.
> >>
> >> Thanks,
> >> Hari
> >>
> >>
> >> On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org>
> >> wrote:
> >>
> >>>  After a long talk with Patrick and TD (thanks guys), I opened the
> >>> following jira
> >>>
> >>> https://issues.apache.org/jira/browse/SPARK-4964
> >>>
> >>> Sample PR has an impementation for the batch and the dstream case, an=
d
> a
> >>> link to a project with example usage.
> >>>
> >>> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >>>
> >>>> yup, we at tresata do the idempotent store the same way. very simple
> >>>> approach.
> >>>>
> >>>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
> >>>> wrote:
> >>>>>
> >>>>> That KafkaRDD code is dead simple.
> >>>>>
> >>>>> Given a user specified map
> >>>>>
> >>>>> (topic1, partition0) -> (startingOffset, endingOffset)
> >>>>> (topic1, partition1) -> (startingOffset, endingOffset)
> >>>>> ...
> >>>>> turn each one of those entries into a partition of an rdd, using th=
e
> >>>>> simple
> >>>>> consumer.
> >>>>> That's it.  No recovery logic, no state, nothing - for any failures=
,
> >>>>> bail
> >>>>> on the rdd and let it retry.
> >>>>> Spark stays out of the business of being a distributed database.
> >>>>>
> >>>>> The client code does any transformation it wants, then stores the
> data
> >>>>> and
> >>>>> offsets.  There are two ways of doing this, either based on
> idempotence
> >>>>> or
> >>>>> a transactional data store.
> >>>>>
> >>>>> For idempotent stores:
> >>>>>
> >>>>> 1.manipulate data
> >>>>> 2.save data to store
> >>>>> 3.save ending offsets to the same store
> >>>>>
> >>>>> If you fail between 2 and 3, the offsets haven't been stored, you
> start
> >>>>> again at the same beginning offsets, do the same calculations in th=
e
> >>>>> same
> >>>>> order, overwrite the same data, all is good.
> >>>>>
> >>>>>
> >>>>> For transactional stores:
> >>>>>
> >>>>> 1. manipulate data
> >>>>> 2. begin transaction
> >>>>> 3. save data to the store
> >>>>> 4. save offsets
> >>>>> 5. commit transaction
> >>>>>
> >>>>> If you fail before 5, the transaction rolls back.  To make this les=
s
> >>>>> heavyweight, you can write the data outside the transaction and the=
n
> >>>>> update
> >>>>> a pointer to the current data inside the transaction.
> >>>>>
> >>>>>
> >>>>> Again, spark has nothing much to do with guaranteeing exactly once.
> In
> >>>>> fact, the current streaming api actively impedes my ability to do t=
he
> >>>>> above.  I'm just suggesting providing an api that doesn't get in th=
e
> >>>>> way of
> >>>>> exactly-once.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
> >>>>> hshreedharan@cloudera.com
> >>>>> > wrote:
> >>>>>
> >>>>> > Can you explain your basic algorithm for the once-only-delivery?
> It is
> >>>>> > quite a bit of very Kafka-specific code, that would take more tim=
e
> to
> >>>>> read
> >>>>> > than I can currently afford? If you can explain your algorithm a
> bit,
> >>>>> it
> >>>>> > might help.
> >>>>> >
> >>>>> > Thanks,
> >>>>> > Hari
> >>>>> >
> >>>>> >
> >>>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <
> cody@koeninger.org>
> >>>>> > wrote:
> >>>>> >
> >>>>> >>
> >>>>> >> The problems you guys are discussing come from trying to store
> state
> >>>>> in
> >>>>> >> spark, so don't do that.  Spark isn't a distributed database.
> >>>>> >>
> >>>>> >> Just map kafka partitions directly to rdds, llet user code speci=
fy
> >>>>> the
> >>>>> >> range of offsets explicitly, and let them be in charge of
> committing
> >>>>> >> offsets.
> >>>>> >>
> >>>>> >> Using the simple consumer isn't that bad, I'm already using this
> in
> >>>>> >> production with the code I linked to, and tresata apparently has
> >>>>> been as
> >>>>> >> well.  Again, for everyone saying this is impossible, have you
> read
> >>>>> either
> >>>>> >> of those implementations and looked at the approach?
> >>>>> >>
> >>>>> >>
> >>>>> >>
> >>>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
> >>>>> >> Sean.McNamara@webtrends.com> wrote:
> >>>>> >>
> >>>>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I thin=
k the
> exactly
> >>>>> >>> once spark streaming semantics can easily be solved using
> >>>>> updateStateByKey.
> >>>>> >>> Make the key going into updateStateByKey be a hash of the event=
,
> or
> >>>>> pluck
> >>>>> >>> off some uuid from the message.  The updateFunc would only emit
> the
> >>>>> message
> >>>>> >>> if the key did not exist, and the user has complete control ove=
r
> >>>>> the window
> >>>>> >>> of time / state lifecycle for detecting duplicates.  It also
> makes
> >>>>> it
> >>>>> >>> really easy to detect and take action (alert?) when you DO see =
a
> >>>>> duplicate,
> >>>>> >>> or make memory tradeoffs within an error bound using a sketch
> >>>>> algorithm.
> >>>>> >>> The kafka simple consumer is insanely complex, if possible I
> think
> >>>>> it would
> >>>>> >>> be better (and vastly more flexible) to get reliability using t=
he
> >>>>> >>> primitives that spark so elegantly provides.
> >>>>> >>>
> >>>>> >>> Cheers,
> >>>>> >>>
> >>>>> >>> Sean
> >>>>> >>>
> >>>>> >>>
> >>>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
> >>>>> >>> hshreedharan@cloudera.com> wrote:
> >>>>> >>> >
> >>>>> >>> > Hi Dibyendu,
> >>>>> >>> >
> >>>>> >>> > Thanks for the details on the implementation. But I still do
> not
> >>>>> >>> believe
> >>>>> >>> > that it is no duplicates - what they achieve is that the same
> >>>>> batch is
> >>>>> >>> > processed exactly the same way every time (but see it may be
> >>>>> processed
> >>>>> >>> more
> >>>>> >>> > than once) - so it depends on the operation being idempotent.=
 I
> >>>>> believe
> >>>>> >>> > Trident uses ZK to keep track of the transactions - a batch
> can be
> >>>>> >>> > processed multiple times in failure scenarios (for example, t=
he
> >>>>> >>> transaction
> >>>>> >>> > is processed but before ZK is updated the machine fails,
> causing a
> >>>>> >>> "new"
> >>>>> >>> > node to process it again).
> >>>>> >>> >
> >>>>> >>> > I don't think it is impossible to do this in Spark Streaming =
as
> >>>>> well
> >>>>> >>> and
> >>>>> >>> > I'd be really interested in working on it at some point in th=
e
> >>>>> near
> >>>>> >>> future.
> >>>>> >>> >
> >>>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
> >>>>> >>> > dibyendu.bhattachary@gmail.com> wrote:
> >>>>> >>> >
> >>>>> >>> >> Hi,
> >>>>> >>> >>
> >>>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident.
> The
> >>>>> Storm
> >>>>> >>> >> Trident has done the exact-once guarantee by processing the
> >>>>> tuple in a
> >>>>> >>> >> batch  and assigning same transaction-id for a given batch .
> The
> >>>>> >>> replay for
> >>>>> >>> >> a given batch with a transaction-id will have exact same set
> of
> >>>>> >>> tuples and
> >>>>> >>> >> replay of batches happen in exact same order before the
> failure.
> >>>>> >>> >>
> >>>>> >>> >> Having this paradigm, if downstream system process data for =
a
> >>>>> given
> >>>>> >>> batch
> >>>>> >>> >> for having a given transaction-id , and if during failure if
> same
> >>>>> >>> batch is
> >>>>> >>> >> again emitted , you can check if same transaction-id is
> already
> >>>>> >>> processed
> >>>>> >>> >> or not and hence can guarantee exact once semantics.
> >>>>> >>> >>
> >>>>> >>> >> And this can only be achieved in Spark if we use Low Level
> Kafka
> >>>>> >>> consumer
> >>>>> >>> >> API to process the offsets. This low level Kafka Consumer (
> >>>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has
> >>>>> implemented the
> >>>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All o=
f
> the
> >>>>> >>> Kafka
> >>>>> >>> >> related logic has been taken from Storm-Kafka spout and whic=
h
> >>>>> manages
> >>>>> >>> all
> >>>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metada=
ta
> >>>>> >>> managements.
> >>>>> >>> >>
> >>>>> >>> >> Presently this Consumer maintains that during Receiver
> failure,
> >>>>> it
> >>>>> >>> will
> >>>>> >>> >> re-emit the exact same Block with same set of messages . Eve=
ry
> >>>>> >>> message have
> >>>>> >>> >> the details of its partition, offset and topic related detai=
ls
> >>>>> which
> >>>>> >>> can
> >>>>> >>> >> tackle the SPARK-3146.
> >>>>> >>> >>
> >>>>> >>> >> As this Low Level consumer has complete control over the Kaf=
ka
> >>>>> >>> Offsets ,
> >>>>> >>> >> we can implement Trident like feature on top of it like havi=
ng
> >>>>> >>> implement a
> >>>>> >>> >> transaction-id for a given block , and re-emit the same bloc=
k
> >>>>> with
> >>>>> >>> same set
> >>>>> >>> >> of message during Driver failure.
> >>>>> >>> >>
> >>>>> >>> >> Regards,
> >>>>> >>> >> Dibyendu
> >>>>> >>> >>
> >>>>> >>> >>
> >>>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
> >>>>> saisai.shao@intel.com>
> >>>>> >>> >> wrote:
> >>>>> >>> >>>
> >>>>> >>> >>> Hi all,
> >>>>> >>> >>>
> >>>>> >>> >>> I agree with Hari that Strong exact-once semantics is very
> hard
> >>>>> to
> >>>>> >>> >>> guarantee, especially in the failure situation. From my
> >>>>> >>> understanding even
> >>>>> >>> >>> current implementation of ReliableKafkaReceiver cannot full=
y
> >>>>> >>> guarantee the
> >>>>> >>> >>> exact once semantics once failed, first is the ordering of
> data
> >>>>> >>> replaying
> >>>>> >>> >>> from last checkpoint, this is hard to guarantee when multip=
le
> >>>>> >>> partitions
> >>>>> >>> >>> are injected in; second is the design complexity of achievi=
ng
> >>>>> this,
> >>>>> >>> you can
> >>>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into th=
e
> >>>>> very
> >>>>> >>> details
> >>>>> >>> >>> of Kafka metadata management system to achieve this, not to
> say
> >>>>> >>> rebalance
> >>>>> >>> >>> and fault-tolerance.
> >>>>> >>> >>>
> >>>>> >>> >>> Thanks
> >>>>> >>> >>> Jerry
> >>>>> >>> >>>
> >>>>> >>> >>> -----Original Message-----
> >>>>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:
> >>>>> langel.groups@gmail.com]
> >>>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
> >>>>> >>> >>> To: Cody Koeninger
> >>>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
> >>>>> >>> >>> Subject: Re: Which committers care about Kafka?
> >>>>> >>> >>>
> >>>>> >>> >>> But idempotency is not that easy t achieve sometimes. A
> strong
> >>>>> only
> >>>>> >>> once
> >>>>> >>> >>> semantic through a proper API would  be superuseful; but I'=
m
> not
> >>>>> >>> implying
> >>>>> >>> >>> this is easy to achieve.
> >>>>> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org>
> >>>>> wrote:
> >>>>> >>> >>>
> >>>>> >>> >>>> If the downstream store for the output data is idempotent =
or
> >>>>> >>> >>>> transactional, and that downstream store also is the syste=
m
> of
> >>>>> >>> record
> >>>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.
> >>>>> Commit
> >>>>> >>> >>>> offsets with / after the data is stored.  On any failure,
> >>>>> restart
> >>>>> >>> from
> >>>>> >>> >>> the last committed offsets.
> >>>>> >>> >>>>
> >>>>> >>> >>>> Yes, this approach is biased towards the etl-like use case=
s
> >>>>> rather
> >>>>> >>> >>>> than near-realtime-analytics use cases.
> >>>>> >>> >>>>
> >>>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> >>>>> >>> >>>> hshreedharan@cloudera.com
> >>>>> >>> >>>>> wrote:
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> I get what you are saying. But getting exactly once right
> is
> >>>>> an
> >>>>> >>> >>>>> extremely hard problem - especially in presence of failur=
e.
> >>>>> The
> >>>>> >>> >>>>> issue is failures
> >>>>> >>> >>>> can
> >>>>> >>> >>>>> happen in a bunch of places. For example, before the
> >>>>> notification
> >>>>> >>> of
> >>>>> >>> >>>>> downstream store being successful reaches the receiver th=
at
> >>>>> updates
> >>>>> >>> >>>>> the offsets, the node fails. The store was successful, bu=
t
> >>>>> >>> >>>>> duplicates came in either way. This is something worth
> >>>>> discussing
> >>>>> >>> by
> >>>>> >>> >>>>> itself - but without uuids etc this might not really be
> >>>>> solved even
> >>>>> >>> >>> when you think it is.
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in
> all
> >>>>> of
> >>>>> >>> the
> >>>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and
> once-only
> >>>>> >>> >>>>> delivery,
> >>>>> >>> >>>> but
> >>>>> >>> >>>>> I doubt the latter is really possible to guarantee -
> though I
> >>>>> >>> really
> >>>>> >>> >>>> would
> >>>>> >>> >>>>> love to have that!
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> Thanks,
> >>>>> >>> >>>>> Hari
> >>>>> >>> >>>>>
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
> >>>>> >>> >>>>> <cody@koeninger.org>
> >>>>> >>> >>>>> wrote:
> >>>>> >>> >>>>>
> >>>>> >>> >>>>>> Thanks for the replies.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization=
.
> >>>>> If you
> >>>>> >>> >>>>>> actually want exactly-once semantics, you need control o=
f
> >>>>> kafka
> >>>>> >>> >>>>>> offsets
> >>>>> >>> >>>> as
> >>>>> >>> >>>>>> well, including the ability to not use zookeeper as the
> >>>>> system of
> >>>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system
> that
> >>>>> has
> >>>>> >>> >>>>>> strong
> >>>>> >>> >>>> ordering
> >>>>> >>> >>>>>> guarantees (within a partition) and does not mandate the
> use
> >>>>> of
> >>>>> >>> >>>> zookeeper
> >>>>> >>> >>>>>> to store offsets.  I think there should be a spark api
> that
> >>>>> acts
> >>>>> >>> as
> >>>>> >>> >>>>>> a
> >>>>> >>> >>>> very
> >>>>> >>> >>>>>> simple intermediary between Kafka and the user's choice =
of
> >>>>> >>> >>>>>> downstream
> >>>>> >>> >>>> store.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> Take a look at the links I posted - if there's already
> been 2
> >>>>> >>> >>>> independent
> >>>>> >>> >>>>>> implementations of the idea, chances are it's something
> >>>>> people
> >>>>> >>> need.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> >>>>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> Hi Cody,
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can
> implement
> >>>>> >>> >>>>>>> something pretty simple and lightweight for that one.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation -
> >>>>> this is
> >>>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though i=
t
> is
> >>>>> a
> >>>>> >>> good
> >>>>> >>> >>>> idea to
> >>>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is =
an
> >>>>> >>> >>>> optimization. For
> >>>>> >>> >>>>>>> that reason, we must be careful in implementation. Ther=
e
> >>>>> are a
> >>>>> >>> >>>>>>> couple
> >>>>> >>> >>>> of
> >>>>> >>> >>>>>>> issues that we need to ensure works properly -
> specifically
> >>>>> >>> >>> ordering.
> >>>>> >>> >>>> To
> >>>>> >>> >>>>>>> ensure we pull messages from different topics and
> >>>>> partitions in
> >>>>> >>> >>>>>>> the
> >>>>> >>> >>>> same
> >>>>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist=
 the
> >>>>> metadata to
> >>>>> >>> >>>>>>> HDFS
> >>>>> >>> >>>> (or
> >>>>> >>> >>>>>>> some other system) - this metadata must contain the
> order of
> >>>>> >>> >>>>>>> messages consumed, so we know how to re-read the
> messages.
> >>>>> I am
> >>>>> >>> >>>>>>> planning to
> >>>>> >>> >>>> explore
> >>>>> >>> >>>>>>> this once I have some time (probably in Jan). In
> addition,
> >>>>> we
> >>>>> >>> must
> >>>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I wi=
ll
> >>>>> file a
> >>>>> >>> >>>>>>> placeholder jira for this one.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while
> >>>>> back -
> >>>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping
> >>>>> that
> >>>>> >>> this
> >>>>> >>> >>>>>>> will get pulled in soon, as this is something I know
> people
> >>>>> want.
> >>>>> >>> >>>>>>> I am open
> >>>>> >>> >>>> to
> >>>>> >>> >>>>>>> feedback on that - anything that I can do to make it
> better.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> Thanks,
> >>>>> >>> >>>>>>> Hari
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
> >>>>> >>> >>>>>>> <pwendell@gmail.com>
> >>>>> >>> >>>>>>> wrote:
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>>> Hey Cody,
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streami=
ng
> >>>>> is TD -
> >>>>> >>> >>>>>>>> he is traveling this week though so I can respond a bi=
t.
> >>>>> To the
> >>>>> >>> >>>>>>>> high level point of whether Kafka is important - it
> >>>>> definitely
> >>>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
> >>>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good suppo=
rt
> >>>>> for
> >>>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not =
a
> >>>>> library.
> >>>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
> >>>>> unstable
> >>>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
> >>>>> stabilize
> >>>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn'=
t
> be
> >>>>> able
> >>>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level
> >>>>> impression
> >>>>> >>> only
> >>>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so
> it's
> >>>>> worth
> >>>>> >>> >>> revisiting given the developments in Kafka.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> Please do bring things up like this on the dev list if
> >>>>> there are
> >>>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> - Patrick
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
> >>>>> >>> >>>>>>>> <cody@koeninger.org>
> >>>>> >>> >>>>>>>> wrote:
> >>>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people
> to
> >>>>> get
> >>>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexibl=
e
> >>>>> for our
> >>>>> >>> >>>>>>>> production
> >>>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint,
> because
> >>>>> I've
> >>>>> >>> >>>>>>>>> seen several different patches and libraries to fix t=
he
> >>>>> same
> >>>>> >>> >>>>>>>>> things we've
> >>>>> >>> >>>>>>>> been
> >>>>> >>> >>>>>>>>> running into.
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Regarding flexibility
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an
> equivalent
> >>>>> of
> >>>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patc=
h
> >>>>> >>> >>>>>>>>> ourselves, then found
> >>>>> >>> >>>>>>>> that
> >>>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn'=
t
> be
> >>>>> able
> >>>>> >>> >>>>>>>>> to
> >>>>> >>> >>>> get
> >>>>> >>> >>>>>>>> our
> >>>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a
> >>>>> whole
> >>>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388,
> >>>>> arbitrary
> >>>>> >>> >>>>>>>>> delay of
> >>>>> >>> >>>>>>>> messages, etc).
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
> >>>>> >>> >>>>>>>>> WriteAheadLog
> >>>>> >>> >>>> as
> >>>>> >>> >>>>>>>> a
> >>>>> >>> >>>>>>>>> general solution for streaming unreliable sources, bu=
t
> >>>>> Kafka
> >>>>> >>> >>>>>>>>> already
> >>>>> >>> >>>>>>>> is a
> >>>>> >>> >>>>>>>>> reliable source. I think there's a need for an api th=
at
> >>>>> treats
> >>>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
> >>>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
> >>>>> >>> >>>>>>>>> write-ahead log in hdfs, I
> >>>>> >>> >>>> need
> >>>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've h=
ad
> >>>>> >>> >>>>>>>>> failures
> >>>>> >>> >>>> that
> >>>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
> >>>>> >>> instance).
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> I've got an implementation i've been using
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>>
> >>>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> >>>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Tresata has something similar at
> >>>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
> >>>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm
> >>>>> code.
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as librarie=
s
> >>>>> rather
> >>>>> >>> >>>>>>>>> than
> >>>>> >>> >>>>>>>> patches
> >>>>> >>> >>>>>>>>> to Spark is problematic, because large portions of th=
e
> >>>>> >>> >>>> implementation
> >>>>> >>> >>>>>>>> are
> >>>>> >>> >>>>>>>>> private[spark].
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention =
to
> >>>>> get.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>>
> >>>>> >>> ---------------------------------------------------------------=
--
> >>>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
> >>>>> dev-unsubscribe@spark.apache.org
> >>>>> >>> For
> >>>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>
> >>>>> >>> >>>>
> >>>>> >>> >>>
> >>>>> >>> >>
> >>>>> >>>
> >>>>> >>>
> >>>>> >>
> >>>>> >
> >>>>>
> >>>>
> >>>
> >>
>

--001a11c24f28fb39de050b62af1e--

From dev-return-10962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 23:08:52 2014
Return-Path: <dev-return-10962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74E19103DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 23:08:52 +0000 (UTC)
Received: (qmail 76256 invoked by uid 500); 29 Dec 2014 23:08:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76174 invoked by uid 500); 29 Dec 2014 23:08:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76153 invoked by uid 99); 29 Dec 2014 23:08:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 23:08:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.179] (HELO mail-lb0-f179.google.com) (209.85.217.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 23:08:45 +0000
Received: by mail-lb0-f179.google.com with SMTP id z11so11408102lbi.24
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 15:08:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=rERWd1l0oP0C9IH8o8KjXyg4w8AMF+4IIC9ipUIHKc4=;
        b=A43l2SbJKrdvL+QkksvAShxwsdadyTKWuTK03FU7O4luhKuiEqg9xsqznwvZRBslqp
         KAq2xUxazCnWNy43EwU0ViQg8P57ARHyqgzYvhZmKgI5W2Q58A009SO9sebMlxg1MhyC
         zte74gcWyOI6c4lqzjZvmfIGZUBKiIF/w3yZmqHCG01dxSgs3zTDuBikSZVSUK7Ozhhx
         pOHYfyha2HA+k9yXjQ5JJcXR7xPs8xvshg8f6uqWup9058PGb1fbDPlQ+kqvPHiSFSN+
         gSsXo8qPFfgPMdY2UYfAn1xAguBgjQyj3VcAniyvcQmtVPwNzXqbvAqY15d9JhBHuxL3
         8Pqg==
X-Gm-Message-State: ALoCoQm8OXQfJPJZcETBI6lvaPtt+rz8HK9ttcZNiwSSoIy/W/xI56/IzzsXFb2mkMBvpL/mU/nC
X-Received: by 10.112.55.101 with SMTP id r5mr41631698lbp.98.1419894483267;
 Mon, 29 Dec 2014 15:08:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 29 Dec 2014 15:07:42 -0800 (PST)
In-Reply-To: <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com> <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 29 Dec 2014 15:07:42 -0800
Message-ID: <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "Wang, Daoyuan" <daoyuan.wang@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133d0b84b4b52050b62f209
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133d0b84b4b52050b62f209
Content-Type: text/plain; charset=UTF-8

I'd love to get both of these in.  There is some trickiness that I talk
about on the JIRA for timestamps since the SQL timestamp class can support
nano seconds and I don't think parquet has a type for this.  Other systems
(impala) seem to use INT96.  It would be great to maybe ask on the parquet
mailing list what the plan is there to make sure that whatever we do is
going to be compatible long term.

Michael

On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Daoyuan,
>
> Thanks for creating the jiras. I need these features by... last week, so
> I'd be happy to take care of this myself, if only you or someone more
> experienced than me in the SparkSQL codebase could provide some guidance.
>
> Alex
> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:
>
>> Hi Alex,
>>
>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>> support decimals that fits in a long.
>>
>> Thanks,
>> Daoyuan
>>
>> -----Original Message-----
>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>> Sent: Saturday, December 27, 2014 2:47 PM
>> To: dev@spark.apache.org; Michael Armbrust
>> Subject: Unsupported Catalyst types in Parquet
>>
>> Michael,
>>
>> I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
>> due to my RDDs having having DateType and DecimalType fields. What would it
>> take to add Parquet support for these Catalyst? Are there any other
>> Catalyst types for which there is no Catalyst support?
>>
>> Alex
>>
>

--001a1133d0b84b4b52050b62f209--

From dev-return-10963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Dec 29 23:19:01 2014
Return-Path: <dev-return-10963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2599C10420
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 29 Dec 2014 23:19:01 +0000 (UTC)
Received: (qmail 93713 invoked by uid 500); 29 Dec 2014 23:19:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93631 invoked by uid 500); 29 Dec 2014 23:19:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93618 invoked by uid 99); 29 Dec 2014 23:18:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 23:18:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 29 Dec 2014 23:18:54 +0000
Received: by mail-qc0-f169.google.com with SMTP id w7so10031636qcr.28
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 15:16:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=OFgLmeBm9oVlWTUo6Oe5HZMT2vNzbd0Yq/x+y2OVBLg=;
        b=ADbKYccigmME3Sy33W4qSV8v608/0V0025wsG/AS8vPOdSihK8wzsh11CDYs2rJmTl
         us3LPg8V2b0UuKk/tUtxC42PXZYEWTau83PMKdyzRqv8drOvleRNAD4t23HlVlhio3IM
         8tNiAOLGbxJQI5UjEEf3U2RGb7ffsbx9kjhQy297Y7hv6dcu2zQQJ4sSas2t+2UJeXbl
         vYQOnH609L6x4Dfjqhfho6Wg3IcnK0sRUaPg2OWpLJXH39NAVvQjWKMpJZjnTbSvnCxn
         FwoEVImkX2Gx0JyUwblOeoc+AHYPHLXpyDTIzQaGHM3/1gq7k2WdiL7bX4xXOfiPPheJ
         sz/Q==
X-Gm-Message-State: ALoCoQneqMWmpsyOzvUT1SfEtp6JmAKtS6LJ0zu11HEm3xCir5FKdbKSTtIAFHRycBIyX6qLRZri
MIME-Version: 1.0
X-Received: by 10.224.61.145 with SMTP id t17mr23222115qah.49.1419895003597;
 Mon, 29 Dec 2014 15:16:43 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Mon, 29 Dec 2014 15:16:43 -0800 (PST)
In-Reply-To: <CAMAsSdKs3inqAU-+yBH98XHuk8TLYn2sm5qeo+b7AvKec7=NeA@mail.gmail.com>
References: <CAPEc=JujFQAmwoxDWj83DghtvYcvF=TWYxsJu4SHsQOH9zSkxA@mail.gmail.com>
	<CAMAsSdKs3inqAU-+yBH98XHuk8TLYn2sm5qeo+b7AvKec7=NeA@mail.gmail.com>
Date: Mon, 29 Dec 2014 18:16:43 -0500
Message-ID: <CAPEc=JvnFr18aebk16G0y0geMddAzmJkc3uZjNhUoVn9uDzDow@mail.gmail.com>
Subject: Re: Spark 1.2.0 build error
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc8d664eda5f050b6311d6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc8d664eda5f050b6311d6
Content-Type: text/plain; charset=UTF-8

I am getting "The command is too long" error.

Is there anything which needs to be done.
However for the time being I followed the "sbt" way of buidling spark in
IntelliJ.

On Mon, Dec 29, 2014 at 3:52 AM, Sean Owen <sowen@cloudera.com> wrote:

> It means a test failed but you have not shown the test failure. This would
> have been logged earlier. You would need to say how you ran tests too. The
> tests for 1.2.0 pass for me on several common permutations.
> On Dec 29, 2014 3:22 AM, "Naveen Madhire" <vmadhire@umail.iu.edu> wrote:
>
>> Hi,
>>
>> I am follow the below link for building Spark 1.2.0
>>
>> https://spark.apache.org/docs/1.2.0/building-spark.html
>>
>> I am getting the below error during the Maven build. I am using IntelliJ
>> IDE.
>>
>> The build is failing in the scalatest plugin,
>>
>> [INFO] Reactor Summary:
>> [INFO]
>> [INFO] Spark Project Parent POM .......................... SUCCESS
>> [3.355s]
>> [INFO] Spark Project Networking .......................... SUCCESS
>> [4.017s]
>> [INFO] Spark Project Shuffle Streaming Service ........... SUCCESS
>> [2.914s]
>> [INFO] Spark Project Core ................................ FAILURE
>> [9.678s]
>> [INFO] Spark Project Bagel ............................... SKIPPED
>> [INFO] Spark Project GraphX .............................. SKIPPED
>>
>>
>>
>> [ERROR] Failed to execute goal
>> org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
>> spark-core_2.10: There are test failures -> [Help 1]
>> org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
>> goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
>> spark-core_2.10: There are test failures
>> at
>>
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
>> at
>>
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
>> at
>>
>> org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
>> at
>>
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
>> at
>>
>> org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
>> at
>>
>> org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
>> at
>>
>> org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
>> at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
>> at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
>> at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
>> at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
>>
>>
>> Is there something which I am missing during the build process. Please
>> suggest.
>>
>> Using
>> IntelliJ 13.1.6
>> Maven 3.1.1
>> Scala 2.10.4
>> Spark 1.2.0
>>
>>
>> Thanks
>> Naveen
>>
>

--047d7bdc8d664eda5f050b6311d6--

From dev-return-10964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 00:11:18 2014
Return-Path: <dev-return-10964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 598DE10624
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 00:11:18 +0000 (UTC)
Received: (qmail 10682 invoked by uid 500); 30 Dec 2014 00:11:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10607 invoked by uid 500); 30 Dec 2014 00:11:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10596 invoked by uid 99); 30 Dec 2014 00:11:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 00:11:16 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alee526@hotmail.com designates 65.55.111.85 as permitted sender)
Received: from [65.55.111.85] (HELO BLU004-OMC2S10.hotmail.com) (65.55.111.85)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 00:11:11 +0000
Received: from BLU184-W22 ([65.55.111.72]) by BLU004-OMC2S10.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Mon, 29 Dec 2014 16:09:03 -0800
X-TMN: [akp0cQZ1bgQEXJ33eatRIlkWYMaLCi0x]
X-Originating-Email: [alee526@hotmail.com]
Message-ID: <BLU184-W227AD3014FCAC41D177389F35E0@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_cd793dd0-b980-4d98-9ae6-34bcee33e597_"
From: Andrew Lee <alee526@hotmail.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
Date: Mon, 29 Dec 2014 16:09:03 -0800
Importance: Normal
In-Reply-To:
 <CABPQxsuRZqFtd4DPM1emG_aD0C27DChWQsJA6wYpF6n3RXLdyQ@mail.gmail.com>
References:
 <BLU184-W301F9045B30DAF51D19204F3620@phx.gbl>,<BLU184-W76EEA650AC39A61793316EF3620@phx.gbl>,<CABPQxsuRZqFtd4DPM1emG_aD0C27DChWQsJA6wYpF6n3RXLdyQ@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 30 Dec 2014 00:09:03.0691 (UTC) FILETIME=[D1F6A9B0:01D023C4]
X-Virus-Checked: Checked by ClamAV on apache.org

--_cd793dd0-b980-4d98-9ae6-34bcee33e597_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Hi Patrick=2C
I manually hardcoded the hive version to 0.13.1a and it works. It turns out=
 that for some reason=2C 0.13.1 is being picked up instead of the 0.13.1a v=
ersion from maven.
So my solution was:hardcode the hive.version to 0.13.1a in my case since I =
am building it against hive 0.13 only=2C so the pom.xml was hardcoded with =
that version string=2C and the final JAR is working now with hive-exec 0.13=
.1a embed.
Possible Reason why it didn't work?I suspect our internal environment is pi=
cking up 0.13.1 since we do use our own maven repo as a proxy and caching. =
 0.13.1a did appear in our own repo and it got replicated from the maven ce=
ntral repo=2C but during the build process=2C maven picked up 0.13.1 instea=
d of 0.13.1a.

> Date: Wed=2C 10 Dec 2014 12:23:08 -0800
> Subject: Re: Build Spark 1.2.0-rc1 encounter exceptions when running Hive=
Context - Caused by: java.lang.ClassNotFoundException: com.esotericsoftware=
.shaded.org.objenesis.strategy.InstantiatorStrategy
> From: pwendell@gmail.com
> To: alee526@hotmail.com
> CC: dev@spark.apache.org
>=20
> Hi Andrew=2C
>=20
> It looks like somehow you are including jars from the upstream Apache
> Hive 0.13 project on your classpath. For Spark 1.2 Hive 0.13 support=2C
> we had to modify Hive to use a different version of Kryo that was
> compatible with Spark's Kryo version.
>=20
> https://github.com/pwendell/hive/commit/5b582f242946312e353cfce92fc3f3fa4=
72aedf3
>=20
> I would look through the actual classpath and make sure you aren't
> including your own hive-exec jar somehow.
>=20
> - Patrick
>=20
> On Wed=2C Dec 10=2C 2014 at 9:48 AM=2C Andrew Lee <alee526@hotmail.com> w=
rote:
> > Apologize for the format=2C somehow it got messed up and linefeed were =
removed. Here's a reformatted version.
> > Hi All=2C
> > I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.=
sh to include auxiliaries JARs and datanucleus*.jars from Hive=2C however=
=2C when I run HiveContext=2C it gives me the following error:
> >
> > Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shade=
d.org.objenesis.strategy.InstantiatorStrategy
> >
> > I have checked the JARs with (jar tf)=2C looks like this is already inc=
luded (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) w=
hich is configured in the System classpath already. I couldn't figure out w=
hat is going on with the shading on the esotericsoftware JARs here.  Any he=
lp is appreciated.
> >
> >
> > How to reproduce the problem?
> > Run the following 3 statements in spark-shell ( This is how I launched =
my spark-shell. cd /opt/spark=3B ./bin/spark-shell --master yarn --deploy-m=
ode client --queue research --driver-memory 1024M)
> >
> > import org.apache.spark.SparkContext
> > val hiveContext =3D new org.apache.spark.sql.hive.HiveContext(sc)
> > hiveContext.hql("CREATE TABLE IF NOT EXISTS spark_hive_test_table (key =
INT=2C value STRING)")
> >
> >
> >
> > A reference of my environment.
> > Apache Hadoop 2.4.1
> > Apache Hive 0.13.1
> > Apache Spark branch-1.2 (installed under /opt/spark/=2C and config unde=
r /etc/spark/)
> > Maven build command:
> >
> > mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=3D2=
.4.1 -Dyarn.version=3D2.4.1 -Dhive.version=3D0.13.1 -DskipTests install
> >
> > Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82
> >
> > My spark-env.sh have the following contents when I executed spark-shell=
:
> >> HADOOP_HOME=3D/opt/hadoop/
> >> HIVE_HOME=3D/opt/hive/
> >> HADOOP_CONF_DIR=3D/etc/hadoop/
> >> YARN_CONF_DIR=3D/etc/hadoop/
> >> HIVE_CONF_DIR=3D/etc/hive/
> >> HADOOP_SNAPPY_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type=
 f -name "snappy-java-*.jar")
> >> HADOOP_LZO_JAR=3D$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f =
-name "hadoop-lzo-*.jar")
> >> SPARK_YARN_DIST_FILES=3D/user/spark/libs/spark-assembly-1.2.0-hadoop2.=
4.1.jar
> >> export JAVA_LIBRARY_PATH=3D$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native
> >> export LD_LIBRARY_PATH=3D$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native
> >> export SPARK_LIBRARY_PATH=3D$SPARK_LIBRARY_PATH:$HADOOP_HOME/lib/nativ=
e
> >> export SPARK_CLASSPATH=3D$SPARK_CLASSPATH:$HADOOP_SNAPPY_JAR:$HADOOP_L=
ZO_JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive=
/lib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
> >
> >
> >> Here's what I see from my stack trace.
> >> warning: there were 1 deprecation warning(s)=3B re-run with -deprecati=
on for details
> >> Hive history file=3D/home/hive/log/alti-test-01/hive_job_log_b5db9539-=
4736-44b3-a601-04fa77cb6730_1220828461.txt
> >> java.lang.NoClassDefFoundError: com/esotericsoftware/shaded/org/objene=
sis/strategy/InstantiatorStrategy
> >>       at org.apache.hadoop.hive.ql.exec.Utilities.<clinit>(Utilities.j=
ava:925)
> >>       at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(Sem=
anticAnalyzer.java:9718)
> >>       at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(Sem=
anticAnalyzer.java:9712)
> >>       at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
> >>       at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
> >>       at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:=
975)
> >>       at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040=
)
> >>       at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
> >>       at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
> >>       at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.sca=
la:305)
> >>       at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.=
scala:276)
> >>       at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectR=
esult$lzycompute(NativeCommand.scala:35)
> >>       at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectR=
esult(NativeCommand.scala:35)
> >>       at org.apache.spark.sql.execution.Command$class.execute(commands=
.scala:46)
> >>       at org.apache.spark.sql.hive.execution.NativeCommand.execute(Nat=
iveCommand.scala:30)
> >>       at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompu=
te(SQLContext.scala:425)
> >>       at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLConte=
xt.scala:425)
> >>       at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike=
.scala:58)
> >>       at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
> >>       at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scal=
a:102)
> >>       at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:1=
06)
> >>       at $iwC$$iwC$$iwC$$iwC.<init>(<console>:16)
> >>       at $iwC$$iwC$$iwC.<init>(<console>:21)
> >>       at $iwC$$iwC.<init>(<console>:23)
> >>       at $iwC.<init>(<console>:25)
> >>       at <init>(<console>:27)
> >>       at .<init>(<console>:31)
> >>       at .<clinit>(<console>)
> >>       at .<init>(<console>:7)
> >>       at .<clinit>(<console>)
> >>       at $print(<console>)
> >>       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >>       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAcces=
sorImpl.java:57)
> >>       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMet=
hodAccessorImpl.java:43)
> >>       at java.lang.reflect.Method.invoke(Method.java:606)
> >>       at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMai=
n.scala:852)
> >>       at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMai=
n.scala:1125)
> >>       at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.s=
cala:674)
> >>       at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:7=
05)
> >>       at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:6=
69)
> >>       at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop=
.scala:828)
> >>       at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkI=
Loop.scala:873)
> >>       at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785=
)
> >>       at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.sca=
la:628)
> >>       at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala=
:636)
> >>       at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)
> >>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ=
$sp(SparkILoop.scala:968)
> >>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(Spa=
rkILoop.scala:916)
> >>       at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(Spa=
rkILoop.scala:916)
> >>       at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(Sc=
alaClassLoader.scala:135)
> >>       at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916=
)
> >>       at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:101=
1)
> >>       at org.apache.spark.repl.Main$.main(Main.scala:31)
> >>       at org.apache.spark.repl.Main.main(Main.scala)
> >>       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >>       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAcces=
sorImpl.java:57)
> >>       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMet=
hodAccessorImpl.java:43)
> >>       at java.lang.reflect.Method.invoke(Method.java:606)
> >>       at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala=
:353)
> >>       at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:7=
5)
> >>       at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> >> Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shad=
ed.org.objenesis.strategy.InstantiatorStrategy
> >>       at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> >>       at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> >>       at java.security.AccessController.doPrivileged(Native Method)
> >>       at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> >>       at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> >>       at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
> >>       at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> >>       ... 61 more
> >>
> >>
> >
>=20
> ---------------------------------------------------------------------
> To unsubscribe=2C e-mail: dev-unsubscribe@spark.apache.org
> For additional commands=2C e-mail: dev-help@spark.apache.org
>=20
 		 	   		  =

--_cd793dd0-b980-4d98-9ae6-34bcee33e597_--

From dev-return-10965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 00:56:06 2014
Return-Path: <dev-return-10965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 37121107B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 00:56:06 +0000 (UTC)
Received: (qmail 94584 invoked by uid 500); 30 Dec 2014 00:56:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94521 invoked by uid 500); 30 Dec 2014 00:56:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94509 invoked by uid 99); 30 Dec 2014 00:56:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 00:56:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 00:55:57 +0000
Received: by mail-ie0-f169.google.com with SMTP id y20so13153127ier.28
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 16:55:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=OiZpnzsn6d6tCBv+Lex2KmKszTfkSJGabalfpQsPMm4=;
        b=hw5uw2unaZKfFg1GhxWTY3CKIA0Bt9myFr5oT44eivvzoR2Eaua8hjvLbhzsDs5J9p
         PUNOBEDcpfm9b6p4xXr0g4RCvH8LUJ+fPcsS6m5+c2lYfOSZmNLnsbfRsinja2SyZd/i
         AREeA5DgNFKmfe3xO/Qd5KwZMAspjqEyRcOjkQrpl9zQ+uDCF5qrLyz1MWpYJwVwOGv2
         IwAxDvPJL4kNOHKGvemBD7uwENRyE++mJMT8PpkoXc27MM4En9WvCrYUrnB5YVXJd+YI
         mdKdXLq5SV/Pvw7vy9mnnBkVhMLL3hkZOKx87pfgXXMVRgH8u8WVrSKC3K5t+JKLmqq4
         SXSw==
MIME-Version: 1.0
X-Received: by 10.50.143.101 with SMTP id sd5mr48642263igb.40.1419900936797;
 Mon, 29 Dec 2014 16:55:36 -0800 (PST)
Received: by 10.107.132.83 with HTTP; Mon, 29 Dec 2014 16:55:36 -0800 (PST)
Date: Mon, 29 Dec 2014 16:55:36 -0800
Message-ID: <CACkSZy3=MTUe1g-ND343aiNJh0NH0Tt8+GXt8RQ4rDroBpiR4w@mail.gmail.com>
Subject: Adding third party jars to classpath used by pyspark
From: Stephen Boesch <javadba@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134bad0f45b40050b64721a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134bad0f45b40050b64721a
Content-Type: text/plain; charset=UTF-8

What is the recommended way to do this?  We have some native database
client libraries for which we are adding pyspark bindings.

The pyspark invokes spark-submit.   Do we add our libraries to
the SPARK_SUBMIT_LIBRARY_PATH ?

This issue relates back to an error we have been seeing "Py4jError: Trying
to call a package" - the suspicion being that the third party libraries may
not be available on the jvm side.

--001a1134bad0f45b40050b64721a--

From dev-return-10966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 01:16:30 2014
Return-Path: <dev-return-10966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2482C108CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 01:16:30 +0000 (UTC)
Received: (qmail 34245 invoked by uid 500); 30 Dec 2014 01:16:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34169 invoked by uid 500); 30 Dec 2014 01:16:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34157 invoked by uid 99); 30 Dec 2014 01:16:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 01:16:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 01:16:24 +0000
Received: by mail-oi0-f41.google.com with SMTP id i138so26448985oig.0
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 17:15:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=B1CBV2fVM16tQQv5QNJjkbkY8+wbsSgNjtEmt/vcXJA=;
        b=VG4X/rRBvNR79muCfvvkAkk3OhCp0xmdiwbISi4UWbT85RU/LhcHrFql76NfgzV6Ki
         caZIgiOd+phl3q4ha3fH5QkxjzSYj+k8gcEbOXehyjWd9jPkDsAEKtx/ihMtMUKVKRa8
         omN7D/sPFa2PJk0DkE1b0EEpBN+Xy5qQXO4ZSAgMf5e3JrxD82DMmDuuZHQclOWwxX//
         7W7jUv4qFG2lDQtYvNBSQ+JkL+CR3e5ZETZ8WT9eRiaeEld0ULgXo85Wzn8nNvocXymU
         9pPxG8OiPzFzHE1p4vdFI2RCOL6/w8PvDsMAGR9Ji46d5u+dBIS02JjQHsJHQhtiFEmt
         9SIw==
X-Received: by 10.60.144.194 with SMTP id so2mr31413572oeb.65.1419902119038;
 Mon, 29 Dec 2014 17:15:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.95.195 with HTTP; Mon, 29 Dec 2014 17:14:58 -0800 (PST)
In-Reply-To: <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com> <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Mon, 29 Dec 2014 17:14:58 -0800
Message-ID: <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: Michael Armbrust <michael@databricks.com>
Cc: "Wang, Daoyuan" <daoyuan.wang@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a94fe6bd6de050b64b90c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a94fe6bd6de050b64b90c
Content-Type: text/plain; charset=UTF-8

Michael,

Actually, Adrian Wang already created pull requests for these issues.

https://github.com/apache/spark/pull/3820
https://github.com/apache/spark/pull/3822

What do you think?

Alex

On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <michael@databricks.com>
wrote:

> I'd love to get both of these in.  There is some trickiness that I talk
> about on the JIRA for timestamps since the SQL timestamp class can support
> nano seconds and I don't think parquet has a type for this.  Other systems
> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
> mailing list what the plan is there to make sure that whatever we do is
> going to be compatible long term.
>
> Michael
>
> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Daoyuan,
>>
>> Thanks for creating the jiras. I need these features by... last week, so
>> I'd be happy to take care of this myself, if only you or someone more
>> experienced than me in the SparkSQL codebase could provide some guidance.
>>
>> Alex
>> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:
>>
>>> Hi Alex,
>>>
>>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>>> support decimals that fits in a long.
>>>
>>> Thanks,
>>> Daoyuan
>>>
>>> -----Original Message-----
>>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>>> Sent: Saturday, December 27, 2014 2:47 PM
>>> To: dev@spark.apache.org; Michael Armbrust
>>> Subject: Unsupported Catalyst types in Parquet
>>>
>>> Michael,
>>>
>>> I'm having trouble storing my SchemaRDDs in Parquet format with
>>> SparkSQL, due to my RDDs having having DateType and DecimalType fields.
>>> What would it take to add Parquet support for these Catalyst? Are there any
>>> other Catalyst types for which there is no Catalyst support?
>>>
>>> Alex
>>>
>>
>

--047d7b3a94fe6bd6de050b64b90c--

From dev-return-10967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 01:52:54 2014
Return-Path: <dev-return-10967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4545F10996
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 01:52:54 +0000 (UTC)
Received: (qmail 77254 invoked by uid 500); 30 Dec 2014 01:52:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77196 invoked by uid 500); 30 Dec 2014 01:52:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77185 invoked by uid 99); 30 Dec 2014 01:52:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 01:52:52 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saisai.shao@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 01:52:22 +0000
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
  by orsmga101.jf.intel.com with ESMTP; 29 Dec 2014 17:50:58 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,862,1389772800"; 
   d="scan'208,217";a="434798324"
Received: from pgsmsx108.gar.corp.intel.com ([10.221.44.103])
  by FMSMGA003.fm.intel.com with ESMTP; 29 Dec 2014 17:38:59 -0800
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 PGSMSX108.gar.corp.intel.com (10.221.44.103) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Tue, 30 Dec 2014 09:50:53 +0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.182]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.216]) with mapi id 14.03.0195.001;
 Tue, 30 Dec 2014 09:50:51 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: Cody Koeninger <cody@koeninger.org>, Tathagata Das
	<tathagata.das1565@gmail.com>
CC: Hari Shreedharan <hshreedharan@cloudera.com>, Sean McNamara
	<sean.mcnamara@webtrends.com>, Patrick Wendell <pwendell@gmail.com>,
	=?utf-8?B?THVpcyDDgW5nZWwgVmljZW50ZSBTw6FuY2hleg==?=
	<langel.groups@gmail.com>, Dibyendu Bhattacharya
	<dibyendu.bhattachary@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>, Koert Kuipers <koert@tresata.com>
Subject: RE: Which committers care about Kafka?
Thread-Topic: Which committers care about Kafka?
Thread-Index: AQHQGtXAzrWADJNrIEmzg2LIwJg5n5yVMYEAgAAIvoCAAAuuAIAAEQIAgAAG9QCAAAFPgIAAx1Sw///+lQCAAJzNgIAAFswAgAAWj4CAAAJmAIAACeaAgAABJACACHVZgIAAAjqAgAABUoCABz2BAIAABIcAgAC2yLA=
Date: Tue, 30 Dec 2014 01:50:51 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F70276D1D6@SHSMSX104.ccr.corp.intel.com>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
	<1419494077309.b1eff13f@Nodemailer>
	<CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
	<CAMwrk0=CaXp9i8=6kzpYEZN7tGgf62cJjmiUOnCSemXnq77pDg@mail.gmail.com>
 <CAKWX9VWfjHmJFicPJNUhhN39TWSub_p_cwrg5D=oqFsOW_xioQ@mail.gmail.com>
In-Reply-To: <CAKWX9VWfjHmJFicPJNUhhN39TWSub_p_cwrg5D=oqFsOW_xioQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_64474308D680D540A4D8151B0F7C03F70276D1D6SHSMSX104ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_64474308D680D540A4D8151B0F7C03F70276D1D6SHSMSX104ccrcor_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_64474308D680D540A4D8151B0F7C03F70276D1D6SHSMSX104ccrcor_--

From dev-return-10968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 02:44:08 2014
Return-Path: <dev-return-10968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47C1310AF5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 02:44:08 +0000 (UTC)
Received: (qmail 48759 invoked by uid 500); 30 Dec 2014 02:44:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48681 invoked by uid 500); 30 Dec 2014 02:44:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48668 invoked by uid 99); 30 Dec 2014 02:44:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 02:44:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 02:44:00 +0000
Received: by mail-la0-f46.google.com with SMTP id q1so12328224lam.33
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 18:43:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=W/GnEJjiLcDrxPpjSjyhx5YOUJRmxVPfbkanptf+xYI=;
        b=fK28bimUTN3+GSe3pug38Tif7SYm3clIgDJIDfswLba4wJ+pq4uRP2yiSQIUcm76q/
         Q96wCCE6NSXteTYig7Xo5FlEjhWgQ5q61GoORc3LvczUG9XPF25ouyrjjvgDoKMEd5zO
         hhZQUbSRF4lgM3l1thfPmbX27D2zGHhXR5/dAW0GqIfgiwstW2c7T2BRnFej0OM1eoIp
         3BREoMzqmO/mSQWoDmAxtDPCyEmOREN9PtO8jAk8iRePpZXmdgH7Ei2g0tuPyB+5Dzwb
         v4g487v9mt/8kcCRtugh9KyRHc4FzbXhvpsgxBPH53JMR3MDWhaZAtsc+FvNZRrT+ekm
         dLFQ==
X-Gm-Message-State: ALoCoQnicHbRpeq+HvEbBV2voMDd23BqNREZ2FET/4KGX/oRZqV/1L2Yn5/W68iKldtg1Z3JpDYv
X-Received: by 10.152.6.67 with SMTP id y3mr59591148lay.90.1419907397595; Mon,
 29 Dec 2014 18:43:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Mon, 29 Dec 2014 18:42:57 -0800 (PST)
In-Reply-To: <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com> <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 29 Dec 2014 18:42:57 -0800
Message-ID: <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: Alessandro Baretta <alexbaretta@gmail.com>
Cc: "Wang, Daoyuan" <daoyuan.wang@intel.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e014940460c44cf050b65f4c6
X-Virus-Checked: Checked by ClamAV on apache.org

--089e014940460c44cf050b65f4c6
Content-Type: text/plain; charset=UTF-8

Yeah, I saw those.  The problem is that #3822 truncates timestamps that
include nanoseconds.

On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Michael,
>
> Actually, Adrian Wang already created pull requests for these issues.
>
> https://github.com/apache/spark/pull/3820
> https://github.com/apache/spark/pull/3822
>
> What do you think?
>
> Alex
>
> On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> I'd love to get both of these in.  There is some trickiness that I talk
>> about on the JIRA for timestamps since the SQL timestamp class can support
>> nano seconds and I don't think parquet has a type for this.  Other systems
>> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
>> mailing list what the plan is there to make sure that whatever we do is
>> going to be compatible long term.
>>
>> Michael
>>
>> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>>> Daoyuan,
>>>
>>> Thanks for creating the jiras. I need these features by... last week, so
>>> I'd be happy to take care of this myself, if only you or someone more
>>> experienced than me in the SparkSQL codebase could provide some guidance.
>>>
>>> Alex
>>> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com>
>>> wrote:
>>>
>>>> Hi Alex,
>>>>
>>>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>>>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>>>> support decimals that fits in a long.
>>>>
>>>> Thanks,
>>>> Daoyuan
>>>>
>>>> -----Original Message-----
>>>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>>>> Sent: Saturday, December 27, 2014 2:47 PM
>>>> To: dev@spark.apache.org; Michael Armbrust
>>>> Subject: Unsupported Catalyst types in Parquet
>>>>
>>>> Michael,
>>>>
>>>> I'm having trouble storing my SchemaRDDs in Parquet format with
>>>> SparkSQL, due to my RDDs having having DateType and DecimalType fields.
>>>> What would it take to add Parquet support for these Catalyst? Are there any
>>>> other Catalyst types for which there is no Catalyst support?
>>>>
>>>> Alex
>>>>
>>>
>>
>

--089e014940460c44cf050b65f4c6--

From dev-return-10969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 03:40:28 2014
Return-Path: <dev-return-10969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A69C110C4D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 03:40:28 +0000 (UTC)
Received: (qmail 32046 invoked by uid 500); 30 Dec 2014 03:40:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31966 invoked by uid 500); 30 Dec 2014 03:40:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31954 invoked by uid 99); 30 Dec 2014 03:40:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 03:40:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 03:40:00 +0000
Received: by mail-qa0-f54.google.com with SMTP id i13so7914669qae.27
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 19:39:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=3nRiGp4PJnfmwm/1VeXJJ01oSSFE/TNlesSTdq39psk=;
        b=hBsnzBsyMs/QkXmX3c5YY8NX7UkhlbutYNs/bOAgXXPPntDQsnHuAw1qYDiyrLGUkU
         2qHwKQ+//r4jKel12I6HTfjQoN3skmVBoB0yHg0SnFiFCxnc4XLkjKEe790Y6wMDK7ok
         l30NdYvau7qNjJHil0UzUe032mUSMXsDd+zMsu4Wb/LOn1pI34l42l8vIsJYEIoNl8Ve
         CMQ+X3EpSGJ2Z1QvLK3/wejt5RPmr2Ld/kycc3qMunKRV5IIOGjrHJwZi1M44/LDLrNS
         OQ++iL8GEjdbwwL75WXOdYVi4hOQK2UlqmFtB2xwtL3/eetMXrzEI3M98AvyKNx+eURd
         vBpw==
X-Received: by 10.224.3.137 with SMTP id 9mr98917978qan.64.1419910799288;
        Mon, 29 Dec 2014 19:39:59 -0800 (PST)
Received: from [10.60.1.51] (simcoe.janelia.org. [206.241.0.254])
        by mx.google.com with ESMTPSA id l48sm35070021qgd.38.2014.12.29.19.39.58
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 29 Dec 2014 19:39:58 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_CEDF472D-0752-4FAA-8FB9-6227B6FAC669"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Adding third party jars to classpath used by pyspark
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <CACkSZy3=MTUe1g-ND343aiNJh0NH0Tt8+GXt8RQ4rDroBpiR4w@mail.gmail.com>
Date: Mon, 29 Dec 2014 22:39:57 -0500
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <1A2835E9-B0E1-484B-8641-7C035CD96B19@gmail.com>
References: <CACkSZy3=MTUe1g-ND343aiNJh0NH0Tt8+GXt8RQ4rDroBpiR4w@mail.gmail.com>
To: Stephen Boesch <javadba@gmail.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_CEDF472D-0752-4FAA-8FB9-6227B6FAC669
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

Hi Stephen, it should be enough to include=20

> --jars /path/to/file.jar

in the command line call to either pyspark or spark-submit, as in

> spark-submit --master local --jars /path/to/file.jar myfile.py

and you can check the bottom of the Web UI=92s =93Environment" tab to =
make sure the jar gets on your classpath. Let me know if you still see =
errors related to this.

=97 Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab

On Dec 29, 2014, at 7:55 PM, Stephen Boesch <javadba@gmail.com> wrote:

> What is the recommended way to do this?  We have some native database
> client libraries for which we are adding pyspark bindings.
>=20
> The pyspark invokes spark-submit.   Do we add our libraries to
> the SPARK_SUBMIT_LIBRARY_PATH ?
>=20
> This issue relates back to an error we have been seeing "Py4jError: =
Trying
> to call a package" - the suspicion being that the third party =
libraries may
> not be available on the jvm side.


--Apple-Mail=_CEDF472D-0752-4FAA-8FB9-6227B6FAC669--

From dev-return-10970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 03:41:46 2014
Return-Path: <dev-return-10970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5541710C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 03:41:46 +0000 (UTC)
Received: (qmail 36182 invoked by uid 500); 30 Dec 2014 03:41:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36111 invoked by uid 500); 30 Dec 2014 03:41:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 21472 invoked by uid 99); 30 Dec 2014 03:30:52 -0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of qinggangwang7@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Mon, 29 Dec 2014 20:29:54 -0700 (MST)
From: evil <qinggangwang7@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419910194415-9959.post@n3.nabble.com>
Subject: A question about using insert into in rdd foreach in spark 1.2
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,
I have a  problem when I try to use insert into in loop, and this is my code  
def main(args: Array[String]) {
    //This is an empty table, schema is (Int,String)
   
sqlContext.parquetFile("Data\\Test\\Parquet\\Temp").registerTempTable("temp")
    //not empty table,  schema is (Int,String)
    val testData = sqlContext.parquetFile("Data\\Test\\Parquet\\2")
    testData.foreach{x=>
      sqlContext.sql("INSERT INTO temp SELECT "+x(0)+" ,'"+x(1)+"'")
    }
    sqlContext.sql("select * from
temp").collect().map(x=>(x(0),x(1))).foreach(println)
  }

when I run the code above in local mode, it will not stop and do not have
error log. The lastest log is as follows:
14/12/30 11:07:44 WARN ParquetRecordReader: Can not initialize counter due
to context is not a instance of TaskInputOutputContext, but is
org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
14/12/30 11:07:44 INFO InternalParquetRecordReader: RecordReader initialized
will read a total of 200 records.
14/12/30 11:07:44 INFO InternalParquetRecordReader: at row 0. reading next
block
14/12/30 11:07:44 INFO CodecPool: Got brand-new decompressor [.gz]
14/12/30 11:07:44 INFO InternalParquetRecordReader: block read in memory in
20 ms. row count = 200
14/12/30 11:07:45 INFO SparkContext: Starting job: runJob at
ParquetTableOperations.scala:325
14/12/30 11:07:45 INFO DAGScheduler: Got job 1 (runJob at
ParquetTableOperations.scala:325) with 1 output partitions
(allowLocal=false)
14/12/30 11:07:45 INFO DAGScheduler: Final stage: Stage 1(runJob at
ParquetTableOperations.scala:325)
14/12/30 11:07:45 INFO DAGScheduler: Parents of final stage: List()
14/12/30 11:07:45 INFO DAGScheduler: Missing parents: List()
14/12/30 11:07:45 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[6]
at mapPartitions at basicOperators.scala:43), which has no missing parents
14/12/30 11:07:45 INFO MemoryStore: ensureFreeSpace(53328) called with
curMem=239241, maxMem=1013836677
14/12/30 11:07:45 INFO MemoryStore: Block broadcast_2 stored as values in
memory (estimated size 52.1 KB, free 966.6 MB)
14/12/30 11:07:45 INFO MemoryStore: ensureFreeSpace(31730) called with
curMem=292569, maxMem=1013836677
14/12/30 11:07:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes
in memory (estimated size 31.0 KB, free 966.6 MB)
14/12/30 11:07:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory
on localhost:52533 (size: 31.0 KB, free: 966.8 MB)
14/12/30 11:07:45 INFO BlockManagerMaster: Updated info of block
broadcast_2_piece0
14/12/30 11:07:45 INFO SparkContext: Created broadcast 2 from broadcast at
DAGScheduler.scala:838
14/12/30 11:07:45 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1
(MapPartitionsRDD[6] at mapPartitions at basicOperators.scala:43)
14/12/30 11:07:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks

Can anyone give me a hand?

Thanks
evil



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/A-question-about-using-insert-into-in-rdd-foreach-in-spark-1-2-tp9959.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 04:18:19 2014
Return-Path: <dev-return-10971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 726FE10D1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 04:18:19 +0000 (UTC)
Received: (qmail 70957 invoked by uid 500); 30 Dec 2014 04:18:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70874 invoked by uid 500); 30 Dec 2014 04:18:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 57399 invoked by uid 99); 30 Dec 2014 04:05:07 -0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of guoxu1231@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Mon, 29 Dec 2014 21:04:09 -0700 (MST)
From: guoxu1231 <guoxu1231@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419912249750-9961.post@n3.nabble.com>
Subject: Help,  pyspark.sql.List flatMap results become tuple
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi pyspark guys, 

I have a json file, and its struct like below:

{"NAME":"George", "AGE":35, "ADD_ID":1212, "POSTAL_AREA":1,
"TIME_ZONE_ID":1, "INTEREST":[{"INTEREST_NO":1, "INFO":"x"},
{"INTEREST_NO":2, "INFO":"y"}]}
{"NAME":"John", "AGE":45, "ADD_ID":1213, "POSTAL_AREA":1, "TIME_ZONE_ID":1,
"INTEREST":[{"INTEREST_NO":2, "INFO":"x"}, {"INTEREST_NO":3, "INFO":"y"}]}

I'm using spark sql api to manipulate the json data in pyspark shell, 

*sqlContext = SQLContext(sc)
A400= sqlContext.jsonFile('jason_file_path')*
/Row(ADD_ID=1212, AGE=35, INTEREST=[Row(INFO=u'x', INTEREST_NO=1),
Row(INFO=u'y', INTEREST_NO=2)], NAME=u'George', POSTAL_AREA=1,
TIME_ZONE_ID=1)
Row(ADD_ID=1213, AGE=45, INTEREST=[Row(INFO=u'x', INTEREST_NO=2),
Row(INFO=u'y', INTEREST_NO=3)], NAME=u'John', POSTAL_AREA=1,
TIME_ZONE_ID=1)/
*X = A400.flatMap(lambda i: i.INTEREST)*
The flatMap results like below, each element in json array were flatten to
tuple, not my expected  pyspark.sql.Row. I can only access the flatten
results by index. but it supposed to be flatten to Row(namedTuple) and
support to access by name.
(u'x', 1)
(u'y', 2)
(u'x', 2)
(u'y', 3)

My spark version is 1.1.







--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Help-pyspark-sql-List-flatMap-results-become-tuple-tp9961.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 05:43:00 2014
Return-Path: <dev-return-10972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90F4E10F82
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 05:43:00 +0000 (UTC)
Received: (qmail 69279 invoked by uid 500); 30 Dec 2014 05:42:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69200 invoked by uid 500); 30 Dec 2014 05:42:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69189 invoked by uid 99); 30 Dec 2014 05:42:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 05:42:57 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of guoxu1231@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 05:42:31 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 73992F090FA
	for <dev@spark.apache.org>; Mon, 29 Dec 2014 21:41:29 -0800 (PST)
Date: Mon, 29 Dec 2014 22:41:28 -0700 (MST)
From: guoxu1231 <guoxu1231@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419918088563-9962.post@n3.nabble.com>
In-Reply-To: <1419912249750-9961.post@n3.nabble.com>
References: <1419912249750-9961.post@n3.nabble.com>
Subject: Re: Help,  pyspark.sql.List flatMap results become tuple
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

named tuple degenerate to tuple. 
*A400.map(lambda i: map(None,i.INTEREST))*
===============================
[(u'x', 1), (u'y', 2)]
[(u'x', 2), (u'y', 3)]



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Help-pyspark-sql-List-flatMap-results-become-tuple-tp9961p9962.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 05:44:53 2014
Return-Path: <dev-return-10973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F46F10F94
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 05:44:53 +0000 (UTC)
Received: (qmail 71987 invoked by uid 500); 30 Dec 2014 05:44:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71911 invoked by uid 500); 30 Dec 2014 05:44:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71900 invoked by uid 99); 30 Dec 2014 05:44:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 05:44:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 05:44:46 +0000
Received: by mail-ob0-f179.google.com with SMTP id va2so43784536obc.10
        for <dev@spark.apache.org>; Mon, 29 Dec 2014 21:43:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=1LnAbCVNmPZQXGB1SDuuYBdPESEONkj5J1lR0gGxDjI=;
        b=LN53zQa/gPotwt6P8rilmjZM5NFBjPgJ+l02EyHBaaYjU6QjiLYhvhP+1p+TKS4m+7
         Qp687SZ04L0FJbzR6nx2aRsO4W3tVfHLpWlfR7rgO2Y4BlG4r27ugCYkttWLupAVWd/L
         FI7wyT23xuFzbZJQwfkYeAhUkIIRLxr4IPverbJ7RGYVt1tGQep1dNG8Q7ricPZSrukU
         6oNKuYC5tXscDikJsd/MLCfgCVM6N+zlxA6uUmg/LkbhP1kbqKVD+Vknr6AGKyBZqZLA
         c8CQi8WMmWLObKnREFcMnRZahfuTyqO89O008DpCKyRZkhVupxF7i1lSn3qfDY3nOKm9
         SElw==
X-Gm-Message-State: ALoCoQnb/fQKQ+4sOkiF+Hwcr1nPMBSvzy+k1MY6raZkvt5OR90bYf9R20y9/sQ2nC2tXPXTiKd4
MIME-Version: 1.0
X-Received: by 10.202.172.5 with SMTP id v5mr32888445oie.48.1419918200476;
 Mon, 29 Dec 2014 21:43:20 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Mon, 29 Dec 2014 21:43:20 -0800 (PST)
In-Reply-To: <64474308D680D540A4D8151B0F7C03F70276D1D6@SHSMSX104.ccr.corp.intel.com>
References: <CAKWX9VWFXv5C2xyTfch_k-ENdGTo8qS39cxXMdccAvS=6k5FFQ@mail.gmail.com>
	<1419494077309.b1eff13f@Nodemailer>
	<CAKWX9VUtv3rDfb6aZAdmPouadUWU4SmXWszJXLiXVRT8PnT4uw@mail.gmail.com>
	<CAMwrk0=CaXp9i8=6kzpYEZN7tGgf62cJjmiUOnCSemXnq77pDg@mail.gmail.com>
	<CAKWX9VWfjHmJFicPJNUhhN39TWSub_p_cwrg5D=oqFsOW_xioQ@mail.gmail.com>
	<64474308D680D540A4D8151B0F7C03F70276D1D6@SHSMSX104.ccr.corp.intel.com>
Date: Mon, 29 Dec 2014 23:43:20 -0600
Message-ID: <CAKWX9VUSx3WqQVp1yCrgRFz7ux1c2RyKnY0WuM-xo6BJRk7e+A@mail.gmail.com>
Subject: Re: Which committers care about Kafka?
From: Cody Koeninger <cody@koeninger.org>
To: "Shao, Saisai" <saisai.shao@intel.com>
Cc: Tathagata Das <tathagata.das1565@gmail.com>, 
	Hari Shreedharan <hshreedharan@cloudera.com>, Sean McNamara <sean.mcnamara@webtrends.com>, 
	Patrick Wendell <pwendell@gmail.com>, 
	=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>, 
	Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Koert Kuipers <koert@tresata.com>
Content-Type: multipart/alternative; boundary=001a113c3800f32b90050b6877f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113c3800f32b90050b6877f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Assuming you're talking about spark.streaming.receiver.maxRate, I just
updated my PR to configure rate limiting based on that setting.  So
hopefully that's issue 1 sorted.

Regarding issue 3, as far as I can tell regarding the odd semantics of
stateful or windowed operations in the face of failure, my solution is no
worse than existing classes such as FileStream that use inputdstream
directly rather than a receiver.  Can we get some specific cases that are a
concern?

Regarding the WAL solutions TD mentioned, one of the disadvantages of them
is that they rely on checkpointing, unlike my approach.  As I noted in this
thread and in the jira ticket, I need something that can recover even when
a checkpoint is lost, and I've already seen multiple situations in
production where a checkpoint cannot be recovered (e.g. because code needs
to be updated).

On Mon, Dec 29, 2014 at 7:50 PM, Shao, Saisai <saisai.shao@intel.com> wrote=
:

>  Hi Cody,
>
>
>
> From my understanding rate control is an optional configuration in Spark
> Streaming and is disabled by default, so user can reach maximum throughpu=
t
> without any configuration.
>
>
>
> The reason why rate control is so important in streaming processing is
> that Spark Streaming and other streaming frameworks are easily prone to
> unexpected behavior and failure situation due to network boost and other
> un-controllable inject rate.
>
>
>
> Especially for Spark Streaming,  the large amount of processed data will
> delay the processing time, which will further delay the ongoing job, and
> finally lead to failure.
>
>
>
> Thanks
>
> Jerry
>
>
>
> *From:* Cody Koeninger [mailto:cody@koeninger.org]
> *Sent:* Tuesday, December 30, 2014 6:50 AM
> *To:* Tathagata Das
> *Cc:* Hari Shreedharan; Shao, Saisai; Sean McNamara; Patrick Wendell;
> Luis =C3=81ngel Vicente S=C3=A1nchez; Dibyendu Bhattacharya; dev@spark.ap=
ache.org;
> Koert Kuipers
>
> *Subject:* Re: Which committers care about Kafka?
>
>
>
> Can you give a little more clarification on exactly what is meant by
>
>
>
> 1. Data rate control
>
>
>
> If someone wants to clamp the maximum number of messages per RDD partitio=
n
> in my solution, it would be very straightforward to do so.
>
>
>
> Regarding the holy grail, I'm pretty certain you can't have end-to-end
> transactional semantics without the client code being in charge of offset
> state.  That means the client code is going to also need to be in charge =
of
> setting up an initial state for updateStateByKey that makes sense; as lon=
g
> as they can do that, the job should be safe to restart from arbitrary
> failures.
>
>
>
> On Mon, Dec 29, 2014 at 4:33 PM, Tathagata Das <
> tathagata.das1565@gmail.com> wrote:
>
> Hey all,
>
> Some wrap up thoughts on this thread.
>
> Let me first reiterate what Patrick said, that Kafka is super super
> important as it forms the largest fraction of Spark Streaming user
> base. So we really want to improve the Kafka + Spark Streaming
> integration. To this end, some of the things that needs to be
> considered can be broadly classified into the following to sort
> facilitate the discussion.
>
> 1. Data rate control
> 2. Receiver failure semantics - partially achieving this gives
> at-least once, completely achieving this gives exactly-once
> 3. Driver failure semantics - partially achieving this gives at-least
> once, completely achieving this gives exactly-once
>
> Here is a run down of what is achieved by different implementations
> (based on what I think).
>
> 1. Prior to WAL in Spark 1.2, the KafkaReceiver could handle 3, could
> handle 1 partially (some duplicate data), and could NOT handle 2 (all
> previously received data lost).
>
> 2. In Spark 1.2 with WAL enabled, the Saisai's ReliableKafkaReceiver
> can handle 3, can almost completely handle 1 and 2 (except few corner
> cases which prevents it from completely guaranteeing exactly-once).
>
> 3. I believe Dibyendu's solution (correct me if i am wrong) can handle
> 1 and 2 perfectly. And 3 can be partially solved with WAL, or possibly
> completely solved by extending the solution further.
>
> 4. Cody's solution (again, correct me if I am wrong) does not use
> receivers at all (so eliminates 2). It can handle 3 completely for
> simple operations like map and filter, but not sure if it works
> completely for stateful ops like windows and updateStateByKey. Also it
> does not handle 1.
>
> The real challenge for Kafka is in achieving 3 completely for stateful
> operations while also handling 1.  (i.e., use receivers, but still get
> driver failure guarantees). Solving this will give us our holy grail
> solution, and this is what I want to achieve.
>
> On that note, Cody submitted a PR on his style of achieving
> exactly-once semantics - https://github.com/apache/spark/pull/3798 . I
> am reviewing it. Please follow the PR if you are interested.
>
> TD
>
>
> On Wed, Dec 24, 2014 at 11:59 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> > The conversation was mostly getting TD up to speed on this thread since
> he
> > had just gotten back from his trip and hadn't seen it.
> >
> > The jira has a summary of the requirements we discussed, I'm sure TD or
> > Patrick can add to the ticket if I missed something.
> > On Dec 25, 2014 1:54 AM, "Hari Shreedharan" <hshreedharan@cloudera.com>
> > wrote:
> >
> >> In general such discussions happen or is posted on the dev lists. Coul=
d
> >> you please post a summary? Thanks.
> >>
> >> Thanks,
> >> Hari
> >>
> >>
> >> On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org>
> >> wrote:
> >>
> >>>  After a long talk with Patrick and TD (thanks guys), I opened the
> >>> following jira
> >>>
> >>> https://issues.apache.org/jira/browse/SPARK-4964
> >>>
> >>> Sample PR has an impementation for the batch and the dstream case, an=
d
> a
> >>> link to a project with example usage.
> >>>
> >>> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >>>
> >>>> yup, we at tresata do the idempotent store the same way. very simple
> >>>> approach.
> >>>>
> >>>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org>
> >>>> wrote:
> >>>>>
> >>>>> That KafkaRDD code is dead simple.
> >>>>>
> >>>>> Given a user specified map
> >>>>>
> >>>>> (topic1, partition0) -> (startingOffset, endingOffset)
> >>>>> (topic1, partition1) -> (startingOffset, endingOffset)
> >>>>> ...
> >>>>> turn each one of those entries into a partition of an rdd, using th=
e
> >>>>> simple
> >>>>> consumer.
> >>>>> That's it.  No recovery logic, no state, nothing - for any failures=
,
> >>>>> bail
> >>>>> on the rdd and let it retry.
> >>>>> Spark stays out of the business of being a distributed database.
> >>>>>
> >>>>> The client code does any transformation it wants, then stores the
> data
> >>>>> and
> >>>>> offsets.  There are two ways of doing this, either based on
> idempotence
> >>>>> or
> >>>>> a transactional data store.
> >>>>>
> >>>>> For idempotent stores:
> >>>>>
> >>>>> 1.manipulate data
> >>>>> 2.save data to store
> >>>>> 3.save ending offsets to the same store
> >>>>>
> >>>>> If you fail between 2 and 3, the offsets haven't been stored, you
> start
> >>>>> again at the same beginning offsets, do the same calculations in th=
e
> >>>>> same
> >>>>> order, overwrite the same data, all is good.
> >>>>>
> >>>>>
> >>>>> For transactional stores:
> >>>>>
> >>>>> 1. manipulate data
> >>>>> 2. begin transaction
> >>>>> 3. save data to the store
> >>>>> 4. save offsets
> >>>>> 5. commit transaction
> >>>>>
> >>>>> If you fail before 5, the transaction rolls back.  To make this les=
s
> >>>>> heavyweight, you can write the data outside the transaction and the=
n
> >>>>> update
> >>>>> a pointer to the current data inside the transaction.
> >>>>>
> >>>>>
> >>>>> Again, spark has nothing much to do with guaranteeing exactly once.
> In
> >>>>> fact, the current streaming api actively impedes my ability to do t=
he
> >>>>> above.  I'm just suggesting providing an api that doesn't get in th=
e
> >>>>> way of
> >>>>> exactly-once.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
> >>>>> hshreedharan@cloudera.com
> >>>>> > wrote:
> >>>>>
> >>>>> > Can you explain your basic algorithm for the once-only-delivery?
> It is
> >>>>> > quite a bit of very Kafka-specific code, that would take more tim=
e
> to
> >>>>> read
> >>>>> > than I can currently afford? If you can explain your algorithm a
> bit,
> >>>>> it
> >>>>> > might help.
> >>>>> >
> >>>>> > Thanks,
> >>>>> > Hari
> >>>>> >
> >>>>> >
> >>>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <
> cody@koeninger.org>
> >>>>> > wrote:
> >>>>> >
> >>>>> >>
> >>>>> >> The problems you guys are discussing come from trying to store
> state
> >>>>> in
> >>>>> >> spark, so don't do that.  Spark isn't a distributed database.
> >>>>> >>
> >>>>> >> Just map kafka partitions directly to rdds, llet user code speci=
fy
> >>>>> the
> >>>>> >> range of offsets explicitly, and let them be in charge of
> committing
> >>>>> >> offsets.
> >>>>> >>
> >>>>> >> Using the simple consumer isn't that bad, I'm already using this
> in
> >>>>> >> production with the code I linked to, and tresata apparently has
> >>>>> been as
> >>>>> >> well.  Again, for everyone saying this is impossible, have you
> read
> >>>>> either
> >>>>> >> of those implementations and looked at the approach?
> >>>>> >>
> >>>>> >>
> >>>>> >>
> >>>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
> >>>>> >> Sean.McNamara@webtrends.com> wrote:
> >>>>> >>
> >>>>> >>> Please feel free to correct me if I=E2=80=99m wrong, but I thin=
k the
> exactly
> >>>>> >>> once spark streaming semantics can easily be solved using
> >>>>> updateStateByKey.
> >>>>> >>> Make the key going into updateStateByKey be a hash of the event=
,
> or
> >>>>> pluck
> >>>>> >>> off some uuid from the message.  The updateFunc would only emit
> the
> >>>>> message
> >>>>> >>> if the key did not exist, and the user has complete control ove=
r
> >>>>> the window
> >>>>> >>> of time / state lifecycle for detecting duplicates.  It also
> makes
> >>>>> it
> >>>>> >>> really easy to detect and take action (alert?) when you DO see =
a
> >>>>> duplicate,
> >>>>> >>> or make memory tradeoffs within an error bound using a sketch
> >>>>> algorithm.
> >>>>> >>> The kafka simple consumer is insanely complex, if possible I
> think
> >>>>> it would
> >>>>> >>> be better (and vastly more flexible) to get reliability using t=
he
> >>>>> >>> primitives that spark so elegantly provides.
> >>>>> >>>
> >>>>> >>> Cheers,
> >>>>> >>>
> >>>>> >>> Sean
> >>>>> >>>
> >>>>> >>>
> >>>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
> >>>>> >>> hshreedharan@cloudera.com> wrote:
> >>>>> >>> >
> >>>>> >>> > Hi Dibyendu,
> >>>>> >>> >
> >>>>> >>> > Thanks for the details on the implementation. But I still do
> not
> >>>>> >>> believe
> >>>>> >>> > that it is no duplicates - what they achieve is that the same
> >>>>> batch is
> >>>>> >>> > processed exactly the same way every time (but see it may be
> >>>>> processed
> >>>>> >>> more
> >>>>> >>> > than once) - so it depends on the operation being idempotent.=
 I
> >>>>> believe
> >>>>> >>> > Trident uses ZK to keep track of the transactions - a batch
> can be
> >>>>> >>> > processed multiple times in failure scenarios (for example, t=
he
> >>>>> >>> transaction
> >>>>> >>> > is processed but before ZK is updated the machine fails,
> causing a
> >>>>> >>> "new"
> >>>>> >>> > node to process it again).
> >>>>> >>> >
> >>>>> >>> > I don't think it is impossible to do this in Spark Streaming =
as
> >>>>> well
> >>>>> >>> and
> >>>>> >>> > I'd be really interested in working on it at some point in th=
e
> >>>>> near
> >>>>> >>> future.
> >>>>> >>> >
> >>>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
> >>>>> >>> > dibyendu.bhattachary@gmail.com> wrote:
> >>>>> >>> >
> >>>>> >>> >> Hi,
> >>>>> >>> >>
> >>>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident.
> The
> >>>>> Storm
> >>>>> >>> >> Trident has done the exact-once guarantee by processing the
> >>>>> tuple in a
> >>>>> >>> >> batch  and assigning same transaction-id for a given batch .
> The
> >>>>> >>> replay for
> >>>>> >>> >> a given batch with a transaction-id will have exact same set
> of
> >>>>> >>> tuples and
> >>>>> >>> >> replay of batches happen in exact same order before the
> failure.
> >>>>> >>> >>
> >>>>> >>> >> Having this paradigm, if downstream system process data for =
a
> >>>>> given
> >>>>> >>> batch
> >>>>> >>> >> for having a given transaction-id , and if during failure if
> same
> >>>>> >>> batch is
> >>>>> >>> >> again emitted , you can check if same transaction-id is
> already
> >>>>> >>> processed
> >>>>> >>> >> or not and hence can guarantee exact once semantics.
> >>>>> >>> >>
> >>>>> >>> >> And this can only be achieved in Spark if we use Low Level
> Kafka
> >>>>> >>> consumer
> >>>>> >>> >> API to process the offsets. This low level Kafka Consumer (
> >>>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has
> >>>>> implemented the
> >>>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All o=
f
> the
> >>>>> >>> Kafka
> >>>>> >>> >> related logic has been taken from Storm-Kafka spout and whic=
h
> >>>>> manages
> >>>>> >>> all
> >>>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metada=
ta
> >>>>> >>> managements.
> >>>>> >>> >>
> >>>>> >>> >> Presently this Consumer maintains that during Receiver
> failure,
> >>>>> it
> >>>>> >>> will
> >>>>> >>> >> re-emit the exact same Block with same set of messages . Eve=
ry
> >>>>> >>> message have
> >>>>> >>> >> the details of its partition, offset and topic related detai=
ls
> >>>>> which
> >>>>> >>> can
> >>>>> >>> >> tackle the SPARK-3146.
> >>>>> >>> >>
> >>>>> >>> >> As this Low Level consumer has complete control over the Kaf=
ka
> >>>>> >>> Offsets ,
> >>>>> >>> >> we can implement Trident like feature on top of it like havi=
ng
> >>>>> >>> implement a
> >>>>> >>> >> transaction-id for a given block , and re-emit the same bloc=
k
> >>>>> with
> >>>>> >>> same set
> >>>>> >>> >> of message during Driver failure.
> >>>>> >>> >>
> >>>>> >>> >> Regards,
> >>>>> >>> >> Dibyendu
> >>>>> >>> >>
> >>>>> >>> >>
> >>>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <
> >>>>> saisai.shao@intel.com>
> >>>>> >>> >> wrote:
> >>>>> >>> >>>
> >>>>> >>> >>> Hi all,
> >>>>> >>> >>>
> >>>>> >>> >>> I agree with Hari that Strong exact-once semantics is very
> hard
> >>>>> to
> >>>>> >>> >>> guarantee, especially in the failure situation. From my
> >>>>> >>> understanding even
> >>>>> >>> >>> current implementation of ReliableKafkaReceiver cannot full=
y
> >>>>> >>> guarantee the
> >>>>> >>> >>> exact once semantics once failed, first is the ordering of
> data
> >>>>> >>> replaying
> >>>>> >>> >>> from last checkpoint, this is hard to guarantee when multip=
le
> >>>>> >>> partitions
> >>>>> >>> >>> are injected in; second is the design complexity of achievi=
ng
> >>>>> this,
> >>>>> >>> you can
> >>>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into th=
e
> >>>>> very
> >>>>> >>> details
> >>>>> >>> >>> of Kafka metadata management system to achieve this, not to
> say
> >>>>> >>> rebalance
> >>>>> >>> >>> and fault-tolerance.
> >>>>> >>> >>>
> >>>>> >>> >>> Thanks
> >>>>> >>> >>> Jerry
> >>>>> >>> >>>
> >>>>> >>> >>> -----Original Message-----
> >>>>> >>> >>> From: Luis =C3=81ngel Vicente S=C3=A1nchez [mailto:
> >>>>> langel.groups@gmail.com]
> >>>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
> >>>>> >>> >>> To: Cody Koeninger
> >>>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
> >>>>> >>> >>> Subject: Re: Which committers care about Kafka?
> >>>>> >>> >>>
> >>>>> >>> >>> But idempotency is not that easy t achieve sometimes. A
> strong
> >>>>> only
> >>>>> >>> once
> >>>>> >>> >>> semantic through a proper API would  be superuseful; but I'=
m
> not
> >>>>> >>> implying
> >>>>> >>> >>> this is easy to achieve.
> >>>>> >>> >>> On 18 Dec 2014 21:52, "Cody Koeninger" <cody@koeninger.org>
> >>>>> wrote:
> >>>>> >>> >>>
> >>>>> >>> >>>> If the downstream store for the output data is idempotent =
or
> >>>>> >>> >>>> transactional, and that downstream store also is the syste=
m
> of
> >>>>> >>> record
> >>>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.
> >>>>> Commit
> >>>>> >>> >>>> offsets with / after the data is stored.  On any failure,
> >>>>> restart
> >>>>> >>> from
> >>>>> >>> >>> the last committed offsets.
> >>>>> >>> >>>>
> >>>>> >>> >>>> Yes, this approach is biased towards the etl-like use case=
s
> >>>>> rather
> >>>>> >>> >>>> than near-realtime-analytics use cases.
> >>>>> >>> >>>>
> >>>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
> >>>>> >>> >>>> hshreedharan@cloudera.com
> >>>>> >>> >>>>> wrote:
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> I get what you are saying. But getting exactly once right
> is
> >>>>> an
> >>>>> >>> >>>>> extremely hard problem - especially in presence of failur=
e.
> >>>>> The
> >>>>> >>> >>>>> issue is failures
> >>>>> >>> >>>> can
> >>>>> >>> >>>>> happen in a bunch of places. For example, before the
> >>>>> notification
> >>>>> >>> of
> >>>>> >>> >>>>> downstream store being successful reaches the receiver th=
at
> >>>>> updates
> >>>>> >>> >>>>> the offsets, the node fails. The store was successful, bu=
t
> >>>>> >>> >>>>> duplicates came in either way. This is something worth
> >>>>> discussing
> >>>>> >>> by
> >>>>> >>> >>>>> itself - but without uuids etc this might not really be
> >>>>> solved even
> >>>>> >>> >>> when you think it is.
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in
> all
> >>>>> of
> >>>>> >>> the
> >>>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and
> once-only
> >>>>> >>> >>>>> delivery,
> >>>>> >>> >>>> but
> >>>>> >>> >>>>> I doubt the latter is really possible to guarantee -
> though I
> >>>>> >>> really
> >>>>> >>> >>>> would
> >>>>> >>> >>>>> love to have that!
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> Thanks,
> >>>>> >>> >>>>> Hari
> >>>>> >>> >>>>>
> >>>>> >>> >>>>>
> >>>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
> >>>>> >>> >>>>> <cody@koeninger.org>
> >>>>> >>> >>>>> wrote:
> >>>>> >>> >>>>>
> >>>>> >>> >>>>>> Thanks for the replies.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization=
.
> >>>>> If you
> >>>>> >>> >>>>>> actually want exactly-once semantics, you need control o=
f
> >>>>> kafka
> >>>>> >>> >>>>>> offsets
> >>>>> >>> >>>> as
> >>>>> >>> >>>>>> well, including the ability to not use zookeeper as the
> >>>>> system of
> >>>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system
> that
> >>>>> has
> >>>>> >>> >>>>>> strong
> >>>>> >>> >>>> ordering
> >>>>> >>> >>>>>> guarantees (within a partition) and does not mandate the
> use
> >>>>> of
> >>>>> >>> >>>> zookeeper
> >>>>> >>> >>>>>> to store offsets.  I think there should be a spark api
> that
> >>>>> acts
> >>>>> >>> as
> >>>>> >>> >>>>>> a
> >>>>> >>> >>>> very
> >>>>> >>> >>>>>> simple intermediary between Kafka and the user's choice =
of
> >>>>> >>> >>>>>> downstream
> >>>>> >>> >>>> store.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> Take a look at the links I posted - if there's already
> been 2
> >>>>> >>> >>>> independent
> >>>>> >>> >>>>>> implementations of the idea, chances are it's something
> >>>>> people
> >>>>> >>> need.
> >>>>> >>> >>>>>>
> >>>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
> >>>>> >>> >>>>>> hshreedharan@cloudera.com> wrote:
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> Hi Cody,
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can
> implement
> >>>>> >>> >>>>>>> something pretty simple and lightweight for that one.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation -
> >>>>> this is
> >>>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though i=
t
> is
> >>>>> a
> >>>>> >>> good
> >>>>> >>> >>>> idea to
> >>>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is =
an
> >>>>> >>> >>>> optimization. For
> >>>>> >>> >>>>>>> that reason, we must be careful in implementation. Ther=
e
> >>>>> are a
> >>>>> >>> >>>>>>> couple
> >>>>> >>> >>>> of
> >>>>> >>> >>>>>>> issues that we need to ensure works properly -
> specifically
> >>>>> >>> >>> ordering.
> >>>>> >>> >>>> To
> >>>>> >>> >>>>>>> ensure we pull messages from different topics and
> >>>>> partitions in
> >>>>> >>> >>>>>>> the
> >>>>> >>> >>>> same
> >>>>> >>> >>>>>>> order after failure, we=E2=80=99d still have to persist=
 the
> >>>>> metadata to
> >>>>> >>> >>>>>>> HDFS
> >>>>> >>> >>>> (or
> >>>>> >>> >>>>>>> some other system) - this metadata must contain the
> order of
> >>>>> >>> >>>>>>> messages consumed, so we know how to re-read the
> messages.
> >>>>> I am
> >>>>> >>> >>>>>>> planning to
> >>>>> >>> >>>> explore
> >>>>> >>> >>>>>>> this once I have some time (probably in Jan). In
> addition,
> >>>>> we
> >>>>> >>> must
> >>>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I wi=
ll
> >>>>> file a
> >>>>> >>> >>>>>>> placeholder jira for this one.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while
> >>>>> back -
> >>>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping
> >>>>> that
> >>>>> >>> this
> >>>>> >>> >>>>>>> will get pulled in soon, as this is something I know
> people
> >>>>> want.
> >>>>> >>> >>>>>>> I am open
> >>>>> >>> >>>> to
> >>>>> >>> >>>>>>> feedback on that - anything that I can do to make it
> better.
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> Thanks,
> >>>>> >>> >>>>>>> Hari
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
> >>>>> >>> >>>>>>> <pwendell@gmail.com>
> >>>>> >>> >>>>>>> wrote:
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>>>> Hey Cody,
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streami=
ng
> >>>>> is TD -
> >>>>> >>> >>>>>>>> he is traveling this week though so I can respond a bi=
t.
> >>>>> To the
> >>>>> >>> >>>>>>>> high level point of whether Kafka is important - it
> >>>>> definitely
> >>>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
> >>>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good suppo=
rt
> >>>>> for
> >>>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not =
a
> >>>>> library.
> >>>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
> >>>>> unstable
> >>>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
> >>>>> stabilize
> >>>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn'=
t
> be
> >>>>> able
> >>>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level
> >>>>> impression
> >>>>> >>> only
> >>>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so
> it's
> >>>>> worth
> >>>>> >>> >>> revisiting given the developments in Kafka.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> Please do bring things up like this on the dev list if
> >>>>> there are
> >>>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> - Patrick
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
> >>>>> >>> >>>>>>>> <cody@koeninger.org>
> >>>>> >>> >>>>>>>> wrote:
> >>>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people
> to
> >>>>> get
> >>>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexibl=
e
> >>>>> for our
> >>>>> >>> >>>>>>>> production
> >>>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint,
> because
> >>>>> I've
> >>>>> >>> >>>>>>>>> seen several different patches and libraries to fix t=
he
> >>>>> same
> >>>>> >>> >>>>>>>>> things we've
> >>>>> >>> >>>>>>>> been
> >>>>> >>> >>>>>>>>> running into.
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Regarding flexibility
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an
> equivalent
> >>>>> of
> >>>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patc=
h
> >>>>> >>> >>>>>>>>> ourselves, then found
> >>>>> >>> >>>>>>>> that
> >>>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn'=
t
> be
> >>>>> able
> >>>>> >>> >>>>>>>>> to
> >>>>> >>> >>>> get
> >>>>> >>> >>>>>>>> our
> >>>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a
> >>>>> whole
> >>>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388,
> >>>>> arbitrary
> >>>>> >>> >>>>>>>>> delay of
> >>>>> >>> >>>>>>>> messages, etc).
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
> >>>>> >>> >>>>>>>>> WriteAheadLog
> >>>>> >>> >>>> as
> >>>>> >>> >>>>>>>> a
> >>>>> >>> >>>>>>>>> general solution for streaming unreliable sources, bu=
t
> >>>>> Kafka
> >>>>> >>> >>>>>>>>> already
> >>>>> >>> >>>>>>>> is a
> >>>>> >>> >>>>>>>>> reliable source. I think there's a need for an api th=
at
> >>>>> treats
> >>>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
> >>>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
> >>>>> >>> >>>>>>>>> write-ahead log in hdfs, I
> >>>>> >>> >>>> need
> >>>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've h=
ad
> >>>>> >>> >>>>>>>>> failures
> >>>>> >>> >>>> that
> >>>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
> >>>>> >>> instance).
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> I've got an implementation i've been using
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>>
> >>>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> >>>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Tresata has something similar at
> >>>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
> >>>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm
> >>>>> code.
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as librarie=
s
> >>>>> rather
> >>>>> >>> >>>>>>>>> than
> >>>>> >>> >>>>>>>> patches
> >>>>> >>> >>>>>>>>> to Spark is problematic, because large portions of th=
e
> >>>>> >>> >>>> implementation
> >>>>> >>> >>>>>>>> are
> >>>>> >>> >>>>>>>>> private[spark].
> >>>>> >>> >>>>>>>>>
> >>>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention =
to
> >>>>> get.
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>>
> >>>>> >>> ---------------------------------------------------------------=
--
> >>>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
> >>>>> dev-unsubscribe@spark.apache.org
> >>>>> >>> For
> >>>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>>
> >>>>> >>> >>>>>>>
> >>>>> >>> >>>>>
> >>>>> >>> >>>>
> >>>>> >>> >>>
> >>>>> >>> >>
> >>>>> >>>
> >>>>> >>>
> >>>>> >>
> >>>>> >
> >>>>>
> >>>>
> >>>
> >>
>
>
>

--001a113c3800f32b90050b6877f8--

From dev-return-10974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 06:19:58 2014
Return-Path: <dev-return-10974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33C73C050
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 06:19:58 +0000 (UTC)
Received: (qmail 97980 invoked by uid 500); 30 Dec 2014 06:19:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97900 invoked by uid 500); 30 Dec 2014 06:19:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 92232 invoked by uid 99); 30 Dec 2014 06:14:14 -0000
X-ASF-Spam-Status: No, hits=5.8 required=10.0
	tests=HTML_IMAGE_ONLY_24,HTML_MESSAGE,SPF_SOFTFAIL,T_REMOTE_IMAGE,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of danqing0703@berkeley.edu does not designate 162.253.133.43 as permitted sender)
Date: Mon, 29 Dec 2014 23:12:47 -0700 (MST)
From: danqing0703 <danqing0703@berkeley.edu>
To: dev@spark.apache.org
Message-ID: <CAG3aN57J1vA5WG1rnjShH80DzoYgUqgu0mOp+vK5x3ArfRqbcg@mail.gmail.com>
Subject: Problems concerning implementing machine learning algorithm from
 scratch based on Spark
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_84656_791737336.1419919967963"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_84656_791737336.1419919967963
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi all,

I am trying to use some machine learning algorithms that are not included
in the Mllib. Like Mixture Model and LDA(Latent Dirichlet Allocation), and
I am using pyspark and Spark SQL.

My problem is: I have some scripts that implement these algorithms, but I
am not sure which part I shall change to make it fit into Big Data.

   - Like some very simple calculation may take much time if data is too
   big,but also constructing RDD or SQLContext table takes too much time. I am
   really not sure if I shall use map(), reduce() every time I need to make
   calculation.
   - Also, there are some matrix/array level calculation that can not be
   implemented easily merely using map(),reduce(), thus functions of the Numpy
   package shall be used. I am not sure when data is too big, and we simply
   use the numpy functions. Will it take too much time?

I have found some scripts that are not from Mllib and was created by other
developers(credits to Meethu Mathew from Flytxt, thanks for giving me
insights!:))

Many thanks and look forward to getting feedbacks!

Best, Danqing


GMMSpark.py (7K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/9964/0/GMMSpark.py>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Problems-concerning-implementing-machine-learning-algorithm-from-scratch-based-on-Spark-tp9964.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_84656_791737336.1419919967963--

From dev-return-10975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 06:25:32 2014
Return-Path: <dev-return-10975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83C84C05F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 06:25:32 +0000 (UTC)
Received: (qmail 2192 invoked by uid 500); 30 Dec 2014 06:25:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2110 invoked by uid 500); 30 Dec 2014 06:25:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2067 invoked by uid 99); 30 Dec 2014 06:25:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 06:25:15 +0000
X-ASF-Spam-Status: No, hits=-1.2 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of daoyuan.wang@intel.com designates 192.55.52.115 as permitted sender)
Received: from [192.55.52.115] (HELO mga14.intel.com) (192.55.52.115)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 06:25:11 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga103.fm.intel.com with ESMTP; 29 Dec 2014 22:18:36 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.07,664,1413270000"; 
   d="scan'208,217";a="654779968"
Received: from pgsmsx101.gar.corp.intel.com ([10.221.44.78])
  by fmsmga002.fm.intel.com with ESMTP; 29 Dec 2014 22:22:49 -0800
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 PGSMSX101.gar.corp.intel.com (10.221.44.78) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Tue, 30 Dec 2014 14:20:09 +0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.110]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.182]) with mapi id 14.03.0195.001;
 Tue, 30 Dec 2014 14:20:08 +0800
From: "Wang, Daoyuan" <daoyuan.wang@intel.com>
To: Michael Armbrust <michael@databricks.com>, Alessandro Baretta
	<alexbaretta@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Unsupported Catalyst types in Parquet
Thread-Topic: Unsupported Catalyst types in Parquet
Thread-Index: AQHQIaF0vtW/afAdlkCSX17MyudGDJyl2VLggABiL4CAAHOeAIAAI48AgAAYlYCAAMIHkA==
Date: Tue, 30 Dec 2014 06:20:07 +0000
Message-ID: <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
 <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
 <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
In-Reply-To: <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7SHSMSX101ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7SHSMSX101ccrcor_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7SHSMSX101ccrcor_--

From dev-return-10976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 10:45:15 2014
Return-Path: <dev-return-10976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3517DC6D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 10:45:15 +0000 (UTC)
Received: (qmail 12965 invoked by uid 500); 30 Dec 2014 10:45:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12889 invoked by uid 500); 30 Dec 2014 10:45:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12405 invoked by uid 99); 30 Dec 2014 10:45:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 10:45:04 +0000
X-ASF-Spam-Status: No, hits=4.0 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLYTO_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [106.10.151.179] (HELO nm30-vm4.bullet.mail.sg3.yahoo.com) (106.10.151.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 10:44:39 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s2048; t=1419936067; bh=a4bY77+KtyYVR6spQFAIxqI4v6SI64IlpWB/piK5CiE=; h=Date:From:Reply-To:To:In-Reply-To:References:Subject:From:Subject; b=kv8HirpcK0vF9XXrkObfh8khLiy0THvJ8id+TGk4XB7j1M8YQ16SafO2pYBSiafiWkq1XzAmuN0B94nYoIH8OEnrxNJpxh3bif8TbayFd7Ppcp0KnXgEzy/xbePPADVWWl8Pgc375hyr33bcV2Ws54DNORWczOfmWxiQkz9Usqx8+5PQcr5fDP0b5fomZhc8A+V5HKfqarhLYpfzy4OPB83Ijbusp6SRM+VtdL+dg4rX6yOawCoB1X93dD/To3zVNHd+wJ+d07JzNioNUP+bPkw6/AIZ85y/wV0G0DWfXv9glatKA/5X+97coGjrMDbHosQAeGMYn6wjsuQrYeygLA==
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.co.in;
	b=F3V7oPCgGXVYJoY4YUFQX5OfNH45rilaECkO+E9rCMrRcftAEu4v+si7Z0zsXUG8E+gk/DydjOOyUHlOd8Huw+jA7IDBANq28jRhdffs5JNSJ+wnxO7xrUTam8PbfkEdS79Tvjrwe93IT2BPnGwGXObPSgU19Cv27IikJ543Q8yxXwa9t4dvcyqnRFLijBejjldIiZ6g5bJaMXylVqWQbblw/CHIgGb0XHtbf7VWelpg6iAzkmg8Wh9/FH3542FDyf5+aUx2GEbhp/6OByUdiDabtbybUoCRNZ0df69xlfXzTbh8L6Ez2B8ZEa6gihx/tIte8+GVli0RZO6ZRiABkA==;
Received: from [106.10.166.63] by nm30.bullet.mail.sg3.yahoo.com with NNFMP; 30 Dec 2014 10:41:06 -0000
Received: from [106.10.151.155] by tm20.bullet.mail.sg3.yahoo.com with NNFMP; 30 Dec 2014 10:41:06 -0000
Received: from [127.0.0.1] by omp1009.mail.sg3.yahoo.com with NNFMP; 30 Dec 2014 10:41:06 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 969516.41274.bm@omp1009.mail.sg3.yahoo.com
X-YMail-OSG: Xhz8PZkVM1mr5H5g61bqC.JkDaZ.ITpto3OqN5jMrgS0_WjyEtHsiqDzvW3huZ5
 vdf1JH3YpNKogkITZ2nrSq4YPlkQloP4DUyW5yYwH1t4CdHdOQoORRYgeNeJglGS8k5_IJrWYyDj
 hV1yudGlCAwWh4bBbQ0LJfVYslrQ0G7bKyryuROyrJG.AHuF9u5BDSOoGlO7abAFkLD2KJVAvQ8x
 hFg9YqA_lWLz1F5CQb69pbvCbXdjzra4UgJSmSVvTWr_KUCjGb5s2pYstLAyaUFbzLVQgFgOU4V9
 4IIQl4AgYS92FFTlwZWnMiURHipVbrQryCjbB5Ik4cE2XAziykpR9SO725QZVmaesGN.HMAbMu_G
 1IXlUVqthvCnlZrIngsHA4JmNTDRHrtntoPTRs41X21Lt7fvuPAjseytBff9xyfLi1o188J4irZm
 EOhc0jmhk.Yrxrv25roeRYlCOc0GpkwrJF_hmk1Q9wpePlyX2ayF1_Squnf3xxQTKap5eeGrxnOx
 uParlDGSVWjfJeEtFslb.T0TybQG07Uolr2p.i1UdHNAIyw.izCNWLHShuQ9FQ..EuUtehKy7cNS
 .ooIoJKh.NLEvZ2TG93ONlFBiO1TM5qSTHgRk56qfStY2cN0bfRzobVR27q7hSQvSJtcT3drzJZk
 4EjMFLyRLoQqI9ZznmcRiIxkwMDGKB8nkjG4jwnf158MedGimeTjEy0nDd3to8cjJu7M4NtXcPJs
 0VlSQ4EUSdwl8.l74Qvlf.UgIayTUmLkmxVthtJtk2aPoVLI5CIgWonvkVM_DJrRfzFH0fBK5YMx
 AgPZ5ae0a21ZKa1MaQhnSRKlOoN8tT5PwbPS8.jTPvJj1lS3S0.3.tGcvsD2.SGw9qNM4uqV7aXf
 MOItYGHxWidGPMss_NoPgV0UXCU_Agj4JRJjFTBGKHO6m9GG5zpbYMCV5WegPC9rK69m6ZZ.VKk_
 XP5oZFkRTMDyvk6zQFkJp2jWOtv8qRtym7eFGVuzW66KOmomD2Vzn3WNdZpBlAXPJqfFp2kK2xOm
 8U7RyLH2TL4I4dw3I_TRFs.2gskSmxi6IieU1fTdkdVWdUMzL_uUlgMzLDSIO6O3pa7XDHV3WzI7
 oM4noE1PQtNEAo62CXu1f_C6rJDO5jLn2XKtthKsH_JcW3peY_YJOIJws3yYqJjjWEY4WNbuS1xj
 IHrTxzrvm
Received: by 106.10.196.180; Tue, 30 Dec 2014 10:41:06 +0000 
Date: Tue, 30 Dec 2014 10:41:06 +0000 (UTC)
From: MEETHU MATHEW <meethu2006@yahoo.co.in>
Reply-To: MEETHU MATHEW <meethu2006@yahoo.co.in>
To: danqing0703 <danqing0703@berkeley.edu>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <2013574208.1850987.1419936066223.JavaMail.yahoo@jws10919.mail.sg3.yahoo.com>
In-Reply-To: <CAG3aN57J1vA5WG1rnjShH80DzoYgUqgu0mOp+vK5x3ArfRqbcg@mail.gmail.com>
References: <CAG3aN57J1vA5WG1rnjShH80DzoYgUqgu0mOp+vK5x3ArfRqbcg@mail.gmail.com>
Subject: Re: Problems concerning implementing machine learning algorithm
 from scratch based on Spark
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_1850986_1026963291.1419936066219"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_1850986_1026963291.1419936066219
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,
The GMMSpark.py you mentioned is the old one.The new code is now added to s=
park-packages and is available at=C2=A0http://spark-packages.org/package/11=
=C2=A0.=C2=A0Have a look at the new code.
We have used numpy functions in our code and didnt notice any slowdown beca=
use of this.=C2=A0Thanks & Regards,
Meethu M=20

     On Tuesday, 30 December 2014 11:50 AM, danqing0703 <danqing0703@berkel=
ey.edu> wrote:
  =20

 Hi all,

I am trying to use some machine learning algorithms that are not included
in the Mllib. Like Mixture Model and LDA(Latent Dirichlet Allocation), and
I am using pyspark and Spark SQL.

My problem is: I have some scripts that implement these algorithms, but I
am not sure which part I shall change to make it fit into Big Data.

=C2=A0 - Like some very simple calculation may take much time if data is to=
o
=C2=A0 big,but also constructing RDD or SQLContext table takes too much tim=
e. I am
=C2=A0 really not sure if I shall use map(), reduce() every time I need to =
make
=C2=A0 calculation.
=C2=A0 - Also, there are some matrix/array level calculation that can not b=
e
=C2=A0 implemented easily merely using map(),reduce(), thus functions of th=
e Numpy
=C2=A0 package shall be used. I am not sure when data is too big, and we si=
mply
=C2=A0 use the numpy functions. Will it take too much time?

I have found some scripts that are not from Mllib and was created by other
developers(credits to Meethu Mathew from Flytxt, thanks for giving me
insights!:))

Many thanks and look forward to getting feedbacks!

Best, Danqing


GMMSpark.py (7K) <http://apache-spark-developers-list.1001551.n3.nabble.com=
/attachment/9964/0/GMMSpark.py>




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Problems-concerning-implementing-machine-learning-algorithm-fr=
om-scratch-based-on-Spark-tp9964.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

   
------=_Part_1850986_1026963291.1419936066219--

From dev-return-10977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 13:14:22 2014
Return-Path: <dev-return-10977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 85FEBCA9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 13:14:22 +0000 (UTC)
Received: (qmail 80945 invoked by uid 500); 30 Dec 2014 13:14:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80867 invoked by uid 500); 30 Dec 2014 13:14:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80855 invoked by uid 99); 30 Dec 2014 13:14:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 13:14:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kavale.rahul@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 13:13:51 +0000
Received: by mail-we0-f182.google.com with SMTP id w62so986372wes.13
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 05:13:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=oRjGdXSTJ9804NbYx7Q+z17d+ST/uMaFfriLG320+74=;
        b=aIjk1WXlhQ8G9tyT1aHpvGUWAuxOegdwo12sFx9JrZiJW3r+6nu3cxl89sw5pR3bXl
         y457dp+erjn7kFssYYqfXTdhBKLcmCljTnveWLJZy0B6Jrq73k7vmElONu0b6O4UjhKL
         spTnLw4IxZAO64NnVvsY/GlP1HqfMnOZEhaHEUga858dmvMjuxHGdF5XlvfeKZy6MaaZ
         bCgpcONdFa33dSnwv4SWgPL1yf2PbgqMFINhB/ZR71OsusAtaYPbeIF51mtCxs5YcGSG
         S3AFVLQkWTxb6EcRpi8yPHt+ZnhyWKl6e3b3+5woeU4BO87NYgmcP7vVMw2ZOOkoJCBP
         T10Q==
MIME-Version: 1.0
X-Received: by 10.181.29.198 with SMTP id jy6mr93230824wid.0.1419945185783;
 Tue, 30 Dec 2014 05:13:05 -0800 (PST)
Received: by 10.216.123.140 with HTTP; Tue, 30 Dec 2014 05:13:05 -0800 (PST)
Date: Tue, 30 Dec 2014 18:43:05 +0530
Message-ID: <CA+ZFnvtZarcaNn5k2x=6M6fLn4fpaW0vjbbzEKQWAfs6qLUwSA@mail.gmail.com>
Subject: [Documentation] Adding external blog to documentation
From: Rahul Kavale <kavale.rahul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c34694663649050b6ec017
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c34694663649050b6ec017
Content-Type: text/plain; charset=UTF-8

Hi all,
I recently wrote a blog comparing MapReduce model with that of Apache Spark
trying to explain some important question I think a beginner might have
while exploring Spark.
The blog can be found here:
http://rahulkavale.github.io/blog/2014/11/16/scrap-your-map-reduce/

The blog received quite good audience and I think it might help people who
are just starting to explore Spark and might help them to get their
question cleared regarding comparison of MapReduce model with that of Spark.

I would like to reach it to a broader audience by including it in official
documentation at "External blogs" section at
https://spark.apache.org/documentation.html. I would really like to get
your thoughts on the same.

Thanks & Regards,
Rahul Kavale

--001a11c34694663649050b6ec017--

From dev-return-10978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 14:33:44 2014
Return-Path: <dev-return-10978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28940CC9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 14:33:44 +0000 (UTC)
Received: (qmail 70352 invoked by uid 500); 30 Dec 2014 14:33:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70262 invoked by uid 500); 30 Dec 2014 14:33:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70244 invoked by uid 99); 30 Dec 2014 14:33:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 14:33:41 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of eshioji@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 14:33:15 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id A4E38F11771
	for <dev@spark.apache.org>; Tue, 30 Dec 2014 06:32:43 -0800 (PST)
Date: Tue, 30 Dec 2014 07:32:43 -0700 (MST)
From: eshioji <eshioji@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419949963646-9968.post@n3.nabble.com>
In-Reply-To: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
References: <CAMc-71mSEY2dt0rudpPfNK-H8PD6Ne3d2H47BWtDsDYEBde1_g@mail.gmail.com>
Subject: Re: Registering custom metrics
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Did you find a way to do this / working on this?
Am trying to find a way to do this as well, but haven't been able to find a
way.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-custom-metrics-tp9030p9968.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 20:25:33 2014
Return-Path: <dev-return-10979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C6AB10B21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 20:25:33 +0000 (UTC)
Received: (qmail 57796 invoked by uid 500); 30 Dec 2014 20:25:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57711 invoked by uid 500); 30 Dec 2014 20:25:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57693 invoked by uid 99); 30 Dec 2014 20:25:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 20:25:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 20:25:26 +0000
Received: by mail-ob0-f180.google.com with SMTP id wp4so46154499obc.11
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 12:24:45 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=YnwQEsFM8yU1BseiUCf3zvuqbQ5CkDPgMO82C0oqR1Q=;
        b=aUVQmOvHWfwmK4vcLlaxGR+mFDVVVcx9AN1NsJeJCA7S/26Onu+pwrfw5K/5TerM3W
         WVQJRePLtrKGeaoGb6vO6jb8S30a4y9Ct8UkFiEu9ufxdGY5WgONPAMEKyL5hBMrYo4T
         EGzwHzrse9T+HIWCLFk5oKrIDN1ApGi2GTsGgRCJt3TQyex6tNG/ibjkHaJ+OB4ZPzDe
         w4pMRzzvHQ/iMfuxJ87oYmulNjwkiIa/6/jMuh03yBuBvzi3mDhonXKvei955KffqsYo
         CymSjppDkOB3rVAz60c48+87rgRm1Vw3H7zpDtfuQALeKWlsPHlwRweoBhq4gaCaHno6
         9IBQ==
X-Gm-Message-State: ALoCoQnHRjt7JqI7M86h3GFO967mSvJeL46V/1oq2jq1EmlCUqxrnQ3MV28mlA2XNT7296CV2ctw
MIME-Version: 1.0
X-Received: by 10.183.3.36 with SMTP id bt4mr37226533obd.44.1419971085419;
 Tue, 30 Dec 2014 12:24:45 -0800 (PST)
Received: by 10.76.110.231 with HTTP; Tue, 30 Dec 2014 12:24:45 -0800 (PST)
Date: Tue, 30 Dec 2014 14:24:45 -0600
Message-ID: <CAKWX9VWNdanon9PMA=OiP25mAKOWjMa6Hg6bw141Htbx_b6sCw@mail.gmail.com>
Subject: Is there any way to tell if compute is being called from a retry?
From: Cody Koeninger <cody@koeninger.org>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113493d2235f4b050b74c8a4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113493d2235f4b050b74c8a4
Content-Type: text/plain; charset=UTF-8

It looks like taskContext.attemptId doesn't mean what one thinks it might
mean, based on

http://apache-spark-developers-list.1001551.n3.nabble.com/Get-attempt-number-in-a-closure-td8853.html

and the unresolved

https://issues.apache.org/jira/browse/SPARK-4014



Is there any alternative way to tell if compute is being called from a
retry?  Barring that, does anyone have any tips on how it might be possible
to get the attempt count propagated to executors?

It would be extremely useful for the kafka rdd preferred location awareness.

--001a113493d2235f4b050b74c8a4--

From dev-return-10980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Dec 30 22:14:55 2014
Return-Path: <dev-return-10980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 444DE10EC5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 30 Dec 2014 22:14:55 +0000 (UTC)
Received: (qmail 53164 invoked by uid 500); 30 Dec 2014 22:14:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53076 invoked by uid 500); 30 Dec 2014 22:14:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52999 invoked by uid 99); 30 Dec 2014 22:14:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 22:14:52 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 30 Dec 2014 22:14:48 +0000
Received: by mail-pd0-f176.google.com with SMTP id r10so19853802pdi.35
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 14:13:43 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=d1kWMr8ZdtON+NF9ZuWlRV74qxETcVRTnFVEwULcbzk=;
        b=Kkc0R9DeghZoy9uE46Nb+hjBBQKM322DJvvOnRdaKn1Vc6SUGI3YMK3DXUrBmekZqE
         8jvvuLbAqrL/7wMxMSwfunCIVxgofNw8hKh78G3aIQadwzr+UoIp12HSo4G1UjLHKEUj
         9CC9pHPF/PgWvsf8AfqR9CIpp1nT0gMZQQaN1Lfs/VSUDHvLtjHsG2eilH2evpbtDpdC
         o4YvQ3yWvuGuw4WTu/YQ9lxP1myyEArAFKM7P44Lbi9wfmQTD/QEoVUR478/s845/AAZ
         a4Tz15hVniJd8Z76t93pPriVqVmo4pXVF8jHcp/L8oX6Caxwpm+Et1vrGK4fUETm6byN
         hXkQ==
MIME-Version: 1.0
X-Received: by 10.66.224.72 with SMTP id ra8mr100901276pac.43.1419977623478;
 Tue, 30 Dec 2014 14:13:43 -0800 (PST)
Received: by 10.70.41.80 with HTTP; Tue, 30 Dec 2014 14:13:43 -0800 (PST)
In-Reply-To: <CAKWX9VWNdanon9PMA=OiP25mAKOWjMa6Hg6bw141Htbx_b6sCw@mail.gmail.com>
References: <CAKWX9VWNdanon9PMA=OiP25mAKOWjMa6Hg6bw141Htbx_b6sCw@mail.gmail.com>
Date: Tue, 30 Dec 2014 14:13:43 -0800
Message-ID: <CAOEPXP5EF2B5HSj6Lrs1gsAvLxnBW_JoVkhm7vcnmhFH4HT_hQ@mail.gmail.com>
Subject: Re: Is there any way to tell if compute is being called from a retry?
From: Josh Rosen <rosenville@gmail.com>
To: Cody Koeninger <cody@koeninger.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8ffba11bd62790050b764dec
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ffba11bd62790050b764dec
Content-Type: text/plain; charset=UTF-8

This is timely, since I just ran into this issue myself while trying to
write a test to reproduce a bug related to speculative execution (I wanted
to configure a job so that the first attempt to compute a partition would
run slow so that a second, fast speculative copy would be launched).

I've opened a PR with a proposed fix:
https://github.com/apache/spark/pull/3849



On Tue, Dec 30, 2014 at 12:24 PM, Cody Koeninger <cody@koeninger.org> wrote:

> It looks like taskContext.attemptId doesn't mean what one thinks it might
> mean, based on
>
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Get-attempt-number-in-a-closure-td8853.html
>
> and the unresolved
>
> https://issues.apache.org/jira/browse/SPARK-4014
>
>
>
> Is there any alternative way to tell if compute is being called from a
> retry?  Barring that, does anyone have any tips on how it might be possible
> to get the attempt count propagated to executors?
>
> It would be extremely useful for the kafka rdd preferred location
> awareness.
>

--e89a8ffba11bd62790050b764dec--

From dev-return-10981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 00:41:17 2014
Return-Path: <dev-return-10981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 002B510619
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 00:41:17 +0000 (UTC)
Received: (qmail 56855 invoked by uid 500); 31 Dec 2014 00:41:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56773 invoked by uid 500); 31 Dec 2014 00:41:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56742 invoked by uid 99); 31 Dec 2014 00:41:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 00:41:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 00:41:11 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so46820510obc.6
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 16:39:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=cOm1bhTsQsnEJrvW3ASLdgeHNt+puu60LqW9BAqvkoc=;
        b=GO19oaccLMJaVMCJh55QsRGlzPZx7YgDNUbaQ/kdTIHzi+/P5HPhOFX4v/z91I0bj2
         1jP5QqeKwYZLV/t1ENn3QAk/LW9EzvA+WKNmGji0tEjkRwQhFi8e+/+91oaTUVPtGHle
         zy2i+rl7FtO75lStDNaRd/IhGi25lym6arlXR7/S/HqMz4ZVmd3+qYvQJG/x4jrAjlWJ
         g0EDXbYQg0sOQGHvMCP2HVgJnnxgrxmadvnQgmUX8x+qvZjY+5VM4fi0Ccd6s8u+kLjb
         heSGsmktKCaIgi+yYWcmcWUN2M8+62cp+39+sFO2ykbsvmwWLSKD5w6914JU1yXey0qD
         6qWQ==
X-Received: by 10.202.93.135 with SMTP id r129mr35463630oib.53.1419986360332;
 Tue, 30 Dec 2014 16:39:20 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Tue, 30 Dec 2014 16:39:00 -0800 (PST)
In-Reply-To: <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
 <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
 <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com> <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Tue, 30 Dec 2014 16:39:00 -0800
Message-ID: <CAJc_syL8C++DHQFj_HAYGmO9Xng0kvswpi+V7sH8vSyEYdVJQw@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d4a9697fa99050b78567d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d4a9697fa99050b78567d
Content-Type: text/plain; charset=UTF-8

Gents,

I tried #3820. It doesn't work. I'm still getting the following exceptions:

Exception in thread "Thread-45" java.lang.RuntimeException: Unsupported
datatype DateType
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:343)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:292)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:291)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:363)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:362)

I would more than happy to fix this myself, but I would need some help
wading through the code. Could anyone explain to me what exactly is needed
to support a new data type in SparkSQL's Parquet storage engine?

Thanks.

Alex

On Mon, Dec 29, 2014 at 10:20 PM, Wang, Daoyuan <daoyuan.wang@intel.com>
wrote:

>  By adding a flag in SQLContext, I have modified #3822 to include
> nanoseconds now. Since passing too many flags is ugly, now I need the whole
> SQLContext, so that we can put more flags there.
>
>
>
> Thanks,
>
> Daoyuan
>
>
>
> *From:* Michael Armbrust [mailto:michael@databricks.com]
> *Sent:* Tuesday, December 30, 2014 10:43 AM
> *To:* Alessandro Baretta
> *Cc:* Wang, Daoyuan; dev@spark.apache.org
> *Subject:* Re: Unsupported Catalyst types in Parquet
>
>
>
> Yeah, I saw those.  The problem is that #3822 truncates timestamps that
> include nanoseconds.
>
>
>
> On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <alexbaretta@gmail.com>
> wrote:
>
> Michael,
>
>
>
> Actually, Adrian Wang already created pull requests for these issues.
>
>
>
> https://github.com/apache/spark/pull/3820
>
> https://github.com/apache/spark/pull/3822
>
>
>
> What do you think?
>
>
>
> Alex
>
>
>
> On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
> I'd love to get both of these in.  There is some trickiness that I talk
> about on the JIRA for timestamps since the SQL timestamp class can support
> nano seconds and I don't think parquet has a type for this.  Other systems
> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
> mailing list what the plan is there to make sure that whatever we do is
> going to be compatible long term.
>
>
>
> Michael
>
>
>
> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <alexbaretta@gmail.com>
> wrote:
>
> Daoyuan,
>
> Thanks for creating the jiras. I need these features by... last week, so
> I'd be happy to take care of this myself, if only you or someone more
> experienced than me in the SparkSQL codebase could provide some guidance.
>
> Alex
>
> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:
>
> Hi Alex,
>
> I'll create JIRA SPARK-4985 for date type support in parquet, and
> SPARK-4987 for timestamp type support. For decimal type, I think we only
> support decimals that fits in a long.
>
> Thanks,
> Daoyuan
>
> -----Original Message-----
> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
> Sent: Saturday, December 27, 2014 2:47 PM
> To: dev@spark.apache.org; Michael Armbrust
> Subject: Unsupported Catalyst types in Parquet
>
> Michael,
>
> I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
> due to my RDDs having having DateType and DecimalType fields. What would it
> take to add Parquet support for these Catalyst? Are there any other
> Catalyst types for which there is no Catalyst support?
>
> Alex
>
>
>
>
>
>
>

--001a113d4a9697fa99050b78567d--

From dev-return-10982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 01:22:48 2014
Return-Path: <dev-return-10982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5EBFD10788
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 01:22:48 +0000 (UTC)
Received: (qmail 4477 invoked by uid 500); 31 Dec 2014 01:22:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4399 invoked by uid 500); 31 Dec 2014 01:22:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4373 invoked by uid 99); 31 Dec 2014 01:22:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:22:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:22:20 +0000
Received: by mail-ob0-f174.google.com with SMTP id nt9so51824565obb.5
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 17:22:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=MaeQFAr4q0Fl8P7R6izuj919YrXniEqvIGjfX8lDvoE=;
        b=FATFvKFFdqdtdIgPao9o+sNGi/xl7gi4WcK4DiXa2zBJS7vr4IvnDj66dmDr5jyBSV
         w5Lpuli5HXmRVJjwSqRPk7f/1H/pui9QE/dZ0ziBHrhCTl5o9J/shfYPpiYQ9wlTHMK+
         5YHdm7p1ERvqpu8HikyngV8o9miopFdN7Qo7E52hh/MSMNUTD5hlOFGZZikiKwUtmaVS
         +S/ekZyHMTJ2YufHNM/U5/MnClNkGWYd6y4D0mj5EDqwuEr2Ooe93PgMGC6NJsFOf4Hi
         w2Dtnsvq0XLRAWAOTuXwyZ6ySdUKwa/DRwpFFJch4l4SK5gulazo+wdKJhJ4ifMzFUAZ
         ZLiw==
X-Received: by 10.60.173.173 with SMTP id bl13mr37556741oec.8.1419988939036;
 Tue, 30 Dec 2014 17:22:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Tue, 30 Dec 2014 17:21:58 -0800 (PST)
In-Reply-To: <CAJc_syL8C++DHQFj_HAYGmO9Xng0kvswpi+V7sH8vSyEYdVJQw@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
 <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
 <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com> <CAJc_syL8C++DHQFj_HAYGmO9Xng0kvswpi+V7sH8vSyEYdVJQw@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Tue, 30 Dec 2014 17:21:58 -0800
Message-ID: <CAJc_syL7A-A7HZJM9TZgBD9_0CDt6X7xUxVc118gcJW6yHb4ow@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01184cfa4bddb2050b78f0e9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184cfa4bddb2050b78f0e9
Content-Type: text/plain; charset=UTF-8

Sorry! My bad. I had stale spark jars sitting on the slave nodes...

Alex

On Tue, Dec 30, 2014 at 4:39 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Gents,
>
> I tried #3820. It doesn't work. I'm still getting the following exceptions:
>
> Exception in thread "Thread-45" java.lang.RuntimeException: Unsupported
> datatype DateType
>         at scala.sys.package$.error(package.scala:27)
>         at
> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:343)
>         at
> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:292)
>         at scala.Option.getOrElse(Option.scala:120)
>         at
> org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:291)
>         at
> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:363)
>         at
> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:362)
>
> I would more than happy to fix this myself, but I would need some help
> wading through the code. Could anyone explain to me what exactly is needed
> to support a new data type in SparkSQL's Parquet storage engine?
>
> Thanks.
>
> Alex
>
> On Mon, Dec 29, 2014 at 10:20 PM, Wang, Daoyuan <daoyuan.wang@intel.com>
> wrote:
>
>>  By adding a flag in SQLContext, I have modified #3822 to include
>> nanoseconds now. Since passing too many flags is ugly, now I need the whole
>> SQLContext, so that we can put more flags there.
>>
>>
>>
>> Thanks,
>>
>> Daoyuan
>>
>>
>>
>> *From:* Michael Armbrust [mailto:michael@databricks.com]
>> *Sent:* Tuesday, December 30, 2014 10:43 AM
>> *To:* Alessandro Baretta
>> *Cc:* Wang, Daoyuan; dev@spark.apache.org
>> *Subject:* Re: Unsupported Catalyst types in Parquet
>>
>>
>>
>> Yeah, I saw those.  The problem is that #3822 truncates timestamps that
>> include nanoseconds.
>>
>>
>>
>> On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>> Michael,
>>
>>
>>
>> Actually, Adrian Wang already created pull requests for these issues.
>>
>>
>>
>> https://github.com/apache/spark/pull/3820
>>
>> https://github.com/apache/spark/pull/3822
>>
>>
>>
>> What do you think?
>>
>>
>>
>> Alex
>>
>>
>>
>> On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <michael@databricks.com>
>> wrote:
>>
>> I'd love to get both of these in.  There is some trickiness that I talk
>> about on the JIRA for timestamps since the SQL timestamp class can support
>> nano seconds and I don't think parquet has a type for this.  Other systems
>> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
>> mailing list what the plan is there to make sure that whatever we do is
>> going to be compatible long term.
>>
>>
>>
>> Michael
>>
>>
>>
>> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>> Daoyuan,
>>
>> Thanks for creating the jiras. I need these features by... last week, so
>> I'd be happy to take care of this myself, if only you or someone more
>> experienced than me in the SparkSQL codebase could provide some guidance.
>>
>> Alex
>>
>> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com> wrote:
>>
>> Hi Alex,
>>
>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>> support decimals that fits in a long.
>>
>> Thanks,
>> Daoyuan
>>
>> -----Original Message-----
>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>> Sent: Saturday, December 27, 2014 2:47 PM
>> To: dev@spark.apache.org; Michael Armbrust
>> Subject: Unsupported Catalyst types in Parquet
>>
>> Michael,
>>
>> I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
>> due to my RDDs having having DateType and DecimalType fields. What would it
>> take to add Parquet support for these Catalyst? Are there any other
>> Catalyst types for which there is no Catalyst support?
>>
>> Alex
>>
>>
>>
>>
>>
>>
>>
>
>

--089e01184cfa4bddb2050b78f0e9--

From dev-return-10983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 01:41:09 2014
Return-Path: <dev-return-10983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62F9D10837
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 01:41:09 +0000 (UTC)
Received: (qmail 33394 invoked by uid 500); 31 Dec 2014 01:41:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33318 invoked by uid 500); 31 Dec 2014 01:41:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33306 invoked by uid 99); 31 Dec 2014 01:41:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:41:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.54] (HELO mail-la0-f54.google.com) (209.85.215.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:41:02 +0000
Received: by mail-la0-f54.google.com with SMTP id pv20so13337950lab.13
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 17:39:36 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type
         :content-transfer-encoding;
        bh=m5cyxWiyQbnRfe2PNiRPDoqoHgTotm5GHB3h1fUpJ4o=;
        b=QQNZa6ZGZQCxOwaZZW4LSv5zIFg07Y19LNHUZhj1YebkzPE/OUMNBWsAmYKESw5I/0
         Fk2rA0DhGn2LkZDu8ljmOHbHI+qWKl/zxuWmU4gxidzjJ6kzRxeasOq4KEDY9W9DGWFE
         fTQlhzMr0yegaHZUmHkXNPDyZ49nwpf1zk2sL10pxWvb34nfDiu1rv44PIOmzBcNi/qh
         3U4nFjbj/Bse9VbRaKkx5twSdzdLNy7g7Wci84y/LYM4k0iG8o6x02e08a7/ox6Tf6tq
         XAsescj/WHeYd7h1LfJi2EiEfVdER5jmYgsb6H10tynpkabPOzMML0V9rd+mUeaKcVtW
         wLrw==
X-Gm-Message-State: ALoCoQl+GwVSQSRIquVpcLq0DkcsxHGohnYbR2dgE3tNojiax1NcpjD68uhJKoRMPqldta/EtITy
MIME-Version: 1.0
X-Received: by 10.112.62.226 with SMTP id b2mr64535510lbs.44.1419989976194;
 Tue, 30 Dec 2014 17:39:36 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Tue, 30 Dec 2014 17:39:36 -0800 (PST)
In-Reply-To: <1A2835E9-B0E1-484B-8641-7C035CD96B19@gmail.com>
References: <CACkSZy3=MTUe1g-ND343aiNJh0NH0Tt8+GXt8RQ4rDroBpiR4w@mail.gmail.com>
	<1A2835E9-B0E1-484B-8641-7C035CD96B19@gmail.com>
Date: Tue, 30 Dec 2014 17:39:36 -0800
Message-ID: <CA+2Pv=hCCPDaunJUiAoaQhJVZxx0FQdXmOgUii6ksL7Mn46Hhw@mail.gmail.com>
Subject: Re: Adding third party jars to classpath used by pyspark
From: Davies Liu <davies@databricks.com>
To: Jeremy Freeman <freeman.jeremy@gmail.com>
Cc: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Dec 29, 2014 at 7:39 PM, Jeremy Freeman
<freeman.jeremy@gmail.com> wrote:
> Hi Stephen, it should be enough to include
>
>> --jars /path/to/file.jar
>
> in the command line call to either pyspark or spark-submit, as in
>
>> spark-submit --master local --jars /path/to/file.jar myfile.py

Unfortunately, you also need '--driver-class-path /path/to/file.jar'
to make it accessible in driver. (This may be fixed in 1.3).

> and you can check the bottom of the Web UI=E2=80=99s =E2=80=9CEnvironment=
" tab to make sure the jar gets on your classpath. Let me know if you still=
 see errors related to this.
>
> =E2=80=94 Jeremy
>
> -------------------------
> jeremyfreeman.net
> @thefreemanlab
>
> On Dec 29, 2014, at 7:55 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
>> What is the recommended way to do this?  We have some native database
>> client libraries for which we are adding pyspark bindings.
>>
>> The pyspark invokes spark-submit.   Do we add our libraries to
>> the SPARK_SUBMIT_LIBRARY_PATH ?
>>
>> This issue relates back to an error we have been seeing "Py4jError: Tryi=
ng
>> to call a package" - the suspicion being that the third party libraries =
may
>> not be available on the jvm side.
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 01:41:46 2014
Return-Path: <dev-return-10984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B946B1083A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 01:41:46 +0000 (UTC)
Received: (qmail 36158 invoked by uid 500); 31 Dec 2014 01:41:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36073 invoked by uid 500); 31 Dec 2014 01:41:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36061 invoked by uid 99); 31 Dec 2014 01:41:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:41:45 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 01:41:40 +0000
Received: by mail-la0-f43.google.com with SMTP id s18so13181969lam.2
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 17:40:14 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=eGWSwQqkl4k43B3mwrjke1diTL7uU3yUlpZntOzCK9o=;
        b=J7OQ5gSzV8SkFjYDgaTAOg6eVOxY+3lRyc6VQB9AyldxqHb95GHJclXCG8oG15IEO9
         u+CZx9bruUpzq7pf+2Id1nUu/iKfMe0Trxs4o6xcNQ1otNDPyDbxdmVfKEYWxBX+dyO9
         pq8kbgS/6qXM/R7JeJa8v7LFIdGENCoxNHYqXYO2QNj6+u+U1aT3G6lzVKDjCO+IvJQd
         bFT43ZSLHS48H/W4tIYASMBcrkqjVWkf3Z2uYl+bmcUoI2eh3iwiPNgXAvEZWKLVFHJm
         dGQ2K141EwG2BAA5jTVgZ6av5bYDyZeVTupgyyVaOm3ulsTZcXr6xdkQw7BqjuVR6w73
         48CA==
X-Gm-Message-State: ALoCoQkdDiIFgiBvDveyBGqnlVX7aYubLJMeiuDOUpg0Frh5p3G20TAy3OSzK51RuAOWxWo2r7bt
MIME-Version: 1.0
X-Received: by 10.112.160.104 with SMTP id xj8mr64589799lbb.62.1419990014538;
 Tue, 30 Dec 2014 17:40:14 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Tue, 30 Dec 2014 17:40:14 -0800 (PST)
In-Reply-To: <1419912249750-9961.post@n3.nabble.com>
References: <1419912249750-9961.post@n3.nabble.com>
Date: Tue, 30 Dec 2014 17:40:14 -0800
Message-ID: <CA+2Pv=hfa9GUONAgTrWxZkbZssEH+8zq4apjpfmfMP+2aBMXbw@mail.gmail.com>
Subject: Re: Help, pyspark.sql.List flatMap results become tuple
From: Davies Liu <davies@databricks.com>
To: guoxu1231 <guoxu1231@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This should be fixed in 1.2, could you try it?

On Mon, Dec 29, 2014 at 8:04 PM, guoxu1231 <guoxu1231@gmail.com> wrote:
> Hi pyspark guys,
>
> I have a json file, and its struct like below:
>
> {"NAME":"George", "AGE":35, "ADD_ID":1212, "POSTAL_AREA":1,
> "TIME_ZONE_ID":1, "INTEREST":[{"INTEREST_NO":1, "INFO":"x"},
> {"INTEREST_NO":2, "INFO":"y"}]}
> {"NAME":"John", "AGE":45, "ADD_ID":1213, "POSTAL_AREA":1, "TIME_ZONE_ID":1,
> "INTEREST":[{"INTEREST_NO":2, "INFO":"x"}, {"INTEREST_NO":3, "INFO":"y"}]}
>
> I'm using spark sql api to manipulate the json data in pyspark shell,
>
> *sqlContext = SQLContext(sc)
> A400= sqlContext.jsonFile('jason_file_path')*
> /Row(ADD_ID=1212, AGE=35, INTEREST=[Row(INFO=u'x', INTEREST_NO=1),
> Row(INFO=u'y', INTEREST_NO=2)], NAME=u'George', POSTAL_AREA=1,
> TIME_ZONE_ID=1)
> Row(ADD_ID=1213, AGE=45, INTEREST=[Row(INFO=u'x', INTEREST_NO=2),
> Row(INFO=u'y', INTEREST_NO=3)], NAME=u'John', POSTAL_AREA=1,
> TIME_ZONE_ID=1)/
> *X = A400.flatMap(lambda i: i.INTEREST)*
> The flatMap results like below, each element in json array were flatten to
> tuple, not my expected  pyspark.sql.Row. I can only access the flatten
> results by index. but it supposed to be flatten to Row(namedTuple) and
> support to access by name.
> (u'x', 1)
> (u'y', 2)
> (u'x', 2)
> (u'y', 3)
>
> My spark version is 1.1.
>
>
>
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Help-pyspark-sql-List-flatMap-results-become-tuple-tp9961.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 03:25:41 2014
Return-Path: <dev-return-10985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9693810A57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 03:25:41 +0000 (UTC)
Received: (qmail 61213 invoked by uid 500); 31 Dec 2014 03:25:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61132 invoked by uid 500); 31 Dec 2014 03:25:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61121 invoked by uid 99); 31 Dec 2014 03:25:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 03:25:39 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of guoxu1231@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 03:25:33 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 6DAA1F1FF2B
	for <dev@spark.apache.org>; Tue, 30 Dec 2014 19:24:43 -0800 (PST)
Date: Tue, 30 Dec 2014 20:24:42 -0700 (MST)
From: guoxu1231 <guoxu1231@gmail.com>
To: dev@spark.apache.org
Message-ID: <1419996282970-9975.post@n3.nabble.com>
In-Reply-To: <CA+2Pv=hfa9GUONAgTrWxZkbZssEH+8zq4apjpfmfMP+2aBMXbw@mail.gmail.com>
References: <1419912249750-9961.post@n3.nabble.com> <CA+2Pv=hfa9GUONAgTrWxZkbZssEH+8zq4apjpfmfMP+2aBMXbw@mail.gmail.com>
Subject: Re: Help, pyspark.sql.List flatMap results become tuple
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Davies, it works in 1.2. 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Help-pyspark-sql-List-flatMap-results-become-tuple-tp9961p9975.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 04:09:36 2014
Return-Path: <dev-return-10986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E74A310B6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 04:09:35 +0000 (UTC)
Received: (qmail 7204 invoked by uid 500); 31 Dec 2014 04:09:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7133 invoked by uid 500); 31 Dec 2014 04:09:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7111 invoked by uid 99); 31 Dec 2014 04:09:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:09:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zsxwing@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:09:28 +0000
Received: by mail-ie0-f178.google.com with SMTP id vy18so12720664iec.9
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 20:09:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=UFcrk5IiLMKOQTUVSJKSNSCbp+NQfc55+CrQykSevog=;
        b=nf/xUjGs8rsi2JjWmOhhO3E9t68cNI6b2i4hVr83VVk+BM0X8xcT8dUnfvHovs8IcN
         9cBimZ1dOuH4pc45yHqaynah9SvTQhjzQHkSM0yJ2huFMRubN/N5AtmmzBru+8dUrJ2j
         w3i8h3DGxEinemkrGqfwXgYQlZgr+EWWxN1iNCcmRu5GSBGAkw+P98695rIWUBrJgY05
         2GubtnOG22Hskw4ogWu1qwpTe/Gd6f6GgitjAdz1SKOwZf/Z4B+EAe8DHQwma0PEccKH
         m0j3OWPq0fTp+7gxnjP3CUrYfWwy3zAci22FCzCtp2GD6K6mTUP4pvlu/VZ2sT503hj3
         uoKg==
MIME-Version: 1.0
X-Received: by 10.107.47.157 with SMTP id v29mr57433347iov.59.1419998947362;
 Tue, 30 Dec 2014 20:09:07 -0800 (PST)
Received: by 10.107.165.203 with HTTP; Tue, 30 Dec 2014 20:09:07 -0800 (PST)
Date: Wed, 31 Dec 2014 12:09:07 +0800
Message-ID: <CAPn6-YSCsSgmS5JV=i9KZNrZrfVhxtejWzRexZMXhrBykJ11GA@mail.gmail.com>
Subject: Why the major.minor version of the new hive-exec is 51.0?
From: Shixiong Zhu <zsxwing@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c15c38d6d08e050b7b448c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c15c38d6d08e050b7b448c
Content-Type: text/plain; charset=UTF-8

The major.minor version of the new org.spark-project.hive.hive-exec is
51.0, so it will require people use JDK7. Is it intentional?

<dependency>
<groupId>org.spark-project.hive</groupId>
<artifactId>hive-exec</artifactId>
<version>0.12.0-protobuf-2.5</version>
</dependency>

You can use the following steps to reproduce it (Need to use JDK6):

1. Create a Test.java file with the following content:

public class Test {

    public static void main(String[] args) throws Exception{
       Class.forName("org.apache.hadoop.hive.conf.HiveConf");
    }

}

2. javac Test.java
3. java -classpath
~/.m2/repository/org/spark-project/hive/hive-exec/0.12.0-protobuf-2.5/hive-exec-0.12.0-protobuf-2.5.jar:.
Test

Exception in thread "main" java.lang.UnsupportedClassVersionError:
org/apache/hadoop/hive/conf/HiveConf : Unsupported major.minor version 51.0
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:169)
at Test.main(Test.java:5)


Best Regards,
Shixiong Zhu

--001a11c15c38d6d08e050b7b448c--

From dev-return-10987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 04:27:47 2014
Return-Path: <dev-return-10987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C31D810C05
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 04:27:47 +0000 (UTC)
Received: (qmail 33912 invoked by uid 500); 31 Dec 2014 04:27:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33829 invoked by uid 500); 31 Dec 2014 04:27:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33811 invoked by uid 99); 31 Dec 2014 04:27:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:27:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:27:40 +0000
Received: by mail-qg0-f43.google.com with SMTP id z107so11142075qgd.16
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 20:26:59 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=VzhFIK8KgdGflylgbmb+bpLBRgEdpDS1urPgIZShFOE=;
        b=K9VnvS4xxj3L9wqXzUeGqwn001O0TY+o6UGMZkQwWtwDfJy2jfKBWjUrqZux/HhI4O
         huVe+aYx2rOrLZO89XBHeUCf9Z7vVPeAjldY5Q48ZwnVt1zpTWeIOFwtJKE/G3wofSb+
         d2m+MhGwX085WdwhnKQT7CsDEMvwzpoDSNfngxoMc0yzBAQeo2CFr965BWeKSmeqocBU
         +Chc6WiNgPLMPDmVhaJ9NVJPQac4vlhhRySEGuVgudrv5zS1tkvb99EZXgS6ZvcPiY1u
         8ABEhX7IncgYOkpjGtQtqlrtalS5ZHav2hJfwTaCq58nmsK6l/KEqlCKEhQAQgCbtmPb
         /9Cg==
X-Gm-Message-State: ALoCoQnMt7xYJv7WZd4nJDXSw+IrRbO9kRHuSddTuoquSB2tsU9NBCJNo3wlCQ14EmESKQ3DEC7X
MIME-Version: 1.0
X-Received: by 10.140.81.71 with SMTP id e65mr24204742qgd.82.1420000018804;
 Tue, 30 Dec 2014 20:26:58 -0800 (PST)
Received: by 10.140.91.203 with HTTP; Tue, 30 Dec 2014 20:26:58 -0800 (PST)
Date: Tue, 30 Dec 2014 23:26:58 -0500
Message-ID: <CAPEc=Ju+oR_EwinTnX-t_Dhi4xxpAHsRupEwHL4u-3Lphddahg@mail.gmail.com>
Subject: Sample Spark Program Error
From: Naveen Madhire <vmadhire@umail.iu.edu>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c13194b3cc6a050b7b841a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13194b3cc6a050b7b841a
Content-Type: text/plain; charset=UTF-8

Hi All,

I am trying to run a sample Spark program using Scala SBT,

Below is the program,

def main(args: Array[String]) {

      val logFile = "E:/ApacheSpark/usb/usb/spark/bin/README.md" // Should
be some file on your system
      val sc = new SparkContext("local", "Simple App",
"E:/ApacheSpark/usb/usb/spark/bin",List("target/scala-2.10/sbt2_2.10-1.0.jar"))
      val logData = sc.textFile(logFile, 2).cache()

      val numAs = logData.filter(line => line.contains("a")).count()
      val numBs = logData.filter(line => line.contains("b")).count()

      println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))

    }


Below is the error log,


14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:0+673
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2032) called
with curMem=34047, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_0 stored as values
in memory (estimated size 2032.0 B, free 267.2 MB)
14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on
zealot:61452 (size: 2032.0 B, free: 267.3 MB)
14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
rdd_1_0
14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 0.0
(TID 0). 2300 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
0.0 (TID 1, localhost, PROCESS_LOCAL, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 0.0
(TID 1)
14/12/30 23:20:21 INFO spark.CacheManager: Partition rdd_1_1 not found,
computing it
14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:673+673
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
0.0 (TID 0) in 3507 ms on localhost (1/2)
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(1912) called
with curMem=36079, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_1 stored as values
in memory (estimated size 1912.0 B, free 267.2 MB)
14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_1 in memory on
zealot:61452 (size: 1912.0 B, free: 267.3 MB)
14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
rdd_1_1
14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 0.0
(TID 1). 2300 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
0.0 (TID 1) in 261 ms on localhost (2/2)
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 0 (count at
Test1.scala:19) finished in 3.811 s
14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
Test1.scala:19, took 3.997365232 s
14/12/30 23:20:21 INFO spark.SparkContext: Starting job: count at
Test1.scala:20
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Got job 1 (count at
Test1.scala:20) with 2 output partitions (allowLocal=false)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Final stage: Stage 1(count
at Test1.scala:20)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Parents of final stage:
List()
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Missing parents: List()
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting Stage 1
(FilteredRDD[3] at filter at Test1.scala:20), which has no missing parents
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2600) called
with curMem=37991, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 stored as
values in memory (estimated size 2.5 KB, free 267.2 MB)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks
from Stage 1 (FilteredRDD[3] at filter at Test1.scala:20)
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
with 2 tasks
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
1.0 (TID 2, localhost, ANY, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 0.0 in stage 1.0
(TID 2)
14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_0 locally
14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 1.0
(TID 2). 1731 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
1.0 (TID 3, localhost, ANY, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 1.0
(TID 3)
14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_1 locally
14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 1.0
(TID 3). 1731 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
1.0 (TID 3) in 7 ms on localhost (1/2)
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
1.0 (TID 2) in 16 ms on localhost (2/2)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 1 (count at
Test1.scala:20) finished in 0.016 s
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0,
whose tasks have all completed, from pool
14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
Test1.scala:20, took 0.041709824 s
Lines with a: 24, Lines with b: 15
14/12/30 23:20:21 ERROR util.Utils: Uncaught exception in thread
SparkListenerBus
java.lang.InterruptedException
at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1301)
at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
[success] Total time: 12 s, completed Dec 30, 2014 11:20:21 PM
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)
14/12/30 23:20:21 ERROR spark.ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
at java.lang.Object.wait(Native Method)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
at org.apache.spark.ContextCleaner.org
$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
14/12/30 23:20:21 INFO network.ConnectionManager: Selector thread was
interrupted!
14/12/30 23:20:21 INFO storage.BlockManager: Removing broadcast 2
14/12/30 23:20:21 INFO storage.BlockManager: Removing block broadcast_2
14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 of size 2600
dropped from memory (free 280210984)
14/12/30 23:20:21 INFO spark.ContextCleaner: Cleaned broadcast 2




Please let me know any pointers to debug the error.


Thanks a lot.

--001a11c13194b3cc6a050b7b841a--

From dev-return-10988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 04:41:23 2014
Return-Path: <dev-return-10988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0E1B10C2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 04:41:23 +0000 (UTC)
Received: (qmail 40764 invoked by uid 500); 31 Dec 2014 04:41:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40685 invoked by uid 500); 31 Dec 2014 04:41:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40671 invoked by uid 99); 31 Dec 2014 04:41:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:41:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.42 as permitted sender)
Received: from [209.85.213.42] (HELO mail-yh0-f42.google.com) (209.85.213.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 04:41:16 +0000
Received: by mail-yh0-f42.google.com with SMTP id v1so7716258yhn.15
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 20:40:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lTIkUNp3M3PSBNNJ3cdaKJkY+ltyRfAcjOp0kte2TeQ=;
        b=JZ518XWTJHL+dT+FPYf9lzcHnWLTc9l1mKFnIa5ys3tIZTZpG7ypXK/SDQmFawwvp1
         S/LAQu+D7J1ia1LPGWbFJmn3aDOqem90OYO12KyBUdcnB2M/xaLq+qIaO1Sy3NWZUiRm
         BsUp3sxDck4IHmWRAI4UnzAaAXQFl24F6JWmL1GGpH/oBLMAYrKDiiIKK2WJ71NYj0Ku
         7r3UhGBGdIsT8+Z/Di6h/PN/3T9/1Wpil4C8BLnPtZy+YHsnI9xLe/+FGET0ko2Q+DaJ
         04tWu7zQMWYqXxz+HReSNhT+khYag8qKT8MK0X7Gqk/AnGOPNyY05NuaOf+uqud1iypN
         ljJA==
MIME-Version: 1.0
X-Received: by 10.236.32.8 with SMTP id n8mr43269211yha.74.1420000811047; Tue,
 30 Dec 2014 20:40:11 -0800 (PST)
Received: by 10.170.139.4 with HTTP; Tue, 30 Dec 2014 20:40:10 -0800 (PST)
In-Reply-To: <CAPn6-YSCsSgmS5JV=i9KZNrZrfVhxtejWzRexZMXhrBykJ11GA@mail.gmail.com>
References: <CAPn6-YSCsSgmS5JV=i9KZNrZrfVhxtejWzRexZMXhrBykJ11GA@mail.gmail.com>
Date: Tue, 30 Dec 2014 20:40:10 -0800
Message-ID: <CALte62xcLeVi3HDisu-mLFTxzHyFOa8QB_34n=GeqdCt5svrbg@mail.gmail.com>
Subject: Re: Why the major.minor version of the new hive-exec is 51.0?
From: Ted Yu <yuzhihong@gmail.com>
To: Shixiong Zhu <zsxwing@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1c134ec63dd050b7bb3b2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c134ec63dd050b7bb3b2
Content-Type: text/plain; charset=UTF-8

I extracted org/apache/hadoop/hive/common/CompressionUtils.class from the
jar and used hexdump to view the class file.
Bytes 6 and 7 are 00 and 33, respectively.

According to http://en.wikipedia.org/wiki/Java_class_file, the jar was
produced using Java 7.

FYI

On Tue, Dec 30, 2014 at 8:09 PM, Shixiong Zhu <zsxwing@gmail.com> wrote:

> The major.minor version of the new org.spark-project.hive.hive-exec is
> 51.0, so it will require people use JDK7. Is it intentional?
>
> <dependency>
> <groupId>org.spark-project.hive</groupId>
> <artifactId>hive-exec</artifactId>
> <version>0.12.0-protobuf-2.5</version>
> </dependency>
>
> You can use the following steps to reproduce it (Need to use JDK6):
>
> 1. Create a Test.java file with the following content:
>
> public class Test {
>
>     public static void main(String[] args) throws Exception{
>        Class.forName("org.apache.hadoop.hive.conf.HiveConf");
>     }
>
> }
>
> 2. javac Test.java
> 3. java -classpath
>
> ~/.m2/repository/org/spark-project/hive/hive-exec/0.12.0-protobuf-2.5/hive-exec-0.12.0-protobuf-2.5.jar:.
> Test
>
> Exception in thread "main" java.lang.UnsupportedClassVersionError:
> org/apache/hadoop/hive/conf/HiveConf : Unsupported major.minor version 51.0
> at java.lang.ClassLoader.defineClass1(Native Method)
> at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
> at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
> at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
> at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
> at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
> at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
> at java.security.AccessController.doPrivileged(Native Method)
> at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> at java.lang.Class.forName0(Native Method)
> at java.lang.Class.forName(Class.java:169)
> at Test.main(Test.java:5)
>
>
> Best Regards,
> Shixiong Zhu
>

--001a11c1c134ec63dd050b7bb3b2--

From dev-return-10989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 06:07:27 2014
Return-Path: <dev-return-10989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DBF510EFF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 06:07:27 +0000 (UTC)
Received: (qmail 46523 invoked by uid 500); 31 Dec 2014 06:07:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46449 invoked by uid 500); 31 Dec 2014 06:07:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45642 invoked by uid 99); 31 Dec 2014 06:07:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 06:07:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 06:06:59 +0000
Received: by mail-ig0-f169.google.com with SMTP id z20so2596583igj.2
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 22:06:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to
         :content-type;
        bh=+LT5b/trX7FT5/0h3ojFmDfAFU6r3k0J0GTpu7NX3Pw=;
        b=ujOCTd0d3LbC/4X6AOP9kAxDSSUWj5ahCpuSZeIpAsZfdDiyOlm+qn+kC+XIGirh77
         sMAJbWRP3TZHxCSaOGYE4/H8p3UjxxOoM1H5LLYjRME5vWzezzSLd2MLZSc/jivLgUYt
         CVFvgm5wv7Efo/mKUuRdDICIL7W8MPUDfwycXefnvjeGxTVhv07u1XGGiUy1fdJOUwBo
         NTXJFTckwZomzN+NJ7Vl7hgqzfHyupZ0eC+yRV9fVpMFRnwmcVs0hXINRrXt5laRd/M5
         gm80/4ABt9QbOKTMecHtbW0lbO63W6J6hxMZuDVZ9idIHg71Y/JNUrK9oc3KdDN+ExVb
         R6Hg==
X-Received: by 10.50.79.230 with SMTP id m6mr33486082igx.20.1420005972525;
 Tue, 30 Dec 2014 22:06:12 -0800 (PST)
MIME-Version: 1.0
References: <CAPEc=Ju+oR_EwinTnX-t_Dhi4xxpAHsRupEwHL4u-3Lphddahg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 31 Dec 2014 06:06:11 +0000
Message-ID: <CAOhmDzfiPG84vw6rRpfN1XCECSDqFr-gTBG9DuoX_g_UP04ohg@mail.gmail.com>
Subject: Re: Sample Spark Program Error
To: Naveen Madhire <vmadhire@umail.iu.edu>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01229aaa924bbc050b7ce789
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01229aaa924bbc050b7ce789
Content-Type: text/plain; charset=UTF-8

You sent this to the dev list. Please send it instead to the user list.

We use the dev list to discuss development on Spark itself, new features,
fixes to known bugs, and so forth.

The user list is to discuss issues using Spark, which I believe is what you
are looking for.

Nick


On Tue Dec 30 2014 at 11:27:52 PM Naveen Madhire <vmadhire@umail.iu.edu>
wrote:

> Hi All,
>
> I am trying to run a sample Spark program using Scala SBT,
>
> Below is the program,
>
> def main(args: Array[String]) {
>
>       val logFile = "E:/ApacheSpark/usb/usb/spark/bin/README.md" // Should
> be some file on your system
>       val sc = new SparkContext("local", "Simple App",
> "E:/ApacheSpark/usb/usb/spark/bin",List("target/scala-2.10/s
> bt2_2.10-1.0.jar"))
>       val logData = sc.textFile(logFile, 2).cache()
>
>       val numAs = logData.filter(line => line.contains("a")).count()
>       val numBs = logData.filter(line => line.contains("b")).count()
>
>       println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
>
>     }
>
>
> Below is the error log,
>
>
> 14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
> file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:0+673
> 14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2032) called
> with curMem=34047, maxMem=280248975
> 14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_0 stored as values
> in memory (estimated size 2032.0 B, free 267.2 MB)
> 14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on
> zealot:61452 (size: 2032.0 B, free: 267.3 MB)
> 14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
> rdd_1_0
> 14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 0.0
> (TID 0). 2300 bytes result sent to driver
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
> 0.0 (TID 1, localhost, PROCESS_LOCAL, 1264 bytes)
> 14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 0.0
> (TID 1)
> 14/12/30 23:20:21 INFO spark.CacheManager: Partition rdd_1_1 not found,
> computing it
> 14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
> file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:673+673
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
> 0.0 (TID 0) in 3507 ms on localhost (1/2)
> 14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(1912) called
> with curMem=36079, maxMem=280248975
> 14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_1 stored as values
> in memory (estimated size 1912.0 B, free 267.2 MB)
> 14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_1 in memory on
> zealot:61452 (size: 1912.0 B, free: 267.3 MB)
> 14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
> rdd_1_1
> 14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 0.0
> (TID 1). 2300 bytes result sent to driver
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
> 0.0 (TID 1) in 261 ms on localhost (2/2)
> 14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
> whose tasks have all completed, from pool
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 0 (count at
> Test1.scala:19) finished in 3.811 s
> 14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
> Test1.scala:19, took 3.997365232 s
> 14/12/30 23:20:21 INFO spark.SparkContext: Starting job: count at
> Test1.scala:20
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Got job 1 (count at
> Test1.scala:20) with 2 output partitions (allowLocal=false)
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Final stage: Stage 1(count
> at Test1.scala:20)
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Parents of final stage:
> List()
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Missing parents: List()
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting Stage 1
> (FilteredRDD[3] at filter at Test1.scala:20), which has no missing parents
> 14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2600) called
> with curMem=37991, maxMem=280248975
> 14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 stored as
> values in memory (estimated size 2.5 KB, free 267.2 MB)
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks
> from Stage 1 (FilteredRDD[3] at filter at Test1.scala:20)
> 14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
> with 2 tasks
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
> 1.0 (TID 2, localhost, ANY, 1264 bytes)
> 14/12/30 23:20:21 INFO executor.Executor: Running task 0.0 in stage 1.0
> (TID 2)
> 14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_0 locally
> 14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 1.0
> (TID 2). 1731 bytes result sent to driver
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
> 1.0 (TID 3, localhost, ANY, 1264 bytes)
> 14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 1.0
> (TID 3)
> 14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_1 locally
> 14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 1.0
> (TID 3). 1731 bytes result sent to driver
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
> 1.0 (TID 3) in 7 ms on localhost (1/2)
> 14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
> 1.0 (TID 2) in 16 ms on localhost (2/2)
> 14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 1 (count at
> Test1.scala:20) finished in 0.016 s
> 14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0,
> whose tasks have all completed, from pool
> 14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
> Test1.scala:20, took 0.041709824 s
> Lines with a: 24, Lines with b: 15
> 14/12/30 23:20:21 ERROR util.Utils: Uncaught exception in thread
> SparkListenerBus
> java.lang.InterruptedException
> at
> java.util.concurrent.locks.AbstractQueuedSynchronizer.acquir
> eSharedInterruptibly(AbstractQueuedSynchronizer.java:1301)
> at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
> at
> org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$
> run$1.apply$mcV$sp(LiveListenerBus.scala:48)
> at
> org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(
> LiveListenerBus.scala:47)
> at
> org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(
> LiveListenerBus.scala:47)
> at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
> [success] Total time: 12 s, completed Dec 30, 2014 11:20:21 PM
> at
> org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveL
> istenerBus.scala:46)
> 14/12/30 23:20:21 ERROR spark.ContextCleaner: Error in cleaning thread
> java.lang.InterruptedException
> at java.lang.Object.wait(Native Method)
> at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
> at
> org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$
> keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
> at
> org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$
> keepCleaning$1.apply(ContextCleaner.scala:134)
> at
> org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$
> keepCleaning$1.apply(ContextCleaner.scala:134)
> at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
> at org.apache.spark.ContextCleaner.org
> $apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
> at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
> 14/12/30 23:20:21 INFO network.ConnectionManager: Selector thread was
> interrupted!
> 14/12/30 23:20:21 INFO storage.BlockManager: Removing broadcast 2
> 14/12/30 23:20:21 INFO storage.BlockManager: Removing block broadcast_2
> 14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 of size 2600
> dropped from memory (free 280210984)
> 14/12/30 23:20:21 INFO spark.ContextCleaner: Cleaned broadcast 2
>
>
>
>
> Please let me know any pointers to debug the error.
>
>
> Thanks a lot.
>

--089e01229aaa924bbc050b7ce789--

From dev-return-10990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 07:02:23 2014
Return-Path: <dev-return-10990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A58D10047
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 07:02:23 +0000 (UTC)
Received: (qmail 16664 invoked by uid 500); 31 Dec 2014 07:02:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16574 invoked by uid 500); 31 Dec 2014 07:02:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15207 invoked by uid 99); 31 Dec 2014 07:02:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 07:02:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 07:01:54 +0000
Received: by mail-ob0-f172.google.com with SMTP id va8so47433160obc.3
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 23:01:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=XaASB1MuqUgHslkrEh0ZjAkvWMpzm4aQ7LBmCB69YHs=;
        b=aRA9QDgi+RiYC1zEeMY1fwd4HZ9DSzdg5K2q0fiflwjg1AlQpxU/8fzDUxitRdlMxN
         +y0A3/A3teAMTQsrr8RUDMM8GtqDGvPZ6BzRz0wmIja2/BVG+5z7VB1w5OmBYVdIECrI
         KLURfY8rDtz6HBSfcG8YhaAuO5FEtEaGP1Gu7r1KH2PELKNu3xP3TDvzjc5Zit2hkuFG
         6J74GH2ARk92NFwnT9vU4ELxLM40D+GuRJupLmQ6jzKpIcavm6JqNqpdf9FHMPm52Sjg
         ms8qWdTpWJyVWptHlFIX3m9aIr3oz1p387S1Tpcfpa7COAz2+EC7ugqo6YRLoDUUmi9d
         BVKw==
X-Received: by 10.202.93.135 with SMTP id r129mr36048628oib.53.1420009312556;
 Tue, 30 Dec 2014 23:01:52 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Tue, 30 Dec 2014 23:01:32 -0800 (PST)
In-Reply-To: <CAJc_syL7A-A7HZJM9TZgBD9_0CDt6X7xUxVc118gcJW6yHb4ow@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
 <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
 <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syL8C++DHQFj_HAYGmO9Xng0kvswpi+V7sH8vSyEYdVJQw@mail.gmail.com> <CAJc_syL7A-A7HZJM9TZgBD9_0CDt6X7xUxVc118gcJW6yHb4ow@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Tue, 30 Dec 2014 23:01:32 -0800
Message-ID: <CAJc_syLQui==wz8SLcNxw8bfCV327NTte3ZkAT0NtE8kB4REmA@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d4a96a71c40050b7dae11
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d4a96a71c40050b7dae11
Content-Type: text/plain; charset=UTF-8

Here's a more meaningful exception:

java.lang.ClassCastException: org.apache.spark.sql.catalyst.types.DateType$
cannot be cast to org.apache.spark.sql.catalyst.types.PrimitiveType
        at
org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:188)
        at
org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:167)
        at
org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:130)
        at
parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
$apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)


This is easy to fix even for a newbie like myself: it suffices to add the
PrimitiveType trait to the DateType object. You can find this change here:

https://github.com/alexbaretta/spark/compare/parquet-date-support

However, even this does not work. Here's the next blocker:

java.lang.RuntimeException: Unsupported datatype DateType, cannot write to
consumer
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:361)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:329)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:315)
        at
parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
$apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Any input on how to address this issue would be welcome.

Alex

On Tue, Dec 30, 2014 at 5:21 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Sorry! My bad. I had stale spark jars sitting on the slave nodes...
>
> Alex
>
> On Tue, Dec 30, 2014 at 4:39 PM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Gents,
>>
>> I tried #3820. It doesn't work. I'm still getting the following
>> exceptions:
>>
>> Exception in thread "Thread-45" java.lang.RuntimeException: Unsupported
>> datatype DateType
>>         at scala.sys.package$.error(package.scala:27)
>>         at
>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:343)
>>         at
>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:292)
>>         at scala.Option.getOrElse(Option.scala:120)
>>         at
>> org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:291)
>>         at
>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:363)
>>         at
>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:362)
>>
>> I would more than happy to fix this myself, but I would need some help
>> wading through the code. Could anyone explain to me what exactly is needed
>> to support a new data type in SparkSQL's Parquet storage engine?
>>
>> Thanks.
>>
>> Alex
>>
>> On Mon, Dec 29, 2014 at 10:20 PM, Wang, Daoyuan <daoyuan.wang@intel.com>
>> wrote:
>>
>>>  By adding a flag in SQLContext, I have modified #3822 to include
>>> nanoseconds now. Since passing too many flags is ugly, now I need the whole
>>> SQLContext, so that we can put more flags there.
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Daoyuan
>>>
>>>
>>>
>>> *From:* Michael Armbrust [mailto:michael@databricks.com]
>>> *Sent:* Tuesday, December 30, 2014 10:43 AM
>>> *To:* Alessandro Baretta
>>> *Cc:* Wang, Daoyuan; dev@spark.apache.org
>>> *Subject:* Re: Unsupported Catalyst types in Parquet
>>>
>>>
>>>
>>> Yeah, I saw those.  The problem is that #3822 truncates timestamps that
>>> include nanoseconds.
>>>
>>>
>>>
>>> On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <
>>> alexbaretta@gmail.com> wrote:
>>>
>>> Michael,
>>>
>>>
>>>
>>> Actually, Adrian Wang already created pull requests for these issues.
>>>
>>>
>>>
>>> https://github.com/apache/spark/pull/3820
>>>
>>> https://github.com/apache/spark/pull/3822
>>>
>>>
>>>
>>> What do you think?
>>>
>>>
>>>
>>> Alex
>>>
>>>
>>>
>>> On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <
>>> michael@databricks.com> wrote:
>>>
>>> I'd love to get both of these in.  There is some trickiness that I talk
>>> about on the JIRA for timestamps since the SQL timestamp class can support
>>> nano seconds and I don't think parquet has a type for this.  Other systems
>>> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
>>> mailing list what the plan is there to make sure that whatever we do is
>>> going to be compatible long term.
>>>
>>>
>>>
>>> Michael
>>>
>>>
>>>
>>> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <
>>> alexbaretta@gmail.com> wrote:
>>>
>>> Daoyuan,
>>>
>>> Thanks for creating the jiras. I need these features by... last week, so
>>> I'd be happy to take care of this myself, if only you or someone more
>>> experienced than me in the SparkSQL codebase could provide some guidance.
>>>
>>> Alex
>>>
>>> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com>
>>> wrote:
>>>
>>> Hi Alex,
>>>
>>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>>> support decimals that fits in a long.
>>>
>>> Thanks,
>>> Daoyuan
>>>
>>> -----Original Message-----
>>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>>> Sent: Saturday, December 27, 2014 2:47 PM
>>> To: dev@spark.apache.org; Michael Armbrust
>>> Subject: Unsupported Catalyst types in Parquet
>>>
>>> Michael,
>>>
>>> I'm having trouble storing my SchemaRDDs in Parquet format with
>>> SparkSQL, due to my RDDs having having DateType and DecimalType fields.
>>> What would it take to add Parquet support for these Catalyst? Are there any
>>> other Catalyst types for which there is no Catalyst support?
>>>
>>> Alex
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>
>>
>

--001a113d4a96a71c40050b7dae11--

From dev-return-10991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 07:33:52 2014
Return-Path: <dev-return-10991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31936100D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 07:33:52 +0000 (UTC)
Received: (qmail 62689 invoked by uid 500); 31 Dec 2014 07:33:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62603 invoked by uid 500); 31 Dec 2014 07:33:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62591 invoked by uid 99); 31 Dec 2014 07:33:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 07:33:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alexbaretta@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 07:33:24 +0000
Received: by mail-ob0-f172.google.com with SMTP id va8so47489550obc.3
        for <dev@spark.apache.org>; Tue, 30 Dec 2014 23:32:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=kAGMS/bX0A2+ldStejuI6nRr8Jg3VRebDnCbdnELFdc=;
        b=arI8BSKF2Y3/43ugURm7Zdrs+vvtPm6058+1UAF5nDjbNNMxBdfYoTS6xbDX/7AK7l
         OEQMO0xIkQrv9WVLry3IWjn4qzgME+j/qCmOaDdYs8i134405KXtBz+VSQCTBgBC7QFu
         DmMkhvRRN4kekk0jlAKRmO+d8THAMbLd9w+JvvVionPRiaWbFmzxc/6ZWILZoNgK0Jb8
         Vi3nyc7BZbyPtoyjCj1u4jihXLtTXzdMYlXJFy5JKs3LPfMW8sZ+A23sQhBVhi4HQk0/
         a7bqKSn6cj71E2u/QpQzd7hg0IFLqA9L2Swk6DnryvxJuszRkBCXQ3EUG55+om+r/W0h
         hcVQ==
X-Received: by 10.60.132.7 with SMTP id oq7mr38007848oeb.57.1420011157649;
 Tue, 30 Dec 2014 23:32:37 -0800 (PST)
MIME-Version: 1.0
Received: by 10.76.111.148 with HTTP; Tue, 30 Dec 2014 23:32:17 -0800 (PST)
In-Reply-To: <CAJc_syLQui==wz8SLcNxw8bfCV327NTte3ZkAT0NtE8kB4REmA@mail.gmail.com>
References: <CAJc_syKTduq1GZyYYDuABgi=Fza3a9v70ahdb02Py_Eoy6=MSA@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC403@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syJQngv_ip9ZmuNEB6OB5q7D1CT9V1X5=WTY4f1DvetYVw@mail.gmail.com>
 <CAAswR-50oXexjLZFqu7OjO_aMEc5qeET++9tk0uFjucrDpwYKg@mail.gmail.com>
 <CAJc_sy+4c7Y=HK4XKoqgefux=cTrdQ_6rRF209moBso_BnGQ0g@mail.gmail.com>
 <CAAswR-78A-Qx_DqPuW=_61akRiL5S4JD5LPRqZV4CWCfz==c1w@mail.gmail.com>
 <CF8AD92F21C4704CAD056AADDD3CCC09012EC8D7@SHSMSX101.ccr.corp.intel.com>
 <CAJc_syL8C++DHQFj_HAYGmO9Xng0kvswpi+V7sH8vSyEYdVJQw@mail.gmail.com>
 <CAJc_syL7A-A7HZJM9TZgBD9_0CDt6X7xUxVc118gcJW6yHb4ow@mail.gmail.com> <CAJc_syLQui==wz8SLcNxw8bfCV327NTte3ZkAT0NtE8kB4REmA@mail.gmail.com>
From: Alessandro Baretta <alexbaretta@gmail.com>
Date: Tue, 30 Dec 2014 23:32:17 -0800
Message-ID: <CAJc_syJzxuBUOBBp+KiKa-LKjeZdLnC5Nncfg+NGZ8iPRcechQ@mail.gmail.com>
Subject: Re: Unsupported Catalyst types in Parquet
To: "Wang, Daoyuan" <daoyuan.wang@intel.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4722aea10102050b7e1cde
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4722aea10102050b7e1cde
Content-Type: text/plain; charset=UTF-8

I think I might have figure it out myself. Here's a pull request for you
guys to check out:

https://github.com/apache/spark/pull/3855

I successfully tested this code on my cluster.

On Tue, Dec 30, 2014 at 11:01 PM, Alessandro Baretta <alexbaretta@gmail.com>
wrote:

> Here's a more meaningful exception:
>
> java.lang.ClassCastException:
> org.apache.spark.sql.catalyst.types.DateType$ cannot be cast to
> org.apache.spark.sql.catalyst.types.PrimitiveType
>         at
> org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:188)
>         at
> org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:167)
>         at
> org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:130)
>         at
> parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
>         at
> parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
>         at
> parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
>         at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
> $apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
>         at
> org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
>         at
> org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>         at org.apache.spark.scheduler.Task.run(Task.scala:56)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
>
>
> This is easy to fix even for a newbie like myself: it suffices to add the
> PrimitiveType trait to the DateType object. You can find this change here:
>
> https://github.com/alexbaretta/spark/compare/parquet-date-support
>
> However, even this does not work. Here's the next blocker:
>
> java.lang.RuntimeException: Unsupported datatype DateType, cannot write to
> consumer
>         at scala.sys.package$.error(package.scala:27)
>         at
> org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:361)
>         at
> org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:329)
>         at
> org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:315)
>         at
> parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
>         at
> parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
>         at
> parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
>         at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
> $apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
>         at
> org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
>         at
> org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>         at org.apache.spark.scheduler.Task.run(Task.scala:56)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:745)
>
> Any input on how to address this issue would be welcome.
>
> Alex
>
> On Tue, Dec 30, 2014 at 5:21 PM, Alessandro Baretta <alexbaretta@gmail.com
> > wrote:
>
>> Sorry! My bad. I had stale spark jars sitting on the slave nodes...
>>
>> Alex
>>
>> On Tue, Dec 30, 2014 at 4:39 PM, Alessandro Baretta <
>> alexbaretta@gmail.com> wrote:
>>
>>> Gents,
>>>
>>> I tried #3820. It doesn't work. I'm still getting the following
>>> exceptions:
>>>
>>> Exception in thread "Thread-45" java.lang.RuntimeException: Unsupported
>>> datatype DateType
>>>         at scala.sys.package$.error(package.scala:27)
>>>         at
>>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:343)
>>>         at
>>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:292)
>>>         at scala.Option.getOrElse(Option.scala:120)
>>>         at
>>> org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:291)
>>>         at
>>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:363)
>>>         at
>>> org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:362)
>>>
>>> I would more than happy to fix this myself, but I would need some help
>>> wading through the code. Could anyone explain to me what exactly is needed
>>> to support a new data type in SparkSQL's Parquet storage engine?
>>>
>>> Thanks.
>>>
>>> Alex
>>>
>>> On Mon, Dec 29, 2014 at 10:20 PM, Wang, Daoyuan <daoyuan.wang@intel.com>
>>> wrote:
>>>
>>>>  By adding a flag in SQLContext, I have modified #3822 to include
>>>> nanoseconds now. Since passing too many flags is ugly, now I need the whole
>>>> SQLContext, so that we can put more flags there.
>>>>
>>>>
>>>>
>>>> Thanks,
>>>>
>>>> Daoyuan
>>>>
>>>>
>>>>
>>>> *From:* Michael Armbrust [mailto:michael@databricks.com]
>>>> *Sent:* Tuesday, December 30, 2014 10:43 AM
>>>> *To:* Alessandro Baretta
>>>> *Cc:* Wang, Daoyuan; dev@spark.apache.org
>>>> *Subject:* Re: Unsupported Catalyst types in Parquet
>>>>
>>>>
>>>>
>>>> Yeah, I saw those.  The problem is that #3822 truncates timestamps that
>>>> include nanoseconds.
>>>>
>>>>
>>>>
>>>> On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <
>>>> alexbaretta@gmail.com> wrote:
>>>>
>>>> Michael,
>>>>
>>>>
>>>>
>>>> Actually, Adrian Wang already created pull requests for these issues.
>>>>
>>>>
>>>>
>>>> https://github.com/apache/spark/pull/3820
>>>>
>>>> https://github.com/apache/spark/pull/3822
>>>>
>>>>
>>>>
>>>> What do you think?
>>>>
>>>>
>>>>
>>>> Alex
>>>>
>>>>
>>>>
>>>> On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <
>>>> michael@databricks.com> wrote:
>>>>
>>>> I'd love to get both of these in.  There is some trickiness that I talk
>>>> about on the JIRA for timestamps since the SQL timestamp class can support
>>>> nano seconds and I don't think parquet has a type for this.  Other systems
>>>> (impala) seem to use INT96.  It would be great to maybe ask on the parquet
>>>> mailing list what the plan is there to make sure that whatever we do is
>>>> going to be compatible long term.
>>>>
>>>>
>>>>
>>>> Michael
>>>>
>>>>
>>>>
>>>> On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <
>>>> alexbaretta@gmail.com> wrote:
>>>>
>>>> Daoyuan,
>>>>
>>>> Thanks for creating the jiras. I need these features by... last week,
>>>> so I'd be happy to take care of this myself, if only you or someone more
>>>> experienced than me in the SparkSQL codebase could provide some guidance.
>>>>
>>>> Alex
>>>>
>>>> On Dec 29, 2014 12:06 AM, "Wang, Daoyuan" <daoyuan.wang@intel.com>
>>>> wrote:
>>>>
>>>> Hi Alex,
>>>>
>>>> I'll create JIRA SPARK-4985 for date type support in parquet, and
>>>> SPARK-4987 for timestamp type support. For decimal type, I think we only
>>>> support decimals that fits in a long.
>>>>
>>>> Thanks,
>>>> Daoyuan
>>>>
>>>> -----Original Message-----
>>>> From: Alessandro Baretta [mailto:alexbaretta@gmail.com]
>>>> Sent: Saturday, December 27, 2014 2:47 PM
>>>> To: dev@spark.apache.org; Michael Armbrust
>>>> Subject: Unsupported Catalyst types in Parquet
>>>>
>>>> Michael,
>>>>
>>>> I'm having trouble storing my SchemaRDDs in Parquet format with
>>>> SparkSQL, due to my RDDs having having DateType and DecimalType fields.
>>>> What would it take to add Parquet support for these Catalyst? Are there any
>>>> other Catalyst types for which there is no Catalyst support?
>>>>
>>>> Alex
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>
>

--047d7b4722aea10102050b7e1cde--

From dev-return-10992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 16:14:40 2014
Return-Path: <dev-return-10992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9290D10EF4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 16:14:40 +0000 (UTC)
Received: (qmail 63827 invoked by uid 500); 31 Dec 2014 16:14:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63749 invoked by uid 500); 31 Dec 2014 16:14:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63716 invoked by uid 99); 31 Dec 2014 16:14:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 16:14:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_IMAGE_ONLY_20,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eshioji@gmail.com designates 209.85.215.42 as permitted sender)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 16:14:31 +0000
Received: by mail-la0-f42.google.com with SMTP id gd6so14163666lab.15
        for <dev@spark.apache.org>; Wed, 31 Dec 2014 08:12:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=3l60dqaTB59LZnViTOm0MMG5VOMa+sRFrB5MecORdKc=;
        b=UKikyP8/45Di4kYq2DqYNi9HRQ4+JcICadvY5RKXNmpIjBeq2XD9dHKvg0ykRVEf9O
         q2kXb+xCMX72tE0PFcVCXlXPz2Z6Ebh2LYHmOo+wxCly6//tFQ7+TqYQLPDyEu1rpnmR
         yKdFRCw1FaqeX43C4Wml8TCRycB8Soj1LbNSiRb8Lr9awDFiXhgkUxSXTx8rArK4tfJN
         YpAxBR0S0kl6FGkDJeTIHXunRmfE+bRByTkyNijjsVUBDgPFNV0xIHeV+reC0Dz3kzsO
         eTYQXX+AKBoTJZ0J7i+s2SUMWzulxtIjg2iSr/MA8U9oWl7y2V6apeutialHLAUKsNv5
         aRcA==
MIME-Version: 1.0
X-Received: by 10.112.130.65 with SMTP id oc1mr67748074lbb.7.1420042360228;
 Wed, 31 Dec 2014 08:12:40 -0800 (PST)
Received: by 10.112.176.36 with HTTP; Wed, 31 Dec 2014 08:12:40 -0800 (PST)
Date: Wed, 31 Dec 2014 16:12:40 +0000
Message-ID: <CAE50=dpVrcp_3_bjbWJMtXnsqrfuThpcJ5j2CRQaamLvbbhSjw@mail.gmail.com>
Subject: Big performance difference between "client" and "cluster" deployment
 mode; is this expected?
From: Enno Shioji <eshioji@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b343318729420050b8560c1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b343318729420050b8560c1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I have a very, very simple streaming job. When I deploy this on the exact
same cluster, with the exact same parameters, I see big (40%) performance
difference between "client" and "cluster" deployment mode. This seems a bit
surprising.. Is this expected?

The streaming job is:

    val msgStream =3D kafkaStream
      .map { case (k, v) =3D> v}
      .map(DatatypeConverter.printBase64Binary)
      .foreachRDD(save)
      .saveAsTextFile("s3n://some.bucket/path", classOf[LzoCodec])

I tried several times, but the job deployed with "client" mode can only
write at 60% throughput of the job deployed with "cluster" mode and this
happens consistently. I'm logging at INFO level, but my application code
doesn't log anything so it's only Spark logs. The logs I see in "client"
mode doesn't seem like a crazy amount.

The setup is:
spark-ec2 [...] \
  --copy-aws-credentials \
  --instance-type=3Dm3.2xlarge \
  -s 2 launch test_cluster

And all the deployment was done from the master machine.

=E1=90=A7

--047d7b343318729420050b8560c1--

From dev-return-10993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 18:24:46 2014
Return-Path: <dev-return-10993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4269410739
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 18:24:46 +0000 (UTC)
Received: (qmail 65254 invoked by uid 500); 31 Dec 2014 18:24:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65160 invoked by uid 500); 31 Dec 2014 18:24:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62541 invoked by uid 99); 31 Dec 2014 18:24:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 18:24:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 18:24:16 +0000
Received: by mail-wi0-f171.google.com with SMTP id bs8so25879184wib.10
        for <dev@spark.apache.org>; Wed, 31 Dec 2014 10:22:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=byFIotF8jM432CH6ZvdiuLD+KaUpVvAnvtDW0M6v+f0=;
        b=Uk7CyoeBs1ZLauQHvOEo5+z4N43sRDpna6NtCCS/y9PZ3MAQZZT+1cNB9paB/d0uJN
         KUYaItLFlpL/QcPh/nBF3PalF4PJxuSuCdmbCzaZtU0LfbKebcpYBpRjDnxYTHfZ93w/
         TTYq3m50+gqz5Jbxztfz5qSP+gcN/N3+3r3RP78nl27J1XonawNssPYjpcMz1tU07zkS
         ZF27q7RJF4Y+UJ8ZxgYkZwqp0veT0tdjL+LJpOx77THc7xlrjST6qRcUsV8f5HCn034o
         e1EPmbQZmC0F1q9HlIMQ8T2POiix4vWqPhKbNp2SC41pLauK9z+6lZpRV1Q+agOpvKAe
         0QiA==
X-Gm-Message-State: ALoCoQlP78PIqclaEUS5p+hoX+TQw0lo+njE3xkG8EZiMj2Osoj3G/7i+kO89z3NZLFURZ7YDIw6
X-Received: by 10.194.77.38 with SMTP id p6mr120714065wjw.62.1420050120533;
 Wed, 31 Dec 2014 10:22:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.204 with HTTP; Wed, 31 Dec 2014 10:21:40 -0800 (PST)
In-Reply-To: <CAE50=dpVrcp_3_bjbWJMtXnsqrfuThpcJ5j2CRQaamLvbbhSjw@mail.gmail.com>
References: <CAE50=dpVrcp_3_bjbWJMtXnsqrfuThpcJ5j2CRQaamLvbbhSjw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 31 Dec 2014 18:21:40 +0000
Message-ID: <CAMAsSd+Gau0QT70oY=iA6dq4MDy6GTvzhuPwNdrO6VSA_y1=DQ@mail.gmail.com>
Subject: Re: Big performance difference between "client" and "cluster"
 deployment mode; is this expected?
To: Enno Shioji <eshioji@gmail.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

-dev, +user

A decent guess: Does your 'save' function entail collecting data back
to the driver? and are you running this from a machine that's not in
your Spark cluster? Then in client mode you're shipping data back to a
less-nearby machine, compared to with cluster mode. That could explain
the bottleneck.

On Wed, Dec 31, 2014 at 4:12 PM, Enno Shioji <eshioji@gmail.com> wrote:
> Hi,
>
> I have a very, very simple streaming job. When I deploy this on the exact
> same cluster, with the exact same parameters, I see big (40%) performance
> difference between "client" and "cluster" deployment mode. This seems a b=
it
> surprising.. Is this expected?
>
> The streaming job is:
>
>     val msgStream =3D kafkaStream
>       .map { case (k, v) =3D> v}
>       .map(DatatypeConverter.printBase64Binary)
>       .foreachRDD(save)
>       .saveAsTextFile("s3n://some.bucket/path", classOf[LzoCodec])
>
> I tried several times, but the job deployed with "client" mode can only
> write at 60% throughput of the job deployed with "cluster" mode and this
> happens consistently. I'm logging at INFO level, but my application code
> doesn't log anything so it's only Spark logs. The logs I see in "client"
> mode doesn't seem like a crazy amount.
>
> The setup is:
> spark-ec2 [...] \
>   --copy-aws-credentials \
>   --instance-type=3Dm3.2xlarge \
>   -s 2 launch test_cluster
>
> And all the deployment was done from the master machine.
>
> =E1=90=A7

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-10994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Dec 31 20:19:28 2014
Return-Path: <dev-return-10994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CBA8109D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 31 Dec 2014 20:19:28 +0000 (UTC)
Received: (qmail 91707 invoked by uid 500); 31 Dec 2014 20:19:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91619 invoked by uid 500); 31 Dec 2014 20:19:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91608 invoked by uid 99); 31 Dec 2014 20:19:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 20:19:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.41] (HELO mail-la0-f41.google.com) (209.85.215.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 31 Dec 2014 20:19:21 +0000
Received: by mail-la0-f41.google.com with SMTP id hv19so14003191lab.14
        for <dev@spark.apache.org>; Wed, 31 Dec 2014 12:17:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Sbpe2ha8aRYEhZ9DGDem9CL5gliTNoGWY3zGEcZRf+Y=;
        b=MxXKKVVvcVxq/W57mGzEVs+qBMnuf0VsD/oeU6fUN5X8m692S3kSQ63A7whTerqw+G
         iDKeHZEBuWsHA/+5WgVzCORoP0LEbPVdDlPe4Gg55MnSD4ynJ7/EllERPbPk8hsKeCZ4
         f3JpW2iRleV01DtVinvVhmFIamtaJ0OJRH8mGHE55h0h2w1j2JPsAH/r93ZIUyMLFejV
         +ZEU68ZqiKd4QvTELsZXttYLzeiedtup4lpz3Np6dK4AHdHXK1qVfFpWN32htq3Pj2rp
         u142gt9UP1ob6RhJBD1fyBuxZwvsSbO9k1w5UYBzLWOCAghpoVNn7xWWUR9k49zHaiTL
         Yrxg==
X-Gm-Message-State: ALoCoQm4htUUDlTEvyK5AWiv3Kn6q112cmu/jVF1tqK715Iy1Z4s/F1M40NV00XuI47YBbW+nW8V
X-Received: by 10.152.36.1 with SMTP id m1mr68158906laj.95.1420057028698; Wed,
 31 Dec 2014 12:17:08 -0800 (PST)
MIME-Version: 1.0
Received: by 10.25.80.208 with HTTP; Wed, 31 Dec 2014 12:16:48 -0800 (PST)
In-Reply-To: <CALte62xcLeVi3HDisu-mLFTxzHyFOa8QB_34n=GeqdCt5svrbg@mail.gmail.com>
References: <CAPn6-YSCsSgmS5JV=i9KZNrZrfVhxtejWzRexZMXhrBykJ11GA@mail.gmail.com>
 <CALte62xcLeVi3HDisu-mLFTxzHyFOa8QB_34n=GeqdCt5svrbg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 31 Dec 2014 12:16:48 -0800
Message-ID: <CAAswR-5jTnB8q-kQ-tgU+ZM3kPrskZSg8Hcs4ccZuJ+Xqtvp4w@mail.gmail.com>
Subject: Re: Why the major.minor version of the new hive-exec is 51.0?
To: Ted Yu <yuzhihong@gmail.com>
Cc: Shixiong Zhu <zsxwing@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b7ccc1b5c3050b88ca6c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b7ccc1b5c3050b88ca6c
Content-Type: text/plain; charset=UTF-8

This was not intended, can you open a JIRA?

On Tue, Dec 30, 2014 at 8:40 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> I extracted org/apache/hadoop/hive/common/CompressionUtils.class from the
> jar and used hexdump to view the class file.
> Bytes 6 and 7 are 00 and 33, respectively.
>
> According to http://en.wikipedia.org/wiki/Java_class_file, the jar was
> produced using Java 7.
>
> FYI
>
> On Tue, Dec 30, 2014 at 8:09 PM, Shixiong Zhu <zsxwing@gmail.com> wrote:
>
> > The major.minor version of the new org.spark-project.hive.hive-exec is
> > 51.0, so it will require people use JDK7. Is it intentional?
> >
> > <dependency>
> > <groupId>org.spark-project.hive</groupId>
> > <artifactId>hive-exec</artifactId>
> > <version>0.12.0-protobuf-2.5</version>
> > </dependency>
> >
> > You can use the following steps to reproduce it (Need to use JDK6):
> >
> > 1. Create a Test.java file with the following content:
> >
> > public class Test {
> >
> >     public static void main(String[] args) throws Exception{
> >        Class.forName("org.apache.hadoop.hive.conf.HiveConf");
> >     }
> >
> > }
> >
> > 2. javac Test.java
> > 3. java -classpath
> >
> >
> ~/.m2/repository/org/spark-project/hive/hive-exec/0.12.0-protobuf-2.5/hive-exec-0.12.0-protobuf-2.5.jar:.
> > Test
> >
> > Exception in thread "main" java.lang.UnsupportedClassVersionError:
> > org/apache/hadoop/hive/conf/HiveConf : Unsupported major.minor version
> 51.0
> > at java.lang.ClassLoader.defineClass1(Native Method)
> > at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
> > at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
> > at
> java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
> > at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
> > at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
> > at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
> > at java.security.AccessController.doPrivileged(Native Method)
> > at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
> > at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
> > at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > at java.lang.Class.forName0(Native Method)
> > at java.lang.Class.forName(Class.java:169)
> > at Test.main(Test.java:5)
> >
> >
> > Best Regards,
> > Shixiong Zhu
> >
>

--089e0158b7ccc1b5c3050b88ca6c--

